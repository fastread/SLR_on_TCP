Val_ID,Document Title,Abstract,Year,PDF Link,RS,HT,XY,JC,TX,FF,Majority Vote,ZY,Full text validation (ZY=yes or Majority Vote=yes),label
36,Test case prioritization: an empirical study,Test case prioritization techniques schedule test cases for execution in an order that attempts to maximize some objective function. A variety of objective functions are applicable; one such function involves rate of fault detection-a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during regression testing can provide faster feedback on a system under regression test and let debuggers begin their work earlier than might otherwise be possible. In this paper we describe several techniques for prioritizing test cases and report our empirical results measuring the effectiveness of these techniques for improving rate of fault detection. The results provide insights into the tradeoffs among various techniques for test case prioritization.,1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792604,yes,,yes,,,,yes,yes,yes,yes
50,Incorporating varying test costs and fault severities into test case prioritization,"Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919106,yes,,yes,,,,yes,yes,yes,yes
53,Prioritizing test cases for regression testing,"Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detection, a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test and let software engineers begin correcting faults earlier than might otherwise be possible. One application of prioritization techniques involves regression testing, the retesting of software following modifications; in this context, prioritization techniques can take advantage of information gathered about the previous execution of test cases to obtain test case orderings. We describe several techniques for using test execution information to prioritize test cases for regression testing, including: 1) techniques that order test cases based on their total coverage of code components; 2) techniques that order test cases based on their coverage of code components not previously covered; and 3) techniques that order test cases based on their estimated ability to reveal faults in the code components that they cover. We report the results of several experiments in which we applied these techniques to various test suites for various programs and measured the rates of fault detection achieved by the prioritized test suites, comparing those rates to the rates achieved by untreated, randomly ordered, and optimally ordered suites.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962562,yes,,yes,,,,yes,yes,yes,yes
55,Test-suite reduction and prioritization for modified condition/decision coverage,"Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique, and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows, and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software, according to some criterion, as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. The paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of a case study of the test-suite reduction algorithm.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972715,yes,,no,,,,yes,yes,yes,yes
57,Understanding and measuring the sources of variation in the prioritization of regression test suites,"Test case prioritization techniques let testers order their test cases so that those with higher priority, according to some criterion, are executed earlier than those with lower priority. In previous work (1999, 2000), we examined a variety of prioritization techniques to determine their ability to improve the rate of fault detection of test suites. Our studies showed that the rate of fault detection of test suites could be significantly improved by using more powerful prioritization techniques. In addition, they indicated that rate of fault detection was closely associated with the target program. We also observed a large quantity of unexplained variance, indicating that other factors must be affecting prioritization effectiveness. These observations motivate the following research questions. (1) Are there factors other than the target program and the prioritization technique that consistently affect the rate of fault detection of test suites? (2) What metrics are most representative of each factor? (3) Can the consideration of additional factors lead to more efficient prioritization techniques? To address these questions, we performed a series of experiments exploring three factors: program structure, test suite composition and change characteristics. This paper reports the results and implications of those experiments.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915525,yes,,yes,,,,yes,yes,yes,yes
59,A history-based test prioritization technique for regression testing in resource constrained environments,"Regression testing is an expensive and frequently executed maintenance process used to revalidate modified software. To improve it, regression test selection (RTS) techniques strive to lower costs without overly reducing effectiveness by carefully selecting a subset of the test suite. Under certain conditions, some can even guarantee that the selected test cases perform no worse than the original test suite. This ignores certain software development realities such as resource and time constraints that may prevent using RTS techniques as intended (e.g., regression testing must be done overnight, but RTS selection returns two days worth of tests). In practice, testers work around this by prioritizing the test cases and running only those that fit within existing constraints. Unfortunately this generally violates key RTS assumptions, voiding RTS technique guarantees and making regression testing performance unpredictable. Despite this, existing prioritization techniques are memoryless, implicitly assuming that local choices can ensure adequate long run performance. Instead, we propose a new technique that bases prioritization on historical execution data. We conducted an experiment to assess its effects on the long run performance of resource constrained regression testing. Our results expose essential tradeoffs that should be considered when using these techniques over a series of software releases.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007961,no,,yes,,,,yes,yes,yes,yes
70,Test case prioritization: a family of empirical studies,"To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of such prioritization is to increase a test suite's rate of fault detection. Previous work reported results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: 1) Can prioritization techniques be effective when targeted at specific modified versions; 2) what trade-offs exist between fine granularity and coarse granularity prioritization techniques; 3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? To address these questions, we have performed several new studies in which we empirically compared prioritization techniques using both controlled experiments and case studies.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988497,yes,,yes,,,,yes,yes,yes,yes
71,A comparison of coverage-based and distribution-based techniques for filtering and prioritizing test cases,"This paper presents an empirical comparison of four different techniques for filtering large test suites: test suite minimization, prioritization by additional coverage, cluster filtering with one-per-cluster sampling, and failure pursuit sampling. The first two techniques are based on selecting subsets that maximize code coverage as quickly as possible, while the latter two are based on analyzing the distribution of the tests' execution profiles. These techniques were compared with data sets obtained from three large subject programs: the GCC, Jikes, and javac compilers. The results indicate that distribution-based techniques can be as efficient or more efficient for revealing defects than coverage-based techniques, but that the two kinds of techniques are also complementary in the sense that they find different defects. Accordingly, some simple combinations of these techniques were evaluated for use in test case prioritization. The results indicate that these techniques can create more efficient prioritizations than those generated using prioritization by additional coverage.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251065,yes,,yes,,,,yes,yes,yes,yes
78,Putting your best tests forward,"Test case prioritization orders tests so that they help you meet your testing goals earlier during regression testing. Prioritization techniques can, for example, order tests to achieve coverage at the fastest rate possible, exercise features in order of expected frequency of use, or reveal faults as early as possible. We focus on the last goal, which we describe as ""increasing a test suite's rate of fault detection"" or the speed with which the test suite reveals faults. A faster fault detection rate during regression testing provides earlier feedback on a system under test, supporting earlier strategic decisions about release schedules and letting engineers begin debugging sooner. Also, if testing time is limited or unexpectedly reduced, prioritization increases the chance that testing resources will have been spent as cost effectively as possible in the available time.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231157,yes,,yes,,,,yes,yes,yes,yes
80,Test-suite reduction and prioritization for modified condition/decision coverage,"Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software according to some criterion as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. This paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of empirical studies of these algorithms.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183927,yes,,yes,,,,yes,yes,yes,yes
90,Empirical studies of test case prioritization in a JUnit testing environment,"Test case prioritization provides a way to run test cases with the highest priority earliest. Numerous empirical studies have shown that prioritization can improve a test suite's rate of fault detection, but the extent to which these results generalize is an open question because the studies have all focused on a single procedural language, C, and a few specific types of test suites, in particular, Java and the JUnit testing framework are being used extensively in practice, and the effectiveness of prioritization techniques on Java systems tested under JUnit has not been investigated. We have therefore designed and performed a controlled experiment examining whether test case prioritization can be effective on Java programs tested under JUnit, and comparing the results to those achieved in earlier studies. Our analyses show that test case prioritization can significantly improve the rate of fault detection of JUnit test suites, but also reveal differences with respect to previous studies that can be related to the language and testing paradigm.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383111,yes,,yes,,,,yes,yes,yes,yes
93,A controlled experiment assessing test case prioritization techniques via mutation faults,"Regression testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may prioritize their test cases so that those that are more important are run earlier in the regression testing process. Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited to hand-seeded faults, primarily due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults. We have therefore designed and performed a controlled experiment to assess the ability of prioritization techniques to improve the rate of fault detection techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. We also compare our results to those collected earlier with respect to the relationship between hand-seeded faults and mutation faults, and the implications this has for researchers performing empirical studies of prioritization.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510136,yes,,yes,,,,yes,yes,yes,yes
102,System test case prioritization of new and regression test cases,"Test case prioritization techniques have been shown to be beneficial for improving regression-testing activities. With prioritization, the rate of fault detection is improved, thus allowing testers to detect faults earlier in the system-testing phase. Most of the prioritization techniques to date have been code coverage-based. These techniques may treat all faults equally. We build upon prior test case prioritization research with two main goals: (1) to improve user-perceived software quality in a cost effective way by considering potential defect severity and (2) to improve the rate of detection of severe faults during system-level testing of new code and regression testing of existing code. We present a value-driven approach to system-level test case prioritization called the prioritization of requirements for test (PORT). PORT prioritizes system test cases based upon four factors: requirements volatility, customer priority, implementation complexity, and fault proneness of the requirements. We conducted a PORT case study on four projects developed by students in advanced graduate software testing class. Our results show that PORT prioritization at the system level improves the rate of detection of severe faults. Additionally, customer priority was shown to be one of the most important prioritization factors contributing to the improved rate of fault detection.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541815,yes,,yes,,,,yes,yes,yes,yes
105,Test prioritization using system models,"During regression testing, a modified system is retested using the existing test suite. Because the size of the test suite may be very large, testers are interested in detecting faults in the system as early as possible during the retesting process. Test prioritization tries to order test cases for execution so the chances of early detection of faults during retesting are increased. The existing prioritization methods are based on the code of the system. System modeling is a widely used technique to model state-based systems. In this paper, we present methods of test prioritization based on state-based models after changes to the model and the system. The model is executed for the test suite and information about model execution is used to prioritize tests. Execution of the model is inexpensive as compared to execution of the system; therefore the overhead associated with test prioritization is relatively small. In addition, we present an analytical framework for evaluation of test prioritization methods. This framework may reduce the cost of evaluation as compared to the existing evaluation framework that is based on experimentation (observation). We have performed an experimental study in which we compared different test prioritization methods. The results of the experimental study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510150,yes,,yes,,,no,yes,yes,yes,yes
116,Improving Testing Efficiency using Cumulative Test Analysis,"It can be impossible to thoroughly test complex software projects with a large library of tests to be run in many environments and configurations. The cumulative test analysis (CTA) technique described reduces the time to find defects by prioritising and minimising the testing. Tests are chosen to target the product areas having the highest risk of defects. Test effectiveness, test code coverage, product code changes and changes to dependencies are monitored and analysed to prioritise the testing. Test results from build to build are accumulated. Build reports clearly identify areas at risk, test results, and the tests that must be run. Experiences with a prototype tool are discussed and conclusions drawn from the use of CTA show that defects are found sooner, more time is available for writing new tests and the focus of test execution moves towards product quality instead of simply test results",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691682,no,,no,,,,no,yes,yes,yes
117,On the Use of Mutation Faults in Empirical Assessments of Test Case Prioritization Techniques,"Regression testing is an important activity in the software life cycle, but it can also be very expensive. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of test case prioritization techniques is to increase a test suite's rate of fault detection (how quickly, in a run of its test cases, that test suite can detect faults). Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited primarily to hand-seeded faults, largely due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults and that the use of hand-seeded faults can be problematic for the validity of empirical results focusing on fault detection. We have therefore designed and performed two controlled experiments assessing the ability of prioritization techniques to improve the rate of fault detection of test case prioritization techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. More importantly, a comparison of our results with those collected using hand-seeded faults reveals several implications for researchers performing empirical studies of test case prioritization techniques in particular and testing techniques in general",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1707670,yes,,yes,,,,yes,yes,yes,yes
123,Test Case Prioritization Using Relevant Slices,"Software testing and retesting occurs continuously during the software development lifecycle to detect errors as early as possible. The sizes of test suites grow as software evolves. Due to resource constraints, it is important to prioritize the execution of test cases so as to increase chances of early detection of faults. Prior techniques for test case prioritization are based on the total number of coverage requirements exercised by the test cases. In this paper, we present a new approach to prioritize test cases based on the coverage requirements present in the relevant slices of the outputs of test cases. We present experimental results comparing the effectiveness of our prioritization approach with that of existing techniques that only account for total requirement coverage, in terms of ability to achieve high rate of fault detection. Our results present interesting insights into the effectiveness of using relevant slices for test case prioritization",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020103,yes,,yes,,,,yes,yes,yes,yes
126,Using the Case-Based Ranking Methodology for Test Case Prioritization,"The test case execution order affects the time at which the objectives of testing are met. If the objective is fault detection, an inappropriate execution order might reveal most faults late, thus delaying the bug fixing activity and eventually the delivery of the software. Prioritizing the test cases so as to optimize the achievement of the testing goal has potentially a positive impact on the testing costs, especially when the test execution time is long. Test engineers often possess relevant knowledge about the relative priority of the test cases. However, this knowledge can be hardly expressed in the form of a global ranking or scoring. In this paper, we propose a test case prioritization technique that takes advantage of user knowledge through a machine learning algorithm, case-based ranking (CBR). CBR elicits just relative priority information from the user, in the form of pairwise test case comparisons. User input is integrated with multiple prioritization indexes, in an iterative process that successively refines the test case ordering. Preliminary results on a case study indicate that CBR overcomes previous approaches and, for moderate suite size, gets very close to the optimal solution",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021329,yes,,yes,,,,yes,yes,yes,yes
134,Applying Interface-Contract Mutation in Regression Testing of Component-Based Software,"Regression testing, which plays an important role in software maintenance, usually relies on test adequacy criteria to select and prioritize test cases. However, with the wide use and reuse of black-box components, such as reusable class libraries and COTS components, it is challenging to establish test adequacy criteria for testing software systems built on components whose source code is not available. Without source code or detailed documents, the misunderstanding between the system integrators and component providers has become a main factor of causing faults in component-based software. In this paper, we apply mutation on interface contracts, which can describe the rights and obligations between component users and providers, to simulate the faults that may occur in this way of software development. The mutation adequacy score for killing the mutants of interface contracts can serve as a test adequacy criterion. We performed an experimental study on three subject systems to evaluate the proposed approach together with four other existing criteria. The experimental results show that our adequacy criterion is helpful for both selecting good-quality test cases and scheduling test cases in an order of exposing faults quickly in regression testing of component-based software.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362630,no,,yes,,,,no,yes,yes,yes
135,Combinatorial Interaction Regression Testing: A Study of Test Case Generation and Prioritization,"Regression testing is an expensive part of the software maintenance process. Effective regression testing techniques select and order (or prioritize) test cases between successive releases of a program. However, selection and prioritization are dependent on the quality of the initial test suite. An effective and cost efficient test generation technique is combinatorial interaction testing, CIT, which systematically samples all t-way combinations of input parameters. Research on CIT, to date, has focused on single version software systems. There has been little work that empirically assesses the use of CIT test generation as the basis for selection or prioritization. In this paper we examine the effectiveness of CIT across multiple versions of two software subjects. Our results show that CIT performs well in finding seeded faults when compared with an exhaustive test set. We examine several CIT prioritization techniques and compare them with a re-generation/prioritization technique. We find that prioritized and re-generated/prioritized CIT test suites may find faults earlier than unordered CIT test suites, although the re-generated/prioritized test suites sometimes exhibit decreased fault detection.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362638,yes,,yes,,,no,yes,yes,yes,yes
145,Prioritization of Regression Tests using Singular Value Decomposition with Empirical Change Records,"During development and testing, changes made to a system to repair a detected fault can often inject a new fault into the code base. These injected faults may not be in the same files that were just changed, since the effects of a change in the code base can have ramifications in other parts of the system. We propose a methodology for determining the effect of a change and then prioritizing regression test cases by gathering software change records and analyzing them through singular value decomposition. This methodology generates clusters of files that historically tend to change together. Combining these clusters with test case information yields a matrix that can be multiplied by a vector representing a new system modification to create a prioritized list of test cases. We performed a post hoc case study using this technique with three minor releases of a software product at IBM. We found that our methodology suggested additional regression tests in 50% of test runs and that the highest-priority suggested test found an additional fault 60% of the time.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402199,yes,,yes,,,,yes,yes,yes,yes
148,Search Algorithms for Regression Test Case Prioritization,"Regression testing is an expensive, but important, process. Unfortunately, there may be insufficient resources to allow for the reexecution of all test cases during regression testing. In this situation, test case prioritization techniques aim to improve the effectiveness of regression testing by ordering the test cases so that the most beneficial are executed first. Previous work on regression test case prioritization has focused on greedy algorithms. However, it is known that these algorithms may produce suboptimal results because they may construct results that denote only local minima within the search space. By contrast, metaheuristic and evolutionary search algorithms aim to avoid such problems. This paper presents results from an empirical study of the application of several greedy, metaheuristic, and evolutionary search algorithms to six programs, ranging from 374 to 11,148 lines of code for three choices of fitness metric. The paper addresses the problems of choice of fitness metric, characterization of landscape modality, and determination of the most suitable search technique to apply. The empirical results replicate previous results concerning greedy algorithms. They shed light on the nature of the regression testing search space, indicating that it is multimodal. The results also show that genetic algorithms perform well, although greedy approaches are surprisingly effective, given the multimodal nature of the landscape",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123325,yes,,yes,,,yes,yes,yes,yes,yes
152,Test Case Prioritization Based on Varying Testing Requirement Priorities and Test Case Costs,"Test case prioritization is an effective and practical technique in regression testing. It schedules test cases in order of precedence that increases their ability to meet some performance goals, such as code coverage, rate of fault detection. In previous work, the test case prioritization techniques and metrics usually assumed that testing requirement priorities and test case costs are uniform. In this paper, basing on varying testing requirement priorities and test case costs, we present a new, general test case prioritization technique and an associated metric. The case study illustrates that the rate of ""units-oftesting-requirement-priority-satisfied- per-unit-test-case-cost"" can be increased, and then the testing quality and customer satisfaction can be improved.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385476,yes,,,yes,,,yes,yes,yes,yes
153,Test Case Prioritization for Black Box Testing,"Test case prioritization is an effective and practical technique that helps to increase the rate of regression fault detection when software evolves. Numerous techniques have been reported in the literature on prioritizing test cases for regression testing. However, existing prioritization techniques implicitly assume that source or binary code is available when regression testing is performed, and therefore cannot be implemented when there is no program source or binary code to be analyzed. In this paper, we presented a new technique for black box regression testing, and we performed an experiment to measure our technique. Our results show that the new technique is helpful to improve the effectiveness of fault detection when performing regression test in black box environment.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291039,yes,,,yes,,,yes,yes,yes,yes
167,An Empirical Study on Bayesian Network-based Approach for Test Case Prioritization,"A cost effective approach to regression testing is to prioritize test cases from a previous version of a software system for the current release. We have previously introduced a new approach for test case prioritization using Bayesian Networks (BN) which integrates different types of information to estimate the probability of each test case finding bugs. In this paper, we enhance our BN-based approach in two ways. First, we introduce a feedback mechanism and a new change information gathering strategy. Second, a comprehensive empirical study is performed to evaluate the performance of the approach and to identify the effects of using different parameters included in the technique. The study is performed on five open source Java objects. The obtained results show relative advantage of using feedback mechanism for some objects in terms of early fault detection. They also provide insight into costs and benefits of the various parameters used in the approach.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539555,no,,,yes,,,yes,yes,yes,yes
168,Application of system models in regression test suite prioritization,"During regression testing, a modified system needs to be retested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Most of the existing test prioritization methods are based on the code of the system, but model-based test prioritization has been recently proposed. System modeling is a widely used technique to model state-based systems. The existing model based test prioritization methods can only be used when models are modified during system maintenance. In this paper, we present model-based prioritization for a class of modifications for which models are not modified (only the source code is modified). After identification of elements of the model related to source-code modifications, information collected during execution of a model is used to prioritize tests for execution. In this paper, we discuss several model-based test prioritization heuristics. The major motivation to develop these heuristics was simplicity and effectiveness in early fault detection. We have conducted an experimental study in which we compared model-based test prioritization heuristics. The results of the study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658073,yes,,,yes,,yes,yes,yes,yes,yes
170,Applying Particle Swarm Optimization to Prioritizing Test Cases for Embedded Real Time Software Retesting,"In recent years, complex embedded systems are used in every device that is infiltrating our daily lives. Since most of the embedded systems are multi-tasking real time systems, the task interleaving issues, dead lines and other factors needs software units retesting to follow the subsequence changes. Regression testing is used for the software maintenance that revalidates the old functionality of the software unit. Testing is one of the most complex and time-consuming activities, in which running of all combination of test cases in test suite may require a large amount of efforts. Test case prioritization techniques can take advantage that orders test cases, which attempts to increase effectiveness in regression testing. This paper proposes to use particle swarm optimization (PSO) algorithm to prioritize the test cases automatically based on the modified software units. Regarding to the recent investigations, PSO is a multi-object optimization technique that can find out the best positions of the objects. The goal is to prioritize the test cases to the new best order, based on modified software components, so that test cases, which have new higher priority, can be selected in the regression testing process. The empirical results show that by using the PSO algorithm, the test cases can be prioritized in the test suites with their new best positions effectively and efficiently.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4568558,yes,,,yes,,,yes,yes,yes,yes
178,Historical Value-Based Approach for Cost-Cognizant Test Case Prioritization to Improve the Effectiveness of Regression Testing,"Regression testing has been used to support software testing activities and assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive because it requires many test case executions, and the number of test cases increases sharply as the software evolves. In this paper, we propose the Historical Value-Based Approach, which is based on the use of historical information, to estimate the current cost and fault severity for cost-cognizant test case prioritization. We also conducted a controlled experiment to validate the proposed approach, the results of which proved the proposed approachpsilas usefulness. As a result of the proposed approach, software testers who perform regression testing are able to prioritize their test cases so that their effectiveness can be improved in terms of average percentage of fault detected per cost.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579792,yes,,,yes,,,yes,yes,yes,yes
179,Incorporating varying requirement priorities and costs in test case prioritization for new and regression testing,"Test case prioritization schedules the test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects the most severe faults at the earliest in its testing life cycle. Test case prioritization techniques have proved to be beneficial for improving regression testing activities. While code coverage based prioritization techniques are found to be taken by most scholars, test case prioritization based on requirements in a cost effective manner has not been taken for study so far. Hence, in this paper, we propose to put forth a model for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software that can also be cost effective and to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the six factors: customer priority, changes in requirement, implementation complexity, usability, application flow and fault impact. The proposed prioritization technique is experimented in three phases with student projects and two sets of industrial projects and the results show convincingly that the proposed prioritization technique improves the rate of severe fault detection.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787662,yes,,,yes,,,yes,yes,yes,yes
183,Prioritizing User-Session-Based Test Cases for Web Applications Testing,"Web applications have rapidly become a critical part of business for many organizations. However, increased usage of Web applications has not been reciprocated with corresponding increases in reliability. Unique characteristics, such as quick turnaround time, coupled with growing popularity motivate the need for efficient and effective Web application testing strategies. In this paper, we propose several new test suite prioritization strategies for Web applications and examine whether these strategies can improve the rate of fault detection for three Web applications and their preexisting test suites. We prioritize test suites by test lengths, frequency of appearance of request sequences, and systematic coverage of parameter-values and their interactions. Experimental results show that the proposed prioritization criteria often improve the rate of fault detection of the test suites when compared to random ordering of test cases. In general, the best prioritization metrics either (1) consider frequency of appearance of sequences of requests or (2) systematically cover combinations of parameter-values as early as possible.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539541,no,,,yes,,,yes,yes,yes,yes
185,Quota-constrained test-case prioritization for regression testing of service-centric systems,"Test-case prioritization is a typical scenario of regression testing, which plays an important role in software maintenance. With the popularity of Web services, integrating Web services to build service-centric systems (SCSs) has attracted attention of many researchers and practitioners. During regression testing, as SCSs may use up constituent Web servicespsila request quotas (e.g., the upper limit of the number of requests that a user can send to a Web service during a certain time range), the quota constraint may delay fault exposure and the subsequent debugging. In this paper, we investigate quota-constrained test-case prioritization for SCSs, and propose quota-constrained strategies to maximize testing requirement coverage. We divide the testing time into time slots, and iteratively select and prioritize test cases for each time slot using integer linear programming (ILP). We performed an experimental study on our strategies together with three other strategies, and the results show that with the constraint of request quotas, our strategies can schedule test cases for execution in an order with higher effectiveness in exposing faults and achieving total and additional branch coverage.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658074,yes,,,yes,,,yes,yes,yes,yes
193,Test Case Prioritization Based on Analysis of Program Structure,"Test case prioritization techniques have been empirically proved to be effective in improving the rate of fault detection in regression testing. However, most of previous techniques assume that all the faults have equal severity, which dose not meet the practice. In addition, because most of the existing techniques rely on the information gained from previous execution of test cases or source code changes, few of them can be directly applied to non-regression testing. In this paper, aiming to improve the rate of severe faults detection for both regression testing and non-regression testing, we propose a novel test case prioritization approach based on the analysis of program structure. The key idea of our approach is the evaluation of testing-importance for each module (e.g., method) covered by test cases. As a proof of concept, we implement $Apros$, a test case prioritization tool, and perform an empirical study on two real, non-trivial Java programs. The experimental result represents that our approach could be a promising solution to improve the rate of severe faults detection.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724580,yes,,,yes,,,yes,yes,yes,yes
194,Test Case Prioritization for Multiple Processing Queues,"Test case prioritization is an effective technique that helps to increase the rate of fault detection or code coverage in regression testing. However, all existing methods can only prioritize test cases to a single queue. Once there are two or more machines that participate in testing, all exiting techniques are not applicable any more. To extend the prioritization methods to parallel scenario, this paper defines the prioritization problem in such scenario and applies the task scheduling method to prioritization algorithms to help partitioning a test suite into multiple prioritized subsets. Besides, this paper also discusses the limitation of previous metrics and proposes a new measure of effectiveness of prioritization methods in a parallel scenario. Finally, a case study is performed to illustrate the algorithms and metrics presented in this article.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732476,yes,,,yes,,,yes,yes,yes,yes
195,"Testing Optimization for Mission-Critical, Complex, Distributed Systems","The goal of the research was to optimize the regression testing of the software application to address the identified problem of a missing, unclear or even contradictory requirement. The approach was mainly aimed at regression test prioritization and selection of regression test cases per test campaigns. A combination of subjective data based on expert knowledge and objective historical data were the inputs to the model where the output was determining the quality of test selection. Proposed model is aimed at finding newly introduced defects, and it could be extremely useful when the system is in the state of cleaning up or ordering requirements.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591679,yes,,,yes,,,yes,yes,yes,yes
199,A Hybrid Approach to Build Prioritized Pairwise Interaction Test Suites,"Traditional interaction testing aims to build test suites that cover all t-way interactions of inputs. But in many test scenarios, the entire test suites cannot be fully run due to the limited budget. Therefore it is necessary to take the importance of interactions into account and prioritize these tests of the test suite. In the paper, we use the hybrid approach to build prioritized pairwise interaction test suites (PITS). It adopts a one-test-at-a-time strategy to construct final test suites. But to generate a single test it firstly generates a candidate test and then applies a specific metaheuristic search strategy to enhance this test. Here we experiment four different metaheuristic search strategies. In the experiments, we compare our approach to weighted density algorithm (WDA). Meanwhile, we also analyze the effectiveness of four different search strategies and the effectiveness of the increasing iterations. Empirical results demonstrate the effectiveness of our proposed approach.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365886,no,,,yes,,,yes,yes,yes,yes
202,Adaptive Random Test Case Prioritization,"Regression testing assures changed programs against unintended amendments. Rearranging the execution order of test cases is a key idea to improve their effectiveness. Paradoxically, many test case prioritization techniques resolve tie cases using the random selection approach, and yet random ordering of test cases has been considered as ineffective. Existing unit testing research unveils that adaptive random testing (ART) is a promising candidate that may replace random testing (RT). In this paper, we not only propose a new family of coverage-based ART techniques, but also show empirically that they are statistically superior to the RT-based technique in detecting faults. Furthermore, one of the ART prioritization techniques is consistently comparable to some of the best coverage-based prioritization techniques (namely, the ""additional"" techniques) and yet involves much less time cost.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431769,yes,,,yes,,no,yes,yes,yes,yes
208,Experimental Comparison of Code-Based and Model-Based Test Prioritization,"During regression testing, a modified system needs to beretested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Code-based test prioritization methods are based on the source code of the system, whereas model-based test prioritization methods are based on system models. System modeling is a widely used technique to model state-based systems. Models can be used not only during software development but also during testing. In this paper, we briefly overview codebased and model-based test prioritization. In addition, we present an experimental study in which the code based test prioritization and the model-based test prioritization are compared.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976373,yes,,,yes,,,yes,yes,yes,yes
215,Jtop: Managing JUnit Test Cases in Absence of Coverage Information,"Test case management may make the testing process more efficient and thus accelerate software delivery. With the popularity of using JUnit for testing Java software, researchers have paid attention to techniques to manage JUnit test cases in regression testing of Java software. Typically, most existing test case management tools are based on the coverage information. However, coverage information may need extra efforts to obtain. In this paper, we present an Eclipse IDE plug-in (named Jtop) for managing JUnit test cases in absence of coverage information. Jtop statically analyzes the program under test and its corresponding JUnit test cases to perform the following management tasks: regression test case selection, test suite reduction and test case prioritization. Furthermore, Jtop also enables the programmer to manually manipulate test cases through a graphical user interface.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431709,no,,,no,,,no,yes,yes,yes
227,Prioritizing JUnit test cases in absence of coverage information,"Better orderings of test cases can detect faults in less time with fewer resources, and thus make the debugging process earlier and accelerate software delivery. As a result, test case prioritization has become a hot topic in the research of regression testing. With the popularity of using the JUnit testing framework for developing Java software, researchers also paid attention to techniques for prioritizing JUnit test cases in regression testing of Java software. Typically, most of them are based on coverage information of test cases. However, coverage information may need extra costs to acquire. In this paper, we propose an approach (named Jupta) for prioritizing JUnit test cases in absence of coverage information. Jupta statically analyzes call graphs of JUnit test cases and the software under test to estimate the test ability (TA) of each test case. Furthermore, Jupta provides two prioritization techniques: the total TA based technique (denoted as JuptaT) and the additional TA based technique (denoted as JuptaA). To evaluate Jupta, we performed an experimental study on two open source Java programs, containing 11 versions in total. The experimental results indicate that Jupta is more effective and stable than the untreated orderings and Jupta is approximately as effective and stable as prioritization techniques using coverage information at the method level.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306350,yes,,,yes,,,yes,yes,yes,yes
228,Prioritizing test cases for resource constraint environments using historical test case performance data,"Regression testing has been widely used to assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive in that, it requires many test case executions and a large number of test cases. To provide the missing flexibility, researchers introduced prioritization techniques. The aim in this paper has been to prioritize test cases during software regression test. To achieve this, a new equation is presented. The proposed equation considers historical effectiveness of the test cases in fault detection, each test case's execution history in regression test and finally the last priority assigned to the test case. The results of applying the proposed equation to compute the priority of regression test cases for two benchmarks, known as Siemens suite and Space program, demonstrate the relatively faster fault detection in resource and time constrained environments.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234968,yes,,,yes,,,yes,yes,yes,yes
234,Tag-Based Techniques for Black-Box Test Case Prioritization for Service Testing,"A web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable. The rich interface specifications of a web service, however, provide peer services with a means to formulate black-box testing strategies. In this paper, we formulate new test case prioritization strategies using tags embedded in XML messages to reorder regression test cases, and reveal how the test cases use the interface specifications of services. We evaluate experimentally their effectiveness on revealing regression faults in modified WS-BPEL programs. The results show that the new techniques can have a high probability of outperforming random ordering.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381531,yes,,,yes,,,yes,yes,yes,yes
236,Test case prioritization based on data reuse an experimental study,"The order in which tests are executed can significantly impact the total test execution time. In this paper, we evaluate two test prioritization techniques (manual and automatic) in the context of mobile phone testing. The manual technique produces test sequences created by test experts, while the automatic one generates sequences mechanically based on the permutation of the tests. Both techniques take into account a data reuse: the more the data is reused among tests, the faster the sequence is executed. In order to evaluate the benefits of these two techniques, we carried out an experiment with 8 testers and 2 test suites arranged in a 2times2 Latin square design replicated four times. The automatic technique reduced approximately 25% of the data generation time and 13.5% of the execution time. The automatic technique is clearly better than the manual one with respect to the generation of sequences. Our experiment showed that the automatic technique also generates sequences whose execution is faster than those created manually by test experts.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5315980,yes,,,yes,,,yes,yes,yes,yes
239,The impact of test case reduction and prioritization on software testing effectiveness,"Software testing is critical but most expensive phase of Software Development Life Cycle (SDLC). Development organizations desire to thoroughly test the software. But this exhaustive testing is impractical due to resource constraints. A large number of test suites are generated using automated tools. But the real challenge is the selection of subset of test cases and/or high order test cases crucial to validate the System Under Test (SUT). Test case reduction and prioritization techniques help test manager to solve this problem at a little cost. In this paper, we investigate their impact on testing process effectiveness using previous empirical studies. The results indicate that these techniques improve the testing effectiveness significantly. At the end, a case study is presented that suggests different useful combinations of these techniques, which are helpful for different testing scenarios.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353136,yes,,,yes,,no,yes,yes,yes,yes
242,Using String Distances for Test Case Prioritisation,"Test case prioritisation aims at finding an ordering which enhances a certain property of an ordered test suite. Traditional techniques rely on the availability of code or a specification of the program under test. In this paper, we propose to use string distances on the text of test cases for their comparison and elaborate a prioritisation algorithm. Such a prioritisation does not require code and can be useful for initial testing and in cases when code is difficult to instrument. We also briefly report on preliminary results of an experiment where the proposed prioritisation technique was compared with random permutations and four classical string distance metrics were evaluated.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431745,yes,,,yes,,,yes,yes,yes,yes
245,A Simulation Study on Some Search Algorithms for Regression Test Case Prioritization,"Test case prioritization is an approach aiming at increasing the rate of faults detection during the testing phase, by reordering test case execution. Many techniques for regression test case prioritization have been proposed. In this paper, we perform a simulation experiment to study five search algorithms for test case prioritization and compare the performance of these algorithms. The target of the study is to have an in-depth investigation and improve the generality of the comparison results. The simulation study provides two useful guidelines: (1) Two search algorithms, Additional Greedy Algorithm and 2-Optimal Greedy Algorithm, outperform the other three search algorithms in most cases. (2) The performance of the five search algorithms will be affected by the overlap of test cases with regard to test requirements.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562946,yes,,,yes,,,yes,yes,yes,yes
255,Design and Analysis of Cost-Cognizant Test Case Prioritization Using Genetic Algorithm with Test History,"During software development, regression testing is usually used to assure the quality of modified software. The techniques of test case prioritization schedule the test cases for regression testing in an order that attempts to increase the effectiveness in accordance with some performance goal. The most general goal is the rate of fault detection. It assumes all test case costs and fault severities are uniform. However, those factors usually vary. In order to produce a more satisfactory order, the cost-cognizant metric that incorporates varying test case costs and fault severities is proposed. In this paper, we propose a cost-cognizant test case prioritization technique based on the use of historical records and a genetic algorithm. We run a controlled experiment to evaluate the proposed technique's effectiveness. Experimental results indicate that our proposed technique frequently yields a higher Average Percentage of Faults Detected per Cost (APFDc). The results also show that our proposed technique is also useful in terms of APFDc when all test case costs and fault severities are uniform.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676289,no,,,yes,,yes,yes,yes,yes,yes
264,Point-of-Interest Aware Test Case Prioritization: Methods and Experiments,"Location based services personalize their behaviors based on location data. When data kept by a service have evolved or the code has been modified, regression testing can be employed to assure the quality of services. Frequent data update however may lead to frequent regression testing and any faulty implementation of a service may affect many service consumers. Proper test case prioritization helps reveal service problems efficiently. In this paper, we review a set of point-of-interest (POI) aware test case prioritization techniques and report an experiment on such techniques. The empirical results show that these POI-aware techniques are more effective than random ordering and input-guided test case prioritization in terms of APFD. Furthermore, their effectiveness is observed to be quite stable over different sizes of the test suite.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563000,yes,,,yes,,,yes,yes,yes,yes
268,Prioritizing State-Based Aspect Tests,"In aspect-oriented programming, aspects are essentially incremental modifications to their base classes. Therefore aspect-oriented programs can be tested in an incremental fashion - we can first test the base classes and then test the base classes and aspects as a whole. This paper demonstrates that, in this incremental testing paradigm, we can prioritize aspect tests so as to report failure earlier. We explore test prioritization for testing aspect-oriented programs against their state models with transition coverage and round-trip coverage. Aspect tests are generated from woven state models obtained by composing aspect models into their base class models. We prioritize aspect tests by identifying the extent to which an aspect modifies its base classes. The modification is measured by the number of new and changed components in state transitions (start state, event, precondition, postcondition, end state). Transitions with more changes have higher priorities for test generation. We evaluate the impact of aspect test prioritization through mutation analysis of two AspectJ programs, where all aspects and their base classes can be modeled by finite state machines. We create aspect mutants of each AspectJ program according to a comprehensive AspectJ fault model. Then we test each mutant with the test suites generated without prioritization and with prioritization, respectively. Our experiment results show that prioritization of aspect tests has accelerated failure report.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477076,yes,,,yes,,,yes,yes,yes,yes
272,Regression test cases prioritization using Failure Pursuit Sampling,"The necessity of lowering the execution of system tests' cost is a consensual point in the software development community. The present study presents an optimization of the regression tests' activity, by adapting a test cases prioritization technique called Failure Pursuit Sampling-previously used and validated for the prioritization of tests in general-improving its efficiency for the exclusive execution of regression test. For this purpose, the clustering and sampling phases of the original technique were modified, so that it becomes capable of receive information from tests made on the previous version of a program, and can use this information to drive de efficiency of the new developed technique, for tests made on a present version. The adapted technique was implemented and executed using the Schedule program, of the Siemens suit. By using Average of the Percentage of Faults Detected charts, the modified Failure Pursuit Sampling technique presented a high level of efficiency improvement.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687069,yes,,,yes,,,yes,yes,yes,yes
274,Requirement based test case prioritization,"Test case prioritization involves scheduling test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects faults at the earliest in its testing life cycle. In this paper, an algorithm is proposed for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software and also to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the three factors: customer priority, changes in requirement, implementation complexity. The proposed prioritization technique is validated with two different sets of industrial projects and the results show that the proposed prioritization technique improves the rate of severe fault detection.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670728,yes,,,yes,,no,yes,yes,yes,yes
278,Sequence-based techniques for black-box test case prioritization for composite service testing,"Web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable to web services. In this paper, we formulate new test case prioritization strategies using sequences in XML messages to reorder regression test cases for composite web services, against the tag based techniques given in and reveal how the test cases use the interface specifications of the composite services. The results were evaluated experimentally and the results show that the new techniques can have a high probability of outperforming random ordering and the techniques given in.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705784,yes,,,yes,,,yes,yes,yes,yes
280,Supporting Concern-Based Regression Testing and Prioritization in a Model-Driven Environment,"Traditional regression testing and prioritization approaches are bottom-up (or white-box). They rely on the analysis of the impact of changes in source code artifacts, identifying corresponding parts of software to retest. While effective in minimizing the amount of testing required to validate code changes, they do not leverage on specification-level design and requirements concerns that motivated these changes. Model-based testing approaches support a top-down (or black box) testing approach, where design and requirements models are used in support of test generation. They augment code-based approaches with the ability to test from a higher-level design and requirements perspective. In this paper, we present a model-based regression testing and prioritization approach that efficiently selects test cases for regression testing based on different concerns. It relies on traceability links between models, test cases and code artifacts, together with user-defined properties associated to model elements. In particular we describe how to support concern-based regression testing and prioritization using TDE/UML, an extensible model-based testing environment.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615818,yes,,,yes,,,yes,yes,yes,yes
282,Test Case Prioritization for Web Service Regression Testing,"Regression testing is necessary to assure the quality of service-oriented business applications in their evolutions. However, because of the constraint of testing resource, entire test suite may not run as a result. Therefore, test case prioritization technique is required to increase the efficiency of Web service application regression testing. In this paper, we propose a dependence analysis based test case prioritization technique. First, we analyze the dependence relationship using control and data flow information in an orchestration language: WS-BPEL. Then we construct a weighted graph and do impact analysis to identify modification-affected elements. After that, we prioritize test cases according to covering more modification-affected elements with the highest weight. Finally we conduct a case study to illustrate the applicability of our method.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569910,yes,,,yes,,yes,yes,yes,yes,yes
284,The Effects of Time Constraints on Test Case Prioritization: A Series of Controlled Experiments,"Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. If this is correct, a better understanding of the effects of time constraints could lead to improved prioritization techniques and improved maintenance and testing processes. We therefore conducted a series of experiments to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our first experiment manipulates time constraint levels and shows that time constraints do play a significant role in determining both the cost-effectiveness of prioritization and the relative cost-benefit trade-offs among techniques. Our second experiment replicates the first experiment, controlling for several threats to validity including numbers of faults present, and shows that the results generalize to this wider context. Our third experiment manipulates the number of faults present in programs to examine the effects of faultiness levels on prioritization and shows that faultiness level affects the relative cost-effectiveness of prioritization techniques. Taken together, these results have several implications for test engineers wishing to cost-effectively regression test their software systems. These include suggestions about when and when not to prioritize, what techniques to employ, and how differences in testing processes may relate to prioritization cost--effectiveness.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482587,yes,,,yes,,no,yes,yes,yes,yes
287,Using Coverage Information to Guide Test Case Selection in Adaptive Random Testing,"Random Testing (RT) is a fundamental software testing technique. Adaptive Random Testing (ART) improves the fault-detection capability of RT by employing the location information of previously executed test cases. Compared with RT, test cases generated in ART are more evenly spread across the input domain. ART has conventionally been applied to programs that have only numerical input types, because the distance between numerical inputs is readily measurable. The vast majority of computer programs, however, involve non-numerical inputs. To apply ART to these programs requires the development of effective new distance measures. Different from those measures that focus on the concrete values of program inputs, in this paper we propose a method to measure the distance using coverage information. The proposed method enables ART to be applied to all kinds of programs regardless of their input types. Empirical studies are further conducted for the branch coverage Manhattan distance measure using the replace and space programs. Experimental results show that, compared with RT, the proposed method significantly reduces the number of test cases required to detect the first failure. This method can be directly applied to prioritize regression test cases, and can also be incorporated into code-based and model-based test case generation tools.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615787,yes,,,no,,,no,yes,yes,yes
289,Web services regression test case prioritization,"Web services and their underlying system grow over time and need to be retested whenever there is a change. This is essential for ensuring uncompromised quality. If we have modified only a small part of the system, it should be possible to reuse the existing test suite. Anyhow, for large modifications or for large systems, retesting the entire test suite will consume large amounts of time and computing resources. In this paper we propose a new method to prioritize test cases in web applications. Our test prioritization technique orders test cases in such a way that the most beneficial is executed first. Most of the existing test prioritization methods are based on the code of the system, but we propose a model-based test prioritization using activity diagram. Our technique identifies difference between original model and modified model. Using this information we plot activity paths for each test case and identify the most promising paths. The test case which covers these paths is considered as the most beneficial test cases. Our approach is effective in revealing the most promising regression test cases. We have applied our method on an online air ticket reservation system in which we could identify the most beneficial test cases from the existing ones.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5643499,yes,,,yes,,,yes,yes,yes,yes
290,A clustering approach to improving test case prioritization: An industrial case study,"Regression testing is an important activity for controlling the quality of a software product, but it accounts for a large proportion of the costs of software. We believe that an understanding of the underlying relationships in data about software systems, including data correlations and patterns, could provide information that would help improve regression testing techniques. We conjecture that if test cases have common properties, then test cases within the same group may have similar fault detection ability. As an initial approach to investigating the relationships in massive data in software repositories, in this paper, we consider a clustering approach to help improve test case prioritization. We implemented new prioritization techniques that incorporate a clustering approach and utilize code coverage, code complexity, and history data on real faults. To assess our approach, we have designed and conducted empirical studies using an industrial software product, Microsoft Dynamics Ax, which contains real faults. Our results show that test case prioritization that utilizes a clustering approach can improve the effectiveness of test case prioritization techniques.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080805,yes,,,yes,,,yes,yes,yes,yes
296,A model based prioritization technique for component based software retesting using uml state chart diagram,"Regression testing is the process of testing a modified system using the old test suite. As the test suite size is large, system retesting consumes large amount of time and computing resources. This issue of retesting of software systems can be handled using a good test case prioritization technique. A prioritization technique schedules the test cases for execution so that the test cases with higher priority executed before lower priority. The objective of test case prioritization is to detect fault as early as possible so that the debuggers can begin their work earlier. In this paper we propose a new prioritization technique to prioritize the test cases to perform regression testing for Component Based Software System (CBSS). The components and the state changes for a component based software systems are being represented by UML state chart diagrams which are then converted into Component Interaction Graph (CIG) to describe the interrelation among components. Our prioritization algorithm takes this CIG as input along with the old test cases and generates a prioritized test suit taking into account total number of state changes and total number of database access, both direct and indirect, encountered due to each test case. Our algorithm is found to be very effective in maximizing the objective function and minimizing the cost of system retesting when applied to few JAVA projects.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941719,yes,,,yes,,,yes,yes,yes,yes
299,A Test Case Design Algorithm Based on Priority Techniques,"Testing is an important step of building e-commerce system. In regression testing, it is the key issue that how to reuse the test suite efficiently. This paper presents a dynamic adjustment prioritization based on the design information of test suite which is a new exploration of regression test prioritization. It improves the shortcoming of the existing technologies which failed to use the design information of test cases effectively. It adjusts the priority of test case by collecting running information, gradually optimizes the test suite and makes the test suite adapted to the current test environment to obtain a better error detection results. Experiments show the error-detected efficiency of it has certain advantages than the existing algorithms.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092632,yes,,,no,,,yes,yes,yes,yes
300,A tool for combination-based prioritization and reduction of user-session-based test suites,"Test suite prioritization and reduction are two approaches to managing large test suites. They play an important role in regression testing, where a large number of tests accumulate over time from previous versions of the system. Accumulation of tests is exacerbated in user-session-based testing of web applications, where field usage data is continually logged and converted into test cases. This paper presents a tool that allows testers to easily collect, prioritize, and reduce user-session-based test cases. Our tool provides four contributions: (1) guidance to users on how to configure their web server to log important usage information, (2) automated parsing of web logs into XML formatted test cases that can be used by test replay tools, (3) automated prioritization of test cases by length-based and combinatorial-based criteria, and (4) automated reduction of test cases by combinatorial coverage.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080833,yes,,,yes,,yes,yes,yes,yes,yes
302,Adaptive Regression Testing Strategy: An Empirical Study,"When software systems evolve, different amounts and types of code modifications can be involved in different versions. These factors can affect the costs and benefits of regression testing techniques in different ways, and thus, there may be no single regression testing technique that is the most cost-effective technique to use on every version. To date, many regression testing techniques have been proposed, but no research has been done on the problem of helping practitioners systematically choose appropriate techniques on new versions as systems evolve. To address this problem, we propose adaptive regression testing (ART) strategies that attempt to identify the regression testing techniques that will be the most cost-effective for each regression testing session considering organization's situations and testing environment. To assess our approach, we conducted an experiment focusing on test case prioritization techniques. Our results show that prioritization techniques selected by our approach can be more cost-effective than those used by the control approaches.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132961,,yes,,no,,,yes,yes,yes,yes
303,Agent-Based Test Case Prioritization,In this paper an Adaptive Test Management System (ATMS) based on software agents is presented which prioritizes test cases considering available information from test teams and from developments teams about the software system and the test cases. The goal of the ATMS is to increase the number of found faults in the available test time with the determined prioritization order.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954404,,yes,,yes,,yes,yes,yes,yes,yes
306,An Improved Metric for Test Case Prioritization,"Test case prioritization is an effective and practical technique of regression testing. To illustrate its effectiveness, many test metrics were proposed. In this paper, the physical meanings of these metrics were explained and their limitations were pointed out. Then, an improved metric and its extension for test case prioritization were proposed. The case study indicates that, compared with existing metrics, our new metric can provide much more precise illustration of the effectiveness of test case prioritization techniques.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093578,,yes,,yes,,no,yes,yes,yes,yes
307,Black box test case prioritization techniques for semantic based composite web services using OWL-S,"Web services are the basic building blocks for the business which is different from web applications. Testing of web services is difficult and increases the cost due to the unavailability of source code. Researchers have, web services are tested based on the syntactic structure using Web Service Description Language (WSDL) for atomic web services. This paper proposes an automated testing framework for composite web services based on semantics where the domain knowledge of the web services is described using prot??g?? tool and the behaviour of the entire business operation flow for the composite web service is described by Ontology Web Language for services (OWL-S). Prioritization of test cases is performed based on various coverage criteria for composite web services. Series of experiments were conducted to assess the effectiveness of prioritization and empirical results shown that prioritization techniques perform well in detecting faults compared to traditional techniques.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972354,,yes,,yes,,,yes,yes,yes,yes
324,Dynamic Prioritization in Regression Testing,"Although used extensively in industry, regression testing is challenging from both a process management as well as a resource management perspective. In literature, proposed test case prioritization techniques assume a constant pool of test cases with non-changing coverage during the regression testing process, and therefore they work with a fixed, prioritized test suite. However, in practice, test cases and their coverage metrics may change during regression testing due to modifications of software artefacts (e.g. due to bug fixing). For example, modifying obsolete test cases or source code may change the coverage metrics during the process. This may lead to some changes in test case priorities. Dealing with manual tests cases, scheduling test case execution in shared environments and other constraints in practice may cause the same effect. In this paper, we highlight these challenges in industrial regression testing and propose a paradigm called Dynamic Prioritization, which uses in-process events and the most up-to-date test suite to re-order test cases.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954402,,yes,,yes,,,yes,yes,yes,yes
330,Hybrid regression testing technique: A multi layered approach,Software needs to be delivered well in time and within budgets. One way of doing this is performing incremental delivery of the software with each increment being adding new features along with changes requests. This incremental delivery is supported with requirement prioritization and needs to be tested for checking the reliability and quality. Testing of this increment calls for testing of not only of old newly added functionality but also of existing features so as to make sure that old parts that works perfectly well do not malfunctions after new code is added. Thus a new hybrid technique is proposed in this paper that clusters the test cases and prioritizes the clusters on basis of priorities of requirements represented by the clusters and series of selections and prioritizations at levels of test cases reduces the number of test cases to manageable level and execution of these test cases guarantees the testing of highest priority requirements associated with statements that are often associated with failures or has highest number of parents or sibling's statements dependent on it and is likely to be influenced by changes or failures in this statement.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139363,,yes,,no,,,yes,yes,yes,yes
333,Improving Regression Testing Transparency and Efficiency with History-Based Prioritization -- An Industrial Case Study,"Background: History based regression testing was proposed as a basis for automating regression test selection, for the purpose of improving transparency and test efficiency, at the function test level in a large scale software development organization. Aim: The study aims at investigating the current manual regression testing process as well as adopting, implementing and evaluating the effect of the proposed method. Method: A case study was launched including: identification of important factors for prioritization and selection of test cases, implementation of the method, and a quantitative and qualitative evaluation. Results: 10 different factors, of which two are history-based, are identified as important for selection. Most of the information needed is available in the test management and error reporting systems while some is embedded in the process. Transparency is increased through a semi-automated method. Our quantitative evaluation indicates a possibility to improve efficiency, while the qualitative evaluation supports the general principles of history-based testing but suggests changes in implementation details.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770626,,yes,,no,,,yes,yes,yes,yes
336,Industrial experiences with automated regression testing of a legacy database application,"This paper presents a practical approach and tool (DART) for functional black-box regression testing of complex legacy database applications. Such applications are important to many organizations, but are often difficult to change and consequently prone to regression faults during maintenance. They also tend to be built without particular considerations for testability and can be hard to control and observe. We have therefore devised a practical solution for functional regression testing that captures the changes in database state (due to data manipulations) during the execution of a system under test. The differences in changed database states between consecutive executions of the system under test, on different system versions, can help identify potential regression faults. In order to make the regression test approach scalable for large, complex database applications, classification tree models are used to prioritize test cases. The test case prioritization can be applied to reduce test execution costs and analysis effort. We report on how DART was applied and evaluated on business critical batch jobs in a legacy database application in an industrial setting, namely the Norwegian Tax Accounting System (SOFIE) at the Norwegian Tax Department (NTD). DART has shown promising fault detection capabilities and cost-effectiveness and has contributed to identify many critical regression faults for the past eight releases of SOFIE.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080803,,no,,no,,no,no,yes,yes,yes
338,Making the Case for MORTO: Multi Objective Regression Test Optimization,This paper argues that regression test optimization problems such as selection and prioritization require multi objective optimization in order to adequately cater for real world regression testing scenarios. The paper presents several examples of costs and values that could be incorporated into such a Multi Objective Regression Test Optimization (MORTO) approach.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954399,,yes,,no,,,no,yes,yes,yes
343,Prioritizing interaction test suite for t-way testing,"In recent years, many new t-way interaction based strategies (where t indicates the interaction strength), particularly based on covering arrays, have been developed in the literature. In search of an optimal strategy that generates the most minimum number of tests, many of existing t-way strategies have not sufficiently dealt with test prioritization (i.e. in terms of maximizing new interaction coverage per test). Addressing this issue, this paper highlights a useful prioritization algorithm to reorganize the test cases in order to improve the rate of interaction coverage. This algorithm takes a pre-generated t-way test suite as input and automatically generates a priority ordered test suite as output. In order to demonstrate its applicability, this paper demonstrates the use of the algorithm to help prioritize the test suite generated by existing t-way strategy, MC-MIPOG.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140686,,yes,,yes,,,yes,yes,yes,yes
344,Prioritizing Requirements-Based Regression Test Cases: A Goal-Driven Practice,"Any changes for maintenance or evolution purposes may break existing working features, or may violate the requirements established in the previous software releases. Regression testing is essential to avoid these problems, but it may be ended up with executing many time-consuming test cases. This paper tries to address prioritizing requirements-based regression test cases. To this end, system-level testing is focused on two practical issues in industrial environments: i) addressing multiple goals regarding quality, cost and effort in a project, and ii) using non-code metrics due to the lack of detailed code metrics in some situations. This paper reports a goal-driven practice at Research In Motion (RIM) towards prioritizing requirements-based test cases regarding these issues. Goal-Question-Metric (GQM) is adopted in identifying metrics for prioritization. Two sample goals are discussed to demonstrate the approach: detecting bugs earlier and maintaining testing effort. We use two releases of a prototype Web-based email client to conduct a set of experiments based on the two mentioned goals. Finally, we discuss lessons learned from applying the goal-driven approach and experiments, and we propose few directions for future research.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741354,,yes,,yes,,,yes,yes,yes,yes
347,Regression testing in Software as a Service: An industrial case study,"Many organizations are moving towards a business model of Software as a Service (SaaS), where customers select and pay for services dynamically via the web. In SaaS, service providers face the challenge of delivering and maintaining high quality software solutions which must continue to work under an enormous number of scenarios; customers can easily subscribe and unsubscribe from services at any point. To date, there has been little research on unique approaches for regression test methodologies for testing in a SaaS environment. In this paper, we present an industrial case study of a regression testing approach to improve test effectiveness and efficiency in SaaS. We model service level use cases from field failures as abstract events and then generate sequences of these for testing to provide a broad coverage of the possible use cases. In subsequent releases of the system we prioritize the tests to improve time to detection of faults in the modified system. We have applied our technique to two releases of a large industrial enterprise level SaaS application and demonstrate that using our approach (1) we could have uncovered escaped faults prior to the system release in both versions of the system; (2) using a priority order we could have improved the efficiency of testing in the first version; and (3) prioritization based on failure history from the first version increases the fault detection rate in the new version, suggesting a correlation between the important sequences in versions that can be leveraged for regression testing.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080804,,no,,no,,,no,yes,yes,yes
353,Test Case Generation and Prioritization from UML Models,"This paper proposes a novel approach to generating test cases from UML 2.0 activity diagrams and prioritizing those test cases using model information encapsulated in the activity diagrams. The test cases generated according to our approach are suitable for system level testing of the application. For prioritization of test cases, we propose a method based on coverage of all transitions in the activity diagram and usage probability of a particular flow in the activity model. We also propose an approach for selecting test data based on analysis of the branch conditions of the decision nodes in the activity diagrams.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734956,,yes,,no,,no,yes,yes,yes,yes
354,Test case prioritization for regression testing based on fault dependency,"Test case prioritization techniques involve scheduling test cases for regression testing in an order that increases their effectiveness at meeting some performance goal. This is inefficient to re execute all the test cases in regression testing following the software modifications. Using information obtained from previous test case execution, prioritization techniques order the test cases for regression testing so that most beneficial are executed first thus allows an improved effectiveness of testing. One performance goal, rate of dependency detected among faults, measures how quickly dependency among faults are detected within the regression testing process. An improved rate of fault dependency can provide faster feedback on software and let developers start debugging on the severe faults that cause other faults to appear later. This paper presents the new metric for assessing rate of fault dependency detection and an algorithm to prioritize test cases. Using the new metric the effectiveness of this prioritization is shown comparing it with non-prioritized test case. Analysis proves that prioritized test cases are more effective in detecting dependency among faults.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941954,,yes,,yes,,yes,yes,yes,yes,yes
355,Test Case Prioritization Technique Based on Genetic Algorithm,"With the rapid development of information technology, software testing, as a software quality assurance, is becoming more and more important. In the software life cycle, each time the code has changed need to be regression testing. The huge test case library makes running a full test case library being challenged. To this end, we designed a genetic algorithm-based test case prioritization algorithm and improved the genetic algorithm proposed software test case prioritization algorithm.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063222,,yes,,yes,,,yes,yes,yes,yes
357,Towards Impact Analysis of Test Goal Prioritization on the Efficient Execution of Automatically Generated Test Suites Based on State Machines,"Test prioritization aims at reducing test execution costs. There are several approaches to prioritize test cases based on collected data of previous test runs, e.g., in regression testing. In this paper, we present a new approach to test prioritization for efficient test execution that is focused on the artifacts used in model-based test generation from state machines. We propose heuristics for test goal prioritizations and evaluate them using two different test models. Our finding is that the prioritizations can have a positive impacton the test execution efficiency. This impact, however, is hard to predict for a concrete situation. Thus, the question for the general gain of test goal prioritizations is still open.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004322,,yes,,yes,,,yes,yes,yes,yes
361,A Framework to Support Research in and Encourage Industrial Adoption of Regression Testing Techniques,"When software developers make changes to a program, it is possible that they will introduce faults into previously working parts of the code. As software grows, a regression test suite is run to ensure that the old functionality still works as expected. Yet, as the number of test cases increases, it becomes more expensive to execute the test suite. Reduction and prioritization techniques enable developers to manage large and unwieldy test suites. However, practitioners and researchers do not always use and study these methods due, in part, to a lack of availability. In response to this issue, this paper describes an already released open-source framework that supports both research and practice in regression testing. The sharing of this framework will enable the replication of empirical studies in regression testing and encourage faster industrial adoption of these useful, yet rarely used, techniques.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200107,,no,,no,,,no,yes,yes,yes
362,A Heuristic Model-Based Test Prioritization Method for Regression Testing,"Due to the resource and time constraints for re-executing large test suites in regression testing, developers are interested in detecting faults in the system as early as possible. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. In this paper, we present a model-based heuristic method to prioritize test cases for regression testing, which takes into account two types of information collected during execution of the modified model on the test suite. The experiment shows that our algorithm has better effectiveness of early fault detection.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228450,,yes,,yes,,,yes,yes,yes,yes
363,A Multi-Objective Technique to Prioritize Test Cases Based on Latent Semantic Indexing,"To early discover faults in source code, test case ordering has to be properly chosen. To this aim test prioritization techniques can be used. Several of these techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test prioritization technique that determines sequences of test cases that maximize the number of discovered faults that are both technical and business critical. The technique uses the information related to the code and requirements coverage, as well as the execution cost of each test case. The approach also uses recovered trace ability links among source code and system requirements via the Latent Semantic Indexing technique. We evaluated our proposal against both a random prioritization technique and two single-objective prioritization techniques on two Java applications. The results indicate that our proposal outperforms the baseline techniques and that additional improvements are still possible.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178873,,yes,,yes,,,yes,yes,yes,yes
366,A Static Approach to Prioritizing JUnit Test Cases,"Test case prioritization is used in regression testing to schedule the execution order of test cases so as to expose faults earlier in testing. Over the past few years, many test case prioritization techniques have been proposed in the literature. Most of these techniques require data on dynamic execution in the form of code coverage information for test cases. However, the collection of dynamic code coverage information on test cases has several associated drawbacks including cost increases and reduction in prioritization precision. In this paper, we propose an approach to prioritizing test cases in the absence of coverage information that operates on Java programs tested under the JUnit framework-an increasingly popular class of systems. Our approach, JUnit test case Prioritization Techniques operating in the Absence of coverage information (JUPTA), analyzes the static call graphs of JUnit test cases and the program under test to estimate the ability of each test case to achieve code coverage, and then schedules the order of these test cases based on those estimates. To evaluate the effectiveness of JUPTA, we conducted an empirical study on 19 versions of four Java programs ranging from 2K-80K lines of code, and compared several variants of JUPTA with three control techniques, and several other existing dynamic coverage-based test case prioritization techniques, assessing the abilities of the techniques to increase the rate of fault detection of test suites. Our results show that the test suites constructed by JUPTA are more effective than those in random and untreated test orders in terms of fault-detection effectiveness. Although the test suites constructed by dynamic coverage-based techniques retain fault-detection effectiveness advantages, the fault-detection effectiveness of the test suites constructed by JUPTA is close to that of the test suites constructed by those techniques, and the fault-detection effectiveness of the test suites constructed by some of JUPTA's variants is better than that of the test suites constructed by several of those techniques.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363461,,yes,,yes,,,yes,yes,yes,yes
367,A Two-Level Prioritization Approach for Regression Testing of Web Applications,A test case prioritization technique reschedules test cases for regression testing in an order to achieve specific goals like early fault detection. We propose a new two level prioritization approach to prioritize test cases for web applications as a whole. Our approach automatically selects modified functionalities in a web application and executes test cases on the basis of the impact of modified functionalities. We suggest several new prioritization strategies for web applications and examine whether these prioritization strategies improve the rate of fault detection for web applications. We propose a new automated test suite prioritization model for web applications that selects test cases related to modified functionalities and reschedules them using our new prioritization strategies to detect faults early in test suite execution.,2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462796,,yes,,yes,,,yes,yes,yes,yes
371,Code coverage-based regression test selection and prioritization in WebKit,"Automated regression testing is often crucial in order to maintain the quality of a continuously evolving software system. However, in many cases regression test suites tend to grow too large to be suitable for full re-execution at each change of the software. In this case selective retesting can be applied to reduce the testing cost while maintaining similar defect detection capability. One of the basic test selection methods is the one based on code coverage information, where only those tests are included that cover some parts of the changes. We experimentally applied this method to the open source web browser engine project WebKit to find out the technical difficulties and the expected benefits if this method is to be introduced into the actual build process. Although the principle is simple, we had to solve a number of technical issues, so we report how this method was adapted to be used in the official build environment. Second, we present results about the selection capabilities for a selected set of revisions of WebKit, which are promising. We also applied different test case prioritization strategies to further reduce the number of tests to execute. We explain these strategies and compare their usefulness in terms of defect detection and test suite reduction.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405252,,yes,,yes,,no,yes,yes,yes,yes
372,Dependency-Based Test Case Selection and Prioritization in Embedded Systems,"Embedded systems in automotive engineering are getting more and more complex due to a higher rate of integration and shared usage of sensor signals. A common solution to testing these systems is deriving test cases from models, so called model-based testing. In practice, generated test suites are typically very huge and have to be reduced by methods of regression-test selection and prioritization. In our field of application, we additionally suffer from the lack of knowledge on system internals like the source code. Therefore, our approach is based on dependences between the components of embedded systems. The model we use is derived from the system architecture and system requirements. We are using graph algorithms for selecting and prioritizing test cases to run on a certain system version. First statistical evaluations and the current implementation already have shown promising reduction of test-cases for regression testing.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200176,,no,,yes,,,yes,yes,yes,yes
376,G-RankTest: Regression testing of controller applications,"Since controller applications must typically satisfy real-time constraints while manipulating real-world variables, their implementation often results in programs that run extremely fast and manipulate numerical inputs and outputs. These characteristics make them particularly suitable for test case generation. In fact a number of test cases can be easily created, due to the simplicity of numerical inputs, and executed, due to the speed of computations. In this paper we present G-RankTest, a technique for test case generation and prioritization. The key idea is that test case generation can run for long sessions (e.g., days) to accurately sample the behavior of a controller application and then the generated test cases can be prioritized according to different strategies, and used for regression testing every time the application is modified. In this work we investigate the feasibility of using the gradient of the output as a criterion for selecting the test cases that activate the most tricky behaviors, which we expect easier to break when a change occurs, and thus deserve priority in regression testing.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228981,,no,,yes,,,no,yes,yes,yes
384,Modular based multiple test case prioritization,"Cost and time effective reliable test case prioritization technique is the need for present software industries. The test case prioritization for the entire program consumes more time and the selection of test case for entire software is also affecting the test performance. In order to alleviate the above problem a new methodology using modular based test case prioritization is proposed for regression testing. In this method the program is divided into multiple modules. The test cases corresponding to each module is prioritized first. In the second stage, the individual modular based prioritized test suites are combined together and further prioritized for the whole program. This method is verified for fault coverage and compared with overall program test case prioritization method. The proposed method is assessed using three standard applications namely University Students Monitoring System, Hospital Management System, and Industrial Process Operation System. The empirical studies show that the proposed algorithm is significantly performed well. The superiority of the proposed method is also highlighted.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510205,,yes,,yes,,,yes,yes,yes,yes
385,MOTCP: A tool for the prioritization of test cases based on a sorting genetic algorithm and Latent Semantic Indexing,"Test prioritization techniques can be used to determine test case ordering and early discover faults in source code. Several of these techniques exploit a single objective function, e.g., code or requirements coverage. In this tool demo paper, we present MOTCP, a software tool that implements a multi-objective test prioritization technique based on the information related to the code and requirements coverage, as well as the execution cost of each test case. To establish users' and system requirements coverage, the MOTCP uses Latent Semantic Indexing to recover traceability links among application source code and requirements specifications. The test case ordering is then obtained by applying a non-dominated sorting genetic algorithm.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405346,,yes,,yes,,,yes,yes,yes,yes
387,On the Fault-Detection Capabilities of Adaptive Random Test Case Prioritization: Case Studies with Large Test Suites,"An adaptive random (AR) testing strategy has recently been developed and examined by a growing body of research. More recently, this strategy has been applied to prioritizing regression test cases based on code coverage using the concepts of Jaccard Distance (JD) and Coverage Manhattan Distance (CMD). Code coverage, however, does not consider frequency, furthermore, comparison between JD and CMD has not yet been made. This research fills the gap by first investigating the fault-detection capabilities of using frequency information for AR test case prioritization, and then comparing JD and CMD. Experimental results show that ""coverage"" was more useful than ""frequency"" although the latter can sometimes complement the former, and that CMD was superior to JD. It is also found that, for certain faults, the conventional ""additional"" algorithm (widely accepted as one of the best algorithms for test case prioritization) could perform much worse than random testing on large test suites.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149572,,yes,,yes,,,yes,yes,yes,yes
388,Oracle-Centric Test Case Prioritization,"Recent work in testing has demonstrated the benefits of considering test oracles in the testing process. Unfortunately, this work has focused primarily on developing techniques for generating test oracles, in particular techniques based on mutation testing. While effective for test case generation, existing research has not considered the impact of test oracles in the context of regression testing tasks. Of interest here is the problem of test case prioritization, in which a set of test cases are ordered to attempt to detect faults earlier and to improve the effectiveness of testing when the entire set cannot be executed. In this work, we propose a technique for prioritizing test cases that explicitly takes into account the impact of test oracles on the effectiveness of testing. Our technique operates by first capturing the flow of information from variable assignments to test oracles for each test case, and then prioritizing to ``cover'' variables using the shortest paths possible to a test oracle. As a result, we favor test orderings in which many variables impact the test oracle's result early in test execution. Our results demonstrate improvements in rate of fault detection relative to both random and structural coverage based prioritization techniques when applied to faulty versions of three synchronous reactive systems.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405379,,yes,,yes,,,yes,yes,yes,yes
391,Prioritization of Test Cases Using Software Agents and Fuzzy Logic,"Limited test time and restricted number of test resources confront test managers with big challenges, especially in the system test. Consequently, the test manager has to prioritize test cases before each test cycle. There is much information available for determining a reasonable prioritization order in software projects. However, due to the complexity of current software systems and the high number of existing test cases, the abundance of information relevant for prioritization is not manageable for the test manager, even with high effort. In this paper we present a concept for an automated prioritization of test cases using software agents and fuzzy logic. Our prioritization system determines the prioritization order which increases the test effectiveness and the fault detection rate.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200143,,yes,,yes,,no,yes,yes,yes,yes
398,Software testing suite prioritization using multi-criteria fitness function,"Regression testing is the process of validating modifications introduced in a system during software maintenance. It is an expensive, yet an important process. As the test suite size is very large, system retesting consumes large amount of time and computing resources. Unfortunately, there may be insufficient resources to allow for the re-execution of all test cases during regression testing. Testcase prioritization techniques aim to improve the effectiveness of regression testing, by ordering the testcases so that the most beneficial are executed first with higher priority. The objective of test case prioritization is to detect faults as early as possible. An approach for automating the test case prioritization process using genetic algorithm with Multi-Criteria Fitness function is presented. It uses multiple control flow coverage metrics. These metrics measure the degree of coverage of conditions, multiple conditions and statements that the test case covers. Theses metrics are weighted by the number of faults revealed and their severity. The proposed Multi-criteria technique showed superior results compared to similar work.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523563,,no,,yes,,no,yes,yes,yes,yes
399,Test Case Prioritization Due to Database Changes in Web Applications,"A regression test case prioritization (TCP) technique reorders test cases for regression testing to achieve early fault detection. Most TCP techniques have been developed for regression testing of source code in an application. Most web applications rely on a database server for serving client requests. Any changes in the database result in erroneous client interactions and may bring down the entire web application. However, most prioritization techniques are unsuitable for prioritizing test suites for early detection of changes in databases. There are very few proposals in the literature for prioritization of test cases that can detect faults in the database early. We propose a new automated TCP technique for web applications that automatically identifies the database changes, prioritizes test cases related to database changes and executes them in priority order to detect faults early.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200175,,yes,,yes,,yes,yes,yes,yes,yes
400,Test Case Prioritization for Regression Testing Based on Function Call Path,"Test case prioritization is an effective and practical technique of regression testing. It is helpful to increase the efficiency of regression testing by sorting and executing test cases according to their importance. Static paths on function call obtained by analyzing the source code, combined with the dynamic path after executing test cases, the correspondence is built between test cases and the static paths, identifying the changes which software developers modify program to correct defects, giving different priority to test case based on path coverage , test cases are selected in accordance with their priorities in regression testing. Firstly, the background and related concept of test case prioritization are introduced. And then, the relevant research work is outlined, a set of new prioritization algorithms are proposed; implementation and analysis of the algorithm are given finally.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6301421,,yes,,yes,,,yes,yes,yes,yes
401,Test case prioritization incorporating ordered sequence of program elements,"Test suites often grow very large over many releases, such that it is impractical to re-execute all test cases within limited resources. Test case prioritization, which rearranges test cases, is a key technique to improve regression testing. Code coverage information has been widely used in test case prioritization. However, other important information, such as the ordered sequence of program elements measured by execution frequencies, was ignored by previous studies. It raises a risk to lose detections of difficult-to-find bugs. Therefore, this paper improves the similarity-based test case prioritization using the ordered sequence of program elements measured by execution counts. The empirical results show that our new technique can increase the rate of fault detection more significantly than the coverage-based ART technique. Moreover, our technique can detect bugs in loops more quickly and be more cost-benefits than the traditional ones.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228980,,yes,,yes,,,yes,yes,yes,yes
409,A novel approach for test case prioritization,"The process of verifying the modified software in the maintenance phase is called Regression Testing. The size of the regression test suite and its selection process is a complex task for regression testers because of time and budget constraints. In this research paper, new Prioritization technique based on hamming distance has been proposed. It is illustrated using an example and found that it produces good results. Average Percentage of Fault Detection (APFD) metrics and charts has been used to show the effectiveness of proposed algorithm.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724209,,yes,,yes,,,yes,yes,yes,yes
410,A refactoring-based approach for test case selection and prioritization,"Refactoring edits, commonly applied during software development, may introduce faults in a previously-stable code. Therefore, regression testing is usually applied to check whether the code maintains its previous behavior. In order to avoid rerunning the whole regression suite, test case prioritization techniques have been developed to order test cases for earlier achievement of a given goal, for instance, improving the rate of fault detection during regression testing execution. However, as current techniques are usually general purpose, they may not be effective for early detection of refactoring faults. In this paper, we propose a refactoring-based approach for selecting and prioritizing regression test cases, which specializes selection/prioritization tasks according to the type of edit made. The approach has been evaluated through a case study that compares it to well-known prioritization techniques by using a real open-source Java system. This case study indicates that the approach can be more suitable for early detection of refactoring faults when comparing to the other prioritization techniques.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595798,,yes,,yes,,,yes,yes,yes,yes
411,A Study in Prioritization for Higher Strength Combinatorial Testing,"Recent studies have shown that combinatorial interaction testing (CIT) is an effective fault detection technique and that early fault detection can be improved by ordering test suites by interaction based prioritization approaches. Despite research that has shown that higher strength CIT improves fault detection, there have been fewer studies that aim to understand the impact of prioritization based on higher strength criteria. In this paper, we aim to understand how interaction based prioritization techniques perform, in terms of early fault detection when we prioritize based on 3-way interactions. We generalize prior work on prioritizing using 2-way interactions to t-way prioritization, and empirically evaluate this on three open source subjects, across multiple versions of each. We examine techniques that prioritize both existing CIT suites as well as generate new ones in prioritized order. We find that early fault detection can be improved when prioritizing 3-way CIT test suites by interactions that cover more code, and to a lesser degree when generating tests in prioritized order. Our techniques that work only from the specification, appear to work best with 2-way generation.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571645,,yes,,yes,,,yes,yes,yes,yes
413,Adaptive Test-Case Prioritization Guided by Output Inspection,"Test-case prioritization is to schedule the execution order of test cases so as to maximize some objective (e.g., revealing faults early). The existing test-case prioritization approaches separate the process of test-case prioritization and the process of test-case execution by presenting the execution order of all test cases before programmers start running test cases. As the execution information of the modified program is not available for the existing test-case prioritization approaches, these approaches mainly rely on only the execution information of the previous program before modification. To address this problem, we present an adaptive test-case prioritization approach, which determines the execution order of test cases simultaneously during the execution of test cases. In particular, the adaptive approach selects test cases based on their fault-detection capability, which is calculated based on the output of selected test cases. As soon as a test case is selected and runs, the fault-detection capability of each unselected test case is modified according to the output of the latest selected test case. To evaluate the effectiveness of the proposed adaptive approach, we conducted an experimental study on eight C programs and four Java programs. The experimental results show that the adaptive approach is usually significantly better than the total test-case prioritization approach and competitive to the additional test-case prioritization approach. Moreover, the adaptive approach is better than the additional approach on some subjects (e.g, replace and schedule).",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649818,,yes,,yes,,,yes,yes,yes,yes
414,Bridging the gap between the total and additional test-case prioritization strategies,"In recent years, researchers have intensively investigated various topics in test-case prioritization, which aims to re-order test cases to increase the rate of fault detection during regression testing. The total and additional prioritization strategies, which prioritize based on total numbers of elements covered per test, and numbers of additional (not-yet-covered) elements covered per test, are two widely-adopted generic strategies used for such prioritization. This paper proposes a basic model and an extended model that unify the total strategy and the additional strategy. Our models yield a spectrum of generic strategies ranging between the total and additional strategies, depending on a parameter referred to as the p value. We also propose four heuristics to obtain differentiated p values for different methods under test. We performed an empirical study on 19 versions of four Java programs to explore our results. Our results demonstrate that wide ranges of strategies in our basic and extended models with uniform p values can significantly outperform both the total and additional strategies. In addition, our results also demonstrate that using differentiated p values for both the basic and extended models with method coverage can even outperform the additional strategy using statement coverage.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606565,,yes,,yes,,,yes,yes,yes,yes
415,Bypassing Code Coverage Approximation Limitations via Effective Input-Based Randomized Test Case Prioritization,"Test case prioritization assigns the execution priorities of the test cases in a given test suite with the aim of achieving certain goals. Many existing test case prioritization techniques however assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in many software development projects. This paper proposes a novel family of LBS techniques. They make adaptive tree-based randomized explorations with an adaptive randomized candidate test set strategy to diversify the explorations among the branches of the exploration trees constructed by the test inputs in the test suite. They get rid of the assumption on the historical correlation of code coverage between program versions. Our techniques can be applied to programs with or without any previous versions, and hence are more general than many existing test case prioritization techniques. The empirical study on four popular UNIX utility benchmarks shows that, in terms of APFD, our LBS techniques can be as effective as some of the best code coverage-based greedy prioritization techniques ever proposed. We also show that they are significantly more efficient and scalable than the latter techniques.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649820,,yes,,yes,,,yes,yes,yes,yes
419,Coverage-Based Test Case Prioritisation: An Industrial Case Study,This paper presents an industrial case study of coverage-based prioritisation techniques on a real world system with real regression faults. The study evaluates four common and different test case prioritisation techniques and examines the effects of using various coverage criteria on the fault detection rates of the prioritised test suites. The results show that prioritisation techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information does not significantly enhance fault detection rates.,2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569742,,yes,,yes,,,yes,yes,yes,yes
424,Evolutionary Search Algorithms for Test Case Prioritization,"To improve the effectiveness of certain performance goals, test case prioritization techniques are used. These technique schedule the test cases in particular order for execution so as to increase the efficacy in meeting the performance goals. For every change in the program it is considered inefficient to re-execute each and every test case. Test case prioritization techniques arrange the test cases within a test suite in such a way that the most important test case is executed first. This process enhances the effectiveness of testing. This algorithm during time constraint execution has been shown to have detected maximum number fault while including the sever test cases.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918806,,yes,,yes,,no,yes,yes,yes,yes
426,History-Based Test Case Prioritization with Software Version Awareness,"Test case prioritization techniques schedule the test cases in an order based on some specific criteria so that the tests with better fault detection capability are executed at an early position in the regression test suite. Many existing test case prioritization approaches are code-based, in which the testing of each software version is considered as an independent process. Actually, the test results of the preceding software versions may be useful for scheduling the test cases of the later software versions. Some researchers have proposed history-based approaches to address this issue, but they assumed that the immediately preceding test result provides the same reference value for prioritizing the test cases of the successive software version across the entire lifetime of the software development process. Thus, this paper describes ongoing research that studies whether the reference value of the immediately preceding test results is version-aware and proposes a test case prioritization approach based on our observations. The experimental results indicate that, in comparison to existing approaches, the presented one can schedule test cases more effectively.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601820,,yes,,yes,,,yes,yes,yes,yes
431,On the Gain of Measuring Test Case Prioritization,"Test case prioritization (TCP) techniques aim to schedule the order of regression test suite to maximize some properties, such as early fault detection. In order to measure the abilities of different TCP techniques for early fault detection, a metric named average percentage of faults detected (APFD) is widely adopted. In this paper, we analyze the metric APFD and explore the gain of measuring TCP techniques from a control theory viewpoint. Based on that, we propose a generalized metric for TCP. This new metric focuses on the gain of defining early fault detection and measuring TCP techniques for various needs in different evaluation scenarios. By adopting this new metric, not only flexibility can be guaranteed, but also explicit physical significance for the metric will be provided before evaluation.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649891,,yes,,yes,,,yes,yes,yes,yes
432,On the Influence of Model Structure and Test Case Profile on the Prioritization of Test Cases in the Context of Model-Based Testing,"Test case prioritization techniques aim at defining an ordering of test cases that favor the achievement of a goal during test execution, such as revealing faults as earlier as possible. A number of techniques have already been proposed and investigated in the literature and experimental results have discussed whether a technique is more successful than others. However, in the context of model-based testing, only a few attempts have been made towards either proposing or experimenting test case prioritization techniques. Moreover, a number of factors that may influence on the results obtained still need to be investigated before more general conclusions can be reached. In this paper, we present empirical studies that focus on observing the effects of two factors: the structure of the model and the profile of the test case that fails. Results show that the profile of the test case that fails may have a definite influence on the performance of the techniques investigated.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800188,,yes,,yes,,,yes,yes,yes,yes
433,Optimization of test suite-test case in regression test,"Exhaustive product evolution and testing is required to ensure the quality of product. Regression testing is crucial to ensure software excellence. Regression test cases are applied to assure that new or adapted features do not relapse the existing features. As innovative features are included, new test cases are generated to assess the new functionality, and then included in the existing pool of test cases, thus escalating the cost and the time required in performing regression test and this unswervingly impacts the release, laid plan and the quality of the product. Hence there is a need to select minimal test cases that will test all the functionalities of the engineered product and it must rigorously test the functionalities that have high risk exposure. Test Suite-Test Case Refinement Technique will reduce regression test case pool size, reduce regression testing time, cost &amp; effort and also ensure the quality of the engineered product. This technique is a regression test case optimization technique that is a hybrid of Test Case Minimization based on specifications and Test Case Prioritization based on risk exposure. This approach will facilitate achievement of quality product with decreased regression testing time and cost yet uncover same amount of errors as the original test cases.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724206,,no,,yes,,,yes,yes,yes,yes
434,Prioritizing Variable-Strength Covering Array,"Combinatorial interaction testing is a well-studied testing strategy, and has been widely applied in practice. Combinatorial interaction test suite, such as fixed-strength and variable-strength interaction test suite, is widely used for combinatorial interaction testing. Due to constrained testing resources in some applications, for example in combinatorial interaction regression testing, prioritization of combinatorial interaction test suite has been proposed to improve the efficiency of testing. However, nearly all prioritization techniques may only support fixed-strength interaction test suite rather than variable-strength interaction test suite. In this paper, we propose two heuristic methods in order to prioritize variable-strength interaction test suite by taking advantage of its special characteristics. The experimental results show that our methods are more effective for variable-strength interaction test suite by comparing with the technique of prioritizing combinatorial interaction test suites according to test case generation order, the random test prioritization technique, and the fixed-strength interaction test suite prioritization technique. Besides, our methods have additional advantages compared with the prioritization techniques for fixed-strength interaction test suite.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649874,,no,,yes,,yes,yes,yes,yes,yes
436,Regression Testing Prioritization Based on Model Checking for Safety-Crucial Embedded Systems,"The order in which test-cases are executed has an influence on the rate at which faults can be detected. In this paper we demonstrate how test-case prioritization can be performed with the use of model-checkers. For this, different well known prioritization techniques are adapted for model-based use. New property based prioritization techniques are introduced. In addition it is shown that prioritization can be done at test-case generation time, thus removing the need for test-suite post-processing. Several experiments for safety-crucial embedded systems are used to show the validity of these ideas.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598153,,no,,yes,,,yes,yes,yes,yes
437,Research on optimization scheme of regression testing,"Regression testing is an important process during software development. In order to reduce costs of regression testing, research on optimization of scheme of regression testing have been done in this paper. For the purpose of reducing the number of test cases and detecting faults of programs early, this paper proposed to combine test case selection with test case prioritization. Regression testing process has been designed and optimization of testing scheme has been implemented. The criterion of test case selection is modify impact of programs, finding programs which are impacted by program modification according to modify information of programs and dependencies between programs. Test cases would be selected during test case selection. The criterion of test case prioritization is coverage ability and troubleshooting capabilities of test case. Test cases which have been selected during test case selection would be ordering in test case prioritization. Finally, the effectiveness of the new method is discussed.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818242,,no,,yes,,yes,yes,yes,yes,yes
440,Selection and Prioritization of Test Cases by Combining White-Box and Black-Box Testing Methods,"In this paper, we present a methodology that combines both white-box and black-box testing, in order to improve testing quality for a given class of embedded systems. The goal of this methodology is generation of test cases for the new functional testing campaign based on the test coverage information from the previous testing campaign, in order to maximize the test coverage. Test coverage information is used for selection of proper test cases in order to improve the quality of testing and save available resources for testing. As an output, a set of test cases is produced. Generated test cases are processed by the test Executor application that decides whether results have passed or failed, based on the results of image grabbing, OCR text extraction, and comparison with expected text. The presented methodology is finally validated by means of a case-study targeting an Android device. The results of the case study are affirmative and they indicate that the proposed methodology is applicable for testing embedded systems of this kind.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664523,,yes,,yes,,no,yes,yes,yes,yes
445,Test Case Prioritization for Continuous Regression Testing: An Industrial Case Study,"Regression testing in continuous integration environment is bounded by tight time constraints. To satisfy time constraints and achieve testing goals, test cases must be efficiently ordered in execution. Prioritization techniques are commonly used to order test cases to reflect their importance according to one or more criteria. Reduced time to test or high fault detection rate are such important criteria. In this paper, we present a case study of a test prioritization approach ROCKET (Prioritization for Continuous Regression Testing) to improve the efficiency of continuous regression testing of industrial video conferencing software. ROCKET orders test cases based on historical failure data, test execution time and domain-specific heuristics. It uses a weighted function to compute test priority. The weights are higher if tests uncover regression faults in recent iterations of software testing and reduce time to detection of faults. The results of the study show that the test cases prioritized using ROCKET (1) provide faster fault detection, and (2) increase regression fault detection rate, revealing 30% more faults for 20% of the test suite executed, comparing to manually prioritized test cases.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676952,,yes,,yes,,,yes,yes,yes,yes
446,Test Case Prioritization Using Requirements-Based Clustering,"The importance of using requirements information in the testing phase has been well recognized by the requirements engineering community, but to date, a vast majority of regression testing techniques have primarily relied on software code information. Incorporating requirements information into the current testing practice could help software engineers identify the source of defects more easily, validate the product against requirements, and maintain software products in a holistic way. In this paper, we investigate whether the requirements-based clustering approach that incorporates traditional code analysis information can improve the effectiveness of test case prioritization techniques. To investigate the effectiveness of our approach, we performed an empirical study using two Java programs with multiple versions and requirements documents. Our results indicate that the use of requirements information during the test case prioritization process can be beneficial.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569743,,yes,,yes,,,yes,yes,yes,yes
447,Test suite prioritisation using trace events technique,"The size of the test suite and the duration of time determines the time taken by the regression testing. Conversely, the testers can prioritise the test cases by the use of a competent prioritisation technique to obtain an increased rate of fault detection in the system, allowing for earlier corrections, and getting higher overall confidence that the software has been tested suitably. A prioritised test suite is more likely to be more effective during that time period than would have been achieved via a random ordering if execution needs to be suspended after some time. An enhanced test case ordering may be probable if the desired implementation time to run the test cases is proven earlier. This research work's main intention is to prioritise the regressiontesting test cases. In order to prioritise the test cases some factors are considered here. These factors are employed in the prioritisation algorithm. The trace events are one of the important factors, used to find the most significant test cases in the projects. The requirement factor value is calculated and subsequently a weightage is calculated and assigned to each test case in the software based on these factors by using a thresholding technique. Later, the test cases are prioritised according to the weightage allocated to them. Executing the test cases based on the prioritisation will greatly decreases the computation cost and time. The proposed technique is efficient in prioritising the regression test cases. The new prioritised subsequences of the given unit test suites are executed on Java programs after the completion of prioritisation. Average of the percentage of faults detected is an evaluation metric used for evaluating the 'superiority' of these orderings.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519507,,yes,,yes,,,yes,yes,yes,yes
448,Using Dependency Structures for Prioritization of Functional Test Suites,"Test case prioritization is the process of ordering the execution of test cases to achieve a certain goal, such as increasing the rate of fault detection. Increasing the rate of fault detection can provide earlier feedback to system developers, improving fault fixing activity and, ultimately, software delivery. Many existing test case prioritization techniques consider that tests can be run in any order. However, due to functional dependencies that may exist between some test cases-that is, one test case must be executed before another-this is often not the case. In this paper, we present a family of test case prioritization techniques that use the dependency information from a test suite to prioritize that test suite. The nature of the techniques preserves the dependencies in the test ordering. The hypothesis of this work is that dependencies between tests are representative of interactions in the system under test, and executing complex interactions earlier is likely to increase the fault detection rate, compared to arbitrary test orderings. Empirical evaluations on six systems built toward industry use demonstrate that these techniques increase the rate of fault detection compared to the rates achieved by the untreated order, random orders, and test suites ordered using existing ""coarse-grained__?_ techniques based on function coverage.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189361,,yes,,yes,,,yes,yes,yes,yes
449,A code coverage-based test suite reduction and prioritization framework,"Software testing is extensively used to ensure the development of a quality software system. The test suite size tends to increase by including new test cases due to software evolution. Consequently, the entire test suite cannot be executed considering budget and time limitations. Researchers have examined test suite reduction and prioritization techniques to address the test suite size problem. However, combination of these techniques can be useful for various regression testing situations. In this paper, we present a new code coverage-based test suite reduction and prioritization framework called TestOptimizer. The framework performs a suitable combination of TestFilter and St-Total techniques to determine optimal test cases, keeping in view of time restrictions. The performance of the proposed framework has been assessed using a case study. Results show that TestOptimizer can be beneficial to solve the test suite size problem within time constraints and has a profound impact on the required cost and effort of regression testing.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076910,,yes,,yes,,,yes,yes,yes,yes
450,A Comparison of Test Case Prioritization Criteria for Software Product Lines,"Software Product Line (SPL) testing is challenging due to the potentially huge number of derivable products. To alleviate this problem, numerous contributions have been proposed to reduce the number of products to be tested while still having a good coverage. However, not much attention has been paid to the order in which the products are tested. Test case prioritization techniques reorder test cases to meet a certain performance goal. For instance, testers may wish to order their test cases in order to detect faults as soon as possible, which would translate in faster feedback and earlier fault correction. In this paper, we explore the applicability of test case prioritization techniques to SPL testing. We propose five different prioritization criteria based on common metrics of feature models and we compare their effectiveness in increasing the rate of early fault detection, i.e. a measure of how quickly faults are detected. The results show that different orderings of the same SPL suite may lead to significant differences in the rate of early fault detection. They also show that our approach may contribute to accelerate the detection of faults of SPL test suites based on combinatorial testing.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823864,,yes,,yes,,,yes,yes,yes,yes
452,A hierarchical test case prioritization technique for object oriented software,"Software reuse is the use of existing artifacts to create new software. Inheritance is the foremost technique of reuse. But the inherent complexity due to inheritance hierarchy found in object - oriented paradigm also affect testing. Every time any change occurs in the software, new test cases are added in addition to the existing test suite. So there is need to conduct effective regression testing having less number of test cases to reduce cost and time. In this paper a hierarchical test case prioritization technique is proposed wherein various factors have been considered that affect error propagation in the inheritance. In this paper prioritization of test cases take place at two levels. In the first level the classes are prioritized and in the second level the test cases of prioritized classes are ordered. To show the effectiveness of proposed technique it was applied and analyze on a C++ program.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019794,,yes,,,yes,no,yes,yes,yes,yes
454,A study of applying severity-weighted greedy algorithm to software test case prioritization during testing,"Regression testing is a very useful technique for software testing. Traditionally, there are several techniques for test case prioritization; two of the most used techniques are Greedy and Additional Greedy Algorithm (GA and AGA). However, it can be found that they may not consider the severity while prioritizing test cases. In this paper, an Enhanced Additional Greedy Algorithm (EAGA) is proposed for test case prioritization. Experiments with eight subject programs are performed to investigate the effects of different techniques under different criteria and fault severity. Experimental results show that proposed EAGA perform well than other techniques.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058806,,no,,,yes,,yes,yes,yes,yes
463,Development test case prioritization technique in regression testing based on hybrid criteria,"Test case prioritization techniques improve the performance of regression testing, and arrange test cases in order to obtain maximum available fault that is going to be detected in a shorter time. In this research the priority is given to test cases that are performed based on multiple criteria and hybrid criteria to enhance the effectiveness of time and cost for proposed technique. This paper shows that our prioritization technique is appropriate for regression testing environment and show that our prioritization approach frequently produces a higher average percentage of fault detection rate value, for web application. The experiments also reveal fundamental tradeoffs in the performance of time aware prioritization. In this technique some fault will be seeded in subject application, then applying the prioritization criteria on test cases to obtain the effective time of average percentage fault detection rate.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986033,,yes,,,yes,,yes,yes,yes,yes
465,Dynamic test case prioritization based on multi-objective,"Test case prioritization technology is to sort the test cases before the software testing designed to improve test efficiency. This paper presents a dynamic test case prioritization technique based on multi-objective. It integrates several traditional single-objective technologies so that makes it more flexible. This technology, from five dimensions, calculates prioritization values of test cases separately. Then a weighted sum is made to the values and it sorts the test cases according to the values. The results return to the storage in order to dynamically adjust the sort of test cases. This technology not only meets the high demands of regression testing, but also ensures the high efficiency of the test results.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888744,,yes,,,yes,,yes,yes,yes,yes
466,Effective Regression Testing Using Requirements and Risks,"The use of system requirements and their risks enables software testers to identify more important test cases that can reveal faults associated with risky components. Having identified those test cases, software testers can manage the testing schedule more effectively by running such test cases earlier so that they can fix faults sooner. Some work in this area has been done, but the previous approaches and studies have some limitations, such as an improper use of requirements risks in prioritization and an inadequate evaluation method. To address the limitations, we implemented a new requirements risk-based prioritization technique and evaluated it considering whether the proposed approach can detect faults earlier overall. It can also detect faults associated with risky components earlier. Our results indicate that the proposed approach is effective for detecting faults early and even better for finding faults associated with risky components of the system earlier than the existing techniques.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6895426,,no,,,yes,,yes,yes,yes,yes
474,Parallelized ACO algorithm for regression testing prioritization in hadoop framework,"Regression testing is an important strategy in the software maintenance phase to produce a high quality software product. This testing ensures that the modified system code does not have an effect on the original software system. Initially, the test suite is generated for the existing software system. After the system undergoes changes the test suite contains both the original test cases and the modified test cases. Regression test prioritization method helps to separate the optimal test cases from the modified test suite. In the existing work, the multi-criteria optimization was applied for generating optimal regression test cases and it was carried out in a non-parallelized environment. The proposed solution is to extend the existing work by generating an optimized test suite using Ant Colony Optimization (ACO) technique on Hadoop Map reduce framework in a parallelized environment.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019371,,yes,,,yes,no,yes,yes,yes,yes
477,Prioritization of Unit Testing on non-object oriented using a top-down based approach,"The issue which makes Unit Testing so tough is the ambiguous ways that the software world keeps moving forward. Although sometimes by implementing simple unit testing methods, this task become easy to handle. However to achieve a comprehensive unit testing it is supposed to test all corners of software such as database, devices, communication etc. This paper proposes an orthogonal software testing approach based on Top-Down technique which treats the input parameters of a software unit in an orthogonal partitioning to dynamic level of testing. Describes how test cases are statistically for each trial of software testing steps and makes a dynamic partitioning approach on non-object oriented experiments. The adequacy of the generated test cases can be validated by examining testing coverage metrics. The authors have considered using of different partitioning and mock objects help to make an isolated testing, improve code's structure and automated testing possibility. The results of the test case executions can be analyzed in order to find the __??IF__?_ metrics for partitioning the traceable ways and detecting defects, to generate more effective test cases in future testing, and to help locate and correct defects in the early stage of testing.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6985991,,yes,,,yes,,yes,yes,yes,yes
479,"RDCC: An effective test case prioritization framework using software requirements, design and source code collaboration","Test case prioritization is a technique for selecting those test cases, which are expected to outperform for determining faulty modules earlier. Different phases of software development lifecycle represent the total software from different point of views, where priority module may vary from phase to phase. However, information from different phases of software development lifecycle is rarely introduced and no one integrates that information to prioritize test cases. This paper presents an effective test case prioritization framework, which takes software requirements specification, design diagrams, source codes and test cases as input and provides a prioritized order of test cases using their collaborative information as output. Requirement IDs are split into words or terms excluding stop words to calculate requirements relativity. Design diagrams are extracted as readable XML format to calculate the degree of interconnectivity among the activities. Source codes are parsed as call graphs where vertices and edges represent classes, and calls between two classes respectively. Requirements relativity, design interconnectivity and class dependencies are multiplied by their assigned weight to calculate final weight and select test cases by mapping the customers' requirements and test cases using that weight. The proposed framework is validated with an academic project and the results show that use of collaborative information during prioritization process can be beneficial.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7073072,,yes,,,yes,,yes,yes,yes,yes
480,Regression Testing Approach for Large-Scale Systems,"Regression testing is an important and expensive activity that is undertaken every time a program is modified to ensure that the changes do not introduce new bugs into previously validated code. Instead of re-running all test cases, different approaches were studied to solve regression testing problems. Data mining techniques are introduced to solve regression testing problems with large-scale systems containing huge sets of test cases, as different data mining techniques were studied to group test cases with similar features. Dealing with groups of test cases instead of each test case separately helped to solve regression testing scalability issues. In this paper, we propose a new methodology for regression testing of large-scale systems using data mining techniques to prioritize and select test cases based on their coverage criteria and fault history.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983822,,yes,,,yes,,yes,yes,yes,yes
484,Test Case Prioritization Based on Information Retrieval Concepts,"In regression testing, running all a system's test cases can require a great deal of time and resources. Test case prioritization (TCP) attempts to schedule test cases to achieve goals such as higher coverage or faster fault detection. While code coverage-based approaches are typical in TCP, recent work has explored the use of additional information to improve effectiveness. In this work, we explore the use of Information Retrieval (IR) techniques to improve the effectiveness of TCP, particularly for testing infrequently tested code. Our approach considers the frequency at which elements have been tested, in additional to traditional coverage information, balancing these factors using linear regression modeling. Our empirical study demonstrates that our approach is generally more effective than both random and traditional code coverage-based approaches, with improvements in rate of fault detection of up to 4.7%.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091286,,yes,,,yes,,yes,yes,yes,yes
485,Test case prioritization techniques __??an empirical study__?_,"Regression testing is an expensive process. A number of methodologies of regression testing are used to improve its effectiveness. These are retest all, test case selection, test case reduction and test case prioritization. Retest all technique involves re-execution of all available test suites, which are critical moreover cost effective. In order to increase efficiency, test case prioritization is being utilized for rearranging the test cases. A number of algorithms has been stated in the literature survey such as Greedy Algorithms and Metaheuristic search algorithms. A simple greedy algorithm focuses on test case prioritization but results in less efficient manner, due to which researches moved towards the additional greedy and 2-Optimal algorithms. Forthcoming metaheuristic search technique (Hill climbing and Genetic Algorithm) produces a much better solution to the test case prioritization problem. It implements stochastic optimization while dealing with problem concern. The genetic algorithm is an evolutionary algorithm which gives an exact mathematical fitness value for the test cases on which prioritization is done. This paper focuses on the comparison of metaheuristic genetic algorithm with other algorithms and proves the efficiency of genetic algorithm over the remaining ones.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045344,,yes,,,yes,,yes,yes,yes,yes
486,Test case prioritization using multi objective particle swarm optimizer,"The goal of regression testing is to validate the modified software. Due to the resource and time constraints, it becomes necessary to develop techniques to minimize existing test suites by eliminating redundant test cases and prioritizing them. This paper proposes a 3-phase approach to solve test case prioritization. In the first phase, we are removing redundant test cases by simple matrix operation. In the second phase, test cases are selected from the test suite such that selected test cases represent the minimal set which covers all faults and also at the minimum execution time. For this phase, we are using multi objective particle swarm optimization (MOPSO) which optimizes fault coverage and execution time. In the third phase, we allocate priority to test cases obtained from the second phase. Priority is obtained by calculating the ratio of fault coverage to the execution time of test cases, higher the value of the ratio higher will be the priority and the test cases which are not selected in phase 2 are added to the test suite in sequential order. We have also performed experimental analysis based on maximum fault coverage and minimum execution time. The proposed MOPSO approach is compared with other prioritization techniques such as No Ordering, Reverse Ordering and Random Ordering by calculating Average Percentage of fault detected (APFD) for each technique and it can be concluded that the proposed approach outperformed all techniques mentioned above.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6884931,,yes,,,yes,,yes,yes,yes,yes
487,Test case prioritization with improved genetic algorithm,"In software development, the most time consuming phase is maintenance. Regression testing, which is a part of maintenance, deals with test case prioritization that aims to increase rate of fault detection with less number of tests. In our study, we used 100 tests and 1000 faults; however, faults are detected by tests using genetic algorithm and improved genetic algorithm. After test case prioritization, we may detect all faults with less number of tests so there'll no need to apply all 100 tests (re-test).",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6830456,,yes,,,yes,,yes,yes,yes,yes
491,The research of the test case prioritization algorithm for black box testing,"In order to improve the efficiency of software test case execution, this paper analyzed the impact of some factors to test cases prioritization and presented two adjustment algorithms. These factors included software requirement prioritization, software failure severity and software failure probability level. Firstly, gave the definition of software requirement prioritization, the ranking methods of software failure severity and software failure probability level, the description of the relationship between test cases and test requirements. Then, presented an initial test case prioritization method based on the analysis. And then, proposed a dynamic adjustment algorithm using of software requirement prioritization and software failure probability level when software failure occurred. Experimental data show that the two test case prioritization algorithms can improve the efficiency of software testing and are helpful to find more software defects in a short period.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933509,,yes,,,yes,no,yes,yes,yes,yes
496,A Clustering-Bayesian Network Based Approach for Test Case Prioritization,"Test case prioritization can effectively reduce the cost of regression testing by executing test cases with respect to their contributions to testing goals. Previous research has proved that the Bayesian Networks based technique which uses source code change information, software quality metrics and test coverage data has better performance than those methods merely depending on only one of the items above. Although the former Bayesian Networks based Test Case Prioritization (BNTCP) focusing on assessing the fault detection capability of each test case can utilize all three items above, it still has a deficiency that ignores the similarity between test cases. For mitigating this problem, this paper proposes a hybrid regression test case prioritization technique which aims to achieve better prioritization by incorporating code coverage based clustering approach with BNTCP to depress the impact of those similar test cases having common code coverage. Experiments on two Java projects with mutation faults and one Java project with hand-seeded faults have been conducted to evaluate the fault detection performance of the proposed approach against Additional Greedy approach, Bayesian Networks based approach (BNTCP), Bayesian Networks based approach with feedback (BNA) and code coverage based clustering approach. The experimental results showed that the proposed approach is promising.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273420,,yes,,,yes,,yes,yes,yes,yes
497,A coupling effect based test case prioritization technique,"Regression testing is a process that executes subset of tests that have already been conducted to ensure that changes have not propagated unintended side effects. Test case prioritization aims at reordering the regression test suit based on certain criteria, so that the test cases with higher priority can be executed first rather than those with lower priority. In this paper, a new approach for test case prioritization has been proposed which is based on a module-coupling effect that considers the module-coupling value for the purpose of prioritizing the modules in the software so that critical modules can be identified which in turn will find the prioritized set of test cases. In this way there will be high percentage of detecting critical errors that have been propagated to other modules due to any change in a module. The proposed approach has been evaluated with the case study of software consisting of ten modules.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100468,,yes,,,yes,,yes,yes,yes,yes
499,A methodology for regression testing reduction and prioritization of agile releases,"Regression testing is the type of software testing that seeks to uncover new software bugs in existing areas of a system after changes have been made to them. The significance of regression testing have grown in the past decade with the amplified adoption of agile development methodologies, which requires the execution of regression testing at the end of each release. In this paper, we present an automated agile regression testing approach that reduces the number of test cases to be used at regression phase depending on the similarity of issues exposed from the different test cases, taking into consideration the user story coverage. It then prioritizes the reduced test cases using user-provided weighted agile parameters. The proposed approach achieves enhancement for both the reduction and prioritization of test cases for agile regression testing.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426903,,yes,,,yes,no,yes,yes,yes,yes
501,A Schema Support for Selection of Test Case Prioritization Techniques,"Regression testing is a vast field of research. It is very costly and time consuming process but on the other hand very important process in software testing. Retest all, Test case Selection, Hybrid and Test Case Prioritization are its various techniques which are used to reduce the efforts in maintenance phase. In technical literature several techniques are present with their different and vast number of goals which can be applied in software projects despite of that they have not proven their true efficiency in the testing process. The major problem in regression testing area is to select the test case prioritization technique/s that is effective in such a way that maximum project characteristics should be cover in a minimum time span. However, consideration of this decision be carefully done so that loss of resources can be avoided in a software project. Based on the above scenario, author proposes a selection schema to support the selection of TCP techniques for a given software project aiming at maximizing the coverage of software project characteristics considering aspect of prioritization of software project characteristics. At the end, preliminary results of an experimental evaluation are presented. The purpose of this research is decision should be based on the objective knowledge of the techniques rather than considering some perception and assumptions.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079143,,yes,,,yes,,yes,yes,yes,yes
502,A similarity-based approach for test case prioritization using historical failure data,"Test case prioritization is a crucial element in software quality assurance in practice, specially, in the context of regression testing. Typically, test cases are prioritized in a way that they detect the potential faults earlier. The effectiveness of test cases, in terms of fault detection, is estimated using quality metrics, such as code coverage, size, and historical fault detection. Prior studies have shown that previously failing test cases are highly likely to fail again in the next releases, therefore, they are highly ranked, while prioritizing. However, in practice, a failing test case may not be exactly the same as a previously failed test case, but quite similar, e.g., when the new failing test is a slightly modified version of an old failing one to catch an undetected fault. In this paper, we define a class of metrics that estimate the test cases quality using their similarity to the previously failing test cases. We have conducted several experiments with five real world open source software systems, with real faults, to evaluate the effectiveness of these quality metrics. The results of our study show that our proposed similarity-based quality measure is significantly more effective for prioritizing test cases compared to existing test case quality measures.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381799,,yes,,,yes,,yes,yes,yes,yes
504,A Subsumption Hierarchy of Test Case Prioritization for Composite Services,"Many composite workflow services utilize non-imperative XML technologies such as WSDL, XPath, XML schema, and XML messages. Regression testing should assure the services against regression faults that appear in both the workflows and these artifacts. In this paper, we propose a refinement-oriented level-exploration strategy and a multilevel coverage model that captures progressively the coverage of different types of artifacts by the test cases. We show that by using them, the test case prioritization techniques initialized on top of existing greedy-based test case prioritization strategy form a subsumption hierarchy such that a technique can produce more test suite permutations than a technique that subsumes it. Our experimental study of a model instance shows that a technique generally achieves a higher fault detection rate than a subsumed technique, which validates that the proposed hierarchy and model have the potential to improve the cost-effectiveness of test case prioritization techniques.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839018,,yes,,,yes,,yes,yes,yes,yes
507,An effective test case prioritization method based on fault severity,"In regression testing area, test case prioritization is one of the main techniques to improve the test validity and test effectiveness. However, when the test cases have the same maximum coverage rate, the random selection of the additional statement will influence the effect of sorting. For dealing with this problem, a new method is proposed to optimize test case prioritization based on fault severity, referred to as additional-statement-on-fault-severity. Facing those same maximum coverage rate, the new technique main consider a factor, fault severity, to sort test cases, it figures out the value of test case based on the algorithm of the new technique and order the sequence from high to low. Experiment results show that the improved technique of test case prioritizaftion can improve the efficiency of regression testing.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339162,,yes,,,yes,no,yes,yes,yes,yes
508,An Information Retrieval Approach for Regression Test Prioritization Based on Program Changes,"Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight open-source Java projects shows that REPiR is computationally efficient and performs better than existing (dynamic or static) techniques for the majority of subject systems.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194580,,yes,,,yes,,yes,yes,yes,yes
513,Clustering based novel test case prioritization technique,"Regression testing is an activity during the maintenance phase to validate the changes made to the software and to ensure that these changes would not affect the previously verified code or functionality. Often, regression testing is performed with limited computing resources and time budget. So in this phase, it is infeasible to run the complete test suite Thus, test-case prioritization approaches are applied to ensure the execution of test cases in some prioritized order and to achieve some specific goals like, increasing the rate of bug detection, identifying the most critical bugs as early as possible etc. In this research work, we are going to propose a new and more effective clustering based prioritization technique that uses various metrics and execution time of test cases to reorder them. The results of implementation will prove that the suggested approach is more productive than the existing coverage and clustering based prioritization techniques.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506447,,yes,,,yes,,yes,yes,yes,yes
516,Component-Based Software System Test Case Prioritization with Genetic Algorithm Decoding Technique Using Java Platform,"Test case prioritization includes testing experiments in a request that builds the viability in accomplishing some execution objectives. The importance amongst the most imperative testing objectives is the fast rate of fault recognition. Test case ought to run in a request that extends the likelihood of fault discovery furthermore that detects the most serious issues at the early stage of testing life cycle. In this paper, we develop and prove the necessity of Component-Based Software testing prioritization framework which plans to uncover more extreme bugs at an early stage and to enhance software product deliverable quality utilizing Genetic Algorithm (GA) with java decoding technique. For this, we propose a set of prioritization keys to plan the proposed Component-Based Software java framework. In our proposed method, we allude to these keys as Prioritization Keys (PK). These keys may be project size, scope of the code, information stream, and bug inclination and impact of fault or bug on overall system, which prioritizes the Component-Based Software framework testing. The integrity of these keys was measured with implementation of key assessment metric called KAM that will likewise be ascertained. This paper demonstrates how software testing can be efficient with management of data integrity factor to avoid major security issues. One of the main advantages of our approach is that domain specific semantics can be integrated with the data quality test cases prioritization, thus being able to discover test feed data quality problems beyond conventional quality measures.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155967,,yes,,,yes,no,yes,yes,yes,yes
525,GUI Test Case Prioritization by State-Coverage Criterion,"Graphical User Interface (GUI) application is a kind of typical event-driven software (EDS) that transforms state according to input events invoked through a user interface. It is time consuming to test a GUI application since there are a large number of possible event sequences generated by the permutations and combinations of user operations. Although some GUI test case prioritization techniques have been proposed to determine ""which test case to execute next"" for early fault detection, most of them use random ordering to break tie cases, which has been proved to be ineffective. Recent research presents the opinion that using hybrid criteria can be an effective way for tie-breaking, but few studies focus on seeking a new criterion cooperating well with other criteria when breaking tie cases. In this paper, we propose a state-distance-based method using state coverage as a new criterion to prioritize GUI test cases. An empirical study on three GUI programs reveals that the state-distance-based method is really suitable for GUI test case prioritization and can cooperate well with the (additional) event length criterion.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166260,,yes,,,yes,no,yes,yes,yes,yes
526,History-Based Test Case Prioritization for Black Box Testing Using Ant Colony Optimization,"Test case prioritization is a technique to improve software testing. Although a lot of work has investigated test case prioritization, they focus on white box testing or regression testing. However, software testing is often outsourced to a software testing company, in which testers are rarely able to access to source code due to a contract. Herein a framework is proposed to prioritize test cases for black box testing on a new product using the test execution history collected from a similar prior product and the Ant Colony Optimization. A simulation using two actual products shows the effectiveness and practicality of our proposed framework.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102622,,yes,,,yes,,yes,yes,yes,yes
535,Modification Impact Analysis Based Test Case Prioritization for Regression Testing of Service-Oriented Workflow Applications,"Test case prioritization for regression testing is an approach that schedules test cases to improve the efficiency of service-oriented workflow application testing. Most of existing prioritization approaches range test cases according to various metrics (e.g., Statement coverage, path coverage) in different application context. Service-oriented workflow applications orchestrate web services to provide value-added service and typically are long-running and time-consuming processes. Therefore, these applications need more precise prioritization to execute earlier those test cases that may detect failures. Surprisingly, most of current regression test case prioritization researches neglect to use internal structure information of software, which is a significant factor influencing the prioritization of test cases. Considering the internal structure information and fault propagation behavior of modifications respect to modified version for service-oriented workflow applications, we present in this paper a new regression test case prioritization approach. Our prioritization approach schedules test cases based on dependence analysis of internal activities in service-oriented workflow applications. Experimental results show that test case prioritization using our approach is more effective than conventional coverage-based techniques.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273631,,yes,,,yes,,yes,yes,yes,yes
536,Multi-perspective Regression Test Prioritization for Time-Constrained Environments,"Test case prioritization techniques are widely used to enable reaching certain performance goals during regression testing faster. A commonly used goal is high fault detection rate, where test cases are ordered in a way that enables detecting faults faster. However, for optimal regression testing, there is a need to take into account multiple performance indicators, as considered by different project stakeholders. In this paper, we introduce a new optimal multi-perspective approach for regression test case prioritization. The approach is designed to optimize regression testing for faster fault detection integrating three different perspectives: business perspective, performance perspective, and technical perspective. The approach has been validated in regression testing of industrial mobile device systems developed in continuous integration. The results show that our proposed framework efficiently prioritizes test cases for faster and more efficient regression fault detection, maximizing the number of executed test cases with high failure frequency, high failure impact, and cross-functional coverage, compared to manual practice.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272927,,yes,,,yes,,yes,yes,yes,yes
537,Mutation-based test-case prioritization in software evolution,"During software evolution, to assure the software quality, test cases for an early version tend to be reused by its latter versions. As a large number of test cases may aggregate during software evolution, it becomes necessary to schedule the execution order of test cases so that the faults in the latter version may be detected as early as possible, which is test-case prioritization in software evolution. In this paper, we proposed a novel test-case prioritization approach for software evolution, which first uses mutation faults on the difference between the early version and the latter version to simulate real faults occurred in software evolution, and then schedules the execution order of test cases based on their fault-detection capability, which is defined based on mutation faults. In particular, we present two models on calculating fault-detection capability, which are statistics-based model and probability-based model. Moreover, we conducted an experimental study and found that our approach with the statistics-based model outperforms our approach with the probability-based model and the total statement coverage-based approach, and slightly outperforms the additional statement-coverage based approach in many cases. Furthermore, compared with the total or additional statement coverage-based approach, our approach with either the statistics-based model or the probability-based model tends to be stably effective when the difference on the source code between the early version and the latter version is non-trivial.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381798,,yes,,,yes,,yes,yes,yes,yes
538,PORA: Proportion-Oriented Randomized Algorithm for Test Case Prioritization,"Effective testing is essential for assuring software quality. While regression testing is time-consuming, the fault detection capability may be compromised if some test cases are discarded. Test case prioritization is a viable solution. To the best of our knowledge, the most effective test case prioritization approach is still the additional greedy algorithm, and existing search-based algorithms have been shown to be visually less effective than the former algorithms in previous empirical studies. This paper proposes a novel Proportion-Oriented Randomized Algorithm (PORA) for test case prioritization. PORA guides test case prioritization by optimizing the distance between the prioritized test suite and a hierarchy of distributions of test input data. Our experiment shows that PORA test case prioritization techniques are as effective as, if not more effective than, the total greedy, additional greedy, and ART techniques, which use code coverage information. Moreover, the experiment shows that PORA techniques are more stable in effectiveness than the others.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272924,,yes,,,yes,,yes,yes,yes,yes
543,Prioritized test-driven reverse engineering process: A case study,"In this study we empirically investigate the adaptation of Test-Driven Development (TDD) practice into software Reverse Engineering (RE) process. We call this adaptation as Test-Driven Reverse Engineering (TDRE) process. We propose a two-layer prioritization process, which firstly prioritizes the already-implemented functionalities using the Cumulative Voting (CV) method and three prioritization criteria (importance, complexity and dependency), and secondly prioritizes test-cases for each prioritized functionality, using the same criteria. We conducted a case study in academia with students to empirically evaluate the usability and effectiveness of the prioritization process and the TDD adaptation into RE process. The results have shown that students with a good performance in testing had also good performance in designing UML class-diagrams. Moreover, the implementation of hierarchical test-cases for the already prioritized functionalities, improves code comprehension and redesigning in the RE process in terms of better total grades obtained.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7388099,,yes,,,yes,,yes,yes,yes,yes
544,Prioritizing Manual Test Cases in Traditional and Rapid Release Environments,"Test case prioritization is one of the most practically useful activities in testing, specially for large scale systems. The goal is ranking the existing test cases in a way that they detect faults as soon as possible, so that any partial execution of the test suite detects maximum number of defects for the given budget. Test prioritization becomes even more important when the test execution is time consuming, e.g., manual system tests vs. automated unit tests. Most existing test case prioritization techniques are based on code coverage, which requires access to source code. However, manual testing is mainly done in a black- box manner (manual testers do not have access to the source code). Therefore, in this paper, we first examine the existing test case prioritization techniques and modify them to be applicable on manual black-box system testing. We specifically study a coverage- based, a diversity-based, and a risk driven approach for test case prioritization. Our empirical study on four older releases of Mozilla Firefox shows that none of the techniques are strongly dominating the others in all releases. However, when we study nine more recent releases of Firefox, where the development has been moved from a traditional to a more agile and rapid release environment, we see a very signifiant difference (on average 65% effectiveness improvement) between the risk-driven approach and its alternatives. Our conclusion, based on one case study of 13 releases of an industrial system, is that test suites in rapid release environments, potentially, can be very effectively prioritized for execution, based on their historical riskiness; whereas the same conclusions do not hold in the traditional software development environments.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102602,,yes,,,yes,,yes,yes,yes,yes
550,Strategies for Prioritizing Test Cases Generated Through Model-Based Testing Approaches,"Software testing is expensive and time consuming,especially for complex software. In order to deal with the costof testing, researchers develop Model-Based Testing (MBT). InMBT, test cases are generated automatically and a drawback isa huge generated test suite. Our research aims at studying the Test Case Prioritization problem in MBT context. So far, we already evaluated the influence of the model structure and the characteristics of the test cases that fail. Results suggest that the former does not affect significantly the performance of techniques, however, the latter indeed represents a major impact. Therefore, a worthy information in this context might be an expert who knows the crucial parts of the software, thus we propose the first version of a prioritization technique that considers hints from the expert and the distance notion in order to prioritize test cases. Evaluation and tuning of the technique are ongoing, but preliminary evaluation reveals promising results.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203104,,yes,,,yes,no,yes,yes,yes,yes
553,Test case prioritization for regression testing based on ant colony optimization,"Test case prioritization technique is an efficient method to improve regression testing activities. It orders a regression test suite to execute the test cases with higher priority earlier than those with lower priority, and the problem is how to optimize the test case ordering according to some criterion. In this paper, we have proposed an algorithm which prioritizes the test cases based on ant colony optimization (ACO), considering three factors: number of faults detected, execution time and fault severity, and these three factors are used in ant colony optimization algorithm to help to reveal more severe faults at earlier stage of the regression testing process. The effectiveness of the algorithm is demonstrated using the metric named APFD, and the results of experiment show the algorithm optimizes the test case orderings effectively.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339054,,yes,,,yes,,yes,yes,yes,yes
554,Test case prioritization with textual comparison metrics,"Regression testing of a large test pool consistently needs a prioritization technique that caters requirements changes. Conventional prioritization techniques cover only the methods to find the ideal ordering of test cases neglecting requirement changes. In this paper, we propose string dissimilarity-based priority assignment to test cases through the combination of classical and non-classical textual comparison metrics and elaborate a prioritization algorithm considering requirement changes. The proposed technique is suitable to be used as a preliminary testing when the information of the entire program is not in possession. We performed evaluation on random permutations and three textual comparison metrics and concluded the findings of the experiment.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475187,,yes,,,yes,,yes,yes,yes,yes
555,Test case prioritization: An approach based on modified ant colony optimization (m-ACO),"Intense and widespread usage of software in every field of life has attracted the researchers to focus their attention on developing the methods to improve the efficiency of software testing; which is the most crucial and cost intensive phase of software development. Software testing aims to uncover the potential faults in Application Under Test by running the test cases on software code. Software code keeps on changing as the uncovered faults during testing are fixed by the developers. Regression testing is concerned with verifying the modified software code to ensure that changes in software code does not induce any undesired effect on rest of the code. Test Case Prioritization is a regression testing technique which re-schedule the execution sequence of test cases to improve the fault detection rate and enhance the performance of regression test suite. This paper focuses on proposing a novel method ""m-ACO"" for test case prioritization and the performance evaluation of the proposed method using Average Percentage of faults Detected.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375627,,yes,,,yes,,yes,yes,yes,yes
556,Test case selection and prioritization using cuckoos search algorithm,"Regression Testing is an inevitable and very costly activity that is implemented to ensure the validity of new version of software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization for proper selection and schedule of test cases in a specific sequence, fulfilling some chosen criteria. Cuckoo search (CS) algorithm is an optimization algorithm proposed by Yang and Deb [13]. It is inspired by the obligate brood parasitism of some cuckoo species by laying their eggs in the nests of other host birds. Cuckoo Search is very easy to implement as it depends on single parameter only unlike other optimization algorithms. In this paper a test case selection and prioritization algorithm has been proposed using Cuckoo Search. This algorithm selects and prioritizes the test cases based on the number of faults covered in minimum time. The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155012,,yes,,,yes,,yes,yes,yes,yes
558,Test cases prioritization for software regression testing using analytic hierarchy process,"Test cases are considered an important asset in the software testing process since they are used to detect defects in the software. In order to produce quality software covering all of the requirements, the test case designer requires much time and effort in designing test cases to cover all requirements and conditions according to the test case structure. This research proposes a method for storing and retrieving of test cases affected by software requirements changes, as well as ranking the retrieved test cases using the AHP method to improve the quality of the ranking. There are to assist system testers in identifying test cases for complete regression testing. An example application of the proposed method will also be presented.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219790,,yes,,,yes,yes,yes,yes,yes,yes
562,Total coverage based regression test case prioritization using genetic algorithm,"Regression Testing is a test to ensure that a program that was changed is still working. Changes introduced to a software product often come with defects. Additional test cases are, this could reduce the main challenges of regression testing is test case prioritization. Time, effort and budget needed to retest the software. Former studies in test case prioritization confirm the benefits of prioritization techniques. Most prioritization techniques concern with choosing test cases based on their ability to cover more faults. Other techniques aim to maximize code coverage. Thus, the test cases selected should secure the total coverage to assure the adequacy of software testing. In this paper, we present an algorithm to prioritize test cases based on total coverage using a modified genetic algorithm. Its performance on the average percentage of condition covered and execution time are compared with five other approaches.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207103,,yes,,,yes,,yes,yes,yes,yes
567,Using Artificial Bee Colony for Code Coverage Based Test Suite Prioritization,"The goal of test suite prioritization is maximizing fault detection and code coverage rate. Several nature inspired optimization algorithms such as Swarm Intelligence (SI) have been studied for the optimization of such problems. The studies revealed the benefits of Artificial Bee Colony (ABC) over other algorithms. ABC and its variations were implemented in software testing areas, test suite prioritization in particular. However, most SI based approaches focus on fault detection ability which is difficult to predict. In this paper, the standard ABC algorithm is used to prioritize test suites based on code coverage. The results reveal that ABC shows promising results and, hence, is a great candidate for prioritizing test suites. It also suggests that a modification to the standard ABC algorithm or combination of ABC and another SI algorithm should yield an even better result.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371038,,yes,,,yes,,yes,yes,yes,yes
569,Using Fuzzy Logic and Symbolic Execution to Prioritize UML-RT Test Cases,"The relative ease of test case generation associated with model-based testing can lead to an increased number of test cases being identified for any given system; this is problematic as it is becoming near impossible to run (or even generate) all of the possible tests in available time frames. Test case prioritization is a method of ranking the tests in order of importance, or priority based on criteria specific to a domain or implementation, and selecting some subset of tests to generate and run. Some approaches require the generation of all tests, and simply prioritize the ones to be run, however we propose an approach that would prevent unnecessary generation of tests through the use of symbolic execution trees to determine which tests provide the most benefit to coverage of execution. Our approach makes use of fuzzy logic, specifically fuzzy control systems, to prioritize test cases generated from these execution; the prioritization is based on natural language rules about testing priority. Within this paper we present our motivation, some background research, our methodology and implementation, results, and conclusions.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102610,,yes,,,yes,,yes,yes,yes,yes
572,A hybrid approach for test case prioritization and selection,"Software testing consists in the dynamic verification of the behavior of a program on a set of test cases. When a program is modified, it must be tested to verify if the changes did not imply undesirable effects on its functionality. The rerunning of all test cases can be impossible, due to cost, time and resource constraints. So, it is required the creation of a test cases subset before the test execution. This is a hard problem and the use of standard Software Engineering techniques could not be suitable. This work presents an approach for test case prioritization and selection, based in relevant inputs obtained from a software development environment. The approach uses Software Quality Function Deployment (SQFD) to deploy the features relevance among the system components, Mamdani fuzzy inference systems to infer the criticality of each class and Ant Colony Optimization to select test cases. An evaluation of the approach is presented, using data from simulations with different number of tests.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744363,,yes,,,yes,,yes,yes,yes,yes
573,A Multi-Objective Technique to Prioritize Test Cases,"While performing regression testing, an appropriate choice for test case ordering allows the tester to early discover faults in source code. To this end, test case prioritization techniques can be used. Several existing test case prioritization techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test case prioritization technique that determines the ordering of test cases that maximize the number of discovered faults that are both technical and business critical. In other words, our new technique aims at both early discovering faults and reducing the execution cost of test cases. To this end, we automatically recover links among software artifacts (i.e., requirements specifications, test cases, and source code) and apply a metric-based approach to automatically identify critical and fault-prone portions of software artifacts, thus becoming able to give them more importance during test case prioritization. We experimentally evaluated our technique on 21 Java applications. The obtained results support our hypotheses on efficiency and effectiveness of our new technique and on the use of automatic artifacts analysis and weighting in test case prioritization.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362042,,yes,,,yes,,yes,yes,yes,yes
574,A novel approach for selecting an effective regression testing technique,"All software systems need modifications with time, these modifications involve different types or amounts of code modifications in different versions. To validate these modifications many regression testing sessions are needed. But researchers do not have a single regression testing technique that can be used on every version. The objective of this scrutiny is to evolve a methodology that attempts to determine the re-testing technique that would be effective for every re-testing period accounting testing domain and conditions. This methodology is based on Revised Analytical Hierarchy Process (Revised AHP). There are numerous regression testing techniques. But this investigation is limited to test case prioritization techniques only. The result showed that prioritization techniques selected by proposed technique are more efficacious than those used by the forgoing techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724438,,yes,,,yes,,yes,yes,yes,yes
579,An Adaptive Sequence Approach for OOS Test Case Prioritization,"Test case prioritization (TCP) attempts to improve fault detection effectiveness by scheduling important test cases earlier, where important is determined by some criteria and strategy. Adaptive random sequences (ARSs) may be applied to improve the effectiveness of TCP in black-box testing. In this paper, to improve the effectiveness of TCP for object-oriented software, we present an ARS approach based on clustering techniques. In the proposed approach, test cases are clustered according to the number of objects and methods, using two clustering algorithms - K-means and K-medoids. Our proposed sampling strategy can construct ARSs within the clustering framework, constructing two ARS sequences based on the two clustering algorithms, which results in generated test cases with different execution sequences. We also report on experimental studies to verify the proposed approach, with the results showing that our approach can enhance the probability of earlier fault detection, and deliver higher effectiveness than random prioritization.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789402,,yes,,,yes,yes,yes,yes,yes,yes
581,Application of Mahalanobis-Taguchi Method and 0-1 Programming Method to Cost-Effective Regression Testing,"To enhance the cost effectiveness of regression testing, this paper proposes a method for prioritizing test cases. In general, a test case can be evaluated from various different points of view, therefore whether it is worth it to re-run should be discussed using multi criteria. This paper shows that the Mahalanobis-Taguchi (MT) method is a useful way to successfully integrate different evaluations of a test case. Moreover, this paper proposes to use the 0-1 programming method together with the MT method in order to take into account not only the priority of a test case but also its cost to run. The empirical study with 300 test cases for an industrial software system shows that the combination of the MT method and the 0-1 programming method is more cost-effective than other conventional methods.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592803,,no,,,yes,,no,yes,yes,yes
582,Applying Assemble Clustering Algorithm and Fault Prediction to Test Case Prioritization,"Cluster application is proposed as an efficient approach to improve test case prioritization, Test case in a same cluster are considered to have similar behaviors. In the process of cluster test case, the selection of test case feature and the clusters number have great influence on the clustering results. but to date almost clustering algorithm to improve test case prioritization are selected random clusters number and clustering result are based on one or a few of the code features, the paper propose a new prioritization techniques that not only consider the best clusters number but also produce the best clustering result based on test case multidimensional feature. After clustering, considering the inter-cluster prioritization and intra-cluster prioritization,in order to improve the effectiveness of our approach, the fault prediction value of code corresponding to the test case is used as one of a prioritization weight. Finally,we implemented an empirical studies using an industrial software to illustrate the effectiveness of the test case prioritization techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780203,,yes,,,yes,no,yes,yes,yes,yes
587,Automation Architecture for Bayesian Network Based Test Case Prioritization and Execution,"An automation architecture for Bayesian Network based test case prioritization is designed for software written in Java programming language following the approach proposed by Mirarab and Tahvildari [2]. The architecture is implemented as an integration of a series of tools and called Bayesian Network based test case prioritization and execution platform. The platform is triggered by a change in the source code, then it collects necessary information to be supplied to Bayesian Network and uses Bayesian Network evaluation results to run high priority unit tests.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552177,,yes,,,yes,,yes,yes,yes,yes
588,AVISAR - a three tier architectural framework for the testing of Object Oriented Programs,"In this research paper, we have proposed a three tier conceptual framework AVISAR for the testing of Object Oriented Programs. We have devised the proposed framework into three levels: Requirements Modeling, Test Case Generation and Effort Estimation. At the first level, we have used the Extend relationship of the Use Case to capture the events generated by the Classes during message passing between them through their objects. These events were captured by an Object Instantiator using Event Templates and further given to a Test case scenario generator, which generates the test cases. At the second level, the test cases will be generated based on the event templates that are used for capturing the events generated by the Extend relationship of the Use cases. Further, we have proposed a Genetic Algorithm (GA) for the effective test case prioritization to ensure maximum code coverage. At the third level, effort estimation, we have used the Cyclometric Complexity and Token count to perform the effort estimation for the Software under Test (SUT).",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918730,,no,,,yes,,yes,yes,yes,yes
589,Comparing White-Box and Black-Box Test Prioritization,"Although white-box regression test prioritization has been well-studied, the more recently introduced black-box prioritization approaches have neither been compared against each other nor against more well-established white-box techniques. We present a comprehensive experimental comparison of several test prioritization techniques, including well-established white-box strategies and more recently introduced black-box approaches. We found that Combinatorial Interaction Testing and diversity-based techniques (Input Model Diversity and Input Test Set Diameter) perform best among the black-box approaches. Perhaps surprisingly, we found little difference between black-box and white-box performance (at most 4% fault detection rate difference). We also found the overlap between black- and white-box faults to be high: the first 10% of the prioritized test suites already agree on at least 60% of the faults found. These are positive findings for practicing regression testers who may not have source code available, thereby making white-box techniques inapplicable. We also found evidence that both black-box and white-box prioritization remain robust over multiple system releases.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886931,,yes,,,yes,yes,yes,yes,yes,yes
597,Dynamic Integration Test Selection Based on Test Case Dependencies,"Prioritization, selection and minimization of test cases are well-known problems in software testing. Test case prioritization deals with the problem of ordering an existing set of test cases, typically with respect to the estimated likelihood of detecting faults. Test case selection addresses the problem of selecting a subset of an existing set of test cases, typically by discarding test cases that do not add any value in improving the quality of the software under test. Most existing approaches for test case prioritization and selection suffer from one or several drawbacks. For example, they to a large extent utilize static analysis of code for that purpose, making them unfit for higher levels of testing such as integration testing. Moreover, they do not exploit the possibility of dynamically changing the prioritization or selection of test cases based on the execution results of prior test cases. Such dynamic analysis allows for discarding test cases that do not need to be executed and are thus redundant. This paper proposes a generic method for prioritization and selection of test cases in integration testing that addresses the above issues. We also present the results of an industrial case study where initial evidence suggests the potential usefulness of our approach in testing a safety-critical train control management subsystem.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528974,,yes,,,yes,no,yes,yes,yes,yes
598,Effect of Time Window on the Performance of Continuous Regression Testing,"Test prioritization is an effective technique used to reduce the amount of work required to support regression testing in continuous integration development. It aims at finding an optimal order of tests that can detect regressions faster, potentially increasing the frequency of software releases. Prioritization techniques based on test execution history use the results of preceding executions to determine an optimal order of regression tests in the succeeding test executions. In this paper, we investigate how can execution history be optimally used to increase the effectiveness of regression test prioritization. We analyze the effect of history time window on the fault detection effectiveness of prioritized regression tests. We report an experimental study using a data set from Cisco. The results suggest that varying the size of the window can considerably change the performance of regression testing. Our findings will potentially help developers and test teams in adjusting test prioritization techniques for achieving higher cost-effectiveness in continuous regression testing.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816510,,no,,,yes,,yes,yes,yes,yes
599,Effectiveness of prioritization of test cases based on Faults,"Regression testing (RT) is an expensive activity. It is applied on a modified program to enhance confidence and reliability by ensuring that the changes are accurately true and have not affected the unmodified portions of the SUT. Due to limited resources, it is not practical to re-run each test cases (TC). To improve the regression testing's effectiveness, the TCs should be arranged according to some objective function or criteria. Test case prioritization (TCP) arranges TCs in an order for execution that enhances their effectiveness by satisfying some testing goals. The highest priority assigned to TCs must execute before the TCs with low priority by virtue of some performance goal. Numerous goals are possible to achieve of which one such goal is rate of fault detection (RFT) in which the faults are surfaced as quickly as possible within the testing process. In this paper, a novel technique is suggested to prioritize the TCs that increase its effectiveness in detecting faults. The effectiveness of the proposed method is compared and matched with other prioritization approaches with the help of Average Percentage of Fault Detection (APFD) metric from which charts have been prepared.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507977,,yes,,,yes,yes,yes,yes,yes,yes
601,Enhanced Weighted Method for Test Case Prioritization in Regression Testing Using Unique Priority Value,"Regression testing is an integral and expensive part in software testing. To reduce its effort, test case prioritization approaches were proposed. The problem with most of the existing approaches is the random ranking of test cases with equal weight. In this paper, an enhanced weighted method to prioritize the full test suite without using random ranking is presented. In addition, a controlled experiment was executed to evaluate the effectiveness of the proposed method. The results show an improved performance in terms of prioritizing test cases and recording higher APFD values over the original weighted method. In future, a larger experiment would be executed to generalize the results.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885851,,,yes,,yes,no,yes,yes,yes,yes
602,Enhancing Test Case Prioritization in an Industrial Setting with Resource Awareness and Multi-objective Search,"Test case prioritization is an essential part of test execution systems for large organizations developing software systems in the context that their software versions are released very frequently. They must be tested on a variety of compatible hardware with different configurations to ensure correct functioning of a software version on a compatible hardware. In practice, test case execution must not only execute cost-effective test cases in an optimal order, but also optimally allocate required test resources, in order to deliver high quality software releases. To optimize the current test execution system for testing software releases developed for Videoconferencing Systems (VCSs) at Cisco, Norway, in this paper, we propose a resource- aware multi-objective optimization solution with a fitness function defined based on four cost-effectiveness measures. In this context, a set of software releases must be tested on a set of compatible VCS hardware (test resources) by executing a set of cost-effective test cases in an optimal order within a given test cycle constrained by maximum allowed time budget and maximum available test resources. We empirically evaluated seven search algorithms regarding their performance and scalability by comparing with the current practice (random ordering (RO)). The results show that the proposed solution with the best search algorithm (i.e., Random-Weighted Genetic Algorithm) improved the current practice by reducing on average 40.6% of time for test resource allocation and test case execution, improved test resource usage on average by 37.9% and fault detection on average by 60%.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883302,,,yes,,yes,,yes,yes,yes,yes
603,Experience Report: Automated System Level Regression Test Prioritization Using Multiple Factors,"We propose a new method of determining an effective ordering of regression test cases, and describe its implementation as an automated tool called SuiteBuilder developed by Westermo Research and Development AB. The tool generates an efficient order to run the cases in an existing test suite by using expected or observed test duration and combining priorities of multiple factors associated with test cases, including previous fault detection success, interval since last executed, and modifications to the code tested. The method and tool were developed to address problems in the traditional process of regression testing, such as lack of time to run a complete regression suite, failure to detect bugs in time, and tests that are repeatedly omitted. The tool has been integrated into the existing nightly test framework for Westermo software that runs on large-scale data communication systems. In experimental evaluation of the tool, we found significant improvement in regression testing results. The re-ordered test suites finish within the available time, the majority of fault-detecting test cases are located in the first third of the suite, no important test case is omitted, and the necessity for manual work on the suites is greatly reduced.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774503,,,yes,,yes,,yes,yes,yes,yes
608,History-Based Dynamic Test Case Prioritization for Requirement Properties in Regression Testing,"Regression testing is an important but extremely costly and time-consuming process. Because of limited resources in practice, test case prioritization focuses on the improvement of testing efficiency. However, traditional test case prioritization techniques emphasize only one-time testing without considering huge historical data generated in regression testing. This paper proposes an approach to prioritizing test cases based on historical data. Requirements are a significant factor in the testing process, the priorities of test cases are initialized based on requirement priorities in our history-based approach, and then are calculated dynamically according to historical data in regression testing. To evaluate our approach, an empirical study on an industrial system is conducted. Experimental results show an improved performance for our proposed method using measurements of Average Percentage of Faults Detected and Fault Detection Rate.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809434,,,yes,,yes,no,yes,yes,yes,yes
609,History-Based Test Case Prioritization for Failure Information,"From regression tests, developers seek to determine not only the existence of faults, but also failure information such as what test cases failed. Failure information can assist in identifying suspicious modules or functions in order to fix the detected faults. In continuous integration environments, this can also help managers of the source code repository address unexpected situations caused by regression faults. We introduce an approach, referred to as AFSAC, which is a test case prioritization technique based on history data, that can be used to effectively obtain failure information. Our approach is composed of two stages. First, we statistically analyze the failure history for each test case to order the test cases. Next, we reorder the test cases utilizing the correlation data of test cases acquired by previous test results. We performed an empirical study on two open-source Apache software projects (i.e., Tomcat and Camel) to evaluate our approach. The results of the empirical study show that our approach provides failure information to testers and developers more effectively than other prioritization techniques, and each prioritizing method of our approach improves the ability to obtain failure information.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890618,,,yes,,yes,,yes,yes,yes,yes
610,How Does Regression Test Prioritization Perform in Real-World Software Evolution?,"In recent years, researchers have intensively investigated various topics in test prioritization, which aims to re-order tests to increase the rate of fault detection during regression testing. While the main research focus in test prioritization is on proposing novel prioritization techniques and evaluating on more and larger subject systems, little effort has been put on investigating the threats to validity in existing work on test prioritization. One main threat to validity is that existing work mainly evaluates prioritization techniques based on simple artificial changes on the source code and tests. For example, the changes in the source code usually include only seeded program faults, whereas the test suite is usually not augmented at all. On the contrary, in real-world software development, software systems usually undergo various changes on the source code and test suite augmentation. Therefore, it is not clear whether the conclusions drawn by existing work in test prioritization from the artificial changes are still valid for real-world software evolution. In this paper, we present the first empirical study to investigate this important threat to validity in test prioritization. We reimplemented 24 variant techniques of both the traditional and time-aware test prioritization, and investigated the impacts of software evolution on those techniques based on the version history of 8 real-world Java programs from GitHub. The results show that for both traditional and time-aware test prioritization, test suite augmentation significantly hampers their effectiveness, whereas source code changes alone do not influence their effectiveness much.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886932,,,yes,,yes,,yes,yes,yes,yes
611,Improving test efficiency through multiple criteria coverage based test case prioritization using Modified heuristic algorithm,"Test case prioritization involves reordering the test cases in an order that helps in attaining certain performance goals. The rate of fault detection is one of the prime goals that we tend to achieve while doing prioritization. Test cases should run in an order to increase the possibility of fault detection and it should be achieved early during the test life cycle. To reduce the cost and time of regression testing, test case prioritization should be done with the intention of periodically modifying the test suite. The humongous set of test cases makes it redundant and cumbersome for the testers who ensure quality for an end application. The fault detection capability of a prioritized test suite is improved up to 15% using Modified PSO which forms the base algorithms for prioritization. The algorithm illustrated detects serious errors at earlier phases of testing process and effectiveness between prioritized and unprioritized test cases.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783254,,,yes,,yes,,yes,yes,yes,yes
612,Improving Testing in an Enterprise SOA with an Architecture-Based Approach,"High resource demand for system testing is a major obstacle for continuous delivery. This resource demand can be reduced by prioritizing test cases, e.g., by focusing on tests that cover a lot of functionality. For large-scale systems, like an enterprise SOA, defining such test cases can be difficult for the tester because of the lack of relevant knowledge about the system. We propose an approach for test case prioritization and selection that is based on architectural viewpoint that provides software testers with the required architectural information. We outline how architectural information is used for defining and selecting prioritized test cases. The approach has been developed in close cooperation with the provider of an enterprise SOA in the banking domain in Austria following an action research approach. In addition, the approach has been validated in an industrial case study. Validation showed that there is no further need for manual architectural analysis to be able to prioritize and select test cases. We also show the limitations of our approach as it is based on static code analysis.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516833,,,no,,yes,,no,yes,yes,yes
613,Improvising the effectiveness of test suites using differential evolution technique,"The process of testing any software system is an atrocious task which indeed consumes a ton of effort, and expensive also. Required effort and time to do adequate as well as effective testing get bigger, as the software gets more complexed that can lead to swarm over the project budget or some test cases left uncovered or delay in completion. A suitably generated test suite does not only locate errors but also aid in reducing cost investment associated with the testing process. This paper implements an optimizing technique called as Differential Evolution to improve the effectiveness of test cases using Average Percentage of Fault Detection (APFD) metric. APFD is taken as the fitness function which is to be optimized. In this work, We have performed comparison of our approach with other existing prioritizing approaches and Experimental computations show that Differential Evolution technique achieve better APFD values than other techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7784924,,,no,,yes,no,yes,yes,yes,yes
619,Model-based test case prioritization using ACO: A review,"Regression testing is very costly and inevitable activity of maintenance that is performed to ensure whether the modified software is valid or not. Running all the test cases of a test suit within given limited time and cost constraints is not possible. So, to cover the maximum number of faults in comparatively less time, it is necessary to prioritize the test cases. To solve the time constraint test case prioritization problems Ant Colony optimization (ACO) is a better approach. This paper presents a review on test case prioritization from a given test suite using ACO.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7913140,,,yes,,yes,,yes,yes,yes,yes
620,Modified ACO to maintain diversity in regression test optimization,"Regression testing is unavoidable maintenance activity that is performed several times in software development life cycle. Optimization of regression test case is required to minimize the test case (which will in-turn reduce the time and cost of testing) and to find the fault in early testing activity. The two widely used regression test case optimization techniques, namely, selection and prioritization are recently found to be integrated with different metaheuristic algorithms for fruitful regression test cases. Among the various meta-heuristic algorithms, Ant colony optimization (ACO) algorithm is most popularly used. ACO will try to find the smallest path out all the test cases and it is not sufficient because it will not cover all the test cases which are needed. In this paper we have proposed a modified ant colony optimization to solve test cases in huge search space. The modified algorithm selects the best test cases that find the maximum fault in minimum time.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507970,,,yes,,yes,,yes,yes,yes,yes
627,Prioritization techniques in combinatorial testing: A survey,"Prioritization techniques have become an indispensable part of the software testing process. They are highly beneficial either due to resource or time constraints or when tester cannot execute the complete test set. Combinatorial test sets are generated with the aim to cover all the possible t-way interactions. In this paper, techniques to prioritize t-way test sets are studied. A comparative study is done of prioritization criteria used by researchers to prioritize t-way test sets. Different evaluation methods used by them are also discussed.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975314,,,no,,yes,,yes,yes,yes,yes
628,Prioritizing Interaction Test Suites Using Repeated Base Choice Coverage,"Combinatorial interaction testing is a well-studied testing strategy that aims at constructing an effective interaction test suite (ITS) of a specific generation strength to identify interaction faults caused by the interactions among factors. Due to limited testing resources in practice, for example in combinatorial interaction regression testing, interaction test suite prioritization (ITSP) has been proposed to improve the efficiency of testing. An intuitive ITSP strategy that has been widely used in practice is fixed-strength interaction coverage based prioritization (FICBP). FICBP makes use of a property of the ITS: interaction coverage at a fixed prioritization strength. However, a challenge facing FICBP is that, when the ITS is large, the prioritization cost can be very high. In this paper, we propose a new FICBP method that, by repeatedly using base choice coverage (i.e., one-wise coverage) during the prioritization process, improves testing efficiency while maintaining testing effectiveness. The empirical studies show that our method has fault detection capability comparable to current FICBP methods, but obtains more stable results in many cases. Additionally, our method requires considerably less prioritization time than other FICBP methods at different prioritization strengths.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552005,,,no,,yes,yes,yes,yes,yes,yes
633,Supporting the regression test of multi-variant systems in distributed production scenarios,"Modern manufacturing systems based on cyber-physical systems with a growing amount of software allow frequent updates and reconfigurations to adapt the systems to volatile usage scenarios in the production. A diverse system environment arises even for similar or equal subsystems based on the same platform used at different locations. A major challenge for such systems is the regression test after changes or updates. The resources for the regression test, in a dedicated test environment or deployed to the assembly lines, are limited. To plan the test in the best possible way, a lot of dependencies, relationships and experiences from former tests and tests from other locations have to be considered. This paper describes an assistance system which supports the planning of the regression test in such distributed manufacturing scenarios by combining manual modeling with automated data processing. Therefore the system calculates a cross-location test progress and suggests a prioritized test case sequence.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733652,,,no,,no,,no,yes,yes,yes
634,System-Level Test Case Prioritization Using Machine Learning,"Regression testing is the common task of retesting software that has been changed or extended (e.g., by new features) during software evolution. As retesting the whole program is not feasible with reasonable time and cost, usually only a subset of all test cases is executed for regression testing, e.g., by executing test cases according to test case prioritization. Although a vast amount of methods for test case prioritization exist, they mostly require access to source code (i.e., white-box). However, in industrial practice, system-level testing is an important task that usually grants no access to source code (i.e., black-box). Hence, for an effective regression testing process, other information has to be employed. In this paper, we introduce a novel technique for test case prioritization for manual system-level regression testing based on supervised machine learning. Our approach considers black-box meta-data, such as test case history, as well as natural language test case descriptions for prioritization. We use the machine learning algorithm SVM Rank to evaluate our approach by means of two subject systems and measure the prioritization quality. Our results imply that our technique improves the failure detection rate significantly compared to a random order. In addition, we are able to outperform a test case order given by a test expert. Moreover, using natural language descriptions improves the failure finding rate.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838169,,,yes,,yes,,yes,yes,yes,yes
638,Test case prioritization based on requirement correlations,"Test case prioritization technique aims to improve test efficiency rate by sorting test cases according to some specific criteria. Requirements play an important role throughout software testing. This paper proposes a test case prioritization method based on requirement correlations. Prioritization of requirements is defined by the users and the developers. This technique focuses on requirements with detected faults after the last regression testing. By readjusting prioritization of fault-related requirements, it can optimize the order of test cases. Experimental results show that this technique exactly contributes to achieving high testing efficiency.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515934,,,yes,,yes,,yes,yes,yes,yes
639,Test Case Prioritization for Compilers: A Text-Vector Based Approach,"Test case prioritization aims to schedule the execution order of test cases so as to detect bugs as early as possible. For compiler testing, the demand for both effectiveness and efficiency imposes challenge to test case prioritization. In the literature, most existing approaches prioritize test cases by using some coverage information (e.g., statement coverage or branch coverage), which is collected with considerable extra effort. Although input-based test case prioritization relies only on test inputs, it can hardly be applied when test inputs are programs. In this paper we propose a novel text-vector based test case prioritization approach, which prioritizes test cases for C compilers without coverage information. Our approach first transforms each test case into a text-vector by extracting its tokens which reflect fault-relevant characteristics and then prioritizes test cases based on these text-vectors. In particular, in our approach we present three prioritization strategies: greedy strategy, adaptive random strategy, and search strategy. To investigate the efficiency and effectiveness of our approach, we conduct an experiment on two C compilers (i.e., GCC and LLVM), and find that our approach is much more efficient than the existing approaches and is effective in prioritizing test cases.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515478,,,yes,,yes,,yes,yes,yes,yes
640,Test case prioritization technique based on early fault detection using fuzzy logic,"Regression testing is time consuming and expensive activity in software testing. In Regression testing when any changes made to already tested program it should not affect to other part of program. Regression testing is crucial activities in software testing and maintenance phases. If some part of code is altered then it is mandatory to validate the modified code. Throughout regression testing test case from test suite will be re-executed and re-execution of all the test case will be very expensive. In this paper we present regression test case prioritization for object oriented program. The most important research is how to select efficient and suitable test cases during regression testing from the test suite. To minimize the regression testing cost we have applied prioritization technique. In this paper prioritization is done based on fault detection rate of program, execution time and requirement coverage using fuzzy logic.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724418,,,yes,,yes,,yes,yes,yes,yes
641,Test case prioritization techniques for software product line: A survey,"Software product line (SPL) testing is a tougher work than testing of single systems. Still testing of each individual SPL product would be perfect but it is too costly in practice. In fact, when the number of features increases then the number of possible products also increases exponentially usually derived from a feature model. Number of features is leading to thousands of different products. Due to cost and time constraints, it is infeasible or large number of effort to run all the test cases in an existing test suite. To decrease the cost of testing, various techniques have been proposed. One of them is test case prioritization (TCP) techniques. Here we presented a survey for TCP techniques for software SPL.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813841,,,yes,,yes,,yes,yes,yes,yes
642,Test Case Prioritization Using Adaptive Random Sequence with Category-Partition-Based Distance,"Test case prioritization schedules test cases in a certain order aiming to improve the effectiveness of regression testing. Random sequence is a basic and simple prioritization technique, while Adaptive Random Sequence (ARS) makes use of extra information to improve the diversity of random sequence. Some researchers have proposed prioritization techniques using ARS with white-box information, such as code coverage information, or with black-box information, such as string distances of the input data. In this paper, we propose new black-box test case prioritization techniques using ARS, and the diversity of test cases is assessed by category-partition-based distance. Our experimental studies show that these new techniques deliver higher fault-detection effectiveness than random prioritization, especially in the case of smaller ratio of failed test cases. In addition, in the comparison of different distance metrics, techniques with category-partition-based distance generally deliver better fault-detection effectiveness and efficiency, meanwhile in the comparison of different ordering algorithms, our ARS-based ordering algorithms usually have comparable fault-detection effectiveness but much lower computation overhead, and thus are much more cost-effective.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589817,,,yes,,yes,,yes,yes,yes,yes
643,Test Case Prioritization Using Lexicographical Ordering,"Test case prioritization aims at ordering test cases to increase the rate of fault detection, which quantifies how fast faults are detected during the testing phase. A common approach for test case prioritization is to use the information of previously executed test cases, such as coverage information, resulting in an iterative (greedy) prioritization algorithm. Current research in this area validates the fact that using coverage information can improve the rate of fault detection in prioritization algorithms. The performance of such iterative prioritization schemes degrade as the number of ties encountered in prioritization steps increases. In this paper, using the notion of lexicographical ordering, we propose a new heuristic for breaking ties in coverage based techniques. Performance of the proposed technique in terms of the rate of fault detection is empirically evaluated using a wide range of programs. Results indicate that the proposed technique can resolve ties and in turn noticeably increases the rate of fault detection.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456343,,,yes,,yes,,yes,yes,yes,yes
644,Test Effectiveness Evaluation of Prioritized Combinatorial Testing: A Case Study,"Combinatorial testing is a widely-used technique to detect system interaction failures. To improve test effectiveness with given priority weights of parameter values in a system under test, prioritized combinatorial testing constructs test suites where highly weighted parameter values appear earlier or more frequently. Such order-focused and frequency-focused combinatorial test generation algorithms have been evaluated using metrics called weight coverage and KL divergence but not sufficiently with fault detection effectiveness so far. We evaluate the fault detection effectiveness on a collection of open source utilities, applying prioritized combinatorial test generation and investigating its correlation with weight coverage and KL divergence.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589785,,,no,,yes,,yes,yes,yes,yes
646,Test-Suite Prioritisation by Application Navigation Tree Mining,"Software tend to evolve over time and so does the test-suite. Regression testing is aimed at assessing that the software evolution did not compromise the working of the existing software components. However, as the software and consequently the test-suite grow in size, the execution of the entire test-suite for each new build becomes infeasible. Techniques like test-suite selection, test-suite minimisation and test-suite prioritisation have been proposed in literature for regression testing. Whilst all of these techniques are essentially an attempt to reduce the testing effort, test-suite selection and minimisation reduce the test-suite size whereas test-suite prioritisation provides a priority order of the test cases without changing the test-suite size. In this work, we focus on test-suite prioritisation. Recently, techniques from data mining have been used for test-suite prioritisation which consider the frequent pairs of interaction among the application interaction patterns. We propose test-Suite prioritisation by Application Navigation Tree mining (t-SANT). First, we construct an application navigation tree by way of extracting both tester and user interaction patterns. Next, we extract frequent sequences of interaction using a sequence mining algorithm inspired from sequential pattern mining. The most frequent longest sequences are assumed to model complex and most frequently used work-flows and hence a prioritisation algorithm is proposed that prioritises the test cases based on the most frequent and longest sequences. We show the usefulness of the proposed scheme with the help of two case studies, an online book store and calculator.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7866754,,,no,,yes,yes,yes,yes,yes,yes
647,The drawbacks of statement code coverage test case prioritization related to domain testing,"In this paper we study the weaknesses of the test case prioritization algorithms based on statement code coverage and applied to the domain test cases. We present the inconsistency between the principles of domain testing and the selection and prioritization practices over domain test cases on criteria unrelated to the scope of the domain testing. We continue the study by discussing the impact of 100% statement code coverage over the suites of domain test cases, studying why this type of code coverage should not produce effects over the domain test cases. Statement code coverage prioritization techniques related to unit testing, integration testing and regression testing phases are discussed, emphasizing the incompatibility between statement code coverage, domain testing and test case prioritization all at once.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507373,,,yes,,yes,yes,yes,yes,yes,yes
649,To Be Optimal or Not in Test-Case Prioritization,"Software testing aims to assure the quality of software under test. To improve the efficiency of software testing, especially regression testing, test-case prioritization is proposed to schedule the execution order of test cases in software testing. Among various test-case prioritization techniques, the simple additional coverage-based technique, which is a greedy strategy, achieves surprisingly competitive empirical results. To investigate how much difference there is between the order produced by the additional technique and the optimal order in terms of coverage, we conduct a study on various empirical properties of optimal coverage-based test-case prioritization. To enable us to achieve the optimal order in acceptable time for our object programs, we formulate optimal coverage-based test-case prioritization as an integer linear programming (ILP) problem. Then we conduct an empirical study for comparing the optimal technique with the simple additional coverage-based technique. From this empirical study, the optimal technique can only slightly outperform the additional coverage-based technique with no statistically significant difference in terms of coverage, and the latter significantly outperforms the former in terms of either fault detection or execution time. As the optimal technique schedules the execution order of test cases based on their structural coverage rather than detected faults, we further implement the ideal optimal test-case prioritization technique, which schedules the execution order of test cases based on their detected faults. Taking this ideal technique as the upper bound of test-case prioritization, we conduct another empirical study for comparing the optimal technique and the simple additional technique with this ideal technique. From this empirical study, both the optimal technique and the additional technique significantly outperform the ideal technique in terms of coverage, but the latter significantly outperforms the former two techniques in terms of fault detection. Our findings indicate that researchers may need take cautions in pursuing the optimal techniques in test-case prioritization with intermediate goals.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314957,,,yes,,yes,,yes,yes,yes,yes
657,A Fault Based Approach to Test Case Prioritization,"Regression testing is performed to ensure that the no new faults have been introduced in the software after modification and the software continues to work correctly. Regression testing is an expensive process because the test suite might be too large to execute in full. Thus to reduce the cost of such testing, regression testing techniques are used. One such technique is test case prioritization. Software testers assign priority to each test case to make sure that the test cases with higher priorities are executed first, in case of not having enough resources to execute the whole test suite. Test case prioritization is mainly used to increase fault detection rate of test suite which is the measure of how early faults are detected. In this paper, we propose an approach which exploits mutation testing in order to assign priorities to test cases. Using mutation testing, we introduce different faults in original program thus creating a number of mutated copies of the program and test case that exposes maximum number of these faults is given the highest priority. We report the outcomes of our experiments in which we applied our technique to test suites and calculated the fault detection rates produced by the prioritized test suites, comparing those rates of fault detection to the rates achieved by existing prioritization technique. The resulting data shows that prioritization technique proposed improved the fault detection rate of test suites.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8261011,,,yes,,yes,,yes,yes,yes,yes
661,A novel approach to multiple criteria based test case prioritization,"When software is modified, it is retested to ensure that no new faults have been introduced in the previously tested code and it still works correctly. Such testing is known as regression testing. The cost of regression testing is high because the original program has large number of test cases. It is not feasible to execute all test cases for regression testing. Test suite minimization, test case selection and test case prioritization are cost commonly used techniques in regression testing to reduce the cost of regression testing. While test suite minimization and test case selection techniques select a subset of test cases, test case prioritization does not eliminate any test case, it only orders the test cases with the objective of increasing the fault detection rate. Prioritization is usually preferred over other two approaches because it does not involve the risk of losing useful test cases. Prioritization techniques assign priority to each test case on the basis of some coverage criteria. A number of different single criterion and multiple criteria based prioritization techniques have been proposed in the literature. Multiple criteria based prioritization techniques perform better than single criterion based prioritization techniques. The existing multiple criteria based prioritization techniques combine the criteria in such a way that __??Additional__?_ strategy cannot be applied on them. In this paper, we propose a new multiple criteria based test case prioritization algorithm that considers two criteria to prioritize test cases using __??Additional__?_ strategy. One criterion is considered as primary and other is considered as secondary. Primary criterion is used to prioritize the test cases whereas secondary criterion is used to break the tie among test cases when two or more test cases provide equal coverage of entities of first criterion. Our proposed multiple criteria based prioritization algorithm performs better than the existing prioritization techniques.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8281742,,,yes,,yes,no,yes,yes,yes,yes
663,A Regression Test Case Prioritization Algorithm Based on Program Changes and Method Invocation Relationship,"Regression testing is essential for assuring the quality of a software product. Because rerunning all test cases in regression testing may be impractical under limited resources, test case prioritization is a feasible solution to optimize regression testing by reordering test cases for the current testing version. In this paper, we propose a new test case prioritization algorithm based on program changes and method (function) invocation relationship. Combining the estimated risk value of each program method (function) and the method (function) coverage information, the fault detection capability of each test case can be calculated. The algorithm reduces the prioritization problem to an integer linear programming (ILP) problem, and finally prioritizes test cases according to their fault detection capabilities. Experiments are conducted on 11 programs to validate the effectiveness of our proposed algorithm. Experimental results show that our approach is more effective than some well studied test case prioritization techniques in terms of average percentage of fault detected (APFD) values.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305939,,,yes,,yes,,yes,yes,yes,yes
664,A survey on prioritization regression testing test case,"Regression testing is a process used to measure the validity of the system during software maintenance. Regression testing process is very expensive and must be introduced each time a modification occurs in software to ensure that the system still work and that the new modification doesn't cause any bugs, this process depends on selecting test cases from a test suite. Selection of test cases is very critical since it affect the regression testing time and effort, so that many algorithms exist to enhance regression testing process. One of the methods used to make enhancements is to select test cases using prioritization testing techniques. Prioritization techniques find the bugs early to improve regression testing efficiency by prioritizing the test cases. In this paper many regression testing prioritization techniques were reviewed and analyzed.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8079958,,,yes,,yes,,yes,yes,yes,yes
666,Adapting code maintainability to bat-inspired test case prioritization,"Time and budget constraints in developing a software create an adverse effect in terms of the adequacy of maintenance and test processes. This case can be considered as a burden for persons who account for test processes. In order to alleviate this burden, test case prioritization is one of the solutions. A nature-inspired method namely BITCP, which was developed based on bat algorithm, produced promising results. However, this method does not involve test case elements with respect to the code maintainability. In this work, the correlation between some code maintainability indicators including WMC, LCOM, and Coupling and cyclomatic complexity is investigated. IMPBITCP appears after adapting the results of the investigation to BITCP. The method is then compared with well known alternatives such as greedy-search, particle swarm optimization, and BITCP. The experiment involving six open source project showed that IMPCBITCP outperformed the others with respect to the APFD. The findings of the work indicates that if the factors affecting code maintenance are considered while developing test case prioritization techniques, APFD results becomes high and stable.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001134,,,no,,yes,,yes,yes,yes,yes
668,An Empirical Comparison of Similarity Measures for Abstract Test Case Prioritization,"Test case prioritization (TCP) attempts to order test cases such that those which are more important, according to some criterion or measurement, are executed earlier. TCP has been applied in many testing situations, including, for example, regression testing. An abstract test case (also called a model input) is an important type of test case, and has been widely used in practice, such as in configurable systems and software product lines. Similarity-based test case prioritization (STCP) has been proven to be cost-effective for abstract test cases (ATCs), but because there are many similarity measures which could be used to evaluate ATCs and to support STCP, we face the following question: How can we choose the similarity measure(s) for prioritizing ATCs that will deliver the most effective results? To address this, we studied fourteen measures and two popular STCP algorithms - local STCP (LSTCP), and global STCP (GSTCP). We also conducted an empirical study of five realworld programs, and investigated the efficacy of each similarity measure, according to the interaction coverage rate and fault detection rate. The results of these studies show that GSTCP outperforms LSTCP - in 61% to 84% of the cases, in terms of interaction coverage rates; and in 76% to 78% of the cases with respect to fault detection rates. Our studies also show that Overlap, the simplest similarity measure examined in this study, could obtain the overall best performance for LSTCP; and that Goodall3 has the best performance for GSTCP.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029584,,,yes,,yes,,yes,yes,yes,yes
669,An Empirical Examination of Abstract Test Case Prioritization Techniques,"Abstract test case prioritization (ATCP) aims at ordering abstract test case in order to increase the speed at which faults are detected, potentially increasing the fault detection rate. This paper empirically examines possible ATCP techniques, according to the following four categories: non-information-guided prioritization (NIGP), interaction coverage based prioritization (ICBP), input-model mutation based prioritization (IMBP), and similarity based prioritization (SBP). We found that the ICBP category has better testing effectiveness than others, according to fault detection rates. Surprisingly, we found that NIGP can achieve similar performance to IMBP, and that SBP can sometimes achieve even better rates of fault detection than some ICBP techniques.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965282,,,yes,,yes,,yes,yes,yes,yes
670,An empirical study on clustering approach combining fault prediction for test case prioritization,"Using Clustering algorithm to improve the effectiveness of test case prioritization has been well recognized by many researchers. Software fault prediction has been one of the active parts of software engineering, but to date, there are few test cases prioritization technique using fault prediction. We conjecture that if the code has a fault-proneness, the test cases covering the code will find fault with higher probability. In addition, most of the existing test cases prioritization techniques using clustering algorithm don't consider the number of clusters. Thus, in this paper, we design a test case prioritization based on clustering approach combining fault prediction. We consider the method to obtain the best number of clusters and the clustering prioritization based on the results of fault prediction. To investigate the effectiveness of our approach, we perform an empirical study using an object which contains test cases and faults. The experiment results indicate that our techniques can improve the effectiveness of test case prioritization.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960104,,,yes,,yes,,yes,yes,yes,yes
671,An empirical study on clustering approach combining fault prediction for test case prioritization,"Using Clustering algorithm to improve the effectiveness of test case prioritization has been well recognized by many researchers. Software fault prediction has been one of the active parts of software engineering, but to date, there are few test cases prioritization technique using fault prediction. We conjecture that if the code has a fault-proneness, the test cases covering the code will findfault with higher probability. In addition, most of the existing test cases prioritization techniques using clustering algorithm don't consider the number of clusters. Thus, in this paper, we design a test case prioritization based on clustering approach combining fault prediction. We consider the method to obtain the best number of clusters and the clustering prioritization based on the results of fault prediction. To investigate the effectiveness of our approach, we perform an empirical study using an object which contains test cases and faults. The experiment results indicate that our techniques can improve the effectiveness of test case prioritization.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960105,,,yes,,yes,,yes,yes,yes,yes
675,An Industrial Study of Natural Language Processing Based Test Case Prioritization,"In mobile application development, the frequentsoftware release limits the testing time resource. In order todetect bugs in early phases, researchers proposed various testcase prioritization (TCP) techniques in past decades. In practice, considering that some test case is described or contains text, theresearchers also employed Natural Language Processing (NLP)to assist the TCP techniques. This paper conducted an extensiveempirical study to analyze the performance of three NLP basedTCP technologies, which is based on 15059 test cases from 30industrial projects. The result shows that all of these threestrategies can help to improve the efficiency of software testing, and the Risk strategy achieved the best performance across thesubject programs.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7928016,,,no,,yes,,yes,yes,yes,yes
676,Automated System-Level Regression Test Prioritization in a Nutshell,"Westermo Research and Development has developed SuiteBuilder, an automated tool to determine an effective ordering of regression test cases. The ordering is based on factors such as fault detection success, the interval since the last execution, and code modifications. SuiteBuilder has enabled Westermo to overcome numerous regression-testing problems, including lack of time to run a complete regression suite, failure to detect bugs in a timely manner, and repeatedly omitted tests. In the tool's first two years of use, reordered test suites finished in the available time, most fault-detecting test cases were located in the first third of suites, no important test case was omitted, and the necessity for manual work on the suites decreased greatly.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7974685,,,yes,,yes,yes,yes,yes,yes,yes
680,Classification model for test case prioritization techniques,"Regression Testing is mainly done in software maintenance aiming to assure that the changes made in the software have correctly been implemented and also to achieve the confidence that the modifications have not affected the other parts of the software. It is very costly and expensive technique. There are number of techniques present in literature that focus on achieving various testing objectives early in the process and hence reduces its cost. Despite of that, testers usually prefer only few already known techniques for test case prioritization. The main reason behind is the absence of guidelines for the selection of TCP techniques. Hence, this piece of research introduces a novel approach for classification of TCP techniques using fuzzy logic to support the efficient selection of test case prioritization techniques. This work is an extension of already proposed selection schema for test case prioritization techniques. To perform the validation of proposed approach results are compared with other classification techniques using Weka tool. The analysis clearly shows the effectiveness of proposed approach as compared to others in terms of its accuracy.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8229925,,,yes,,yes,,yes,yes,yes,yes
682,Comparison analysis of two test case prioritization approaches with the core idea of adaptive,"Test case prioritization problem (TCP) has been widely discussed. It aims to controlling the test case execution sequence to improve the effectiveness of software testing. The key issue of TCP is to identify which test cases can provide useful information for failure detection and fault localization. So far, many TCP approaches have been proposed. Among them, Adaptive Random Testing (ART) and Dynamic Random Testing (DRT) are two of the most popular approaches to solve TCP with a basic idea borrowed from Cybernetics: adaptive. Both ART and DRT has been widely explored and observed with good performances in experimental studies. Nevertheless, although they are proposed by two related research groups, they are developed independently and in parallel. In fact, their mechanisms have many similarities and differences and, for the completeness of the domains of Adaptive Testing and Software Cybernetics, many issues concerning the comparison between these two approaches should be further explored. In this paper, we specifically explores the relationship between these two adaptive TCP approaches. Their mechanisms are described respectively with explorations of their distinctions, similarities, and respective characteristics. Moreover, based on these explorations, we analyse their advantages from the aspects of failure detection and fault understanding. During the analysis, a symbolic-graphic combination method is applied. Finally simulation based on real-life programs is conducted to observe our analysis. Our comparison analysis can support the selection of a proper testing approach according to various practical environments with different targets. Furthermore, the clarification of the two easily confused concepts is also a complement for the framework of Adaptive Testing and Software Cybernetics.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7978795,,,yes,,yes,,yes,yes,yes,yes
683,Cost aware test suite reduction algorithm for regression testing,"Regression testing is the process that a recent code change has not adversely affect the existing features. The re-running of all the test cases during regression testing is very expensive as it requires huge time and resources. Test case prioritization techniques are to schedule the test cases in accordance with some criteria such that important test cases are executed with that given period. This study presents test case prioritization using genetic algorithm and their effectiveness is measured using APFD. Then the prioritized test cases are reduced. Test suite reduction techniques aim at identifying and eliminating redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of the software testing activity. Our aim is to reduce the cost by reducing the number of test suite after prioritization. MFTS algorithm is used to reduce the given test suite with maximum coverage and it improves the rate of fault detection effectiveness.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300829,,,yes,,yes,,yes,yes,yes,yes
684,Cost-Effective Regression Testing Using Bloom Filters in Continuous Integration Development Environments,"Regression testing in continuous integration development environments must be cost-effective and should provide fast feedback on test suite failures to the developers. In order to provide faster feedback on failures to developers while using computing resources efficiently, two types of regression testing techniques have been developed: Regression Testing Selection (RTS) and Test Case Prioritization (TCP). One of the factors that reduces the effectiveness of the RTS and TCP techniques is the inclusion of test suites that fail only once over a period. We propose an approach based on Bloom filtering to exclude such test suites during the RTS process, and to assign such test suites with a lower priority during the TCP process. We experimentally evaluate our approach using a Google dataset, and demonstrate that cost-effectiveness of the proposed RTS and TCP techniques outperforms the state-of-the-art techniques.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305938,,,no,,yes,,yes,yes,yes,yes
686,d(mu)Reg: A Path-Aware Mutation Analysis Guided Approach to Regression Testing,"Regression testing re-runs some previously executed test cases, with the purpose of checking whether previously fixed faults have re-emerged and ensuring that the changes do not negatively affect the existing behaviors of the software under development. Today's software is rapidly developed and evolved, and thus it is critical to implement regression testing quickly and effectively. In this paper, we propose a novel technique for regression testing, based on a family of mutant selection strategies. The preliminary results show that the proposed technique can significantly improve the efficiency of different regression testing activities, including test case reduction and prioritization. Our work also makes it possible to develop a unified framework that effectively implements various activities in regression testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962333,,,no,,yes,no,no,yes,yes,yes
692,Efficient Product-Line Testing Using Cluster-Based Product Prioritization,"A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962326,,,no,,yes,,yes,yes,yes,yes
694,Epistasis Based ACO for Regression Test Case Prioritization,"Metaheuristics that are inspired by natural systems have been widely applied into search-based software engineering. It has been shown that combining knowledge of the application domain with a biological theory for metaheuristics can narrow down the search space and speed up the convergence for metaheuristics based algorithms. This paper introduces Epistatic Test case Segment (ETS) for multiobjective search-based regression Test Case Prioritization (MoTCP), based on epistasis theory that reflects the correlation between genes in evolution process. An ETS-based pheromone update strategy for ant colony optimization (ACO) algorithm is proposed. The experiments with three benchmarks and a real industrial program V8 illustrate that proposed pheromone update strategy guided by the epistasis theory can significant improve the performance of ACO in terms of effectiveness and efficiency for search-based MoTCP.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7935487,,,no,,yes,,yes,yes,yes,yes
702,Impact of Static and Dynamic Coverage on Test-Case Prioritization: An Empirical Study,"Most of existing research in Test-Case Prioritization uses coverage information as the input during the process of prioritization and these coverage can be classified into two categories: static coverage and dynamic coverage. As these coverage information are collected in different ways, they have different influence on test-case prioritization. In this work, we present the first empirical study comparing the impact of static coverage and dynamic coverage with five typical techniques at different test-case granularities (e.g., test-method and test-class level) and different coverage criteria (e.g., method and statement coverage). This study is performed on 15 real-world Java projects (using 163 versions) and we find that the dynamic coverage performs better than static coverage in terms of the results of test-case prioritization.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899091,,,yes,,yes,,yes,yes,yes,yes
703,Improving test case prioritization based on practical priority factors,"Test case prioritization involves prioritized the test cases for regression testing which improve the effectiveness of the testing process. By improving test case scheduling we can optimize time and cost as well as can produce better tested products. There are a number of methods to do prioritized test cases but not that effective or practical for the real-life large commercial systems. Most of the technique deals with finding defects or covering more test cases. In this paper, we will extend the previous work to incorporate real life practical aspects to schedule test cases. This will cover most of the businesses functionally based on the practical aspects. This approach covers more business area and ensure more defects. By prioritized test cases with this technique we will cover most important business functionally with less number of test cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8343055,,,yes,,yes,,yes,yes,yes,yes
707,Learning to Prioritize Test Programs for Compiler Testing,"Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985706,,,no,,yes,,yes,yes,yes,yes
717,Requirement dependencies-based formal approach for test case prioritization in regression testing,"Regression testing is the testing activity performed after changes occurred on software. Its aim is to increase confidence that achieved software adjustments have no negative impact on the already functional parts of the software. Test case prioritization is one technique that could be applied in regression testing with the aim to find faults early, resulting in reduced cost and shorten time of testing activities. Thus, prioritizing in the context of regression testing means to re-order test cases such that high priority ones are run first. The current paper addresses the test case prioritization as a consistent part of a larger approach on regression testing, which combines both test case prioritization and test case selection in order to overcome the limitations of each of them. A comprehensive formalization of test case prioritization is provided, incorporating beside the well known ingredients (test case, test requirement, fault, cost) also elements relating to the functional requirements and dependencies between requirements. An evolutionary algorithm is used to construct the re-ordering of test cases, considering as optimization objectives fault detection and cost. A synthetic case study was used to empirically prove our perspective for test case prioritization approach.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117002,,,yes,,yes,,yes,yes,yes,yes
722,Test Case Generation and Prioritization: A Process-Mining Approach,"Test cases are an essential tool in software quality assurance: they ensure that code behaves as specified in the requirement. However, writing test cases does not have only benefits, it comes with a cost: the programmer has to formulate the test cases and maintain them when the tested source code changes. Particularly for start-ups or small enterprises such costs become prohibitive, which often prefer to invest their time into the development of new functionalities instead of testing. This paper explores the use of process-mining as an approach to create a model of how users interact with a system to a) generate test cases and b) prioritize them. Using process-mining, it is possible to mine from the user behaviour which parts of the system are the most used, in which order they are executed, generate test cases repeating user input, and prioritizing test cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899028,,,yes,,yes,,yes,yes,yes,yes
724,Test Prioritization with Optimally Balanced Configuration Coverage,"Testing configurable software for high assurancesystems developed in continuous integration requires effectivetechniques for selecting failure-inducing test cases, thoroughlycovering entire configuration space, while providing rapid feedbackon failures. This involves satisfying multiple objectives:maximizing test fault detection, maximizing test coverage ofthe configuration space, and minimizing test execution time, which often leads to compromises in practice. In this paper, weaddress this problem with a practical test optimization approachthat uses historical test data to determine an optimal order oftests ensuring high progressively uniform configuration coverage, early fault detection, and rapid test feedback. We extensivelyvalidate the approach in a set of experiments using industry testsuites, and report experimental results showing the improvementin efficiency compared to industry practice. In particular, theapproach showed to increase the uniformity of configurationcoverage by 39% on average, which increases fault detectionup to 15%, while just slightly delaying test feedback.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911878,,,yes,,yes,yes,yes,yes,yes,yes
725,Test Suite Prioritization for Efficient Regression Testing of Model-Based Automotive Software,"Up to 80% of the automotive software can be generated from models. MATLAB Simulink is a common tool for creation of complex combinations of block diagrams and state machines, automated generation of executable code, and its deployment on a target ECU. The automotive safety standards require extensive testing of the developed models. Regression testing should be undertaken every time a model is updated to ensure that the modifications do not introduce new faults into the previously validated model. A common, time-consuming way is to rerun an entire test suite after even minor changes. This paper introduces a new method for automatic prioritization of test cases. The method is based on two principles: (i) A test case should stimulate an error in an updated block and (ii) the stimulated error should propagate to the place where it can be detected. The proposed method includes the evaluation of input vectors that are provided to updated blocks by each test case and a Markov-based stochastic error propagation analysis of the model. The application of the method is demonstrated with a Simulink model of a gearbox and a test-suite, automatically generated with the Reactis Tester.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8118517,,,yes,,yes,,yes,yes,yes,yes
734,Weighting for Combinatorial Testing by Bayesian Inference,"Combinatorial testing (CT) is a widely-used technique to detect system interaction failures. To improve the test effectiveness of CT, prioritized combinatorial testing inputs priority weights of parameter-values, and generates combinatorial test suites based on the weights. This paper proposes a method to automatically determine the weights of parameter-values by Bayesian inference using previous testing results. Using two open source projects, we evaluate the fault detection effectiveness of the proposed weighting based prioritized combinatorial testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899090,,,yes,,no,no,yes,yes,yes,yes
742,Clustering Based Prioritization of Test Cases,"Regression testing is the procedure of retesting the product and checking whether additional faults or errors have been created in the existing one. It is vital for keeping up programming quality. But it is a costly process. By., utilizing prioritization technique cost can be diminished. Prioritization increases productiveness of regression testing and its main criteria is to build the rate of error detection. Merging requirements information into current testing practice helps the engineers to recognize the source of faults easily. In this paper a research is done on whether the requirements-based grouping methodology can enhance the viability of prioritization techniques. So., here a grouping approach is performed on given requirements and prioritization techniques based on code scope metric.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8473253,,,yes,,yes,,yes,yes,yes,yes
747,Dynamic Random Testing Strategy for Test Case Optimization in Cloud Environment,"Dynamic Random Testing (DRT) strategy employs feedback mechanism to guide the selection of test cases, which has shown to be effective in fault detection process. Cloud testing is the combination of cloud computing and software testing, in which the parallel mechanism is introduced to handle multiple test tasks simultaneously. The efficiency of DRT can be improved by combining it into cloud environment. However, it faces challenges in cloud testing as its test cases are selected sequentially, which does not consist with the characteristic of parallelism underlying cloud testing. In this paper, we present a cloud-based DRT strategy to adapt DRT in cloud testing, in which both the test case prioritization and resource allocation are considered. The results of the experiments show that the cloud-based DRT can improve the efficiency of original DRT and provide stable fault detection performance enhancement over other testing strategies.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539185,,,yes,,yes,,yes,yes,yes,yes
749,FAST Approaches to Scalable Similarity-Based Test Case Prioritization,"Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in efficiency with no significant impact on effectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453081,,,yes,,yes,,yes,yes,yes,yes
750,"Feature-Based Testing by Using Model Synthesis, Test Generation and Parameterizable Test Prioritization","An approach for feature-based testing in efficient test processes, especially for use in agile development, is presented. Methods of model synthesis, model-based test generation, as well as coverage-based and requirement-based test prioritization are linked together in order to systematically and efficiently obtain prioritized test cases. The result is a reordered test suite promising quick feedback for the test engineer during test execution. The process is highly parameterizable in regard to the selection of features to be tested and the optimization criteria for the test prioritization. Using an example from industrial automation, the results of the work are demonstrated.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411744,,,yes,,yes,,yes,yes,yes,yes
752,Graphite: A Greedy Graph-Based Technique for Regression Test Case Prioritization,"To date, various test prioritization techniques have been developed, but the majority of these techniques consider a single objective that could limit the applicability of prioritization techniques by ignoring practical constraints imposed on regression testing. Multi-objective prioritization techniques try to reorder test cases so that they can optimize multiple goals that testers want to achieve. In this paper, we introduced a novel graph-based framework that maps the prioritization task to a graph traversal algorithm. To evaluate our approach, we performed an empirical study using 20 versions of four open source applications. Our results indicate that the use of the graph-based technique can improve the effectiveness and efficiency of test case prioritization technique.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539203,,,yes,,yes,no,yes,yes,yes,yes
753,How Do Software Metrics Affect Test Case Prioritization?,"In this paper we consider a statistical method to prioritize software test cases with operational profile, where the system behavior is described by a Markov reward model. Especially, we introduce software code metrics as reward parameters and apply the resulting Markov reward model to the test case prioritization problem, where our research question is set as how software code metrics affect the test case prioritization. In a numerical example with a real application software, we embed some seeding faults in advance and carry out 1,000 random test experiments. It is shown that our metrics-based test case prioritization can reduce a large amount of testing effort efficiently.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377662,,,yes,,yes,,yes,yes,yes,yes
755,Industrially Applicable System Regression Test Prioritization in Production Automation,"When changes are performed on an automated production system (aPS), new faults can be accidentally introduced into the system, which are called regressions. A common method for finding these faults is regression testing. In most cases, this regression testing process is performed under high time pressure and onsite in a very uncomfortable environment. Until now, there has been no automated support for finding and prioritizing system test cases regarding the fully integrated aPS that are suitable for finding regressions. Thus, the testing technician has to rely on personal intuition and experience, possibly choosing an inappropriate order of test cases, finding regressions at a very late stage of the test run. Using a suitable prioritization, this iterative process of finding and fixing regressions can be streamlined and a lot of time can be saved by executing test cases likely to identify new regressions earlier. Thus, an approach is presented in this paper that uses previously acquired runtime data from past test executions and performs a change identification and impact analysis to prioritize test cases that have a high probability to unveil regressions caused by side effects of a system change. The approach was developed in cooperation with reputable industrial partners active in the field of aPS engineering, ensuring a development in line with industrial requirements. An industrial case study and an expert evaluation were performed, showing promising results.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320514,,,no,,yes,no,yes,yes,yes,yes
758,Investigating NLP-Based Approaches for Predicting Manual Test Case Failure,"System-level manual acceptance testing is one of the most expensive testing activities. In manual testing, typically, a human tester is given an instruction to follow on the software. The results as ""passed"" or ""failed"" will be recorded by the tester, according to the instructions. Since this is a labourintensive task, any attempt in reducing the amount of this type of expensive testing is essential, in practice. Unfortunately, most of the existing heuristics for reducing test executions (e.g., test selection, prioritization, and reduction) are either based on source code or specification of the software under test, which are typically not being accessed during manual acceptance testing. In this paper, we propose a test case failure prediction approach for manual testing that can be used as a noncode/ specifcation-based heuristic for test selection, prioritization, and reduction. The approach uses basic Information Retrieval (IR) methods on the test case descriptions, written in natural language. The IR-based measure is based on the frequency of terms in the manual test scripts. We show that a simple linear regression model using the extracted natural language/IR-based feature together with a typical history-based feature (previous test execution results) can accurately predict the test cases' failure in new releases. We have conducted an extensive empirical study on manual test suites of 41 releases of Mozilla Firefox over three projects (Mobile, Tablet, Desktop). Our comparison of several proposed approaches for predicting failure shows that a) we can accurately predict the test case failure and b) the NLP-based feature can improve the prediction models.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367058,,,no,,no,,no,yes,yes,yes
763,On the Selection of Strength for Fixed-Strength Interaction Coverage Based Prioritization,"Abstract test cases are derived by modeling the system under test, and have been widely applied in practice, such as for software product line testing and combinatorial testing. Abstract test case prioritization (ATCP) is used to prioritize abstract test cases and aims at achieving higher rates of fault detection. Many ATCP algorithms have been proposed, using different prioritization criteria and information. One ATCP approach makes use of fixed-strength level-combinations information covered by abstract test cases, and is called fixed-strength interaction coverage based prioritization (FICBP). Before using FICBP, the prioritization strength ?__ needs to be decided. Previous studies have generally focused on ?__ values ranging between 1 and 6. However, no study has investigated the appropriateness of such a range, nor how to assign the prioritization strength for FICBP. To answer these questions, this paper reports on an empirical study involving four real-life programs (each of which with six versions). The experimental results indicate that ?__ should be set approximately equal to a value corresponding to half of the number of parameters, when testing resources are sufficient. Our results also show that when testing resources are limited or insufficient, either small or large ?__ values are suggested for FICBP.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377673,,,yes,,yes,no,yes,yes,yes,yes
773,REMAP: Using Rule Mining and Multi-objective Search for Dynamic Test Case Prioritization,"Test case prioritization (TP) prioritizes test cases into an optimal order for achieving specific criteria (e.g., higher fault detection capability) as early as possible. However, the existing TP techniques usually only produce a static test case order before the execution without taking runtime test case execution results into account. In this paper, we propose an approach for black-box dynamic TP using rule mining and multi-objective search (named as REMAP). REMAP has three key components: 1) Rule Miner, which mines execution relations among test cases from historical execution data; 2) Static Prioritizer, which defines two objectives (i.e., fault detection capability (FDC) and test case reliance score (TRS)) and applies multi-objective search to prioritize test cases statically; and 3) Dynamic Executor and Prioritizer, which executes statically-prioritized test cases and dynamically updates the test case order based on the runtime test case execution results. We empirically evaluated REMAP with random search, greedy based on FDC, greedy based on FDC and TRS, static search-based prioritization, and rule-based prioritization using two industrial and three open source case studies. Results showed that REMAP significantly outperformed the other approaches for 96% of the case studies and managed to achieve on average 18% higher Average Percentage of Faults Detected (APFD).",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367035,,,yes,,yes,,yes,yes,yes,yes
777,Test Case Prioritization Based on Method Call Sequences,"Test case prioritization is widely used in testing with the purpose of detecting faults as early as possible. Most existing techniques exploit coverage to prioritize test cases based on the hypothesis that a test case with higher coverage is more likely to catch bugs. Statement coverage and function coverage are the two widely used coverage granularity. The former typically achieves better test case prioritization in terms of fault detection capability, while the latter is more efficient because it incurs less overhead. In this paper we argue that static information such as statement and function coverage may not be the best criteria for guiding dynamic executions. Executions that cover the same set of statements /functions can may exhibit very different behavior. Therefore, the abstraction that reduces program behavior to statement/function coverage can be too simplistic to predicate fault detection capability. We propose a new approach that exploits function call sequences to prioritize test cases. This is based on the observation that the function call sequences rather than the set of executed functions is a better indicator of program behavior. Test cases that reveal unique function call sequences may have better chance to encounter faults. We choose function instead of statement sequences due to the consideration of efficiency. We have developed and implemented a new prioritization strategy AGC (Additional Greedy method Call sequence), that exploit function call sequences. We compare AGC against existing test case prioritization techniques on eight real-world open source Java projects. Our experiments show that our approach outperforms existing techniques on large programs (but not on small programs) in terms of bug detection capability. The performance shows a growth trend when the size of program increases.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377663,,,yes,,yes,,yes,yes,yes,yes
778,Test Case Prioritization for GUI Regression Testing Based on Centrality Measures,"Regression testing has been widely used in GUI software testing. For the reason of economy, the prioritization of test cases is particularly important. However, few studies discussed test case prioritization (TCP) for GUI software. Based on GUI software features, a two-layer model is proposed to assist the test case prioritization in this paper, in which, the outer layer is an event handler tree (EHT), and the inner layer is a function call graph (FCG). Compared with the conventional methods, more source code information is used based on the two-layer model for prioritization. What is more, from a global perspective, centrality measure, a complex network viewpoint is used to highlight the importance of modified functions for specific version TCP. The experiment proved the effectiveness of this model and this method.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377903,,,yes,,yes,,yes,yes,yes,yes
779,Test Re-Prioritization in Continuous Testing Environments,"New changes are constantly and concurrently being made to large software systems. In modern continuous integration and deployment environments, each change requires a set of tests to be run. This volume of tests leads to multiple test requests being made simultaneously, which warrant prioritization of such requests. Previous work on test prioritization schedules queued tests at set time intervals. However, after a test has been scheduled it will never be reprioritized even if new higher risk tests arrive. Furthermore, as each test finishes, new information is available which could be used to reprioritize tests. In this work, we use the conditional failure probability among tests to reprioritize tests after each test run. This means that tests can be reprioritized hundreds of times as they wait to be run. Our approach is scalable because we do not depend on static analysis or coverage measures and simply prioritize tests based on their co-failure probability distributions. We named this approach CODYNAQ and in particular, we propose three prioritization variants called CODYNAQSINGLE, CODYNAQDOUBLE and CODYNAQFLEXI. We evaluate our approach on two data sets, CHROME and Google testing data. We find that our co-failure dynamic re-prioritization approach, CODYNAQ, outperforms the default order, FIFOBASELINE, finding the first failure and all failures for a change request by 31% and 62% faster, respectively. CODYNAQ also outperforms GOOGLETCP by finding the first failure 27% faster and all failures 62% faster.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530018,,,yes,,yes,,yes,yes,yes,yes
780,Using Controlled Numbers of Real Faults and Mutants to Empirically Evaluate Coverage-Based Test Case Prioritization,"Used to establish confidence in the correctness of evolving software, regression testing is an important, yet costly, task. Test case prioritization enables the rapid detection of faults during regression testing by reordering the test suite so that effective tests are run as early as is possible. However, a distinct lack of information about the regression faults found in complex real-world software forced prior experimental studies of these methods to use artificial faults called mutants. Using the Defects4J database of real faults, this paper presents the results of experiments evaluating the effectiveness of four representative test prioritization techniques. Since this paper's results show that prioritization is susceptible to high amounts of variance when only one fault is present, our experiments also control the number of real faults and mutants in the program subject to regression testing. Our overall findings are that, in comparison to mutants, real faults are harder for reordered test suites to quickly detect, suggesting that mutants are not a surrogate for real faults.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8536351,,,yes,,yes,,yes,yes,yes,yes
781,Using Mutant Stubbornness to Create Minimal and Prioritized Test Sets,"In testing, engineers want to run the most useful tests early (prioritization). When tests are run hundreds or thousands of times, minimizing a test set can result in significant savings (minimization). This paper proposes a new analysis technique to address both the minimal test set and the test case prioritization problems. This paper precisely defines the concept of mutant stubbornness, which is the basis for our analysis technique. We empirically compare our technique with other test case minimization and prioritization techniques in terms of the size of the minimized test sets and how quickly mutants are killed. We used seven C language subjects from the Siemens Repository, specifically the test sets and the killing matrices from a previous study. We used 30 different orders for each set and ran every technique 100 times over each set. Results show that our analysis technique performed significantly better than prior techniques for creating minimal test sets and was able to establish new bounds for all cases. Also, our analysis technique killed mutants as fast or faster than prior techniques. These results indicate that our mutant stubbornness technique constructs test sets that are both minimal in size, and prioritized effectively, as well or better than other techniques.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424996,,,yes,,yes,,yes,yes,yes,yes
45,A selective software testing method based on priorities assigned to functional modules,"As software systems have been introduced to many advanced applications, the size of software systems increases so much. Simultaneously, the lifetime of software systems becomes very small and thus their development is required within a relatively short period. We propose a novel selective software testing method that aims to attain the requirement of short period development. The proposed method consists of 3 steps: assign priorities to functional modules (Step 1), derive a test specification (Step 2), and construct a test plan (Step 3) according to the priorities. In Step 1, for development of functional modules, we select both product and process properties to calculate priorities. Then, in Step 2, we generate detailed test items for each module according to its priority. Finally, in Step 3, we manage test resources including time and developer's skill to attain the requirement. As a result of experimental application, we can show the superiority of the proposed testing method compared to the conventional testing method.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990028,yes,,yes,,,,yes,undetermined,yes,yes
62,Elimination of crucial faults by a new selective testing method,"Recent software systems contain a lot of functions to provide various services. According to this tendency, software testing becomes more difficult than before and cost of testing increases so much, since many test items are required. In this paper we propose and discuss such a new selective software testing method that is constructed from previous testing method by simplifying testing specification. We have presented, in the previous work, a selective testing method to perform highly efficient software testing. The selective testing method has introduced an idea of functional priority testing and generated test items according to their functional priorities. Important functions with high priorities are tested in detail, and functions with low priorities are tested less intensively. As a result, additional cost for generating testing instructions becomes relatively high. In this paper in order to reduce its cost, we change the way of giving information, with respect to priorities. The new method gives the priority only rather than generating testing instructions to each test item, which makes the testing method quite simple and results in cost reduction. Except for this change, the new method is essentially the same as the previous method. We applied this new method to actual development of software tool and evaluated its effectiveness. From the result of the application experiment, we confirmed that many crucial faults can be detected by using the proposed method.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166937,yes,,no,,,,yes,undetermined,yes,yes
136,Compatibility and Regression Testing of COTS-Component-Based Software,"Software engineers frequently update COTS components integrated in component-based systems, and can often chose among many candidates produced by different vendors. This paper tackles both the problem of quickly identifying components that are syntactically compatible with the interface specifications, but badly integrate in target systems, and the problem of automatically generating regression test suites. The technique proposed in this paper to automatically generate compatibility and prioritized test suites is based on behavioral models that represent component interactions, and are automatically generated while executing the original test suites on previous versions of target systems.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222571,yes,,yes,,,,yes,undetermined,yes,yes
147,Prioritizing Coverage-Oriented Testing Process - An Adaptive-Learning-Based Approach and Case Study,"This paper proposes a graph-model-based approach to prioritizing the test process. Tests are ranked according to their preference degrees which are determined indirectly, i.e., through classifying the events. To construct the groups of events, unsupervised neural network is trained by adaptive competitive learning algorithm. A case study demonstrates and validates the approach.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291124,yes,,no,,,,yes,undetermined,yes,yes
205,Building Prioritized Pairwise Interaction Test Suites with Ant Colony Optimization,"Interaction testing offers a stable cost-benefit ratio in identifying faults. But in many testing scenarios, the entire test suite cannot be fully executed due to limited time or cost. In these situations, it is essential to take the importance of interactions into account and prioritize these tests. To tackle this issue, the biased covering array is proposed and the Weighted Density Algorithm (WDA) is developed. To find a better solution, in this paper we adopt ant colony optimization (ACO) to build this prioritized pairwise interaction test suite (PITS). In our research, we propose four concrete test generation algorithms based on Ant System, Ant System with Elitist, Ant Colony System and Max-Min Ant System respectively. We also implement these algorithms and apply them to two typical inputs and report experimental results. The results show the effectiveness of these algorithms.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381411,no,,,yes,,,yes,undetermined,yes,yes
225,Prioritized Test Generation Strategy for Pair-Wise Testing,"Pair-wise testing is widely used to detect faults in software systems. In many applications where pair-wise testing is needed, the whole test set can not be run completely due to time or budget constraints. In these situations, it is essential to prioritize the tests. In this paper, we drive weight for each value of each parameter, and adapt UWA algorithm to generate an ordered pair-wise coverage test suite. UWA algorithm is to accord weights set for each value of each parameter of the system, then produce ordered pair-wise coverage test set for having generated but unordered one. Finally, a greedy algorithm is adopted to prioritize generated pair-wise coverage test set with driven weights, so that whenever the testing is interrupted, interactions deemed, most important are tested.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368244,no,,,yes,,no,yes,undetermined,yes,yes
226,Prioritizing component compatibility tests via user preferences,"Many software systems rely on third-party components during their build process. Because the components are constantly evolving, quality assurance demands that developers perform compatibility testing to ensure that their software systems build correctly over all deployable combinations of component versions, also called configurations. However, large software systems can have many configurations, and compatibility testing is often time and resource constrained. We present a prioritization mechanism that enhances compatibility testing by examining the ldquomost importantrdquo configurations first, while distributing the work over a cluster of computers. We evaluate our new approach on two large scientific middleware systems and examine tradeoffs between the new prioritization approach and a previously developed lowest-cost-configuration-first approach.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306357,yes,,,yes,,yes,yes,undetermined,yes,yes
246,A source-based risk analysis approach for software test optimization,"In this paper we introduce our proposed technique for software component test prioritization and optimization which is based on a source-code based risk analysis. Software test is one of the most critical steps in the software development. Considering that the time and human resources of a software project are limited, software test should be scheduled and planned very carefully. In this paper we introduce a classification approach that provides the developers with a risk model of the application which is specifically designed to assist the testing process by identifying the most important components and their corresponding test effort estimation. We designed an analyser tool to apply our technique to a test software project and we presented the results in this paper.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485639,yes,,,yes,,,yes,undetermined,yes,yes
254,Constructing Prioritized Interaction Test Suite with Interaction Relationship,"Interaction testing has addressed some issues on how to select a small subset of test cases. In many systems where interaction testing is needed, the entire test suite is not executed because of time or budget constraints. It is important to prioritize the test cases in these situations. On the other hand, there are not always interactions among any factors in real systems. Moreover, some factors may need N-way (N&gt;2) testing since there is a closer relationship among them. We present a model for prioritized interaction testing with interaction relationship and propose a greedy algorithm for generating variable strength covering arrays with bias.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459713,no,,,yes,,,yes,undetermined,yes,yes
308,Calculating Prioritized Interaction Test Sets with Constraints Using Binary Decision Diagrams,"Combinatorial interaction testing has become an established technique to systematically determine test sets for highly-configurable software systems. The generation of minimal test sets that fullfill the demanded coverage criteria is an NP-complete problem. Constraint handling and integrated test case prioritization, features necessary for practical use, further complicate the problem. We present a novel algorithm that exploits our observation that the combinatorial interaction testing problem with constraints can be modelled as a single propositional logic formula. Our test set calculation algorithm uses binary decision diagrams as efficient data structure for this formula. The algorithm supports constraints and prioritization. Our evaluation results prove its cost effectiveness. For many benchmark problems the algorithm calculates the best results compared to other greedy approaches.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954420,,yes,,yes,,,yes,undetermined,yes,yes
311,Change Sensitivity Based Prioritization for Audit Testing of Webservice Compositions,"Modern software systems have often the form of Web service compositions. They take advantage of the availability of a variety of external Web services to provide rich and complex functionalities, obtained as the integration of external services. However, Web services change at a fast pace and while syntactic changes are easily detected as interface incompatibilities, other more subtle changes are harder to detect and may give raise to faults. They occur when the interface is compatible with the composition, but the semantics of the service response has changed. This typically involves undocumented or implicit aspects of the service interface. Audit testing of services is the process by which the service integrator makes sure that the service composition continues to work properly with the new versions of the integrated services. Audit testing of services is conducted under strict (sometimes extreme) time and budget constraints. Hence, prioritizing the audit test cases so as to execute the most important ones first becomes of fundamental importance. We propose a test case prioritization method specifically tailored for audit testing of services. Our method is based on the idea that the most important test cases are those that have the highest sensitivity to changes injected into the service responses (mutations). In particular, we consider only changes that do not violate the explicit contract with the service (i.e., the WSDL), but may violate the implicit assumptions made by the service integrator.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954434,,yes,,no,,no,yes,undetermined,yes,yes
315,"CRANE: Failure Prediction, Change Analysis and Test Prioritization in Practice -- Experiences from Windows","Building large software systems is difficult. Maintaining large systems is equally hard. Making post-release changes requires not only thorough understanding of the architecture of a software component about to be changed but also its dependencies and interactions with other components in the system. Testing such changes in reasonable time and at a reasonable cost is a difficult problem as infinitely many test cases can be executed for any modification. It is important to obtain a risk assessment of impact of such post-release change fixes. Further, testing of such changes is complicated by the fact that they are applicable to hundreds of millions of users, even the smallest mistakes can translate to a very costly failure and re-work. There has been significant amount of research in the software engineering community on failure prediction, change analysis and test prioritization. Unfortunately, there is little evidence on the use of these techniques in day-to-day software development in industry. In this paper, we present our experiences with CRANE: a failure prediction, change risk analysis and test prioritization system at Microsoft Corporation that leverages existing research for the development and maintenance of Windows Vista. We describe the design of CRANE, validation of its useful-ness and effectiveness in practice and our learnings to help enable other organizations to implement similar tools and practices in their environment.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770625,,yes,,yes,,no,yes,undetermined,yes,yes
316,Critical component analyzer __?? A novel test prioritization framework for component based real time systems,"Component based software development system is composed of many components and it uses the reusable components as the building blocks for constructing the complex software system. The major challenges in CBS are testing component dependency that is; it is a tricky task to test each and every component for each possible input data which will lead to exhaustive testing. To reduce the cost, the industries are following some stopping criteria and release the product to the customer side. These stopping criteria will at times lead to skipping up of some of the components from rigorous testing. This will lead to hazardous side effects such as loss in terms of revenue, human life and resources. This insight leads to the need to identify critical components which have the higher dependability measure in terms of functionality and receives higher priority in testing with rigorous test procedures. Hence, this paper proposes a novel method for identifying the critical components from the Software under Test (SUT) and prioritizes them for testing with at most care based on various dependency metrics and measures among the components with the help of Component Execution Sequence Graph (CESG).",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140684,,yes,,yes,,,yes,undetermined,yes,yes
322,Developing a Single Model and Test Prioritization Strategies for Event-Driven Software,"Event-Driven Software (EDS) can change state based on incoming events; common examples are GUI and Web applications. These EDSs pose a challenge to testing because there are a large number of possible event sequences that users can invoke through a user interface. While valuable contributions have been made for testing these two subclasses of EDS, such efforts have been disjoint. This work provides the first single model that is generic enough to study GUI and Web applications together. In this paper, we use the model to define generic prioritization criteria that are applicable to both GUI and Web applications. Our ultimate goal is to evolve the model and use it to develop a unified theory of how all EDS should be tested. An empirical study reveals that the GUI and Web-based applications, when recast using the new model, show similar behavior. For example, a criterion that gives priority to all pairs of event interactions did well for GUI and Web applications; another criterion that gives priority to the smallest number of parameter value settings did poorly for both. These results reinforce our belief that these two subclasses of applications should be modeled and studied together.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401169,,no,,yes,,,yes,undetermined,yes,yes
328,Goal-Oriented Test Case Selection and Prioritization for Product Line Feature Models,"The software product line engineering paradigm is amongst the widely used means for capturing and handling the commonalities and variabilities of the many applications of a target domain. The large number of possible products and complex interactions between software product line features makes the effective testing of them a challenge. To conquer the time and space complexity involved with testing a product line, an intuitive approach is the reduction of the test space. In this paper, we propose an approach to reduce the product line test space. We introduce a goal-oriented approach for the selection of the most desirable features from the product line. Such an approach allows us to identify the features that are more important and need to be tested more comprehensively from the perspective of the domain stakeholders. The more important features and the configurations that contain them will be given priority over the less important configurations, hence providing a hybrid test case reduction and prioritization strategy for testing software product lines.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945249,,yes,,yes,,yes,yes,undetermined,yes,yes
375,Dynamic Fault Visualization Tool for Fault-based Testing and Prioritization,"Fault-based testing has been proven to be a cost effective testing technique for software logics and rules expressed in Boolean expressions. It can guarantee the elimination of common faults without exhaustive testing. However, average software testing practitioners may not have in-depth knowledge on Boolean algebra and complex logic derivations required to apply existing fault-based testing techniques. In this paper, a dynamic fault visualization tool has been proposed. This tool allows its user to visualize fault-based testing and prioritize test inputs with a simple greedy method. The performance evaluation of this tool has been done on Boolean expressions extracted from a real life aviation tool. The results show that it can achieve significant performance improvements compared to ordinary sequential order test execution and existing static technique. The proposed visualization tool could also identify possible faults to guide the debugging process.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516370,,yes,,yes,,,yes,undetermined,yes,yes
509,Applying Ant Colony Optimization in software testing to generate prioritized optimal path and test data,"Software testing is one of the most important parts of software development lifecycle. Among various types of software testing approaches structural testing is widely used. Structural testing can be improved largely by traversing all possible code paths of the software. Genetic algorithm is the most used search technique to automate path testing and test case generation. Recently, different novel search based optimization techniques such as Ant Colony Optimization (ACO), Artificial Bee Colony (ABC), Artificial Immune System (AIS), Particle Swarm Optimization (PSO) have been applied to generate optimal path to complete software coverage. In this paper, ant colony optimization (ACO) based algorithm has been proposed which will generate set of optimal paths and prioritize the paths. Additionally, the approach generates test data sequence within the domain to use as inputs of the generated paths. Proposed approach guarantees full software coverage with minimum redundancy. This paper also demonstrates the proposed approach applying it in a program module.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307500,,yes,,,yes,,yes,undetermined,yes,yes
541,Prioritization and ranking of ERP testing components,"Software Testing is one of the most important activities but more often than not attracts less attention than it deserves in software development and implementation. It often takes twenty to sometimes even more than fifty percent of the total software development time. Enterprise Resource Planning (ERP) Systems provide synergy by integrating all operations of an enterprise. So implementation of ERP systems need even more rigorous testing than that employed in stand-alone software development. Software testing is a well-researched area but software testing as employed on ERP systems albeit is droughted with respect to research. This research paper is an extension of the patent by Kapur et al.[8] that identified the ERP Testing Components to measure ERP testing efficiency. Here, the ERP Testing Components have been accumulated and categorized under five heads. Thereafter, these testing components have been prioritized and ranked with the help of Analytic Hierarchy Process (AHP), as given by Saaty [17].",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359245,,yes,,,yes,yes,yes,undetermined,yes,yes
542,Prioritization of test scenarios using hybrid genetic algorithm based on UML activity diagram,"Software testing is an essential part of the SDLC(Software Development Life Cycle). Test scenarios are used to derive test cases for model based testing. However, with the software rapidly growing in size and complexity, the cost of software will be too high if we want to test all the test cases. So this paper presents an approach using Hybrid Genetic Algorithm(HGA) to prioritize test scenarios, which improves efficiency and reduces cost as well. The algorithm combines Genetic Algorithm(GA) with Particle Swarm Optimization(PSO) algorithm and uses Local Search Strategy to update the local and global best information of the PSO. The proposed algorithm can prioritize test scenarios so as to find a critical scenario. Finally, the proposed method is applied to several typical UML activity diagrams, and compared with the Simple Genetic Algorithm(SGA). The experimental results show that the proposed method not only prioritizes test scenarios, but also improves the efficiency, and further saves effort, time as well as cost.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339189,,yes,,,yes,,yes,undetermined,yes,yes
571,A hybrid approach for test case prioritization and optimization using meta-heuristics techniques,"Software testing is a very crucial and important phase for (SDLC) software development life cycle. Software is being tested on its effectiveness for generating good quality software. Regression testing is done by considering the constraints of resources and in this phase optimization of test suite is very important and crucial. This paper mainly aims to make use of hybrid approach of meta-heuristics, It comprises of two algorithms first is genetic algorithm and second is particle swarm optimization. In addition to algorithm the comparison of proposed algorithm hybrid GA_PSO with other optimization algorithms are been done. To validate the research Average Percentage Fault Detection (APFD) metric is used for comparison and fitness evaluation of the proposed algorithm.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975319,,yes,,,yes,,yes,undetermined,yes,yes
653,Using memetic algorithms for test case prioritization in model based software testing,"Building high quality software is one of the main goals in software industry. Software testing is a critical step in confirming the quality of software. Testing is an expensive activity because it consumes about 30% to 50% of all software developing cost. Today much research has been done in generating and prioritizing tests. First, tester should find the most important and critical path in software. They can reduce cost by finding errors and preventing to propagate it in design step. In this paper, a model based testing method is introduced. This method can prioritize tests using activity diagram, control flow graph, genetic and memetic algorithm. Different version of memetic algorithm has been made by stochastic local search, randomize iterative improvement, hill climbing and simulated annealing algorithms. The results show that the using local search methods with genetic algorithm (GA) provide efficiency and produce competitive results in comparison with GA.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482129,,,yes,,yes,,yes,undetermined,yes,yes
704,Improving the Cooperation of Fuzzing and Symbolic Execution by Test-cases Prioritizing,"Nowadays much attention is paid to the threat of vulnerabilities on the software security. Fuzzing and symbolic execution, complementary to each other, are two effective techniques in software testing. In this paper, we develop the prototype called FAS(Fuzzing and Symbolic) for software testing under both fuzzing and symbolic execution. In our method, the test cases are prioritized through deep-oriented strategy and large-distance-first strategy, in order to get a higher path-coverage with the condition of limited resource.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288548,,,no,,yes,,yes,undetermined,yes,yes
723,Test Optimization from Release Insights: An Analytical Hierarchy Approach,"Software Testing is an essential aspect to ensure software quality, reliability and consistent user experience. Digital applications such as mobile app usually follow rapid software delivery which consists of various releases. It typically uses insights from the development data such as defects, test logs for test execution optimization. Once the application is released and deployed, there is rich availability of untapped heterogeneous data which can also be effectively utilized for the next release test execution optimization. The data from the release includes direct customer feedback, application monitoring data such as user behavioral traces, device usages, release logs. In this position paper, we discuss about the various data sources and the multiple insights which can be derived from them. We also propose a framework which uses Analytical Hierarchy Process to prioritize the tests based on these insights available from the release data. The framework also recommends the prioritized and missed device configurations for next release test planning.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967947,,,no,,yes,,yes,undetermined,yes,yes
729,TITAN: Test Suite Optimization for Highly Configurable Software,"Exhaustive testing of highly configurable software developed in continuous integration is rarely feasible in practice due to the configuration space of exponential size on the one hand, and strict time constraints on the other. This entails using selective testing techniques to determine the most failure-inducing test cases, conforming to highly-constrained time budget. These challenges have been well recognized by researchers, such that many different techniques have been proposed. In practice, however, there is a lack of efficient tools able to reduce high testing effort, without compromising software quality. In this paper we propose a test suite optimization technology TITAN, which increases the time-and cost-efficiency of testing highly configurable software developed in continuous integration. The technology implements practical test prioritization and minimization techniques, and provides test traceability and visualization for improving the quality of testing. We present the TITAN tool and discuss a set of methodological and technological challenges we have faced during TITAN development. We evaluate TITAN in testing of Cisco's highly configurable software with frequent high quality releases, and demonstrate the benefit of the approach in such a complex industry domain.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7928010,,,yes,,no,,yes,undetermined,yes,yes
737,An Empirical Comparison of Fixed-Strength and Mixed-Strength for Interaction Coverage Based Prioritization,"Test case prioritization (TCP) plays an important role in identifying, characterizing, diagnosing, and correcting faults quickly. The TCP has been widely used to order test cases of different types, including model inputs (also called<italic>abstract test cases</italic>). Model inputs are constructed by modeling the program according to its input parameters, values, and constraints, and has been used in different testing methods, such as combinatorial interaction testing and software product line testing. The<italic>Interaction coverage-based TCP</italic>(ICTCP) uses interaction coverage information derived from the model input to order inputs. Previous studies have focused generally on the<italic>fixed-strength</italic>ICTCP, which adopts a fixed strength (<italic>i.e.</italic>, the level of parameter interactions) to support the ICTCP process. It is generally accepted that using more strengths for ICTCP,<italic>i.e.</italic>,<italic>mixed-strength</italic>ICTCP, may give better ordering than fixed-strength. To confirm whether mixed-strength is better than fixed-strength, in this paper, we report on an extensive empirical study using five real-world programs (written in C), each of which has six versions. The results of the empirical studies show that mixed-strength has better rates of interaction coverage overall than fixed-strength, but they have very similar rates of fault detection. Our results also show that fixed-strength should be used instead of the mixed-strength at the later stage of software testing. Finally, we offer some practical guidelines for testers when using interaction coverage information to prioritize model inputs, under different testing scenarios and resources.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8523673,,,yes,,yes,no,yes,undetermined,yes,yes
739,Assessing Test Case Prioritization on Real Faults and Mutants,"Test Case Prioritization (TCP) is an important component of regression testing, allowing for earlier detection of faults or helping to reduce testing time and cost. While several TCP approaches exist in the research literature, a growing number of studies have evaluated them against synthetic software defects, called mutants. Hence, it is currently unclear to what extent TCP performance on mutants would be representative of the performance achieved on real faults. To answer this fundamental question, we conduct the first empirical study comparing the performance of TCP techniques applied to both real-world and mutation faults. The context of our study includes eight well-studied TCP approaches, 35k+ mutation faults, and 357 real-world faults from five Java systems in the Defects4J dataset. Our results indicate that the relative performance of the studied TCP techniques on mutants may not strongly correlate with performance on real faults, depending upon attributes of the subject programs. This suggests that, in certain contexts, the best performing technique on a set of mutants may not be the best technique in practice when applied to real faults. We also illustrate that these correlations vary for mutants generated by different operators depending on whether chosen operators reflect typical faults of a subject program. This highlights the importance, particularly for TCP, of developing mutation operators tailored for specific program domains.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530033,,,yes,,yes,,yes,undetermined,yes,yes
745,DevOps Improvements for Reduced Cycle Times with Integrated Test Optimizations for Continuous Integration,"DevOps, as a growing development practice that aims to enable faster development and efficient deployment of applications without compromising on quality, is often hampered by long cycle times. One contributing factor to long cycle times in DevOps is long build time. Automated testing in continuous integration is one of the build stages that is highly prone to long run-time due to software complexity and evolution, and inefficient due to unoptimized testing approaches. To be cost-effective, testing in continuous integration needs to use only a fast-running set of comprehensive tests that are able to ensure the level of quality needed for deployment to production. Known approaches use time-aware test selection methods to improve time-efficiency of continuous integration testing by providing optimized combinations and order of tests with respect to decreased run-time. However, focusing on time-efficiency as the sole criterion in DevOps often jeopardizes the quality of software deliveries. This paper proposes a technique that integrates fault-based and risk-based test selection and prioritization optimized for low run-time, to improve time-effectiveness of continuous integration testing, and thus reduce long cycle times in DevOps, without compromising on quality. The technique has been evaluated in testing of a large-scale configurable software in continuous integration, and has shown considerable improvement over industry practice with respect to time-efficiency.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377636,,,yes,,no,no,yes,undetermined,yes,yes
206,Configuration aware prioritization techniques in regression testing,"Configurable software lets users customize applications in many ways, and is becoming increasingly prevalent. Regression testing is an important but expensive way to build confidence that software changes introduce no new faults as software evolves, resulting in many attempts to improve its performance given limited resources. Whereas problems such as test selection and prioritization at the test case level have been extensively researched in the regression testing literature, they have rarely been considered for configurations, though there is evidence that we should not ignore the effects of configurations on regression testing. This research intends to provide a framework for configuration aware prioritization techniques, evaluated through empirical studies.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071025,yes,,,yes,,yes,yes,no,yes,yes
252,Arranging software test cases through an optimization method,"During the software testing process, the customers would be invited to review or inspect an ongoing software product. This phase is called the __??in-plant__?_ test, often known as an __??alpha__?_ test. Typically, this test phase lasts for a very short period of time in which the software test engineers or software quality engineers rush to execute a list of software test cases in the test suite with customers. Because of the time constraint, the test cases have to be arranged in terms of test case severities, estimated test time, and customers' demands. As important as the test case arrangement is, this process is mostly performed manually by the project managers and software test engineers together. As the software systems are getting more sophisticated and complex, a greater volume of test cases have to be generated, and the manual arrangement approach may not be the most efficient way to handle this. In this paper, we propose a framework for automating the process of test case arrangement and management through an optimization method. We believe that this framework will help software test engineers facing with the challenges of prioritizing test cases.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602131,yes,,,yes,,,yes,no,yes,yes
275,Requirement-based test case generation and prioritization,"Software release testing is a critical phase in the software development life cycle, as it validates the software against its requirements. Designing comprehensive release test cases that are driven by the software requirements remain the major success factor of the testing phase as far as the software customers are concerned. Further, availing sufficient traceability information to ensure complete coverage of requirements validation in the designed test case suite is significant to software quality assurance. In this paper, we propose a systematic mechanism to derive a set of release test cases from a set of requirements modeled with the Genetic Software Engineering (GSE) method. GSE models functional requirements with a semi-formal visual notation called Behavior Trees (BT). Our algorithm prioritizes the requirements modeled with BTs and derives a set of prioritized release test cases systematically. Additionally, our algorithm provides sufficient traceability information relating test cases to the requirements being tested. This allows for ensuring completeness of test case coverage. We also demonstrate our test case derivation mechanism through a case study.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720443,yes,,,yes,,,yes,no,yes,yes
281,Taking Advantage of Service Selection: A Study on the Testing of Location-Based Web Services Through Test Case Prioritization,"Dynamic service compositions pose new verification and validation challenges such as uncertainty in service membership. Moreover, applying an entire test suite to loosely coupled services one after another in the same composition can be too rigid and restrictive. In this paper, we investigate the impact of service selection on service-centric testing techniques. Specifically, we propose to incorporate service selection in executing a test suite and develop a suite of metrics and test case prioritization techniques for the testing of location-aware services. A case study shows that a test case prioritization technique that incorporates service selection can outperform their traditional counterpart - the impact of service selection is noticeable on software engineering techniques in general and on test case prioritization techniques in particular. Further-more, we find that points-of-interest-aware techniques can be significantly more effective than input-guided techniques in terms of the number of invocations required to expose the first failure of a service composition.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552784,yes,,,no,,,yes,no,yes,yes
435,Randomizing regression tests using game theory,"As software evolves, the number of test-cases in the regression test suites continues to increase, requiring testers to prioritize their execution. Usually only a subset of the test cases is executed due to limited testing resources. This subset is often known to the developers who may try to __??game__?_ the system by committing insufficiently tested code for parts of the software that will not be tested. In this new ideas paper, we propose a novel approach for randomizing regression test scheduling, based on Stackelberg games for deployment of scarce resources. We apply this approach to randomizing test cases in such a way as to maximize the testers' expected payoff when executing the test cases. Our approach accounts for resource limitations (e.g., number of testers) and provides a probabilistic distribution for scheduling test cases. We provide an example application of our approach showcasing the idea of using Stackelberg games for randomized regression test scheduling.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693122,,yes,,yes,,yes,yes,no,yes,yes
488,Test Suite Prioritization by Switching Cost,"Test suite generation and prioritization are two main research fields to improve testing efficiency. Combinatorial testing has been proven as an effective method to generate test suite for highly configurable software systems, while test suites are often prioritized by interaction coverage to detect faults as early as possible. However, for some cases, there exists reasonable cost of reconfiguring parameter settings when switching test cases in different orders. Surprisingly, only few studies paid attention to it. In this paper, by proposing greedy algorithms and graph-based algorithms, we aim to prioritize a given test suite to minimize its total switching cost. We also compare two different prioritization strategies by a series of experiments, and discuss the advantages of our prioritization strategy and the selection of prioritization techniques. The results show that prioritization by switching cost can improve testing efficiency and our prioritization strategy can produce a small test suite with a reasonably low switching cost. This prioritization can be used widely and help locate fault causing interactions. The results also suggest that when testing highly configurable software systems and no knowledge of fault detection can be used, prioritization by switching cost is a good choice to detect faults earlier.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825648,,yes,,,yes,,yes,no,yes,yes
500,A novel dynamic analysis of test cases to improve testing efficiency in object-oriented systems,"In this paper, we present a series of methods to improve testing efficiency especially for regression testing from a novel view, namely dynamic analysis of test cases suitable for class testing in object-oriented systems. We mine static call graphs and dynamic call trees to represent the static features and dynamic tests of the program. By graph analysis, we present a series of methods and testing criteria to evaluate test cases from the view of code coverage. These methods improve testing efficiency for class testing from the following aspects: automation; multi-angle evaluations of test cases; improvement and management of test cases; providing different prioritization criteria and optimization criteria for regression testing to meet different testing requirements etc. What's more, they can be used in large-scale OO systems, and the test results are quantifiable.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490789,,yes,,,yes,,yes,no,yes,yes
540,Preemptive Regression Testingof Workflow-Based Web Services,"An external web service may evolve without prior notification. In the course of the regression testing of a workflow-based web service, existing test case prioritization techniques may only verify the latest service composition using the not-yet-executed test cases, overlooking high-priority test cases that have already been applied to the service composition before the evolution. In this paper, we propose Preemptive Regression Testing (PRT), an adaptive testing approach to addressing this challenge. Whenever a change in the coverage of any service artifact is detected, PRT recursively preempts the current session of regression test and creates a sub-session of the current test session to assure such lately identified changes in coverage by adjusting the execution priority of the test cases in the test suite. Then, the sub-session will resume the execution from the suspended position. PRT terminates only when each test case in the test suite has been executed at least once without any preemption activated in between any test case executions. The experimental result confirms that testing workflow-based web service in the face of such changes is very challenging; and one of the PRT-enriched techniques shows its potential to overcome the challenge.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6812226,,no,,,yes,no,yes,no,yes,yes
552,Test case analytics: Mining test case traces to improve risk-driven testing,"In risk-driven testing, test cases are generated and/or prioritized based on different risk measures. For example, the most basic risk measure would analyze the history of the software and assigns higher risk to the test cases that used to detect bugs in the past. However, in practice, a test case may not be exactly the same as a previously failed test, but quite similar. In this study, we define a new risk measure that assigns a risk factor to a test case, if it is similar to a failing test case from history. The similarity is defined based on the execution traces of the test cases, where we define each test case as a sequence of method calls. We have evaluated our new risk measure by comparing it to a traditional risk measure (where the risk measure would be increased only if the very same test case, not a similar one, failed in the past). The results of our study, in the context of test case prioritization, on two open source projects show that our new risk measure is by far more effective in identifying failing test cases compared to the traditional risk measure.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070482,,yes,,,yes,no,yes,no,yes,yes
578,ACO based embedded system testing using UML Activity Diagram,"This paper proposed a model-based technique for test scenario generation using Activity Diagram (AD). We transform an AD specification into an intermediate graph called Activity Interaction Graph (AIG) using the proposed parser. After that, we apply combination of BFS and DFS algorithms for generating test scenarios. Then, we apply an algorithm called ACOToTSP (Ant Colony Optimization for Test Scenarios Prioritization) algorithm on the generated test scenarios with respect to some decision and concurrent criteria, for prioritizing the test scenarios. This approach generates test scenarios according to forks, Joins, and merge point's strength in the activity diagram.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847997,,no,,,yes,no,yes,no,yes,yes
623,Neuro-fuzzy based approach to event driven software testing: A new opportunity,"Event Driven Software (EDS) testing is a very challenging task as a large number of events can be invoked by users. So far it is difficult to test all the user inputs invoked, therefore, test case prioritization is essentially required for giving more priority to test cases which reveal higher faults comparatively. We have proposed test case prioritization for EDS: as the Event Type, Interaction of Event, and Coverage of Event. Priority assigned in the proposed model uses these factors in Adaptive Neuro-Fuzzy Inference System (ANFIS) MATLAB Toolbox based on Neuro-Fuzzy logic model. Evaluation and validation will be done using Average Percentage of Fault Detection (APFD). APFD rate for prioritized sequence using the proposed Neuro-Fuzzy logic model exhibited 81% rate, whereas, non-prioritized test sequences showed70% suggesting, thereby, that after prioritization; rate of fault detection has improved considerably. Data shows that proposed Neuro-Fuzzy logic model is apt for Test Case Prioritization of EDS Testing.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975349,,,no,,yes,,yes,no,yes,yes
689,Design and Implementation of Combinatorial Testing Tools,"As an effective software testing technique, combinatorial testing has been gradually applied in various types of test practice. In this case, it is necessary to provide useful combinatorial testing tools to support the application of combinatorial testing technique on industrial scenarios, as well as the academic research for combinatorial testing technique. To this end, on the basis of the research results of this group, a suite of combinatorial testing tools has been developed, whose functions include test case generation, test case optimization, and etc. For the requirements from both industrial and academic scenarios, the tools should be configurable, scalable, modular, and etc. This paper gives a brief introduction to the design and implementation of these tools. Keywords-combinatorial testing, combinatorial testing tools, test generation, test prioritization.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004338,,,yes,,yes,no,yes,no,yes,yes
14,Priority based data flow testing,"Software testing is an expensive component of software development and maintenance. For data flow testing, test cases must be found to test the def-use pairs in a program. Since some of the def-use pairs identified through static analysis may be infeasible, no amount of testing effort may result in exhaustive testing of a program. Therefore in practice a fixed amount of effort is spent in testing a program. We develop an approach for assigning priorities to def-use pairs, such that the def-use pairs with higher priorities can be expected to require less effort for test case generation and therefore testing. Thus, by using the priorities as a guide for ordering the def-use pairs for testing, we can maximize the number of def-use pairs tested using a fixed amount of testing effort. We apply the technique to regression testing during the software maintenance phase, in which case the priorities are assigned to capture not only the difficulty in test case generation but also the likelihood that an error introduced by a program change is uncovered by the test case.",1995,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526556,yes,,yes,,,,yes,yes,no,no
266,Prioritization of test case scenarios derived from activity diagram using genetic algorithm,"Software testing involves identifying the test cases which discovers the errors in the program. However, the exhaustive testing is rarely impossible and very time consuming. In this paper, the software testing efficiency is optimized by identifying the critical path clusters. The test case scenarios are derived from the activity diagram and the testing efficiency is optimized by applying the genetic algorithm on the test data. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of activity diagram.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640479,no,,,yes,,,yes,yes,no,no
294,A genetic algorithm based approach for prioritization of test case scenarios in static testing,"White box testing is a test technique that takes into account program code, code structure and internal design flow. White box testing is primarily of two kinds-static and structural. Whereas static testing requires only the source code of the product, not the binaries or executables, in structural testing tests are actually run by the computer on built products. In this paper, we propose a technique for optimizing static testing efficiency by identifying the critical path clusters using genetic algorithm. The testing efficiency is optimized by applying the genetic algorithm on the test data. The test case scenarios are derived from the source code. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of the control flow graph generated from the source code. This research paper is an extension of our previous research paper [18].",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075160,no,,,yes,,,yes,yes,no,no
1,Application of Decision Theory to the Testing of Large Systems,"A methodology for determining priorities in allocating test resources among the various subsystems within a large system is described. The methodology is based on concepts from applied decision theory. Two versions of the methodology are presented: a complete version, called the extensive form, and an approximate version, called the diminutive form.",1971,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4103708,yes,,yes,,,,yes,undetermined,no,no
2,Multiaccess in a Nonqueueing Mailbox Environment,"A new and flexible solution to the problem of multiple users accessing a single resource, such as communication bandwidth or composite object in memory, is derived. The means of communication consists of sending and receiving messages in known locations (or equivalently, mailboxes without queueing). Any particular user is able to deposit, and hence destroy, previous messages in a mailbox. It is assumed that exclusive access to a mailbox is supplied by an underlying system. The major results of this paper are: 1) a simple tree-based algorithm that guarantees __?___ no user or group of users can conspire to prevent access by some other user to the resource; __?___ only one user accesses the resource at a time; __?___ if there are N users, an individual user is guaranteed access, when requested, to the resource in no more than N-1 turns; Knuth's solution [6] can delay a user up to 2** (N-1)-1 turns; 2) an extension of Dekker's algorithm (2 users) [2] that allows the relative rates of reservations for access to the resource to be proportional to a set of N integers. When a reservation is not being used by its ``owner,'' it will be assigned to another contending request. The assignment is optimal for periodic requests.",1984,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5010232,no,,no,,,,no,undetermined,,no
4,Testing and Evaluation of the Defense Data Network,"The purpose of this document is to establish a concept for test and evaluation (T&E) for the Defense Data Network (DDN) as a whole. This concept is based on the requirements of the DDN in its evolving configurations (e.g., MILNET, SACDIN, DODIIS, ..., DDN); the goals and priorities of the DDN DCS DSD; the needs of the various DDN user communities; knowledge of the ARPANET technology; and experience in T&E of systems and independent verification, validation, and test (IVV&T) of software and hardware. Since 1983 Intermetrics has been a contractor for Independent Validation and Verification (IV&V) support to the Testing branch of the Defense Data Network Defense Communications System Data Systems Directorate (DDN DCS DSD).",1987,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4795240,no,,no,,,,no,undetermined,,no
5,Formal methods and iterative design,"Formal methods allow a system designer to describe precisely how the system will be. In the area of human-computer interaction this means describing the structure and detail of the user interface. Giving the designer tools to think clearly about the decisions made must be helpful. However, the problem of deciding how the user interface is to be designed remains. In particular, to use such tools effectively a designer must have a good understanding of how the users think about the task, what their expectations and priorities are. This kind of information is best obtained by user testing with prototypes. An initial design is put forward and then refined using feedback from typical users doing typical tasks with a prototype. This refinement should happen in parallel with the refinement of a formal model of the user interface by the application of domain independent principles. In the early stages the cost of this procedure can be minimised by using simulations and mock ups rather than full prototypes. This is known as iterative design. This paper describes, through an example, some techniques which can be used to get insights about how a user approaches a task and the difficulties they have with a particular prototype.<<ETX>>",1988,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=209309,no,,no,,,,no,undetermined,,no
6,Real-time adaptive control of knowledge based avionics tasks,"Advanced decision-making capabilities are being developed to aid the pilots of the next generation of tactical fighters. Due to the limited processing resources available in an avionics suite, efforts have focused on developing a distributed fault-tolerant software architecture that permits the real-time prioritization and scheduling of these tasks. The authors outline the design details of an architecture under development to meet these performance requirements. The system has been tested with a threat-avoidance system, implemented on a testbed of five internetted LISP workstations, to evaluate overall system capabilities including scheduling, task operations, and database accesses. It has a simulation cycle of 50 ms and synchronization between distributed nodes can be achieved within 2 ms. This test case has nine knowledge tasks, one of which is defined as a simulation cycle that drives the test case. This system has been evaluated with the current trace capabilities and runs with a peak of 16 task instances active at any time.<<ETX>>",1989,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=40357,no,,no,,,,no,undetermined,,no
7,Group decision support and multiple criteria optimization,The paper proposes a structured group decision aid based on multiple criteria optimization. The procedure is designed to solve optimization problems which involve conflicting objectives and multiple decision-makers with different priorities. Most of the empirical findings regarding the performance of multi criteria techniques involve a single decision-maker. The focus in these algorithms is on determining a compromise solution to a multicriteria problem which best coincides with the preference structure of a decision-maker. The approach taken is to imbed the task of finding a compromise solution in a more general and flexible framework. The underlying concept in this framework is the analytic hierarchy process and the Tchebycheff algorithm is used to solve the multiple criteria problem. The objectives and the alternative solutions to the multiple criteria problem are evaluated through the analytic hierarchy process.<<ETX>>,1991,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=184201,no,,no,,,,no,undetermined,,no
8,Setting maintenance quality objectives and prioritizing maintenance work by using quality metrics,"Metrics that are collected and validated during development can be used during maintenance to control quality and prioritize maintenance work. Validity criteria are defined mathematically. The approach is based on validating selected metrics against related quality factors during development and using the validated metrics during maintenance to: establish initial quality objectives and quality control criteria and prioritize software components (e.g., module) and allocate resources to maintain them. The author illustrates both a case of passing a validation test (discriminative power) and failing a validation test (tracking).<<ETX>>",1991,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=160337,no,,no,,,,no,undetermined,,no
9,Specification criticism via goal-directed envisionment,"Validating a complex system specification is a difficult problem. Generating behaviors and using them to critique a specification is one effective approach. Up until now, symbolic evaluation has been the key technique of behavior generation. Unfortunately, it has drawbacks both in the amount of time it takes to complete a symbolic run, and in the large amount of uninteresting data it produces. The authors propose goal-directed envisionment as an alternative to symbolic evaluation, supplementing the basic envisioning techniques of qualitative physics with behavioral goals. This approach overcomes the problems of symbolic evaluation by generating interpretations in a reasonable amount of time and by exploiting goals to prioritize and analyze the interpretations. The authors describe and evaluate SC, an implemented system which employs goal-directed envisionment to critique specifications.<<ETX>>",1991,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=213080,no,,no,,,,no,undetermined,,no
10,Quality Function Deployment (QFD) in testing,"QFD is a quality-oriented process that can play an important role in the market-driven, total quality control environment. It can be deployed in almost all areas of product development, test and manufacturing processes. QFD is one way to ensure the reliability of the software products. The author is concerned with the role that software testing can play in increasing the reliability of software. He also examines how Quality Function Deployment (QFD) could be used to achieve this objective. QFD provides integration of the various functions by tying design and process activities together. It priorities product and manufacturing-process characteristics and highlights areas which would require further analysis.<<ETX>>",1992,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=168144,no,,no,,,,no,undetermined,,no
11,A method for automatic evaluation of fault effects in the advanced intelligent network,"The method described here provides network operators with criteria for deciding the priorities with which services degraded by network faults should be restored. This method consists of four processes: the first lists the unavailable services of customers affected by the fault, the second predicts the mean time needed to repair the fault, the third predicts the traffic trends for the affected services, and the fourth calculates the criteria used to decide the service restoration priorities. This method enables network operators to choose suitable means restoring services and to avoid future congestion due to faults. It also lets them respond more quickly and precisely to customer claims. We are introducing this method into our advanced IN operations systems.<<ETX>>",1993,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318129,no,,no,,,,no,undetermined,,no
12,New probabilistic measures for accelerating the automatic test pattern generation algorithm,"In order to approximate the signal controllabilities, the authors introduce new probabilistic measures called signal priorities, whose computation relies on the minimum-value distributions of fanout input variables of a digital circuit. The signal priorities serve the same purpose as do the signal controllabilities. That is, they are used to accelerate the automatic test pattern generation algorithm; however, their computation requires much less effort. This new method is formally defined and tested with several practical example circuits.<<ETX>>",1993,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396314,no,,no,,,,no,undetermined,,no
13,Cost-benefit analysis of electric power system reliability,"The purpose of this investigation is to determine an appropriate cost-benefit formula that will help the power system planners in prioritizing transmission system projects. This paper deals with describing the value of increased reliability and security in bulk power systems. Three different approaches used for prioritization of transmission system projects by the electric utilities are discussed and analyzed for two different transmission system alternatives. Utilizing the best approach, transmission alternatives are prioritized and the best alternative is placed on top of the prioritized table. An analysis of the three approaches and a relative comparison is performed on the IEEE 25 bus Reliability Test System. TRELSS (Transmission Reliability Evaluation of Large Scale Systems), a software package developed by EPRI, is utilized in determining the probabilistic indices that are used in the proposed approach.<<ETX>>",1994,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=287831,no,,no,,,,no,undetermined,,no
15,The peer tasking design method,"This paper is a preliminary report of an ARPA sponsored study. It focuses on designing real-time command and control or battle management systems for parallel and distributed architectures. Due to delays in other ARPA programs, the targeted architectures were not available during the time frame of the study. The results of the study were, however, tested on more conventional sequential and parallel platforms. The design method discussed here is fundamentally different from those assumed by current real-time scheduling theories, e.g., rate-monotonic, earliest-deadline-first, least-laxity or best-effort. These theories assume that the fundamental unit of prioritization is the task. In this new method, the fundamental unit of prioritization is called a work item. Work items are functions the system performs that have timing requirements (deadlines) associated with them in the requirements specification. Current scheduling theories are applied using artifact deadlines introduced by designers whereas this new method schedules work items to meet specification deadlines (sometimes called end-to-end deadlines) required by the user. With this method, tasks have no priorities. A collection of tasks with no priorities will be called a collection of peer tasks. The study showed that it is possible to schedule work items based on importance rather than urgency while still meeting as many work item deadlines as can be met by scheduling tasks with respect to urgency. Also, it showed that the minimum on-line deadline that can be guaranteed for a work item of highest importance, scheduled at run-time, is approximately the inverse of the throughput, measured in work items per second, for a work load consisting only of work items of that type. Further, it was shown that it provides optimal utilization of a single processor machine, and that its timing behavior is predictable (provable) for both single and multiprocessor machines. Finally, it was shown that throughput is not degraded during overload.<<ETX>>",1995,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470511,no,,no,,,,no,undetermined,,no
16,High-level synthesis of recoverable microarchitectures,Two algorithms that combine the operations of scheduling and recovery point insertion for high-level synthesis of recoverable microarchitectures are presented. The first uses a prioritized cost function in which functional unit cost is minimized first and register cost second. The second algorithm minimizes a weighted sum of functional unit and register costs. Both algorithms are optimal according to their respective cost functions and require less than 10 minutes of CPU time on widely-used high-level synthesis benchmarks. The best previous result reported several hours of CPU time for some of the same benchmarks.,1996,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494128,no,,no,,,,no,undetermined,,no
17,Selecting engineering techniques using fuzzy logic based decision support,"The task of selecting software engineering methods, techniques, metrics, and tools is usually performed manually, based on the expertise of individuals. This paper presents a systematic tool supported approach, that bases its suggestions an the technical situation, the existing goals, and constraints of a specific organization or a particular project. A prototype of the decision support system supports the elaboration of test strategies. The approach uses information about the technical situation that is provided by answering predefined questions with fuzzy data. The objective is to assign ""adequacy values"" to combinations of test methods, techniques, metrics, tools, and quantified test situations. The priorities of goals and constraints have assessed by applying a technique that is based on comparing goals in pairs. This permits to check certain consistency criteria by static analysis. A hierarchy of the importance of goals and constraints is calculated, which provides the basis for the determination of the suitability of test methods; techniques, metrics, and tools with respect to goals and constraints.",1996,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494570,no,,no,,,,no,undetermined,,no
18,A high-level synthesis approach to design of fault-tolerant systems,"Fault-tolerance in embedded systems is a requirement of increasing importance; solutions must achieve a balance between performances and costs that was not usually requested in design of more classical fault-tolerant applications and that involves as a consequence new approaches. A design technique is here proposed supporting fault-tolerance of hardware modules in complex hardware-software systems, fault-tolerance requirements for each hardware-mapped process are specified in terms of time constraints and of relative priorities, and a high-level synthesis methodology allowing to design - for each process - a processor capable of supporting both the nominal execution of the process itself in a fault-free environment and simultaneous execution of a reconfigured pair of processes in a fault-affected environment is defined Performances of the scheduling algorithm, allowing to achieve reconfiguration with minimum resource increase and within the required limits of speed degradation, are evaluated on some relevant instances of algorithms discussed in current literature on high-level synthesis.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=600305,no,,no,,,,no,undetermined,,no
19,A multi-platform support environment,"Legacy weapon systems, such as attack aircraft, have taken advantage of embedded computers and software to provide enormous capabilities for flexibility and expandability. The provision of these capabilities has been at a cost, and that is in the dedicated software development facilities which have sprung up to support these legacy systems. Unfortunately, the costs of these dedicated facilities is becoming prohibitive. The Advanced Avionics Multi-Radar Software Support Study (AAMRSSS) project offers experience in handling the above problem. AAMRSSS studied the feasibility of using a dedicated Software Development Facility (SDF) to support multiple software system platforms. Issues of the study were: commonality; unique requirements of the new system to be added; platform priorities; and future expansion. In particular, this study has addressed supporting the AC-130U Gunship's Radar Operational Flight Program (OFF) in the F-15's Radar Software Development Facility.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=622734,no,,no,,,,no,undetermined,,no
21,A systematic tradeoff analysis for conflicting imprecise requirements,"The need to deal with conflicting system requirements has become increasingly important over the past several years. Often, these requirements are elastic in that they can be satisfied to a degree. The overall goal of this research is to develop a formal framework that facilitates the identification and the tradeoff analysis of conflicting requirements by explicitly capturing their elasticity. Based on a fuzzy set theoretic foundation for representing imprecise requirements, we describe a systematic approach for analyzing the tradeoffs between conflicting requirements using the techniques in decision science. The systematic tradeoff analyses are used for three important tasks in the requirement engineering process: (1) for validating the structure used in aggregating prioritized requirements, (2) for identifying the structures and the parameters of the underlying representation of imprecise requirements and (3) for assessing the priorities of conflicting requirements. We illustrate these techniques using the requirements of a conference room scheduling system.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566845,no,,no,,,,no,undetermined,,no
22,An optimal algorithm for scheduling soft aperiodic tasks in dynamic-priority preemptive systems,"The paper addresses the problem of jointly scheduling tasks with both hard and soft real time constraints. We present a new analysis applicable to systems scheduled using a priority preemptive dispatcher, with priorities assigned dynamically according to the EDF policy. Further, we present a new efficient online algorithm (the acceptor algorithm) for servicing aperiodic work load. The acceptor transforms a soft aperiodic task into a hard one by assigning a deadline. Once transformed, aperiodic tasks are handled in exactly the same way as periodic tasks with hard deadlines. The proposed algorithm is shown to be optimal in terms of providing the shortest aperiodic response time among fixed and dynamic priority schedulers. It always guarantees the proper execution of periodic hard tasks. The approach is composed of two parts: an offline analysis and a run time scheduler. The offline algorithm runs in pseudopolynomial time O(mn), where n is the number of hard periodic tasks and m is the hyperperiod/min deadline.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601081,no,,no,,,,no,undetermined,,no
23,Design and implementation of a real-time switch for segmented Ethernets,"Providing network bandwidth guarantees over an Ethernet requires coordination of the network nodes for traffic prioritization such that real-time data can have deterministic access to the network. We have shown previously how RETHER, a software based token passing protocol can efficiently provide real-time support over a single shared Ethernet segment. This work extends the token passing mechanism into a switched, multi-segment Ethernet environment. This paper describes the detailed design issues, their solutions, and a fully operational switch implementation built into the FreeBSD kernel. By testing the protocol independently and as the underlying network protocol of the Stony Brook Video Server, we have verified that the bandwidth guarantees are successfully provided, with relatively low run-time overhead, for real-time connections crossing multiple Ethernet segments. This paper also provides a comprehensive performance evaluation of the prototype.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643709,no,,no,,,,no,undetermined,,no
24,Failproof team projects in software engineering courses,"The computer science department of the University of Pittsburgh offers two undergraduate and two graduate courses in software engineering in which we emphasize the importance of general engineering principles for software development. For the last ten years the undergraduate courses have been based on team projects. This structure has advantages: students see immediately the relevance of what they learn, and the team setting leads to a better understanding of what they learn. The projects in the two courses are of different types. In one course the result is the formal specification and design of a software system. In the other, the teams implement such a system, but emphasis is on testing rather than on the implementation itself. The success of each project is guaranteed by making it open-ended. A team establishes a list of priorities that is to ensure that a useful product will have been built by the time the term ends. We discuss the nature of team projects, and our evaluation scheme.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=636027,no,,no,,,,no,undetermined,,no
25,Prioritizing Software Requirements In An Industrial Setting,,1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=610408,no,,no,,,,no,undetermined,,no
26,READS: a prototyping environment for real-time active applications,"We present our Real-time Active Database System, READS, which is a prototyping environment for real-time active database applications on a conventional Unix environment, e.g., Solaris 2.4. In READS, transactions are associated with deadlines and priorities. Priority scheduling is supported by the real-time extensions provided in the underlying operating system. READS can be served as a testbed for evaluating different issues in the design of real-time active database systems (RTADBS). Different approaches for assigning deadlines and priorities to triggered transactions have been suggested and discussed. An application, the programmed trading database system, is implemented with READS and experiments have been performed to study the impact of the deadline constraints on the performance of the deferred and immediate coupling modes for triggering.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617285,no,,no,,,,no,undetermined,,no
27,Stochastic Petri nets applied to the performance evaluation of static task allocations in heterogeneous computing environments,"A stochastic Petri net (SPN) is systematically constructed from a task graph whose component subtasks are statically allocated onto the processor suite of a heterogeneous computing system (HCS). Given that subtask execution times are exponentially distributed an exponential distribution can be generated for the overall completion time. In particular the enabling functions and rate functions used to specify the SPN model provide needed versatility to integrate processor heterogeneity, task priorities, allocation schemes, communication costs, and other factors characteristic of a HCS into a comprehensive performance analysis. The manner in which these parameters are incorporated into the SPN allows the model to be transformed into a testbed for optimization schemes and heuristics. The proposed approach can be applied to arbitrary task graphs including non-series-parallel.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=581420,no,,no,,,,no,undetermined,,no
28,Testing for millennium risk management,"The Year 2000 conversion is a challenge to the economics of both testing and maintenance, but as a whole we are not responding with balanced priorities. Those looking for Year 2000 solutions typically allocate their first energies and budgets to acquiring automated analysis and conversion deals and services. This disturbing tendency ignores two facts that surface in virtually every analysis of the Year 2000 challenge: fifty percent or more of the effort will be in testing; and despite consuming a wealth of resources each year, current testing practices cannot satisfy the demands of current maintenance unrelated to the Y2K conversion. For the moment, most organizations continue to delay action on the Y2K Test problem while wading through the dozens of available analysis and conversion solutions. As a result, many Y2K projects have started on master plans that will need major revision once the true needs and benefits of testing automation become apparent. A growing number of those projects have already corrected course, revising strategy and reallocating budgets once they appreciated the nature of the Y2K testing challenge. Embarrassment will probably be the least of many worries for those that ignore the challenge much longer.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=589256,no,,no,,,,no,undetermined,,no
29,Derivation of an integrated operational profile and use case model,"Requirements engineering and software reliability engineering both involve model building related to the usage of the intended system; requirements models and test case models respectively are built. Use case modelling for requirements engineering and operational profile testing for software reliability engineering are techniques which are evolving into software engineering practice. Approaches towards integration of the use case model and the operational profile model are proposed. By integrating the derivation of the models, effort may be saved in both development and maintenance of software artifacts. Two integration approaches are presented, transformation and extension. It is concluded that the use case model structure can be transformed into an operational profile model adding the profile information. As a next step, the use case model can be extended to include the information necessary for the operational profile. Through both approaches, modelling and maintenance effort as well as risks for inconsistencies can be reduced. A positive spin-off effect is that quantitative information on usage frequencies is available in the requirements, enabling planning and prioritizing based on that information.",1998,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730843,no,,no,,,,no,undetermined,,no
30,Prioritized token-based mutual exclusion for distributed systems,"A number of solutions have been proposed for the problem of mutual exclusion in distributed systems. Some of these approaches have since been extended to a prioritized environment suitable for real-time applications but impose a higher message passing overhead than our approach. We present a new protocol for prioritized mutual exclusion in a distributed environment. Our approach uses a token-based model working on a logical tree structure, which is dynamically modified. In addition, we utilize a set of local queues whose union would resemble a single global queue. Furthermore, our algorithm is designed for out-of-order message delivery, handles messages asynchronously and supports multiple requests from one node for multi-threaded nodes. The prioritized algorithm has an average overhead of O(log(n)) messages per request for mutual exclusion with a worst-case overhead of O(n), where n represents the number of nodes in the system. Thus, our prioritized algorithm matches the message complexity of the best non-prioritized algorithms while previous prioritized algorithms have a higher message complexity, to our knowledge. Our concept of local queues can be incorporated into arbitrary token-based protocols with or without priority support to reduce the amount of messages. Performance results indicate that the additional functionality of our algorithm comes at the cost of 30% longer response times within our test environment for distributed execution when compared with an unprioritized algorithm. This result suggests that the algorithm should be used when strict priority ordering is required.",1998,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670018,no,,no,,,,no,undetermined,,no
31,Software testability measurements derived from data flow analysis,"The purpose of the research is to develop formulations to measure the testability of a program. Testability is a program's property which is introduced with the intention of predicting efforts required for testing the program. A program with a high degree of testability indicates that a selected testing criterion could be achieved with less effort and the existing faults can be revealed more easily during testing. We propose a new program normalization strategy that makes the measurement of testability more precise and reasonable. If the program testability metric derived from data flow analysis could be applied at the beginning of a software testing phase, much more effective testing of resource allocation and prioritizing is possible.",1998,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=665760,no,,yes,,,,no,undetermined,,no
32,Scenario based integration testing for object-oriented software development,"The adaptive use case methodology for software development proposed by Carlson and Hurlbutt [1997, 1998] forms the backdrop for this paper. Their methodology integrates the software design, development and testing processes through a series of design preserving, algorithmic transformations. This paper focuses on the software testing metrics used in the generation of object oriented test plans as part of that methodology. During the design phase, interaction diagrams are developed from which use case action matrices are then generated. A use case action matrix contains a collection of related scenarios each describing a specific variant of an executable sequence of use case actions. The proposed software testing metrics are employed to improve the productivity of the testing process through scenario prioritization.",1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=810764,yes,,no,,,no,no,undetermined,,no
33,"Static properties of commercial embedded real-time programs, and their implication for worst-case execution time analysis","We have used a modified C compiler to analyze a large number of commercial real time and embedded applications written in C for 8- and 18-bit processors. Only static aspects of the programs have been studied i.e., such information that can be obtained from the source code without running the programs. The purpose of the study is to provide guidance for the development of worst-case execution time (WCET) analysis tools, and to increase the knowledge about embedded programs in general. Knowing how real programs are written makes it easier to focus research in relevant areas and set priorities. The conclusion is that real time and embedded programs are not necessarily simple just because they are written for small machines. This indicates that real life WCET analysis tools need to handle advanced programming constructions, including function pointer calls and recursion.",1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777660,no,,no,,,no,no,undetermined,,no
34,Static-priority scheduling of multiframe tasks,The multiframe model of hard-real-time tasks is a generalization of the well-known periodic task model of C. Liu and J. Layland (1973). The feasibility analysis of systems of multiframe tasks which are assigned priorities according to the rate-monotonic priority assignment scheme is studied. An efficient sufficient feasibility test for such systems of multiframe tasks is presented and proved correct-this generalizes a result of A.K. Mok and D. Chen (1997).,1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777448,no,,no,,,,no,undetermined,,no
35,Teenagers identify causes of violence in schools and develop strategies to eliminate violence using GroupSystems electronic meeting system (EMS),"Is it possible to engage teenagers in a serious effort to identify the root causes of school violence and to develop strategies to deal with it? If so, will computer-aided group decision support tools add value to the process? Those are the questions we addressed with the 1998 Teen Think Tank on School Violence. While this was neither a formal nor a scientific treatment of the subject, the results of the initial experiment were overwhelmingly impressive. Using the GroupSystems Electronic Meeting System (EMS), sixteen teenagers grappled with the issue of school violence and generated more than 800 different ways to predict, prevent, avoid, protect, react, eliminate, or cope with youth violence. After brainstorming for ideas, they also used EMS to categorize, prioritize, and to reach consensus about their best ideas. Then they developed teen recommendations for students, parents, teachers, school administrators, and law enforcement officers. This was all accomplished in two EMS sessions; and none of the students had any prior knowledge or experience with EMS. This report presents a synopsis of their findings and a brief description of the EMS process.",1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=772733,no,,no,,,,no,undetermined,,no
37,The far ultraviolet spectroscopic explorer (FUSE) instrument data system,"This paper describes the architecture for the IDS flight hardware and its real-time embedded flight software. The design uses commercial off-the-shelf (COTS) software components as much as possible, to reduce cost and software development time. The features of the IDS design that provide radiation hardness and fault tolerance are described. Implementation of software to meet the functional requirements is accomplished using a relatively small number of prioritized real-time tasks. A commercial real-time operating system kernel manages and supports these tasks. Inter-task communication is described, as are the software test and validation methods. The paper shows how custom ground support equipment was developed to facilitate software development and testing. Reliable communications between the IDS and the FUSE spacecraft bus are accomplished using a MIL-STD-1553B bus that has an imposed, deterministic real-time protocol. Similarly, communication between the IDS and the other instrument subsystems uses a second MIL-STD-1553B bus that has its own time division multiplex real-time protocol. The design of these real-time protocols is described, with particular attention to reliability and testability.",1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=821995,no,,no,,,,no,undetermined,,no
38,A physics/engineering of failure based analysis and tool for quantifying residual risks in hardware,"NASA Code Q is supporting efforts to improve the verification and validation and the risk management processes for spaceflight projects. A physics-of-failure based Defect Detection and Prevention (DDP) methodology previously developed has been integrated into a software tool and is currently being implemented on various NASA projects and as part of NASA's new model-based spacecraft development environment. The DDP methodology begins with prioritizing the risks (or failure modes, FMs) relevant to a mission which need to be addressed. These risks can be reduced through the implementation of a set of detection and prevention activities referred to herein as PACTs (preventative measures, analyses, process controls and tests). Each of these PACTs has some effectiveness against one or more FMs but also has an associated resource cost. The FMs can be weighted according to their likelihood of occurrence and their mission impact should they occur. The net effectiveness of various combinations of PACTs can then be evaluated against these weighted FMs to obtain the residual risk for each of these FMs and the associated resource costs to achieve these risk levels. The process thus identifies the project-relevant ""tall pole"" FMs and design drivers and allows real time tailoring with the evolution of the design and technology content. The DDP methodology allows risk management in its truest sense: it identifies and assesses risk, provides options and tools for risk decision making and mitigation and allows for real-time tracking of current risk status.",2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816338,no,,no,,,,no,undetermined,,no
39,Architecture of the simplified Chinese embedded system STARTH,"There is a trend for the information products that are integrated by computer, communication, and consumer electronics. The OS is required more compact and practical. An embedded system STARTH is developed based on the core of the Motorola PPSM that is a real time 32-bit kernel with prioritized interrupt scheduling. All tasks are interrupt-driven. The PPSM kernel does not access hardware device directly. The kernel controls all peripherals indirectly, through software device drivers. The PPSM tools consist of pen input, graphics, database, text, character input, system and communication. The PPSM toolsets, together with its device drivers, provides the basic contro of the LCD, the drawing functions, the real time clock and the UART. The architecture of the embedded system STARTH on the Dragon Ball EZ platform is discussed in parts in details. The development environments of both software and hardware are described. The system is analyzed from its initialization, registration to system management, even the applications programming. The STARTH is tested and run on the hardware system of Dragon Ball platform. It is found that STARTH is practical and reliable for personal information devices.",2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=843622,no,,no,,,,no,undetermined,,no
40,Building an IP network quality-of-service testbed,The Drexel Network Toolkit is a software package for testing various approaches to QoS on IP-based networks. It uses Linux and DiffServ packet-marking primitives to classify and prioritize packets. DNT was used in a project to evaluate satellite based IP delivery for multimedia applications in telemedicine and telemaintenance.,2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=865089,no,,no,,,,no,undetermined,,no
41,GBS IP multicast tunneling,"Internet protocol (IP) multicast over the DoD's Global Broadcast Service (GBS) provides a one-way communications path to the warfighter as part of an interactive communications system. GBS provides the capability to deliver information products of varying size, timeliness requirements, and security levels. The products share satellite resources based on CINC/JTF commanders priorities, operational locations, availability of the satellite resources, and platform capabilities of the deployed users. Currently, IP multicast (IPMC) is not supported by the Defense Information Infrastructure (DII). This document defines a real-time IP solution for tunneling multicast data across the Defense Information Services Network (DISN), through GBS components, and to the end-user LANs. This paper describes a cost-effective, operationally sound, incremental approach for rapid prototyping and integration of these technologies. It outlines each of the implementation approaches researched to date, the designs selected for systematic testing, and an overview of the plan for testing selected designs. The initial architecture is based on commercial-off-the-shelf IP routers, ATM switches, multicast software, and VPN techniques.",2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=904019,no,,no,,,,no,undetermined,,no
42,Generating test cases for GUI responsibilities using complete interaction sequences,"Testing graphical user interfaces (GUI) is a difficult problem due to the fact that the GUI possesses a large number of states to be tested, the input space is extremely large due to different permutations of inputs and events which affect the GUI, and complex GUI dependencies which may exist. There has been little systematic study of this problem yielding a resulting strategy which is effective and scalable. The proposed method concentrates upon user sequences of GUI objects and selections which collaborate, called complete interaction sequences (CIS), that produce a desired response for the user. A systematic method to test these CIS utilizes a finite-state model to generate tests. The required tests can be substantially reduced by identifying components of the CIS that can be tested separately. Since consideration is given to defects totally within each CIS, and the components reduce required testing further, this approach is scalable. An empirical investigation of this method shows that substantial reduction in tests can still detect the defects in the GUI. Future research will prioritize testing related to the CIS testing for maximum benefit if testing time is limited.",2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885865,no,,no,,,,no,undetermined,,no
43,The Digital Library for Earth System Education: implementing the DLESE Community Plan,"Over the past year, geoscience educators, librarians and information technologists have made substantial progress in initiating the construction ofa Digital Library for Earth System Education (DLESE). Two major efforts, the Portal to the Future Workshop and the Geoscience Digital Library (GDL) project, have established a vision for the library, a governance process to enable community ownership, management, and construction, and have begun development of a testbed collection, discovery system, and user interface. The DLESE Community Plan lays out in detail the need for this facility, a community vision for its goals and priorities, and a strategy for initial construction of the library. From this initial work, two conclusions emerge as paramount in moving forward with the library. First, it is essential that development of the library community and the building of the technological infrastructure for the library go hand in hand. Second, the library will be most effectively built as a highly coordinated, but distributed community effort. In this way, the full range of talents in the community can be leveraged and rapid development of the library is possible. This paper briefly reviews our technical accomplishments to date and outlines their plans for further development. Progress and plans can be charted in the following three areas: 1. Community-centered design and use case development 2. Discovery system, metadata, and collection testbeds 3. System architecture and interoperability.",2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860416,no,,no,,,,no,undetermined,,no
44,A hybrid genetic algorithm for generating optimal synthetic aperture radar target servicing strategies,"The purpose of this research was to develop a software tool for generating optimal target servicing strategies for imaging fixed ground targets with a spaceborne SAR. Given a list of targets and their corresponding geographic locations and relative priorities, this tool generates a target servicing strategy that maximizes the overall collection utility based on the number of targets successfully imaged weighted by their relative priorities. This tool is specifically designed to maximize sensor utility in the case of a target-rich environment. For small numbers of targets, a target servicing strategy is unnecessary, and the targets may be imaged in any order without paying any particular attention to geographic proximity or target priority. However, for large, geographically diverse target decks, the order in which targets are serviced is of great importance. The target servicing problem is shown to be of the class NP-hard, and thus cannot be solved to optimality in polynomial time. Therefore, global search techniques such as genetic algorithms are called for. A unique hybrid algorithm that combines genetic algorithms with simulated annealing has been developed to generate optimized target servicing strategies. The performance of this hybrid algorithm was compared against that of three different greedy algorithms in a series of 20 test cases. Preliminary results indicate consistent performance improvements over greedy algorithms for target-rich environments. Over the course of 20 trials, the hybrid optimizing algorithm produced weighted collection scores that were on average 10% higher than the best greedy algorithm.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931250,no,,no,,,,no,undetermined,,no
46,Air traffic control improvement using prioritized CSMA,"Version 7 simulations of the industry-standard network simulation software ""OPNET"" are presented of two applications of the Aeronautical Telecommunications Network (ATN), Controller Pilot Data Link Communications (CPDLC) and Automatic Dependent Surveillance-Broadcast mode (ADS-B), over VHF Data Link mode 2 (VDL-2). Communication is modeled for air traffic between just three cities. All aircraft are assumed to have the same equipage. The simulation involves Air Traffic Control (ATC) ground stations and 105 aircraft taking off, flying realistic free-flight trajectories, and landing in a 24-hr period. All communication is modeled as unreliable. Collision-less, prioritized carrier sense multiple access (CSMA) is successfully tested. The statistics presented include latency, queue length, and packet loss. This research may show that a communications system simpler than the currently accepted standard envisioned may not only suffice, but also surpass performance of the standard at a lower cost of deployment.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931366,no,,no,,,,no,undetermined,,no
47,Applying Moore's technology adoption life cycle model to quality of EDA software,"This paper describes a methodology for allocating priority levels and resources to quality activities during the development of EDA software projects. Geoffrey Moore's technology adoption life cycle model is used to provide a baseline understanding of what the market and the target users require at any point in time during the product life cycle. Applying this model, EDA software development teams can make choices and prioritize quality objectives which are based on the customer segment that they are targeting at any point in time during the product life cycle.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915209,no,,no,,,,no,undetermined,,no
48,DDP-a tool for life-cycle risk management,"At JPL we have developed, and implemented, a process for achieving life-cycle risk management. This process has been embodied in a software tool and is called Defect Detection and Prevention (DDP). The DDP process can be succinctly stated as: determine where we want to be, what could get in the way and how we will get there. The 'determine where we want to be' is captured as trees of requirements and the 'what could get in the way' is captured as trees of potential failure modes. Scoring the impacts of these failure modes on the requirements results in a prioritized set of failure modes. The user then selects from a set of PACTs (Preventative measures, Analyses, process Controls and Tests) each of which has an effectiveness versus the various failure modes. It is the goal of the DDP process to optimally select the subset of the PACTs/spl Dagger/ which minimizes the residual risk subject to the project resource constraints.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931736,no,,no,,,,no,undetermined,,no
49,Failure modes and effects analysis for software reliability,"This paper presents a systematic problem solving approach, which is based on the failure modes and effects analysis (FMEA), to system software reliability. This approach will practically: (a) ensure that all of conceivable failure modes and their effects on operational success of the software system have been considered; (b) list potential failures, and identify the magnitude of their effects; (c) develop criteria for test planning, design of the tests, and checkout systems (e.g., logging mechanism); (d) provide a basis for quantitative reliability and availability analysis; and (e) provide a basis for establishing corrective action priorities. This approach was created for software reliability analysis and testing in the multimedia digital distribution system (MDDS) at Thomson-CSF Sextant In-Flight Systems. First it was used to improve the software reliability for the ISDN Communication Control Unit (CCU) subsystem of the MDDS, and then globally applied to the software reliability analysis MDDS and improvement for the whole MDDS. It has been proven to be an effective and efficient approach to system software reliability.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902470,no,,no,,,,no,undetermined,,no
51,"Integrated reliability analysis, diagnostics and prognostics for critical power systems","Critical power systems, such as data centers and communication switching facilities, have very high availability requirements (5 min./year downtime). A data center that consumes electricity at a rate of 3 MW can have a downtime cost of $300,000 an hour. Even a momentary interruption of two seconds may cause a loss of two hours of data processing. Consequently, power quality has emerged as an issue of significant importance in the operation of these systems. In this paper, we address three issues of power quality: real-time detection and diagnosis of power quality problems, reliability and availability evaluation, and capacity margin analysis. The objective of real-time detection and diagnosis is to provide a seamless on-line monitoring and off-line maintenance process. The techniques are being applied to monitor the power quality of a few facilities at the University of Connecticut. Reliability analysis, based on a computationally efficient sum of disjoint products, enables analysts to decide on the optimum levels of redundancy, aids operators in prioritizing the maintenance options within a given budget, and in monitoring the system for capacity margin. Capacity margin analysis helps operators to plan for additional loads and to schedule repair/replacement activities. The resulting analytical and software tool is demonstrated on a sample data center.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949035,no,,no,,,,no,undetermined,,no
52,"Managing the maintenance of ported, outsourced, and legacy software via orthogonal defect classification","From the perspective of maintenance, software systems that include COTS software, legacy, ported or outsourced code pose a major challenge. The dynamics of enhancing or adapting a product to address evolving customer usage and the inadequate documentation of these changes over a period of time (and several generations) are just two of the factors which may have a debilitating effect on the maintenance effort. While many approaches and solutions have been offered to address the underlying problems, few offer methods which directly affect a team's ability to quickly identify and prioritize actions targeting the product which is already in front of them. The paper describes a method to analyze the information contained in the form of defect data and arrive at technical actions to address explicit product and process weaknesses which can be feasibly addressed in the current effort. The defects are classified using Orthogonal Defect Classification (ODC) and actual case studies are used to illustrate the key points.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972791,no,,no,,,,no,undetermined,,no
54,Providing absolute differentiated services for real-time applications in static-priority scheduling networks,"We propose and analyze a methodology for providing absolute differentiated services for real-time applications in networks that use static-priority schedulers. We extend previous work on worst-case delay analysis and develop a method that can be used to derive delay bounds without specific information on flow population. With this new method, we are able to successfully employ a utilization-based admission control approach for flow admission. This approach does not require explicit delay computation at admission time and hence is scalable to large systems. We assume the underlying network to use static-priority schedulers. We design and analyze several priority assignment algorithms, and investigate their ability to achieve higher utilization bounds. Traditionally, schedulers in differentiated services networks assign priorities on a class-by-class basis, with the same priority for each class on each router. We show that relaxing this requirement, that is, allowing different routers to assign different priorities to classes, achieves significantly higher utilization bounds.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=916255,no,,no,,,,no,undetermined,,no
56,The importance of quality requirements in software platform development-a survey,"This paper presents a survey where some quality requirements that commonly affect software architecture have been prioritized with respect to cost and lead-time impact when developing software platforms and when using them. Software platforms are the basis for a product-line, i.e. a collection of functionality that a number of products is based on. The survey has been carried out in two large software development organizations using 34 senior participants. The prioritization was carried out using the Incomplete Pairwise Comparison method (IPC). The analysis shows that there are large differences between the importance of the quality requirements studied. The differences between the views of different stakeholders are also analysed and it is found to be less than the difference between the quality requirements. Yet this is identified as a potential source of negative impact on product development cost and lead-time, and rules of thumb for reducing the impact are given.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=927252,no,,no,,,yes,no,undetermined,,no
58,Yemanja-a layered event correlation engine for multi-domain server farms,"Yemanja is a model-based event correlation engine for multi-layer fault diagnosis. It targets complex propagating fault scenarios, and can smoothly correlate low-level network events with high-level application performance alerts related to quality of service violations. Entity-models that represent devices or abstract components encapsulate entity behavior. Distantly associated entities are not explicitly aware of each other, and communicate through event propagation chains. Yemanja's state-based engine supports generic scenario definitions, prioritization of alternate solutions, integrated problem-state and device testing, and simultaneous analysis of overlapping problems. The system of correlation rules was developed based on device, layer, and dependency analysis, and reveals the layered structure of computer networks. The primary objectives of this research include the development of reusable, configuration independent, correlation scenarios; adaptability and the extensibility of the engine to match the constantly changing topology of a multi-domain server farm; and the development of a concise specification language that is relatively simple yet powerful.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918051,no,,no,,,,no,undetermined,,no
60,"Analysis of preemptive periodic real-time systems using the (max, plus) algebra with applications in robotics","We present the model of a system of periodic real-time tasks with fixed priorities, preemption and synchronization, performed by a robot controller, using marked graphs. Then, with the help of the (max, plus) algebra, we derive simple tests to check real-time constraints on those tasks such as response times and the respect of deadlines. This method takes into account the precedence and synchronization constraints and is not limited to a particular scheduling policy.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998024,no,,no,,,yes,no,undetermined,,no
61,Dual purpose simulation: new data link test and comparison with VDL-2,"While the results of this paper are similar to those of previous research, in this paper the technical difficulties present previously are eliminated, producing better results, enabling one to more readily see the benefits of Prioritized CSMA (PCSMA). A new analysis section also helps to generalize this research so that it is not limited to exploration of the new concept of PCSMA. Commercially available network simulation software, OPNET version 7.0, simulations are presented involving an important application of the Aeronautical Telecommunications Network (ATN), Controller Pilot Data Link Communications (CPDLC) over the Very High Frequency Data Link Mode 2 (VDL-2). Communication is modeled for essentially all incoming and outgoing nonstop air-traffic for just three United States cities: Cleveland, Cincinnati, and Detroit. Collision-less PCSMA is successfully tested and compared with the traditional CSMA typically associated with VDL-2. The performance measures include latency, throughput, and packet loss. As expected, PCSMA is much quicker and more efficient than traditional CSMA. These simulation results show the potency of PCSMA for implementing low latency, high throughput and efficient connectivity. We are also testing a new and better data link that could replace CSMA with relative ease.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1067949,no,,no,,,,no,undetermined,,no
63,Extreme programming modified: embrace requirements engineering practices,"Extreme programming (XP) is an agile (lightweight) software development methodology and it becomes more and more popular. XP proposes many interesting practices, but it also has some weaknesses. From the software engineering point of view the most important issues are: maintenance problems resulting from very limited documentation (XP relies on code and test cases only), and lack of wider perspective of a system to be built. Moreover, XP assumes that there is only one customer representative. In many cases there are several representatives (each one with his own view of the system and different priorities) and then some XP practices should be modified. In the paper we assess XP from two points of view: the capability maturity model and the Sommerville-Sawyer model (1997). We also propose how to introduce documented requirements to XP, how to modify the planning game to allow many customer representatives and how to get a wider perspective of a system to be built at the beginning of the project lifecycle.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048543,no,,no,,,yes,no,undetermined,,no
64,How much information is needed for usage-based reading? A series of experiments,"Software inspections are regarded as an important technique to detect faults throughout the software development process. The individual preparation phase of software inspections has enlarged its focus from only comprehension to also include fault searching. Hence, reading techniques to support the reviewers on fault detection are needed. Usage-based reading (UBR) is a reading technique, which focuses on the important parts of a software document by using prioritized use cases. This paper presents a series of three UBR experiments on design specifications, with focus on the third. The first experiment evaluates the prioritization of UBR and the second compares UBR against checklist-based reading. The third experiment investigates the amount of information needed in the use cases and whether a more active approach helps the reviewers to detect more faults. The third study was conducted at two different places with a total of 82 subjects. The general result from the experiments is that UBR works as intended and is efficient as well as effective in guiding reviewers during the preparation phase of software inspections. Furthermore, the results indicate that use cases developed in advance are preferable compared to developing them as part of the preparation phase of the inspection.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166932,no,,no,,,,no,undetermined,,no
65,ICT in Japanese university language education: a case study,This paper reports part of a study conducted at a university in Japan. The belief that ICT provides students with more opportunities to negotiate target forms has been used to justify its use in language education. This study could not substantiate that ICT will change the way language is used in Japan or that CALL is providing new ways for learning and acquiring a new language. Students were experiencing difficulties in prioritising their learning repertoire between the acquisition of computer skills and language proficiency. Individuals experienced positive and negative coding and de-coding filters when communicating in ICT and this was related to the validity and reliability applied to the text. The value attached to the ICT interaction may influence the degree to which the ICT event influences face-to-face communicative acts.,2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1186116,no,,no,,,,no,undetermined,,no
66,Maintaining software with a security perspective,"Testing for software security is a lengthy, complex and costly process. Currently, security testing is done using penetration analysis and formal verification of security kernels. These methods are not complete and are difficult to use. Hence it is essential to focus testing effort in areas that have a greater number of security vulnerabilities to develop secure software as well as meet budget and time constraints. We propose a testing strategy based on a classification of vulnerabilities to develop secure and stable systems. This taxonomy will enable a system testing and maintenance group to understand the distribution of security vulnerabilities and prioritize their testing effort according to the impact the vulnerabilities have on the system. This is based on Landwehr's (1994) classification scheme for security flaws and we evaluated it using a database of 1360 operating system vulnerabilities. This analysis indicates vulnerabilities tend to be focused in relatively few areas and associated with a small number of software engineering issues.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167766,no,,no,,,,no,undetermined,,no
68,Parameter optimization tool for enhancing on-chip network performance,"In this paper, we present a tool to be used in the optimization of interconnection parameters in order to achieve optimal performance and implementation with minimal costs. The optimization tool uses an iterative algorithm to optimize the interconnection parameters, such as data width, priorities, and the time an agent can reserve the interconnection, to fulfill the given constraints. In the used test case, the required area decreased 50% while 85% of the original bandwidth was obtained. This was due to an improved arbitration process.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010388,no,,no,,,,no,undetermined,,no
69,Simulation of restaurant operations using the Restaurant Modeling Studio,"The operation of quick service restaurants (QSR) is a highly engineered process, with many factors coming into play: physical layout, equipment availability, and worker staffing levels, positioning, and priorities. The Restaurant Modeling Studio (RMS) provides an analysis platform for investigating the impacts of these factors on critical performance metrics, especially speed of service and service capacity. The key components of the RMS are a simulation engine built in Arena, and two custom applications built on Microsoft Visio - the Kitchen and Process Designers. The simulation engine supports a large number of behaviors, including parallel operations, inventory replenishment, prioritized task selection and many more. The Kitchen Designer and Process Designer provide the user with powerful tools for specifying the physical layout and order fulfillment processes. This paper presents the components of the RMS and its use in an analysis kitchen design comparison and labor deployment standards.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166417,no,,no,,,,no,undetermined,,no
72,A task space redundancy-based scheme for motion planning,"In many applications, the manipulations require only part of the degrees of freedom (DOFs) of the end-effector, or some DOFs are more important than the rest. We name these applications prioritized manipulations. The end-effectors DOFs are divided into those which are critical and must be controlled as precisely as possible, and those which have loose specifications, so their tracking performance can traded-off to achieve other needs. In this paper, we derive a formulation for partitioning the task space into major and secondary task directions and finding the velocity and static force mappings that precisely accomplish the major task and locally optimize some secondary goals. The techniques are tested on a 6-DOF parallel robot performing a 2-DOF tracking task.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244063,no,,no,,,,no,undetermined,,no
73,"Generating, selecting and prioritizing test cases from specifications with tool support","The classification-tree method provides a systematic way for software testers to derive test cases by considering important relevant aspects that are identified from the specification. The method has been used in many real-life applications and shown to be effective. This paper presents several enhancements to the method by annotating the classification tree with additional information to reduce manual effort in the generation, selection and prioritization of test cases. A tool for supporting this enhanced process is also described.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319089,no,,yes,,,,yes,undetermined,no,no
74,Global implementation of ERP software - critical success factors on upgrading technical infrastructure,"Implementing an Enterprise Resource planning (ERP) software in a global environment, executive sponsors face two key challenges. While business processes are to be re-engineered to align with the ERP software best practices, technical architecture and infrastructure needs to be in place globally as per specifications of the packaged software. In the legacy environment, different countries or different business units use different systems, based on local standards supported by local resources. In the new ERP world, globally all the countries must conform to same technical infrastructure. Technical managers face multiple critical issues implementing a global solution. Most of the ERP software are developed in technically advanced countries, standards are often too high for under developed or developing countries. In an effort to bring the global organization to a common platform different countries needs different levels of upgrades. In this paper the authors review key technical issues faced is a global upgrade process to support a global ERP implementation and how to resolve those. We conclude although technical infrastructure and business process reengineering both are equally important and each implementation is unique, but following some simple steps it is easy to prioritize each ones during different phases of the project. Also time lines of two sub-projects must converge after initial phase and must follow a common plan for the project to be successful. Multiple scenarios are described to facilitate the process.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1252285,no,,no,,,yes,no,undetermined,,no
75,Global multiprocessor scheduling of aperiodic tasks using time-independent priorities,"We provide a constant time schedulability test for a multiprocessor server handling aperiodic tasks. Dhall's effect is avoided by dividing the tasks in two priority classes based on task utilization: heavy and light. We prove that if the load on the multiprocessor server stays below U/sub threshold/ = 3 - /spl radic/7 = 35.425%, the server can accept incoming aperiodic tasks and guarantee that the deadlines of all accepted tasks will be met. 35.425% utilization is also a threshold for a task to be characterized as heavy. The bound U/sub threshold/ = 3 - /spl radic/7 = 35.425% is easy-to-use, but not sharp if we know the number of processors in the multiprocessor. For a server with m processors, we calculate a formula for the sharp bound U/sub threshold/(m), which converges to Uthreshold from above as m - -. The results are based on a utilization function u/sub m/(x) = 2(1 - x)/(2 + /spl radic/(2 + 2x)) + x/m. By using this function, the performance of the multiprocessor can in some cases be improved beyond U/sub threshold/ (m) by paying the extra overhead of monitoring the individual utilization of the current tasks.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203049,no,,no,,,,no,undetermined,,no
76,Market-based task allocation for dynamic processing environments,"Flexible and large-scale information processing across enterprises entails dynamic and decentralized control of workflow through adaptive allocation of knowledge and processing resources. Markets comprise a well-understood class of mechanisms for decentralized resource allocation, where agents interacting through a price system direct resources toward their most valued uses as indicated by these prices. The information-processing domain presents several challenges for market-based approaches, including (1) representing knowledge-intensive tasks and capabilities, (2) propagating price signals across multiple levels of information processing, (3) handling dynamic task arrival and changing priorities, and (4) accommodating the increasing-returns and public-good characteristics of information products. A market gaming environment provides a methodology for testing alternative market structures and agent strategies, and evaluating proposed solutions in a realistic decentralized manner.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245031,no,,no,,,,no,undetermined,,no
77,Prioritized use cases as a vehicle for software inspections,"The usage-based reading technique combines traditional inspection principles, use cases, and operational profile testing to create an efficient user-oriented software inspection reading technique. UBR can find faults more effectively and efficiently than the traditional checklist-based method.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207451,no,,no,,,,no,undetermined,,no
79,Smart debugging software architectural design in SDL,"Statistical data show that it is much less expensive to correct software bugs at the early design stage rather than the late stage of the development process when the final system has already been implemented and integrated together. The use of slicing and execution histories as an aid in software debugging is well established for programming languages like C and C++; however, it is rarely applied in the field of software design specification. We propose a solution by applying the source code level technologies to debugging software designs represented in a high-level specification and description language such as SDL. More specifically, we extend execution slice-based heuristics from source code-based debugging to the software design specification level. Suspicious locations in an SDL specification are prioritized by their likelihood of containing faults. Locations with a higher priority should be examined first rather than those with a lower priority as the former are more likely to contain the faults. A debugging tool, SmartD/sub DSL/, with user-friendly interfaces was developed to support our method. An illustration is provided to demonstrate the feasibility of using our method to effectively debug an architectural design.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245320,no,,no,,,,no,undetermined,,no
81,Web-technology of university's evaluation rating,"At present there is no a unified standard system of both qualitative and quantitative criteria to estimate objectively the conformity of the University activities and its infrastructure with the requirements of the public state attestation and accreditation. So the use of heuristic and expert procedures is considered to be necessary. In the suggested model of the University (institute, faculty, department) rating, the fragments of the models used by the Ministry of Education of Russia, the Association for Technical Universities of Russia, the Independent Accreditation Center, the advanced technical universities of Russia, as well as long-term developments of the Tomsk Polytechnic University in quality control of the University activities and its departments, modern tendencies and priorities of the universities developments, are accepted as the basic ones. 55 indices have been determined. Numerical values of indices are normalized on the maximum value achieved in the objects under consideration that provides a commensurability of indices by their essential spread in various objects. The data convolution by sections and subsections is carried out linearly taking into account their importance. The value of the index is determined by an expert method taking into account its relative importance within the limits of each section (subsection). The numerical value of each rating index is determined by its relation to the ""base"". The ""base"" is determined as the on-budget salary fund of the University (institute, faculty, department) staff, providing educational process, and the on-budget salary fund of the staff carrying out state budgetary research. The software for the collective interactive remote input of the indices, as well as for the centralized calculation and operative analysis of the universities rating has been developed. The server Apache, the database server MySQL, and the scripting language PHP have been used for the above work to be carried out.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222647,no,,no,,,,no,undetermined,,no
82,Agile development: evaluation and experience,"Agile methods such as Extreme Programming, Crystal, Scrum, and others have attracted a lot of attention recently. Agile methods stress early and continuous delivery of software, welcome changing requirements, and value early feedback from customers. Agile methods seek to cut out inefficiency, bureaucracy, and anything that adds no value to a software product. Proponents of agile methods often see software specification and documentation as adding no value, which has led observers to conclude that agile development is nothing but unprincipled hacking, perhaps even an anarchic counter-reaction to bureaucratic, heavyweight software processes that demand ever more intermediate deliverables from developers. The purpose of this panel is to discuss under what circumstances agile methods work and don't work. Some of the key practices of agile methods are: scheduling according to feature priorities, incremental delivery of software, feedback from expert users, emphasis on face-to-face communication, pair development, minimalist design combined with refactoring, test-driven development, automated regression testing, daily integration, self-organizing teams, and periodic tuning of the methods. Working software is the primary measure of success. Find out what the latest practical experience with agile methods is and learn about the latest thinking in this area.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317492,no,,no,,,,no,undetermined,,no
83,An approach to generate the thin-threads from the UML diagrams,"Software testing plays a crucial role in assuring software quality. One of the most important issues in software testing research is the generation of the test cases. For scenario-based software testing, the thin-threads, which are the usage scenarios in a software system from the end user's point of view, are frequently used to generate test cases. However, the generation of the thin-threads is not an easy task. A scenario-based business model has to be manually derived or labor-intensive business analysis has to be manually carried out in order to extract the thin-threads from a software system. In this work, we propose an automated approach to directly generate thin-threads from the UML artifacts. The generated thin-threads can be used to generate and to prioritize the test cases for scenario-based software testing.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342893,yes,,no,,,,no,undetermined,,no
84,An execution slice and inter-block data dependency-based approach for fault localization,"Localizing a fault in a program is a complex and time-consuming process. In this paper we present a novel approach using execution slice and inter-block data dependency to effectively identify the locations of program faults. An execution slice with respect to a given test case is the set of code executed by this test, and two blocks are data dependent if one block contains a definition that is used by another block or vice versa. Not only can our approach reduce the search domain for program debugging, but also prioritize suspicious locations in the reduced domain based on their likelihood of containing faults. More specifically, the likelihood of a piece of code containing a specific fault is inversely proportional to the number of successful tests that execute it. In addition, the likelihood also depends on whether this piece of code is data dependent on other suspicious code. A debugging tool, DESiD, was developed to support our method. A case study that shows the effectiveness of our method in locating faults on an application developed for the European Space Agency is also reported.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371939,no,,no,,,no,no,undetermined,,no
85,An improved AHP method in performance assessment,"In order to reduce subjective errors in the traditional analytic hierarchy process (AHP), an improved AHP method integrated with orthogonal experimental design was presented in this paper. The new method combined both AHP and orthogonal design principles to make decisions, objectives in assessment system and values computed through mapping and satisfying function respectively as the corresponding factors, levels and results in orthogonal experimental design. With software SAS analysis, priorities on performance were to be ranked out accordingly. Application has showed that the method can improve the accuracy of AHP in performance assessment.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1340551,no,,no,,,,no,undetermined,,no
86,Assessing staffing needs for a software maintenance project through queuing simulation,"We present an approach based on queuing theory and stochastic simulation to help planning, managing, and controlling the project staffing and the resulting service level in distributed multiphase maintenance processes. Data from a Y2K massive maintenance intervention on a large COBOL/JCL financial software system were used to simulate and study different service center configurations for a geographically distributed software maintenance project. In particular, a monolithic configuration corresponding to the customer's point-of-view and more fine-grained configurations, accounting for different process phases as well as for rework, were studied. The queuing theory and stochastic simulation provided a means to assess staffing, evaluate service level, and assess the likelihood to meet the project deadline while executing the project. It turned out to be an effective staffing tool for managers, provided that it is complemented with other project-management tools, in order to prioritize activities, avoid conflicts, and check the availability of resources.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265735,no,,no,,,,no,undetermined,,no
87,Aviation application over IPv6: performance issues,"Aviation industries in United States and in Europe are undergoing a major paradigm shift in the introduction of new network technologies. In the US, NASA is also actively investigating the feasibility of IPv6 based networks for the aviation needs of the United States. In Europe, the Eurocontrol lead, Internet protocol for aviation exchange (iPAX) Working Group is actively investigating the various ways of migrating the aviation authorities backbone infrastructure from X.25 based networks to an IPv6 based network. For the last 15 years, the global aviation community has pursued the development and implementation of an industry-specific set of communications standards known as the aeronautical telecommunications network (ATN). These standards are now beginning to affect the emerging military global air traffic management (GATM) community as well as the commercial air transport community. Efforts are continuing to gain a full understanding of the differences and similarities between ATN and Internet architectures as related to communications, navigation, and surveillance (CNS) infrastructure choices. This research paper describes the implementation of the IPv6 testbed at Computer Networks & Software, Inc. and it's interface connection mechanism to Eurocontrol and NASA's (Cleveland) testbed in the first phase of the project. In the second phase this research work investigates the performance issues of aviation applications such as controller to pilot data link communication (CPDLC), on an IPv6 based backbone network. Aviation applications are grouped into different priority levels. Desired quality of service (QoS) to each priority level is implemented via Diffserv implementation. This research work looks into the possibility of providing similar QoS performance for aviation application in an IPv6 network as is provided in an ATN based network. The testbed consists of three autonomous systems. The autonomous system represents CNS domain, NASA domain and a EUROCONTROL domain. The primary mode of connection between CNS IPv6 testbed and NASA and EUROCONTROL IPv6 testbed is initially a set of IPv6 over IPv4 tunnels. The aviation application under test (CPDLC) consists of two processes running on different IPv6 enabled machines. These processes communicate with each other over the IPv6 network. One machine resides on the CNS portion of the testbed and other may reside in NASA (Cleveland) and/or in Eurocontrol. The IPv6 packets between Eurocontrol, NASA and CNS testbeds would be carried on IPv6 over IPv4 tunnels. We present some results, which suggest that IPv6 QoS has matured enough, so as to provide the QoS service, which is similar in capability to die ATN architecture. We implemented three basic priorities of flow: (1) command & control; (2) surveillance; and (3) general traffic. Various parameters like throughput, packet loss and delay are investigated. The results are analyzed to get a conceptual view of the effect of IPv6 based network on the aviation applications.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1367941,no,,no,,,,no,undetermined,,no
88,Best practices for a FRACAS implementation,"Many companies use a FRACAS (failure reporting analysis and corrective action system) process, better known as a closed-loop analysis and correction action process, to track and report problems or failures. Very few companies, however, fully realize all of the possible benefits of a FRACAS process, such as improving quality and productivity while reducing costs. Although many issues may prevent an effective implementation, there are three areas of concern that may cause negative impacts irrespective of the best-intentioned technology. In particular, complex organization interaction, inefficient and ineffective data tracking, and a lack of prioritized goals prevent the dramatic results that can be achieved with a FRACAS. By following the suggested eight step methodology and best practices presented, the potential for implementing a high-performance FRACAS can be greatly increased.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285523,no,,no,,,,no,undetermined,,no
92,Requirements driven software evolution,"Software evolution is an integral part of the software life cycle. Furthermore in the recent years the issue of keeping legacy systems operational in new platforms has become critical and one of the top priorities in IT departments worldwide. The research community and the industry have responded to these challenges by investigating and proposing techniques for analyzing, transforming, integrating, and porting software systems to new platforms, languages, and operating environments. However, measuring and ensuring that compliance of the migrant system with specific target requirements have not been formally and thoroughly addressed. We believe that issues such as the identification, measurement, and evaluation of specific re-engineering and transformation strategies and their impact on the quality of the migrant system pose major challenges in the software re-engineering community. Other related problems include the verification, validation, and testing of migrant systems, and the design of techniques for keeping various models (architecture, design, source code) during evolution, synchronized. In this working session, we plan to assess the state of the art in these areas, discuss on-going work, and identify further research issues.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311070,no,,no,,,,no,undetermined,,no
94,Concealment of whole-frame losses for wireless low bit-rate video based on multiframe optical flow estimation,"In low bit-rate packet-based video communications, video frames may have very small size, so that each frame fills the payload of a single network packet; thus, packet losses correspond to whole-frame losses, to which the existing error concealment algorithms are badly suited and generally not applicable. In this paper, we deal with the problem of concealment of whole frame-losses, and propose a novel technique which is capable of handling this very critical case. The proposed technique presents other two major innovations with respect to the state-of-the-art: i) it is based on optical flow estimation applied to error concealment and ii) it performs multiframe estimation, thus optimally exploiting the multiple reference frame buffer featured by the most modern video coders such as H.263+ and H.264. If data partitioning is employed, by e.g., sending headers, motion vectors, and coding modes in prioritized packets as can be done in the DiffServ network model, the algorithm is capable of exploiting the motion vectors to improve the error concealment results. The algorithm has been embedded in the H.264 test model software, and tested under both independent and correlated packet loss models with parameters typical of the wireless environment. Results show that the proposed algorithm significantly outperforms other techniques by several dBs in peak signal-to-noise ratio (PSNR), provides good visual quality, and has a rather low complexity, which makes it possible to perform real-time operation with reasonable computational resources.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407904,no,,no,,,,no,undetermined,,no
95,Current results from a rover science data analysis system,"The Onboard Autonomous Science Investigation System (OASIS) evaluates geologic data gathered by a planetary rover. This analysis is used to prioritize the data for transmission, so that the data with the highest science value is transmitted to Earth. In addition, the onboard analysis results are used to identify science opportunities. A planning and scheduling component of the system enables the rover to take advantage of the identified science opportunity. OASIS is a NASA-funded research project that is currently being tested on the FIDO rover at JPL for use on future missions. In this paper, we provide a brief overview of the OASIS system, and then describe our recent successes in integrating with and using rover hardware. OASIS currently works in a closed loop fashion with onboard control software (e.g., navigation and vision) and has the ability to autonomously perform the following sequence of steps: analyze gray scale images to find rocks, extract the properties of the rocks, identify rocks of interest, retask the rover to take additional imagery of the identified target and then allow the rover to continue on its original mission. We also describe the early 2004 ground test validation of specific OASIS components on selected Mars exploration rover (MER) images. These components include the rock-finding algorithm, RockIT, and the rock size feature extraction code. Our team also developed the RockIT GUI, an interface that allows users to easily visualize and modify the rock-finder results. This interface has allowed us to conduct preliminary testing and validation of the rock-finder's performance.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559328,no,,no,,,,no,undetermined,,no
96,Early community building: a critical success factor for XP projects,"Extreme programming (XP) literature and discussions often view successful projects only as customer-driven product development: planning, coding and testing an unfolding series of prioritized units of vertical functionality. I claim, however, that a successful project also requires a prospering community, comprising an introspective group of committed professionals communicating effectively, and using a well-understood, stable process. Weakness on any of these fronts presents a high risk of failure; therefore, I advise every XP project's members to actively engage in building their community, such that it reaches its critical level of development already by the first internal release. To help in this endeavor, I provide a comprehensive list of activities and attitudes to practice and avoid during the first release.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609817,no,,no,,,,no,undetermined,,no
97,Empirical results from an experiment on value-based review (VBR) processes,"As part of our research on value-based software engineering, we conducted an experiment on the use of value-based review (VBR) processes. We developed a set of VBR checklists with issues ranked by success-criticality, and a set of VBR processes prioritized by issue criticality and stakeholder-negotiated product capability priorities. The experiment involved 28 independent verification and validation (IV&V) subjects (full-time working professionals taking a distance learning course) reviewing specifications produced by 18 real-client, full-time student e-services projects. The IV&V subjects were randomly assigned to use either the VBR approach or our previous value-neutral checklist-based reading (CBR) approach. The difference between groups was not statistically significant for number of issues reported, but was statistically significant for number of issues per review hour, total issue impact, and cost effectiveness in terms of total issue impact per review hour. For the latter, the VBRs were roughly twice as cost-effective as the CBRs.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541809,no,,no,,,,no,undetermined,,no
98,Identification of test process improvements by combining fault trigger classification and faults-slip-through measurement,"Successful software process improvement depends on the ability to analyze past projects and determine which parts of the process that could become more efficient. One source of such an analysis is the faults that are reported during development. This paper proposes how a combination of two existing techniques for fault analysis can be used to identify where in the test process improvements are needed, i.e. to pinpoint which activities in which phases that should be improved. This was achieved by classifying faults after which test activities that triggered them and which phase each fault should have been found in, i.e. through a combination of orthogonal defect classification (ODC) and faults-slip-through measurement. As a part of the method, the paper proposes a refined classification scheme due to identified problems when trying to apply ODC classification schemes in practice. The feasibility of the proposed method was demonstrated by applying it on an industrial software development project at Ericsson AB. The obtained measures resulted in a set of quantified and prioritized improvement areas to address in consecutive projects.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541824,no,,no,,,,no,undetermined,,no
99,Integrated Product Policy and distributed supplier structures: SME and sound LCA data in conflict,"The sustainable development of our societies is one of the priorities of the European Commission. Through its new Integrated Product Policy (IPP), the European Commission is developing a series of measures that influence the supply and demand of environmentally sound products. Some IPP tools are based on product and process self-declarations, while others require the performance of a Life-Cycle Assessment (LCA). Life-Cycle Inventory (LCI) data availability is the fundamental premise in order to be able to perform an LCA. In this paper, the work is to investigate the diffusion of required LCA data along the supplier chain with the aim of identifying strategies to increase the awareness of Small and Medium Enterprises (SMEs) in respect to LCA, to suggest methodologies to facilitate the collection of sound LCI data and to test available low-cost software tools to support LCA, with particular reference to the production phase is reported",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1619261,no,,no,,,,no,undetermined,,no
100,Plug and play testbed to enable responsive space missions,"There exists a growing need in the DOD for a tactical or responsive space asset to support real-time battlefield intelligence, surveillance, and reconnaissance. The desired attributes include being: 1. responsive /sub e/ployable in days; 2. affordable - expendable tactical resources at a cost comparable to other tactical systems; 3. employable - assets must support the joint force commander (JFC); 4. integrated space/air/terrestrial system-of-systems - full network connectivity, bandwidth on demand, and augment other assets. To the warfighter, a responsive space asset would provide the capability to respond to unanticipated military needs in days, providing flexibility of response to rapidly field tailored payloads and coverage. This capability could also provide rapid reconstitution after a loss from attack or failure and counteract enemy adaptation, through denial or deception, to existing space capabilities. Most importantly the short deployment times and low cost would provide the United States a means for efficiently using the versatility, and relative safety of space to provide real-time support to the war fighter. To be responsive the space element must possess a modular design supporting ""plug and play"" (PnP) architecture, leveraging commercial parts and standards. Lending itself to a lean production and integration environment again utilizing standard interfaces and taking advantage of pre-qualified inventoried subsystems. Rapid deployment of these elements will make use of ""canned"" mission planning tools, tailored orbits for a given theater, built-in health and status monitoring, and autonomous test and checkout software and operations. The two emerging responsive mission objectives include space control and tailored, tactical intelligence, surveillance, reconnaissance (ISR). Space control involves a situational awareness to sense threats against, and provide protection for, US space assets. The tailored, tactical missions offer high tempo ISR operations such as target characterization and emitter location in theater, perform real-time blue force tracking, and provide gap-filler, specialized communications support. The challenge is to develop and qualify the satellite technologies and rapid integration and test processes to support an operational responsive system in the next five years. Under an Air Force Research Laboratory SBIR program, MicroSat Systems is developing a PnP testbed to enable an operational responsive capability through development and ground validation of the various elements. Those elements include the mission definition and CONOPS specification processes, space segment, and the operational prioritization, tasking, processing, exploitation, and dissemination (PTPED) process/infrastructure. Specific to providing an end-to-end mission simulation, the testbed should be equipped with the modeling and simulation tools to develop tactical satellite CONOPS and provide the warfighter with a front end tool for training. To validate utility to the end-user the testbed must be equipped with the capability to simulate the data processing and dissemination infrastructure envisioned to provide the battlefield commander with real-time data. The space segment of the testbed should consist of the hardware and software elements required to simulate the operations of a fully functional satellite system. The approach to developing a responsive mission testbed includes defining requirements, hardware/software architecture, technology development roadmap, starting with a core capability, and incrementally integrating and validating the developing components and processes as they emerge.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559345,no,,no,,,,no,undetermined,,no
101,Prioritize code for testing to improve code coverage of complex software,"Code prioritization for testing promises to achieve the maximum testing coverage with the least cost. This paper presents an innovative method to provide hints on which part of code should be tested first to achieve best code coverage. This method claims two major contributions. First it takes into account a ""global view"" of the execution of a program being tested, by considering the impact of calling relationship among methods/functions of complex software. It then relaxes the ""guaranteed"" condition of traditional dominator analysis to be ""at least"" relationship among dominating nodes, which makes dominator calculation much simpler without losing its accuracy. It also then expands this modified dominator analysis to include global impact of code coverage, i.e. the coverage of the entire software other than just the current function. We implemented two versions of code prioritization methods, one based on original dominator analysis and the other on relaxed dominator analysis with global view. Our comparison study shows that the latter is consistently better in terms of identifying code for testing to increase code coverage",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544723,yes,,yes,,,,yes,undetermined,no,no
103,"Systematic incremental development of agent systems, using Prometheus","This paper presents a mechanism for dividing an agent oriented application into the three IEEE defined scoping levels of essential, conditional and optional. This mechanism is applied after the initial system specification, and is then used to direct incremental development with three separate releases. The scoping described can be applied at any stage of a project, in order to guide consistent scoping back if such is needed. The three levels of scoping that are used are consistent with the approach used in many companies. The approach to scoping requires that scenarios are prioritised manually on a five point scale. All other aspects are then prioritised automatically, based on this information. The approach used allows a developer to indicate what size partitions - based on number of scenarios - are required for each scoping level. The mechanisms are applied to the Prometheus development methodology and are integrated into the Prometheus design tool (PDT).",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579165,no,,no,,,,no,undetermined,,no
106,Using occurrence properties of defect report data to improve requirements,"Defect reports generated for faults found during testing provide a rich source of information regarding problematic phrases used in requirements documents. These reports indicate that faults often derive from instances of ambiguous, incorrect or otherwise deficient language. In this paper, we report on a method combining elements of linguistic theory and information retrieval to guide the discovery of problematic phrases throughout a requirements specification, using defect reports and correction requests generated during testing to seed our detection process. We found that phrases known from these materials to be problematic have occurrence properties in requirements documents that both allow the direction of resources to prioritize their correction, and generate insights characterizing more general locations of difficulty within the requirements. Our findings lead to some recommendations for more efficiently and effectively managing certain natural language issues in the creation and maintenance of requirements specifications.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531046,no,,no,,,,no,undetermined,,no
107,A Dynamic Partitioning Approach for GUI Testing,"Previous works on GUI testing are mainly concerned with how to define or generate GUI test cases. The issue of how to employ generated GUI test cases or primitive actions is seldom discussed. In this paper we propose a dynamic partitioning approach for GUI testing to address the issue. In this approach, the given GUI primitive actions are dynamically partitioned into two disjoint classes: one comprising prioritized primitive actions and the other comprising non-prioritized ones. The testing process is divided into two stages and contains two feedback loops. The first stage prioritizes primitive actions and the second stage selects and performs prioritized primitive actions. The first feedback loop is local and occurs in the second stage, which adjusts the memberships of primitive actions after they are performed. The second feedback loop is global and occurs between the first and second stages. It switches GUI testing from the second stage to the first stage upon no prioritized primitive actions are available. Two testing experiments with real GUI applications show that the proposed dynamic partitioning approach can really work in practice and may significantly outperform the random testing approach in the sense that the dynamic partitioning approach uses fewer primitive actions to achieve given testing goals and behaves more stable. The dynamic partitioning approach adopts explicit feedback mechanisms and contributes to the emerging area of software cybernetics that explores the interplay between software and control",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020171,no,,no,,,,no,undetermined,,no
108,A Feature-Oriented Requirements Tracing Method: A Study of Cost-benefit Analysis,"Establishing and maintaining traceability links places a big burden since complex systems have especially yield an enormous number of various artifacts although traceability links is useful for requirements change impact analysis, requirements conflict analysis, and requirements consistency checking. Hence, we propose a feature-oriented requirements tracing method including value consideration and intermediate catalysis. To achieve our goal in this paper, we present (1) a meta-model of feature-oriented requirements tracing, (2) a featureoriented requirement tracing process overview, and (3) cost-benefits analysis. The meta-model is a formalization of feature-oriented requirement tracing using UML notation. The feature-oriented requirement tracing process consists of requirements definition, feature modeling, feature prioritization, requirements linking, and traceability links evaluation. We also carry out cost-benefit analysis through a case study to demonstrate the feasibility of our approach.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021275,no,,no,,,,no,undetermined,,no
109,A hardware implementation of layer 2 MPLS,"This paper presents a hardware architecture for layer 2 Multi Protocol Label Switching (MPLS). MPLS is a protocol framework used primarily to prioritize internet traffic and improve bandwidth utilization. Furthermore it increases the performance of internet applications and overall efficiency. However, most existing MPLS solutions are entirely software based which decreases performance. MPLS performance can be enhanced by executing core tasks in hardware while allowing other tasks to be executed in software to guard against performance degradation. This paper proposes a hardware design of MPLS on an FPGA for increased performance and efficiency.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1581247,no,,no,,,,no,undetermined,,no
110,An Add-On for Managing Behaviours with Priority in JADE,"In this article, two new implementations for behaviours in JADE are presented. These new behaviours, while being able to reproduce the functioning of the old JADE's behaviours, allow the user to define priorities. This fact is of vital importance for several multiagent applications. Finally, a test was developed to show that the performance of the new behaviours is similar to the original ones.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053000,no,,no,,,,no,undetermined,,no
111,An Agent-Based E-Learning Assessing and Instructing System,"The research work on e-learning has become very important field in the education. Many demands for instructing strategies in the adaptive learning have turn out. Base on these instructing strategies, the learners could reduce their blindness in the learning process. This paper proposes a method to build a Bayesian networks model in order to assess the learner's knowledge level and instruct the learner. This e-learning assessing and instructing system is designed and implemented based on multi-agent systems (ELAIS). In this system, the knowledge cognitive level, the learning priorities and weakness of a learner could be analyzed and assessed through the tracking information. Then the corresponding instruction will give to the learner in order to improve the learner's learning efficiency. The parameters assessment is achieved using EM algorithm and the assessment results in this system reveal the pretty accuracy to the real testing situation",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019230,no,,no,,,,no,undetermined,,no
112,Applying CMMI and Strategy to ATE Development,"This paper provides a viewpoint of the capability maturity model integration (CMMI<sup>SM</sup> ) from the perspective of automated test equipment (ATE) development and test engineering. ATE development is a specialized segment of product development and shares many of the same issues. Requirements for the test equipment are very dependent on continually evolving product characteristics. Even with the best planning, lead times for ATE development are typically eroded by late changes to product requirements and designs, and eventually the critical path leads right through test! Without a solid process foundation, chaos ensues. The CMMI process models provide a framework for the integration of best practices in many disciplines. Portions of the systems engineering, software engineering, Integrated Product and Process Development and Supplier Sourcing models all offer important perspectives which affect ATE developers. This paper focuses on the CMMI processes and best practices which yield the greatest impact to test organizations and groups that provide ATE. The overall Test Strategy should help prioritize the process areas which deserve the most attention. Mature ATE organizations use a Balanced Scorecard approach to provide alignment with corporate and program level goals. Metrics monitor their progress against their corporate goals. At the program level, they apply a risk- driven approach to selectively apply resources that achieve the highest ROI for test dollars. From this business-oriented vantage point, organizations are likely to see increased efficiencies that will decrease overall system development costs by streamlining the testing component of their budgets.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062482,no,,no,,,,no,undetermined,,no
113,DDP: a tool for life-cycle risk management,"At JPL we have developed and implemented a process for achieving life-cycle risk management. This process has been embodied in a software tool and is called defect detection and prevention (DDP). The DDP process can be succinctly stated as: determine where we want to be, what could get in the way and how we will get there. The ""determine where we want to be"" is captured as trees of requirements and the ""what could get in the way"" is captured as trees of potential failure modes. Scoring the impacts of these failure modes on the requirements results in a prioritized set of failure modes. The user then selects from a set of preventative measures, analyses, process controls and tests (PACTs) each of which has an effectiveness versus the various failure modes. It is the goal of the DDP process to optimally select the subset of the PACTs which minimizes the residual risk subject to the project resource constraints. The DDP process is intended to facilitate risk management over the entire project life cycle beginning with architectural and advanced technology decisions all the way through operation. As the project design, technology content, and implementation approach matures, the requirements and failure mode trees are elaborated upon to accommodate the additional information. Thus, the DDP process is a systematic, continuous, top-down approach to managing risk. Implementation of the DDP process requires a critical mass of expertise (usually the project team and a few specialists) and captures both their engineering judgement as well as available quantitative data. This additional data may result from models, layouts, prototype testing, other focused risk evaluations and institutional experiences. The DDP process also identifies areas where additional information would be advantageous, thus allowing a project to target critical areas of risk or risk uncertainty. This also allows the project to identify those areas which would benefit the most from application of other quantitative tools and methods (e.g. Monte Carlo simulations, FMECAs, fault trees). The software tool supports the DDP process by providing guidance for implementing the process steps, graphical visualizations of the various trees, their interrelationships and the current risk landscape. The tool is capable of supporting on-the-fly knowledge elicitation as well as integrating off-line deliberations. There are a variety of available outputs including graphs, trees and reports as well as clear identification of the driving requirements, ""tall-pole"" residual risks and the PACTs which have been selected and agreed upon. The DDP process has been applied at various levels of assembly including the system and subsystem levels, as well as down to the component level. Recently, significant benefits have been realized from application to advanced technologies, where the focus has been on increasing the infusion rates of these technologies by identification and mitigation of risks prior to delivery to a project",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662004,no,,no,,,,no,undetermined,,no
114,"Expert System for Quality Cost Planning, Monitoring and Control","On the market, there are some commercial available software tools for quality cost management. However, these tools do not incorporate specialized agents for handling complex tasks related to the current needs in quality cost planning, like interpretation of the results over a horizon of time and automatic generation of reliable guidelines to prioritize resources in order to improve the quality of the business processes. This paper is going to introduce the results of some researches performed by the authors in designing and developing an expert system for comprehensive monitoring, controlling and planning of quality costs within business processes. Results are already successful implemented in a large enterprise from chemical industry",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022922,no,,no,,,,no,undetermined,,no
115,How International Standards Such as ATML and IEEE 1641 STD can Make the Realisation of an Open System Architecture on a Common Test Platform a Reality,"A perspective on how the DoD and MoD are integrating open standards into their ATS frameworks and policy in the search for a common test platform architecture solution for use on all test platforms. The paper examines the two approaches being taken and draws on their commonality to propose how open standards can help meet both their aims and circumstances. Benefits such as TPS interoperability, re-host and re-use are examined and contrasted for open systems versus common architecture to identify the practical implication for real systems. The life cycle cost of support for the system is identified and the trade-off in cost between fast optimal TPSs and fully interoperable TPSs is considered. The paper goes on to show the difference between using information models utilizing a development process versus the use of run time interfaces and how they can lead to different solutions to the same basic problem but with different peripheral benefits. In conclusion an approach to maximize benefit between the two framework groups is considered whilst maintaining individual priorities.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062469,no,,no,,,,no,undetermined,,no
118,Optimizing the selection of representative configurations in verification of evolving product lines of distributed embedded systems,"Electronics and computer science play a more and more prominent role in automotive technology. In the future the prevalence of those new technologies and the customers' demand for individuality leads to tremendously large configuration spaces of vehicle control systems. To cope with the resulting complexity in verification, new strategies need to be explored. One likely future challenge is to determine a set of vehicle configurations, such that the successful verification of this small set implies the correctness of the entire product family. This paper presents a method to address this task, based on exploiting communalities in architecture and requirements. We introduce efficient algorithms with provable quality guarantees for the optimization problems of choosing the minimum set of configurations necessary to verify all possible configurations and choosing the best k configurations to maximize the verification coverage of the entire product family. We discuss extensions of our method which allow requirement priorities and the consideration of configuration costs, and present a technique for automatically determining communalities in architecture and requirements which can be exploited by our optimization methods. We demonstrate the effectiveness of our method on an indicator light system product family. In this example a configuration reduction by 60% can be achieved",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691579,no,,no,,,,no,undetermined,,no
119,Prioritizing Software Inspection Results using Static Profiling,"Static software checking tools are useful as an additional automated software inspection step that can easily be integrated in the development cycle and assist in creating secure, reliable and high quality code. However, an often quoted disadvantage of these tools is that they generate an overly large number of warnings, including many false positives due to the approximate analysis techniques. This information overload effectively limits their usefulness. In this paper we present ELAN, a technique that helps the user prioritize the information generated by a software inspection tool, based on a demand-driven computation of the likelihood that execution reaches the locations for which warnings are reported. This analysis is orthogonal to other prioritization techniques known from literature, such as severity levels and statistical analysis to reduce false positives. We evaluate feasibility of our technique using a number of case studies and assess the quality of our predictions by comparing them to actual values obtained by dynamic profiling.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026864,no,,no,,,,no,undetermined,,no
120,Prognostics usefulness criteria,"Prognostics and health management (PHM) can provide remarkable insight for maintenance management of large systems, but must be implemented with a healthy respect for the end user and a practical view of the hardware and software capabilities. The F-35 Joint Strike Fighter (JSF) program is implementing a comprehensive PHM system to maximize the supportability of the air system. The prognostic algorithms must have a defined minimum capability to aid implementation and verification. However, the complexity of the air system precludes creating exact requirements for remaining useful life and confidence. Instead, ""usefulness criteria"" are created to link the user need with the minimum capability of the algorithm. The usefulness criteria are a list of goals related to aircraft supportability which can be used to define the minimum acceptable time to maintenance indication for the prognostic algorithm. The goals in the usefulness criteria were applied to each prognostic algorithm in the F-35 PHM system. When assigned, these usefulness criteria provide a means to measure the improved performance of the aircraft and fleet maintenance as well as prioritize the implementation of the prognostic algorithms. The development and implementation of the algorithms in relation to these usefulness criteria are still in process, but it is expected that most algorithms exceeds the criteria. Those which do not meet the criteria are re-evaluated with a trade study to determine if further efforts in hardware and software development are warranted. This process of usefulness criteria development and application can be rigorously applied to the development of any PHM system. This paper covers the development of the usefulness criteria for the F-35 program and the implementation results to date",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656123,no,,no,,,,no,undetermined,,no
121,"Recorders, Reasoners and Artificial Intelligence - Integrated Diagnostics on Military Transport Aircraft","A research group at Boeing has developed a prototype for an integrated diagnostic system to optimize maintenance on military transport aircraft by decreasing maintenance costs and increasing aircraft availability. The integrated diagnostic system comprises an on-board recorder, and a ground-based reasoner that analyzes the recorded data, to optimize maintenance. The functions of the ground-based reasoner (GBR) include identification of root-cause, filtering of false alarms, and prioritization of maintenance actions. The technologies used include expert systems/fast state recognition methods, data mining technologies and Bayesian analyses. The ground-based reasoner provides an open plug-n-play software framework for incorporating these technologies into a software tool, for field maintenance. The tool has a simple, intuitive graphic user interface that is designed to help the end-user, the maintenance technician, with everyday maintenance tasks. The integrated diagnostic system prototype is currently undergoing testing on pre-delivery test flights for the C-17 military transport, at a Boeing facility, and initial results in applying the system to the aerial delivery subsystem and the hydraulic subsystem are discussed",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656074,no,,no,,,,no,undetermined,,no
122,Search Based Approaches to Component Selection and Prioritization for the Next Release Problem,This paper addresses the problem of determining the next set of releases in the course of software evolution. It formulates both ranking and selection of candidate software components as a series of feature subset selection problems to which search based software engineering can be applied. The approach is automated using greedy and simulated annealing algorithms and evaluated using a set of software components from the component base of a large telecommunications organization. The results are compared to those obtained by a panel of (human) experts. The results show that the two automated approaches convincingly outperform the expert judgment approach,2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021335,no,,no,,,,no,undetermined,,no
124,The Incremental Evolution of Attack Agents in Xpilot,"In the research presented in this paper, we use incremental evolution to learn multifaceted neural network (NN) controllers for agents operating in the space game Xpilot. Behavioral components specific to the accomplishment of specific tasks, such as bullet-dodging, shooting, and closing on an enemy, are learned in the first increment. These behavioral components are used in the second increment to evolve a NN that prioritizes the output of a two-layer NN depending on that agent's current situation.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688415,no,,no,,,,no,undetermined,,no
125,Towards Interactive Fault Localization Using Test Information,"Finding the location of a fault is a central task of debugging. Typically, a developer employs an interactive process for fault localization. To accelerate this task, several approaches have been proposed to automate fault localization. In practice, testing-based fault localization (TBFL), which uses test information to locate faults, has become a research focus. However, experimental results reported in the literature showed that current automation of fault localization can only serve as a means to confirming the search space and prioritizing search sequences, not a substitute of the interactive fault localization process. In this paper, we propose an approach based on test information to support the entire interactive fault localization process. During this process, the information gathered from previous interaction steps can be used to provide the ranking of suspicious statements for the current interaction step. As a feasibility study of our approach, we performed an experiment on applying our approach together with some other TBFL approaches on the Siemens programs, which have been used in the literature. Our experimental results show the effectiveness of our approach.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137428,no,,no,,,,no,undetermined,,no
127,A histogram-matching approach to the evolution of bin-packing strategies,"We present a novel algorithm for the one- dimension offline bin packing problem with discrete item sizes based on the notion of matching the item-size histogram with the bin-gap histogram. The approach is controlled by a constructive heuristic function which decides how to prioritise items in order to minimise the difference between histograms. We evolve such a function using a form of linear register-based genetic programming system. We test our evolved heuristics and compare them with hand-designed ones, including the well- known best fit decreasing heuristic. The evolved heuristics are human-competitive, generally being able to outperform high- performance human-designed heuristics.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424926,no,,no,,,,no,undetermined,,no
128,A model to predict anti-regressive effort in Open Source Software,"Accumulated changes on a software system are not uniformly distributed: some elements are changed more often than others. For optimal impact, the limited time and effort for complexity control, called anti-regressive work, should be applied to the elements of the system which are frequently changed and are complex. Based on this, we propose a maintenance guidance model (MGM) which is tested against real-world data. MGM takes into account several dimensions of complexity: size, structural complexity and coupling. Results show that maintainers of the eight open source systems studied tend, in general, to prioritize their anti-regressive work in line with the predictions given by our MGM, even though, divergences also exist. MGM offers a history-based alternative to existing approaches to the identification of elements for anti-regressive work, most of which use static code characteristics only.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362632,no,,no,,,,no,undetermined,,no
129,A Multi-Agent Solution to Distribution Systems Restoration,"The goal to provide faster and faster restoration after a fault is pushing the technical envelope related to new algorithms. While many approaches use centralized strategies, the concept of multi-agent systems (MAS) is creating a new option related to distributed analyses for restoration. This paper provides details on a MAS that restores a power system after a fault. The development of agents and behaviors of the agents are described, including communication of agents. The MAS is tested on two test systems and facilitates both full and partial restoration, including load prioritization and shedding.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4282055,no,,no,,,,no,undetermined,,no
132,A process framework for customising software quality models,The quality objective of many software organisations is to deliver software products that meet and or exceed customer expectations. The key to achieving this is to capture these expectations at the beginning of the project by clearly defining all quality requirements. The characteristics particularly defined in ISO/IEC 9126-1 (2001) provide the framework for specifying quality requirements. The ISO/IEC 9126-1 quality model is intended to be applicable to any type of software product or intermediate product. Before application this model needs to be tailored to a specific software and specific need. Since these characteristics cannot be directly measured this makes it difficult to directly prioritise and choose the most relevant characteristics and sub-characteristics. Hence a process framework that will link these characteristics and sub- characteristics to user needs is required. This will in turn help customise software quality models like ISO/IEC 9126-1 (2001) and other general software quality models. A process framework for customising software quality models is proposed in the text and it is further shown how this framework was applied in a real working environment in an attempt to quantitatively validate it. The results collected in the study showed that the framework could be used reliably in customising a generic software quality model at characteristic level only. The deviations at sub-characteristic level were due to unclear questions in the generated Generic Quality Questionnaire that resulted in misunderstandings. And the metrics used to create these questions were not fully tested for validity and reliability due to time constraints. Enhancements are discussed in the study and it is further shown how reliability can also be achieved at sub-characteristic level.,2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401495,no,,no,,,no,no,undetermined,,no
133,A Visualization Framework for Web Service Discovery and Selection Based on Quality of Service,"The visualization of Web service execution process is an emerging research in service-oriented computing (SOC) area. This paper presents a practical visualization framework in putting service discovery and selection process based on quality of service (QoS) attributes into a visual context. The proposed practical architecture serves as a foundation for designing the novel GUIs for different users. By considering the preferences and priorities for service consumers and service providers, we adopt different application UI design criteria and design patterns which tailored to service-based visualization design. Based on that, evaluation is carried out to test the usability, effectiveness and acceptability of the proposed visualization framework.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4414476,no,,no,,,,no,undetermined,,no
137,Control unit for a laboratory motor test bench for monitoring and controlling PMSM and induction motors,"The work presents a state-of-art control unit for pulse width modulated inverter-fed AC motors. The unit is suitable for a wide range of voltage source inverters with frequencies ranging from 1 KHz to up to 100 KHz. The hardware is characterized by a powerful floating point DSP, FPGA unit, asynchronous serial and IEEE 1394 communication interface, and 12 channels Analog/Digital interface with sample and conversion times together equaling 250 ns. The control software is managed by a specially designed real-time multitasking operating system. The operating system guarantees less than 300 ns time duration when jumping from one task to another upon internal or external event. The operating system can be adapted easily for arbitrary number of tasks with various prioritization levels and triggering events, and therefore suitable for interfacing hardware in the loop (HIL) simulation environments. The on-line interaction between the user and the running control software is implemented by a specially designed IEEE1394 driver for Windows XP and a graphical user interface (GUI). This allows graphical and numerical monitoring of software variables and their modification at will.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4417431,no,,no,,,,no,undetermined,,no
138,Data Mining Static Code Attributes to Learn Defect Predictors,"The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of ""McCabes versus Halstead versus lines of code counts"" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027145,no,,no,,,,no,undetermined,,no
139,Effective Fault Localization using Code Coverage,"Localizing a bug in a program can be a complex and time- consuming process. In this paper we propose a code coverage-based fault localization method to prioritize suspicious code in terms of its likelihood of containing program bugs. Code with a higher risk should be examined before that with a lower risk, as the former is more suspicious (i.e., more likely to contain program bugs) than the latter. We also answer a very important question: how can each additional test case that executes the program successfully help locate program bugs? We propose that with respect to a piece of code, the aid introduced by the first successful test that executes it in computing its likelihood of containing a bug is larger than or equal to that of the second successful test that executes it, which is larger than or equal to that of the third successful test that executes it, etc. A tool, chiDebug, was implemented to automate the computation of the risk of the code and the subsequent prioritization of suspicious code for locating program bugs. A case study using the Siemens suite was also conducted. Data collected from our study support the proposal described above. They also indicate that our method (in particular Heuristics III (c), (d), and (e)) can effectively reduce the search domain for locating program bugs.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291037,no,,no,,,,no,undetermined,,no
141,"Increased Mars Rover Autonomy using AI Planning, Scheduling and Execution","This paper presents technology for performing autonomous commanding of a planetary rover. Through the use of AI planning, scheduling and execution techniques, the OASIS autonomous science system provides capabilities for the automated generation of a rover activity plan based on science priorities, the handling of opportunistic science, including new science targets identified by onboard data analysis software, other dynamic decision-making such as modifying the rover activity plan in response to problems or other state and resource changes. We first describe some of the particular challenges this work has begun to address and then describe our system approach. Finally, we report on our experience testing this software with a Mars rover prototype.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209854,no,,no,,,no,no,undetermined,,no
142,Indexing Noncrashing Failures: A Dynamic Program Slicing-Based Approach,"Recent software systems usually feature an automated failure reporting component, with which a huge number of failures are collected from software end-users. With a proper support of failure indexing, which identifies failures due to the same fault, the collected failure data can help developers prioritize failure diagnosis, among other utilities of the failure data. Since crashing failures can be effectively indexed by program crashing venues, current practice has seen great success in prioritizing crashing failures. A recent study of bug characteristics indicates that as excellent memory checking tools are widely adopted, semantic bugs and the resulting noncrashing failures have become dominant. Unfortunately, the problem of how to index non-crashing failures has not been seriously studied before. In previous study, two techniques have been proposed to index noncrashing failures, and they are T-Proximity and R-Proximity. However, as T-Proximity indexes failures by the profile of the entire execution, it is generally not effective because most information in the profile is fault-irrelevant. On the other hand, although R-Proximity is more effective than T-Proximity, it relies on a sufficient number of correct executions that may not be available in practice. In this paper, we propose a dynamic slicing-based approach, which does not require any correct executions, and is comparably effective as R-Proximity. A detailed case study with gzip is reported, which clearly strates the advantages of the proposed approach.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362658,no,,no,,,,no,undetermined,,no
143,Model Checking Safety-Critical Systems Using Safecharts,"With rapid developments in science and technology, we now see the ubiquitous use of different types of safety-critical systems in our daily lives such as in avionics, consumer electronics, and medical systems. In such systems, unintentional design faults might result in injury or even death to human beings. To make sure that safety-critical systems are really safe, there is a need to verify them formally. However, the verification of such systems is getting more and more difficult because designs are becoming very complex. To cope with high design complexity, currently, model-driven architecture design is becoming a well-accepted trend. However, existing methods of testing and standards conformance are restricted to implementation code, so they do not fit very well with model-based approaches. To bridge this gap, we propose a model-based formal verification technique for safety-critical systems. In this work, the model-checking paradigm is applied to the Safecharts model, which was used for modeling but not yet used for verification. Our contributions listed are as follows: first, the safety constraints in Safecharts are mapped to semantic equivalents in timed automata for verification. Second, the theory for safety constraint verification is proven and implemented in a compositional model checker (that is, the state-graph manipulator (SGM)). Third, prioritized and urgent transitions are implemented in SGM to model the risk semantics in Safecharts. Finally, it is shown that the priority-based approach to mutual exclusion of resource usage in the original Safecharts is unsafe and corresponding solutions are proposed. Application examples show the feasibility and benefits of the proposed model-driven verification of safety-critical systems",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4141241,no,,no,,,,no,undetermined,,no
146,Prioritized Constraints with Data Sampling Scores for Automatic Test Data Generation,"Many automatic test data generation approaches use constraint solvers to find data values. One problem with this method is that it cannot generate test data when the constraints are not solvable, either because there is no solution or the constraints are too complex. We propose a constraint prioritization method using data sampling scores to generate valid test data even when a set of constraints is not solvable. Our case study illustrates the effectiveness of this method.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288019,no,,no,,,,no,undetermined,,no
150,Speaking Truth to Power,"Whenever the author conducts an architectural assessment for software development projects, he endeavors to speak truth to power: those with true power never fear the truth. Sam Guckenheimer has observed that in software code there is truth. Code represents the stark reality of a software development organization's labor. There is also truth to be found in a system's architecture. Every system's architecture is molded by the forces that swirl around it, and the collective concerns of all the stakeholders represent the most dynamic forces shaping a system. The software development organization's unique task is to address all the essential concerns of all the important stakeholders and to ensure that they aren't blindsided by unexpected problems and stakeholders. This is why employing a process that incrementally and iteratively grows a system's architecture through the regular release of testable executables is so important. Such a process lets the software team engage the right stakeholders at the right time and to make the right decisions, neither too early nor too late. In creating a software-intensive system that's both relevant and beautiful, every stakeholder, no matter how close or how far from the code, deserves the truth",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118643,no,,no,,,,no,undetermined,,no
151,Techniques for building excellent Operator Machine Interfaces (OMI),"Establishing a process to continually improve understanding of operator requirements -the why as well as the how-is key to developing an optimal Operator-Machine Interface (OMI) for a large program. The P-8A program is one example of a complex software development, with over 2 million lines of code and a unique and complex OMI. The OMI itself is used as a tool for achieving, and is designed in parallel with understanding end goals, operator decision making chains, and thought processes. The OMI can be designed to support and anticipate information needs, and alert operators to unusual occurrences. Operator actions and decision making techniques can be discovered by using expert operators and capturing their knowledge through the use of multiple tools to identify the problem set: storytelling, decision mapping, is/is not matrices, etc. When a solution set has been identified and agreed to by the customer, resources allocated, and schedule set, a standard process captures and prioritizes further changes, identifies their impact, and decides on implementation. Documents describing design and processes and a Design Description Document describing the current version of the OMI are made accessible to stakeholders at all times. ""What's important is not that we can conceive the idea, but that when we actually test it on people you discover it doesn't work... your intuition is wrong."" -Daniel M. Russell (IBM Almaden / Xerox PARC).",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4391947,no,,,no,,,no,undetermined,,no
154,The Effect of Organization Process Focus and Organizational Learning on Project Performance: An Examination of Taiwan's Companies,"The impact of organizational learning on project performance has received a great deal of attention in recent years. Process focus is recognized as one of five factors which help to promote organizational learning through out the process. A theoretical model is derived based upon prior researches in literature to examine the effects of organizational learning and process focus on project performance. The structural equation modeling was adopted to test the proposed hypotheses, and Taiwanese corporate IS companies served as examples. The results revealed that organization process focus has a positive impact on organizational learning, which in turn has a positive influence on project performance. Both organization process focus and organizational learning play the influence on project performance. These findings should give valuable information for managers to revisit their priorities in terms of the relative efforts in organization process focus and organization learning.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349429,no,,,no,,,no,undetermined,,no
155,The Impact of Organizational Learning on Lack of Team's Expertise Risk in Information Systems Projects,"During the past decade, information systems investment has grown rapidly worldwide and information systems project development has become one of the most important targets in e-business. Yet, the failure of information systems projects is a common occurrence in many organizations around the world. A theoretical model is derived based upon organizational learning theory and prior research in order to examine the effects of organizational learning on lack of team's expertise risk. A survey method is applied to test the hypotheses proposed by the research model, and Taiwanese corporate companies serve as examples. After survey by questionnaire and analyze the data by structure equation modeling, the result reveals that organizational learning has significantly negative impacts on all of the lack of development expertise risk, lack of domain expertise risk, and lack of general expertise risk. These findings support information systems managers with valuable information to revisit their priorities in terms of the relative efforts in organization learning.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402174,no,,,no,,,no,undetermined,,no
156,The need for self-managed access nodes in grid environments.,"The Grid is constantly growing and it is being used by more and more applications. In this scenario the entry node is an important component in the whole architecture and will become a contention point. In this paper we will demonstrate that the use of a self-managed layer on the entry node of a grid is necessary. A self-managed system can allow more jobs to be accepted and finished correctly. Since it's not acceptable for a grid middleware layer to lose jobs, we would normally need to prioritize the finishing/acceptance of jobs over the response time or the throughput. A prototype of what could be considered an autonomous system, is presented and tested over an installation of Globus Toolkit (GT4) and shows that we can greatly improve the performance of the original middleware by a factor of 30%. In this paper GT is used as an example but it could be added to any grid middleware",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148878,no,,,no,,,no,undetermined,,no
157,The TeraPaths Testbed: Exploring End-to-End Network QoS,"The TeraPaths project at Brookhaven National Laboratory (BNL) investigates the combination of DiffServ-based LAN QoS with WAN MPLS tunnels in creating end-to-end (host-to-host) virtual paths with bandwidth guarantees. These virtual paths prioritize, protect, and throttle network flows in accordance with site agreements and user requests, and prevent the disruptive effects that conventional network flows can cause in one another. This paper focuses on the TeraPaths testbed, a collection of end-site subnets connected through high-performance WANs, serving the research and software development needs of the TeraPaths project. The testbed is rapidly evolving towards a multiple end-site infrastructure, dedicated to QoS networking research, and it offers unique opportunities for experimentation with minimal or no impact on regular, production networking operations.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4444698,no,,,no,,,no,undetermined,,no
158,Toward the Use of Automated Static Analysis Alerts for Early Identification of Vulnerability- and Attack-prone Components,"Extensive research has shown that software metrics can be used to identify fault- and failure-prone components. These metrics can also give early indications of overall software quality. We seek to parallel the identification and prediction of fault- and failure-prone components in the reliability context with vulnerability- and attack-prone components in the security context. Our research will correlate the quantity and severity of alerts generated by source code static analyzers to vulnerabilities discovered by manual analyses and testing. A strong correlation may indicate that automated static analyzers (ASA), a potentially early technique for vulnerability identification in the development phase, can identify high risk areas in the software system. Based on the alerts, we may be able to predict the presence of more complex and abstract vulnerabilities involved with the design and operation of the software system. An early knowledge of vulnerability can allow software engineers to make informed risk management decisions and prioritize redesign, inspection, and testing efforts. This paper presents our research objective and methodology.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271764,no,,,no,,,no,undetermined,,no
159,Towards an empirical method of efficiency testing of system parts: A methodological study,"Current usability evaluation methods are essentially holistic in nature. However, engineers that apply a component-based software engineering approach might also be interested in understanding the usability of individual parts of an interactive system. This paper examines the efficiency dimension of usability by describing a method, which engineers can use to test, empirically and objectively, the physical interaction effort to operate components in a single device. The method looks at low-level events, such as button clicks, and attributes the physical effort associated with these interaction events to individual components in the system. This forms the basis for engineers to prioritise their improvement effort. The paper discusses face validity, content validity, criterion validity, and construct validity of the method. The discussion is set within the context of four usability tests, in which 40 users participated to evaluate the efficiency of four different versions of a mobile phone. The results of the study show that the method can provide a valid estimation of the physical interaction event effort users made when interacting with a specific part of a device.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8147077,no,,,no,,,no,undetermined,,no
161,Verification and Validation of (Real Time) COTS Products using Fault Injection Techniques,"With the goal of reducing time to market and project costs, the current trend of real time business and mission critical systems is evolving from the development of custom made applications to the use of commercial off the shelf (COTS) products. Obviously, the same confidence and quality of the custom made software components is expected from the commercial applications. In most cases, such products (COTS) are not designed with stringent timing and/or safety requirements as priorities. Thus, to decrease the gap between the use of custom made components and COTS components, this paper presents a methodology for evaluating COTS products in the scope of dependable, real time systems, through the application of fault injection techniques at key points of the software engineering process. By combining the use of robustness testing (fault injection at interface level) with software fault injection (using educated fault injection operators), a COTS component can be assessed in the context of the system it will belong to, with special emphasis given to timing and safety constraints that are usually imposed by the target real time dependable environment. In the course of this work, three case studies have been performed to assess the methodology using realistic scenarios that used common COTS products. Results for one case study are presented",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127318,no,,,no,,,no,undetermined,,no
162,"""It's Not the Pants, it's the People in the Pants"" Learnings from the Gap Agile Transformation What Worked, How We Did it, and What Still Puzzles Us","After 7 years of traditional IT delivery, Gap Inc.Direct decided to adopt Agile. This experience report discusses three key factors that contributed to our successful (and ongoing) Agile transformation: 1. Ambitious Pilot Project, 2. Massive Investment in Continuous Integration, 3. Rethinking our Assets. The choices we made might seem risky and even counter-intuitive, but understanding them could help other organizations consider different points of view and priorities as they embark on the transition to Agile. Additionally, we will identify ongoing challenges and what is left in our transformation backlog.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599459,no,,,no,,,no,undetermined,,no
163,A dynamic scheduler for balancing HPC applications,"Load imbalance cause significant performance degradation in High Performance Computing applications. In our previous work we showed that load imbalance can be alleviated by modern MT processors that provide mechanisms for controlling the allocation of processors internal resources. In that work, we applied static, hand-tuned resource allocations to balance HPC applications, providing improvements for benchmarks and real applications. In this paper we propose a dynamic process scheduler for the Linux kernel that automatically and transparently balances HPC applications according to their behavior. We tested our new scheduler on an IBM POWER5 machine, which provides a software-controlled prioritization mechanism that allows us to bias the processor resource allocation. Our experiments show that the scheduler reduces the imbalance of HPC applications, achieving results similar to the ones obtained by hand-tuning the applications (up to 16%). Moreover, our solution reduces the application's execution time combining effect of load balance and high responsive scheduling.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5217785,no,,,no,,,no,undetermined,,no
164,Adaptive Test Question Selection for Web-Based Educational System,"In this paper we present a method proposed to select test questions adapting to individual needs of students in the context of Web-based educational system. It functions as a combination of three particular methods. First one is based on course structure and focuses on the selection of the most appropriate topic for learning, second uses the Item Response Theory to select k-best questions with adequate difficulty for particular learner and the last is based on usage history and prioritizes questions according to specific strategies, e.g. to filter out the questions that was recently asked. We describe how these methods evaluate user answers to gather information concerning their characteristics for more precise selection of further questions. We evaluated proposed method within our Web-based system called Flip on domain of functional programming.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724867,no,,,no,,,no,undetermined,,no
165,Addressing Low Base Rates in Intrusion Detection via Uncertainty-Bounding Multi-Step Analysis,"Existing approaches to characterizing intrusion detection systems focus on performance under test conditions. While it is well-understood that operational conditions may differ from test conditions, little attention has been paid to the question of assessing the effect on IDS results of parameter estimation errors resulting from these differences. In this paper we consider this question in the context of multi-step attacks. We derive simulated distributions of the posterior probability of exploit given the observation of a series of alerts and bounds on the posterior uncertainty given a particular distribution of the model parameters. Knowledge of such bounds introduces the novel prospect of a confidence versus agility tradeoff in IDS administration. Such a tradeoff could give administrators flexibility in IDS configuration, allowing them to choose detection confidence at the price of detection latency, according to organizational priorities.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721564,no,,,no,,,no,undetermined,,no
166,Agent multiplication: An economical large-scale testing environment for system management solutions,"System management solutions are designed to scale to thousands or more machines and networked devices. However, it is challenging to test and verify the proper operation and scalability of management software given the limited resources of a testing lab. We have developed a method called agent multiplication, in which one physical testing machine is used to represent hundreds of client machines. This provides the necessary client load to test the performance and scalability of the management software and server within limited resources. In addition, our approach guarantees that the test environment remains consistent between test runs, ensuring that test results can be meaningfully compared. We used agent multiplication to test and verify the operation of a server managing 4,000 systems. We exercised the server functions with only 8 test machines. Applying this test environment to an early version of a real enterprise system management solution we were able to uncover critical bugs, resolve race conditions, and examine and adjust thread prioritization levels for improved performance.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536552,no,,,no,,,no,undetermined,,no
169,Application-Level QoS: Improving Video Conferencing Quality through Sending the Best Packet Next,"In a traditional network stack, data from an application is transmitted in the order that it is received. An algorithm is proposed where information about the priority of packets and expiry times is used by the transport layer to reorder or discard packets at the time of transmission to optimise the use of available bandwidth. This can be used for video conferencing to prioritise important data. This scheme is implemented and compared to unmodified datagram congestion control protocol (DCCP). This algorithm is implemented as an interface to DCCP and tested using traffic modelled on video conferencing software. The results show improvement can be made to video conferencing during periods of congestion - substantially more audio packets arrive on time with the algorithm, which leads to higher quality video conferencing. In many cases video packet arrival rate also increases and adopting the algorithm gives improvements to video conferencing that are better than using unmodified queuing for DCCP. The algorithm proposed is implemented on the server only, so benefits can be obtained on the client without changes being required to the client.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756479,no,,,no,,,no,undetermined,,no
172,Change Priority Determination in IT Service Management Based on Risk Exposure,"In the Change Management process within IT Service Management, some activities need to evaluate the risk exposure associated with changes to be made to the infrastructure and services. The paper presents a method to evaluate risk exposure associated with a change. Further, we show how to use the risk exposure metric to automatically assign priorities to changes. The formal model developed for this purpose captures the business perspective by using financial metrics in the evaluation of risk. Thus the method is an example of Business-Driven IT Management. A case study, performed in conjunction with a large IT service provider, is reported and provides good results when compared to decisions made by human managers.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4805134,no,,,no,,,no,undetermined,,no
173,Choosing the Right Prioritisation Method,"There are many methods available for prioritising software requirements. Choosing the most suitable one can often be quite difficult. A number of factors need to be considered such as the project development methodology being used, the amount of time available, the amount of information known about requirements, the stage of the project and the degree of information about priority required. This paper examines the type of information available at different stages in a project and matches it to the properties of prioritisation methods. It then recommends the usage of specific prioritisation methods at certain stages of a project.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483241,no,,,no,,,no,undetermined,,no
174,Collaborative group membership and access control for JXTA,"This paper presents a proposal for group membership and access control services for JXTA, both based on the principle of self-organization and collaboration of peer group members. The need for collaboration strengthens the resistance against free riding and eases management of revocation data. The proposal prioritizes group autonomy and makes use of the concepts of web of trust and small world phenomenon in order to achieve its ends, distancing itself from approaches based on centralized PKI models or trusted third parties external to the group. It also offers an alternative to the basic group membership services distributed with the JXTA platform implementations.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554399,no,,,no,,,no,undetermined,,no
175,Creating Agile Streams for Business & Technical Value,"Have you ever played the role of business owner and found yourself between ""a rock and a hard place"" of organizational politics when prioritizing backlog features? The Agile Stream approach negates those politics by dedicating development teams to organizational units and allowing those teams to continue working, iteration after iteration, as long as they continue delivering business value.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599466,no,,,no,,,no,undetermined,,no
176,Effective RTL Method to Develop On-Line Self-Test Routine for the Processors Using the Wavelet Transform,"In this paper, we introduce a new efficient register transfer level (RTL) method to develop on-line self- test routines. We consider some prioritizations to select the components and instructions of the processor. In addition, we choose test patterns based on spectral RTL test pattern generation (TPG) strategy. For the purpose of spectral analysis, we use the wavelet transform. Also, we use a few extra instructions for the purpose of the signature monitoring to detect control flow errors. We demonstrate that the combination of these three strategies is effective for developing small test programs with high fault coverage in a small test development time. In this case, we only need the instruction set architecture (ISA) and RTL information. Our method not only provides a simple and fast algorithm for on-line self-test applications, also gains the advantages of utilizing lower memory and reducing the test generation time complexities in comparison with proposed methods so far. We focus on the application of this approach for Parwan processor. We develop a self-test routine using our proposed method for Parwan processor and demonstrate the effectiveness of our proposed methodology for on-line testing by presenting experimental results for Parwan processor.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529795,no,,,no,,,no,undetermined,,no
177,Evaluating ALPHAN: A Communication Protocol for Haptic Interaction,"In our previous work we introduced a novel application layer protocol, named ALPHAN, for haptic data communication. The protocol is characterized by three distinguished features: first, it is designed at the application layer to enable the application to define and control the networking parameters. Second, it is made highly customizable using XML-based descriptions. Finally, the protocol supports multi-buffering mechanisms to prioritize the communicated information. In this paper, we present a thorough evaluation of the protocol using a collaborative haptic game; the balance ball game that we developed is for this purpose. The performance metrics and the test bed of the protocol evaluation are also discussed. Finally, we comment on our findings and provide directions for prospective research.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479972,no,,,no,,,no,undetermined,,no
180,mod kaPoW: Protecting the web with transparent proof-of-work,"Attacks from automated Web clients are a significant problem on the Internet. Web sites often employ Turing tests known as CAPTCHAs to combat automated agents. Unfortunately, such defenses require frequent human user input, are becoming less effective as computer vision techniques improve, and can be subverted by adversaries willing to hire humans to solve challenges. Several alternative defenses based upon cryptographic methods rather than human input have been proposed to achieve the same goals. Such ""proof-of-work"" techniques prioritize clients based on their willingness to solve computational challenges of client-specific difficulty set by the server. Unfortunately, few proof-of-work schemes have been deployed since they require wide-scale adoption of special client software to operate properly. To address these problems we present mocLkaPoW, a novel system that has the efficiency and human-transparency of proof-of-work schemes as well as the software backwards-compatibility of CAPTCHA schemes. The system leverages common Web technologies to deliver a challenge, solve it, and submit the client response, while providing accessibility for legacy clients. This paper describes and evaluates a prototype of this system.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544602,no,,,no,,,no,undetermined,,no
181,Model for optimizing software testing period using non homogenous poisson process based on cumulative test case prioritization,"Most of the software organizations has trouble when deciding the release dates their product. This difficulty is due to the fact that an under tested software could lead to many bugs propping up at the client side which would in turn lead to expensive bug-fixes and more importantly loss of customer goodwill. On the other hand, testing beyond certain time would lead to loss of revenue to the organization due to the dilution of the early bird advantage. The aim of our paper is optimizes the time and cost of entire software. In this paper we used non homogeneous Poisson process model based on cumulative priority. Our paper also tries to answer when to release any software.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766422,no,,,no,,,no,undetermined,,no
182,"Perspective on Embedded Systems: Challenges, Solutions and Research Priorities","This paper introduces THALES vision and research priorities for embedded systems and illustrates them through presentations of solutions and on-going research projects and initiatives. Thales effort related to mission-critical systems is focused on advanced high-performance embedded computing platforms, on middleware technologies, on software systems design and verification tools for safety and security and on the emergence of open standards in these domains. THALES is also actively contributing to the development of innovation eco-systems: the Joint Undertaking ARTEMIS in Europe; the Pole de Competitivite SYSTEM@TIC PARIS REGION in France.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484650,no,,,no,,,no,undetermined,,no
184,Providing the Guideline of Determining Quality Checklists Priorities Based on Evaluation Records of Software Products,"COTS (commercial-off-the-shelf) software products are usually provided in a packaged style without the source code but with many ready-to-use functions. Generally, their vendors are reluctant to disclose the source code. Thus, the major way of quality evaluation and certification requires dynamic behavior testing, essentially black-box testing. Since observing every aspect of external software behavior is almost impossible, it is crucial to designate an adequate range for quality evaluation such as an adequate number of quality checklists or product quality metrics for external behavior testing. Hence, to establish rules of selecting quality evaluation criteria in systematic ways, there have been attempts to analyze and utilize the past records of software evaluation. In this paper, multiple characteristics of software are mapped as nodes to affect and determine the priority ranks of external software quality metrics on Bayesian belief network. The nodes are set to be under the influence of multiple inheritances so that every external characteristic of COTS software is considered thoroughly.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724545,no,,,no,,yes,no,undetermined,,no
186,Ranking Attack-Prone Components with a Predictive Model,"Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. An early security risk analysis that ranks software components by probability of being attacked can provide an affordable means to prioritizing fortification efforts to the highest risk components. We created a predictive model using classification and regression trees and the following internal metrics: quantity of Klocwork static analysis warnings, file coupling, and quantity of changed and added lines of code. We validated the model against pre-release security testing failures on a large commercial telecommunications system. The model assigned a probability of attack to each file where upon ranking the probabilities in descending order we found that 72% of the attack-prone files are in the top 10% of the ranked files and 90% in the top 20% of the files.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700353,no,,,no,,,no,undetermined,,no
187,Scheduling Product Line Features for Effective Roadmapping,"Large industrial product lines may produce tens of thousands of variants each year. Each variant typically contains both reusable assets as well as product specific code created by different organizational units. To produce this vast number of variants the organizational resources must be used efficiently. For roadmapping this means an ability to schedule production of reusable assets so that all variants can be completed according to their requirements. When aiming for centralized variability management, roadmapping requires effective management of product line feature dependences and priorities. In this paper, we first introduce the problems haunting feature roadmapping in industrial product lines. Then we investigate how these problems can be solved using a novel approach for organizing product lines based on our practical experiences. Finally, we discuss our experiences and compare our approach with results by other researchers.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724548,no,,,no,,,no,undetermined,,no
188,Scheduling Timed Modules for Correct Resource Sharing,"Real-time embedded systems typically include concurrent tasks of different priorities with time-dependent operations accessing common resources. In this context, unsynchronized parallel executions may lead to hazard situations caused by e.g., race conditions. To be able to detect such faulty system behaviors before implementation, we introduce a unified model of resource constrained, scheduled real-time system descriptions, in Alur's and Henzinger's rigorous framework of timed reactive modules. We take a component-based design perspective and construct the realtime system model, by refinement, as a composition of realtime periodic preemptible tasks with encoded functionality, and a fixed-priority scheduler, all modeled as timed modules. For the model, we express the notions of race condition and redundant locking, formally, as invariance properties that can be verified by model-checking.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539537,no,,,no,,,no,undetermined,,no
189,Slack-based global multiprocessor scheduling of aperiodic tasks in parallel embedded real-time systems,"We provide a constant time schedulability test and priority assignment algorithm for an on-line multiprocessor server handling aperiodic tasks. Dhall's effect is avoided by dividing tasks in two priority classes based on their utilization: heavy and light. The improvement in this paper is due to assigning priority of light tasks based on slack - not on deadlines. We prove that if the load on the multiprocessor stays below (3 - radic5)/2 ap 38.197%, the server can accept an incoming aperiodic task and guarantee that the deadlines of all accepted tasks will be met. This is better than the current state-of- the-art algorithm where the priorities of light tasks are based on deadlines (the corresponding bound is in that case 35.425%).",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493574,no,,,no,,,no,undetermined,,no
190,Software development methods and usability: Perspectives from a survey in the software industry in Norway,"This paper investigates the relationship between software development methodologies and usability. The point of departure is the assumption that two important disciplines in software development, one of software development methods (SDMs) and one of usability work, are not integrated in industrial software projects.Building on previous research we investigate two questions; (1) Will software companies generally acknowledge the importance of usability, but not prioritise it in industrial projects? and (2) To what degree are software development methods and usability perceived by practitioners as being integrated? To this end a survey in the Norwegian IT industry was conducted. From a sample of 259 companies we received responses from 78 companies.In response to our first research question, our findings show that although there is a positive bias towards usability, the importance of usability testing is perceived to be much less than that of usability requirements. Given the strong time and cost pressures associated with the software industry, we believe that these results highlight that there is a gap between intention and reality. Regarding our second research question our survey revealed that companies perceive usability and software development methods to be integrated. This is in contrast to earlier research, which, somewhat pessimistically, has argued for the existence of two different cultures, one of software development and one of usability. The findings give hope for the future, in particular because the general use of system development methods are pragmatic and adaptable.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8149869,no,,,no,,,no,undetermined,,no
191,Software Quality model Analysis Program,"It is vital that data is obtained so that actions can be taken to improve the performance. Such improvement can be measured in terms of improved quality, increased customer satisfaction and decreased cost of quality. Different researchers have proposed software quality models to help measure the quality of software products. These models often include metrics for this purpose. Some of the classical and recent models are discussed and analyzed in this paper showing the points of strength and weakness of each model type. A new comprehensive model is proposed and analyzed. A complete solution is discussed through the paper to enable an effective and efficient use of the proposed model to help the development team in prioritizing the important metrics while developing the software products according to some inputs from the user and the objectives of the software being developed. The solution developed is called the quality model analysis program (QAP) and is a fuzzy system that weights the proposed model attributes according to certain rules. The solution enables software project managers to better utilize their resources and take specific actions to better improve the quality of the software produced.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4773015,no,,,no,,,no,undetermined,,no
192,Software Quality Requirements: How to Balance Competing Priorities,"The elicitation, analysis, and specification of quality requirements involve careful balancing of a broad spectrum of competing priorities. Developers must therefore focus on identifying qualities and designing solutions that optimize the product's value to its stakeholders.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4455627,no,,,no,,,no,undetermined,,no
196,The use of explicit congestion notification to shape traffic of an intelligent satellite system,"Since the official standardization of explicit congestion notification (ECN) in 2001, the differentiated service (DS) bits 6 and 7 in packets are now classified for purposes of shaping and prioritization. These classifications are used to mark packet streams for controlling traffic flow in an intelligent satellite system (ISS). Using a forward link connection between a transmitting ground terminal to a geostationary satellite (GEO) acting as a relay, to a receiving hub back on the earth; a traffic shaping software which evaluates the ECN type or classification is used to control traffic flow and the results are observed.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463618,no,,,no,,,no,undetermined,,no
197,Two Case Studies of User Experience Design and Agile Development,"How can user experience design (UED) practices be leveraged in agile development to improve product usability? UED practices identify the needs and goals of the user through user research and testing. By incorporating UED in agile development, user research and testing can be utilized to prioritize features in the product backlog and to iteratively refine designs to achieve better usability. Furthermore, integrating UED and Agile processes can be accomplished with little or no impact on release schedules. The cases studies presented in this paper describe two examples of UED and agile integration at VeriSign.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599534,no,,,no,,,no,undetermined,,no
198,Using Statistical Models to Predict Software Regressions,"Incorrect changes made to the stable parts of a software system can cause failures - software regressions. Early detection of faulty code changes can be beneficial for the quality of a software system when these errors can be fixed before the system is released. In this paper, a statistical model for predicting software regressions is proposed. The model predicts risk of regression for a code change by using software metrics: type and size of the change, number of affected components, dependency metrics, developerpsilas experience and code metrics of the affected components. Prediction results could be used to prioritize testing of changes: the higher is the risk of regression for the change, the more thorough testing it should receive.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700331,no,,,no,,,no,undetermined,,no
200,A new method of test data generation for branch coverage in software testing based on EPDG and Genetic Algorithm,"A new method called EPDG-GA which utilizes the edge partitions dominator graph (EPDG) and genetic algorithm (GA) for branch coverage testing is presented in this paper. First, a set of critical branches (CBs) are obtained by analyzing the EPDG of the tested program, while covering all the CBs implies covering all the branches of the control flow graph (CFG). Then, the fitness functions are instrumented in the right position by analyzing the pre-dominator tree (PreDT), and two metrics are developed to prioritize the CBs. Coverage-Table is established to record the CBs information and keeps track of whether a branch is executed or not. GA is used to generate test data to cover CBs so as to cover all the branches. The comparison results show that this approach is more efficient than random testing approach.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276897,no,,,no,,yes,no,undetermined,,no
201,A Survey of Coverage-Based Testing Tools,"Test coverage is sometimes used to measure how thoroughly software is tested and developers and vendors sometimes use it to indicate their confidence in the readiness of their software. This survey studies and compares 17 coverage-based testing tools primarily focusing on, but not restricted to, coverage measurement. We also survey features such as program prioritization for testing, assistance in debugging, automatic generation of test cases and customization of test reports. Such features make tools more useful and practical, especially for large-scale, commercial software applications. Our initial motivations were both to understand the available test coverage tools and to compare them to a tool that we have developed, called eXVantage (a tool suite that includes code coverage testing, debugging, performance profiling and reporting). Our study shows that each tool has some unique features tailored to its application domains. The readers may use this study to help pick the right coverage testing tools for their needs and environment. This paper is also valuable to those who are new to the practice and the art of software coverage testing, as well as those who want to understand the gap between industry and academia.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8130777,no,,,yes,,,no,undetermined,,no
203,Agent Based Replica Placement in a Data Grid Environement,"In a data grid, large quantities of data files are produced and data replication is applied to reduce data access time. Determining when and where to replicate data in order to meet performance goals in grid systems with many users and files, dynamic network and resource characteristics and changing user behavior is difficult. Therefore efficiency and fast access to replicated data are influenced by the location of the resource holding the replica. In this paper, we present an agent based replica placement algorithm to determine the candidate site for the placement of replica. An agent is deployed at each site holding the master copies of the shared data files. To create a replica, each agent prioritizes the resources in the grid based on the resource configuration, bandwidth in the network and the demand for the replica at their sites and then creates a replica at suitable resource locations. We have carried out the simulation using GridSim Toolkit-4.0 for EU Data Grid Testbed1. The simulation results show that the aggregated data transfer time and the execution time for jobs at various resources is less for agent based replica placement.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231873,no,,,no,,,no,undetermined,,no
204,BugFix: A learning-based tool to assist developers in fixing bugs,"We present a tool called BugFix that can assist developers in fixing program bugs. Our tool automatically analyzes the debugging situation at a statement and reports a prioritized list of relevant bug-fix suggestions that are likely to guide the developer to an appropriate fix at that statement. BugFix incorporates ideas from machine learning to automatically learn from new debugging situations and bug fixes over time. This enables more effective prediction of the most relevant bug-fix suggestions for newly-encountered debugging situations. The tool takes into account the static structure of a statement, the dynamic values used at that statement by both passing and failing runs, and the interesting value mapping pairs associated with that statement. We present a case study illustrating the efficacy of BugFix in helping developers to fix bugs.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090029,no,,,no,,,no,undetermined,,no
207,Dynamic admission control and path allocation for SLAs in DiffServ networks,"Today's converged networks are mainly characterized by their support of real-time and high priority traffic requiring a certain level of quality of service (QoS). In this context, traffic classification and prioritization are key features in providing preferential treatments of the traffic in the core of the network. In this paper, we address the joint problem of path allocation and admission control (JPAC) of new Service Level Agreements (SLA) in a DiffServ domain. In order to maximize the resources utilization and the number of admitted SLAs in the network, we consider a statistical bandwidth constraints allowing for a certain overbooking over the network's links. SLAs' admissibility decisions are based on solving to optimality an integer linear programming (ILP) model. When tested by simulations, numerical results confirm that the proposed model can be solved to optimality for real-sized instances within acceptable computation times and substantially reduces the SLAs blocking probability, compared to a the Greedy mechanism proposed in the literature.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090106,no,,,no,,,no,undetermined,,no
209,Formal Study of Prioritized Service Compositions,"This paper presents an enhanced derivation procedure to obtain a system of services, from a given choreography. In addition to the basic framework, we introduce several situations where nondeterminism appears and it is resolved by using a dynamic prioritized system. The priority policy is based on several parameters such as the request dispatching, the response time, the quality of the response, etc. These parameters are identified as resources used by a utility function, which determines the priority of each possible option in a nondeterministic choice.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5633969,no,,,no,,,no,undetermined,,no
210,From Cradle to Sprint: Creating a Full-Lifecycle Request Pipeline at Nationwide Insurance,"After a successful transition from a prescriptive waterfall process to Scrum and XP, the Corporate Internet Solutions group at Nationwide Insurance found velocity and efficiency stumbling due to the competing and vague priorities of corporate silos. This presentation discusses how the team evolved the traditional Scrum process to better manage 17 dependent projects, and reluctant internal business partners, through a combination of activities including clear Pre-Discovery activities, scenario planning, RITE usability testing, and kanban-style visual management systems.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5261080,no,,,no,,,no,undetermined,,no
211,Fundamentals of risk based inspection __?? a practical approach,"New API structural integrity management recommended practice and post-hurricane inspection bulletin, MMS proposed changes to CFR's on decommissioning, and increased interest in developing risk based inspection plans for offshore assets require better understanding of reliability principles. This paper will discuss the fundamentals of RBI from a practical viewpoint and gives details of recently published applicable DNV offshore standards. Offshore structures must be inspected to maintain an acceptable safety level throughout their lifetime. Inspections have traditionally been based upon experience, and judgment of likelihood and consequence of failure. Risk based inspection planning (RBI) for structures, as developed by DNV, represents a systematic, qualitative and quantitative approach which combines theoretical models, test results and in-service experiences. The method is specially developed for application to all types of offshore structures including jackets, jack-ups, TLPs, FPSOs, spars, semi-submersibles, GBSS, and subsea templates. The basis of RBI is to prioritize individual items and systems by considering the associated risks. Attention is given to high risk items, while low risk items receive a more appropriately lesser level of inspection. RBI focuses on cost optimization in all associated activities, to ensure a cost optimal inspection program. The RBI analysis is performed in two steps, comprising a risk screening and a subsequent inspection scheduling. Dedicated software models are utilized to establish the Risk Matrix and to calculate the ?__time to next inspection?__. The final deliverable of an RBI analysis is an inspection plan, in which inspection efforts are prioritized from an overall risk perspective.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422330,no,,,no,,,no,undetermined,,no
212,Host based intrusion detection using RBF neural networks,A novel approach of host based intrusion detection is suggested in this paper that uses Radial basis Functions Neural Networks as profile containers. The system works by using system calls made by privileged UNIX processes and trains the neural network on its basis. An algorithm is proposed that prioritize the speed and efficiency of the training phase and also limits the false alarm rate. In the detection phase the algorithm provides implementation of window size to detect intrusions that are temporally located. Also a threshold is implemented that is altered on basis of the process behavior. The system is tested with attacks that target different intrusion scenarios. The result shows that the radial Basis Functions Neural Networks provide better detection rate and very low training time as compared to other soft computing methods. The robustness of the training phase is evident by low false alarm rate and high detection capability depicted by the application.,2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353204,no,,,no,,,no,undetermined,,no
214,Implementation of the Software Quality Ranks method in the legacy product development environment,"Software quality ranks (SQR) is an important method to manage and improve software quality. Component software quality has a major influence in development project lead time and cost. SQR enables better management and visibility of the quality effort associated with the component implementation. It also provides a roadmap for continuous improvement leading to value add quality attributes like low maintenance, self optimizing software and short development lifecycles. SQR method focuses attention to prioritizing the quality investment on design component level through different quality assurance mechanisms (basic test, code review, desk checks, documentation and other actions). The resulting design delivery to verification phase will be more predictable quality software with shorter lead-time and time-to-market (TTM).",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206363,no,,,no,,,no,undetermined,,no
216,Keynote: Security Engineering: Developments and Directions,"Security Engineering is a critical component of systems engineering. When complex and large systems are put together, one needs to ensure that the systems are secure. Security engineering methodologies include gathering the security requirements, specifying the security policies, designing the security model, identifying the security critical components of the system design, security verification and validation and security testing. Before installation, one needs to develop a concept of operation (CONOPS) as well as carry out certification and accreditation. Much of the previous work in security engineering has focused on end to end security. That is, the organization needs to ensure that the applications, database systems, operating systems and networks have to be secure. In addition, one needs to ensure security when the subsystems are composed to form a larger system. More recently with open systems and the Web, secure system development is taking a whole new direction. The Office of the Deputy Assistant Secretary of Defense in the United States (Information and Identity Assurance) has stated that ""the Department of Defense's (DoD) policy, planning, and war fighting capabilities are heavily dependent on the information technology foundation provided by the Global Information Grid (GIG). However, the GIG was built for business efficiency instead of mission assurance against sophisticated adversaries who have demonstrated intent and proven their ability to use cyberspace as a tool for espionage and criminal theft of data. GIG mission assurance works to ensure the DoD is able to accomplish its critical missions when networks, services, or information are unavailable, degraded, or distrusted."" To meet the needs of mission assurance challenges, President's (George W. Bush) cyber plan (CNCI) has listed the area of developing multipronged approaches to supply chain risk management as one of the priorities. CNCI states that the reality of global supply chains presents significant challenges in thwarting counterfeit, or maliciously designed hardware and software products. To overcome such challenges and support successful mission assurance we need to design flexible and secure systems whose components may be untrusted or faulty. We need to achieve the secure operation of mission critical systems constructed from untrusted, semitrusted and fully trusted components for successful mission assurance. This keynote address will discuss the developments in security engineering from requirements, to policy to model to design to verification to testing as well as developing CONOPS and conducting certification and accreditation. System evaluation, usability and metrics related issues will also be discussed. Finally we will discuss the changes that have to be made to security engineering to support the next generation of secure systems for mission critical applications.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325402,no,,,no,,,no,undetermined,,no
217,Language Identification from an Indian Multilingual Document Using Profile Features,"In order to reach a larger cross section of people, it is necessary that a document should be composed of text contents in different languages. But on the other hand, this causes practical difficulty in OCRing such a document, because the language type of the text should be pre-determined, before employing a particular OCR. In this research work, this problem of recognizing the language of the text content is addressed, however it is perhaps impossible to design a single recognizer which can identify a large number of scripts/languages. As a via media, in this research we have proposed to work on the prioritized requirements of a particular region, for instance in Karnataka state in India,generally any document including official ones, would contain the text in three languages-English-the language of general importance, Hindi-the language of National importance and Kannada -the language of State/Regional importance. We have proposed to learn identifying the language of the text by thoroughly understanding the nature of top and bottom profiles of the printed text lines in these three languages.Experimentation conducted involved 800 text lines for learning and 600 text lines for testing. The performance has turned out to be 95.4%.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804543,no,,,no,,,no,undetermined,,no
218,Lightweight Elicitation and Analysis of Software Product Quality Goals: A Multiple Industrial Case Study,"We developed and used a method that gathers relevant stakeholders to elicit, prioritize, and elaborate the quality goals of a software product. It is designed to be lightweight and easy to learn compared to methods for a more comprehensive analysis of non-functional requirements. The method and the resulting quality goals are meant especially for improving the software product management process. We used it in four software product companies, and report lessons learned and evaluation of the method based on practitioners' comments. We found it better to set the goals first for the product in general before discussing a specific release project. In addition to identifying goals that needed improvement, the practitioners considered identifying already achieved goals relevant, but they were neg- lected unless explicitly considered. Using ISO 9126 as a checklist after brainstorming did not add many goals. Prioritization was challenging due to numerous relevant perspectives. Conceiving measures for impor- tant goals seemed to concretize them.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457326,no,,,no,,,no,undetermined,,no
219,Measurement and control for risk-based test cases and activities,"Risk-based testing is an approach that consists of a set of activities regarding risk factors identification related to software requirements. Once identified, the risks are prioritized according to its likelihood and impact, and the test cases are projected based on the strategies for treatment of the identified risk factors. Then, test efforts are continuously adjusted according the risk monitoring. Most risk-based testing approaches focuses on activities related to risk identification, analysis and prioritizing. However, metrics are fundamental as they quantify characteristics of a process or product and support software project management activities. In this light, this paper proposes and discusses risk-based testing metrics to measure and control test cases and test activities progress, efforts and costs.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813802,no,,,no,,,no,undetermined,,no
220,Mesh your Senses: Multimedia Applications over WiFi-based Wireless Mesh Networks,"This demo aims at (i) validating the design choices we have made in conceiving and deploying the WING testbed, and (ii) showing the capability of out software toolkit to properly support heterogeneous multimedia applications. Additionally, the mesh networking toolkit's fault management features is demonstrated. We hope that our wireless mesh networking toolkit is considered by both researchers and practitioners as platform of choice to test innovative solutions and to provide end-users with wireless connectivity. WING is an experimental multi-radio WMN testbed designed and built exploiting commodity hardware and open-source software components. WING implements a flexible and scalable WMN architecture capable of supporting next-generation Internet services with a particular focus on multimedia applications. The WING project aims at providing an open-platform on top of which innovative solution can be implemented and tested in a realistic environment. Currently, the testbed consist of 10 nodes deployed at CREATE-NET premises and implementing a two-tiers architecture. Other well-known IEEE 802.11-based WMNs include Roofnet, Hyacinth, Microsoft's MCL, and Meraki. We establish the uniqueness of our mesh solution in that it is capable of achieving both service differentiation and performance isolation in IEEE 802.11-based WMNs. While not providing strict QoS performance bounds, the proposed scheme aims at enhancing the perceived quality of experience by combining opportunistic scheduling and packet aggregation and by implementing a DiffServ-like architecture in order to provide traffic prioritization.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172945,no,,,no,,,no,undetermined,,no
221,Performance impact analysis with KPP using application response measurement in E-government systems,"In this paper, the performance impact analysis of e-government systems with key performance parameters is being considered. Meaningful impact analysis in sustained government systems is required for considering non-functional requirements and functional requirements. Performance requirements are a critical component of non-functional areas. For example, if a new system change is set to the system, the impact in terms of the response time must be implemented in each sub-system. In this paper, an XML-based framework can be used to analyze performance impacts on sub-systems and can provide a scheme to enhance impact analysis by performance monitoring using application response measurement. Through a health system example as a case study, a performance requirement model to describe extended trees and adapting analysis result of performance monitoring using application response measurement and XML tree representation are addressed. This paper also proposes a scheme for prioritized processing and an algorithm for effectively enhancing impact analysis in a timely fashion.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306282,no,,,no,,,no,undetermined,,no
222,Policy-Based Network Management in Home Area Networks: Interim Test Results,"This paper argues that Home Area Networks (HANs) are a good candidate for advanced network management automation techniques, such as Policy-Based Network Management (PBNM). What is proposed is a simple use of policy based network management to introduce some level of Quality of Service (QoS) and Security management in the HAN, whilst hiding this complexity from the home user. In this paper we have presented the interim test results of our research experiments (based on a scenario) using the HAN testbed. After using policies to prioritize different traffic, packet loss decreased to 30% and VoIP quality improved dramatically without employing any intelligent bandwidth allocation technique.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384722,no,,,no,,,no,undetermined,,no
223,Predicting Attack-prone Components,"Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. This limitation necessitates security risk management where security efforts are prioritized to the highest risk vulnerabilities that cause the most damage to the end user. We created a predictive model that identifies the software components that pose the highest security risk in order to prioritize security fortification efforts. The input variables to our model are available early in the software life cycle and include security-related static analysis tool warnings, code churn and size, and faults identified by manual inspections. These metrics are validated against vulnerabilities reported by testing and those found in the field. We evaluated our model on a large Cisco software system and found that 75.6% of the system's vulnerable components are in the top 18.6% of the components predicted to be vulnerable. The model's false positive rate is 47.4% of this top 18.6% or 9.1% of the total system components. We quantified the goodness of fit of our model to the Cisco data set using a receiver operating characteristic curve that shows 94.4% of the area is under the curve.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815350,no,,,no,,,no,undetermined,,no
230,Quasi-Renewal Time-Delay Fault-Removal Consideration in Software Reliability Modeling,"Software reliability growth models based on a nonhomogeneous Poisson process (NHPP) have been considered as one of the most effective among various models since they integrate the information regarding testing and debugging activities observed in the testing phase into the software reliability model. Although most of the existing NHPP models have progressed successfully in their estimation/prediction accuracies by modifying the assumptions with regard to the testing process, these models were developed based on the instantaneous fault-removal assumption. In this paper, we develop a generalized NHPP software reliability model considering quasi-renewal time-delay fault removal. The quasi-renewal process is employed to estimate the time delay due to identifying and prioritizing the detected faults before actual code change in the software reliability assessment. Model formulation based on the quasi-renewal time-delay assumption is provided, and the generalized mean value function (MVF) for the proposed model is derived by using the method of steps. The general solution of the MVFs for the proposed model is also obtained for some specific existing models. The numerical examples, based on a software failure data set, show that the consideration of quasi-renewal time-delay fault-removal assumption improves the descriptive properties of the model, which means that the length of time delay is getting decreased since testers and programmers adapt themselves to the working environment as testing and debugging activities are in progress.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694128,no,,,no,,,no,undetermined,,no
232,Simple Time-to-Failure Estimation Techniques for Reliability and Maintenance of Equipment,"Proper reliability and maintenance best practice processes have a direct impact on equipment availability, throughput capacity, and spare inventories. The purpose of the time-to-failure estimation (TTFE) technique is to provide a tool for engineers and technicians for risk-based reporting of condition- based maintenance tests and inspections. Through the proper application of this technique, corrective action may be prioritized improving the effectiveness of the maintenance program. Instead of stakeholders being required to make decisions based upon experience only, equipment failure, and repair history can be used to enhance the process, improving the availability of critical equipment.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5191412,no,,,no,,,no,undetermined,,no
233,Surviving Insecure IT: Effective Patch Management,"The amount of time to protect enterprise systems against potential vulnerability continues to shrink. Enterprises need an effective patch management mechanism to survive the insecure IT environment. Effective patch management is a systematic and repeatable patch distribution process which includes establishing timely and practical alerts, receiving notification of patches or discovering them, downloading patches and documentation, assessing and prioritizing vulnerabilities, performing testing, deploying patches, and auditing.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804050,no,,,no,,,no,undetermined,,no
235,Techniques for building excellent operator machine interfaces (OMI),"Establishing a process to continually improve understanding of operator requirements - the why as well as the how - is key to developing an optimal Operator-Machine Interface (OMI) for a large program. The P-8A program is one example of a complex software development, with over 2 million lines of code and a unique and complex OMI. The OMI itself is used as a tool for achieving and is designed in parallel with understanding end goals, operator decision making chains, and thought processes. The OMI can be designed to support and anticipate information needs and alert operators to unusual occurrences. Operator actions and decision-making techniques can be discovered by using expert operators and capturing their knowledge through the use of multiple tools to identify the problem set: storytelling, decision mapping, and is/is not matrices. When a solution set has been identified and agreed to by the customer, resources allocated, and schedule set, a standard process captures and prioritizes further changes, identities their impact, and decides on implementation. Documents describing design and processes and a design description document describing the current version of OMI are made accessible to stakeholders at all times.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5317781,no,,,no,,,no,undetermined,,no
238,Testing Processes in Business-Critical Chain Software Lifecycle,"The business-critical chain lifecycle is an agile software development lifecycle that aims at aligning the software project deliverables to attain the business objectives based on business priorities. The traditional software development projects work on the assumption that all `equal effort consumers' would be treated equally and worked upon. The agile methodology ensures that the delivery cycles are reduced thus introducing agility in the way business is supported by underlying technologies. The proposed lifecycle model introduces new variables pertaining to the business value generation each finished piece of code would produce. Hence the software project processes have to be modified to cater to it. Even the usual agile lifecycle testing strategies need to be modified to suit the proposed model. The test plans, test estimations, test resource management, quality control, regression plans and automation road map and plans have to be customized to cater to the new life cycle model. The secondary project management activities such as risk management, procurement management, etc also may to be modified with respect to the testing processes. My paper aims at using the critical chain principles and proposes a software lifecycle model that can cater to business priorities and aligning the testing processes not only to development cycles but also to the actual business value created. In this paper I would take a case study and compare and contrast when project uses the regular models and this new model. I would also provide guidelines to use this lifecycle model and modify regular project management activities to cater to the new model with emphasis on the testing processes. The paper would try to provide the ideal scenarios; in project teams, in consulting firms and in new customers and expectations; where such a model could provide high impact on the way consulting companies can do successful projects and creating more value to the customers.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319542,no,,,yes,,no,no,undetermined,,no
240,Type Inference for Soft-Error Fault-Tolerance Prediction,"Software systems are becoming increasingly vulnerable to a new class of soft errors, originating from voltage spikes produced by cosmic radiation. The standard technique for assessing the source-level impact of these soft errors, fault injection - essentially a black-box testing technique - provides limited high-level information. Since soft errors can occur anywhere, even control-structured white-box techniques offer little insight. We propose a type-based approach, founded on data-flow structure, to classify the usage pattern of registers and memory cells. To capture all soft errors, the type system is defined at the assembly level, close to the hardware, and allows inferring types in the untyped assembly representation. In a case study, we apply our type inference scheme to a prototype brake-by-wire controller, developed by Volvo Technology, and identify a high correlation between types and fault-injection results. The case study confirms that the inferred types are good predictors for soft-error impact.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431783,no,,,no,,,no,undetermined,,no
241,"Uncertainty management in software engineering: Past, present, and future","Software development has significantly matured in the last decade. However, one of the critical challenges today is uncertainty inherent to every aspect of software development including requirement specifications, design, coding, and testing. In this paper, we propose a framework for uncertainty management in software engineering. The framework is used to model uncertainty inherent to software development activities and manage their consequences. The framework consists of four main phases: identification and prioritization, modeling and analysis, management and planning, and monitoring and evaluation. Commercial off-the-shelf (COTS)-based development is selected as an example to illustrate how the proposed framework is used in a simple but intuitive case study to represent uncertainty and manage its consequences.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090081,no,,,no,,,no,undetermined,,no
243,Visualizing the structure of field testing problems,Field testing of a software application prior to general release is an important and essential quality assurance step. Field testing helps identify unforeseen problems. Extensive field testing leads to the reporting of a large number of problems which often overwhelm the allocated resources. Prior efforts focus primarily on studying the reported problems in isolation. We believe that a global view of the interdependencies between these problems will help in rapid understanding and resolution of reported problems. We present a visualization that highlights the commonalities between reported problems. The visualization helps developers identify two patterns that they can use to prioritize and focus their efforts. We demonstrate the applicability of our visualization through a case study on problems reported during field testing efforts for two releases of a large scale enterprise application.,2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306297,no,,,yes,,,no,undetermined,,no
244,A Regression Testing Approach for Software Product Lines Architectures,"In the Software Product Lines (SPL) context, where products are derived from a common platform, the reference architecture can be considered the main asset. In order to maintain its correctness and reliability after modifications, a regression testing approach based on architecture specification and code was developed. It aims to reduce the testing effort, by reusing test cases, execution results, as well as, selecting and prioritizing an effective set of test cases. Taking advantage of SPL architectures similarities, this approach can be applied among product architectures and between the reference and product architecture. This study also presents an evaluation performed in order to calibrate and improve the proposed approach.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631684,no,,,no,,no,no,undetermined,,no
247,An Approach for Classifying Program Failures,"In this work, we leverage hardware performance counters-collected data to automatically group program failures that stem from closely related causes into clusters, which can in turn help developers prioritize failures as well as diagnose their causes. Hardware counters have been used for performance analysis of software systems in the past. By contrast, in this paper they are used as abstraction mechanisms for program executions. The results of our feasibility studies conducted on two widely-used applications suggest that hardware counters-collected data can be used to reliably classify failures.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5617204,no,,,no,,,no,undetermined,,no
248,An Automatic Configuration Approach to Improve Real-Time Application Throughput While Attaining Determinism,"Determinism and throughput are two important performance measures for Java-based real-time applications, but they often conflict. Therefore, it is significant to improve throughput for Java-based real-time applications while guaranteeing its execution time determinism. In this paper, we propose an automatic configuration approach to assign real-time thread priorities to solve the above-mentioned problem. In this approach, we propose an innovative representation of determinism related with real-time thread priorities using stochastic process. Java-based real-time application's throughput is quantified with thread priorities as parameters. The algorithm of integer programming is used to optimize throughput with boundary conditions of the level of determinism. Finally, the Sweet Factory application is tested to evaluate the effect of our approach. Experiment results show that throughput for Java-based real-time applications could be efficiently improved while keeping the execution time determinism with our approach.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676292,no,,,no,,,no,undetermined,,no
249,An Intelligent Approach of Obtaining Feasible Machining Processes and Their Selection Priorities for Features Based on Neural Network,"To obtain all feasible machining processes and their quantitative selection priorities, an intelligent making decision approach combining back-propagation neural network and backward planning is proposed. Uniform design method, which is adapted for the problem of multiple factors and multiple levels, is adopted to build representative sample sets for the neural network. The neural network is trained by an improved back-propagation algorithm which can adjust momentum factor and learning rate simultaneously, and tested by linear regression analysis. A case study has been conducted to demonstrate the effectiveness of the proposed approach.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677004,no,,,no,,,no,undetermined,,no
251,Analytical survey on automated software test data evaluation,Automated software test data optimization has become a major aspect in quality of any software. For quality different test cases has to be performed for testing. In order to evaluate every aspect of the software program the number of test cases has increased tremendously. In this paper author have tried to evaluate different proposed techniques for automated software test data optimization and emphasize is made to extract the critical factors which need to be present in any technique to make the technique optimized one. These factors are then evaluated on the basis of different papers and concluded some results which are beneficial to work for the creation of an optimized technique.,2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488549,no,,,no,,,no,undetermined,,no
253,Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows,"We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062117,no,,,no,,,no,undetermined,,no
257,Evaluating and Enhancing Xen-Based Virtual Routers to Support Real-Time Applications,"Router virtualization seems as the obvious next step to system virtualization and the key to easily deploy and manage next generation overlay virtual networks. In this paper, we investigate the viability of virtual routers on a Xen-based system. We first evaluate the system throughput when achieving forwarding in the virtual routers. Then, we consider the context where virtual routers are dedicated to flows of different types and propose a mechanism to guarantee the required throughput and latency to real time applications while maintaining an optimal aggregated system throughput. We achieved this through both configuring the Xen Credit scheduler and establishing priorities between packets in the driver domain before switching them to the target virtual router.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421620,no,,,no,,yes,no,undetermined,,no
258,Making defect-finding tools work for you,"Given the high costs of software testing and fixing bugs after release, early detection of bugs using static analysis can result in significant savings. However, despite their many benefits, recent availability of many such tools, and evidence of a positive return-on-investment, static-analysis tools are not used widely because of various usability and usefulness problems. The usability inhibitors include the lack of features, such as capabilities to merge reports from multiple tools and view warning deltas between two builds of a system. The usefulness problems are related primarily to the accuracy of the tools: identification of false positives (or, spurious bugs) and uninteresting bugs among the true positives. In this paper, we present the details of an online portal, developed at IBM Research, to address these problems and promote the adoption of static-analysis tools. We report our experience with the deployment of the portal within the IBM developer community. We also highlight the problems that we have learned are important to address, and present our approach toward solving some of those problems.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062143,no,,,no,,,no,undetermined,,no
259,Managing Testing Complexity in Dynamically Adaptive Systems: A Model-Driven Approach,"Autonomous systems are increasingly conceived as a means to allow operation in changeable or poorly understood environments. However, granting a system autonomy over its operation removes the ability of the developer to be completely sure of the system's behaviour under all operating contexts. This combination of environmental and behavioural uncertainty makes the achievement of assurance through testing very problematic. This paper focuses on a class of system, called an m-DAS, that uses run-time models to drive run-time adaptations in changing environmental conditions. We propose a testing approach which is itself model-driven, using model analysis to significantly reduce the set of test cases needed to test for emergent behaviour. Limited testing resources may therefore be prioritised for the most likely scenarios in which emergent behaviour may be observed.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463659,no,,,no,,,no,undetermined,,no
260,Measuring unmeasurable attributes of software quality using Pragmatic Quality Factor,"Software quality is evolving beyond static measurement to a wider scope of quality definition. Previous studies have indicated the importance of human aspect in software quality. But the quality models have not included comprehensively this aspect together with the behavioural aspect of software quality. This research has proposed a Pragmatic Quality Factors (or PQF) as a software quality measurement and metrics that includes both aspects of quality. These aspects of quality are essential as to balance between technical and non-technical (human) facet. In addition, this model provides flexibility by giving priorities and weights to the quality attributes. The priority and weight are necessary to reflect business requirement in the real business environment. Therefore, it is more practical that suits with different users and purposes. It is implemented through collaborative perspective approach between users, developers and independent assessor. This model shows how the unmeasurable characteristics can be measured indirectly using measures and metrics approach. It has been tested involving assessment and certification exercises in real case studies in Malaysia.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564077,no,,,no,,,no,undetermined,,no
261,Notice of Retraction<br>Designing and formulating organization performance evaluation model in AHP method based on EFQM criteria (case study),"Designing and formulating a comprehensive organization performance evaluation model based on European Foundation for Quality Management (EFQM) in AHP (Analytic Hierarchy Process) method is the main aim of the present research study. Evaluation is considered as one of the most important activities in each organization in a way that reformation of processes and procedures of doing activity without evaluation of results will be impossible. At the present research activity, AHP (Analytic Hierarchy Process) has been used as one of MADM (Multi-Attribute Decision Making) methods for the evaluation of performance of organizations through the application of EFQM (European Foundation for Quality Management) excellence model criteria. Also, Municipality of City of Tabriz has been selected as subjects for testing the presented model. In the same direction, seven districts of this municipality were selected as sample model. Necessary and required information were accumulated through questionnaire, interview and also taking advantage of data and library resources, details of which were analyzed and studied through the application of advanced Excel and Expert Choice 11.5 software package system. Eventually, various districts of this organization were evaluated and prioritized in terms of performance.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553098,no,,,no,,,no,undetermined,,no
263,Optimizing the Software Architecture for Extensibility in Hard Real-Time Distributed Systems,"We consider a set of control tasks that must be executed on distributed platforms so that end-to-end latencies are within deadlines. We investigate how to allocate tasks to nodes, pack signals to messages, allocate messages to buses, and assign priorities to tasks and messages, so that the design is extensible and robust with respect to changes in task requirements. We adopt a notion of extensibility metric that measures how much the execution times of tasks can be increased without violating end-to-end deadlines. We optimize the task and message design with respect to this metric by adopting a mathematical programming front-end followed by postprocessing heuristics. The proposed algorithm as applied to industrial strength test cases shows its effectiveness in optimizing extensibility and a marked improvement in running time with respect to an approach based on randomized optimization.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535223,no,,,no,,,no,undetermined,,no
265,Prioritization of Issues and Requirements by Cumulative Voting: A Compositional Data Analysis Framework,"Cumulative Voting (CV), also known as Hundred-Point Method, is a simple and straightforward technique, used in various prioritization studies in software engineering. Multiple stakeholders (users, developers, consultants, marketing representatives or customers) are asked to prioritize issues concerning requirements, process improvements or change management in a ratio scale. The data obtained from such studies contain useful information regarding correlations of issues and trends of the respondents towards them. However, the multivariate and constrained nature of data requires particular statistical analysis. In this paper we propose a statistical framework; the multivariate Compositional Data Analysis (CoDA) for analyzing data obtained from CV prioritization studies. Certain methodologies for studying the correlation structure of variables are applied to a dataset concerning impact analysis issues prioritized by software professionals under different perspectives. These involve filling of zeros, transformation using the geometric mean, principle component analysis on the transformed variables and graphical representation by biplots and ternary plots.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598119,no,,,no,,,no,undetermined,,no
267,Prioritizing Mutation Operators Based on Importance Sampling,"Mutation testing is a fault-based testing technique for measuring the adequacy of a test suite. Test suites are assigned scores based on their ability to expose synthetic faults (i.e., mutants) generated by a range of well-defined mathematical operators. The test suites can then be augmented to expose the mutants that remain undetected and are not semantically equivalent to the original code. However, the mutation score can be increased superfluously by mutants that are easy to expose. In addition, it is infeasible to examine all the mutants generated by a large set of mutation operators. Existing approaches have therefore focused on determining the sufficient set of mutation operators and the set of equivalent mutants. Instead, this paper proposes a novel Bayesian approach that prioritizes operators whose mutants are likely to remain unexposed by the existing test suites. Probabilistic sampling methods are adapted to iteratively examine a subset of the available mutants and direct focus towards the more informative operators. Experimental results show that the proposed approach identifies more than 90% of the important operators by examining ? 20% of the available mutants, and causes a 6% increase in the importance measure of the selected mutants.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635074,no,,,no,,yes,no,undetermined,,no
270,Prioritizing Unit Test Creation for Test-Driven Maintenance of Legacy Systems,"Test-Driven Development (TDD) is a software development practice that prescribes writing unit tests before writing implementation code. Recent studies have shown that TDD practices can significantly reduce the number of pre-release defects. However, most TDD research thus far has focused on new development. We investigate the adaptation of TDD-like practices for already implemented code, in particular legacy systems. We call this adaptation of TDD-like practices for already implemented code ``Test-Driven Maintenance'' (TDM). In this paper, we present an approach that assists software development and testing managers, who employ TDM, utilize the limited resources they have for testing legacy systems efficiently. The approach leverages the development history of the project to generate a prioritized list of functions that managers should focus their unit test writing resources on. The list is updated dynamically as the development of the legacy system progresses. To evaluate our approach, we conduct a case study on a large commercial legacy software system. Our findings suggest that heuristics based on the function size, modification frequency and bug fixing frequency should be used to prioritize the unit test writing of legacy systems.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562952,no,,,yes,,,yes,undetermined,no,no
271,Ranking Attacks Based on Vulnerability Analysis,"Now that multiple-known attacks can affect one software product at the same time, it is necessary to rank and prioritize those attacks in order to establish a better defense. The purpose of this paper is to provide a set of security metrics to rank attacks based on vulnerability analysis. The vulnerability information is retrieved from a vulnerability management ontology, which integrates commonly used standards like CVE, CWE, CVSS, and CAPEC. Among the benefits of ranking attacks through the method proposed here are: a more effective mitigation or prevention of attack patterns against systems, a better foundation to test software products, and a better understanding of vulnerabilities and attacks.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428663,no,,,no,,yes,no,undetermined,,no
276,Risk-Based Testing: A Case Study,"This paper describes the application of risk-based testing for a software product evaluation in a real case study. Risk-based testing consists of a set of activities regarding risk factors identification related to software requirements. Once identified, the risks are prioritized according to their likelihood and impact and test cases are designed based on the strategies for treatment of the identified risk factors. Thus, test efforts are continuously adjusted according to risk monitoring. The paper also briefly reviews available risk-based approaches, describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of problems, challenges and future work.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501497,no,,,no,,,no,undetermined,,no
277,Risks of unrecognized commonalities in information technology supply chains,"In this paper we examine the interdependencies and common points of failure (and attack) that plague commonly-used system and network hardware and software. The proposed approach requires not only generating inventories of acquiring organizations' equipment and software products, and clear and detailed descriptions of every link in the supply chain, but also the identification of common components and their sources. This information is required not only for manufacturer and OEM supply chains, but also for the services supply chains of maintenance and repair organizations. When such critical components and services have been identified, one must prioritize their importance and apply appropriate security and testing. Such an identification and tracking system is only as good as its ability to incorporate up-to-the-minute changes and additions. This requires extensive real-time reporting and information sharing. The author presents a general description of a proprietary tool that facilitates the collaboration needed for such an approach to be effective.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5654970,no,,,no,,,no,undetermined,,no
285,Time Windows Based Dynamic Routing in Multi-AGV Systems,"This paper presents a dynamic routing method for supervisory control of multiple automated guided vehicles (AGVs) that are traveling within a layout of a given warehouse. In dynamic routing a calculated path particularly depends on the number of currently active AGVs' missions and their priorities. In order to solve the shortest path problem dynamically, the proposed routing method uses time windows in a vector form. For each mission requested by the supervisor, predefined candidate paths are checked if they are feasible. The feasibility of a particular path is evaluated by insertion of appropriate time windows and by performing the windows overlapping tests. The use of time windows makes the algorithm apt for other scheduling and routing problems. Presented simulation results demonstrate efficiency of the proposed dynamic routing. The proposed method has been successfully implemented in the industrial environment in a form of a multiple AGV control system.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907246,no,,,no,,,no,undetermined,,no
286,"To Strengthen Security, Change Developers' Incentives","Many of the most common software vulnerabilities, such as buffer overflows, cross-site scripting, and misapplications of cryptography, are wholly avoidable if software makers apply an appropriate level of training, testing, and care.Yet developers today have the ""wrong"" incentives, often leading them to underinvest in security or even to directly harm it. If we can understand these incentives and their causes, we might be able to reshape them and radically improve security.Software makers have shown a dramatic ability to strengthen their products' security given sufficient motivation.The most famous example is Microsoft's transformation over the past decade from a security laughingstock to a leader. In 2002, stung by several widely publicized vulnerabilities across its product line, the company began a major security initiative that produced lasting changes in its priorities, processes, and culture. Gone were the days of ""creating designs and code that emphasize features over security."" Yet changes like these are exceptional. Microsoft's shift was motivated by an intense level of scrutiny and withering global publicity that few firms experience, and it had the unusual luxury of responding with vast engineering resources paid for by monopoly rents. Most developers face far weaker security incentives.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439535,no,,,no,,,no,undetermined,,no
288,Using Methods & Measures from Network Analysis for GUI Testing,"Graphical user interfaces (GUIs) for today's applications are extremely large. Moreover, they provide many degrees of freedom to the end-user, thus allowing the user to perform a very large number of event sequences on the GUI. The large sizes and degrees of freedom create severe problems for GUI quality assurance, including GUI testing. In this paper, we leverage methods and measures from network analysis to analyze and study GUIs, with the goal of aiding GUI testing activities. We apply these methods and measures on the event-flow graph model of GUIs. Results of a case study show that ""network centrality measures"" are able to identify the most important events in the GUI as well as the most important sequences of events. These events and sequences are good candidates for test prioritization. In addition, the ""betweenness clustering"" method is able to partition the GUI into regions that can be tested separately.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463654,no,,,no,,no,no,undetermined,,no
291,A Diagnostic Point of View for the Optimization of Preparation Costs in Runtime Testing,"Runtime testing is emerging as the solution for the validation and acceptance testing of service-oriented systems, where many services are external to the organization, and duplicating the system's components and their context is too complex, if possible at all. In order to perform runtime tests, an additional expense in the test preparation phase is required, both in software development and in hardware. Preparation cost prioritization methods have been based on runtime testability (i.e, coverage) and do not consider whether a good runtime testability is sufficient for a good runtime diagnosis quality in case faults are detected, and whether this diagnosis will be obtained efficiently (i.e., with a low number of test cases). In this paper we show (1) the direct relationship between testability and diagnosis quality, that (2) these two properties do not guarantee an efficient diagnosis, and (3) a measurement that ensures better prediction of efficiency.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954478,no,,,no,,,no,undetermined,,no
292,A Fuzzy Logic Approach for Scheduling Preventive Maintenance in ERP System,"Many firms have proceeded to the adoption of Enterprise Resources Planning ERP solutions to maintain competitiveness. ERP is a packaged software system that enables enterprises to integrate operations, business processes and functions through common database. However, the majority of ERP systems do not support Preventive Maintenance (PM) scheduling process. The objective of PM is to minimize equipment downtime using the limited resources of an organization. Therefore, prioritizing PM activities for equipment is essential. In this paper, a fuzzy logic-based system for PM scheduling is proposed to interpret the linguistic variables extracted from expert's knowledge for determining equipment priorities, which could be incorporated as a custom module in ERP systems. The system was tested and proved to be reliable in solving PM scheduling problem.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999330,no,,,no,,,no,undetermined,,no
293,A general purpose Ethernet based readout data acquisition system,"A flexible dedicated readout system is one of the most important part of any kind of dedicated detection system, especially for its testing phase as well as when the final system is ready for implementation. An obvious choice is to use a FPGA (apart from dedicated front-end electronics) as a first stage of data storage and processing element. Furthermore the FPGA has to prepare and transfer the incoming/processed data to the host PC. The implementation of the data exchange can be a problem, especially for small groups of developers, who have an option to buy a general solution with its limitations and a price, or to do time-consuming development of their own system practically from scratch. This paper presents a FPGA based general purpose readout solution which lies in between the two opposite approaches. Presented system uses a FPGA mezzanine board equipped with Ethernet Gigabit connection to PC. The FPGA FIFO based readout of a digital data stream is packed directly into the Ethernet frames and send to the destination PC using point-to point connection. The standard Ethernet frames are used in this design, additionally equipped with one byte carrying information on data type. When a high throughput is needed the data type is employed to prioritize them. This moderately simple but very powerful interface is relatively easy to be implemented in many applications [1]. The custom approach chosen for FPGA implementation causes a need to prepare dedicated software suite to process all incoming data in the PC side. The developed software package is called EPPRO (Ethernet Packet PROxy) since it exploits special Ether net frames for data exchange. The core part of EPPRO is a Linux kernel module, responsible for data reception/transmission and dispatching, taking into account their types to filter and prioritize the incoming packets. Overall performance of the whole system has been evaluated in respect to its throughput and reliability, presented test results confirm that all of the design goals have been fulfilled.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154542,no,,,no,,,no,undetermined,,no
295,"A low-cost distributed instrumentation system for monitoring, identifying and diagnosing irregular patterns of behavior in critical ITS components","ITS telecommunication infrastructure and information gathering/distribution equipment such as fiber-optic routing devices, communication huts, uninterrupted power supplies (UPSs), cameras and dynamic message signs (DMSs) are critical to the effective management of resources in emergency situations. Frequently encountered scenarios include evaluating the severity of vehicle collisions to dispatch appropriate law enforcement and ambulatory services, or issuing time-sensitive AMBER alerts to assist in the effort to find missing or kidnapped children. Due to the unacceptably high cost of ITS equipment failures, preventative maintenance and dense operational testing are high priorities. In this paper we present a low-cost distributed instrumentation system (DIS) for continuous monitoring of critical ITS components. Over the last three years, we have developed and deployed the DIS in the Oklahoma Department of Transportation (ODOT) private fiberoptic network that spans several major metropolitan areas in and around Oklahoma City, Tulsa and Lawton. The Oklahoma DIS is responsible for monitoring the health of the entire ODOT ITS communications network as well as the integrity of each camera video signal and the operational status of each DMS. All of the information acquired by the DIS is integrated into an operational summary that is available on a private website for the design and execution of ITS equipment maintenance plans. In Oklahoma, information acquired by the DIS has been successfully integrated into a wide range of operation and maintenance (O&M) planning, which has led to a significant improvement in terms of overall ITS quality of service (QoS) and a quantifiable reduction in wasted costs associated with the premature discarding of energy storage devices.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082868,no,,,no,,yes,no,undetermined,,no
297,A software safety test approach based on FTA and Bayesian networks,"As an important way to verify software safety, software safety test has caught more attentions in practice. However, it is still an open question that how engineers could make software safety test more efficient. Currently, FTA based method is one of the approaches in software safety test, but it can not utilize the finished software test results, and can not be determined the priorities of all the use cases. In order to solve these problems, this paper gives a quantitative approach of software safety test based on FTA and Bayesian networks. In the approach, top-level events of fault trees are identified from system hazards firstly. Then, fault trees are built using FTA and transferred into Bayesian networks. Finally, test cases of software safety test are determined by the Bayesian networks. Besides, the paper also shows an example using the approach, which could guide software engineers to make software safety test more efficient. The example shows that the approach could take advantage of Bayesian Theorem and FTA methodology together, and give reasonable priorities of use cases in software safety test.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939497,yes,,,no,,,no,undetermined,,no
298,A statistical approach to TPS transport optimization,"This paper discusses the statistical challenges of TPS Transport. A TPS is not considered fieldable until all tests are passing. Based on the number of tests in a TPS and the inherent complexity of the transport process, the probability exists that less than 100% of all tests will pass the first time they are tested after undergoing the transport process. Optimizing what types of tests are transported significantly improves the probability that the next TPS will be successfully transported. This paper illustrates that prioritizing which functions to focus on can greatly improve the probability of success while reducing the overall characterization effort.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058752,no,,,no,,no,no,undetermined,,no
301,A Workflow Scheduling Algorithm for Optimizing Energy-Efficient Grid Resources Usage,"Grid computing represents the main solution to integrate distributed and heterogeneous resources in global scale. However, the infrastructure necessary for maintaining a global grid in production is huge. Such fact has led to excessive power consumption. On the other hand, most green strategies for data centers are DVS (Dynamic Voltage Scaling)-based and become difficult to implement them in global grids. This paper proposes the HGreen heuristic (Heavier Tasks on Maximum Green Resource) and defines a workflow scheduling algorithm in order to implement it on global grids. HGreen algorithm aims to prioritize energy-efficient resources and explores workflow application profiles. Simulation results have shown that the proposed algorithm can significantly reduce the power consumption in global grids.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6119053,,no,,no,,,no,undetermined,,no
304,An Empirical Study on the Relation between Dependency Neighborhoods and Failures,"Changing source code in large software systems is complex and requires a good understanding of dependencies between software components. Modification to components with little regard to dependencies may have an adverse impact on the quality of the latter, i.e., increase their risk to fail. We conduct an empirical study to understand the relationship between the quality of components and the characteristics of their dependencies such as their frequency of change, their complexity, number of past failures and the like. Our study has been conducted on two large software systems: Microsoft VISTA and ECLIPSE. Our results show that components that have outgoing dependencies to components with higher object-oriented complexity tend to have fewer field failures for VISTA, but the opposite relation holds for ECLIPSE. Likewise, other notable observations have been made through our study that (a) confirm that certain characteristics of components increase the risk of their dependencies to fail and (b) some of the characteristics are project specific while some were also found to be common. We expect that such results can be leveraged for use to provide new directions for research in defect prediction, test prioritization and related research fields that utilize code dependencies in their empirical analysis. Additionally, these results provide insights to engineers on the potential reliability impacts of new component dependencies based upon the characteristics of the component.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770624,,yes,,no,,,no,undetermined,,no
305,An empirical validation of FindBugs issues related to defects,"Background: Effective use of bug finding tools promise to speed up the process of source code verification and to move a portion of discovered defects from testing to coding phase. However, many problems related to their usage, especially the large number of false positives, could easily hinder the potential benefits of such tools. Aims: Assess the percentage and type of issues of a popular bugfinding tool (FindBugs) that are actual defects. Method: We analyzed 301 Java Projects developed at a university with FindBugs, collecting the issues signalled on the source code. Afterwards, we checked the precision of issues with information on changes, we ranked and validated them using both manual inspection and validation with tests failures. Results: We observed that a limited set of issues have high precision and conversely we identified those issues characterized by low precision. We compared findings first with our previous experiment and then to related work: results are consistent with both of them. Conclusions: Since our and other empirical studies demonstrated that few issues are related to real defects with high precision, developers could enable only them (or prioritize), reducing the information overload of FindBugs and having the possibility to discover defects earlier. Furthermore, the technique presented in the paper can be adopted to other tools on a code base with tests to find issues with high precision that can be checked on code in production to find defects earlier.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083173,,yes,,no,,no,no,undetermined,,no
309,Challenges in Audit Testing of Web Services,"Audit testing of Web services is a form of regression testing that applies to the integration of external services in a service composition. When such services change, the application that integrates them needs to be regressiontested, to ensure that the existing functionalities are not broken by the service changes. This means the external services must be audited for preserved compatibility with the integration. In this paper, we analyze the main research challenges behind audit testing of services. Such challenges descend from the limited testability of external services and encompass traditional regression testing problems, such as test case selection and prioritization, for which existing solutions are nonetheless inadequate. Moreover, in the paper we discuss in detail one such challenge, i.e., the problem of defining an oracle by making the integrator's assumptions explicit.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954397,,no,,no,,,no,undetermined,,no
310,"Challenges, benefits and opportunities in operating cabled ocean observatories: Perspectives from NEPTUNE Canada","The advent of the first cabled ocean observatories, with several others being planned, demonstrates the challenges, benefits and opportunities for ocean science and commercial applications. Examples are drawn primarily from NEPTUNE Canada (NC), which completed installation of the subsea infrastructure and 60 diverse instruments in 2009, with 40 more in 2010, thereby establishing the world's first regional cabled ocean observatory, northeast Pacific Ocean, off British Columbia's coast. Initial data flow started in December 2009. Another 30 instruments will be deployed in 2011-12. Introducing abundant power and high bandwidth communications into a range of ocean environments allows discrimination between short and long-term events, interactive experiments, real time data and imagery, and complex multidisciplinary teams interrogating a vast database over the observatory's 25-year design life. Scientific priorities and observatory node sites were identified through workshops. Alcatel-Lucent Submarine Networks designed, manufactured and installed the 800km backbone cable and five nodes (stepping 10kV DC to 400V DC). Node sites are located at the coast (Folger Passage), continental slope (ODP 889; Barkley Canyon), abyssal plain (ODP 1027), and ocean-spreading ridge (Endeavour), in water depths of 100-2660m. Principal scientific themes are: plate tectonic processes and earthquake dynamics; dynamic processes of seabed fluid fluxes and gas hydrates; regional ocean/climate dynamics and effects on marine biota; deep-sea ecosystem dynamics; and engineering and computational research. The Data Management and Archive System (DMAS) provides controls for the observatory network and transparent access to other data providers using interoperability techniques within a Web 2.0 environment. Users can perform data visualization and analysis on-line with either default or custom processing code, as well as simultaneously interacting with each other. Oceans 2.0 is adding tools to perform software-aided feature detection and classification of sounds in acoustic data streams. New knowledge and scientific interpretations are addressing important science applications of the observatory: ocean/climate change, ocean acidification, recognizing and mitigating natural hazards, non-renewable and renewable natural resources. Challenges are considerable: technical innovations, enlarging the user base, management, funding, maximizing educational/outreach activities. Socio-economic benefits are substantial: not only the transformation of ocean sciences but with many applications in sectors such as sovereignty, security, transportation, data services, and public policy. Opportunities for commercialization of technologies and data services/products are being facilitated by the Centre of Enterprise and Engagement (www.onccee.ca) within Ocean Networks Canada (www.networkscanada.ca) that manages the NC and VENUS observatories (www.neptunecanada.ca; www.uvic.venus.ca). Cabled ocean observatories are transforming the ocean sciences and will result in a progressive wiring of the oceans. They are designed to be expandable in footprint, nodes and instruments, and the range of scientific questions, and to provide facilities for testing technology prototypes. They will provide a wealth of new research opportunities and socio-economic benefits.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5774134,,yes,,no,,,no,undetermined,,no
312,Code Hot Spot: A tool for extraction and analysis of code change history,"Commercial software development teams have limited time available to focus on improvements to their software. These teams need a way to quickly identify areas of the source code that would benefit from improvement, as well as quantifiable data to defend the selected improvements to management. Past research has shown that mining configuration management systems for change information can be useful in determining faulty areas of the code. We present a tool named Code Hot Spot, which mines change records out of Microsoft's TFS configuration management system and creates a report of hot spots. Hot spots are contiguous areas of the code that have higher values of metrics that are indicators of faulty code. We present a study where we use this tool to study projects at ABB to determine areas that need improvement. The resulting data have been used to prioritize areas for additional code reviews and unit testing, as well as identifying change prone areas in need of refactoring.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080806,,yes,,no,,yes,no,undetermined,,no
313,Compiling SyncCharts to Synchronous C,"SyncCharts are a synchronous Statechart variant to model reactive systems with a precise and deterministic semantics. The simulation and software synthesis for SyncCharts usually involve the compilation into Esterel, which is then further compiled into C code. This can produce efficient code, but has two principal drawbacks: 1) the arbitrary control flow that can be expressed with SyncChart transitions cannot be mapped directly to Esterel, and 2) it is very difficult to map the resulting C code back to the original SyncChart, which hampers traceability. This paper presents an alternative software synthesis approach for SyncCharts that compiles SyncCharts directly into Synchronous C (SC). The compilation preserves the structure of the original SyncChart, which is advantageous for validation and possibly certification. We present a static thread-scheduling scheme that reflects data dependencies and optimizes both the number of used threads as well as the maximal used priorities. This results in SC code with competitive speed and little memory requirements.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763284,,no,,no,,no,no,undetermined,,no
314,Cost Optimizations in Runtime Testing and Diagnosis of Systems of Systems,"In practically all development processes tests are used to detect the presence of faults. This is not an exception for critical and high-availability systems. However, these systems cannot be taken offline or duplicated for testing in some cases. This makes runtime testing necessary. This paper presents work aimed at optimizing the three main sources of testing cost: preparation, execution and diagnosis. First, preparation cost is optimized by defining a metric of the runtime testability of the system, used to elaborate an implementation plan of preparative work for runtime testing. Second, the interrelated nature of test execution cost and diagnostic cost is highlighted and a new diagnostic test prioritization is introduced.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770638,,yes,,no,,,no,undetermined,,no
317,Critical components identification and verification for effective software test prioritization,"Nowadays, software complexity increases as the number of components in a component based system (CBS) increases. As the complexity level increases, the testing and verification of components also increases. This in turn rose up the testing time and cost which thus made industries to skip off some of the components due to the hard timeline and resource limitations especially during maintenance. This leads to hazardous effects if some of these missed components are critical in term of their core functionality and dependability with other components. Hence, a regression testing which is usually performed during maintenance phase should be developed meticulously to identify and test these critical components rigorously before releasing the software on to the customer side. This paper proposed a novel regression testing method based on the criticality measure calculated by means of dependability metrics and internal complexity metrics. Also, this paper compares the performance of the proposed approach with existing approaches and concluded that the proposed framework outperforms them.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165171,,no,,yes,,,no,undetermined,,no
319,Design of mice maze based on PLC control unit,"In recent years, there are a number of mouse maze devices developed. In this paper, we introduce the Y-maze which is based on the traditional programmable logic controller as a control center to the new Y-maze. In the subject, the maze of automatic control system will be in addition to the sensor controller sub- section also part and actuator part. This paper mainly discusses the new Maze automatic control system, describes the software design process and priorities, hardware selection and layout. Among them, the interface circuit of the programmable controller, I/O port assignments, infrared sensor module and the maze rotation module are described in detail The maze of automatic control system to achieve the experimental data in mice can reduce errors, improve test efficiency and reduce human labor, side by side, in addition to the traditional Y-maze caused by the interference of some unavoidable factors, such as mice, leaving the smell of mice in the experimental results, the reliability of the experiment can be further improved.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6067621,,no,,no,,yes,no,undetermined,,no
320,Design Principles for Integration of Model-Driven Quality Assurance Tools,"The engineering of software systems is supported by tools in different phases of the software development. The integration of these tools is crucial to assure the trace ability of existing models and artifacts, and to support the automation of critical software development phases such as software testing and validation. In particular, the integration of novel software quality assurance tools into existing environments must be performed in a way that minimizes its impact on existing software process, while the benefits of the tool are leveraged. This guarantees the adoption of new methodologies with minimal interference in existing production workflow. In this paper we discuss our experience in integrating a model-driven software testing tool developed within SIEMENS with a widely-adopted model-driven design tool. In particular, we establish a set of design principles from the lessons learned in this integration. We conclude showing a design that prioritizes data integration over control and presentation that achieves a high degree of tool integration while minimizing the integration development effort.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114546,,no,,no,,,no,undetermined,,no
321,Designing VM schedulers for embedded real-time applications,"Virtual Machines (VMs) allow for platform-independent software development and their use in embedded systems is increasing. In particular, VMs are rewarding in the context of mixed-criticality applications to provide isolation between critical and non-critical tasks running on the same processor. In this paper, we study the design of a real-time system based on a VM monitor/hypervisor that supports multiple VMs/domains. Since each VM in the system runs several real-time tasks, scheduling the VMs leads to a hierarchical scheduling problem. So far, most published techniques for analyzing hierarchical scheduling deal with the schedulabil-ity problem, i.e., for a given hierarchical scheduler, testing whether a set of real-time tasks meet their deadlines. In this paper, we are rather concerned with the synthesis of hier-archical/VM schedulers; that is, how to design a scheduler such that all real-time tasks running on the different VMs meet their deadlines. We consider a setup where the tasks are scheduled on multiple VMs under fixed priorities according to the Deadline Monotonic (DM) policy. The VMs are scheduled under fixed priorities on a Rate Monotonic (RM) basis using one or more processors. A partitioned scheduling of VMs is considered, i.e., VMs are not allowed to migrate from one processor to the other. In this context, we propose a method for selecting optimum time slices and periods for each VM in the system. Our goal is to configure the VM scheduler such that not only all tasks are schedulable but also the minimum possible resources are used. Finally, to illustrate the proposed design technique, we present a case study based on automotive control applications.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062302,,no,,no,,,no,undetermined,,no
323,Dynamic performance stubs to simulate the main memory behavior of applications,"Dynamic performance stubs provide a framework to simulate the performance behavior of software modules and functions. Hence, they can be used as an extension to software performance engineering methodologies. The methodology of dynamic performance stubs targets to gain oriented performance improvement. Other applications include the identification of ""hidden"" bottlenecks and the prioritization of optimization alternatives. Main memory stubs have been developed to extend the simulation possibilities of the dynamic performance stubs framework. They are able to simulate the heap and stack behavior of software modules or functions. This paper evaluates an algorithm to generate the simulation data file, which serves as input for the main memory stubs simulation algorithm. Moreover, it presents an automatic error correction algorithm to consider the results from the calibration functions to improve the simulation results. Additionally, a proof of concept is given to depict the results of the simulation data file generation and the automatic error correction algorithm. This paper shows that, it is possible to generate the simulation data file as well as to optimize the simulation data to compensate inaccuracies in order to create main memory stubs.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984857,,no,,no,,yes,no,undetermined,,no
325,"Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities","Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560680,,no,,no,,,no,undetermined,,no
326,Fast Start-up for Spartan-6 FPGAs using Dynamic Partial Reconfiguration,"This paper introduces the first available tool flow for Dynamic Partial Reconfiguration on the Spartan-6 family. In addition, the paper proposes a new configuration method called Fast Start-up targeting modern FPGA architectures, where the FPGA is configured in two-steps, instead of using a single (monolithic) full device configuration. In this novel approach, only the timing-critical modules are loaded at power-up using the first high-priority bitstream, while the non-timing critical modules are loaded afterwards. This two-step or prioritized FPGA start-up is used in order to meet the extremely tight startup timing specifications found in many modern applications, like PCI-express or automotive applications. Finally, the developed tool flow and methods for Fast Start-up have been used and tested to implement a CAN-based automotive ECU on a Spartan-6 evaluation board (i.e., SP605). By using this novel approach, it was possible to decrease the initial bitstream size and hence, achieve a configuration time speed-up of up to 4.5??, when compared to a standard configuration solution.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763244,,no,,no,,,no,undetermined,,no
327,GeTeX: A Tool for Testing Real-Time Embedded Systems Using CAN Applications,"Real-Time Embedded Systems (RTES) have an increasing role in controlling the IT that we use on a day-to-day basis. The behaviour of an RTES is not based solely on the interactions it might have with its surrounding environment, but also on the timing requirements it induces. As a result, ensuring that an RTES behaves correctly is non-trivial, especially after adding time as a new dimension to the complexity of the testing process. In previous research, we introduced a 'priority-based' approach which tested the logical and timing behaviour of an RTES modeled formally as UPPAAL Timed Automata (UTA). The 'priority-based' approach was based on producing sets of timed test traces by achieving timing constraints coverage according to three sets of priorities, namely boundary, out-boundary and in-boundary. In this paper, we introduce a new testing tool 'GeTeX' that deploys the ""priority-based"" testing approach. GeTeX is a complete testing tool which generates timed test-cases from UTA models and executes them on the System Under Test (SUT) to identify faults. In its current version, GeTeX supports Control Area Network (CAN) applications.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5934805,,no,,yes,,,yes,undetermined,no,no
329,How to Shop for Free Online -- Security Analysis of Cashier-as-a-Service Based Web Stores,"Web applications increasingly integrate third-party services. The integration introduces new security challenges due to the complexity for an application to coordinate its internal states with those of the component services and the web client across the Internet. In this paper, we study the security implications of this problem to merchant websites that accept payments through third-party cashiers (e.g., PayPal, Amazon Payments and Google Checkout), which we refer to as Cashier-as-a-Service or CaaS. We found that leading merchant applications (e.g., NopCommerce and Interspire), popular online stores (e.g., Buy.com and JR.com) and a prestigious CaaS provider (Amazon Payments) all contain serious logic flaws that can be exploited to cause inconsistencies between the states of the CaaS and the merchant. As a result, a malicious shopper can purchase an item at an arbitrarily low price, shop for free after paying for one item, or even avoid payment. We reported our findings to the affected parties. They either updated their vulnerable software or continued to work on the fixes with high priorities. We further studied the complexity in finding this type of logic flaws in typical CaaS-based checkout systems, and gained a preliminary understanding of the effort that needs to be made to improve the security assurance of such systems during their development and testing processes.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958046,,no,,no,,,no,undetermined,,no
332,Improving independence in the community for stroke survivors: The role of biomechanics visualisation in ankle-foot orthosis tuning,"One of the key priorities for stroke survivors in their rehabilitation process is regaining their ability to walk. Evidence has shown that provision of ankle-foot orthoses (AFOs) can have a positive impact on walking. This paper discusses the role of gait analysis in the provision of AFOs for stroke survivors. A discussion of the shortcomings of gait analysis techniques is included, with a description of how these might be overcome during the AFO tuning process through the ongoing development of data visualisation software. The design of a randomised controlled trial in conjunction with a series of qualitative measures is described, which will be used to test the efficacy of the visualisation software.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038838,,no,,no,,,no,undetermined,,no
334,IMS Threat and Attack Surface Analysis Using Common Vulnerability Scoring System,"For the purposes of this study, IMS specifications and public sources were analyzed using the general attack surface analysis methodology. These findings were verified and augmented by active scanning and passive analysis of the available real-world IMS test setups that were investigated during the project. As various tests and security probes were performed against the test setups, the system behaviour was analyzed for previously undetermined interactions and transient attack surfaces. After the IMS attack vectors had been identified, the Common Vulnerability Scoring System version 2 (CVSSv2) Base Scores were used to prioritize the IMS attack surface interfaces. CVSS is an industry standard for classifying vulnerabilities. It must be noted however that the idea of applying CVSS scoring to an a priori comparison of vulnerability categories and potential attack surfaces is original research by the authors of this study.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032214,,no,,no,,yes,no,undetermined,,no
337,Lower bounds for single machine subproblems occurring in weighted tardiness oriented shifting bottleneck procedures,"In this paper, we propose lower bounds for single machine scheduling problems which occur during a run of a shifting bottleneck procedure for total weighted tardiness job shops. The specific structure of this kind of problem and its objective function in particular prevent an immediate transfer or an adaption of existing lower bounds from __??conventional__?_ single machine problems with tardiness related objectives. Hence it has been necessary to develop bounding approaches which are to some extent conceptually new. Potential application scenarios range from exact subproblem solution methods or machine prioritization criteria in a shifting bottleneck procedure to branch-and-bound algorithms for job shops with total weighted tardiness objective. In order to provide a significant evaluation of the proposed lower bounds regarding their effectiveness and efficiency, we tested them based on problem instances which actually have been generated in a shifting bottleneck procedure applied to benchmark job shop problems.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976548,,no,,no,,,no,undetermined,,no
339,Modeling the Diagnostic Efficiency of Regression Test Suites,"Diagnostic performance, measured in terms of the manual effort developers have to spend after faults are detected, is not the only important quality of a diagnosis. Efficiency, i.e., the number of tests and the rate of convergence to the final diagnosis is a very important quality of a diagnosis as well. In this paper we present an analytical model and a simulation model to predict the diagnostic efficiency of test suites when prioritized with the information gain algorithm. We show that, besides the size of the system itself, an optimal coverage density and uniform coverage distribution are needed to achieve an efficient diagnosis. Our models allow us to decide whether using IG with our current test suite will provide a good diagnostic efficiency, and enable us to define criteria for the generation or improvement of test suites.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954476,,no,,yes,,no,no,undetermined,,no
341,Pragmatic prioritization of software quality assurance efforts,"A plethora of recent work leverages historical data to help practitioners better prioritize their software quality assurance efforts. However, the adoption of this prior work in practice remains low. In our work, we identify a set of challenges that need to be addressed to make previous work on quality assurance prioritization more pragmatic. We outline four guidelines that address these challenges to make prior work on software quality assurance more pragmatic: 1) Focused Granularity (i.e., small prioritization units), 2) Timely Feedback (i.e., results can be acted on in a timely fashion), 3) Estimate Effort (i.e., estimate the time it will take to complete tasks), and 4) Evaluate Generality (i.e., evaluate findings across multiple projects and multiple domains). We present two approaches, at the code and change level, that demonstrate how prior approaches can be more pragmatic.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032601,,no,,no,,yes,no,undetermined,,no
342,Prioritising Refactoring Using Code Bad Smells,"We investigated the relationship between six of Fowler et al.'s Code Bad Smells (Duplicated Code, Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man) and software faults. In this paper we discuss how our results can be used by software developers to prioritise refactoring. In particular we suggest that source code containing Duplicated Code is likely to be associated with more faults than source code containing the other five Code Bad Smells. As a consequence, Duplicated Code should be prioritised for refactoring. Source code containing Message Chains seems to be associated with a high number of faults in some situations. Consequently it is another Code Bad Smell which should be prioritised for refactoring. Source code containing only one of the Data Clumps, Switch Statements, Speculative Generality, or Middle Man Bad Smell is not likely to be fault-prone. As a result these Code Bad Smells could be put into a lower refactoring priority.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954447,,no,,no,,,no,undetermined,,no
346,Ranking of technology transfer barriers in developing countries; case study of Iran's biotechnology industry,"In this paper, first we shall define a list of technology transfer barriers in the biotechnology field through interviews and literature reviews. Next, we shall categorize them into four major categories of technology, and at last introduce all of the hypotheses on the existence of relationship between each part of technology and failure in the technology transfer process. Questionnaires consisting of two parts were created and handed out to all of the specialists and biotechnology idea-holders in the Iranian National Research center of Biotechnology, Lidco Co, Iran Pastor Institute, Biotechnology Department of University of Tehran, etc. Going on, we've evaluated the factors' validity through statistical tests and the recognized barriers' priorities in each category, which were generally, identified by Analytical Hierarchy Process (AHP) via the software Expert Choice. These priorities describe the high importance of organization-ware (34.1%), information-ware (27%), technique-ware (20.7%) and eventually human-ware (18.1%) respectively.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118187,,no,,no,,,no,undetermined,,no
348,Research and implementation of resource management system based on Xen virtual machine,"With the development of computer technology, there is gradually abundant resource available for computer systems. Virtualization technology provides a viable solution for effective management and rational allocation of system resources. Xen virtual machine is an excellent open source virtual machine, so attracting widespread attention, with broad application prospects. However, the traditional resource management of Xen virtual machine focuses on sharing processor resources fairly, while ignoring the effect of the virtual machines with the different priorities. This would cause the practicality and the performance issues in using virtual machine. This paper proposes a resource management system model based on Xen virtual machine. The model monitors guest domain and analyzes runtime information for automating resource allocation. It mainly take into account the virtual machines with different priorities. Through a series of comparative tests, the results verify that this model can enhance practicality.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6182220,,no,,no,,,no,undetermined,,no
349,Risk-Based Testing of Safety-Critical Embedded Systems Driven by Fault Tree Analysis,"One important aspect of the quality assurance process of safety-critical embedded systems is verifying the appropriateness, correctness of the implementation and effectiveness of safety functions. Due to the rapid growth in complexity, manual verification activities are no longer feasible. This holds especially for testing. A popular method for testing such complex systems is model-based testing. Recent techniques for model-based testing do not sufficiently take into consideration the information derived from the safety analyses like Failure Mode and Effect Analysis and Fault Tree Analyses (FTA). In this paper, we describe an approach to use the results of FTA during the construction of test models, such that test cases can be derived, selected and prioritized according to the severity of the identified risks and the number of basic events that cause it. This approach is demonstrated on an example from the automation domain, namely a modular production system. We find that the method provides a significant increase in coverage of safety functions, compared to regular model based testing.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954386,,no,,no,,,no,undetermined,,no
350,"Robotic test bed for autonomous surface exploration of Titan, Mars, and other planetary bodies","Tier-scalable robotic reconnaissance missions are called for in extreme space environments, including planetary atmospheres, surfaces (both solid and liquid), and subsurfaces (e.g., oceans), as well as in potentially hazardous or inaccessible operational areas on Earth. Such future missions will require increasing degrees of operational autonomy: (1) Automatic mapping of an operational area from different vantages (i.e., spaceborne, airborne, surface, subsurface); (2) automatic sensor deployment and sensor data gathering; (3) automatic feature extraction and target/region-of-interest/anomaly identification within the mapped operational area; (4) automatic target prioritization for follow-up or close-up (in-situ) examination; and (5) subsequent automatic, targeted deployment and navigation/relocation of agents/sensors (e.g., to follow up on transient events). We report on recent progress in developing an Earth-based (outdoors) robotic test bed for Tier-scalable Reconnaissance at the University of Arizona and Caltech for distributed, science-driven, and significantly less constrained (compared to state-of-the-art) reconnaissance of prime locations on a variety of planetary bodies, with particular focus on Saturn's moon Titan with its methane/hydrocarbon lakes and Mars. The test bed currently comprises several computer-controlled robotic surface vehicles, i.e., rovers and lake landers/boats equipped with a variety of sensors. To achieve a fully operational Tier-scalable Reconnaissance test bed, aerial platforms will be integrated as a next step. The robotic surface vehicles can be interactively or automatically controlled from anywhere in the world in near real-time via the Internet. The test bed enables the implementation, field-testing, and validation of algorithms and strategies for navigation, exploration, sensor deployment, sensor data gathering, feature extraction, anomaly detection, and science goal prioritization for autonomous planetary exploration. Furthermore, it permits field-testing of novel instruments and sensor technologies, as well as testing of cooperative multi-agent scenarios and distributed scientific exploration of operational areas. As such the robotic test bed enables the development, implementation, field-testing, and validation of software packages for inter-agent communication and coordination to navigate and explore operational areas with greatly reduced reliance on (ultimately without assistance from) ground operators, thus affording the degree of mission autonomy/flexibility necessary to support future missions to Titan, Mars, and other planetary bodies, including asteroids.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747267,,no,,no,,,no,undetermined,,no
352,"Security Requirements Analysis, Specification, Prioritization and Policy Development in Cyber-Physical Systems","In recent past, the security of cyber-physical systems (CPSs) has been the subject of major concern. One of the reasons is that, CPSs are often applied to mission-critical processes. Also, the automation CPSs bring in managing physical processes, and the detail of information available to them for carrying out their tasks, make securing them a prime importance. Securing CPSs is a difficult task as systems are interconnected. In order to achieve a continuous secured CPS environment, there is the need for an integrated methodology to analyze, specify and prioritize security requirements and also to develop policies to meet them. First, CPS assets are represented using high-order object models. Second, swim lane diagrams are extended to include malactivities and prevention or mitigation options to decompose use cases. We analyze security threats pertaining to the hardware components, software components and the hardware-software interaction. Security requirements are then specified, and an analytical prioritization approach, based on relative priority analysis is employed to prioritize them. Finally, security policies are then developed to meet the requirements. To demonstrate its effectiveness and evaluate its application, the proposed methodology is applied in a structured approach to a test bed - Ayushman, a Pervasive Health Monitoring System (PHMS).",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004511,,no,,yes,,,no,undetermined,,no
356,Test Generation for X-machines with Non-terminal States and Priorities of Operations,"Testing methods aiming to demonstrate that an implementation behaves the same as a specification X-machine (extended finite-state machine) usually assume that (1) all states are terminal states and (2) there are no priorities associated with operations on transitions. The considered model for the machine is such that outputs for transitions leading to non-terminal states will be buffered and contents of buffers will only be made observable when terminal states are entered. The X-machine testing method has been extended in this work to handle such an extension of X-machines (EFSM).Priorities of operations determine the order in which guards of transitions are evaluated. This makes it possible to reduce the size of a test suite. For instance, if testing has shown that a transition with a specific guard g has been implemented from some state, then no lower-priority transition with a guard implied by g may ever be executed from that state. It is hence not necessary to test for the presence of such a lower-priority transition.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770602,,no,,no,,,no,undetermined,,no
358,Usability Testing Methodology: Effectiveness of Heuristic Evaluation in E-Government Website Development,"Software development organizations consist of marketing, project management, development, design, and quality assurance team. It is important for the various teams within the organization to understand the benefits and limitation of incorporating various usability testing methods within the software development life cycle. Some of the reasons for poor usability include effort prioritization conflicts from project management, development and design team. The role of the usability engineer is to get involved as the heuristic evaluator and facilitate the development and design efforts are based on usability principles and at the same time adhering to the project time line. Two of the common usability inspection methods consist of user experience testing and expert review or more commonly known as Heuristic Evaluation (HE). This paper focuses on understanding the effectiveness of HE as a methodology for defect detection. The results show the effectiveness of the HE as a usability testing methodology in capturing defects and prioritizing development and design efforts. The results also reinforce the need for integrating traditional heuristics with modified heuristics customized to the domain or field of the project being tested such as E-Government.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5961243,,no,,no,,,no,undetermined,,no
359,Using SQL Hotspots in a Prioritization Heuristic for Detecting All Types of Web Application Vulnerabilities,"Development organizations often do not have time to perform security fortification on every file in a product before release. One way of prioritizing security efforts is to use metrics to identify core business logic that could contain vulnerabilities, such as database interaction code. Database code is a source of SQL injection vulnerabilities, but importantly may be home to unrelated vulnerabilities. The goal of this research is to improve the prioritization of security fortification efforts by investigating the ability of SQL hotspots to be used as the basis for a heuristic for prediction of all vulnerability types. We performed empirical case studies of 15 releases of two open source PHP web applications: Word Press, a blogging application, and WikkaWiki, a wiki management engine. Using statistical analysis, we show that the more SQL hotspots a file contains per line of code, the higher the probability that file will contain any type of vulnerability.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770611,,no,,no,,,no,undetermined,,no
360,Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts,"Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these __??top crashes__?_ thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711013,,no,,no,,,no,undetermined,,no
364,A Remote Sensing Approach for Landslide Hazard Assessment on Engineered Slopes,"Earthworks such as embankments and cuttings are integral to road and rail networks but can be prone to instability, necessitating rigorous and continual monitoring. To date, the potential of remote sensing for earthwork hazard assessment has been largely overlooked. However, techniques such as airborne laser scanning (ALS) are now ripe for addressing these challenges. This research presents the development of a novel hazard assessment strategy, combining high-resolution remote sensing with a numerical modeling approach. The research was implemented at a railway test site located in northern England, U.K.; ALS data and multispectral aerial imagery facilitated the determination of key slope stability variables, which were then used to parameterize a coupled hydrological-geotechnical model, in order to simulate slope behavior under current and future climates. A software toolset was developed to integrate the core elements of the methodology and determine resultant slope failure hazard which could then be mapped and queried within a geographical information system environment. Results indicate that the earthworks are largely stable, which is in broad agreement with the management company's slope hazard grading data, and in terms of morphological analysis, the remote methodology was able to correctly identify 99% of earthworks classed as embankments and 100% of cuttings. The developed approach provides an effective and practicable method for remotely quantifying slope failure hazard at fine spatial scales (0.5 m) and for prioritizing and reducing on-site inspection.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032742,,no,,no,,,no,undetermined,,no
365,"A Scalable, Lightweight WebOS Application Framework","Frequently, as applications scale, they are considered in the context of a web OS-based architecture. In support of this goal, we present a lightweight framework designed as a middleware application. This architecture is highly influenced by hypermedia-based techniques, leveraging metadata in the context of HTML5. Our framework relies on a novel incorporation of a number of open source technologies including node.js and couchDB to support priorities of fast-prototyping, scalability and maintainability. Initial experiments have demonstrated that our approach performs effectively among the dynamics of our environment.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424523,,no,,no,,,no,undetermined,,no
368,Appropriate placement of series compensators to improve transient stability of power system,"Trajectory sensitivity analysis is used to find the best places for installation of thyristor controlled series capacitors (TCSC) to improve transient stability of the power system. Based on the rotor angles of generators, an equivalent angle (??<sub>eq</sub>) is defined by determining accelerating and decelerating machines, and then using trajectory sensitivities of this angle with respect to the impedances of the transmission lines in the post-fault system, appropriate locations for placing TCSC will be found. Severity of the faults is also considered in this calculation. This method is applied to the IEEE 3-machine 9-bus test system to find the priorities of the transmission lines for installation of TCSC. Simulation with industrial software verifies the obtained results.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6303243,,no,,no,,,no,undetermined,,no
369,Automated prediction of defect severity based on codifying design knowledge using ontologies,"Assessing severity of software defects is essential for prioritizing fixing activities as well as for assessing whether the quality level of a software system is good enough for release. In filling out defect reports, developers routinely fill out default values for the severity levels. The purpose of this research is to automate the prediction of defect severity. Our aim is to research how this severity prediction can be achieved through reasoning about the requirements and the design of a system using ontologies. In this paper we outline our approach based on an industrial case study.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227962,,no,,no,,,no,undetermined,,no
373,Diffusion of Software Features: An Exploratory Study,"New features are frequently proposed in many software libraries. These features include new methods, classes, packages, etc. These features are utilized in many open source and commercial software systems. Some of these features are adopted very quickly, while others take a long time to be adopted. Each feature takes much resource to develop, test, and document. Library developers and managers need to decide what feature to prioritize and what to develop next. As a first step to aid these stakeholders, we perform an exploratory study on the diffusion or rate of adoption of features in Java Development Kit (JDK) library. Our empirical study proposes such questions as how many new features are adopted by client applications, how long it takes for a new feature to spread to various software products, what features are diffused quickly, and what features are diffused widely. We perform an exploratory study with new features in Java Development Kit (JDK, from version 1.3 to 1.6) and provide empirical findings to answer the above research questions.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462682,,no,,no,,yes,no,undetermined,,no
377,Graph-Based Optimization Algorithm and Software on Kidney Exchanges,"Kidney transplantation is typically the most effective treatment for patients with end-stage renal disease. However, the supply of kidneys is far short of the fast-growing demand. Kidney paired donation (KPD) programs provide an innovative approach for increasing the number of available kidneys. In a KPD program, willing but incompatible donor-candidate pairs may exchange donor organs to achieve mutual benefit. Recently, research on exchanges initiated by altruistic donors (ADs) has attracted great attention because the resultant organ exchange mechanisms offer advantages that increase the effectiveness of KPD programs. Currently, most KPD programs focus on rule-based strategies of prioritizing kidney donation. In this paper, we consider and compare two graph-based organ allocation algorithms to optimize an outcome-based strategy defined by the overall expected utility of kidney exchanges in a KPD program with both incompatible pairs and ADs. We develop an interactive software-based decision support system to model, monitor, and visualize a conceptual KPD program, which aims to assist clinicians in the evaluation of different kidney allocation strategies. Using this system, we demonstrate empirically that an outcome-based strategy for kidney exchanges leads to improvement in both the quantity and quality of kidney transplantation through comprehensive simulation experiments.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188515,,no,,no,,no,no,undetermined,,no
378,Guiding Testing Activities by Predicting Defect-Prone Parts Using Product and Inspection Metrics,"Product metrics, such as size or complexity, are often used to identify defect-prone parts or to focus quality assurance activities. In contrast, quality information that is available early, such as information provided by inspections, is usually not used. Currently, only little experience is documented in the literature on whether data from early defect detection activities can support the identification of defect prone parts later in the development process. This article compares selected product and inspection metrics commonly used to predict defect-prone parts. Based on initial experience from two case studies performed in different environments, the suitability of different metrics for predicting defect-prone parts is illustrated. These studies revealed that inspection defect data seems to be a suitable predictor, and a combination of certain inspection and product metrics led to the best prioritizations in our contexts.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328182,,no,,no,,,no,undetermined,,no
379,High Performance Memory Requests Scheduling Technique for Multicore Processors,"In modern computer systems, long memory latency is one of the main bottlenecks micro-architects are facing for leveraging the system performance especially for memory-intensive applications. This emphasises the importance of the memory access scheduling to efficiently utilize memory bandwidth. Moreover, in recent micro-processors, multithread and multicore is turned to be the default choice for their design. This resulted in more contention on memory. Hence, the effect of memory access scheduling schemes is more critical to the overall performance boost. Although memory access scheduling techniques have been recently proposed for performance improvement, most of them have overlooked the fairness among the running applications. Achieving both high-throughput and fairness simultaneously is challenging. In this paper, we focus on the basic idea of memory requests scheduling, which includes how to assign priorities to threads, what request should be served first, and how to achieve fairness among the running applications for multicore microprocessors. We propose two new memory access scheduling techniques FLRMR, and FIQMR. Compared to recently proposed techniques, on average, FLRMR achieves 8.64% speedup relative to LREQ algorithm, and FIQMR achieves 11.34% speedup relative to IQ-based algorithm. FLRMR outperforms the best of the other techniques by 8.1% in 8-cores workloads. Moreover, FLRMR improves fairness over LREQ by 77.2% on average.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332168,,no,,no,,,no,undetermined,,no
380,Impact Analysis in the Presence of Dependence Clusters Using Static Execute after in WebKit,"Impact analysis based on code dependence can be an integral part of software quality assurance by providing opportunities to identify those parts of the software system that are affected by a change. Because changes usually have far reaching effects in programs, effective and efficient impact analysis is vital, which has different applications including change propagation and regression testing. Static Execute After (SEA) is a relation on program elements (procedures) that is efficiently computable and accurate enough to be a candidate for use in impact analysis in practice. To assess the applicability of SEA in terms of capturing real defects, we present results on integrating it into the build system of Web Kit, a large, open source software system, and on related experiments. We show that a large number of real defects can be captured by impact sets computed by SEA, albeit many of them are large. We demonstrate that this is not an issue in applying it to regression test prioritization, but generally it can be an obstacle in the path to efficient use of impact analysis. We believe that the main reason for large impact sets is the formation of dependence clusters in code. As apparently dependence clusters cannot be easily avoided in the majority of cases, we focus on determining the effects these clusters have on impact analysis.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392099,,no,,no,,,no,undetermined,,no
381,Investment optimization methodology applied to investments on non-technical losses reduction actions,"This electronic document presents the research developed on a R&D Project for the Energy Recovery Department of Rio de Janeiro's distribution company, Light S.E.S.A. The main purpose is prioritizing the grid investments on non-technical losses reduction actions. The work was developed along eighteen months, and resulted in experimental software. It analyses the historical results of the actions, allocating optimally the resources to a pre-defined period of time. The methodology consists of a statistic model based on historical results processed through a decision three optimization algorithm in order to maximize the objective function. It was tested on decision making process regarding grid investments to reduce non-technical losses, and the Return-on-Investment results were quite satisfactory.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249321,,no,,no,,,no,undetermined,,no
382,Log-based approach for performance requirements elicitation and prioritization,"Requirements engineering activities are a critical part of a project's lifecycle. Success of subsequent project phases is highly dependent on good requirements definition. However, eliciting and achieving consensus on priority between all stakeholders is a complex task. Considering software development of large scale global applications, the challenges increase by the need of managing discussions between groups of stakeholders with different roles and background. This paper presents a practical approach for requirements elicitation and prioritization based on realistic user behaviors observation. It uses basic statistic analysis and application usage information to automatically identify the most relevant requirements for majority of stakeholders. An industry case illustrates the feasibility and efficiency of our approach.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345818,,no,,yes,,,no,undetermined,,no
383,Managing Software Quality Requirements,"This research study explores current quality requirements (QR) management practices in Australian organisations focusing on the elicitation, handling processes, challenges faced, quantification methods used and interdependency management. This research was conducted through six mini case studies, examining organizations that varied in size, structure, industry and function. A mixed methodology was utilised through an online survey for gathering quantitative data and semi-structured interviews for gathering explanatory qualitative data. The results found that five out of the six organisations studied did not have a formal and defined process for the handling of QRs. Large organisations treated QRs are part of their overall project specifications, while smaller organisations saw the management of QRs as more ad hoc. When prioritising QRs, Accuracy was considered the most important priority followed by Security and Reliability. The main challenges that organisations face in their management of QRs is defining and quantifying these requirements.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328174,,no,,no,,no,no,undetermined,,no
389,Predicting the priority of a reported bug using machine learning techniques and cross project validation,"In bug repositories, we receive a large number of bug reports on daily basis. Managing such a large repository is a challenging job. Priority of a bug tells that how important and urgent it is for us to fix. Priority of a bug can be classified into 5 levels from PI to P5 where PI is the highest and P5 is the lowest priority. Correct prioritization of bugs helps in bug fix scheduling/assignment and resource allocation. Failure of this will result in delay of resolving important bugs. This requires a bug prediction system which can predict the priority of a newly reported bug. Cross project validation is also an important concern in empirical software engineering where we train classifier on one project and test it for prediction on other projects. In the available literature, we found very few papers for bug priority prediction and none of them dealt with cross project validation. In this paper, we have evaluated the performance of different machine learning techniques namely Support Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbors (KNN) and Neural Network (NNet) in predicting the priority of the newly coming reports on the basis of different performance measures. We performed cross project validation for 76 cases of five data sets of open office and eclipse projects. The accuracy of different machine learning techniques in predicting the priority of a reported bug within and across project is found above 70% except Naive Bayes technique.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416595,,no,,yes,,,no,undetermined,,no
392,Prioritizing demand response programs from reliability aspect,"In this paper, the impact of demand response programs (DRPs) on reliability improvement of the restructured power systems is quantified. In this regard, the demand response (DR) model which treats consistently the main characteristics of the demand curve is developed for modeling. In proposed model, some penalties for customers in case of no responding to load reduction and incentives for customers who respond to reducing their loads are considered. In order to make analytical evaluation of the reliability, a mixed integer DCOPF is proposed by which load curtailments and generation re-dispatches for each contingency state are determined. Both transmission and generation failures are considered in contingency enumeration. The proposed technique is modeled in the GAMS software and solved using CPLEX. Reliability indices for generation-side, transmission network and whole system are calculated using this technique. Different DRPs based on the DR model are implemented over the IEEE RTS 24-bus test system, and reliability indices for different parties are calculated. Afterward, using proposed performance index, the priority of the considered programs is determined from view point of different market participants.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221578,,no,,no,,,no,undetermined,,no
393,Quality Playbook: Ensuring Release to Release Improvement,"Summary form only given. Before a major feature release is made available to customers, it is important to be able to anticipate if the release will be of lesser quality than its predecessor release. Our research group has developed models that use development and test times, resource levels, code added, and bugs found and fixed (or not fixed) to predict whether or not a new feature release will achieve a key quality goal - to be of better quality than its predecessor release. If the release quality prediction models, developed early in the development branches integration phase, indicate a likely upcoming quality problem in the field, another set of predictive models ('playbook' models) are then developed and used by our team to identify development or test practices that are in need of improvement. These playbook models are key components of what we call 'quality playbooks,' that are designed to address several objectives: . Identify 'levers' that positively influence feature release quality. Levers are in-process engineering metrics that are associated with specific development or test processes/practices and measure their adoption and effectiveness. . If possible, identify levers that can be invoked early in the lifecycle, to enable the development and test teams to improve deficient practices and remediate the current release under development. If it is not possible to identify early levers but possible to identify levers later in the lifecycle, we can only change deficient practices to improve the quality of future successor releases. . Determine the potential quality impact of changes suggested by the profile of significant levers. Low impact levers are likely not to be addressed by development teams. . Determine the resource and schedule investments needed to change and implement practices: Training, disruption, additional engineering time, etc. . Using impact and investment calculations identify which practices to change, either for the current release or just for subsequent releases. Develop a prioritization/ROI scheme to provide planning guidance to development and test teams. . Identify specific practice changes needed, or new practices to adopt. . Design and plan pilot programs to test the models, including the impact and investment components. Using this 'playbook' approach, our team has developed models for 31 major feature releases that are resident on 11 different hardware platforms. These models have identified six narrowly-defined classes of metrics that include both actionable levers and 'indicator' metrics that correlate well with release quality. (Indicator metrics do also correlate well, but are less specifically actionable.) The models for these six classes of metrics (and their associated practices) include strong levers and strong indicators for all releases and platforms thus far examined. Impact and investment results are also described in this paper, as are pilot programs that have tested the validity of the modeling and business calculation results. Two additional large-scale pilots of the 'playbook' approach are underway, and these are also described.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405417,,no,,no,,,no,undetermined,,no
394,Resilient system design for Prognosis and Health Monitoring of an ocean power generator,"In this paper we introduce a new methodology that integrates system resilience engineering and hazard analysis into complex system design. We then demonstrate its performance by applying it to the design of a Prognosis and Health Monitoring (PHM) system for an ocean current power generator. Three common methodologies for system hazard analysis were tested by applying them to the PHM system's network topology architecture; STAMP-based Process Analysis (STPA), Hazard and Operability Analysis (HAZOP), and a Resilience Engineering, Heuristic-based approach. While all three approaches adequately revealed most PHM system hazards, which assisted in identifying the means with which to mitigate them, none of the approaches fully addressed the multi-state dimensionality of the sub-components of the system, missing risky and hazardous scenarios. We developed the System Hazard Indication and Extraction Learning Diagnosis (SHIELD) methodology for system hazard analysis and resilient design. SHIELD integrates state space analysis into the hazard analysis process in order to facilitate the location of undiscovered hazard scenarios. Our approach uses recursive, top-down system decomposition with subsystem, interface, and process cycle identification. Then, a bottom-up recursive evaluation is completed where we analyze the subsystem state space and state transitions with regard to hazards/failures in process cycles. This yields a comprehensive list of failure states and scenarios. Finally, a top-down prioritized application of resilient engineering heuristics which address hazard scenarios is prescribed. This final phase results in a comprehensive, complete analysis of complex system architectures forcing resilience into the final system design.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189490,,no,,no,,,no,undetermined,,no
395,School that has a vision,"The article is devoted to presentation of the concept and results of the Private secondary professional school in Poprad that is actually realization of the vision of two high school teachers on how education might look like at high school. Based on the inclusion of Private secondary professional school into the school system and the subsequent approval of the experimental verification of branches of study the founder began to build a school, which is based on two main priorities - high quality educational process and modern hardware and software. The school where students have their own laptops, use electronic books along with an educational portal, do test and homework via the internet and where parents can check the results by means of the electronic register of students' grades. The school which offers their students new perspective branches of study tailor-made according to the current and future requirements of the market, i.e. without the school subjects that are not relevant anymore, and with an ongoing innovative curriculum; the school with enthusiastic highly qualified teachers using new methods and forms of teaching including modern technique. The school that partners with the university sector to ensure the follow-up study of the graduates provides video conferencing for specialized subjects and conversational courses with schools abroad. Private secondary professional school, Ul.29.augusta 4812 in Poprad, officially began on 01.09.2008, the founder of which is Tatransk?__ Akad??mia.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418322,,no,,no,,,no,undetermined,,no
396,Selecting an appropriate framework for value-based requirements prioritization,"There are usually more requirements than feasible in a given schedule. Thus, it's imperative to be able to choose the most valuable ones for implementation to ensure the delivery of a high value software system. There are myriad requirements prioritization frameworks and selecting the most appropriate one is a decision problem in its own right. In this paper we present our approach in selecting the most appropriate value based requirements prioritization framework as per the requirements of our stakeholders. Based on our analysis a single framework was selected, validated by requirements engineers and project managers and deployed for company-wide use by a major IT player in India.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345819,,no,,no,,,no,undetermined,,no
403,Use of embedded intelligence in tactical grids for energy surety and fuel conservation,"This paper describes a system for creating an energy-sharing infrastructure, effectively creating redundant sources of energy supply and significantly reducing the logistical burdens associated with providing power. An intelligent power management and power grid system has been developed and tested. This system optimizes performance and efficiency through local and system-level autonomous controls. The grid system was based on existing military, trailer-mounted, mobile power equipment. A reduction in fuel consumption of 36 percent was observed. In addition, prioritized load shedding was demonstrated as a means to prevent the generators from being overloaded.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345122,,no,,no,,,no,undetermined,,no
404,Using Non-redundant Mutation Operators and Test Suite Prioritization to Achieve Efficient and Scalable Mutation Analysis,"Mutation analysis is a powerful and unbiased technique to assess the quality of input values and test oracles. However, its application domain is still limited due to the fact that it is a time consuming and computationally expensive method, especially when used with large and complex software systems. Addressing these challenges, this paper makes several contributions to significantly improve the efficiency of mutation analysis. First, it investigates the decrease in generated mutants by applying a reduced, yet sufficient, set of mutants for replacing conditional (COR) and relational (ROR) operators. The analysis of ten real-world applications, with 400,000 lines of code and more than 550,000 generated mutants in total, reveals a reduction in the number of mutants created of up to 37% and more than 25% on average. Yet, since the isolated use of non-redundant mutation operators does not ensure that mutation analysis is efficient and scalable, this paper also presents and experimentally evaluates an optimized workflow that exploits the redundancies and runtime differences of test cases to reorder and split the corresponding test suite. Using the same ten open-source applications, an empirical study convincingly demonstrates that the combination of non-redundant operators and prioritization leveraging information about the runtime and mutation coverage of tests reduces the total cost of mutation analysis further by as much as 65%.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405400,,no,,no,,,no,undetermined,,no
405,Using Prioritized Disk Service to Expedite Program Execution,"Computer systems often host multiple programs in execution simultaneously. Among those running programs, some may be important and time-critical, which users would expect them to finish their execution as soon as possible. Generally speaking, the course of program execution includes CPU operation and hard disk operation (disk I/O). For the CPU operation, modern computer systems have the ability to adjust the CPU scheduling sequence according to program priority. However, most computer systems do not have effective ways to conduct disk I/O based on program priority. The Linux operating system has been widely used in many areas. It supports several disk schedulers. The Complete Fair Queuing (CFQ) and the Anticipatory Scheduling (AS) are among those most well-known. Currently, CFQ is the default disk scheduler in the Linux operating system. AS is the predecessor of CFQ. Unfortunately, CFQ only offers prioritized disk I/O to some extent through the tool ""ionice"", while AS does not provide any prioritized service at all. We propose and implement a new disk scheduler, namely Prioritized Anticipatory Scheduling (PAS), by adding schemes of supporting prioritized disk I/O into AS in the Linux kernel. Our experimental results show that PAS surpasses CFQ with ionice for the vast majority of all test cases. Compared with AS, PAS can improve the performance of programs with high disk I/O priority by up to 71.88%.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332286,,no,,no,,,no,undetermined,,no
406,Value-Based Coverage Measurement in Requirements-Based Testing: Lessons Learned from an Approach Implemented in the TOSCA Testsuite,"Testing is one of the most widely practiced quality assurance measures and also one of the most resource-intensive activities in software development. Still, however, most of the available methods, techniques and tools for software testing are value-neutral and do not realize the potential value contribution of testing. In this paper we present an approach for value-based coverage measurement that can be used to align the testing effort with the achievable value associated with requirements and functional units. It has been implemented as part of a commercial test tool and was successfully applied in real-world projects. The results demonstrated its ability to adequately capture the distribution of the business value and risks involved in different requirements. The paper concludes with sharing important lessons learned from developing value-based coverage measurement in the practical setting of commercial tool development and real-world test projects.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328176,,no,,yes,,,no,undetermined,,no
407,A Comparison of Different Defect Measures to Identify Defect-Prone Components,"(Background) Defect distribution in software systems has been shown to follow the Pareto rule of 20-80. This motivates the prioritization of components with the majority of defects for testing activities. (Research goal) Are there significant variations between defective components and architectural hotspots identified by other defect measures? (Approach) We have performed a study using post-release data of an industrial Smart Grid application with a well-maintained defect tracking system. Using the Pareto principle, we identify and compare defect-prone and hotspots components based on four defect metrics. Furthermore, we validated the quantitative results against qualitative data from the developers. (Results) Our results show that at the top 25% of the measures 1) significant variations exist between the defective components identified by the different defect metrics and that some of the components persist as defective across releases 2) the top defective components based on number of defects could only identify about 40% of critical components in this system 3) other defect metrics identify about 30% additional critical components 4) additional quality challenges of a component could be identified by considering the pair wise intersection of the defect metrics. (Discussion and Conclusion) Since a set of critical components in the system is missed by using largest-first or smallest-first prioritization approaches, this study, therefore, makes a case for an all-inclusive metrics during defect model construction such as number of defects, defect density, defect severity and defect correction effort to make us better understand what comprises defect-prone components and architectural hotspots, especially in critical applications.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693238,,no,,no,,no,no,undetermined,,no
408,A Fuzzy Expert System for Cost-Effective Regression Testing Strategies,"Different testing environments and software change characteristics can affect the choice of regression testing techniques. In our prior work, we developed adaptive regression testing (ART) strategies to investigate this problem. While the ART strategies showed promising results, we also found that the multiple criteria decision making processes required for the ART strategies are time-consuming, often inaccurate and inconsistent, and limited in their scalability. To address these issues, in this research, we develop and empirically study a fuzzy expert system (FESART) to aid decision makers in choosing the most cost-effective technique for a particular software version. The results of our study show that FESART is consistently more cost-effective than the previously proposed ART strategies. One of the biggest contributors to FESART being more cost-effective is the reduced time required to apply the strategy. This contribution has significant impact because a strategy that is less time-consuming will be easier for researchers and practitioners to adopt, and will provide even greater cost-savings for regression testing sessions.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676871,,no,,no,,,no,undetermined,,no
417,Closed-loop analysis of soft decisions for serial links,"We describe the benefit of using closed-loop measurements for a radio receiver paired with a counterpart transmitter. We show that real-time analysis of the soft decision output of a receiver can provide rich and relevant insight far beyond the traditional hard-decision bit error rate (BER) test statistic. We describe a Soft Decision Analyzer (SDA) implementation for closed-loop measurements on single- or dual-(orthogonal) channel serial data communication links. The analyzer has been used to identify, quantify, and prioritize contributors to implementation loss in live-time during the development of software defined radios. This test technique gains importance as modern receivers are providing soft decision symbol synchronization as radio links are challenged to push more data and more protocol overhead through noisier channels, and software-defined radios (SDRs) use error-correction codes that approach Shannon's theoretical limit of performance.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737541,,no,,no,,,no,undetermined,,no
418,Compositional analysis of switched Ethernet topologies,"In this paper we study distributed automotive control applications whose tasks are mapped onto different ECUs communicating via a switched Ethernet network. As traditional automotive communication buses like CAN, FlexRay, LIN and MOST are gradually reaching their performance limits because of the increasing complexity of automotive architectures and applications, Ethernet-based in-vehicle communication systems have attracted a lot of attention in recent times. However, currently there is very little work on systematic timing analysis for Ethernet which is important for its deployment in safety-critical scenarios like in an automotive architecture. In this work, we propose a compositional timing analysis technique that takes various features of switched Ethernet into account like network topology, frame priorities, communication delay, memory requirement on switches, performance, etc. Such an analysis technique is particularly suitable during early design phases of automotive architectures and control software deployment. We demonstrate its use in analyzing mixed-criticality traffic patterns consisting of messages from performance-oriented control loops and timing-sensitive real-time tasks. We further evaluate the tightness of the obtained analytical bounds with an OMNeT++ based network simulation environment, which involves long simulation time and does not provide formal guarantees.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513677,,no,,no,,,no,undetermined,,no
420,Data Centers as Software Defined Networks: Traffic Redundancy Elimination with Wireless Cards at Routers,"We propose a novel architecture of data center networks (DCN), which adds wireless network card to both servers and routers. Existing traffic redundancy elimination (TRE) mechanisms reduce link loads and increase network capacity in several environments by removing strings that have appeared in earlier packets through encoding and decoding them several hops downstream. This article is the first to explore TRE mechanisms in large-scale DCNs and the first to exploit cooperative TRE among servers. Moreover, it also achieves the `logically centralized' control over the physically distributed states in emerging software defined networks (SDN) paradigm, by sharing information among servers and routers in data centers with wireless cards. We first formulate the TREDaCeN (TRE in Data Center Networks) problem and reduce the cycle cover problem to prove that finding an optimal caching task assignment for TREDaCeN problem is NP-hard. We further describe an offline TREDaCeN algorithm which is proved to have good approximation ratio. We then discuss efficient online zero-delay and semi-distributed implementations of TREDaCeN supported by physical proximity of servers and routers, enabling status updates in a single wireless transmission, using an efficient prioritized schedule. We also address online cache replacement and consistency of information in servers and routers with and without delay. Our framework is tested on different parameters and shows superior performance in comparison to other mechanisms (imported directly to this setting). Our results show the robustness and the trade-off between the `logically centralized' implementation and the overhead on handling inconsistency of distributed information in DCN.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678112,,no,,no,,,no,undetermined,,no
422,Developing new Automatic Test Equipments (ATE) using systematic design approaches,"Keeping Automatic Test Equipments (ATE) current with technology is one of the major challenges in automatic testing world. Needs and priorities can quickly evolve throughout the life cycle of ATEs and handling obsolescence via performing upgrades on hardware and software can be impossible after several years. While developing Test Program Sets (TPS), if existing ATE systems cannot meet the necessary requirements without inserting extra test devices or decreasing test coverage, then designing a new ATE can be seen inevitable. If a new ATE system is to be designed, it is very crucial that the requirements for the new ATE system should be identified before design process begins. Determining the requirements is a very critical stage in designing ATE because if enough effort is not focused and extended analysis is not carried out on determining the requirements, then the newly formed ATE system will likely fail to cover the test requirements of the DUTs. Setting up the hardware and software architecture is the next stage after the process of determining the requirements of the ATE system. Practical and cost effective solutions should be considered without compromising performance and capabilities of the test devices. The architectures should be suitable for future enhancements to the system. Throughout the design process, the design requirements, critical design descriptions, verification and validation procedures should be clearly documented and reviewed with relevant engineers. In this paper, the design process of a new ATE system by using systematic design approach is discussed. This process is followed during the design of the ATE system which is under use from the beginning of the year 2013. The challenges in the design process, determining the requirements and the formation of hardware and software architecture are explained in detail benefiting from real experiences.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645035,,no,,yes,,,no,undetermined,,no
425,Getting more from requirements traceability: Requirements testing progress,"Requirements Engineering (RE) and Testing are important steps in many software development processes. It is critical to monitor the progress of the testing phase to allocate resources (person-power, time, computational resources) properly, and to make sure the prioritization of requirements are reflected during testing, i.e. more critical requirements are given higher priority and tested well. In this paper, we propose a new metric to help stakeholders monitor the progress of the testing phase from a requirements perspective, i.e. which requirements are tested adequately, and which ones insufficiently. Unlike existing progress related metrics, such as code coverage and MC/DC (modified condition/decision) coverage, this metric is on the requirements level, not source code level. We propose to automatically reverse engineer this metric from the existing test cases of a system. We also propose a method to evaluate this metric, and report the results of three case studies. On these case studies, our technique obtains results within 75.23% - 91.11% of the baseline on average.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620148,,no,,no,,,no,undetermined,,no
427,"Leveraging the Crowd: How 48,000 Users Helped Improve Lync Performance","Performance is a critical component of customer satisfaction with network-based applications. Unfortunately, accurately evaluating the performance of collaborative software that operates in extremely heterogeneous environments is difficult with traditional techniques such as modeling workloads or testing in controlled environments. To evaluate performance of an application in the wild during development, the authors deployed early versions of the software, collecting performance data from application users for key usage scenarios. The analysis package they used produces visualizations to help development teams identify and prioritize performance issues by focusing on performance early in the development cycle, evaluating progress, identifying defects, and estimating timelines.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509371,,no,,no,,,no,undetermined,,no
428,Managing technical debt: An industrial case study,"Technical debt is the consequence of trade-offs made during software development to ensure speedy releases. The research community lacks rigorously evaluated guidelines to help practitioners characterize, manage and prioritize debt. This paper describes a study conducted with an industrial partner during their implementation of Agile development practices for a large software development division within the company. The report contains our initial findings based on ethnographic observations and semi-structured interviews. The goal is to identify the best practices regarding managing technical debt so that the researchers and the practitioners can further evaluate these practices to extend their knowledge of the technical debt metaphor. We determined that the developers considered their own taxonomy of technical debt based on the type of work they were assigned and their personal understanding of the term. Despite management's high-level categories, the developers mostly considered design debt, testing debt and defect debt. In addition to developers having their own taxonomy, assigning dedicated teams for technical debt reduction and allowing other teams about 20% of time per sprint for debt reduction are good initiatives towards lowering technical debt. While technical debt has become a well-regarded concept in the Agile community, further empirical evaluation is needed to assess how to properly apply the concept for various development organizations.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608672,,no,,no,,,no,undetermined,,no
429,On Combining Model-Based Analysis and Testing,"Testing a computer system is a challenging task, both due to the large number of possible test cases and the limited resources allocated for testing activities. This means that only a subset of all possible test cases can be chosen to test a system, and therefore the decision on the selection of test cases becomes important. The result of static analysis of a system can be used to help with this decision, in the context of model-based development of systems, this means that the analysis performed on a system model can be used to prioritize and guide the testing efforts. Furthermore, since models allow expression of non-functional requirements (such as performance, timing and security), model-guided testing can be used to direct testing towards specific parts of the system which have large impact on such requirements. In this paper, we focus on modeling and trade-off analysis of non-functional requirements and how static analysis helps to identify problematic parts of a system and thus guide the selection of test cases to target such parts.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614319,,no,,yes,,,no,undetermined,,no
430,On the Correlation between the Effectiveness of Metamorphic Relations and Dissimilarities of Test Case Executions,"Metamorphic testing (MT) is a property-based automated software testing method. It alleviates the oracle problem by testing programs against metamorphic relations (MRs), which are necessary properties among multiple executions of the target program. For a given problem, usually more than one MR can be identified. It is therefore of practical importance for testers to know the nature of good MRs, that is, which MRs are likely to have higher chances of revealing failures. To address this issue we investigate the correlation between the fault-detection effectiveness of MRs and the dissimilarity (distance) of test case execution profiles. Empirical study results reveal that there is a strong and statistically significant positive correlation between the fault-detection effectiveness and the distance. The findings of this research can help to develop automated means of selecting/prioritizing MRs for cost-effective metamorphic testing.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605921,,no,,no,,no,no,undetermined,,no
439,RisCal -- A Risk Estimation Tool for Software Engineering Purposes,"Decision making in software engineering requires the consideration of risk information. The reliability of risk information is strongly influenced by the underlying risk estimation process which consists of the steps risk identification, risk analysis and risk prioritization. In this paper we present a novel risk estimation tool for software engineering pruposes called RisCal. RisCal is based on a generic risk model and supports the integration of manually and automatically determined metrics into the risk estimation. This makes the tool applicable for arbitrary software engineering activities like risk-based testing or release planning. We show how RisCal supports risk identification, analysis and prioritizations, provide an estimation example, and discuss its application to risk-based testing and release planning.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619524,,yes,,no,,no,no,undetermined,,no
441,SMT Malleability in IBM POWER5 and POWER6 Processors,"While several hardware mechanisms have been proposed to control the interaction between hardware threads in an SMT processor, few have addressed the issue of software-controllable SMT performance. The IBM POWER5 and POWER6 are the first high-performance processors implementing a software-controllable hardware-thread prioritization mechanism that controls the rate at which each hardware-thread decodes instructions. This paper shows the potential of this basic mechanism to improve several target metrics for various applications on POWER5 and POWER6 processors. Our results show that although the software interface is exactly the same, the software-controlled priority mechanism has a different effect on POWER5 and POWER6. For instance, hardware threads in POWER6 are less sensitive to priorities than in POWER5 due to the in order design. We study the SMT thread malleability to enable user-level optimizations that leverage software-controlled thread priorities. We also show how to achieve various system objectives such as parallel application load balancing, in order to reduce execution time. Finally, we characterize user-level transparent execution on POWER5 and POWER6, and identify the workload mix that best benefits from it.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138854,,no,,no,,,no,undetermined,,no
442,Software components prioritization using OCL formal specification for effective testing,"In soft real time system development, testing effort minimization is a challenging task. Earlier research has shown that often a small percentage of components are responsible for most of the faults reported at the later stages of software development. Due to the time and other resource constraints, fault-prone components are ignored during testing activity which leads to compromises on software quality. Thus there is a need to identify fault-prone components of the system based on the data collected at the early stages of software development. The major focus of the proposed methodology is to identify and prioritize fault-prone components of the system using its OCL formal specifications. This approach enables testers to distribute more effort on fault-prone components than non fault-prone components of the system. The proposed methodology is illustrated based on three case study applications.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844288,,no,,no,,,no,undetermined,,no
443,Software defect prediction using software metrics - A survey,"Traditionally software metrics have been used to define the complexity of the program, to estimate programming time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the metric values are to be used in mathematical equations designed to represent a model of the software process, metrics associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully apply. Work on the mechanics of implementing metrics programs. The goal of this research is to help developers identify defects based on existing software metrics using data mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in the development and maintenance phase. This research focuses in identifying defective modules and hence the scope of software that needs to be examined for defects can be prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology helps in identifying modules that require immediate attention and hence the reliability of the software can be improved faster as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the Data mining algorithm. To initiate this process we initially propose to evaluate the existing classification algorithms and based on its weakness we propose a novel Neural network algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508369,,no,,no,,,no,undetermined,,no
444,Suitable placements of multiple FACTS devices to improve the transient stability using trajectory sensitivity analysis,"Trajectory sensitivity analysis (TSA) is used as a tool for suitable placement of multiple series compensators in the power system. The goal is to maximize the benefit of these devices in order to enhance the transient stability of the system. For this purpose, the trajectory sensitivities of the rotor angles of the most critical generators with respect to the reactances of transmission lines are calculated in the presence of the most severe faults. Based on the obtained trajectory sensitivities, a method is proposed to determine how effective the series compensation of each transmission line is for improving the transient stability. This method is applied to the Nordic-32 test system to find the priorities of the transmission lines for installation of several series compensators. Simulation with industrial software shows the validity and efficiency of the proposed method.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666828,,no,,no,,,no,undetermined,,no
451,A distributed algorithm for virtual traffic lights with IEEE 802.11p,"A virtual traffic light (VTL) is a mechanism that allows vehicles to autonomously solve priorities at road junctions in the absence of fixed infrastructures (i.e., conventional traffic lights). To develop an effective VTL system, communication between vehicles is a crucial factor and can be handled either using cellular infrastructure or adopting a vehicle-to-vehicle (V2V) communication paradigm. In this paper we present the design, the implementation, and the field trial of a VTL which exploits V2V communications based on IEEE 802.11p. Specif-ically, we propose a decentralized algorithm, that adopts both broadcast signaling and unicast messages to assign priorities to the vehicles approaching intersections, thus preventing accidents and reducing traffic congestions. The algorithm has been tested both in a controlled laboratory environment and in a field trial with equipped vehicles.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6882621,,no,,,yes,,no,undetermined,,no
453,A Method to Test the Information Quality of Technical Documentation on Websites,"In software engineering, testing is one of the corner-stones of quality assurance. The idea of software testing can be applied to information quality as well. Technical documentation has a set of intended uses that correspond to use cases in a software system. Documentation is, in many cases, presented via software systems, e.g., web servers and browsers, and contains software, e.g., Javascript for user interaction, animation, and customization, etc. This makes it difficult to find a clear-cut distinction between a software system and technical documentation. However, we can assume that each use case of a technical documentation involves retrieval of some sort of information that helps a user answer a specific questions. To assess information testing as a method, we implemented QAnalytics, a tool to assess the information quality of documentation that is provided by a website. The tool is web-based and allows test managers and site owners to define test cases and success criteria, disseminate the test cases to testers, and to analyze the test results. This way, information testing is easily manageable even for non-technical stakeholders. We applied our testing method and tool in a case study. According to common perception, the website of Linnaeus University needs to be re-engineered. Our method and tool helped the stakeholders identify what information is presented well and which parts of the website that need to change. The test results allowed the design and development effort to prioritize actual quality issues and potentially save time and resources.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958417,,no,,,no,,no,undetermined,,no
456,Applying Parameter Value Weighting to a Practical Application,"This paper reports a case study where pair-wise testing was applied to a real-world program. In particular we focus on weighting, an added feature which allows the tester to prioritize particular parameter values. In our previous work we proposed a weighting method that can reflect given weights in the resulting test suite more directly than can existing methods. To asses the effects of weighting in a practical testing process, we compare the number of execution times of the program's methods among three pair-wise test suites, including the test suite generated by our weighting method and those generated by an existing test case generation tool with and without the weighting option. The results show that the effects of weighting were most clearly observed when our weighting method was used.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983821,,no,,,yes,,no,undetermined,,no
457,Athena: A Visual Tool to Support the Development of Computational Intelligence Systems,"Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984580,,no,,,yes,no,no,undetermined,,no
458,Automated Prioritization of Metrics-Based Design Flaws in UML Class Diagrams,"The importance of software architecture in software development prolongs throughout the entire software life cycle. This is because quality of the architectural design defines the structural aspects of the system that are difficult to change, and hence will affect most of the subsequent development and maintenance activities. This paper considers software design flaws (related to the system structure) and not flaws identified at run time (by testing). These design flaws are akin to what is described in the literature as anti-patterns, bad smells or rotting design. Recently, two tools that have been developed for quality assurance of software designs represented in the UML notation: SDMetrics and Metric View. However these tools are not considered practical because they report many design flaws which are not considered by developers (false positives). This paper explores an approach that tries to identify which design flaws should be considered important and which are not. To this end, we propose an approach for automated prioritization of software design flaws (BX approach), to facilitate developers to focus on important design flaws more effectively. We designed and implemented a tool (PoSDef) that implements this approach. The BX approach and the PoSDef tool have been validated using two open source projects and one large industrial system. Our validation consists of comparing our approach and tool with the existing design flaw tools. The evaluation has shown that the proposed approach could facilitate developers to identify and prioritize important design flaws effectively.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928837,,no,,,yes,no,no,undetermined,,no
459,Bench level automotive EMC validation test laboratory challenges and preferences,"Automotive original equipment manufacturers (OEMs) and suppliers of electrical and electronic subsystems are required to perform bench level electrical and electromagnetic compatibility (EMC) validation testing. This is an important process that requires a significant investment in time and money. In an effort to improve the efficiency of testing, a survey was developed to gain an understanding of the challenges faced by test laboratories and also their preferences. This paper summarizes the results of the survey. Given the amount of time and money spent annually for the type of testing considered, the results suggest that pursuing improvements will result in a long term savings for the original equipment manufacturers (OEMs), suppliers, and labs involved. In order to increase test efficiency, the OEMs, suppliers, and laboratories will need to work together to better prepare test plans and test setups. It is suggested that the results of this survey are used to prioritize the improvement activities.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898989,,no,,,no,no,no,undetermined,,no
460,Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines,"Large Software Product Lines (SPLs) are common in industry, thus introducing the need of practical solutions to test them. To this end, t-wise can help to drastically reduce the number of product configurations to test. Current t-wise approaches for SPLs are restricted to small values of t. In addition, these techniques fail at providing means to finely control the configuration process. In view of this, means for automatically generating and prioritizing product configurations for large SPLs are required. This paper proposes (a) a search-based approach capable of generating product configurations for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of product configurations. Both these techniques employ a similarity heuristic. The ability of the proposed techniques is assessed in an empirical study through a comparison with state of the art tools. The comparison focuses on both the product configuration generation and the prioritization aspects. The results demonstrate that existing t-wise tools and prioritization techniques fail to handle large SPLs. On the contrary, the proposed techniques are both effective and scalable. Additionally, the experiments show that the similarity heuristic can be used as a viable alternative to t-wise.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823132,,no,,,yes,,no,undetermined,,no
461,Competence transfer through enterprise mobile application development,"Large world corporations need corresponding information technology (IT) support as well as constant improvement of software tools, which should enable further business development and more efficient work to operational organizations. The main interest of IT support organizations is currently more and more connected with mobile IT equipment of the employees. Specific business mobile applications improve the efficiency and result in new operating possibilities. The cooperation between Ericsson Nikola Tesla (ENT) and Faculty of Electrical Engineering and Computing (FER) of the Zagreb University, has enabled a fast development of competencies in ENT, necessary for mobile application development and quality realization of innovative solution for platforms iOS and Android. In this cooperation at the project realization the methodology of application development was defined, the corresponding competencies were developed, the system architecture was designed together with the communication of mobile application and back-end IT systems. In the project the iterative development approach, tools for software code versioning and project control have been used thus enabling continuous insight in project progressing. At any moment it was possible to determine priorities of functional development and solution elements, as well as to realize the necessary additional transfer of knowledge. The project has resulted in an enterprise mobile application for iOS and Android platforms, which had been implemented and tested in several countries. The complete solution enables data management by means of the portal, thus decreasing the frequent changes of users' mobile application and significantly accelerating the process of new functionalities introduction.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859609,,no,,,no,no,no,undetermined,,no
462,Development and design of a platform for arbitration and sharing control applications,"In this paper, a description of the ADAS development platform in DESERVE project framework is presented. This work is framed within the Sub Project 2 (Development platform) of DESERVE project, and it is divided in 6 different and complementary lines of work. Most of the functions described in the tools and development systems, perception layer and the platform system architecture show the modularity and scalability of our proposal. Moreover, based on vehicle modelling, driver behaviour and intention, a first approach for arbitration and control strategies, which can anticipate the priorities on the control in emergency situations, is described. Furthermore, some simulations will allow the virtual testing for the future implementation in demonstrators. The presented work is the core of DESERVE project, and it is developed in parallel with Driver behaviour and HMI activities (SP3). This work presents some of the achievements in SP2, mainly the application platform integration in one of the demonstrators, along with the arbitration and sharing control, based on intelligent techniques (Fuzzy logic). Simulation shows the feasibility of proposal. This approach will be tested, integrated and validated in a real vehicle in the next stages of the project.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6893228,,no,,,no,,no,undetermined,,no
464,DITEC (DoD-Centric and Independent Technology Evaluation Capability): A Process for Testing Security,"Information Assurance (IA) is one of the Department of Defense's (DoD) top priorities today. IA technologies are constantly evolving to protect critical information from the growing number of cyber threats. Furthermore, DoD spends millions of dollars each year procuring, maintaining, and discontinuing various IA and Cyber technologies. Today, there is no process and/or standardized method for making informed decisions about which IA technologies are better/best. Due to this, efforts for selecting technologies go through very disparate evaluations that are often times non-repeatable and very subjective. DITEC (DoD-centric and Independent Technology Evaluation Capability) is a new capability that streamlines IA technology evaluation. DITEC defines a Process for evaluating whether or not a product meets DoD needs, Security Metrics for measuring how well needs are met, and a Framework for comparing various products that address the same IA technology area. DITEC seeks to reduce the time and cost of creating a test plan and expedite the test and evaluation effort for considering new IA technologies, consequently streamlining the deployment of IA products across DoD and increasing the potential to meet its needs.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825634,,no,,,no,,no,undetermined,,no
467,Enabling Prioritized Cloud I/O Service in Hadoop Distributed File System,"Cloud computing has become more and more popular nowadays. Both governments and enterprises provide service through the construction of public and private clouds accordingly. Among the platforms used in cloud computing, Hadoop is considered one of the most practical and stable systems. Nevertheless, as with other regular software, Hadoop still needs to rely on the underlying operating system to communicate with hardware to function appropriately. For modern computer systems, CPUs excessively outrun hard drives (hard disks). The computer hard disk has become a major bottleneck to the overall system performance. Consequently, computer programs can execute faster if their corresponding I/O operation can be completed sooner. This is important in particular when we want to expedite the execution of urgent programs in a busy system. Unfortunately, under the current Hadoop environment, users cannot prioritize operation of disk and memory for programs which they would like them to run faster. With the help of prioritized I/O service we developed earlier, we proposed and implemented a Hadoop environment with the ability of providing prioritized I/O service. Our Hadoop environment could accelerate the execution of programs with high priority assigned by users. We evaluated our design by executing prioritized programs in environments with different busy levels. Experimental results show that programs can improve their performance by up to 33.79% if executed with high priority.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056748,,no,,,yes,yes,no,undetermined,,no
468,Exploring Model-Based Repositories for a Broad Range of Industrial Applications and Challenges,"Nowadays, systems are becoming increasingly complex and large and the process of developing such large-scale systems is becoming complicated with high cost and enormous effort required. Such a complicated process has a prominent challenge to ensure the quality of delivered artifacts. Therefore there is clearly a need to facilitate reuse of developed artifacts (e.g., requirements, architecture, tests) and enable automated analyses such as risk analyses, prioritizing test cases, change impact analysis, with the objective to reduce cost, effort and improve quality. Model-based engineering provides a promising mechanism to facilitate reuse and enable automation. The key idea is to use models as the backbone of structuring repositories that contain reusable artifacts (e.g., test cases, requirements). Such a backbone model is subse-quently used to enable various types of automation such as model-based testing and automated rule verification. In this paper, we report 12 industrial projects from five different industry domains that all require the construction of model-based repositories to enable various types of automation. We believe using models as the backbone to structure repositories for the purpose of enabling different types of automation in different contexts is a new and non-conventional model-based development research approach. This exploratory paper will serve the basis for future research to derive a generic model-based repository.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958385,,no,,,yes,no,no,undetermined,,no
469,Fair and delay adaptive scheduler for UC and NGN networks,"Fair bandwidth allocation while conforming to stringent end-to-end delay constraints is a major requirement for the successful delivery of next generation QoS demanding traffic. Research has been carried out in the area of processor and scheduler sharing for decades to try to achieve the QoS requirements of network traffic. Fairness and traffic prioritization are two main objectives that many schedulers were originally designed to meet. Another important issue is how schedulers treat the delay sensitive traffic. Although the combination of fairness and prioritization is implemented in several schedulers but, to our knowledge, incorporating adaptive traffic delay treatment in fair and prioritized schedulers has not yet been successfully implemented. In this paper, we introduce a new scheduler that balances between the fairness of bandwidth allocation between flows while implementing prioritization and minimizes the number of end-to-end delay bound breaches. The scheduler combines the virtual clock concept used in well-known fair schedulers together with schedulability testing and evaluation implemented in delay sensitive schedulers. The scheduler is designed to achieve the fairness of bandwidth allocation, such as in fair schedulers, while minimizing the number of possible violation of end-to-end QoS delay of individual flows' packets.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900970,,no,,,yes,no,no,undetermined,,no
470,Genetic algorithm secure procedures algorithm to manage data integrity of test case prioritization methodology,"The focus of present research paper is to manage data integrity and trustworthiness of issues involved in software testing phase of software development life cycle. Many times, it seems that, data integration left behind the software testing phase and it is directly focused at software deployment or delivery time. To avoid issues due to lack of data integration, we developed algorithm which can track integration of test case prioritization along with trustworthiness of test case execution. This paper demonstrates how software testing can be efficient with management of data integrity factor to avoid major security issues. One of the main advantages of our approach is that domain specific semantics can be integrated with the data quality test cases prioritization, thus being able to discover test feed data quality problems beyond conventional quality measures.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030880,,yes,,,yes,no,yes,undetermined,no,no
471,How Does the UML Testing Profile Support Risk-Based Testing,"The increasing complexity of software-intensive systems raises a lot of challenges demanding new techniques for ensuring their overall quality. The risk of not meeting the expected level of quality has negative impact on business, customers, environment and people, especially in the context of safety/security-critical systems. The importance of risk assessment, analysis and management has been well understood both in the literature and practice, which has led to the definition of a number of well-known standards. In the recent years, Risk-Based Testing (RBT) is gaining more attention, especially focusing on test prioritization and selection based on risks. On the other hand, model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software-intensive systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in literature and practice in the last decade. In this paper, we study the feasibility of combining RBT with MBT by using the upcoming version of UML Testing Profile (UTP 2) as the mechanism. We present potential traceability between RBT and UTP 2 concepts.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983859,,no,,,yes,,no,undetermined,,no
472,Identifying usability risk: A survey study,"As defined in various quality models, usability is recognized as an important attribute of software quality. Failing to address usability requirements in a software product could lead to poor quality and high usability problems in software product. Research is still in progress to introduce the best methods for reducing usability problems and increase the rate of successful usable software products. Studies have shown that problems in software products can also be controlled using Software Risk Management methods, even though these problems cannot be eliminated totally. Using Software Risk Management, problems in software products are dealt before it occurs. This paper presents usability problems as a risk factor and by managing usability risk at earlier phases of the development process, successful and high usability software products can be developed. Unfortunately, currently there is little effort in identifying, analyzing and prioritizing potential usability risks at earlier phases of the development process. This paper focuses on usability risk identification as it is the first stage in usability risk management. This paper presents the results of an industry survey based on the opinion of Malaysian Public Sector involving sample size of 330 software developers and software projects managers regarding potential usability risk that could occur during Software Development Life Cycle (SDLC). Our finding has identified 42 potential usability risks, defined as a list for further risk analysis in future.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986005,,no,,,no,no,no,undetermined,,no
475,"Planning of Prioritized Test Procedures in Large Integrated Systems: Best Strategy of Defect Discovery and Early Stop of Testing Session, The Selex-ES Experience","Large integrated systems verification can be made more efficient if driven by specific strategy, classification and prioritization of test procedures is the Selex ES way to speed up important defects discovery and cost of testing activities.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983814,,yes,,,yes,,yes,undetermined,no,no
476,Practical Software Quality Prediction,"Software systems continue to play an increasingly important role in our daily lives, making the quality of software systems an extremely important issue. Therefore, a significant amount of recent research focused on the prioritization of software quality assurance efforts. One line of work that has been receiving an increasing amount of attention is Software Defect Prediction (SDP), where predictions are made to determine where future defects might appear. Our survey showed that in the past decade, more than 100 papers were published on SDP. Nevertheless, the practical adoption of SDP to date is limited. In this paper, we highlight the findings of our thesis, which identifies the challenges that hinder the adoption of SDP in practice. These challenges include the fact that the majority of SDP research rarely considers the impact of defects when performing their predictions, seldom provides guidance on how to use the SDP results, and is too reactive and defect-centric in nature. Therefore, we propose approaches that tackle these challenges. First, we present approaches that predict high-impact defects. Our approaches illustrate how SDP research can be tailored to consider the impact of defects when making their predictions. Second, we present approaches that simplify SDP models so they can be easily understood and illustrates how these simple models can be used to assist practitioners in prioritizing the creation of unit tests in large software systems. These approaches illustrate how SDP research can provide guidance to practitioners using SDP. Then, we argue that organizations are interested in proactive risk management, which covers more than just defects. For example, risky changes may not introduce defects but they could delay the release of projects. Therefore, we present an approach that predicts risky changes, illustrating how SDP can be more encompassing (i.e., by predicting risk, not only defects) and proactive (i.e., by predicting changes before they are incorporated into the code base).",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976158,,no,,,no,,no,undetermined,,no
478,Quality of Service (QoS)-Guaranteed Network Resource Allocation via Software Defined Networking (SDN),"Quality of Service (QoS) -- based bandwidth allocation plays a key role in real-time computing systems and applications such as voice IP, teleconferencing, and gaming. Likewise, customer services often need to be distinguished according to their service priorities and requirements. In this paper, we consider bandwidth allocation in the networks of a cloud carrier in which cloud users' requests are processed and transferred by a cloud provider subject to QoS requirements. We present a QoS-guaranteed approach for bandwidth allocation that satisfies QoS requirements for all priority cloud users by using Open vSwitch, based on software defined networking (SDN). We implement and test the proposed approach on the Global Environment for Networking Innovations (GENI). Experimental results show the effectiveness of the proposed approach.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945296,,no,,,no,,no,undetermined,,no
481,Reverse Engineering Complex Feature Correlations for Product Line Configuration Improvement,"As a Software Product Line (SPL) evolves with increasing number of variant features and feature values, the feature correlations become extremely intricate. However, these correlations are often incompletely documented (e.g., In feature models) so that most features can only be configured manually. In order to make product configuration processes more efficient, we present an approach to extracting complex feature correlations from existing product configurations using association mining techniques. Then these correlations are pruned and prioritized in order to minimize the effort of correlation validation. Our approach is conducted on an industrial SPL with 100 product configurations across 480 features. While 80 out of the 100 configurations are used as training data to automatically extract 4834 complex feature correlations, the rest 20 configurations are used as test data to evaluate the improvement potential of configuration efficiency. In the end, avg. 25% features in each of the 20 products can be configured automatically.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928830,,no,,,yes,,no,undetermined,,no
482,RSUs placement using overlap based greedy method for urban and rural roads,"The deployment of efficient roadside networks is a necessity for ITS deployment. The main challenge for the roadside deployment is to find a satisfying or best distribution of RSUs on the roads network according to the given conditions in order to meet the requested requirements of the roads operator. Additionally, various factors affect this process such as traffic, infrastructure and topological characteristics of the roads. This paper introduces an Overlap based Greedy Method (OGM) as a basis for RSUs placement and can be applied on urban and rural roads. This method in its current development mainly considers the RSU coverage radius and the overlap rate into the RSUs distribution process. Moreover, a wide range of influencing factors is considered through the prioritization of Sites of Interest (SoIs). The OGM method is developed within the recently started PRONET project and is integrated into its software platform. The tests conducted on selected roads environment in the city of Rostock (Germany) ensure: (1) decrease in number of chosen SoIs for placement compared to the original scanned number of SoIs, and (2) full (continuous) coverage cannot be guaranteed for a roads network using only the available junctions.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000905,,no,,,no,no,no,undetermined,,no
483,SimLatte: A Framework to Support Testing for Worst-Case Interrupt Latencies in Embedded Software,"Embedded systems tend to be interrupt-driven, yet the presence of interrupts can affect system dependability because there can be delays in servicing interrupts. Such delays can occur when multiple interrupt service routines and interrupts of different priorities compete for resources on a given CPU. For this reason, researchers have sought approaches by which to estimate worst-case interrupt latencies (WCILs) for systems. Most existing approaches, however, are based on static analysis. In this paper, we present SIMLATTE, a testing-based approach for finding WCILs. SIMLATTE uses a genetic algorithm for test case generation that converges on a set of inputs and interrupt arrival points that are likely to expose WCILs. It also uses an opportunistic interrupt invocation approach to invoke interrupts at a variety of feasible locations. Our evaluation of SIMLATTE on several non-trivial embedded systems reveals that it is considerably more effective and efficient than random testing. We also determine that the combination of the genetic algorithm and opportunistic interrupt invocation allows SIMLATTE to perform better than it can when using either one in isolation.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823893,,no,,,no,,no,undetermined,,no
489,Test Suite Reduction by Combinatorial-Based Coverage of Event Sequences,"Combinatorial-based criteria are useful in several studies for test suite generation, prioritization, and minimization. In this paper, we extend previous work by using combinatorial-based criteria for test suite reduction. We use criteria that are based on combinatorial coverage of events and consider the order in which events occur. A simple combinatorial-based criterion covers t-way events and does not differentiate between the order of events. The event pair (e<sub>1</sub>, e<sub>2</sub>) is counted the same as if it occurs in the order (e<sub>2</sub>, e<sub>1</sub>). We also use two criteria that consider the order in which events occur since different orderings of events may be valuable during testing. First, the consecutive sequence-based criterion counts all event sequences in different orders, but they must occur adjacent to each other. The sequence-based criterion counts pairs in all orders without the requirement that events must be adjacent. We evaluate the new criteria on three GUI applications. We use 2way inter-window coverage in our studies. All of the 2way combinatorial-based criteria are effective in reducing the test suites and maintaining close to 100% fault finding effectiveness. Our future work examines larger test suites, higher strength coverage, techniques to partition event data, and further empirical studies.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825647,,yes,,,no,,no,undetermined,,no
490,Testability of object-oriented systems: An AHP-based approach for prioritization of metrics,This paper investigates testability from the perspective of metrics used in an object-oriented system. The idea is to give an overview of object oriented design metrics with the prioritization of same keeping testability as the overall goal. We have used Analytic Hierarchy Process (AHP) method to attain which metric is mostly used and is best for testability.,2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019595,,no,,,yes,no,no,undetermined,,no
492,Toolset and Program Repository for Code Coverage-Based Test Suite Analysis and Manipulation,"Code coverage is often used in academic and industrial practice of white-box software testing. Various test optimization methods, e.g. Test selection and prioritization, rely on code coverage information, but other related fields benefit from it as well, such as fault localization. These methods require access to the fine details of coverage information and efficient ways of processing this data. The purpose of the (free) SoDA library and toolset is to provide an efficient set of data structures and algorithms which can be used to prepare, store and analyze in various ways data related to code coverage. The focus of SoDA is not on the calculation of coverage data (such as instrumentation and test execution) but on the analysis and manipulation of test suites based on such information. An important design goal of the library was to be usable on industrial-size programs and test suites. Furthermore, there is no limitation on programming language, analysis granularity and coverage criteria. In this paper, we demonstrate the purpose and benefits of the library, the associated toolset, which also includes a graphical user interface, as well as possible usage scenarios. SoDA also includes a repository of prepared programs, which are from small to large sizes and can be used for experimentation and as a benchmark for code coverage related research.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975635,,no,,,no,,no,undetermined,,no
493,Using Dual Priority scheduling to improve the resource utilization in the nMPRA microcontrollers,"The current practice in most of the safety-critical areas, including automotive, avionics systems and factory automation, encouraging the use of real-time time-trigger schedulers that does not allow interference to take place between safety-critical components and non-critical. Furthermore, in these systems the lack of interference between safety-critical components and non-critical components is achieved by a strict isolation between components with different degrees of severity. This approach can assure, easily, the certification of the safety-critical functionality, but leads to very low resource utilization. For this purpose it will be presented a solution that when the system enters into a state that is different from the normal running state (test service), allowing relaxation and a change in the activation time of tasks (release) violating the fixed priorities scheduling, but avoiding starvation of the system tasks. The proposed solution modifies a static scheduler in a dynamic scheduler depending on the system status using Dual Priority scheduling. The algorithm has been proposed to be implemented on a nMPRA processor, by multiplying hardware resources (PC, pipeline registers and file registers) and other facilities (events, mutexes, interrupts, IPC communication, timer's, the static scheduler and support for dynamic scheduler) provides a switching and response time for events within 1 to 3 machine cycles.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842431,,no,,,yes,,no,undetermined,,no
494,Using TTCN-3 to Test SPDY Protocol Interaction Property,"SPDY protocol is a new application-layer communication protocol which was proposed by Google in order to overcome the defects of HTTP. On the basis of HTTP protocol, SPDY offered four improvements to shorten the page loading time such as multiplexed requests, prioritized requests, server pushed streams and compressed headers. However, there is no much test work of the SPDY especially treating it as a black box, focusing on its interaction property, or testing it by using TTCN-3. In this paper, the interaction property of SPDY protocol is analyzed according to the SPDY protocol draft specification, and a novel test work designed in view of SPDY interaction property is implemented. During the test work, the interaction granularity of the SPDY peers is divided into three kinds of granularity from different levels, meanwhile the test cases were generated in accordance with the draft specification and encoded by utilizing TTCN-3 language for executing on the TT work bench Professional software. Above all we must ensure the SPDY server-side can support the SPDY protocol and the test host installs TT work bench Professional software to complete TTCN-3 testing. Finally, the interaction property of SPDY protocol was tested and the test results were reported. Moreover, some failed test cases were analyzed in detail.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903186,,no,,,no,no,no,undetermined,,no
495,8th International Workshop on Search-Based Software Testing (SBST 2015),"This paper is a report on the 8th International Workshop on Search-Based Software Testing at the 37th International Conference on Sofrware Engineering (ICSE). Search-Based Software Testing (SBST) is a form of Search-Based Software Engineering (SBSE) that optimizes testing through the use of computational search. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service-orientated architectures, construct test suites for interaction testing, and validate real time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.Three full research papers, three short papers, and threeposition papers will be presented in the two-day workshop. Additionally, six development groups have pitted their test generation tools against a common set of programs and benchmarks, and will present their techniques and results. This report will give the background of the workshop and detail the provisional program.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203148,,no,,,no,,no,undetermined,,no
498,A defect dependency based approach to improve software quality in integrated software products,"Integrated software products are complex in design. They are prone to defects caused by integrated and non-integrated modules of the entire integrated software suite. In such software products, a small proportion of defects are fixed as soon as they are reported. Rest of the defects are targeted for fixes in future product release cycles. Among such targeted defects, most of them seem to be insignificant and innocuous in the current version but have the potential to become acute in future versions. In this paper, we propose an approach to study defect dependency of the reported defect using a dependency metric. Identifying the dependency of a defect in an integrated product suite can help the product stake-owners to prioritize them and help improve software quality.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320343,,no,,,yes,,no,undetermined,,no
503,A simple statically reconfigurable processor architecture,"A reconfigurable Processor architecture is presented that has been designed prioritizing modularity, scalability and simplicity. Modular design enables swapping of functional units within the main processing core while maintaining the same programming model. This ensures that the associated software tools chain such as Assembler and Compiler need not be redesigned. Scalable design enables reconfiguring the datapath width to suite application requirements without redesigning the processor architecture or making changes to the software program already written. Applications for such design range from academia where real world performance of many proposed Adder/Multiplier structures may be tested; to data centers where the nature of operation to be performed on massive chunks of data changes regularly requiring ASIC like performance.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226112,,no,,,no,,no,undetermined,,no
505,A Test Framework for Communications-Critical Large-Scale Systems,"Today's large-scale systems couldn't function without the reliable availability of a range of network communications capabilities. Software, hardware, and communications technologies have been advancing throughout the past two decades. However, the methods that industry commonly uses to test large-scale systems that incorporate critical communications interfaces haven't kept pace. The need exists for a specifically tailored framework to achieve effective, precise testing of communications-critical large-scale systems. A proposed test framework offers an alternative to the current generic approaches that lead to inefficient, costly testing in industry. A case study illustrates its benefits, which can also be realized with other comparable systems.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6785925,,no,,,no,,no,undetermined,,no
506,AEGIS autonomous targeting for the Curiosity rover's ChemCam instrument,"AEGIS (Autonomous Exploration for Gathering Increased Science) is a software suite that will imminently be operational aboard NASA's Curiosity Mars rover, allowing the rover to autonomously detect and prioritize targets in its surroundings, and acquire geochemical spectra using its ChemCam instrument. ChemCam, a Laser-Induced Breakdown Spectrometer (LIBS), is normally used to study targets selected by scientists using images taken by the rover on a previous sol and relayed by Mars orbiters to Earth. During certain mission phases, ground-based target selection entails significant delays and the use of limited communication bandwidth to send the images. AEGIS will allow the science team to define the properties of preferred targets, and obtain geochemical data more quickly, at lower data penalty, without the extra ground-inthe-loop step. The system uses advanced image analysis techniques to find targets in images taken by the rover's stereo navigation cameras (NavCam), and can rank, filter, and select targets based on properties selected by the science team. AEGIS can also be used to analyze images from ChemCam's Remote Micro Imager (RMI) context camera, allowing it to autonomously target very fine-scale features - such as veins in a rock outcrop - which are too small to detect with the range and resolution of NavCam. AEGIS allows science activities to be conducted in a greater range of mission conditions, and saves precious time and command cycles during the rover's surface mission. The system is currently undergoing initial tests and checkouts aboard the rover, and is expected to be operational by late 2015. Other current activities are focused on science team training and the development of target profiles for the environments in which AEGIS is expected to be used on Mars.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444544,,no,,,yes,,no,undetermined,,no
510,Approximating Attack Surfaces with Stack Traces,"Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202964,,no,,,yes,,no,undetermined,,no
511,Architecting to Ensure Requirement Relevance: Keynote TwinPeaks Workshop,"Research has shown that up to two thirds of features in software systems are hardly ever used or not even used at all. This represents a colossal waste of R&amp;D resources and occurs across the industry. On the other hand, product management and many others work hard at interacting with customers, building business cases and prioritizing requirements. A fundamentally different approach to deciding what to build is required: requirements should be treated as hypothesis throughout the development process and constant feedback from users and systems in the field should be collected to dynamically reprioritize and change requirements. This requires architectural support beyond the current state of practice as continuous deployment, split testing and data collection need to be an integral part of the architecture. In this paper, we present a brief overview of our research and industry collaboration to address this challenge.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184704,,no,,,yes,no,no,undetermined,,no
512,Challenges and Issues of Mining Crash Reports,"Automatic crash reporting tools built in many software systems allow software practitioners to understand the origin of field crashes and help them prioritise field crashes or bugs, locate erroneous files, and/or predict bugs and crash occurrences in subsequent versions of the software systems. In this paper, after illustrating the structure of crash reports in Mozilla, we discuss some techniques for mining information from crash reports, and highlight the challenges and issues of these techniques. Our aim is to raise the awareness of the research community about issues that may bias research results obtained from crash reports and provide some guidelines to address certain challenges related to mining crash reports.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070480,,no,,,no,no,no,undetermined,,no
514,Communication and collaboration of heterogeneous unmanned systems using the joint architecture for Unmanned Systems (JAUS) standards,"The Naval Undersea Warfare Center Division Newport (NUWCDIVNPT) and Georgia Tech Research Institute (GTRI) completed a successful at-sea exercise with autonomous UAS and UUV systems demonstrating cross-domain unmanned system communication and collaboration. The exercise was held at the NUWC Narragansett Bay Shallow Water Test Facility (NBSWTF) range, and it represented for the first time the use of standard protocols and formats that effectively support cross-domain unmanned system operations. Four man-portable Iver2 UUVs operating in coordinated missions autonomously collected environmental data, which was compressed in-stride, re-formatted, and exfiltrated via UAS relay for display and tactical decision making. Two UAS with autonomous flight take-off and mission execution were sequenced to serve as ISR platforms and to support communications as RF relays for the UUVs performing Intelligence Preparation of the Environment missions. Two Command and Control nodes ashore provided unmanned system tasking and re-tasking, and they served to host and display both geo-positional data and status for UAS and UUV vehicles during the operational scenarios run during the exercise. The SAE Joint Architecture for Unmanned Systems (JAUS) standards were used for all message traffic between shore-based C2 nodes, UAS, and UUVs active in the NBSWTF exercise area. Exercise goals focused on CNO priorities expressed in the Undersea Domain Operating Concept of AUG 2013 which emphasized protocols essential to effective command and control of networked unmanned systems with decentralization and flexibility of command structures. Development for this project highlighted both the strengths and shortfalls of JAUS and captured the requirements for moving forward in effective cross-domain communications that support distributed, agile C2 nodes to meet evolving CONOPS for growing unmanned system presence and mission roles. The scenario employed operating parameters for UAS and UUV that have been established in real-world operations and ongoing unmanned system programs. The tactical information from unmanned systems was displayed in real-time on shore-based C2 displays: the tactical FalconView display and the developmental TOPSIDE command and control station. This work represents a critical step in communications for networking of heterogeneous unmanned systems and establishes a solid platform for alignment of development and ongoing programs. The evaluation of JAUS suitability for near-term operational applications provides significant value as Concepts of Operation that rely on netted heterogeneous systems are being targeted. The focus on affordable commercial unmanned systems for this experimentation establishes the value of highly capable, portable systems to provide economical development and test opportunities with low-cost and low-risk alternatives to many planned and fielded systems. The JAUS architecture was introduced to the NUWC and GTRI unmanned systems though an instantiation of the Mission Oriented Operating Suite (MOOS) autonomy framework on secondary CPUs integrated into the Iver2 UUVs and the GTRI UAS. Since the GTRI UASs already had ROS installed, a MOOS-ROS bridge was employed to support use of the developed JAUS messaging capability. Established JAUS services were employed where the required functions could be met. New JAUS services were developed to meet functionality required for the operational scenarios in this exercise but not yet supported in the existing releases of SAE JAUS. Independent C++ header libraries that could be compiled at run time for specific autonomy frameworks, such as MOOS, were employed to support a software-agnostic approach. Immediate targets for broadening the influence of this work to coalition partners include the NATO Recognized Environmental Picture (REP) 2015 and The Technical Cooperation Program (TTCP) 2015 exercises. This project and demonstration was funded under a NUWC Strategic Initiative and GTRI program support.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271613,,yes,,,no,,no,undetermined,,no
515,Component based reliability assessment from UML models,"Model based development and testing techniques have ventured diverse research directions to assure quality of the software product. Models developed during architecture and design phases are efficient tools to assess quality at an early development stage. However, testing the extra-functional or non-functional properties of software systems is not frequently practised eg. reliability. The motivation to our work is to model the context of execution which is significant in system reliability analysis. In this paper we visualize the components of complex software systems and their interactions in the form of Functional Flow Diagram (FFD). This notation specifies the dynamic aspect of system behavior as the context of execution. To further asses reliability, the FFD is translated into Reliability Block Diagram (RBD). The relative importance of the components in terms of reliability is evaluated and is associated with prioritization of the component. The model is simple but significant for system maintenance, improvisation and modification. This model supports analysis and testing through better understanding of the interacting components and their reliabilities.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275704,,no,,,yes,no,no,undetermined,,no
517,Considerations on application of selective hardening based on software fault tolerance techniques,"This paper analyses the nature of fault tolerance software-based techniques and the influence of their overheads to determine an efficient strategy for applying those techniques in a selective way. Several considerations that have to be taken into account are presented in this work. These include an analysis of fault coverage and overheads when selective hardening is adopted; side effects of selective protection based on software; and the need of new criticality metrics, apart from those used for hardware-based techniques (e.g., AVF), to facilitate and prioritize the selection of resources to be protected.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102509,,no,,,yes,,no,undetermined,,no
518,Decision support system prototype on obstetrics ultrasonography for primary service physicians,"Introduction. The National Social Security System (SJSN) prioritizes primary service as a spearhead to assist Primary Care Physicians to make medical decisions. The purpose of this research is to develop computer software that will assist primary care physicians in the fields of Obstetrics Ultrasonography, related to referral decision-making abilities. Methods. A quasi-experimental post-test only design without a control group. The stages of the research process: Systems Analysis and Design, Prototyping and Testing by Lecture, Students, Programmers and Doctors. Results. From Analysis and Systems design document, has been produced prototype of software, and a test run has been proven successful Decision Support System software helping doctors develop diagnosis and specialty referrals. Conclusion: The Decision Support System software can be used in Obstetrics Ultrasonography by Primary Care Physicians to provide aid in their diagnosis and referrals. Before it is used, it is recommended for trainings and application tests.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401367,,no,,,no,no,no,undetermined,,no
519,Detecting Display Energy Hotspots in Android Apps,"Energy consumption of mobile apps has become an important consideration as the underlying devices are constrained by battery capacity. Display represents a significant portion of an app's energy consumption. However, developers lack techniques to identify the user interfaces in their apps for which energy needs to be improved. In this paper, we present a technique for detecting display energy hotspots - user interfaces of a mobile app whose energy consumption is greater than optimal. Our technique leverages display power modeling and automated display transformation techniques to detect these hotspots and prioritize them for developers. In an evaluation on a set of popular Android apps, our technique was very accurate in both predicting energy consumption and ranking the display energy hotspots. Our approach was also able to detect display energy hotspots in 398 Android market apps, showing its effectiveness and the pervasiveness of the problem. These results indicate that our approach represents a potentially useful technique for helping developers to detect energy related problems and reduce the energy consumption of their mobile apps.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102585,,no,,,yes,,no,undetermined,,no
520,Detecting hardware Trojans in unspecified functionality using mutation testing,"Existing functional Trojan detection methodologies assume Trojans violate the design specification under carefully crafted rare triggering conditions. We present a new type of Trojan that leaks secret information from the design by only modifying unspecified functionality, meaning the Trojan is no longer restricted to being active only under rare conditions. We provide a method based on mutation testing for detecting this new Trojan type along with mutant ranking heuristics to prioritize analysis of the most dangerous functionality. Applying our method to a UART controller design, we discover unspecified and untested bus functionality with the potential to leak 32 bits of information during hundreds of cycles without being detected! Our method also reveals poorly tested interrupt functionality with information leakage potential. After modifying the specification and test bench to remove the discovered vulnerabilities, we close the verification loop by re-analyzing the design using our methodology and observe the functionality is no longer flagged as dangerous.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372619,,no,,,yes,no,no,undetermined,,no
521,Effective test strategy for testing automotive software,"Electronic content is increasing in automobiles day by day. Functionalities like Air Bags, Anti-lock Braking, Driver Assistance Systems, Body Controllers, Passive entry Passive Start, Electronic Power Steering etc. are realized electronically with complex software. These functionalities are related to automobile system safety. Hence, safety is one of the key issues of future automobile development. Risk of system failure is high due to increasing technological complexity and software content. The software shall be tested well to arrest almost all the defects. This paper explains a test case development and execution strategy based on practical implementation. It explains how test case reduction using Taguchi method, prioritization of test execution and automation help to make testing effective. It also demonstrates how maximum defects are discovered in short time.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150821,,no,,,no,no,no,undetermined,,no
522,Effective verification of low-level software with nested interrupts,"Interrupt-driven software is difficult to test and debug, especially when interrupts can be nested and subject to priorities. Interrupts can arrive at arbitrary times, leading to an explosion in the number of cases to be considered. We present a new formal approach to verifying interrupt-driven software based on symbolic execution. The approach leverages recent advances in the encoding of the execution traces of interacting, concurrent threads. We assess the performance of our method on benchmarks drawn from embedded systems code and device drivers, and experimentally compare it to conventional formal approaches that use source-to-source transformations. Our experimental results show that our method significantly outperforms conventional techniques. To the best of our knowledge, our technique is the first to demonstrate effective formal verification of low-level embedded software with nested interrupts.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092387,,no,,,no,,no,undetermined,,no
523,Feasibility Analysis of Engine Control Tasks under EDF Scheduling,"Engine control applications include software tasks that are triggered at predetermined angular values of the crankshaft, thus generating a computational workload that varies with the engine speed. To avoid overloads at high rotation speeds, these tasks are implemented to self adapt and reduce their computational demand by switching mode at given rotation speeds. For this reason, they are referred to as adaptive variable rate (AVR) tasks. Although a few works have been proposed in the literature to model and analyze the schedulability of such a peculiar type of tasks, an exact analysis of engine control applications has been derived only for fixed priority systems, under a set of simplifying assumptions. The major problem of scheduling AVR tasks with fixed priorities, however, is that, due to engine accelerations, the interarrival period of an AVR task is subject to large variations, therefore there will be several speeds at which any fixed priority assignment is far from being optimal, significantly penalizing the schedulability of the system. This paper proposes for the first time an exact feasibility test under the Earliest Deadline First scheduling algorithm for tasks sets including regular periodic tasks and AVR tasks triggered by a common rotation source. In addition, a set of simulation results are reported to evaluate the schedulability gain achieved in this context by EDF over fixed priority scheduling.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176033,,no,,,yes,,no,undetermined,,no
524,Fusion of LIDAR and video cameras to augment medical training and assessment,"The Mobile Medical Lane Trainer (MMLT) is a multi-sensor rapidly deployed After-Action Review (AAR) system for Army medical lane training. Current AAR systems have two main drawbacks: 1) video does not provide a complete view of the medical and tactical situation, and 2) the video is not readily available for effective evaluation. The MMLT program is developing a __??smarter__?_ AAR system by using 3D LIDAR (LIght Detection And Ranging), a camera array, People Tracking software and Medical Training Evaluation and Review (MeTER) software. This system can be brought to the field and deployed in less than an hour to provide hands-off data collection for the exercise. MMLT supplements existing evaluation systems deployed at the Medical Simulation Training Centers (MSTCs) by providing a 3-D perspective of the training event for tactical evaluation with synchronized video technology to capture both tactical and clinical skills and instructor scoring. This capability is used in conjunction with the MeTER system's skill assessment checklists for automated performance review. An immediate synchronized playback capability has been developed, ultimately resulting in a rapid AAR for debriefing. This paper will discuss the technical components of the system, including hardware components, data fusion technique, tracking algorithms, and camera prioritization approaches, and will conclude with operational test results and lessons learned.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7295832,,no,,,yes,no,no,undetermined,,no
527,Improving Multi-Objective Test Case Selection by Injecting Diversity in Genetic Algorithms,"A way to reduce the cost of regression testing consists of selecting or prioritizing subsets of test cases from a test suite according to some criteria. Besides greedy algorithms, cost cognizant additional greedy algorithms, multi-objective optimization algorithms, and multi-objective genetic algorithms (MOGAs), have also been proposed to tackle this problem. However, previous studies have shown that there is no clear winner between greedy and MOGAs, and that their combination does not necessarily produce better results. In this paper we show that the optimality of MOGAs can be significantly improved by diversifying the solutions (sub-sets of the test suite) generated during the search process. Specifically, we introduce a new MOGA, coined as DIversity based Genetic Algorithm (DIV-GA), based on the mechanisms of orthogonal design and orthogonal evolution that increase diversity by injecting new orthogonal individuals during the search process. Results of an empirical study conducted on eleven programs show that DIV-GA outperforms both greedy algorithms and the traditional MOGAs from the optimality point of view. Moreover, the solutions (sub-sets of the test suite) provided by DIV-GA are able to detect more faults than the other algorithms, while keeping the same test execution cost.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936894,,yes,,,no,,no,undetermined,,no
529,Improving reliability using software operational profile and testing profile,"Software testing has ever remained a challenge particularly when testing is done with intention in enhancing the reliability. Conventional testing is increasing the testing in an unpredictable way by reducing the number of faults. There is a need to enhance the reliability by assigning probabilistic priorities to testing mechanism, which is done through software operational profile. This study adopts a case study to generate test cases and test suites with perspective of probabilistic reliability using the proposed framework based on software operational profile and testing profile.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219603,,no,,,no,no,no,undetermined,,no
530,Introducing Continuous Delivery of Mobile Apps in a Corporate Environment: A Case Study,"Software development is conducted in increasingly dynamic business environments. Organizations need the capability to develop, release and learn from software in rapid parallel cycles. The abilities to continuously deliver software, to involve users, and to collect and prioritize their feedback are necessary for software evolution. In 2014, we introduced Rugby, an agile process model with workflows for continuous delivery and feedback management, and evaluated it in university projects together with industrial clients. Based on Rugby's release management workflow we identified the specific needs for project-based organizations developing mobile applications. Varying characteristics and restrictions in projects teams in corporate environments impact both process and infrastructure. We found that applicability and acceptance of continuous delivery in industry depend on its adaptability. To address issues in industrial projects with respect to delivery process, infrastructure, neglected testing and continuity, we extended Rugby's workflow and made it tailor able. Eight projects at Capgemini, a global provider of consulting, technology and outsourcing services, applied a tailored version of the workflow. The evaluation of these projects shows anecdotal evidence that the application of the workflow significantly reduces the time required to build and deliver mobile applications in industrial projects, while at the same time increasing the number of builds and internal deliveries for feedback.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167166,,no,,,no,,no,undetermined,,no
531,Investigating the Correspondence between Mutations and Static Warnings,"This paper provides evidences on the correspondence between mutations and static warnings. We used mutation operators as a fault model to evaluate the direct correspondence between mutations and static warnings. The main advantage of using mutation operators is that they generate a large number of programs containing faults of different types, which can be used to decide the ones most probable to be detected by static analyzers. Since static analyzers, in general, report a substantial number of false positive warnings, the intention of this study is to define a prioritization approach of static warnings based on the probability they correspond to a true positive and lead to detect software faults. The results obtained for a set of open-source programs indicate that a correspondence exist when considering specific mutation operators such that static warnings may be prioritized based on their correspondence level with mutations.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328004,,no,,,no,,no,undetermined,,no
532,Kvazaar HEVC encoder for efficient intra coding,"This paper presents an open-source Kvazaar encoder for HEVC intra coding. This academic software encoder has been developed from the scratch using C as an implementation language by prioritizing modularity, portability, and readability of the source code. Kvazaar implements almost the same intra coding functionality as HEVC reference encoder (HM) but its rewritten source code makes it significantly faster. In all-intra (AI) coding, a single-threaded C implementation of Kvazaar is 2.3 times faster than HM at a cost of 1.7% bit rate increase. The respective values with a high speed preset of Kvazaar are 10.6 and 8.8%. Compared to a single-threaded C++ implementation of x265, Kvazaar improves rate-distortion performance and increases encoding speed in both high-quality and high-speed test cases. Kvazaar has a particular edge in the high-speed test case where it almost halves the BD-rate loss and more than doubles the performance.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168970,,no,,,no,,no,undetermined,,no
533,Lexical Parsing Expression Recognition Schemata,"Parsing expression grammars (PEGs) have emerged as a promising substitute for context-free grammars (CFGs) and regular expressions (REs) in programming language specification. The benefits of PEGs are twofold. First, parsing expression grammars replace unordered choice between alternatives by prioritized choice, which naturally solves the ubiquitous ""dangling else"" problem in grammar definitions. Second, PEGs employ ""character-level syntax"" specifications that eliminate the need to separate the lexical and hierarchical components of a language specification. However, there is ""no free lunch"" in PEGs. PEGs capture only syntactic relationships, but many language constructs cannot be parsed without additional semantic information. Moreover, character-level specifications can become unwieldy, as every aspect of the language, including spacing, has to be accounted for. To overcome these issues, we extend the original PEG formalism to incorporate semantic predicates that yield a programmatic means for state-based token recognition control. Furthermore, rather than requiring a single complete specification, we capture lexical components as PEG closures that provide a self-contained token recognition mechanism to reduce the clutter associated with purely character-level PEGs. To test the effectiveness of our approach, we use it for the construction of a Delphi language front-end and practically confirm that Ford's theoretical linear-time result also holds for PEG closures.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365805,,no,,,no,,no,undetermined,,no
534,METEOR: An Enterprise Health Informatics Environment to Support Evidence-Based Medicine,"Goal: The aim of this paper is to propose the design and implementation of next-generation enterprise analytics platform developed at the Houston Methodist Hospital (HMH) system to meet the market and regulatory needs of the healthcare industry. Methods: For this goal, we developed an integrated clinical informatics environment, i.e., Methodist environment for translational enhancement and outcomes research (METEOR). The framework of METEOR consists of two components: the enterprise data warehouse (EDW) and a software intelligence and analytics (SIA) layer for enabling a wide range of clinical decision support systems that can be used directly by outcomes researchers and clinical investigators to facilitate data access for the purposes of hypothesis testing, cohort identification, data mining, risk prediction, and clinical research training. Results: Data and usability analysis were performed on METEOR components as a preliminary evaluation, which successfully demonstrated that METEOR addresses significant niches in the clinical informatics area, and provides a powerful means for data integration and efficient access in supporting clinical and translational research. Conclusion: METEOR EDW and informatics applications improved outcomes, enabled coordinated care, and support health analytics and clinical research at HMH. Significance: The twin pressures of cost containment in the healthcare market and new federal regulations and policies have led to the prioritization of the meaningful use of electronic health records in the United States. EDW and SIA layers on top of EDW are becoming an essential strategic tool to healthcare institutions and integrated delivery networks in order to support evidence-based medicine at the enterprise level.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7137654,,no,,,no,,no,undetermined,,no
539,Practical Combinatorial Interaction Testing: Empirical Findings on Efficiency and Early Fault Detection,"Combinatorial interaction testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. Simulated Annealing (SA) and Greedy Algorithms have been widely used to find CIT test suites. From the literature, there is a widely-held belief that SA is slower, but produces more effective tests suites than Greedy and that SA cannot scale to higher strength coverage. We evaluated both algorithms on seven real-world subjects for the well-studied two-way up to the rarely-studied six-way interaction strengths. Our findings present evidence to challenge this current orthodoxy: real-world constraints allow SA to achieve higher strengths. Furthermore, there was no evidence that Greedy was less effective (in terms of time to fault revelation) compared to SA; the results for the greedy algorithm are actually slightly superior. However, the results are critically dependent on the approach adopted to constraint handling. Moreover, we have also evaluated a genetic algorithm for constrained CIT test suite generation. This is the first time strengths higher than 3 and constraint handling have been used to evaluate GA. Our results show that GA is competitive only for pairwise testing for subjects with a small number of constraints.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081752,,yes,,,no,,no,undetermined,,no
546,Pushing to the top,"IC3 is undoubtedly one of the most successful and important recent techniques for unbounded model checking. Understanding and improving IC3 has been a subject of a lot of recent research. In this regard, the most fundamental questions are how to choose Counterexamples to Induction (CTIs) and how to generalize them into (blocking) lemmas. Answers to both questions influence performance of the algorithm by directly affecting the quality of the lemmas learned. In this paper, we present a new IC3-based algorithm, called QUIP1, that is designed to more aggressively propagate (or push) learned lemmas to obtain a safe inductive invariant faster. QUIP modifies the recursive blocking procedure of IC3 to prioritize pushing already discovered lemmas over learning of new ones. However, a naive implementation of this strategy floods the algorithm with too many useless lemmas. In QUIP, we solve this by extending IC3 with may-proof-obligations (corresponding to the negations of learned lemmas), and by using an under-approximation of reachable states (i.e., states that witness why a may-proof-obligation is satisfiable) to prune non-inductive lemmas. We have implemented QUIP on top of an industrial-strength implementation of IC3. The experimental evaluation on HWMCC benchmarks shows that the QUIP is a significant improvement (at least 2x in runtime and more properties solved) over IC3. Furthermore, the new reasoning capabilities of QUIP naturally lead to additional optimizations and new techniques that can lead to further improvements in the future.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542254,,no,,,no,no,no,undetermined,,no
547,Quantifying security risk by measuring network risk conditions,"Software vulnerabilities are the weaknesses in the software that inadvertently allow dangerous operations. If the vulnerability is in a network service, it poses serious security threats because a cyber-attacker can exploit it to gain unauthorized access to the system. Hence, rapid discovery and remediation of network vulnerabilities is critical issues in network security. In today's dynamic IT environment, it is common practice that an organization prioritizes the mitigation of discovered vulnerabilities according to their risk levels. Currently available technologies, however, associate each vulnerability to the static risk level which does not take the unique characteristics of the target network into account. This often leads to inaccurate risk prioritization and less-than-optimal resource allocation. In this research, we introduce a novel way of quantifying the risk of network vulnerability by augmenting the static risk level with conditions specific to the target network. The method calculates the risk value of each vulnerability by measuring the proximity to the untrusted network and risk of the neighboring hosts. The resulting risk value, RCR is a composite index of the individual risk, network location and neighborhood risk conditions. Thus, it can be effectively used for prioritization, comparison and trending. We tested the methodology through the network intrusion simulation. The results shows average 88.9% the correlation between RCR and number of successful attacks on each vulnerability.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166562,,no,,,yes,,no,undetermined,,no
548,Replicating and Re-Evaluating the Theory of Relative Defect-Proneness,"A good understanding of the factors impacting defects in software systems is essential for software practitioners, because it helps them prioritize quality improvement efforts (e.g., testing and code reviews). Defect prediction models are typically built using classification or regression analysis on product and/or process metrics collected at a single point in time (e.g., a release date). However, current defect prediction models only predict if a defect will occur, but not when, which makes the prioritization of software quality improvements efforts difficult. To address this problem, Koru et al. applied survival analysis techniques to a large number of software systems to study how size (i.e., lines of code) influences the probability that a source code module (e.g., class or file) will experience a defect at any given time. Given that 1) the work of Koru et al. has been instrumental to our understanding of the size-defect relationship, 2) the use of survival analysis in the context of defect modelling has not been well studied and 3) replication studies are an important component of balanced scholarly debate, we present a replication study of the work by Koru et al. In particular, we present the details necessary to use survival analysis in the context of defect modelling (such details were missing from the original paper by Koru et al.). We also explore how differences between the traditional domains of survival analysis (i.e., medicine and epidemiology) and defect modelling impact our understanding of the size-defect relationship. Practitioners and researchers considering the use of survival analysis should be aware of the implications of our findings.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914599,,no,,,no,no,no,undetermined,,no
549,Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling,"Mixed-criticality real-time scheduling has been developed to improve resource utilization while guaranteeing safe execution of critical applications. These studies use optimistic resource reservation for all the applications to improve utilization, but prioritize critical applications when the reservations become insufficient at runtime. Many of them however share an impractical assumption that all the critical applications will simultaneously demand additional resources. As a consequence, they under-utilize resources by penalizing all the low-criticality applications. In this paper we overcome this shortcoming using a novel mechanism that comprises a parameter to model the expected number of critical applications simultaneously demanding more resources, and an execution strategy based on the parameter to improve resource utilization. Since most mixed criticality systems in practice are component-based, we design our mechanism such that the component boundaries provide the isolation necessary to support the execution of low-criticality applications, and at the same time protect the critical ones. We also develop schedulability tests for the proposed mechanism under both a flat as well as a hierarchical scheduling framework. Finally, through simulations, we compare the performance of the proposed approach with existing studies in terms of schedulability and the capability to support low-criticality applications.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176022,,no,,,yes,,no,undetermined,,no
559,Testing analytics on software variability,"Software testing is a tool-driven process. However, there are many situations in which different hardware/software components are tightly integrated. Thus system integration testing has to be manually executed to evaluate the system's compliance with its specified requirements and performance. There could be many combinations of changes as different versions of hardware and software components could be upgraded and/or substituted. Occasionally, some software components could even be replaced by clones. The whole system after each component change demands to be re-tested to ensure proper system behavior. For better utilization of resources, there is a need to prioritize the past test cases to test the newly integrated systems. We propose a way to facilitate the use of historical testing records of the previous systems so that a testcase portfolio can be developed, which intends to maximize testing resources for the same integrated product family. As the proposed framework does not consider much of internal software complexity, the implementation costs are relatively low.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070483,,no,,,no,,no,undetermined,,no
560,The Effect of GoF Design Patterns on Stability: A Case Study,"Stability refers to a software system's resistance to the __??ripple effect__?_, i.e., propagation of changes. In this paper, we investigate the stability of classes that participate in instances/occurrences of GoF design patterns. We examine whether the stability of such classes is affected by (a) the pattern type, (b) the role that the class plays in the pattern, (c) the number of pattern occurrences in which the class participates, and (d) the application domain. To this end, we conducted a case study on about 65.000 Java open-source classes, where we performed change impact analysis on classes that participate in zero, one (single pattern), or more than one (coupled) pattern occurrences. The results suggest that, the application of design patterns can provide the expected __??shielding__?_ of certain pattern-participating classes against changes, depending on their role in the pattern. Moreover, classes that participate in coupled pattern occurrences appear to be the least stable. The results can be used for assessing the benefits and liabilities of the use of patterns and for testing and refactoring prioritization, because less stable classes are expected to require more effort while testing, and urge for refactoring activities that would make them more resistant to change propagation.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7066925,,no,,,no,,no,undetermined,,no
561,The evaluation of the results of an eye tracking based usability tests of the so called Instructor's Portal framework (http://tanitlap.ektf.hu/csernaiz),"The research discussed in this paper can be positioned at the cross-section of Applied Computer Science, Didactics and Human-Computer Interaction. Accordingly, an Instructor's Portal (IPo) framework system was developed at the Department of Human Informatics of the Eszterh?__zy K?__roly College (EKC) in 2015. The aim of the framework system is to enable instructors working in higher education institutions to establish, customize, and update their own webpages independently without any help from informatics professionals. Said system not only fulfills a gap filling function in the higher education sphere, but performs complex tasks while serving a wide range of users. In order to establish a logically arranged content structure and user interface the respective development process observed several developmental principles, methods, and web-ergonomic rules. This paper introduces the results of usability tests and examinations pertaining to the system. The examination utilized, an eye-tracking hardware device and a mouse movement recording software along with a special software facilitating the evaluation and presentation of the respective results. This essay introduces and highlight how the respective apparatus complements the traditional human observation-based usability tests while identifying the cognitive skills to be ascertained with such devices, one of the main priorities of Cognitive Infocommunications (CogInfoCom).",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390637,,no,,,no,,no,undetermined,,no
563,Towards a framework for automatic correction of anti-patterns,"One of the biggest concerns in software maintenance is design quality; poor design hinders software maintenance and evolution. One way to improve design quality is to detect and correct anti-patterns (i.e., poor solutions to design and implementation problems), for example through refactorings. There are several approaches to detect anti-patterns, that rely on metrics and structural properties. However, finding a specific solution to remove anti-patterns is a challenging task as candidate refactorings can be conflicting and their number very large, making it costly. Hence, development teams often have to prioritize the refactorings to be applied on a system. In addition to this, refactoring is risky, since non-experienced developers can change the behaviour of a system, without a comprehensive test suite. Therefore, there is a need for tools that can automatically remove anti-patterns. We will apply meta-heuristics to propose a technique for automated refactoring that improves design quality.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081891,,no,,,no,,no,undetermined,,no
564,Towards testing variability intensive systems using user reviews,"Variability intensive systems are software systems that describe a large set of diverse and different configurations that share some characteristics. This high number of configurations makes testing such systems an expensive and error-prone task. For example, in the Android ecosystem we can find up to 24 different valid configurations, thus, making it impossible to test an application on all of them. To alleviate this problem, previous research suggest the selection of a subset of test cases that maximize the changes of finding errors while maximizing the diversity of configurations. Concretely, the proposals focus on the prioritization and selection of tests, so only relevant configurations are tested according to some criterion. In this paper, we envision the use of user reports to prioritize and select meaningful tests. To do this, we explore the use of recommender systems as a possible improvement to the selection of test cases in intensive variability systems.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333410,,yes,,,yes,,yes,undetermined,no,no
566,UPMOA: An improved search algorithm to support user-preference multi-objective optimization,"Multi-objective search algorithms (e.g., non-dominated sorting genetic algorithm II (NSGA-II)) have been applied extensively to solve various multi-objective optimization problems in software engineering such as problems in testing. However, existing multi-objective algorithms usually treat all the objectives with equivalent priorities and do not provide a mechanism to reflect various user preferences when guiding search. The need to have such a mechanism was observed in one of our industrial projects on applying search algorithms for test optimization of a product line of Videoconferencing Systems (VCSs) called Saturn, where user preferences must be incorporated into optimization objectives, based on domain knowledge of test engineers for VCS testing. To address this, we propose an extension to the most commonly-used multi-objective search algorithm NSGA-II, which has shown promising results with user preferences. We name the extension as User-Preference Multi-Objective Optimization Algorithm (UPMOA), which includes a user preference indicator p and is based on existing weight assignment strategies. We empirically evaluated UPMOA with two industrial problems focusing on optimizing the test execution system for Saturn in Cisco. To assess the performance and scalability of UPMOA, inspired by the two industrial problems, in total we created 64000 artificial problems with 128 different sets of user preferences. The evaluation includes two aspects: 1) Three weight assignment strategies together with UPMOA were empirically evaluated to identify a best weight assignment strategy for p. Results show that the Uniformly Distributed Weights (UDW) strategy can assist UPMOA in achieving the best performance; 2) UPMOA was compared with three representative multi-objective search algorithms (including NSGA-II) and results show that UPMOA significantly outperformed the others and has the ability to solve problems with a wide range of complexity.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381833,,no,,,no,yes,no,undetermined,,no
568,Using Defect Taxonomies for Testing Requirements,"Systematic defect management based on bug-tracking systems such as Bugzilla is well established and has been successfully used in many software organizations. Defect management weights the failures observed during test execution according to their severity and forms the basis for effective defect taxonomies. In practice, most defect taxonomies are used only for the a posteriori allocation of testing resources to prioritize failures for debugging. Thus, these taxonomies' full potential to control and improve all the steps of testing has remained unexploited. This is especially the case for testing a system's user requirements. System-level defect taxonomies can improve the design of requirements-based tests, the tracing of defects to requirements, the quality assessment of requirements, and the control of the relevant defect management. So, we developed requirements-based testing with defect taxonomies (RTDT). This approach is aligned with the standard test process and uses defect taxonomies to support all phases of testing requirements. To illustrate this approach and its benefits, we use an example project (which we call Project A) from a public health insurance institution.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799150,,no,,,no,,no,undetermined,,no
576,A study of critical success factors on software quality assurance of cloud networking devices,"In recent years, many more enterprises and organizations are embracing cloud computing for its commercial benefits and competitive advantages. However, these companies have prioritized hardware specifications when evaluating cloud networking devices, often neglecting the importance of software quality as a result. Most cloud networking devices currently offered on the market include embedded software systems, which mean that software and hardware compatibility can be one of the most essential characteristics to look for. In order to investigate the critical success factors (CSFs) for the software quality assurance (QA) of cloud networking devices, this study employed a Plan-Do-Check-Act (PDCA) research framework. Important Software QA factors from relevant IEEE standards and hardware factors related to software quality were then employed in the 4 facets of the PDCA process to conduct a 2-stage analysis. For the first stage of this study, 5 experts were interviewed and asked to complete a survey form and select CSFs that were then used to generate a questionnaire for the second stage where a total of 15 experts and QA engineers were invited to fill out questionnaires. Completed questionnaires were then subject to an analytic hierarchy process (AHP) to calculate the weight and priority orders for each of the CSFs. The overall results of Stage 2 indicate that Cloud devices security testing, Security of cloud networking devices, and Resources and their allocation are the top three orders of all CSFs when experts take a real concern in software QA of cloud networking devices.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811054,,no,,,yes,,no,undetermined,,no
577,A tool for constrained pairwise test case generation using statistical user profile based prioritization,"Pairwise testing is a wildly used approach in order to reduce the size of test suite and take steps to combinatorial testing problems because of an extensively large number of possible combinations between input parameters and values. In some cases, there will be invalid combinations between input parameters and values if constraints have not been handled. In this paper, we present a pairwise test generation tool called CPTG, a tool to generate test cases for pairwise testing by applying user profile for guiding and prioritizing in order to select optimal input parameters and values which do not depend on individual tester skills and also providing constraint handling solution between input parameters and values. We performed experiments and comparison with other tools. The experimental results of our tool demonstrated that our tool becomes particularly valuable in guiding testing with a maximized reliability by testing the most frequently used of the system and can generate comparable results of the size of the test case set.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748881,,no,,,yes,no,no,undetermined,,no
580,An exploration of various quality of service mechanisms in an OpenFlow and software defined networking environment,"Technology prevalent in networks today logically detaches the control from the data plane yet physically deploys them on the same device on all network devices. This has negatively resulted in expensive management of disparate and closed network devices and the inability to implement coherent, synchronized, and wide-ranged network policies. Software Defined Networking (SDN) solves these issues by providing an open and virtually centralized network system through cost-effective and gradual network introduction that minimizes investment in infrastructure. The OpenFlow protocol defines communications from the control plane where the logical center of the network, the controller, resides to the packet-routing devices in the data plane and vice-versa. The Mininet SDN emulator mimics real-world SDN network environments for rapid system testing and proof-of-concept experimentation. Quality Of Service (QoS) represents a ripe and proven technology that allows for the enforcement of priorities and assured performance levels on network links and objects. This study examined the application of QoS-based methods to an SDN OpenFlow environment. QoS parameters were utilized to implement rudimentary Class-Based Queuing (CBQ) scheduling algorithms on a custom OpenFlow controller within a Mininet-emulated SDN network topology. Two CBQ algorithms were used in the experiments contrasted by the classes on which they were based. One algorithm is Basic CBQ which modeled scheduling based on the __??Streaming__?_, __??Bursty__?_, and __??Catch-All__?_ traffic types. The other algorithm, Source CBQ, was based on predefined Source-IP address groupings of client hosts. Each Class Profile was broken down in terms of QoS enforcement points at the Leaf Switches and at the Core Switch. Tests were conducted on the topology with these CBQ-based Class Profiles to gather performance data and observe any issues encountered. Experiment results showed that CBQs at the Leaves yielded better overall average bandwidth than CBQs at the Core with Basic CBQ at the Leaves at 2% higher than Basic CBQ at the Core and Source CBQ at the Leaves at 30% higher than Source CBQ at the Core. This indicated that QoS implemented at the Client Leaf Switches rather than at the Core Switch yielded better performance as they were not prone to the same bottlenecks as the Core Switch. Additionally, experiment results seemingly showed that Basic CBQ, whose classes are based on traffic types, yielded optimum performance at less variability as it managed bandwidth better by segmenting traffic and limiting errors.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811055,,no,,,no,,no,undetermined,,no
583,Automated comparison of X-ray images for cargo scanning,"Customs administrations are responsible for the enforcement of fiscal integrity and security of movements of goods across land and sea borders. In order to verify whether the transported goods match the transport declaration, X-ray imaging of containers is used at many customs site worldwide. The main objective of the research and development project __??Automated Comparison of X-ray Images for Cargo Scanning (ACXIS)__?_, which is funded by the European 7<sup>th</sup>Framework Program, is to improve the efficiency and effectiveness of the inspection procedures of cargo at customs using X-ray technology. The current inspection procedures are reviewed to identify risks, catalogue illegal cargo, and prioritize detection scenarios. Based on these results, we propose an integrated solution that provides automation, information exchange between customs administrations, and computer-based training modules for customs officers. Automated target recognition (ATR) functions analyze the X-ray image after a scan is made to detect certain types of goods such as cigarettes, weapons and drugs in the freight or container. Other helpful information can also be provided, such as the load homogeneity, total or partial weight, or the number of similar items. The ATR functions are provided as an option to the user. The X-ray image is transformed into a manufacturer-independent format through geometrical and spectral corrections and stored into a database along with the user feedback and other related data. This information can be exchanged with similar systems at other sites, thus facilitating information exchange between customs administrations. The database is seeded with over 30'000 examples of legitimate and illegal goods. These examples are used by the ATR functions through machine learning techniques, which are further strengthened by the information exchange. In order to improve X-ray image interpretation competency of human operators (customs officers), a computer-based training software is developed that simulates these new inspection procedures. A study is carried out to validate the effectiveness and efficiency of the computer-based training as well as the implemented procedures. Officers from the Dutch and Swiss Customs administrations partake in the study, covering both land and sea borders.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815714,,no,,,yes,,no,undetermined,,no
585,Automatic Reproducible Crash Detection,"Crash reproduction, which spends much time of developers in reading and understanding source code, is a crucial yet time-consuming task in program debugging. To reduce the time and resource cost, automatic techniques of test generation have been proposed. These techniques aim to automatically generate test cases to reproduce the scenario of a crashed project. Unfortunately, due to the lack of a detailed comprehension of the source code, a generated test case may fail in reproducing an expected crash. In this paper, we propose an automatic approach to reproducible bug detection. This approach predicts whether a crash is difficult to reproduce or not via training a classifier based on historical reproducible crash data. If a crash is difficult to reproduce, it is better to assign the crash to a developer, instead of using an automatic technique of test generation. Our work can help to prioritize crashes and to save the cost of developers. Preliminary experiments show that our approach effectively detects reproducible crashes via evaluating 45 crashes.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780193,,no,,,no,,no,undetermined,,no
586,Automatically Learning Semantic Features for Defect Prediction,"Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886912,,no,,,no,,no,undetermined,,no
590,Conc-iSE: Incremental symbolic execution of concurrent software,"Software updates often introduce new bugs to existing code bases. Prior regression testing tools focus mainly on test case selection and prioritization whereas symbolic execution tools only handle code changes in sequential software. In this paper, we propose the first incremental symbolic execution method for concurrent software to generate new tests by exploring only the executions affected by code changes between two program versions. Specifically, we develop an inter-thread and inter-procedural change-impact analysis to check if a statement is affected by the changes and then leverage the information to choose executions that need to be re-explored. We also check if execution summaries computed in the previous program can be used to avoid redundant explorations in the new program. We have implemented our method in an incremental symbolic execution tool called Conc-iSE and evaluated it on a large set of multithreaded C programs. Our experiments show that the new method can significantly reduce the overall symbolic execution time when compared with state-of-the-art symbolic execution tools such as KLEE.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582788,,no,,,no,,no,undetermined,,no
591,Concolic test generation for PLC programs using coverage metrics,"This paper presents a technique for fully automated generation of test cases for PLC programs adhering to the IEC 61131-3 standard. While previous methods strive for completeness and therefore struggle with the state explosion we pursue a symbolic execution based approach, dropping completeness but nevertheless achieving similar or even better results in practice. The core component is a symbolic execution engine which chooses the next state to execute, handles constraints emerging during the execution and derives respective test vectors leading to a state. To make for a high coverage of the generated tests, we adopt techniques from concolic testing, allow for use of heuristics to prioritise promising states but also merge states to alleviate the path explosion. We exploit peculiarities of PLC semantics to determine reasonable merge-points and unlike similar approaches even handle unreachable code. To examine the feasibility of our technique we evaluate it on function blocks used in industry.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497884,,no,,,yes,,no,undetermined,,no
593,Data flow based quality testing approach using ACO for component based software development,"Component-based Software Engineering (CBSE) has been centered around advancements identified with configuration and implementation of software components and frameworks assembled from software components. Quality Assurance (QA) for CBSE is a new subject in the software development research area. In this paper an enhanced data flow based QA model is presented for CBSE by employing the Ant Colony Optimization (ACO)algorithm to optimize the given code for automatic generation and prioritization of optimal path in decision to decision Control Flow Graph (CFG), which results an enhanced testing phase for QA model with reduced complexity. After that, proposed ACO based approach is also utilized for the generation of test data to satisfy the generated set of paths. This paper additionally exhibits the proposed approach applying it in a program module. Results show that a better testing is achieved by applying proposed ACO based scheme on component based software. Proposed approach ensures full software coverage with least redundancy.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813850,,no,,,yes,,no,undetermined,,no
594,Database design for error searching system based on keyword priority,"Because there are too many types of errors occurred in a multi-software integrated platform, such as Integrated Decision Information System (IDIS). It is urgent to design an error searching system to solve all different problems. However, those errors belong to different stages like setup, configuration, and operation, or those errors may occurred in different services, applications, or IP ports, or may be happened in different system software, different version of software, and those errors are also can be classified into different types. The new requirement on the design of database has been announced, that it has to locate the error, find out the reason of this error, as well as the corresponding solution. Also, it requires to provide the location, phase, etc. of an error. For a multi-software integrated platform, this paper proposed a database design for error searching system based on keyword priority. This DB made the correspondence among error, the reason of the error, and corresponding solution, and put them to different categories in terms of their characteristics, such that it is easy to manage, search, and use. This method treats those characteristics as keywords with higher priority, which are correspondent with errors, reasons, and solutions, respectively. While the keywords extracted from errors, reasons, and solutions are treated as keywords with lower priority. Keywords with different priorities can all be used as index to search errors, reasons, and solutions, while keywords with higher priority can be used to filter the searching results finer, and make the searching results more accurate. On one hand, the database design method based on keyword priority has simplified the logical structure of the database. On the other hand, users do not only search the reason and solution of an error, but also can find out the accurate information of the error, such as in which stage, layer, software, or categories, etc. the error has happened. The data of the database is complete, which has been provided by 500 technicians who had found thousands of errors. Those errors haven been added to the DB after tests to make the DB more complete.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811011,,no,,,yes,no,no,undetermined,,no
595,Diversity-Aware Mutation Adequacy Criterion for Improving Fault Detection Capability,"Many existing testing techniques adopt diversity as an important criterion for the selection and prioritization of tests. However, mutation adequacy has been content with simply maximizing the number of mutants that have been killed. We propose a novel mutation adequacy criterion that considers the diversity in the relationship between tests and mutants, as well as whether mutants are killed. Intuitively, the proposed criterion is based on the notion that mutants can be distinguished by the sets of tests that kill them. A test suite is deemed adequate by our criterion if the test suite distinguishes all mutants in terms of their kill patterns. Our hypothesis is that, simply by using a stronger adequacy criterion, it is possible to improve fault detection capabilities of mutation-adequate test suites. The empirical evaluation selects tests for real world applications using the proposed mutation adequacy criterion to test our hypothesis. The results show that, for real world faults, test suites adequate to our criterion can increase the fault detection success rate by up to 76.8 percentage points compared to test suites adequate to the traditional criterion.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528954,,yes,,,no,no,no,undetermined,,no
596,Do We Have a Chance to Fix Bugs When Refactoring Code Smells?,"Code smells are used to describe code structures that may cause detrimental effects on software and should be refactored. Previous studies show that some code smells have significant effect on faults. However, how to refactor code smells to reduce bugs still needs more concern. We investigate the possibility of prioritizing code smell refactoring with the help of fault prediction results. We also investigate the possibility of improving the performance of fault prediction by using code smell detection results. We use Cohen's Kappa statistic to report agreements between results of code smell detections and fault predictions. We use fault prediction result as an indicator to guide code smell refactoring. Our results show that refactoring Blob, Long Parameter List, and Refused Parent Be Request may have a good chance to detect and fix bugs, and some code smells are particularly useful for improving the recall of fault prediction.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780189,,no,,,yes,no,no,undetermined,,no
600,Empirical Evaluation of Cross-Release Effort-Aware Defect Prediction Models,"To prioritize quality assurance efforts, various fault prediction models have been proposed. However, the best performing fault prediction model is unknown due to three major drawbacks: (1) comparison of few fault prediction models considering small number of data sets, (2) use of evaluation measures that ignore testing efforts and (3) use of n-fold cross-validation instead of the more practical cross-release validation. To address these concerns, we conducted cross-release evaluation of 11 fault density prediction models using data sets collected from 2 releases of 25 open source software projects with an effort-aware performance measure known as Norm(P<sub>opt</sub>). Our result shows that, whilst M5 and K* had the best performances, they were greatly influenced by the percentage of faulty modules present and size of data set. Using Norm(P<sub>opt</sub>) produced an overall average performance of more than 50% across all the selected models clearly indicating the importance of considering testing efforts in building fault-prone prediction models.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589801,,no,,,yes,,no,undetermined,,no
604,Experience Report: Understanding Cross-Platform App Issues from User Reviews,"App developers publish apps on different platforms, such as Google Play, App Store, and Windows Store, to maximize the user volumes and potential revenues. Due to the different characteristics of the platforms and the different user preference (e.g., Android is more customized than iOS), app testing cases on these three platforms should also be designed differently. Comprehensive app testing can be time-consuming for developers. Therefore, understanding the differences of the app issues on these platforms can facilitate the testing process. In this paper, we propose a novel framework named CrossMiner to analyze the essential app issues and explore whether the app issues exhibit differently on the three platforms. Based on five million user reviews, the framework automatically captures the distributions of seven app issues, i.e., ""battery"", ""crash"", ""memory"", ""network"", ""privacy"", ""spam"", and ""UI"". We discover that the apps for different platforms indeed generate different issue distributions, which can be employed by app developers to schedule and design the testing cases. The verification based on the official user forums also demonstrates the effectiveness of our framework. Furthermore, we also identify that the issues related to ""crash"" and ""network"" are more concerned by users than the other issues on these three platforms. To assist developers in gaining a deep insight on the user issues, we also prioritize the user reviews corresponding to the issues. Overall, we aim at understanding the differences of issues on different platforms and facilitating the testing process for app developers.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774515,,,no,,yes,,no,undetermined,,no
605,Foreword of the Thematic Track Quality Aspects in Agile Methods,"There is no doubt that agile methods have become mainstream and with their increased use unanswered questions start to appear: How do we address cross-cutting concerns when software is developed vertically? Does value prioritization lead to increases in technical debt by promoting feature development over refactoring? Isn__??t the reticence to write initial specifications on the premise of change an invitation to unnecessary change? As agile development matures answers, albeit partial, responses start to appear. The recurring themes in this year presentations are not whether agile is good or bad, better or worse,",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814524,,,no,,no,no,no,undetermined,,no
607,Guiding Dynamic Symbolic Execution toward Unverified Program Executions,"Most techniques to detect program errors, such as testing, code reviews, and static program analysis, do not fully verify all possible executions of a program. They leave executions unverified when they do not check certain properties, fail to verify properties, or check properties under certain unsound assumptions such as the absence of arithmetic overflow. In this paper, we present a technique to complement partial verification results by automatic test case generation. In contrast to existing work, our technique supports the common case that the verification results are based on unsound assumptions. We annotate programs to reflect which executions have been verified, and under which assumptions. These annotations are then used to guide dynamic symbolic execution toward unverified program executions. Our main technical contribution is a code instrumentation that causes dynamic symbolic execution to abort tests that lead to verified executions, to prune parts of the search space, and to prioritize tests that cover more properties that are not fully verified. We have implemented our technique for the .NET static analyzer Clousot and the dynamic symbolic execution tool Pex. It produces smaller test suites (by up to 19.2%), covers more unverified executions (by up to 7.1%), and reduces testing time (by up to 52.4%) compared to combining Clousot and Pex without our technique.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886899,,,no,,yes,yes,no,undetermined,,no
614,Integrating risk assessment and threat modeling within SDLC process,"Risk assessment and threat modeling are conducted for different purpose. The integration of risk assessment and threat modeling process limit the risk of software-based system. Incorporating security in all phases of software development life cycle is a tedious task in many organizations. In design phase of SDLC, the 50 % software defects are identified and detected. Most of the security attacks are happen in application layer. This paper explains the combined use of risk assessment and threat model to understand the security risk of an application. We also discuss how the model may be identifying threats and how to frame threat prioritization for threat category. Finally, we recommend understanding of risk of detection and creating a fair environment to reduce the likelihood of committing criminal acts by attackers.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823275,,,no,,no,yes,no,undetermined,,no
615,Investigating the Effects of Balanced Training and Testing Datasets on Effort-Aware Fault Prediction Models,"To prioritize software quality assurance efforts, fault prediction models have been proposed to distinguish faulty modules from clean modules. The performances of such models are often biased due to the skewness or class imbalance of the datasets considered. To improve the prediction performance of these models, sampling techniques have been employed to rebalance the distribution of fault-prone and non-fault-prone modules. The effect of these techniques have been evaluated in terms of accuracy/geometric mean/F1-measure in previous studies; however, these measures do not consider the effort needed to fix faults. To empirically investigate the effect of sampling techniques on the performance of software fault prediction models in a more realistic setting, this study employs Norm(P<sub>opt</sub>), an effort-aware measure that considers the testing effort. We performed two sets of experiments aimed at (1) assessing the effects of sampling techniques on effort-aware models and finding the appropriate class distribution for training datasets (2) investigating the role of balanced training and testing datasets on performance of predictive models. Of the four sampling techniques applied, the over-sampling techniques outperformed the under-sampling techniques with Random Over-sampling performing best with respect to the Norm(P<sub>opt</sub>) evaluation measure. Also, performance of all the prediction models improved when sampling techniques were applied between the rates of (20-30)% on the training datasets implying that a strictly balanced dataset (50% faulty modules and 50% clean modules) does not result in the best performance for effort-aware models. Our results also indicate that performances of effort-aware models are significantly dependent on the proportions of the two types of the classes in the testing dataset. Models trained on moderately balanced datasets are more likely to withstand fluctuations in performance as the class distribution in the testing data varies.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552003,,,no,,no,,no,undetermined,,no
616,MACKE: Compositional analysis of low-level vulnerabilities with symbolic execution,"Concolic (concrete+symbolic) execution has recently gained popularity as an effective means to uncover non-trivial vulnerabilities in software, such as subtle buffer overflows. However, symbolic execution tools that are designed to optimize statement coverage often fail to cover potentially vulnerable code because of complex system interactions and scalability issues of constraint solvers. In this paper, we present a tool (MACKE) that is based on the modular interactions inferred by static code analysis, which is combined with symbolic execution and directed inter-procedural path exploration. This provides an advantage in terms of statement coverage and ability to uncover more vulnerabilities. Our tool includes a novel feature in the form of interactive vulnerability report generation that helps developers prioritize bug fixing based on severity scores. A demo of our tool is available at https://youtu.be/icC3jc3mHEU.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582815,,,no,,yes,,no,undetermined,,no
617,Minimum complexity APP prioritization by bandwidth apportioning in smart phones,"The volume of best effort traffic is exploded by rapid adoption of peer-to-peer and content applications. Smart phone consumers are spending more times on applications which include video and music streaming, playing games, video chatting, social media like uploading photos to Facebook, Twitter etc. Many such applications are always running in background and sometimes come in foreground based on user preferences. In this work we propose an approach to improve the user experience by giving more bandwidth to preferred applications. We describe a preliminary model explaining our technique in detail. Further, we validate our proposal using real time test setup with Wireshark traffic analyzer, and results are detailed with respect to (1) Percentage of network share (2) Jitter experience and (3) Time taken for the algorithm to adapt. Proposed algorithm has been tested in two different platforms such as Android and Tizen. Our preliminary observations show that our proposed algorithm allocates more bandwidth to high priority applications while maintaining the low priority APPs are intact with above minimum bandwidth. Our approach gives users better jitter free experience for video streaming (high priority) applications in both Android (KitKat) and Tizen (Z1) platforms.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7564972,,,no,,yes,no,no,undetermined,,no
618,Mixed-Criticality Systems as a Service for Non-critical Tasks,"Mixed-Criticality Systems are capable of accommodating tasks of varying criticality. In this paper, these are [life, mission, and non-critical]. Tasks usually have an overestimated execution time to allow for the Worst Case Execution Time (WCET). When these tasks finish execution prior to their allotted execution time due to pessimistic assumptions present in the static analysis of the system. The surplus time is used to accommodate tasks with tolerance for deadline-misses. Non-critical tasks are often treated in a __?_best-effort__?_ capacity where no quality of service is considered. When processor utilisation is not overconstrained, all deadlines will be met. However, in cases where not enough processing resources exist to meet all deadlines for non-critical tasks, the allotted time for the non-critical tasks must be rationed between non-critical tasks. This paper proposes a novel method of prioritising non-critical tasks. By treating task execution as a service, non-critical tasks with unbounded deadline miss tolerance are given a Grade of Service for their met deadlines. This Grade of Service is used for the dynamic scheduling of non-critical tasks. Four different scheduling algorithms were tested with the proposed Highest Penalty First algorithm for distributing the effort of task execution amongst non-critical tasks in a proportionate manner and showing superior fairness of task execution compared to all other tested algorithms.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515632,,,no,,no,no,no,undetermined,,no
622,Negative Effects of Bytecode Instrumentation on Java Source Code Coverage,"Code coverage measurement is an important element in white-box testing, both in industrial practice and academic research. Other related areas are highly dependent on code coverage as well, including test case generation, test prioritization, fault localization, and others. Inaccuracies of a code coverage tool sometimes do not matter that much but in certain situations they can lead to serious confusion. For Java, the prevalent approach to code coverage measurement is to use bytecode instrumentation due to its various benefits over source code instrumentation. However, if the results are to be mapped back to source code this may lead to inaccuracies due to the differences between the two program representations. In this paper, we systematically investigate the amount of differences in the results of these two Java code coverage approaches, enumerate the possible reasons and discuss the implications on various applications. For this purpose, we relied on two widely used tools to represent the two approaches and a set of benchmark programs from the open source domain.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476645,,,no,,no,no,no,undetermined,,no
624,Opportunistic Competition Overhead Reduction for Expediting Critical Section in NoC Based CMPs,"With the degree of parallelism increasing, performance of multi-threaded shared variable applications is not only limited by serialized critical section execution, but also by the serialized competition overhead for threads to get access to critical section. As the number of concurrent threads grows, such competition overhead may exceed the time spent in critical section itself, and become the dominating factor limiting the performance of parallel applications. In modern operating systems, queue spinlock, which comprises a low-overhead spinning phase and a high-overhead sleeping phase, is often used to lock critical sections. In the paper, we show that this advanced locking solution may create very high competition overhead for multithreaded applications executing in NoC-based CMPs. Then we propose a software-hardware cooperative mechanism that can opportunistically maximize the chance that a thread wins the critical section access in the low-overhead spinning phase, thereby reducing the competition overhead. At the OS primitives level, we monitor the remaining times of retry (RTR) in a thread's spinning phase, which reflects in how long the thread must enter into the high-overhead sleep mode. At the hardware level, we integrate the RTR information into the packets of locking requests, and let the NoC prioritize locking request packets according to the RTR information. The principle is that the smaller RTR a locking request packet carries, the higher priority it gets and thus quicker delivery. We evaluate our opportunistic competition overhead reduction technique with cycle-accurate full-system simulations in GEM5 using PARSEC (11 programs) and SPEC OMP2012 (14 programs) benchmarks. Compared to the original queue spinlock implementation, experimental results show that our method can effectively increase the opportunity of threads entering the critical section in low-overhead spinning phase, reducing the competition overhead averagely by 39.9% (maximally by 61.8%) and accelerating the execution of the Region-of-Interest averagely by 14.4% (maximally by 24.5%) across all 25 benchmark programs.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7551400,,,no,,no,yes,no,undetermined,,no
625,Optimal test sequence generation using River Formation Dynamics,"Software testing is a complex and exhaustive process, often limited by the resources. Although many approaches for test sequence generation exist in the literature, but none of it is ideal as far as coverage and redundancy is concerned. This paper aims at improving the efficiency of software testing process by generating the optimal test sequences in the control flow graph (CFG) of the program under test (PUT) by using a novel swarm intelligence method called River Formation Dynamics(RFD). RFD is inspired by a natural phenomenon of how drops transformed into river and river into sea. It provides full path coverage with zero edge/transition redundancy. It also tries to prioritize the paths based on their strength, calculated in terms of their traversal by the drops.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894483,,,no,,no,,no,undetermined,,no
626,PRADA: Prioritizing Android Devices for Apps by Mining Large-Scale Usage Data,"Selecting and prioritizing major device models are critical for mobile app developers to select testbeds and optimize resources such as marketing and quality-assurance resources. The heavily fragmented distribution of Android devices makes it challenging to select a few major device models out of thousands of models available on the market. Currently app developers usually rely on some reported or estimated general market share of device models. However, these estimates can be quite inaccurate, and more problematically, can be irrelevant to the particular app under consideration. To address this issue, we propose PRADA, the first approach to prioritizing Android device models for individual apps, based on mining large-scale usage data. PRADA adapts the concept of operational profiling (popularly used in software reliability engineering) for mobile apps - the usage of an app on a specific device model reflects the importance of that device model for the app. PRADA includes a collaborative filtering technique to predict the usage of an app on different device models, even if the app is entirely new (without its actual usage in the market yet), based on the usage data of a large collection of apps. We empirically demonstrate the effectiveness of PRADA over two popular app categories, i.e., Game and Media, covering over 3.86 million users and 14,000 device models collected through a leading Android management app in China.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886887,,,no,,no,,no,undetermined,,no
629,Product feature prioritization using the Hidden Structure method: A practical case at Ericsson,"In this paper, we present a case were we employ the Hidden Structure method to product feature prioritization at Ericsson. The method extends the more common Design Structure Matrix (DSM) approach that has been used in technology management (e.g. project management and systems engineering) for quite some time in order to model complex systems and processes. The hidden structure method focuses on analyzing a DSM based on coupling and modularity theory, and it has been used in a number of software architecture and software portfolio cases. In previous work by the authors the method was tested on organization transformation at Ericsson, however this is the first time it has been employed in the domain of product feature prioritization. Today, at Ericsson, features are prioritized based on a business case approach where each feature is handled isolated from other features and the main focus is customer or market-based requirements. By employing the hidden structure method we show that features are heavily dependent on each other in a complex network, thus they should not be treated as isolated islands. These dependencies need to be considered when prioritizing features in order to save time and money, as well as increase end customer satisfaction.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7806519,,,no,,yes,yes,no,undetermined,,no
630,Radius aware probabilistic testing of deadlocks with guarantees,"Concurrency bugs only occur under certain interleaving. Existing randomized techniques are usually ineffective. PCT innovatively generates scheduling, before executing a program, based on priorities and priority change points. Hence, it provides a probabilistic guarantee to trigger concurrency bugs. PCT randomly selects priority change points among all events, which might be effective for non-deadlock concurrency bugs. However, deadlocks usually involve two or more threads and locks, and require more ordering constraints to be triggered. We interestingly observe that, every two events of a deadlock usually occur within a short range. We generally formulate this range as the bug Radius, to denote the max distance of every two events of a concurrency bug. Based on the bug radius, we propose RPro (Radius aware Probabilistic testing) for triggering deadlocks. Unlike PCT, RPro selects priority change points within the radius of the targeted deadlocks but not among all events. Hence, it guarantees larger probabilities to trigger deadlocks. We have implemented RPro and PCT and evaluated them on a set of real-world benchmarks containing 10 unique deadlocks. The experimental results show that RPro triggered all deadlocks with higher probabilities (i.e., &gt;7.7x times larger on average) than that by PCT. We also evaluated RPro with radius varying from 1 to 150 (or 300). The result shows that the radius of a deadlock is much smaller (i.e., from 2 to 114 in our experiment) than the number of all events. This further confirms our observation and makes RPro meaningful in practice.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582772,,,no,,no,no,no,undetermined,,no
631,Signature limits: an entire map of clone features and their discovery in nearly linear time,"We address an increasingly critical problem of identifying the potential signatures for identifying a given family of malware or unwanted software (i.e., or generally any corpus of artifacts of unknown provenance). We address this with a novel methodology designed to create an entire and complete maps of software code clones (copy features in data). We report on a practical methodology, which employs enhanced suffix data structures and partial orderings of clones to compute a compact representation of most interesting clones features in data. The enumeration of clone features is useful for malware triage and prioritization when human exploration, testing and verification is the most costly factor. We further show that the enhanced arrays may be used for discovery of provenance relations in data and we introduce two distinct Jaccard similarity coefficients to measure code similarity in binary artifacts. We illustrate the use of these tools on real malware data including a retro-diction experiment for measuring and enumerating evidence supporting common provenance in Stuxnet and Duqu. The results indicate the practicality and efficacy of mapping completely the clone features in data.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888740,,,no,,no,,no,undetermined,,no
632,Solving exercise generation problems by diversity oriented meta-heuristics,"Evolutionary algorithms used for multi-objective optimization mostly prioritize fitness over diversity to achieve a single optimum fast, or a region in the Pareto-front. In this paper, we argue on that diversity should be a primary objective as well, and we propose a novel approach called EGAL to solve a well-known problem: to generate very different exercises to test students' knowledge in a specific range of topics. We show that focusing on diversity and fitness at the same time result in a better quality of solutions in the resulting population.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916196,,,no,,no,,no,undetermined,,no
635,Targeted Scrum: Applying Mission Command to Agile Software Development,"Software engineering and mission command are two separate but similar fields, as both are instances of complex problem solving in environments with ever changing requirements. Our research hypothesis is that modifications to agile software development based on inspirations from mission command can improve the software engineering process in terms of planning, prioritizing, and communication of software requirements and progress, as well as improving the overall software product. Targeted Scrum is a modification of Traditional Scrum based on three inspirations from Mission Command: End State, Line of Effort, and Targeting. These inspirations have led to the introduction of the Product Design Meeting and modifications of some current Scrum meetings and artifacts. We tested our research hypothesis using a semester-long undergraduate level software engineering class. Students developed two software projects, one using Traditional Scrum and the other using Targeted Scrum. We then assessed how well both methodologies assisted the software development teams in planning and developing the software architecture, prioritizing requirements, and communicating progress. We also evaluated the software product produced by both methodologies. We found that Targeted Scrum did better in assisting the software development teams in the planning and prioritization of the requirements. However, Targeted Scrum had a negligible effect on improving the software development teams external and internal communications. Finally, Targeted Scrum did not have an impact on the product quality by the top performing and worst performing teams. Targeted Scrum did assist the product quality of the teams in the middle of the performance spectrum.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7296686,,,no,,yes,,no,undetermined,,no
648,The Perception of Technical Debt in the Embedded Systems Domain: An Industrial Case Study,"Technical Debt Management (TDM) has drawn the attention of software industries during the last years, including embedded systems. However, we currently lack an overview of how practitioners from this application domain perceive technical debt. To this end, we conducted a multiple case study in the embedded systems industry, to investigate: (a) the expected life-time of components that have TD, (b) the most frequently occurring types of TD in them, and (c) the significance of TD against run-time quality attributes. The case study was performed on seven embedded systems industries (telecommunications, printing, smart manufacturing, sensors, etc.) from five countries (Greece, Netherlands, Sweden, Austria, and Finland). The results of the case study suggest that: (a) maintainability is more seriously considered when the expected lifetime of components is larger than ten years, (b) the most frequent types of debt are test, architectural, and code debt, and (c) in embedded systems the run-time qualities are prioritized compared to design-time qualities that are usually associated with TD. The obtained results can be useful for both researchers and practitioners: the former can focus their research on the most industrially-relevant aspects of TD, whereas the latter can be informed about the most common types of TD and how to focus their TDM processes.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776447,,,no,,no,,no,undetermined,,no
650,Tolerance to complexity: Measuring capacity of development teams to handle source code complexity,"A well defined testing strategy is essential for any software development project. Testing efforts need to be carefully planed and executed in order to ensure effectiveness. Programming failures can represent a high risk for business. In order to mitigate such risk, companies have been increasingly investing more resources on software testing. In despite of massive investments on software testing and extensive collection of static analysis techniques and tools, there are still few conclusive explanations for what causes human programming failures on software. The hypothesis investigated in this paper is that a metric based on development teams characteristics can be more effective to predict defective source code than metrics purely focused on information about source code, alone. Aiming to assist software engineers during testing initiatives, this article presents a new approach to systematically measure capacity of development teams to handle source code complexity. The proposed metric can be effective for raising information and comparing multiple development teams, planning training initiatives and prioritising testing efforts. Experiments were carried out with the entire source code base of device drivers for Linux Operating System. Our approach was able to predict, with 80% of accuracy rate, which development teams introduced more issues from 2010 to 2014.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844689,,,no,,yes,,no,undetermined,,no
651,UML-based reconfigurable middleware for design-level timing verification in model-based approach,"Model-based approaches for the development of software intensive real-time embedded systems allow early verification of timing properties at the design phase. In order to perform such verification, some aspects of the target software platform (i.e. the Real-Time Operating System (RTOS)) need to be considered such as priorities, scheduling policies, etc. However, one of the basic principles of model-based approaches, is to keep RTOS-independence of the design model. Hence, some assumptions on the software platform are implicitly made to achieve timing verification. This approach may lead to a mismatch between the design model and the RTOS-specific model describing the real-time application and thus, at the implementation level, timing properties may be affected. To tackle this issue, we define in this paper a reconfigurable middleware called RT-Mw. This middleware aims to explicitly describe the software assumptions at the design level for timing verification. Such approach allows early verification of these assumptions before the effective deployment which may prevents the mismatch between the design and the RTOS-Specific models. RT-Mw is described using UML modeling language together with the MARTE Standard.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843037,,,no,,no,no,no,undetermined,,no
652,User-Centric Network Provisioning in Software Defined Data Center Environment,"Present data center (DC) network provisioning schemes primarily utilize conventional load-balancing technologies, offering individual application performance improvement. Diversity in application usage however, makes isolated application prioritization a performance caveat for users with varying application trends. The present paper proposes a user profiling approach to capture application trends based on generic flow measurements (NetFlow) and employs the extracted profiles to create DC traffic forwarding policies. The scheme allows operators to define a global profile and application hierarchy based on extracted profiles to prioritize traffic for individual user classes. The proposed design was tested by extracting user profiles from a realistic enterprise network, and further simulated to dynamically manage DC traffic using the software defined networking paradigm. Compared to conventional traffic management schemes, the frame delivery ratio and effective throughput of our design was significantly higher for high priority north-south user traffic as well as the inter-server east-west application traffic.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796801,,,no,,no,,no,undetermined,,no
654,Using Software Metrics Thresholds to Predict Fault-Prone Classes in Object-Oriented Software,"Most code-based quality measurement approaches are based, at least partially, on values of multiple source code metrics. A class will often be classified as being of poor quality if the values of its metrics are above given thresholds, which are different from one metric to another. The metrics thresholds are calculated using various techniques. In this paper, we investigated two specific techniques: ROC curves and Alves rankings. These techniques are supposed to give metrics thresholds which are practical for code quality measurements or even for fault-proneness prediction. However, Alves Rankings technique has not been validated as being a good choice for fault-proneness prediction, and ROC curves only partially on few datasets. Fault-proneness prediction is an important field of software engineering, as it can be used by developers and testers as a test effort indication to prioritize tests. This will allow a better allocation of resources, reducing therefore testing time and costs, and an improvement of the effectiveness of testing by testing more intensively the components that are likely more fault-prone. In this paper, we wanted to compare empirically the selected threshold calculation methods used as part of fault-proneness prediction techniques. We also used a machine learning technique (Bayes Network) as a baseline for comparison. Thresholds have been calculated for different object-oriented metrics using four different datasets obtained from the PROMISE Repository and another one based on the Eclipse project.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916977,,,no,,no,,no,undetermined,,no
656,10th International Workshop on Search-Based Software Testing (SBST 2017),"Summary form only given, as follows. SBST 2017 Workshop Summary. Search-Based Software Testing (SBST) is the application of optimizing search techniques (for example, Genetic Algorithms) to solve problems in software testing. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service- orientated architectures, construct test suites for interaction testing, and validate real-time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967910,,,no,,no,no,no,undetermined,,no
658,A Framework for Combining and Ranking Static Analysis Tool Findings Based on Tool Performance Statistics,"This paper proposes a conceptual, performance-based ranking framework that prioritises the output of multiple Static Analysis Tools, to improve the tool effectiveness and usefulness. The framework weights the performance of Static Analysis Tools per defect type and cross-validates the findings between different Static Analysis Tools' reports. An initial validation shows the potential benefits of the proposed framework.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004389,,,no,,yes,,no,undetermined,,no
659,A General Framework for Dynamic Stub Injection,"Stub testing is a standard technique to simulate the behavior of dependencies of an application under test such as the file system. Even though existing frameworks automate the actual stub injection, testers typically have to implement manually where and when to inject stubs, in addition to the stub behavior. This paper presents a novel framework that reduces this effort. The framework provides a domain specific language to describe stub injection strategies and stub behaviors via declarative rules, as well as a tool that automatically injects stubs dynamically into binary code according to these rules. Both the domain specific language and the injection are language independent, which enables the reuse of stubs and injection strategies across applications. We implemented this framework for both unmanaged (assembly) and managed (.NET) code and used it to perform fault injection for twelve large applications, which revealed numerous crashes and bugs in error handling code. We also show how to prioritize the analysis of test failures based on a comparison of the effectiveness of stub injection rules across applications.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985696,,,no,,no,no,no,undetermined,,no
662,A QoS Guaranteed Technique for Cloud Applications Based on Software Defined Networking,"Due to the centralized control, network-wide monitoring and flow-level scheduling of software-defined-networking (SDN), it can be utilized to achieve quality of service (QoS) for cloud applications and services, such as voice over IP, video conference, and online games. However, most existing approaches stay at the QoS framework design and test level, while few works focus on studying the basic QoS techniques supported by SDN. In this paper, we enable SDN with QoS guaranteed abilities, which could provide end-to-end QoS routing for each cloud user service. First of all, we implement an application identification technique on SDN controller to determine required QoS levels for each application type. Then, we implement a queue scheduling technique on SDN switch. It queues the application flows into different queues and schedules the flows out of the queues with different priorities. At last, we evaluate the effectiveness of the proposed SDN-based QoS technique through both theoretical and experimental analysis. Theoretical analysis shows that our methods can provide differentiated services for the application flows mapped to different QoS levels. Experiment results show that when the output interface has sufficiently available bandwidth, the delay can be reduced by 28% on average. In addition, for the application flow with the highest priority, our methods can reduce 99.99% delay and increase 90.17% throughput on average when the output interface utilization approaches to the maximum bandwidth limitation.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048495,,,no,,no,,no,undetermined,,no
667,ALOJA: A Framework for Benchmarking and Predictive Analytics in Hadoop Deployments,"This article presents the ALOJA project and its analytics tools, which leverages machine learning to interpret big data benchmark performance data and tuning. ALOJA is part of a long-term collaboration between Barcelona Supercomputing Center and Microsoft to automate the characterization of cost-effectiveness on big data deployments, currently focusing on Hadoop. Hadoop presents a complex run-time environment, where costs and performance depend on a large number of configuration choices. The ALOJA project has created an open, vendor-neutral repository, featuring over 40000 Hadoop job executions and their performance details. The repository is accompanied by a test bed and tools to deploy and evaluate the cost-effectiveness of different hardware configurations, parameters, and cloud services. Despite early success within ALOJA, a comprehensive study requires automation of modeling procedures to allow an analysis of large and resource-constrained search spaces. The predictive analytics extension, ALOJA-ML, provides an automated system allowing knowledge discovery by modeling environments from observed executions. The resulting models can forecast execution behaviors, predicting execution times for new configurations and hardware choices. That also enables model-based anomaly detection or efficient benchmark guidance by prioritizing executions. In addition, the community can benefit from ALOJA data sets and framework to improve the design and deployment of big data applications.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312954,,,no,,no,yes,no,undetermined,,no
672,An Empirical Study on the Effect of Testing on Code Quality Using Topic Models: A Case Study on Software Development Systems,"Previous research in defect prediction has proposed approaches to determine which files require additional testing resources. However, practitioners typically create tests at a higher level of abstraction, which may span across many files. In this paper, we study software testing, especially test resource prioritization, from a different perspective. We use topic models to generate topics that provide a high-level view of a system, allowing developers to look at the test case coverage from a different angle. We propose measures of how well tested and defect prone a topic is, allowing us to discover which topics are well tested and which are defect prone. We conduct case studies on the histories of Mylyn, Eclipse, and NetBeans. We find that 34-78% of topics are shared between source code and test files, indicating that we can use topic models to study testing; well-tested topics are usually less defect prone, defect-prone topics are usually undertested; we can predict which topics are defect prone but not well tested with an average precision and recall of 75% and 77%, respectively; our approach complements traditional prediction-based approaches by saving testing and code inspection effort; and our approach is not particularly sensitive to the parameters that we use.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949108,,,yes,,yes,,yes,undetermined,no,no
673,An extended adaptive process model for agile software development methodology,"Agile methodologies focus on the agility for the development of software. Among various agile methods, eXtreme Programming (XP) is the most adopted agile method. XP has been used for the development of other agile methods, for example, __??adaptive software development process model__?_ (ASDPM), which is the modified approach of XP. ASDPM was proposed to support the following activities: __??(a) communication and planning, (b) analysis, (c) design and development, and (d) testing__?_. Based on our literature review of ASDPM, we identify that ASDPM does not support the following: (i) how to identify the different types of agile team members who will participate during the communication and planning phase?; and (ii) how to deliver the most important requirements of the software during analysis phase?. Therefore, in order to address this issue and to strengthen the analysis phase of agile process models, in this paper we propose an extended adaptive process model (APM) for agile software development methodology. This method includes the following steps: (1) identification of agile team members, (2) communication and planning, (3) analysis includes the computation of function point of each requirement; and the selection and prioritization of the requirements, (4) design and development, (5) testing. Finally, the utilization of the proposed method is demonstrated with the help of Institute Examination System, as a case study.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342770,,,no,,yes,,no,undetermined,,no
674,An Ilities-Driven Methodology for the Analysis of Gaps of Stakeholders Needs in Space Systems Conceptual Design,"The new generation of space-based services includes large-scale, integrated, and distributed informational systems for which traditional system engineering approaches show some limits in delivering the __??big picture.__?_ Missing the view of the full range of design options, or prematurely translating the perceived stakeholders needs into design requirements, is often a consequence of insufficient regard to the end-users priorities. Objective of the present research is to bring to light the gaps extant between what system architects prioritize, and the preferences of potential system users. To this purpose, the proposed method aims at incorporating lifecycle properties (-ilities) in the concept design phase, by submitting attributes of these properties for the evaluation of two stakeholders representative groups. The case study refers to the integration of environmental measurements, coming from a global-navigation-satellite-systems-based remote sensing satellite constellation, as complementary data to the traditional weather-forecasting service, resulting in a new system of systems. The method runs through an interview-based quality function deployment process and collaborative sessions of teams of stakeholders. The strength of the formulation relies on the ability to treat a quantitative measure of the gaps extant between system desired capabilities as perceived by architects, and real end-user needs. The method can be potentially tested in a concurrent design environment as a complementary tool for eliciting requirements and suggesting the areas where investments and resources should be preferably allocated. Results can be used by researchers as pieces of knowledge to be further investigated, and by practitioners in development projects, taking into account that they are preliminary findings.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498637,,,no,,no,no,no,undetermined,,no
677,Autonomous observation of multiple USVs from UAV while prioritizing camera tilt and yaw over UAV motion,"This paper proposes a scheme for observing cooperative Unmanned Surface Vehicles (USV), using a rotorcraft Unmanned Aerial Vehicle (UAV) with camera movements (tilt and yaw) prioritized over UAV movements. Most of the current researches consider a fixed-wing type UAV for surveillance of multiple moving targets (MMT), whose functionality is limited to just UAV movements. Experiments in simulation are conducted and verified that, prioritizing camera movements increased the number of times each USV is visited (on an average by 5.68 times more), decreased the percentage of the duration that the UAV is not observing any USV (on an average by 19.8%) and increased the efficiency by decreasing the distance traveled by the UAV (on an average by 747 pixels) for the six test cases. Autonomous repositioning of the UAV at regular intervals to observe USVs during a disaster scenario will provide the operator with better situational awareness. Using a rotorcraft over a fixed-wing type UAV provides the operator with a flexibility of observing the target for the required duration by hovering and freedom of unrestricted movements, which help improve the efficiency of target observation.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8088154,,,no,,no,,no,undetermined,,no
678,Big RF Data Assisted Cognitive Radio Network Coexistence in 3.5GHz Band,"In this paper, big Radio Frequency (RF) data assisted optimization is considered for future wireless networks employing cognitive radio technology with machine learning capability. A cognitive radio network (CRN) with multiple Secondary Users (SUs) may coexist with other wireless systems such as Small Cells (SC) and Radar systems, both Primary Users (PUs) with different level of priorities. Traditional spectrum sensing typically only gives information about the presence or absence of a PU. However, when multiple heterogeneous systems coexist, it becomes imperative to acquire the knowledge of the systems operating in a specific band at a particular time so as to choose an appropriate transmission strategy. In this work, we take advantage of the learning capability of a Neural Network Predictor (NNP) to obtain the statistics of the coexisted wireless systems from the RF traces collected in our Universal Software Radio Peripheral (USRP) based test bed. The NNP is able to learn the features of the RF traces and make accurate prediction of the signals prevalent in the wireless environment. Because of the augmented information learned from the RF traces, a novel optimization problem incorporating the outputs from the NNP is formulated to maximize the throughput of the CRN. The solution is derived using Karush- Kuhn-Tucker (KKT) and extensive simulations using the real RF traces are carried out. It is demonstrated that the NNP can detect the type and number of coexisted users reliably and the proposed scheme will improve the performance of the coexisted CRN.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038357,,,no,,no,,no,undetermined,,no
679,CBGA-ES: A Cluster-Based Genetic Algorithm with Elitist Selection for Supporting Multi-Objective Test Optimization,"Multi-objective search algorithms (e.g., non-dominated sorting genetic algorithm II (NSGA-II)) have been frequently applied to address various testing problems requiring multi-objective optimization such as test case selection. However, existing multi-objective search algorithms have certain randomness when selecting parent solutions for producing offspring solutions. In the worse case, suboptimal parent solutions may result in offspring solutions with bad quality, and thus affect the overall quality of the next generation. To address such a challenge, we propose a cluster-based genetic algorithm with elitist selection (CBGA-ES) with the aim to reduce such randomness for supporting multi-objective test optimization. We empirically compared CBGA-ES with random search, greedy (as baselines) and four commonly used multi-objective search algorithms (e.g., NSGA-II) using two industrial and one real world test optimization problem, i.e., test suite minimization, test case prioritization, and test case selection. The results showed that CBGA-ES significantly outperformed the baseline algorithms (e.g., greedy), and the four selected search algorithms for all the three test optimization problems. CBGA-ES managed to outperform more than 75% of the objectives for all the four algorithms in each test optimization problem. Moreover, CBGA-ES was able to improve the quality of the solutions for an average of 32.5% for each objective as compared to the four algorithms for the three test optimization problems.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927990,,,no,,no,,no,undetermined,,no
681,Cloud-based parallel concolic execution,"Path explosion is one of the biggest challenges hindering the wide application of concolic execution. Although several parallel approaches have been proposed to accelerate concolic execution, they neither scale well nor properly handle resource fluctuations and node failures, which often happen in practice. In this paper, we propose a novel approach, named PACCI, which parallelizes concolic execution and adapts to the drastic changes of computing resources by leveraging cloud infrastructures. PACCI tailors concolic execution to the MapReduce programming model and takes into account the features of cloud infrastructures. In particular, we tackle several challenging issues, such as making the exploration of different program paths independently and constructing an extensible path exploration module to support the prioritization of test inputs from a global perspective. Preliminary experimental results show that PACCI is scalable (e.g., gaining about 20?? speedup using 24 nodes) and its efficiency declines slightly about 5% and 6.1% under resource fluctuations and node failures, respectively.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884649,,,no,,no,,no,undetermined,,no
687,Delta-Oriented Product Prioritization for Similarity-Based Product-Line Testing,"Testing every product of a software product line (SPL) is often not feasible due to the exponential number of products in the number of features. Thus, the order in which products are tested matters, because it can increase the early rate of fault detection. Several approaches have been proposed to prioritize products based on configuration similarity. However, current approaches are oblivious to solution-space differences among products, because they consider only problem-space information. With delta modeling, we incorporate solution-space information in product prioritization to improve the effectiveness of SPL testing. Deltas capture the differences between products facilitating the reasoning about product similarity. As a result, we select the most dissimilar product to the previously tested ones, in terms of deltas, to be tested next. We evaluate the effectiveness of our approach using an SPL from the automotive domain showing an improvement in the effectiveness of SPL testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968061,,,no,,yes,no,no,undetermined,,no
688,Deriving high-priority acceptance test cases using utility trees: A case study,"Even though software testing is considered a mature field in software engineering, deriving test cases is still an important issue and even more when related to quality requirements. Utility Trees are used to evaluate software architectures, organizing requirements as scenarios associated to quality attributes and decorating them with stakeholder-given priority and developer- given difficulty. In this article, we propose an approach to use Utility Trees to derive prioritized acceptance test cases allowing to focus in high-value tests. The technique has been tried in two medium-sized projects for a Chilean public agency, with positive results. This innovative use of Utility Trees offers a simple, collaborative way to focus testing resources.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8405142,,,no,,yes,no,no,undetermined,,no
690,Digital learning as a tool to overcome school failure in minority groups,"In the European Union development strategy formulated in the Europe 2020 document (European Commission, 2015) it was indicated, that the smart growth of the EU as a whole should be reached through the realization of three priorities: the increase in employment, the increase of productiveness and the social cohesion and specialized agendas: Digital Agenda, Education and Learning, E-skills and Employment. The main documents identify main weaknesses and risk areas. One of the most significant was described as the early school leaving of Roma minority members. Roma constitute Europe's largest transnational ethnic minority with an estimate of ten million people. Learning outcomes of this minority are significantly lower than outcomes of the majority. As one of the reasons for early school leaving of Roma, insufficient understanding of learning materials is identified. The result is that most of the Roma community members drop out of education before attending a secondary school and continue their lives as unemployed or enter the labor market as unskilled workers. Within the paper will be presented the CloudLearning project that represents an alternative and innovative educational method: the way of the SOLE method implemented in their education. This paper will include partial results from the pilot tests realized these days.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973525,,,no,,no,,no,undetermined,,no
691,Educational prototype demonstrating frequency spectrum sharing through channel borrowing and priority assignment,"The purpose of this research is to build an educational prototype for attracting high school seniors and college students to pursue university degrees. The prototype entails demonstrable hardware and software comprising of a set of communication nodes with call priorities, which are used to help educate students on future and practical implications of spectrum sharing. Two objectives are achieved by building this hands-on prototype: (1) Students learn firsthand basics of communication systems and (2) Students are taught the concept and feasibility of __??priority__?_ in RF device communication. In crafting this easy-to-use prototype, integrated Arduinos, RF modules, and open-source libraries are utilized. Eight simplex devices (or nodes) that can communicate over just three channels in the 2.4GHz ISM band are built to articulate the spectrum scarcity challenge. When placing a call, a device requests a channel from the base station, which notifies the device about available channels, taking into account both device priorities (e.g. some devices have higher priorities) and channels that are in use by other calls. The base station has the authority to remove devices from a channel or transfer them to another channel when necessary. The paradigm of __??priority assignment__?_ is implemented keeping in mind futuristic trends in communication systems aimed at optimal use of the transmission spectrum. The prototype is validated via many test cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053422,,,no,,no,,no,undetermined,,no
693,Enabling Software Defined Networking with QoS Guarantee for Cloud Applications,"Due to the centralized control, network-wide monitoring and flow-level scheduling of Software-Defined-Networking (SDN), it can be utilized to achieve Quality of Service (QoS) for cloud applications and services, such as voice over IP, video conference and online games, etc. However, most existing approaches stay at the QoS framework design and test level, while few works focus on studying the basic QoS techniques supported by SDN. In this paper, we enable SDN with QoS guaranteed abilities, which could provide end-to-end QoS routing for each cloud user service. First of all, we implement an application identification technique on SDN controller to determine required QoS levels for each application type. Then, we implement a queue scheduling technique on SDN switch. It queues the application flows into different queues and schedules the flows out of the queues with different priorities. At last, we evaluate the effectiveness of the proposed SDN-based QoS technique through an experimental analysis. Results show that when the output interface has sufficiently available bandwidth, the delay can be reduced by 28% on average. In addition, for the application flow with the highest priority, our methods can reduce 99.99% delay and increase 90.17% throughput on average when the output interface utilization approaches to the maximum bandwidth limitation.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030581,,,no,,no,yes,no,undetermined,,no
695,Evaluation of AV systems against modern malware,"Countering the proliferation of malware has been for recent years one of the top priorities for governments, businesses, critical infrastructure, and end users. Despite the apparent evolvement of anti-virus (AV) systems, malicious authors have managed to create a sense of insecurity amongst computer users. Security controls do not appear to be sufficiently strong to stop malware proliferating. There seems to be a disconnect between public reports on AV tests and what people are experiencing on the daily basis. In this research, we are testing the efficiency of AV products and their ability to detect malicious files commonly known as malware. We manually generated payloads from five malware frameworks freely available to download and use. We use two modes of tests during our experiments. We manually installed a selection of AV systems in one first instance. We also use an online framework for testing malicious files. The findings in this study show that many antivirus systems were not able to achieve a higher score than 80% detection rate. Certain attack frameworks were much more successful in generating payloads that were not detectable by AV systems. We conclude that AV systems have their roles to play as they are the most common first line of defense, but more work is needed to successfully detect most malware the first day of their release.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356397,,,no,,no,yes,no,undetermined,,no
696,Exniffer: Learning to Prioritize Crashes by Assessing the Exploitability from Memory Dump,"An important component of software reliability is the assurance of certain security guarantees, such as absence of low-level bugs that may result in code exploitation, for example. A program crash is an early indicator of possible errors in the program like memory corruption, access violation or division by zero. In particular, a crash may indicate the presence of safety or security critical errors. A safety-error crash does not result in any exploitable condition, whereas a security-error crash allows an attacker to exploit a vulnerability. However, distinguishing one from the other is a non-trivial task. This exacerbates the problem in cases where we get hundreds of crashes and programmers have to make choices which crash to patch first! In this work, we present a technique to identify security critical crashes by applying machine learning on a set of features derived from core-dump files and runtime information obtained from hardware assisted monitoring such as the last branch record (LBR) register. We implement the proposed technique in a prototype called Exniffer. Our empirical results, obtained by experimenting Exniffer on several crashes on real-world applications show that proposed technique is able to classify a given crash as exploitable or not-exploitable with high accuracy.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305946,,,no,,yes,,no,undetermined,,no
697,Extravehicular activity operations concepts under communication latency and bandwidth constraints,"The Biologic Analog Science Associated with Lava Terrains (BASALT) project is a multi-year program dedicated to iteratively develop, implement, and evaluate concepts of operations (ConOps) and supporting capabilities intended to enable and enhance human scientific exploration of Mars. This paper describes the planning, execution, and initial results from the first field deployment, referred to as BASALT-1, which consisted of a series of ten simulated extravehicular activities on volcanic flows in Idaho's Craters of the Moon National Monument and Preserve. The ConOps and capabilities deployed and tested during BASALT-1 were based on previous NASA trade studies and analog testing. Our primary research question was whether those ConOps and capabilities work acceptably when performing real (non-simulated) biological and geological scientific exploration under four different Mars-to-Earth communication conditions: 5 and 15 min one-way light time communication latencies and low (0.512 Mb/s uplink, 1.54 Mb/s downlink) and high (5.0 Mb/s uplink, 10.0 Mb/s downlink) bandwidth conditions, which represent two alternative technical communication capabilities currently proposed for future human exploration missions. The synthesized results, based on objective and subjective measures, from BASALT-1 established preliminary findings that the baseline ConOp, software systems, and communication protocols were scientifically and operationally acceptable with minor improvements desired by the __??Mars__?_ extravehicular and intravehicular crewmembers. However, unacceptable components of the ConOps and required improvements were identified by the __??Earth__?_ Mission Support Center. These data provide a basis for guiding and prioritizing capability development for future BASALT deployments and, ultimately, future human exploration missions.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7943570,,,no,,no,no,no,undetermined,,no
698,Flow Reconnaissance via Timing Attacks on SDN Switches,"When encountering a packet for which it has no matching forwarding rule, a software-defined networking (SDN) switch requests an appropriate rule from its controller; this request delays the routing of the flow until the controller responds. We show that this delay gives rise to a timing side channel in which an attacker can test for the recent occurrence of a target flow by judiciously probing the switch with forged flows and using the delays they encounter to discern whether covering rules were previously installed in the switch. We develop a Markov model of an SDN switch to permit the attacker to select the best probe (or probes) to infer whether a target flow has recently occurred. Our model captures practical challenges related to rule evictions to make room for other rules; rule timeouts due to inactivity; the presence of multiple rules that apply to overlapping sets of flows; and rule priorities. We show that our model enables detection of target flows with considerable accuracy in many cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979967,,,no,,no,,no,undetermined,,no
699,Formal Methods for Validation and Test Point Prioritization in Railway Signaling Logic,"The EN50128 Railway Safety Standard recommends the use of formal methods for proving the correctness of the yard-specific logic, which was developed for electronic signaling and interlocking systems. We present a tool flow, which consists of three components. The core component uses a novel method for automatically generating the relevant safety properties for a yard from its control table. The second component proves the validity of the properties on the application logic by using a new theory of invariant checking. The third component leverages the suite of formal properties to prioritize site acceptance test points. Experimental results are presented on real application data for the yards in India that are demonstrating the performance of the proposed methods.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529152,,,no,,yes,,no,undetermined,,no
700,Game Theoretic Study on Channel-Based Authentication in MIMO Systems,"In this paper, we investigate the authentication based on radio channel information in multiple-input multiple-output (MIMO) systems and formulate the interactions between a receiver with multiple antennas and a spoofing node as a zero-sum physical (PHY)-layer authentication game. In this game, the receiver chooses the test threshold of the hypothesis test to maximize its Bayesian risk-based utility in the spoofing detection, while the adversary chooses its attack rate, i.e., how often a spoofing signal is sent. We derive the Nash equilibrium (NE) of the static PHY-layer authentication game and present the condition that the NE exists, showing that both the spoofing detection error rates and the spoofing rate decrease with the number of transmit and receive antennas. We propose a PHY-layer spoofing detection algorithm for MIMO systems based on Q-learning, in which the receiver applies the reinforcement learning technique to achieve the optimal test threshold via trials in a dynamic game without knowing the system parameters, such as the channel time variation and spoofing cost. We also use Dyna architecture and prioritized sweeping (Dyna-PS) to improve the spoofing detection in time-variant radio environments. The proposed authentication algorithms are implemented over universal software radio peripherals and evaluated via experiments in an indoor environment. Experimental results show that the Dyna-PS-based spoofing detection algorithm further reduces the spoofing detection error rates and increases the utility of the receiver compared with the Q-learning-based algorithm, and both performances improve with more number of transmit or receive antennas.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815442,,,no,,no,no,no,undetermined,,no
701,Gamifying Collaborative Prioritization: Does Pointsification Work?,"Gamification has been applied in software engineering contexts, and more recently in requirements engineering with the purpose of improving the motivation and engagement of people performing specific engineering tasks. But often an objective evaluation that the resulting gamified tasks successfully meet the intended goal is missing. On the other hand, current practices in designing gamified processes seem to rest on a try, test and learn approach, rather than on first principles design methods. Thus empirical evaluation should play an even more important role.We combined gamification and automated reasoning techniques to support collaborative requirements prioritization in software evolution. A first prototype has been evaluated in the context of three industrial use cases. To further investigate the impact of specific game elements, namely point-based elements, we performed a quasi-experiment comparing two versions of the tool, with and without pointsification. We present the results from these two empirical evaluations, and discuss lessons learned.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8049138,,,no,,no,,no,undetermined,,no
705,Knowledge Transfer for Global Roles in GSE,"This practice paper presents how a software engineering organization spread across three countries successfully transferred the knowledge of a few identified roles for a large mission-critical software system that had to conform to regulatory requirements. Multiple releases of the system have been delivered to customers over the 15 years it has been in the market. Each release of the product had a focus area. The competence availability for these focus areas was distributed. As a natural evolution of the globally distributed team, greater responsibility is devolved to a particular location, based on the availability of the competence at that location. Moving the increased responsibility to a location, created a global role, which did not exist earlier. Building the new role required a new skill, what is unique about a global role. Equipping the team members in the new skill was necessary to take up the roles effectively and quickly. The first step was the identification of the competence for a function/role, training for which may be imparted to another person, who will take over the function/role. This is followed by a process of knowledge transfer, which ensured that a person can take up a new global role from another location. Prioritization based on ease of knowledge transfer for different areas of work, that was found to be effective is described. This helped reduce possible problems that could occur due to incorrect or incomplete transfer of knowledge. The advantages by such knowledge transfer that resulted in new persons taking up global roles have outweighed its disadvantages. The practices described are generic and can be applied to any organization of similar size and complexity.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976692,,,no,,yes,yes,no,undetermined,,no
706,Learning to Predict Severity of Software Vulnerability Using Only Vulnerability Description,"Software vulnerabilities pose significant security risks to the host computing system. Faced with continuous disclosure of software vulnerabilities, system administrators must prioritize their efforts, triaging the most critical vulnerabilities to address first. Many vulnerability scoring systems have been proposed, but they all require expert knowledge to determine intricate vulnerability metrics. In this paper, we propose a deep learning approach to predict multi-class severity level of software vulnerability using only vulnerability description. Compared with intricate vulnerability metrics, vulnerability description is the ""surface level"" information about how a vulnerability works. To exploit vulnerability description for predicting vulnerability severity, discriminative features of vulnerability description have to be defined. This is a challenging task due to the diversity of software vulnerabilities and the richness of vulnerability descriptions. Instead of relying on manual feature engineering, our approach uses word embeddings and a one-layer shallow Convolutional Neural Network (CNN) to automatically capture discriminative word and sentence features of vulnerability descriptions for predicting vulnerability severity. We exploit large amounts of vulnerability data from the Common Vulnerabilities and Exposures (CVE) database to train and test our approach.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094415,,,no,,no,no,no,undetermined,,no
708,On a Pursuit for Perfecting an Undergraduate Requirements Engineering Course,"Requirements Engineering (RE) is an essential component of any software development cycle. Understanding and satisfying stakeholder needs and wants is the difference between the success and failure of a product. However, RE is often perceived as a ""soft"" skill by our students and is often ignored by students who prioritize the learning of coding, testing, and algorithmic thinking. This view contrasts with the industry, where ""soft"" skills are instead valued equal to any other engineering ability. A key challenge in teaching RE is that students who are accustomed to technical work have a hard time relating to something that is non-technical. Furthermore, students are rarely afforded the opportunity to practice requirements elicitation and management skills in a meaningful way while learning the RE concepts as an adjunct to other content. At Rose-Hulman, we have experimented with several project-based approaches to teaching RE, which have evolved over time. In this paper, we document the progress of our teaching methodologies, capture the pros and cons of these varied approaches, and reflect on what worked and what did not in teaching RE to undergraduate engineering students.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8166688,,,no,,no,yes,no,undetermined,,no
709,Performance analysis of OSPF and hybrid networks,"Software Defined Network (SDN) for large-scale IP provider network is an open issue and different solutions were proposed. However, the hybrid IP networks in which both distributed and centralized approach provide centralization of SDN and reliability of distributed networks. The common approach in which SDN controls the prioritized traffic and OSPF (Open Shortest Path First) guarantees the operation of traffic. In this research, we propose the SDN segregation, which maintain central management over dispersed routing control. A given topology is split in some fields with OpenFlow enabled switches as in between nodes. OSPF enabled router triggered updates to other routers in other field via SDN switches. The centralized controller defines how two OSPF routers observe each other. There will be a tradeoff between central control of SDN and fault tolerance capability of OSPF. As we increase SDN nodes control will increase and fault tolerance capacity of overall network decreases. The novelty of research work for balanced topology segregation also offers the models for network management. To show the enhancement provided by hybrid network over routing protocol deployment we have deployed separate test beds for routing protocol and proposed hybrid network.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8250022,,,no,,no,,no,undetermined,,no
710,Petri net based software testing scheduling and selecting,Computer software system has a profound impact on human society. It increasingly highlights the importance of software testing. Reducing the cost and improving the efficiency of software testing has an important practical significance and economic value. This paper investigates on software testing workflow from the perspective of discrete event dynamic systems and presents a method to improve the efficiency of software testing by optimizing task scheduling and execution priorities. We developed a simulation program of task scheduling based on Petri net to compare the performance of each scheduling option in different situations and made the analysis of their differences.,2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000086,,,no,,no,,no,undetermined,,no
711,Predicting Fault-Prone Classes in Object-Oriented Software: An Adaptation of an Unsupervised Hybrid SOM Algorithm,"Many fault-proneness prediction models have been proposed in literature to identify fault-prone code in software systems. Most of the approaches use fault data history and supervised learning algorithms to build these models. However, since fault data history is not always available, some approaches also suggest using semi-supervised or unsupervised fault-proneness prediction models. The HySOM model, proposed in literature, uses function-level source code metrics to predict fault-prone functions in software systems, without using any fault data. In this paper, we adapt the HySOM approach for object-oriented software systems to predict fault-prone code at class-level granularity using object-oriented source code metrics. This adaptation makes it easier to prioritize the efforts of the testing team as unit tests are often written for classes in object-oriented software systems, and not for methods. Our adaptation also generalizes one main element of the HySOM model, which is the calculation of the source code metrics threshold values. We conducted an empirical study using 12 public datasets. Results show that the adaptation of the HySOM model for class-level fault-proneness prediction improves the consistency and the performance of the model. We additionally compared the performance of the adapted model to supervised approaches based on the Naive Bayes Network, ANN and Random Forest algorithms.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009935,,,no,,no,,no,undetermined,,no
712,PV-OWL __?? Pharmacovigilance surveillance through semantic web-based platform for continuous and integrated monitoring of drug-related adverse effects in open data sources and social media,"The recent EU regulation on Pharmacovigilance [Regulation (EU) 1235/2010, Directive 2010/84/EU] imposes both to Pharmaceutical companies and Public health agencies to maintain updated safety information of drugs, monitoring all available data sources. Here, we present our project aiming to develop a web platform for continuous monitoring of adverse effects of medicines (pharmacovigilance), by integrating information from public databases, scientific literature and social media. The project will start by scanning all available data sources concerning drug adverse events, both open (e.g., FAERS - FDA Adverse Event Reporting Systems, medical literature, social media, etc.) and proprietary data (e.g., discharge hospital records, drug prescription archives, electronic health records), that require agreement with respective data owners. Subsequent, pharmacovigilance experts will perform a semi-automatic mapping of codes identifying drugs and adverse events, to build the thesaurus of the web based platform. After these preliminary activities, signal generation and prioritization will be the core of the project. This task will result in risk confidence scores for each included data source and a comprehensive global score, indicating the possible association between a specific drug and an adverse event. The software framework MOMIS, an open source data integration system, will allow semi-automatic virtual integration of heterogeneous and distributed data sources. A web platform, based on MOMIS, able to merge many heterogeneous data sets concerning adverse events will be developed. The platform will be tested by external specialized subjects (clinical researchers, public or private employees in pharmacovigilance field). The project will provide a) an innovative way to link, for the first time in Italy, different databases to obtain novel safety indicators; b) a web platform for a fast and easy integration of all available data, useful to verify and validate hypothesis generated in signal detection. Finally, the development of the unified safety indicator (global risk score) will result in a compelling, easy-to-understand, visual format for a broad range of professional and not professional users like patients, regulatory authorities, clinicians, lawyers, human scientists.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8065931,,,no,,no,,no,undetermined,,no
713,QoS-based routing over software defined networks,"Quality of Service (QoS) relies on the shaping of preferential delivery services for applications in favour of ensuring sufficient bandwidth, controlling latency and reducing packet loss. QoS can be achieved by prioritizing important broadband data traffic over the less important one. Thus, depending on the users' needs, video, voice or data traffic take different priority based on the prevalent importance within a particular context. This prioritization might require changes in the configuration of each network entity which can be difficult in traditional network architecture. To this extent, this paper investigates the use of a QoS-based routing scheme over a Software Defined Network (SDN). A real SDN test-bed is constructed using Raspberry Pi computers as virtual SDN switches managed by a centralized controller. It is shown that a QoS-based routing approach over SDN generates enormous control possibilities and enables automation.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986239,,,no,,no,,no,undetermined,,no
714,Ranking Modules for Integrate Testing Based on PageRank Algorithm,"The testing industry need to prioritize the limited resources and focus on testing modules whose failure is mostly likely to cause faults. This paper discusses a method that can rank modules in a software package for integrate testing using the PageRank algorithm. In this algorithm, a sequences of random walks iteratively can find a high likelihood of encountering a node, which is interpreted as it being an important performance resource. An experiment result prove that the proposed method actually can be used to prioritize testing of specific modules when testing resource are scarce.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8055343,,,no,,no,,no,undetermined,,no
715,Regression Testing Goals - View of Practitioners and Researchers,"Context: Regression testing is a well-researched area. However, the majority regression testing techniques proposed by the researchers are not getting the attention of the practitioners. Communication gaps between industry and academia, and disparity in the regression testing goals are the main reasons. Close collaboration can help in bridging the communication gaps and resolving the disparities. Objective: The study aims at exploring the views of academics and practitioners about the goals of regression testing. The purpose is to investigate the commonalities and differences in their viewpoints and defining some common goals for the success of regression testing. Method: We conducted a focus group study, with 7 testing experts from industry and academia. 4 testing practitioners from 2 companies and 3 researchers from 2 universities participated in the study. We followed GQM approach, to elicit the regression testing goals, information needs, and measures. Results: 43 regression testing goals were identified by the participants, which were reduced to 10 on the basis of similarity among the identified goals. Later during the priority assignment process, 5 goals were discarded, because the priority assigned to these goals was very low. Participants identified 47 information needs/questions required to evaluate the success of regression testing with reference to goal G5 (confidence). Which were then reduced to 10 on the basis of similarity. Finally, we identified measures to gauge those information needs/questions, which were corresponding to the goal (G5). Conclusions: We observed that participation level of practitioners and researchers during the elicitation of goals and questions was same. We found a certain level of agreement between the participants regarding the regression testing definitions and goals. But there was some level of disagreement regarding the priorities of the goals. We also identified the need to implement a regression testing evaluation framework in the participating companies.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8312521,,,no,,no,,no,undetermined,,no
718,Requirement paramerisation of flap actuation system: Product life-cycle management processes &amp; tools,"Flap actuation systems (FAS) have numerous operational states, modes and environmental constraints, which translate to thousands of requirements associated with them. FAS must be analyzed and tested to determine compliance with requirements in any operating conditions. In this paper, examples of parametrized requirements for centrally driven FAS are presented and the advantages brought by a requirement parametrization process in the systems engineering life- cycle discussed, from requirements elicitation to requirements validation and verification. The challenges of robust re-use and customization of components within legacy systems to drive cost reduction is significant as often legacy products were not developed within a model base framework. The requirements prioritization process provides a low cost, high impact example of alternative to ensure integrity and full traceability of requirements without the need to implement complex simulation platforms.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8088317,,,no,,no,,no,undetermined,,no
719,Risk-based attack surface approximation: how much data is enough?,"Proactive security reviews and test efforts are a necessary component of the software development lifecycle. Resource limitations often preclude reviewing the entire code base. Making informed decisions on what code to review can improve a team's ability to find and remove vulnerabilities. Risk-based attack surface approximation (RASA) is a technique that uses crash dump stack traces to predict what code may contain exploitable vulnerabilities. The goal of this research is to help software development teams prioritize security efforts by the efficient development of a risk-based attack surface approximation. We explore the use of RASA using Mozilla Firefox and Microsoft Windows stack traces from crash dumps. We create RASA at the file level for Firefox, in which the 15.8% of the files that were part of the approximation contained 73.6% of the vulnerabilities seen for the product. We also explore the effect of random sampling of crashes on the approximation, as it may be impractical for organizations to store and process every crash received. We find that 10-fold random sampling of crashes at a rate of 10% resulted in 3% less vulnerabilities identified than using the entire set of stack traces for Mozilla Firefox. Sampling crashes in Windows 8.1 at a rate of 40% resulted in insignificant differences in vulnerability and file coverage as compared to a rate of 100%.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965451,,,no,,no,,no,undetermined,,no
720,Studies on open source real time operating systems: For vehicle suspension control,"Studying the TIMELINESS of PERIODIC tasks of real time operating systems available in open source, to determine the reliability and efficiency of the systems for implementing them in suspension control of ground vehicles. We take several operating systems which are open source and are available to the public through GNU license (ex: Linux). The Kernels, which are the building blocks of the operating systems which connect hardware and software, are patched using selective real time patches which makes the operating system real time. This is done in order to unlock real time capabilities such as unbounded latencies and real time priorities. These operating systems are tested for their efficiency and timeliness. The results are compared with the test logs of non-real time operating system. These kernels are then cross compiled and built for ARM architecture. This kernel is then applied to an embedded system which is then tested using the same afore mentioned tests. The results are logged and analyzed.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070787,,,no,,no,,no,undetermined,,no
721,Table of contents,The following topics are dealt with: software design; mutation testing; collective online testing; collective offline testing; usability tests; Web services; delta-oriented programming; delta-oriented product prioritization and feature models.,2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968031,,,no,,no,,no,undetermined,,no
726,The organization of arrangements set to ensure enterprise IPV6 network secure work by modern switching equipment tools (using the example of a network attack on a default gateway),"The article issue is the enterprise information protection within the internet of things concept. The aim of research is to develop arrangements set to ensure secure enterprise IPv6 network operating. The object of research is the enterprise IPv6 network. The subject of research is modern switching equipment as a tool to ensure network protection. The research task is to prioritize functioning of switches in production and corporation enterprise networks, to develop a network host protection algorithm, to test the developed algorithm on the Cisco Packet Tracer 7 software emulator. The result of research is the proposed approach to IPv6-network security based on analysis of modern switches functionality, developed and tested enterprise network host protection algorithm under IPv6-protocol with an automated network SLAAC-configuration control, a set of arrangements for resisting default enterprise gateway attacks, using ACL, VLAN, SEND, RA Guard security technology, which allows creating sufficiently high level of networks security.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8239505,,,no,,no,,no,undetermined,,no
727,The Significant Effects of Data Sampling Approaches on Software Defect Prioritization and Classification,"Context: Recent studies have shown that performance of defect prediction models can be affected when data sampling approaches are applied to imbalanced training data for building defect prediction models. However, the magnitude (degree and power) of the effect of these sampling methods on the classification and prioritization performances of defect prediction models is still unknown. Goal: To investigate the statistical and practical significance of using resampled data for constructing defect prediction models. Method: We examine the practical effects of six data sampling methods on performances of five defect prediction models. The prediction performances of the models trained on default datasets (no sampling method) are compared with that of the models trained on resampled datasets (application of sampling methods). To decide whether the performance changes are significant or not, robust statistical tests are performed and effect sizes computed. Twenty releases of ten open source projects extracted from the PROMISE repository are considered and evaluated using the AUC, pd, pf and G-mean performance measures. Results: There are statistical significant differences and practical effects on the classification performance (pd, pf and G-mean) between models trained on resampled datasets and those trained on the default datasets. However, sampling methods have no statistical and practical effects on defect prioritization performance (AUC) with small or no effect values obtained from the models trained on the resampled datasets. Conclusions: Existing sampling methods can properly set the threshold between buggy and clean samples, while they cannot improve the prediction of defect-proneness itself. Sampling methods are highly recommended for defect classification purposes when all faulty modules are to be considered for testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170123,,,no,,yes,,no,undetermined,,no
728,Thermo-mechanical reliability analysis of flip-chip bonded silicon carbide Schottky diodes,"This paper presents the thermo-mechanical reliability analysis of a novel chip-scale wire bondless packaging technique for a SiC Schottky diode that leads to lower parasitics, higher reliability, lower costs, and lower losses. The proposed approach uses a flip-chip solder ball array to make connections to the anode. A copper connector was used to make contact with the bottom cathode, thus reconfiguring the bare die into a chip-scale, flip-chip capable device. Thermo-mechanical analysis in a finite element software showed that the proposed approach could better manage Coefficient of Thermal Expansion (CTE) mismatch stresses arising at the critical module interfaces as compared with a conventional wire bonded module. A detailed analysis of the flip-chip structure is presented and contrasted with a state-of-the-art wire bonded module. Different design parameters were explored for the drain connector to be able to make an optimized decision. However, keeping production costs low was prioritized without compromising significant performance. The fabrication process for manufacturing a flip-chip schottky diode module was also demonstrated along with preliminary test results to demonstrate functionality.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7936756,,,no,,no,no,no,undetermined,,no
731,Towards the design of a secure and compliant framework for OpenEMR,"The purpose of this research is to explore and identify the vulnerabilities in OpenEMR 5.0.0, which is a free and open source medical practice management application. We are to provide recommendations/suggestions to OpenEMR developers on identifying the vulnerabilities. We chose to use vulnerabilities scanning tools to manually explore the demo site of OpenEMR 5.0.0. The targeted vulnerabilities belong to the following three types, namely, SQL Injection, Cross-Site Scripting (XSS) including persistent XSS and reflected XSS and Arbitrary File Upload. We have inducted a qualitative based risk assessment to determine the risk levels for the vulnerabilities identified. The results of risk assessment include two kinds of risk levels, which are high risk and medium risk, and two kinds of priorities, which are priority 1 (high) and priority 2 (medium). In addition, we provided recommendations and best practices about how to prevent the identified vulnerabilities. Furthermore, the research also presents an exploit automation program written in Python to test and exploit the vulnerabilities including SQL Injection and reflected XSS on the demo server of OpenEMR.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8217792,,,no,,no,,no,undetermined,,no
732,Trends on empty exception handlers for Java open source libraries,"Exception-handling structures provide a means to recover from unexpected or undesired flows that occur during software execution, allowing the developer to put the program in a valid state. Still, the application of proper exception-handling strategies is at the bottom of priorities for a great number of developers. Studies have already discussed this subject pinpointing that, frequently, the implementation of exception-handling mechanisms is enforced by compilers. As a consequence, several anti-patterns about Exception-handling are already identified in literature. In this study, we have picked several releases from different Java programs and we investigated one of the most well-known anti-patterns: the empty catch handlers. We have analysed how the empty handlers evolved through several releases of a software product. We have observed some common approaches in terms of empty catches' evolution. For instance, often an empty catch is transformed into a empty catch with a comment. Moreover, for the majority of the programs, the percentage of empty handlers has decreased when comparing the first and last releases. Future work includes the automation of the analysis allowing the inclusion of data collected from other software artefacts: test suites and data from issue tracking systems.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884644,,,no,,no,,no,undetermined,,no
733,Value-Based Decision-Making Using a Web-Based Tool: A Multiple Case Study,"[Context]: To remain competitive, innovative and to grow, companies should use a value-based decision-making where decisions are the best for that company's overall value creation. However, without tool support, the use of explicit value propositions and aggregation of different key stakeholders' decisions during decision-making may be a challenge for many companies. [Goal]: The goal of this paper is to investigate the extent to which a Web-based tool for value-based decision-making can successfully support stakeholders' decision-making process. [Method]: We conducted three case studies across four software projects, during six weeks, in the contexts of feature selection, test cases execution prioritization and user interfaces design selection. Prior to using the tool, stakeholders' value propositions were elicited via focus-group meetings; later, during a post-mortem phase, data was gathered via observation, semi-structured interviews and structured questionnaires. [Results]: Participants reported an improvement of their decision-making process and quality of decisions; further, they also felt confident about using the tool, and that it can be useful to their work. [Conclusions]: Results suggested that the use of tool support by the stakeholders in the investigated company for value-based decision-making improved their decision-making process and the quality of decisions.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305950,,,yes,,no,,no,undetermined,,no
736,Aggregation process for implementation of application security management based on risk assessment,This article is devoted to the review and analysis of existing methods of ensuring information security based on risk models. The strengths and weaknesses of the model are investigated on the basis of reliability theory. The article discusses potential obstacle to managing application security effectively and describes five steps for managing security. Create inventory of application and their attributes and evaluating their role in business impact (Create a profile for each application and conduction analysis of date processed in the application). Software vulnerability search (Static Analysis (__??white-box__?_); Dynamic Analysis (__??black-box__?_); Interactive Analysis (__??glass-box__?_); Mobile Application Analysis); Risk assessment and prioritization of vulnerabilities (Setting priorities for applications; Setting priorities for types of vulnerabilities; Setting priorities for the development team; Changing vulnerability priorities and reassessing risks). Elimination of vulnerabilities and minimization of risks (security manager sets priorities and firmed tasks for the development team.,2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8317039,,,no,,yes,,no,undetermined,,no
738,Assessing Technical Debt in Automated Tests with CodeScene,"Test automation promises several advantages such as shorter lead times, higher code quality, and an executable documentation of the system's behavior. However, test automation won't deliver on those promises unless the quality of the automated test code itself is maintained, and to manually inspect the evolution of thousands of tests that change on a daily basis is impractical at best. This paper investigates how CodeScene - a tool for predictive analyses and visualizations - could be used to identify technical debt in automated test code. CodeScene combines repository mining, static code analysis, and machine learning to prioritize potential code improvements based on the most likely return on investment.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411742,,,no,,no,,no,undetermined,,no
740,BP: Profiling Vulnerabilities on the Attack Surface,"Security practitioners use the attack surface of software systems to prioritize areas of systems to test and analyze. To date, approaches for predicting which code artifacts are vulnerable have utilized a binary classification of code as vulnerable or not vulnerable. To better understand the strengths and weaknesses of vulnerability prediction approaches, vulnerability datasets with classification and severity data are needed. The goal of this paper is to help researchers and practitioners make security effort prioritization decisions by evaluating which classifications and severities of vulnerabilities are on an attack surface approximated using crash dump stack traces. In this work, we use crash dump stack traces to approximate the attack surface of Mozilla Firefox. We then generate a dataset of 271 vulnerable files in Firefox, classified using the Common Weakness Enumeration (CWE) system. We use these files as an oracle for the evaluation of the attack surface generated using crash data. In the Firefox vulnerability dataset, 14 different classifications of vulnerabilities appeared at least once. In our study, 85.3% of vulnerable files were on the attack surface generated using crash data. We found no difference between the severity of vulnerabilities found on the attack surface generated using crash data and vulnerabilities not occurring on the attack surface. Additionally, we discuss lessons learned during the development of this vulnerability dataset.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543394,,,no,,yes,,no,undetermined,,no
741,Characterizing Defective Configuration Scripts Used for Continuous Deployment,"In software engineering, validation and verification (V&V) resources are limited and characterization of defective software source files can help in efficiently allocating V&V resources. Similar to software source files, defects occur in the scripts used to automatically manage configurations and software deployment infrastructure, often known as infrastructure as code (IaC) scripts. Defects in IaC scripts can have dire consequences, for example, creating large-scale system outages. Identifying the characteristics of defective IaC scripts can help in mitigating these defects by allocating V&V efforts efficiently based upon these characteristics. The objective of this paper is to help software practitioners to prioritize validation and verification efforts for infrastructure as code (IaC) scripts by identifying the characteristics of defective IaC scripts. Researchers have previously extracted text features to characterize defective software source files written in general purpose programming languages. We investigate if text features can be used to identify properties that characterize defective IaC scripts. We use two text mining techniques to extract text features from IaC scripts: the bag-of-words technique, and the term frequency-inverse document frequency (TF-IDF) technique. Using the extracted features and applying grounded theory, we characterize defective IaC scripts. We also use the text features to build defect prediction models with tuned statistical learners. We mine open source repositories from Mozilla, Openstack, and Wikimedia Commons, to construct three case studies and evaluate our methodology. We identify three properties that characterize defective IaC scripts: filesystem operations, infrastructure provisioning, and managing user accounts. Using the bag-of-word technique, we observe a median F-Measure of 0.74, 0.71, and 0.73, respectively, for Mozilla, Openstack, and Wikimedia Commons. Using the TF-IDF technique, we observe a median F-Measure of 0.72, 0.74, and 0.70, respectively, for Mozilla, Openstack, and Wikimedia Commons.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367034,,,no,,no,no,no,undetermined,,no
743,Component Selection in Software Engineering - Which Attributes are the Most Important in the Decision Process?,"Component-based software engineering is a common approach to develop and evolve contemporary software systems where different component sourcing options are available: 1)Software developed internally (in-house), 2)Software developed outsourced, 3)Commercial of the shelf software, and 4) Open Source Software. However, there is little available research on what attributes of a component are the most important ones when selecting new components. The object of the present study is to investigate what matters the most to industry practitioners during component selection. We conducted a cross-domain anonymous survey with industry practitioners involved in component selection. First, the practitioners selected the most important attributes from a list. Next, they prioritized their selection using the Hundred-Dollar ($100) test. We analyzed the results using Compositional Data Analysis. The descriptive results showed that Cost was clearly considered the most important attribute during the component selection. Other important attributes for the practitioners were: Support of the component, Longevity prediction, and Level of off-the-shelf fit to product. Next, an exploratory analysis was conducted based on the practitioners' inherent characteristics. Nonparametric tests and biplots were used. It seems that smaller organizations and more immature products focus on different attributes than bigger organizations and mature products which focus more on Cost.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8498206,,,no,,no,,no,undetermined,,no
744,Context-Aware Patch Generation for Better Automated Program Repair,"The effectiveness of search-based automated program repair is limited in the number of correct patches that can be successfully generated. There are two causes of such limitation. First, the search space does not contain the correct patch. Second, the search space is huge and therefore the correct patch cannot be generated (ie correct patches are either generated after incorrect plausible ones or not generated within the time budget). To increase the likelihood of including the correct patches in the search space, we propose to work at a fine granularity in terms of AST nodes. This, however, will further enlarge the search space, increasing the challenge to find the correct patches. We address the challenge by devising a strategy to prioritize the candidate patches based on their likelihood of being correct. Specifically, we study the use of AST nodes' context information to estimate the likelihood. In this paper, we propose CapGen, a context-aware patch generation technique. The novelty which allows CapGen to produce more correct patches lies in three aspects: (1) The fine-granularity design enables it to find more correct fixing ingredients; (2) The context-aware prioritization of mutation operators enables it to constrain the search space; (3) Three context-aware models enable it to rank correct patches at high positions before incorrect plausible ones. We evaluate CapGen on Defects4J and compare it with the state-of-the-art program repair techniques. Our evaluation shows that CapGen outperforms and complements existing techniques. CapGen achieves a high precision of 84.00% and can prioritize the correct patches before 98.78% of the incorrect plausible ones.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453055,,,no,,yes,,no,undetermined,,no
746,Dynamic programming optimization algorithm applied in test case selection,"In this paper we propose a quadratic dynamic programming algorithm applied in software testing domain, more specific in the test case selection decision making. We addressed a specific problem in software testing: running a subset of test cases from the whole set of available test cases in a limited time frame with the goal of maximizing the chances of finding potential defects. We employed both objective methods as the dynamic programming algorithm and subjective and empiric human decision as defining the selection and prioritization criteria. The proposed solution is suited for medium to large projects where in the worst-case scenarios the memory space complexity of the proposed algorithm does not exceed the order of GBytes. The proposed optimization algorithm is presented in pseudocode along with the dynamic programming recurrence formula and potential selection criteria as currently used in the industry.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8583984,,,no,,no,yes,no,undetermined,,no
748,Embedded Platform for Gas Applications Using Hardware/Software Co-Design and RFID,"This paper presents the development of a wireless low power reconfigurable self-calibrated multi-sensing platform for gas sensing applications. The proposed electronic nose (EN) system monitors gas temperatures, concentrations, and mixtures wirelessly using the radio-frequency identification (RFID) technology. The EN takes the form of a set of gas and temperature sensors and multiple pattern recognition algorithms implemented on the Zynq system on chip (SoC) platform. The gas and temperature sensors are integrated on a semi-passive RFID tag to reduce the consumed power. Various gas sensors are tested, including an in-house fabricated 4??4 SnO<sub>2</sub>based sensor and seven commercial Figaro sensors. The data is transmitted to the Zynq based processing unit using a RFID reader, where it is processed using multiple pattern recognition algorithms for dimensionality reduction and classification. Multiple algorithms are explored for optimum performance, including principal component analysis (PCA) and linear discriminant analysis (LDA) for dimensionality reduction while decision tree (DT) and k-nearest neighbors (KNN) are assessed for classification purpose. Different gases are targeted at diverse concentration, including carbon monoxide (CO), ethanol (C<sub>2</sub>H<sub>6</sub>O), carbon dioxide (CO<sub>2</sub>), propane (C<sub>3</sub>H<sub>8</sub>), ammonia (NH<sub>3</sub>), and hydrogen (H<sub>2</sub>). An accuracy of 100% is achieved in many cases with an overall accuracy above 90% in most scenarios. Finally, the hardware/software heterogeneous solution to implementation PCA, LDA, DT, and KNN on the Zynq SoC shows promising results in terms of resources usage, power consumption, and processing time.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330010,,,no,,no,,no,undetermined,,no
754,Identifying Technical Debt in Database Normalization Using Association Rule Mining,"In previous work, we explored a new context of technical debt that relates to database normalization design decisions. We claimed that database normalization debts are likely to be incurred for tables below the fourth normal form. We proposed a method to prioritize the tables that should be normalized based on their impact on data quality and performance. In this study, we propose a framework to identify normalization debt items (i.e. tables below the fourth normal form) by mining the data stored in each table. Our framework makes use of association rule mining to discover functional dependencies between attributes in a table, which will help determine the current normal form of that table and reveal debt tables. To illustrate our method, we use a case study from Microsoft, AdventureWorks database. The results revealed the applicability of our framework to identify debt tables.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8498244,,,no,,yes,,no,undetermined,,no
756,Influencers of Quality Assurance in an Open Source Community,"ROS (Robot Operating System) is an open source community in robotics that is developing standard robotics operating system facilities such as hardware abstraction, low-level device control, communication middleware, and a wide range of software components for robotics functionality. This paper studies the quality assurance practices of the ROS community. We use qualitative methods to understand how ideology, priorities of the community, culture, sustainability, complexity, and adaptability of the community affect the implementation of quality assurance practices. Our analysis suggests that software engineering practices require social and cultural alignment and adaptation to the community particularities to achieve seamless implementation in open source environments. This alignment should be incorporated into the design and implementation of quality assurance practices in open source communities.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445538,,,no,,no,yes,no,undetermined,,no
757,Integrating Weight Assignment Strategies With NSGA-II for Supporting User Preference Multiobjective Optimization,"Driven by the needs of several industrial projects on the applications of multiobjective search algorithms, we observed that user preferences must be properly incorporated into optimization objectives. However, existing algorithms usually treat all the objectives with equal priorities and do not provide a mechanism to reflect user preferences. To address this, we propose an extension-user-preference multiobjective optimization algorithm (UPMOA), to the most commonly applied, nondominated sorting genetic algorithm II by introducing a user preference indicator ??, based on existing weight assignment strategies [e.g., uniformly distributed weights (UDW)]. We empirically evaluated UPMOA using four industrial problems from three diverse domains (i.e., communication, maritime, and subsea oil and gas). We also performed a sensitivity analysis for UPMOA with 625 algorithm parameter settings. To further assess the performance and scalability, 103 500 artificial problems were created and evaluated representing 207 sets of user preferences. Results show that the UDW strategy with UPMOA achieves the best performance and UPMOA significantly outperformed other three multiobjective search algorithms, and has the ability to solve problems with a wide range of complexity. We also observed that different parameter settings led to the varied performance of UPMOA, thus suggesting that configuring proper parameters is highly problem-specific.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8123878,,,no,,no,,no,undetermined,,no
759,Learning to Accelerate Compiler Testing,"Compilers are one of the most important software infrastructures. Compiler testing is an effective and widely-used way to assure the quality of compilers. While many compiler testing techniques have been proposed to detect compiler bugs, these techniques still suffer from the serious efficiency problem. This is because these techniques need to run a large number of randomly generated test programs on the fly through automated test-generation tools (e.g., Csmith). To accelerate compiler testing, it is desirable to schedule the execution order of the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. Since different test programs tend to trigger the same compiler bug, the ideal goal of accelerating compiler testing is to execute the test programs triggering different compiler bugs in the beginning. However, such perfect goal is hard to achieve, and thus in this work, we design four steps to approach the ideal goal through learning, in order to largely accelerate compiler testing.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449630,,,no,,yes,,no,undetermined,,no
760,MDroid+: A Mutation Testing Framework for Android,"Mutation testing has shown great promise in assessing the effectiveness of test suites while exhibiting additional applications to test-case generation, selection, and prioritization. Traditional mutation testing typically utilizes a set of simple language specific source code transformations, called operators, to introduce faults. However, empirical studies have shown that for mutation testing to be most effective, these simple operators must be augmented with operators specific to the domain of the software under test. One challenging software domain for the application of mutation testing is that of mobile apps. While mobile devices and accompanying apps have become a mainstay of modern computing, the frameworks and patterns utilized in their development make testing and verification particularly difficult. As a step toward helping to measure and ensure the effectiveness of mobile testing practices, we introduce MDroid+, an automated framework for mutation testing of Android apps. MDroid+ includes 38 mutation operators from ten empirically derived types of Android faults and has been applied to generate over 8,000 mutants for more than 50 apps.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449438,,,no,,no,no,no,undetermined,,no
761,Methods and Tools for Focusing and Prioritizing the Testing Effort,"Software testing is essential for any software development process, representing an extremely expensive activity. Despite its importance recent studies showed that developers rarely test their application and most programming sessions end without any test execution. Indeed, new methods and tools able to better allocating the developers effort are needed to increment the system reliability and to reduce the testing costs. In this work we focus on three activities able to optimize testing activities, specifically, bug prediction, test case prioritization, and energy leaks detection. Indeed, despite the effort devoted in the last decades by the research community led to interesting results, we highlight some aspects that might be improved and propose empirical investigations and novel approaches. Finally, we provide a set of open issues that should be addressed by the research community in the future.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530092,,,yes,,yes,,yes,undetermined,no,no
762,MS-guided many-objective evolutionary optimisation for test suite minimisation,"Test suite minimisation is a process that seeks to identify and then eliminate the obsolete or redundant test cases from the test suite. It is a trade-off between cost and other value criteria and is appropriate to be described as a many-objective optimisation problem. This study introduces a mutation score (MS)-guided many-objective optimisation approach, which prioritises the fault detection ability of test cases and takes MS, cost and three standard code coverage criteria as objectives for the test suite minimisation process. They use six classical evolutionary many-objective optimisation algorithms to identify efficient test suite, and select three small programs from the Software-Artefact Infrastructure Repository (SIR) and two larger program space and gzip for experimental evaluation as well as statistical analysis. The experiment results of the three small programs show non-dominated sorting genetic algorithm II (NSGA-II) with tuning was the most effective approach. However, MOEA/D-PBI and MOEA/D-WS outperform NSGA-II in the cases of two large programs. On the other hand, the test cost of the optimal test suite obtained by their proposed MS-guided many-objective optimisation approach is much lower than the one without it in most situation for both small programs and large programs.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8572623,,,yes,,no,,no,undetermined,,no
764,On the Suitability of a Portfolio-Based Design Improvement Approach,"The design debt metaphor tries to illustrate quality deficits in the design of a software and the impact thereof to the business value of the system. To pay off the debt, the literature offers various approaches for identifying and prioritizing these design flaws, but without proper support in aligning strategic improvement actions to the identified issues. This work addresses this challenge and examines the suitability of our proposed portfolio-based design assessment approach. Therefore, this investigation is conducted based on three case studies where the product source code was analyzed and assessed using our portfolio-based approach. As a result, the approach has proven to be able to recommend concrete and valuable design improvement actions that can be adapted to project constraints.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424976,,,no,,no,,no,undetermined,,no
765,Poster: An Experimental Analysis of Fault Detection Capabilities of Covering Array Constructors,"Combinatorial Interaction Testing (CIT) aims at constructing an effective test suite, such as a Covering Array (CA), that can detect faults that are caused by the interaction of parameters. In this paper, we report on some empirical studies conducted to examine the fault detection capabilities of five popular CA constructors: ACTS, Jenny, PICT, CASA, and TCA. The experimental results indicate that Jenny has the best performance, because it achieves better fault detection than the other four constructors in many cases. Our results also indicate that CAs generated using ACTS, PICT, or CASA should be prioritized before testing.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449522,,,no,,no,,no,undetermined,,no
766,Poster: Identification of Methods with Low Fault Risk,"Test resources are usually limited and therefore it is often not possible to completely test an application before a release. Therefore, testers need to focus their activities on the relevant code regions. In this paper, we introduce an inverse defect prediction approach to identify methods that contain hardly any faults. We applied our approach to six Java open-source projects and show that on average 31.6% of the methods of a project have a low fault risk; they contain in total, on average, only 5.8% of all faults. Furthermore, the results suggest that, unlike defect prediction, our approach can also be applied in cross-project prediction scenarios. Therefore, inverse defect prediction can help prioritize untested code areas and guide testers to increase the fault detection probability.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449591,,,yes,,no,,no,undetermined,,no
769,"Prioritizing Alerts from Multiple Static Analysis Tools, Using Classification Models","Static analysis (SA) tools examine code for flaws without executing the code, and produce warnings (""alerts"") about possible flaws. A human auditor then evaluates the validity of the purported code flaws. The effort required to manually audit all alerts and repair all confirmed code flaws is often too much for a project's budget and schedule. An alert triaging tool enables strategically prioritizing alerts for examination, and could use classifier confidence. We developed and tested classification models that predict if static analysis alerts are true or false positives, using a novel combination of multiple static analysis tools, features from the alerts, alert fusion, code base metrics, and archived audit determinations. We developed classifiers using a partition of the data, then evaluated the performance of the classifier using standard measurements, including specificity, sensitivity, and accuracy. Test results and overall data analysis show accurate classifiers were developed, and specifically using multiple SA tools increased classifier accuracy, but labeled data for many types of flaws were inadequately represented (if at all) in the archive data, resulting in poor predictive accuracy for many of those flaws.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445036,,,no,,no,no,no,undetermined,,no
770,Prioritizing Browser Environments for Web Application Test Execution,"When testing client-side web applications, it is important to consider different web-browser environments. Different properties of these environments such as web-browser types and underlying platforms may cause a web application to exhibit different types of failures. As web applications evolve, they must be regression tested across these different environments. Because there are many environments to consider this process can be expensive, resulting in delayed feedback about failures in applications. In this work, we propose six techniques for providing a developer with faster feedback on failures when regression testing web applications across different web-browser environments. Our techniques draw on methods used in test case prioritization; however, in our case we prioritize web-browser environments, based on information on recent and frequent failures. We evaluated our approach using four non-trivial and popular open-source web applications. Our results show that our techniques outperform two baseline methods, namely, no ordering and random ordering, in terms of the cost-effectiveness. The improvement rates ranged from -12.24% to 39.05% for no ordering, and from -0.04% to 45.85% for random ordering.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453107,,,yes,,yes,,yes,undetermined,no,no
772,Region Priority Based Adaptive 360-Degree Video Streaming Using DASH,"With the continuous improvement of Virtual Reality (VR) hardware and software facilities and development of VR streaming platform, the VR industry ushered in a period of rapid development. VR makes use of 360-degree panoramic or omnidirectional video with high resolution and high frame rate in order to create the immersive experience to the user. However those characteristics cause the big volume of 360-degree video and bandwidth intensive during transmission. Due to the limitation of the human eye vision, the user can only watch the part of the 360-degree video in a head-mounted display (HMD) at one time. Hence, streaming the VR video by the traditional video transmission method causes bandwidth waste. In view of this, this paper proposed a 360-degree video adaptive transmission method based on user viewport. Firstly, a 360-degree video region prioritization scheme is proposed based on user vision characteristics and head motion features. Different priority regions transmit 360-degree video content with different quality levels as the format of tiles. Then, based on the bandwidth estimation, buffer status and user viewport prediction, a 360-degree video adaptive transmission decision strategy is given. According to the predicted available bandwidth and viewport, the quality combination of the 360-degree video tiles to be transmitted in the near future is determined to implement adaptive transmission. The test results of the 360-degree video transmission experiment system based on the DASH standard show that the proposed 360-degree video adaptive transmission strategy can effectively reduce the bandwidth consumption on the basis of guaranteeing the user Quality of Experience(QoE).",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455396,,,no,,no,,no,undetermined,,no
774,Requirements Analysis Skills: How to Train Practitioners?,"One of the goals of any software development organization (SDO) is the assurance of a high quality software. To achieve this, it is important to perform all the software related activities, especially those of the requirements engineering (RE) phase, in the right way and ideally by experts. However, the current practice reveals that this crucial phase is commonly performed by people with limited experience in RE, indeed, some of them ignoring the basic activities. We present a training plan in order to improve practitioners' RE analysis skills. The training plan was applied to 44 practitioners working at a Mexican SDO. We developed such a plan based on the idea of considering six main dimensions that include theory, tests and mentoring sessions. The so called dimensions are: understanding the organization's domain, basic concepts of RE, requirements elicitation, requirements expression, requirements prioritization and requirements analysis. In this paper, we present what are the topics discussed in each dimension, the feedback received by the practitioners after the training and how we envision the evolution of the training plan.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8501281,,,no,,no,,no,undetermined,,no
775,SDN-Based Architecture for Providing QoS to High Performance Distributed Applications,"The specification of quality of service (QoS) requirements in traditional networks is limited by the high administrative cost of these environments. Nevertheless, newer network paradigms, as software-defined networks (SDNs), simplify and relaxes the management of networks. In this sense, SDN can provide a simple/effective way to develop QoS provisioning. In this paper, we propose a QoS provision architecture exploiting the capabilities of SDN. Our approach allows the specification of classes of service and also negotiates the QoS requirements between applications and the SDN network controller. The SDN controller, in turn, monitors the network and adjusts its performance through resource reservation and traffic prioritization. We developed a proof-of-concept of our proposal and, our experimental results show that the additional routines present low overhead, whereas -for a given test application- we observe a reduction of up to 47% in transfer times.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538694,,,no,,no,,no,undetermined,,no
776,Search-Based Optimization for the Testing Resource Allocation Problem: Research Trends and Opportunities,"This paper explores the usage of search-based techniques for the Testing Resource Allocation Problem (TRAP). We focus on the analysis of the literature, surveying the research proposals where search-based techniques are exploited for different formulations of the TRAP. Three dimensions are considered: the model formulation, solution, and validation. The analysis allows to derive several observations, and finally outline some new research directions towards better (namely, closer to real-world settings) modelling and solutions, highlighting the most promising areas of investigation.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452803,,,no,,no,,no,undetermined,,no
782,Varying defect prediction approaches during project evolution: A preliminary investigation,"Defect prediction approaches use various features of software product or process to prioritize testing, analysis and general quality assurance activities. Such approaches require the availability of project's historical data, making them inapplicable in early phase. To cope with this problem, researchers have proposed cross-project and even cross-company prediction models, which use training material from other projects to build the model. Despite such advances, there is limited knowledge of how, as the project evolves, it would be convenient to still keep using data from other projects, and when, instead, it might become convenient to switch towards a local prediction model. This paper empirically investigates, using historical data from four open source projects, on how the performance of various kinds of defect prediction approaches - within-project prediction, local and global cross-project prediction, and mixed (injected local cross) prediction - varies over time. Results of the study are part of a long-term investigation towards supporting the customization of defect prediction models over projects' history.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368451,,,yes,,no,,no,undetermined,,no
783,Virtual Test Method for Complex and Variant-Rich Automotive Systems,"The fast development of embedded automotive systems in form of connected Electronic Control Units (ECUs) has led to complex development processes. Especially for safetycritical functions, the testing activities are essential to check if the designed system complies with the requirements. Nowadays, the continuous development of mobile electronic devices through software updates is performed almost on a daily basis. This trend is now starting to be observed in cyber-physical systems with higher safety priorities. In the automotive field, the rising software portion in the vehicles and the shortening technology life-cycles are accentuating the need for Software Over The Air (SOTA) updates. Despite the opportunities offered by SOTA updates, the current test processes and methods must be adapted to manage the resulting complexity throughout the life-cycle of the vehicles. Especially the typical variants abundance in automotive product lines is considered as an important challenge, which cannot be solved only by __?_classical__?_ testing methods such as Hardware-In-the-Loop. In this paper, we present a testing method for variantrich systems, which can be applied for automotive software updates. It uses virtual platforms for automated delta testing to handle the abundance of system configurations. Virtual testing is introduced as a powerful tool to reduce the amount of real tests and allow efficient variants verification. As a proof of concept, an Adaptive Cruise Control (ACC) composed of two ECUs has been implemented both in real hardware and using a virtual platform. With this approach, virtual delta tests, i. e. specific test-benches targeting the differences to a basic variant, can be rapidly executed for various system configurations. To prove the feasibility of the presented test method in more complex systems, a scalability study has been conducted.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519599,,,no,,no,no,no,undetermined,,no
3,Practical Priorities in System Testing,"During the system test phase, ""thorough testing"" can pass the limits of practicality. Test case selection, based on simple priority rules, is one solution to the problem of practicality vs. thoroughness.",1985,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1695401,yes,,yes,,,,yes,no,no,no
20,A study of effective regression testing in practice,"The purpose of regression testing is to ensure that changes made to software, such as adding new features or modifying existing features, have not adversely affected features of the software that should not change. Regression testing is usually performed by running some, or all, of the test cases created to test modifications in previous versions of the software. Many techniques have been reported on how to select regression tests so that the number of test cases does not grow too large as the software evolves. Our proposed hybrid technique combines modification, minimization and prioritization-based selection using a list of source code changes and the execution traces from test cases run on previous versions. This technique seeks to identify a representative subset of all test cases that may result in different output behavior on the new software version. We report our experience with a tool called ATAC (Automatic Testing Analysis tool in C) which implements this technique.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630875,yes,,yes,,,,yes,no,no,no
67,Modeling the cost-benefits tradeoffs for regression testing techniques,"Regression testing is an expensive activity that can account for a large proportion of the software maintenance budget. Because engineers add tests into test suites as software evolves, over time, increased test suite size makes revalidation of the software more expensive. Regression test selection, test suite reduction, and test case prioritization techniques can help with this, by reducing the number of regression tests that must be run and by helping testers meet testing objectives more quickly. These techniques, however can be expensive to employ and may not reduce overall regression testing costs. Thus, practitioners and researchers could benefit from cost models that would help them assess the cost-benefits of techniques. Cost models have been proposed for this purpose, but some of these models omit important factors, and others cannot truly evaluate cost-effectiveness. In this paper, we present new cost-benefits models for regression test selection, test suite reduction, and test case prioritization, that capture previously omitted factors, and support cost-benefits analyses where they were not supported before. We present the results of an empirical study assessing these models.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167767,yes,,yes,,,,yes,no,no,no
89,Code-coverage guided prioritized test generation,"With Internet applications spreading like wildfire, software testing is challenged with new topics related to the distributed nature of Web applications. We apply code based testing techniques to the testing of Web applications, specifically Java programs. Source code based automatic test generation is difficult because most previous methods use constraint satisfaction models as a solution, which is an NP complete problem [M. J. Gallagher et al. (1997)]. We present a method of guiding users through test case generations. Instead of automating the entire procedure, our method aims at generating a framework of test cases and providing instructions for users to instantiate the framework into actual executable test cases. An early experimental study of this method shows the effectiveness of this method.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342705,no,,no,,,,no,no,,no
91,New optical switches enable automated testing with true flexibility,"The proliferation of fiber optic systems in military and avionics platforms is driven by the ever increasing need for higher data rates to support multi-sensor data fusion. Traditionally, the test systems to support these optical deployments are manual and inefficient. Increasingly fast optical components require optical test equipment that is very expensive. To make cost effective test suites, it is essential that these high value resources be used efficiently. This is most effectively accomplished through test architectures that are remotely controlled and automatically scheduled. These test architectures also enable a diverse set of testing applications to be simultaneously executed within an optical test lab or manufacturing environment. The advent of optical matrix switching technology with sub 1dB insertion loss performance and repeatability measured in milli dB's opens up new doors for highly efficient, remotely controlled, automated test systems. The ultra low loss aspects of these switches enable distributed test architectures that were previously unrealizable. Distributed test architectures create a test environment where expensive test equipment can be leveraged over a greater number of test samples in a more timely and automated fashion. This allows the lab manager to prioritize and schedule tests across many users, DUTs, and test equipment bays in an operation that can run 24/7. This paper explores the enabling photonic switch technology and a couple generic test architectures that can be applied in a variety of automated applications to increase test equipment usage and efficiency, thus lowering end costs for deployable fiber optic components and systems.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436883,no,,no,,,,no,no,,no
104,Test factoring: focusing test suites for the task at hand,"Frequent execution of a test suite during software maintenance can catch regression errors early, indicate whether progress is being made, and improve productivity. However, if the test suite takes a long time to produce feedback, the developer is slowed down, and the benefit of frequent testing is reduced. After a program is edited, ideally, only changed code would be tested. Any time spent executing previously tested, unchanged parts of the code is wasted. For a large test suite containing many small unit tests, test selection and prioritization can be effective. Test selection runs only those tests that are possibly affected by the most recent change, and test prioritization can run first the tests that are most likely to reveal a recently-introduced error.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553636,yes,,no,,,,no,no,,no
130,A Multipurpose Code Coverage Tool for Java,"Most test coverage analyzers help in evaluating the effectiveness of testing by providing data on statement and branch coverage achieved during testing. If made available, the coverage information can be very useful for many other related activities, like, regression testing, test case prioritization, test-suite augmentation, test-suite minimization, etc. In this paper, we present a Java-based tool JavaCodeCoverage for test coverage reporting. It supports testing and related activities by recording the test coverage for various code-elements and updating the coverage information when the code being tested is modified. The tool maintains the test coverage information for a set of test cases on individual as well as test suite basis and provides effective visualization for the same. Open source database support of the tool makes it very useful for software testing research",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076910,no,,no,,,,no,no,,no
131,A Novel Approach of Prioritizing Use Case Scenarios,"Modern softwares are very large and complex. As the size and complexity of software increases, software developers feel an urgent need for a better management of different activities during the course of software development. In this paper, we present an approach of use case scenario prioritization suitable for project planning at an early phase of the software development. We consider only use case model in our work. For prioritization, we focus on how critical a scenario path is, which essentially depends on density of overlapping of sub path of a scenario path with other scenario path(s) of a use case. Our proposed approach provides an analytical solution on use case scenario prioritization and is very much effective in project management related activities as substantiated by our experimental results.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425898,no,,yes,,,,no,no,,no
140,Enhancing the Efficiency of Regression Testing through Intelligent Agents,"Software testing is indispensable for all software development. As all mature engineering disciplines need to have systematic testing methodologies, software testing is a very important subject of software engineering. In software development practice, testing accounts for as much as 50% of total development efforts. Testing can be manual, automated, or a combination of both.Manual testing is the process of executing the application and manually interacting with the application, specifying inputs and observing outputs. Manually testing the software is inefficient and costly. It is imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process, which contains many testing related activities using various techniques and methods. In order to automate the process, we have to have some ways to generate oracles from the specification, and generate test cases to test the target software against the oracles to decide their correctness. Today we still don't have a full-scale system that has achieved this goal. In general, significant amount of human intervention is still needed in testing. The degree of automation remains at the automated test script level. This paper therefore provides a timely summary and enhancement of agent theory in software testing ,which motivates recent efforts in adapting concepts and methodologies for agent oriented software testing to complex system which has not previously done. Agent technologies facilitate the automated software testing by virtue of their high level decomposition, independency and parallel activation[4]. Usage of agent based regression testing reduces the complexity involved in prioritizing the testcases.With the ability of agents to act autonomously, monitoring code changes and generating test cases for the changed version of the code can be done dynamically. Agent-Oriented Software testing (AOST) is a nascent but active field of research . A comprehensive methodology that plays an essential role in Software testing must be robust but easy-to use. Moreover, it should provide a roadmap to guide engineers in creating Agent-based systems. The agent based regression testing(ABRT) proposed here is to offer a definition for encompassing to cover the regression testing phenomena based on agents, yet sufficiently tight that it can rule out complex systems that are clearly not agent based.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426561,no,,yes,,,,no,no,,no
144,Multi - Layered Best Basis Image Compression,"Software testing is indispensable for all software development. As all mature engineering disciplines need to have systematic testing methodologies, software testing is a very important subject of software engineering. In software development practice, testing accounts for as much as 50% of total development efforts. Testing can be manual, automated, or a combination of both. Manual testing is the process of executing the application and manually interacting with the application, specifying inputs and observing outputs. Manually testing the software is inefficient and costly. It is imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process, which contains many testing related activities using various techniques and methods. In order to automate the process, we have to have some ways to generate oracles from the specification, and generate test cases to test the target software against the oracles to decide their correctness. Today we still don't have a full-scale system that has achieved this goal. In general, significant amount of human intervention is still needed in testing. The degree of automation remains at the automated test script level. This paper therefore provides a timely summary and enhancement of agent theory in software testing ,which motivates recent efforts in adapting concepts and methodologies for agent oriented software testing to complex system which has not previously done. Agent technologies facilitate the automated software testing by virtue of their high level decomposition, independency and parallel activation. Usage of agent based regression testing reduces the complexity involved in prioritizing the test cases.With the ability of agents to act autonomously, monitoring code changes and generating test cases for the changed version of the code can be done dynamically. Agent-oriented software testing (AOST) is a nascent but active field of research . A comprehensive methodology that plays an essential role in software testing must be robust but easy-to use. Moreover, it should provide a roadmap to guide engineers in creating agent-based systems. The agent based regression testing (ABRT) proposed here is to offer a definition for encompassing to cover the regression testing phenomena based on agents, yet sufficiently tight that it can rule out complex systems that are clearly not agent based.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426345,no,,no,,,,no,no,,no
149,Software Fault Localization Based on Testing Requirement and Program Slice,"A heuristic approach is proposed to locate a fault according to the priority. To a given test case wt, fault localization has to be proceeded when its output wrong. Firstly, four assistant test cases, one failed and three successful test cases, are selected out according to the biggest cardinality of Req(wt,t<sub>i</sub>), which stand for the common testing requirements both covered by wt and t<sub>i</sub>. Then, code prioritization methodology is put forward based on program slice technique. Dynamic slice technique is taken for wt and execution slice technique for four assistant test cases. Some dices are constructed with different priority which means the possibility of containing bug and is evaluated according to the occurrences in the selected slices. Thirdly, the key algorithm including two procedures, refining and augmenting, is followed here to fault localization based on priority. In the refining phase, the most suspicious codes am checked step by step; in the augmenting phase, more codes will be gradually considered on the basis of direct data dependency. At last, experimental studies are performed to illustrate the effectiveness of the technique.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286423,no,,no,,,,no,no,,no
160,Value-Oriented Requirements Prioritization in a Small Development Organization,"Requirements engineering, especially requirements prioritization and selection, plays a critical role in overall project development. In small companies, this often difficult process can affect not only project success but also overall company survivability. A value-oriented prioritization (VOP) framework can help this process by clarifying and quantifying the selection and prioritization issues. A case study of a small development company shows a successful VOP deployment that improved communications and saved time by focusing requirements decisions for new product releases on core company values",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052549,no,,,no,,,no,no,,no
171,Automated Generation and Assessment of Autonomous Systems Test Cases,"Verification and validation testing of autonomous spacecraft routinely culminates in the exploration of anomalous or faulted mission-like scenarios. Prioritizing which scenarios to develop usually comes down to focusing on the most vulnerable areas and ensuring the best return on investment of test time. Rules-of-thumb strategies often come into play, such as injecting applicable anomalies prior to, during, and after system state changes; or, creating cases that ensure good safety-net algorithm coverage. Although experience and judgment in test selection can lead to high levels of confidence about the majority of a system's autonomy, it's likely that important test cases are overlooked. One method to fill in potential test coverage gaps is to automatically generate and execute test cases using algorithms that ensure desirable properties about the coverage. For example, generate cases for all possible fault monitors, and across all state change boundaries. Of course, the scope of coverage is determined by the test environment capabilities, where a faster-than-real-time, high-fidelity, software-only simulation would allow the broadest coverage. Even real-time systems that can be replicated and run in parallel, and that have reliable set-up and operations features provide an excellent resource for automated testing. Making detailed predictions for the outcome of such tests can be difficult, and when algorithmic means are employed to produce hundreds or even thousands of cases, generating predicts individually is impractical, and generating predicts with tools requires executable models of the design and environment that themselves require a complete test program. Therefore, evaluating the results of large number of mission scenario tests poses special challenges. A good approach to address this problem is to automatically score the results based on a range of metrics. Although the specific means of scoring depends highly on the application, the use of formal scoring metrics has high value in identifying and prioritizing anomalies, and in presenting an overall picture of the state of the test program. In this paper we present a case study based on automatic generation and assessment of faulted test runs for the Dawn mission, and discuss its role in optimizing the allocation of resources for completing the test program.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526484,no,,,no,,,no,no,,no
213,How Well Do Test Case Prioritization Techniques Support Statistical Fault Localization,"In continuous integration, a tight integration of test case prioritization techniques and fault-localization techniques may both expose failures faster and locate faults more effectively. Statistical fault-localization techniques use the execution information collected during testing to locate faults. Executing a small fraction of a prioritized test suite reduces the cost of testing, and yet the subsequent fault localization may suffer. This paper presents the first empirical study to examine the impact of test case prioritization on the effectiveness of fault localization. Among many interesting empirical results, we find that coverage-based and random techniques can be more effective than distribution-based techniques in supporting statistical fault localization.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254274,yes,,,yes,,,yes,no,no,no
224,Prioritization of Scenarios Based on UML Activity Diagrams,"Increased size and complexity of software requires better methods for different activities in the software development lifecycle. Quality assurance of software is primarily done by means of testing, an activity that faces constraints of both time and resources. Hence, there is need to test effectively within the constraints in order to maximize throughput i.e. rate of fault detection, coverage, etc. Test case prioritization involves techniques aimed at finding the best prioritized test suite. In this paper, we propose a prioritization technique based on UML activity diagrams. The constructs of an activity diagram are used to prioritize scenarios. Preliminary results obtained on a case-study indicate that the technique is effective in extracting the critical scenarios from the activity diagram.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231936,no,,,yes,,,no,no,,no
229,Prioritizing Use Cases to Aid Ordering of Scenarios,"Models are used as the basis for design and testing of software. The unified modeling language (UML) is used to capture and model the requirements of a software system. One of the major requirements of a development process is to detect defects as early as possible. Effective prioritization of scenarios helps in early detection of defects as well maximize effort and utilization of resources. Use case diagrams are used to represent the requirements of a software system. In this paper, we propose using data captured from the primitives of the use case diagrams to aid in prioritization of scenarios generated from activity diagrams. Interactions among the primitives in the diagrams are used to guide prioritization. Customer prioritization of use cases is taken as one of the factors. Preliminary results on a case study indicate that the technique is effective in prioritization of test scenarios.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358815,yes,,,yes,,,yes,no,no,no
231,Reducing Field Failures in System Configurable Software: Cost-Based Prioritization,"System testing of configurable software is an expensive and resource constrained process. Insufficient testing often leads to escaped faults in the field where failures impact customers and are costly to repair. Prior work has shown that it is possible to efficiently sample configurations for testing using combinatorial interaction testing, and to prioritize these configurations to increase the rate of early fault detection. The underlying assumption to date has been that there is no added complexity to configuring a system level environment over a user configurable one; i.e. the time required to setup and test each individual configuration is nominal. In this paper we examine prioritization of system configurable software driven not only by fault detection but also by the cost of configuration and setup time that moving between different configurations incurs. We present a case study on two releases of an enterprise software system using failures reported in the field. We examine the most effective prioritization technique and conclude that (1) using failure history of configurations can improve the early fault detection rate, but that (2) we must consider fault detection rate over time, not by the number of configurations tested. It is better to test related configurations which incur minimal setup time than to test fewer, more diverse configurations.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362084,no,,,no,,,no,no,,no
237,Test Selection Prioritization Strategy,"A wide divergence is observed in projects between test activities planned in the test plan and the actual tests that can be executed. Estimates for test execution computed during the planning are inaccurate without test design. The actual time and resources available are usually less than planned. Assuming that time and resources cannot be changed, a dynamic selection of tests for execution that maximizes quality is required.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254084,no,,,yes,,,no,no,,no
250,Analysis and optimization of software requirements prioritization techniques,"Prioritizing requirements helps the project team to understand which requirements are most important and most urgent. Based on this finding a software engineer can decide what to develop/implement in the first release and what on the coming releases. Prioritization is also a useful activity for decision making in other phases of software engineering like development, testing, and implementation. There are a number of techniques available to prioritize the requirements with their associated strengths and limitations. In this paper we will examine state of the art techniques and analyze their applicability on software requirements domain. At the end we present a framework that will help the software engineer of how to perform prioritization process by combining existing techniques and approaches.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625687,no,,,no,,,no,no,,no
256,Efficient Reduction of Model-Based Generated Test Suites through Test Case Pair Prioritization,"During the development and maintenance of software, test suites often reach a size that exceeds the costs allocated for test suite execution. In such a case, the test suite needs to be reduced. Many papers are dedicated to the problem of test suite reduction. Most of them consider the removal or merging of test cases. However, less attention has been paid to the identification of test case pairs, which are eminently suitable for merging. In this paper, we fill this gap by presenting a novel approach that helps identifying those test case pairs within a given set of systematically generated test cases which, when merged, have potential for high test suite reduction. As a result, test suites reduced by our approach are considerably smaller in size than those, whose pairs are selected randomly.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772249,no,,,yes,,yes,yes,no,no,no
262,"On the Integration of Test Adequacy, Test Case Prioritization, and Statistical Fault Localization","Testing and debugging account for at least 30% of the project effort. Scientific advancements in individual activities or their integration may bring significant impacts to the practice of software development. Fault localization is the foremost debugging sub-activity. Any effective integration between testing and debugging should address how well testing and fault localization can be worked together productively. How likely does a testing technique provide test suites for effective fault localization? To what extent may such a test suite be prioritized so that the test cases having higher priority can be effectively used in a standalone manner to support fault localization? In this paper, we empirically study these two research questions in the context of test data adequacy, test case prioritization and statistical fault localization. Our preliminary postmortem analysis results on 16 test case prioritization techniques and four statistical fault localizations show that branch-adequate test suites on the Siemens suite are unlikely to support effective fault localization. On the other hand, if such a test suite is effective, around 60% of the test cases can be further prioritized to support effective fault localization, which indicates that the potential savings in terms of effort can be significant.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562990,yes,,,yes,,,yes,no,no,no
269,Prioritizing Tests for Software Fault Localization,"Test prioritization techniques select test cases that maximize the confidence on the correctness of the system when the resources for quality assurance (QA) are limited. In the event of a test failing, the fault at the root of the failure has to be localized, adding an extra debugging cost that has to be taken into account as well. However, test suites that are prioritized for failure detection can reduce the amount of useful information for fault localization. This deteriorates the quality of the diagnosis provided, making the subsequent debugging phase more expensive, and defeating the purpose of the test cost minimization. In this paper we introduce a new test case prioritization approach that maximizes the improvement of the diagnostic information per test. Our approach minimizes the loss of diagnostic quality in the prioritized test suite. When considering QA cost as the combination of testing cost and debugging cost, on the Siemens set, the results of our test case prioritization approach show up to a 53% reduction of the overall QA cost, compared with the next best technique.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562943,yes,,,yes,,,yes,no,no,no
273,Regression Test Generation Approach Based on Tree-Structured Analysis,"Regression test generation is an important process to make sure that changes of program have no unintended side-effects. To achieve full confidence, many projects have to re-run all the test cases for entire program, which makes it a time consuming and expensive activity. In this paper, a code based regression testing approach is proposed to generate selected test suites for unit testing. The framework contains five phases: program change detection phase, logical verification phase, branch pruning phase, test case prioritization phase and test suite generation phase. These five phases can achieve detection of program's modification, coding standard, test case pruning, test case prioritizing and inputs generation for regression test cases respectively. A prototype based on this framework is implemented using logical tree-structured analysis, and the preliminary experiment shows that proposed approach can provide efficient regression test suites.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476640,yes,,,no,,,no,no,,no
279,Stress Testing an AI Based Web Service: A Case Study,"The stress testing of AI-based systems differs from the approach taken for more traditional Web services, both in terms of the design of test cases and the metrics used to measure quality. The expected variability in responses of an AI-based system to the same request adds a level of complexity to stress testing, when compared to more standard systems where the system response is deterministic and any deviations may easily be characterized as product defects. Generating test cases for AI-based systems requires balancing breadth of test cases with depth of response quality: most AI-systems may not return a perfect answer. An example of a machine learning translation system is considered, and the approach used for stress testing it is presented, alongside comparisons with a more traditional approach. The challenges of shipping such a system to support a growing set of features and language pairs necessitate a mature prioritization of test cases. This approach has been successful in shipping a Web service that currently serves millions of users per day.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501500,no,,,no,,,no,no,,no
283,The Effectiveness of Regression Testing Techniques in Reducing the Occurrence of Residual Defects,"Regression testing is a necessary maintenance activity that can ensure high quality of the modified software system, and a great deal of research on regression testing has been performed. Most of the studies performed to date, however, have evaluated regression testing techniques under the limited context, such as a short-term assessment, which do not fully account for system evolution or industrial circumstances. One important issue associated with a system lifetime view that we have overlooked in past years is the effects of residual defects - defects that persist undetected - across several releases of a system. Depending on an organization's business goals and the type of system being built, residual defects might affect the level of success of the software products. In this paper, we conducted an empirical study to investigate whether regression testing techniques are effective in reducing the occurrence and persistence of residual defects across a system's lifetime, in particular, considering test case prioritization techniques. Our results show that heuristics can be effective in reducing both the occurrence of residual defects and their age. Our results also indicate that residual defects and their age have a strong impact on the cost-benefits of test case prioritization techniques.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477096,yes,,,no,,,no,no,,no
318,Dealing with Test Automation Debt at Microsoft,"At Microsoft, substantial time and resources are expended in test case development, execution and verification. Thousands of new tests are added to existing test suites without any kind of review regarding their unique contribution to test suite effectiveness or impact on test suite efficiency. This talk describes how we leverage existing code coverage data, together with reduction and prioritization techniques, to help each test team analyze its test suite and guide them in improving their suite's effectiveness and efficiency. The analysis focuses on identifying and deprecating/prioritizing groups of tests cases, given specific tactical goals for example, increasing current test suite stability and reliability, better structuring of test suite migration efforts, reducing test suite execution time and testing with limited hardware resources.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032226,,yes,,no,,no,yes,no,no,no
331,Impact Analysis of Configuration Changes for Test Case Selection,"Testing configurable systems, which are becoming prevalent, is expensive due to the large number of configurations and test cases. Existing approaches reduce this expense by selecting or prioritizing configurations. However, these approaches redundantly run the full test suite for the selected configurations. To address this redundancy, we propose a test case selection approach by analyzing the impact of configuration changes with static program slicing. Given an existing test suite T used for testing a system S under a configuration C, our approach decides for each t in T if t has to be used for testing S under a different configuration C'. We have evaluated our approach on a large industrial system within ABB with promising results.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132962,,no,,no,,yes,no,no,,no
335,Increasing test coverage using human-based approach of fault injection testing,"Fault injection testing (FIT) approach validates system's fault tolerance mechanism by actively injecting software faults into the targeted areas in the system in order to accelerate its failure rate. This highly complements other testing approaches such as requirements and regression testing implemented during the same testing phase. During testing, it is impossible to run all possible test scenarios. It is especially difficult to predict how the user might use the system functionality correctly as per design. The human interaction through the system may be varies and will leads to the functionality loophole. It is therefore important to have strategic testing approach for evaluating the dependability of computer systems especially in human errors. This paper proposed on applying Knowledge-Based, Fault Prediction Model and Test Case Prioritization approaches that can be combined to increase the test coverage. The goal of this paper is to highlight the needs and advantages of the selected approaches in performing FIT as one of effective testing techniques in the ongoing quest for increased software quality.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140685,,yes,,no,,,no,no,,no
340,On Practical Adequate Test Suites for Integrated Test Case Prioritization and Fault Localization,"An effective integration between testing and debugging should address how well testing and fault localization can work together productively. In this paper, we report an empirical study on the effectiveness of using adequate test suites for fault localization. We also investigate the integration of test case prioritization and statistical fault localization with a postmortem analysis approach. Our results on 16 test case prioritization techniques and four statistical fault localization techniques show that, although much advancement has been made in the last decade, test adequacy criteria are still insufficient in supporting effective fault localization. We also find that the use of branch-adequate test suites is more likely than statement-adequate test suites in the effective support of statistical fault localization.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004308,,no,,yes,,no,yes,no,no,no
345,Prioritizing tests for fault localization through ambiguity group reduction,"In practically all development processes, regression tests are used to detect the presence of faults after a modification. If faults are detected, a fault localization algorithm can be used to reduce the manual inspection cost. However, while using test case prioritization to enhance the rate of fault detection of the test suite (e.g., statement coverage), the diagnostic information gain per test is not optimal, which results in needless inspection cost during diagnosis. We present RAPTOR, a test prioritization algorithm for fault localization, based on reducing the similarity between statement execution patterns as the testing progresses. Unlike previous diagnostic prioritization algorithms, RAPTOR does not require false negative information, and is much less complex. Experimental results from the Software Infrastructure Repository's benchmarks show that RAPTOR is the best technique under realistic conditions, with average cost reductions of 40% with respect to the next best technique, with negligible impact on fault detection capability.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100153,,no,,no,,,no,no,,no
351,Scenario Driven Testing,"Software testing has traditionally focused on evaluating the functionality of implemented modules against feature specifications. This approach assumes that customer requirements and usage scenarios are accurately translated into specifications and that individual modules implemented using the feature specifications would work seamlessly and coherently to solve business problems meant to be addressed by the software under test. To ensure software built would help customers solve their business problems as intended, test teams have to go beyond traditional feature driven testing approach and test software for quality and completeness with respect to targeted customer scenarios. For this, test teams have to adopt scenario driven test methodology which involves understanding the targeted customer scenarios and use them along with feature specifications for the intended software solution to translate them into test specifications, prioritization of test work items and use them throughout project for shared understanding of tradeoffs and making decisions. In this short paper, we describe scenario driven testing and share how it was applied to test a feature-set developed for a successful product line at Microsoft??.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945250,,no,,no,,,no,no,,no
370,Case based reasoning approach for adaptive test suite optimization,"Case-based reasoning is an approach to problem solving and learning that has got a lot of attention over the last few years. This paper provides an overview of the foundational issues related to case-based reasoning, describing some of the leading methodological approaches within the field, and exemplifying the current state through pointers to some systems. The framework influences the recent methodologies for knowledge level descriptions of intelligent systems. The methods for case retrieval reuse, solution testing, and learning are summarized, and realization is discussed with few example systems that represent different CBR approaches. Regression testing occurs during the maintenance stage of the software life cycle, however, it requires large amounts of test cases to assure the attainment of a certain degree of quality. So, test suite sizes may grow significantly. This paper focuses primarily on application of CBR to test suite optimization.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395870,,no,,yes,,,no,no,,no
374,Diversity maximization speedup for fault localization,"Fault localization is useful for reducing debugging effort. However, many fault localization techniques require non-trivial number of test cases with oracles, which can determine whether a program behaves correctly for every test input. Test oracle creation is expensive because it can take much manual labeling effort. Given a number of test cases to be executed, it is challenging to minimize the number of test cases requiring manual labeling and in the meantime achieve good fault localization accuracy. To address this challenge, this paper presents a novel test case selection strategy based on Diversity Maximization Speedup (DMS). DMS orders a set of unlabeled test cases in a way that maximizes the effectiveness of a fault localization technique. Developers are only expected to label a much smaller number of test cases along this ordering to achieve good fault localization results. Our experiments with more than 250 bugs from the Software-artifact Infrastructure Repository show (1) that DMS can help existing fault localization techniques to achieve comparable accuracy with on average 67% fewer labeled test cases than previously best test case prioritization techniques, and (2) that given a labeling budget (i.e., a fixed number of labeled test cases), DMS can help existing fault localization techniques reduce their debugging cost (in terms of the amount of code needed to be inspected to locate faults). We conduct hypothesis test and show that the saving of the debugging cost we achieve for the real C programs are statistically significant.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494903,,no,,no,,,no,no,,no
386,On Capturing Effects of Modifications as Data Dependencies,"Dependence analysis on an Extended Finite State Machine (EFSM) representation of the requirements of a system under test has been used in requirements-based regression testing for regression test suite (RTS) reduction (reducing the size of a given test suite by eliminating redundancies), for RTS prioritization (ordering test cases in a given test suite for early fault detection) or for RTS selection (selecting a subset of a test suite covering the identified dependencies). These particular uses of dependence analysis are based on definitions of various types of control and data dependencies (between transitions in an EFSM) caused by a given set of modifications on the requirements. This abstract considers the definitions of data dependencies, gives examples of incompleteness of existing definitions, and presents insights on completing these definitions.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340172,,no,,no,,,no,no,,no
390,Preemptive Regression Test Scheduling Strategies: A New Testing Approach to Thriving on the Volatile Service Environments,"A workflow-based web service may use ultra-late binding to invoke external web services to concretize its implementation at run time. Nonetheless, such external services or the availability of recently used external services may evolve without prior notification, dynamically triggering the workflow-based service to bind to new replacement external services to continue the current execution. Any integration mismatch may cause a failure. In this paper, we propose Preemptive Regression Testing (PRT), a novel testing approach that addresses this adaptive issue. Whenever such a late-change on the service under regression test is detected, PRT preempts the currently executed regression test suite, searches for additional test cases as fixes, runs these fixes, and then resumes the execution of the regression test suite from the preemption point.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340125,,no,,yes,,no,no,no,,no
397,Size-Constrained Regression Test Case Selection Using Multicriteria Optimization,"To ensure that a modified software system has not regressed, one approach is to rerun existing test cases. However, this is a potentially costly task. To mitigate the costs, the testing effort can be optimized by executing only a selected subset of the test cases that are believed to have a better chance of revealing faults. This paper proposes a novel approach for selecting and ordering a predetermined number of test cases from an existing test suite. Our approach forms an Integer Linear Programming problem using two different coverage-based criteria, and uses constraint relaxation to find many close-to-optimal solution points. These points are then combined to obtain a final solution using a voting mechanism. The selected subset of test cases is then prioritized using a greedy algorithm that maximizes minimum coverage in an iterative manner. The proposed approach has been empirically evaluated and the results show significant improvements over existing approaches for some cases and comparable results for the rest. Moreover, our approach provides more consistency compared to existing approaches.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928351,,yes,,no,,,no,no,,no
402,Towards reliable web applications: ISO 19761,"This research adopts a new scenario-based black box testing methodology for testing web applications. It combines a black box testing strategy with the functions (scenarios) measured by the COSMIC-FFP measurement procedure (ISO/IEC 19761 standard) to produce an optimal set of test cases. This testing approach shows its applicability during all the development phases. Moreover, it can be applied during the early development phase once the specifications have been documented as well as after the development phase where we don't have the access to the code. This paper also considers the use of a functional complexity measure for assigning priorities to the generated test cases. Finally, those concepts have been applied on part of Online Banking System as a case study.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6389396,,no,,no,,,no,no,,no
412,A Uniform Representation of Hybrid Criteria for Regression Testing,"Regression testing tasks of test case prioritization, test suite reduction/minimization, and regression test selection are typically centered around criteria that are based on code coverage, test execution costs, and code modifications. Researchers have developed and evaluated new individual criteria; others have combined existing criteria in different ways to form what we--and some others--call hybrid criteria. In this paper, we formalize the notion of combining multiple criteria into a hybrid. Our goal is to create a uniform representation of such combinations so that they can be described unambiguously and shared among researchers. We envision that such sharing will allow researchers to implement, study, extend, and evaluate the hybrids using a common set of techniques and tools. We precisely formulate three hybrid combinations, Rank, Merge, and Choice, and demonstrate their usefulness in two ways. First, we recast, in terms of our formulations, others' previously reported work on hybrid criteria. Second, we use our previous results on test case prioritization to create and evaluate new hybrid criteria. Our findings suggest that hybrid criteria of others can be described using our Merge and Rank formulations, and that the hybrid criteria we developed most often outperformed their constituent individual criteria.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6484067,,no,,no,,,no,no,,no
416,CDM-Suite: An Attributed Test Selection Tool,"Many reasons lead to embedded systems getting more complex than ever. The higher integration and interconnection and the additional effort spend for safety-critical functions, require new techniques to support the testing process. Beneath using model-driven test techniques, test engineers need support to take the right decisions on test case selection and prioritization during the whole development process. Available approaches on test selection and prioritization lack in adaptability to different testing techniques. We previously presented our approach of modeling components and dependences of a system, such as black-box systems, using Component-Dependency-Models (CDMs) as shown in earlier publications. In this paper we present our tool CDM-Suite in detail and show the way it can be used by test engineers. We therefore start with the presentation of our approach and discussing related tools. The next section describes the graphical user interface (GUI) and the different ways of interaction for data generation. This point includes the introduction of metrics build upon fuzzy logic and graph analysis. At last we show how to interpret analysis results and derive knowledge for test case selection and prioritization. We sum the paper up by giving a conclusion and a presentation outline.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569753,,no,,no,,,no,no,,no
421,Defect Prioritization in the Software Industry: Challenges and Opportunities,"Defect prioritization is a decision making process wherein stakeholders determine the temporal order of open defects to be fixed. It is critical to the software development lifecycle as the decisions made during this process directly affect release planning, resource management, and maintenance costs. In fact, defect prioritization is complex as many factors need to be taken into consideration and the decisions made can be subjective or incorporate inherent knowledge and intuition of decision makers. We believe that managing the complexities of the decision making process can provide valuable support and help in uncovering any inconsistencies in the interpretation of criteria to prioritize defects. In this paper, we explore the defect triaging process in Research In Motion to gain a better understanding of the shortcomings and challenges of the current practices. Based on our findings, we sketch some research directions to improve industrial software defect prioritization.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569718,,no,,no,,,no,no,,no
423,Efficient Automated Program Repair through Fault-Recorded Testing Prioritization,"Most techniques for automated program repair use test cases to validate the effectiveness of the produced patches. The validation process can be time-consuming especially when the object programs ship with either lots of test cases or some long-running test cases. To alleviate the cost for testing, we first introduce regression test prioritization insight into the area of automated program repair, and present a novel prioritization technique called FRTP with the goal of reducing the number of test case executions in the repair process. Unlike most existing prioritization techniques frequently requiring additional cost for gathering previous test executions information, FRTP iteratively extracts that information just from the repair process, and thus incurs trivial performance lose. We also built a tool called TrpAutoRepair, which implements our FRTP technique and has the ability of automatically repairing C programs. To evaluate TrpAutoRepair, we compared it with GenProg, a state-of-the-art tool for automated C program repair. The experiment on the 5 subject programs with 16 real-life bugs provides evidence that TrpAutoRepair performs at least as good as GenProg in term of success rate, in most cases (15/16), TrpAutoRepair can significantly improve the repair efficiency by reducing efficiently the test case executions when searching a valid patch in the repair process.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676889,,yes,,no,,no,no,no,,no
438,Reusing black box test paths for white box testing of websites,"As the numbers of web users are increasing exponentially, the software complexity is increasing exponentially and the malwares are increasing exponentially, so exhaustive and extensive testing of websites has become a necessity today. But testing of a website is not 100% exhaustive as the page explosion problem is also very usual. In this paper, we propose to reuse the basis test paths as obtained from the Page-Test-Trees (PTTs) for white box testing of websites. We traverse the same set of paths (obtained above) and test for the source code at these nodes. This saves significant amount of time required to generate test paths and hence test cases as compared to the existing approaches of white box testing. The cost and efforts are also minimized. The proposed technique ensures better website testing coverage as white box testing provides better results than black box testing. Then we validate the proposed reusability testing with two web navigational structures. The results show that doing regression testing can save several billion dollars. These test cases can be further minimized by using prioritization techniques of regression testing.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514424,,yes,,no,,yes,no,no,,no
455,An improved genetic approach for test path generation,"Quality of a software system depends on testing approaches adopted to analyze the software product. Testing process itself depends on two main vectors called test sequence generation and test data generation. Test sequence generation is about to identify the order in which the particular test cases will be executed and the test data defines the various checks performed on each test case. In this present work, a fuzzy improved genetic approach is suggested for test case generation. The sequence on these test cases is here dependent on module interaction analysis. Based on this analysis, the test case prioritization will be defined. Once the test cases will be prioritized, the next work is to apply fuzzy improved genetic approach for test path generation. The work is analyzed under different prioritization vectors. Analysis of work is defined in terms of test cost estimation under different prioritization scenarios.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012823,,yes,,,yes,yes,yes,no,no,no
473,Implementing test case selection and reduction techniques using meta-heuristics,"Regression Testing is an inevitable and very costly maintenance activity that is implemented to make sure the validity of modified software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization to select and prioritize a minimum set of test cases, fulfilling some chosen criteria, that is, covering all possible faults in minimum time and other. In this paper a test case reduction hybrid Particle Swarm Optimization (PSO) algorithm has been proposed. This PSO algorithm uses GA mutation operator while processing. PSO is a swarm intelligence algorithm based on particles behavior. GA is an evolutionary algorithm (EA). The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949377,,yes,,,no,,no,no,,no
528,Improving prioritization of software weaknesses using security models with AVUS,"Testing tools for application security have become an integral part of secure development life-cycles. Despite their ability to spot important software weaknesses, the high number of findings require rigorous prioritization. Most testing tools provide generic ratings to support prioritization. Unfortunately, ratings from established tools lack context information especially with regard to the security requirements of respective components or source code. Thus experts often spend a great deal of time re-assessing the prioritization provided by these tools. This paper introduces our lightweight tool AVUS that adjusts context-free ratings of software weaknesses according to a user-defined security model. We also present a first evaluation applying AVUS to a well-known open source project and the findings of a popular, commercially available application security testing tool.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335423,,no,,,yes,no,no,no,,no
545,Priority Integration for Weighted Combinatorial Testing,"Priorities (weights) for parameter values can improve the effectiveness of combinatorial testing. Previous approaches have employed weights to derive high-priority test cases either earlier or more frequently. Our approach integrates these order-focused and frequency-focused prioritizations. We show that our priority integration realizes a small test suite providing high-priority test cases early and frequently in a good balance. We also propose two algorithms that apply our priority integration to existing combinatorial test generation algorithms. Experimental results using numerous test models show that our approach improves the existing approaches w.r.t. Order-focused and frequency-focused metrics, while overheads in the size and generation time of test suites are small.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273624,,no,,,yes,,no,no,,no
551,Supporting Continuous Integration by Code-Churn Based Test Selection,"Continuous integration promises advantages in large-scale software development by enabling software development organizations to deliver new functions faster. However, implementing continuous integration in large software development organizations is challenging because of organizational, social and technical reasons. One of the technical challenges is the ability to rapidly prioritize the test cases which can be executed quickly and trigger the most failures as early as possible. In our research we propose and evaluate a method for selecting a suitable set of functional regression tests on system level. The method is based on analysis of correlations between test-case failures and source code changes and is evaluated by combining semi-structured interviews and workshops with practitioners at Ericsson and Axis Communications in Sweden. The results show that using measures of precision and recall, the test cases can be prioritized. The prioritization leads to finding an optimal test suite to execute before the integration.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167168,,no,,,yes,,no,no,,no
557,Test case selection for networked production systems,"This paper provides a discussion on the coming technological changes in process automation of networked production systems, which will change the testing procedure. In the smart factory of the future there will be no possibility to reach a test coverage of 100%, assuming a flexible automation with continuous reconfiguration and dynamic changes during runtime. Consequently, large amounts of test cases and powerful algorithms for their prioritization are needed in order to certify the correct functionality of the production systems in the network. A concept is presented on how to analyze and prioritize the enormous amount of test cases resulting from the changes during runtime. The proposed approach for test case selection utilizes information of the product, the process and the status of the for the prioritization and selection.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301604,,no,,,yes,,yes,no,no,no
565,Tracking down high coverage configuration using clustering and fault detection,"Mostly all software systems are highly configured, it has many benefits but there is a difficulty of software testing because there will be unique errors could be hidden in any of the configurations and undergoing testing for each of the configurations will lead to expensive testing and it is also impractical. The dependable systems will have some mechanism for fault tolerance in software testing. If the rate of the fault detection is calculated then the coverage of the configuration can be easily generated. First load the application for which it is going to be tested by using our test case prioritization approach and loading the dataset for the test case for the given application. After this process, need to assign the individual ids for all the test cases in the test case dataset. Also it is able to add the test cases in the dynamic nature. Then to compute the test case prioritization, first built the dependency structure for the test cases. Through the approach get the height and weight matrix for the test cases after this computation the test cases. The cosine similarity values between the test cases. In the similarity values it will show how it is highly related with the other test cases. Thus the clustering approach is introduced for grouping the test cases. These test cases are analyzed for measuring their relevancy and relationship between the test cases using their constrains and the clustering of the test cases is done for the better result in the rate of fault detect. With the Average percentage fault detection the graph is drawn and it shown the high coverage configurations.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453839,,no,,,yes,,yes,no,no,no
570,Using Partition Information to Prioritize Test Cases for Fault Localization,"Fault Localization Prioritization (FLP) aims at reordering existing test cases so that the location of detected faulty components can be identified earlier, using certain fault localization techniques. Although some researchers have proposed adaptive prioritization strategies with white-box code coverage information, such information may not always be available. In this paper, we address the FLP problem using black-box information derived from partitioning the input domain. Based on the well-known technique of Spectra-Based Fault Localization (SBFL), three test case prioritization strategies are designed following some basic SBFL heuristics. The implementation of these proposed strategies relies only on the partition information, and does not require any test case execution history. Experiments show that our strategies, when compared with pure random selection, result in a faster localization of faulty statements, reducing the number of test case executions required. Here, we analyze the characteristics and merits of the three proposed strategies.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273609,,yes,,,yes,,yes,no,no,no
575,A random and coverage-based approach for fault localization prioritization,"Fault Localization Prioritization (FLP) aims to order the execution sequence of test cases so that faulty statements in a faulty program can be localized faster. FLP is an important part of the automation of testing and fault localization in software engineering. The key issue is to identify which test cases can provide most useful information to help locate the faulty statement. Assuming the well-known technique of Spectra-Based Fault Localization (SBFL) is applied, this paper evaluates the quality of a test case based on the characteristics of its statement coverage information. We propose the COverage-based Random (COR) approach to address the FLP problem. Two statement coverage characteristics, the diversity characteristic and the failure-like characteristic, are analyzed and identified as having significant impacts on the effectiveness of fault localization. When using the COR approach, each test case is examined and the degree of each characteristic is measured, with test cases showing high degrees of the characteristics being assigned higher priority for execution. Because of the power of random strategies to improve the robustness of the approach, some random factors in the selection of test cases are included. Empirical studies show that, compared with existing approaches, the COR approach results in a faster localization of faulty statements, reducing the number of necessary test case executions.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7531562,,yes,,,yes,,yes,no,no,no
584,Automated testcase generation for software quality assurance,"The overall venture of the software engineering is to guarantee the delivery of high quality software to the client. To certify high quality software, it is required to test software. Testing is a decisive constituent of software engineering. In software testing there are number of underlying issues like effective generation of test cases, prioritisation of test cases which need to be tackled. This automated test case generation mainly depends on these four aspects: test strategy, test case generation, test execution and test evaluation. Test strategy is a collection of events that determines the testing approach to be followed by the testing team. The test case generation refers to the generation of testcases based on the certain application. The test execution briefs about the execution of those tests then comparing the expected result with actual result. The test evaluation investigates the test cases and helps us to generate test report and software quality assurance report automatically. The intention of producing this tool is to generate test cases automatically and to decrease the cost of testing in addition to accumulate the time of deriving test cases physically. Hence this system helps to improve overall quality of the software.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727003,,no,,,no,no,no,no,,no
592,Customized Regression Testing Using Telemetry Usage Patterns,"Pervasive telemetry in modern applications is providing new possibilities in the application of regression testing techniques. Similar to how research in bioinformatics is leading to personalized medicine, tailored to individuals, usage telemetry in modern software allows for custom regression testing, tailored to the usage patterns of an installation. By customizing regression testing based on software usage, the effectiveness of regression testing techniques can be greatly improved, leading to reduced testing costs and enhanced detection of defects that are most important to that customer. In this research, we introduce the concept of fingerprinting software usage patterns through telemetry. We provide various algorithms tocompute fingerprints and conduct an empirical study that shows that fingerprints are effective in identifying distinct usage patterns. Further, we discuss how usage fingerprints can be used to improve regression test prioritization run time by over 30 percent compared to traditional prioritization techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816511,,no,,,no,no,no,no,,no
606,Functional stratification of biomarkers selected from microarray data for understanding oral leukoplakia associated carcinogenesis,"In this research paper, we have proposed a three tier conceptual framework AVISAR for the testing of Object Oriented Programs. We have devised the proposed framework into three levels: Requirements Modeling, Test Case Generation and Effort Estimation. At the first level, we have used the Extend relationship of the Use Case to capture the events generated by the Classes during message passing between them through their objects. These events were captured by an Object Instantiator using Event Templates and further given to a Test case scenario generator, which generates the test cases. At the second level, the test cases will be generated based on the event templates that are used for capturing the events generated by the Extend relationship of the Use cases. Further, we have proposed a Genetic Algorithm (GA) for the effective test case prioritization to ensure maximum code coverage. At the third level, effort estimation, we have used the Cyclometric Complexity and Token count to perform the effort estimation for the Software under Test (SUT).",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7915096,,,no,,yes,,yes,no,no,no
621,Multi-objective test report prioritization using image understanding,"In crowdsourced software testing, inspecting the large number of test reports is an overwhelming but inevitable software maintenance task. In recent years, to alleviate this task, many text-based test-report classification and prioritization techniques have been proposed. However in the mobile testing domain, test reports often consist of more screenshots and shorter descriptive text, and thus text-based techniques may be ineffective or inapplicable. The shortage and ambiguity of natural-language text information and the well defined screenshots of activity views within mobile applications motivate our novel technique based on using image understanding for multi-objective test-report prioritization. In this paper, by taking the similarity of screenshots into consideration, we present a multi-objective optimization-based prioritization technique to assist inspections of crowdsourced test reports. In our technique, we employ the Spatial Pyramid Matching (SPM) technique to measure the similarity of the screenshots, and apply the natural-language processing technique to measure the distance between the text of test reports. Furthermore, to validate our technique, an experiment with more than 600 test reports and 2500 images is conducted. The experimental results show that image-understanding techniques can provide benefit to test-report prioritization for most applications.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582758,,,no,,yes,,no,no,,no
636,Test case design based technique for the improvement of test case selection in software maintenance,"Test case selection is the maintenance technique regarding the concept of choosing the appropriate representative of the modified program that has to be changed depending upon the new requirements added for the next modification. The main problem of updating software is the amounts of the test suite that is generated for the tests increases, which can drop the performance of using the new code that added to the previous program. According to this, execution and testing time will increase, including the complexity of integrating the codes also increase. Therefore, the test suite minimization approaches are offered to solve these problems. The objective of proposing the design based technique is to refine the competency of using test case selection to control the large programs by applying four algorithms, which are testing test case, classification, deletion, and selection. The result of determining the size reduction is greater than random selection, general regression technique, and test case prioritization as about 28.00%, 17.00%, and 11.00% approximately.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749202,,,no,,no,no,no,no,,no
637,Test Case Prioritization Approach to Improving the Effectiveness of Fault Localization,"Fault localization aims to use testing information from executed test cases to help locate the fault position. However, obtaining testing information (including test results and coverage information) is expensive because it needs much manual effort. How to orderly choose and execute a small number of test cases, and in the meantime achieve a good effectiveness of fault localization in the case of unknowing testing information is still a challenge for us. In this paper, we propose a new test case prioritization algorithm for fault localization, which is based on the rank changes of suspicious values of program elements. Test case which can maximize the improvements of suspicious ranks of program elements may assign the highest priority for execution. A set of empirical studies have been designed and conducted on Siemens programs and four medium-sized programs. The results show that our algorithm can help reduce the debugging effort in terms of the percentage of statements needed to be inspected to locate faults in both single-fault and multi-fault programs.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780195,,,yes,,yes,,yes,no,no,no
645,Testing and Debugging in Continuous Integration with Budget Quotas on Test Executions,"In Continuous Integration, a software application is developed through a series of development sessions, each with limited time allocated to testing and debugging on each of its modules. Test Case Prioritization can help execute test cases with higher failure estimate earlier in each session. When the testing time is limited, executing such prioritized test cases may only produce partial and prioritized execution coverage data. To identify faulty code, existing Spectrum-Based Fault Localization techniques often use execution coverage data but without the assumption of execution coverage priority. Is it possible to decompose these two steps for optimization within individual steps? In this paper, we study to what extent the selection of test case prioritization techniques may reduce its influence on the effectiveness of spectrum-based fault localization, thereby showing the possibility to decompose the process of continuous integration for optimization in workflow steps. We present a controlled experiment using the Siemens suite as subjects, nine test case prioritization techniques and four spectrum-based fault localization techniques. The findings showed that the studied test cases prioritization and spectrum-based fault localization can be customized separately, and, interestingly, prioritization over a smaller test suite can enable spectrum-based fault localization to achieve higher accuracy by assigning faulty statements with higher ranks.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589824,,,no,,yes,yes,yes,no,no,no
655,Visualization of combinatorial models and test plans,"Combinatorial test design (CTD) is an effective and widely used test design technique. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. One challenge for successful application of CTD in practice relates to this manual model definition and maintenance process. Another challenge relates to the comprehension and use of the test plan generated by CTD for prioritization purposes. In this work we introduce the use of visualizations as a means to address these challenges. We apply three different forms of visualization, matrices, graphs, and treemaps, to visualize the relationships between the different elements of the model, and to visualize the strength of each test in the test plan and the relationships between the different tests in terms of combinatorial coverage. We evaluate our visualizations via a user survey with 19 CTD practitioners, as well as via two industrial projects in which our visualization was used and allowed test designers to get vital insight into their models and into the coverage provided through CTD generated test plans.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582753,,,no,,yes,,no,no,,no
660,A Greedy-Based Method for Modified Condition/Decision Coverage Testing Criterion,"During software regression testing, the code coverage of target program is a crucial factor while we perform test case reduction and prioritization. Modified Condition/ Decision Coverage (MC/DC) is one of the most strict and high-accuracy criterion in code coverage and it is usually considered necessary for adequate testing of critical software. In the past, Hayhurst et al proposed a method to implement the MC/DC criterion that complies with regulatory guidance for DO-178B level A software. Hayhurst's MC/DC approach was to find some test cases which are satisfied by MC/DC criterion for each operator (and, or, not, or xor) in the Boolean expression. However, there could be some problems when using Hayhurst's MC/DC approach to select test cases. In this paper, we discuss how to improve and/or enhance Hayhurst's MC/DC approach by using a greedy-based method. Some experiments are performed based on real programs to evaluate as well as compare the performance of our proposed and Hayhurst's approaches.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8069087,,,no,,yes,,no,no,,no
665,"A Test Case Recommendation Method Based on Morphological Analysis, Clustering and the Mahalanobis-Taguchi Method","This paper focuses on the content of test cases, and categorizes test cases into clusters using the similarity between test cases, their degree of similarity is obtained through a morphological analysis. If there are two similar test cases, they would test the same or similar functionalities in similar but different conditions. Thus, when one of them is run for a regression testing, the remaining one should be run as well, in order to reduce a risk of overlooking regressions. Once a test engineer decides to run a set of test cases, the method proposed in this paper can recommend adding similar test cases to their candidate set. The proposed method also considers the priorities of recommended test cases by using the Mahalanobis-Taguchi method. This paper reports on an empirical study with an industrial software product. The results show that the proposed method is useful to prevent overlooking regressions.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899026,,,yes,,yes,,yes,no,no,no
685,Coverage-Based Reduction of Test Execution Time: Lessons from a Very Large Industrial Project,"There exist several coverage-based approaches to reduce time and resource costs of test execution. While these methods are well-investigated and evaluated for smaller to medium-size projects, we faced several challenges in applying them in the context of a very large industrial software project, namely SAP HANA. These issues include: varying effectiveness of algorithms for test case selection/prioritization, large amounts of shared (non-specific) coverage between different tests, high redundancy of coverage data, and randomness of test results (i.e. flaky tests), as well as of the coverage data (e.g. due to concurrency issues). We address these issues by several approaches. First, our study shows that compared to standard algorithms, so-called overlap-aware solvers can achieve up to 50% higher code coverage in a fixed time budget, significantly increasing the effectiveness of test case prioritization and selection. We also detected in our project high redundancy of line coverage data (up to 97%), providing opportunities for data size reduction. Finally, we show that removal of coverage shared by tests can significantly increase test specificity. Our analysis and approaches can help to narrow the gap between research and practice in context of coverage-based testing approaches, especially in case of very large software projects.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899023,,,no,,yes,yes,no,no,,no
716,Regression Testing of Database Applications Under an Incremental Software Development Setting,"Software regression testing verifies previous features on a software product when it is modified or new features are added to it. Because of the nature of regression testing it is a costly process. Different approaches have been proposed to reduce the costs of this activity, among which are: minimization, prioritization, and selection of test cases. Recently, soft computing techniques, such as data mining, machine learning, and others have been used to make regression testing more efficient and effective. Currently, in different contexts, to a greater or lesser extent, software products have access to databases (DBs). Given this situation, it is necessary to consider regression testing also for software products such as information systems that are usually integrated with or connected to DBs. In this paper, we present a selection regression testing approach that utilizes a combination of unsupervised clustering with random values, unit tests, and the DB schema to determine the test cases related to modifications or new features added to software products connected to DBs. Our proposed approach is empirically evaluated with two database software applications in a production context. Effectiveness metrics, such as test suite reduction, fault detection capability, recall, precision, and the F-measure are examined. Our results suggest that the proposed approach is enough effective with the resulting clusters of test cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8027014,,,no,,no,,no,no,,no
730,Towards Execution Time Prediction for Manual Test Cases from Test Specification,"Knowing the execution time of test cases is important to perform test scheduling, prioritization and progress monitoring. This work in progress paper presents a novel approach for predicting the execution time of test cases based on test specifications and available historical data on previously executed test cases. Our approach works by extracting timing information (measured and maximum execution time)for various steps in manual test cases. This information is then used to estimate the maximum time for test steps that have not previously been executed, but for which textual specifications exist. As part of our approach, natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test activities is already available or not. Finally, linear regression is used to predict the actual execution time for test cases. A proof-of-concept use case at Bombardier Transportation serves to evaluate the proposed approach.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8051381,,,no,,no,yes,no,no,,no
735,A Model-Based Test Case Management Approach for Integrated Sets of Domain-Specific Models,"Due to rapid improvements in the area of embedded processing hardware, the complexity of developed systems constantly increases. In order to ensure a high quality level of such systems, related quality assurance concepts have to evolve. The introduction of Model-Based Testing (MBT) approaches has shown promising results by automating and abstracting multiple activities of the software testing life cycle. Nevertheless, there is a strong need for approaches supporting scoped test models, i.e. subsets of test cases, reflecting specific test purposes driven by risk-oriented development strategies. Therefore, we developed an integrated and model-based approach supporting test case management, which incorporates the beneficial aspects of abstract development methodologies with predominant research for test case management in non-model-based scenarios. Based on a new model artifact, the integration model, tasks like cross-domain information mapping and the integration of domain-specific KPIs derived by analyses favor the subsequently applied constraint-based mechanism for test case management. Further, a prototypical implementation of these concepts within the Architecture And Analysis Framework (A3F) is elaborated and further evaluated based on representative application scenarios. A comparative view on related work leads to a conclusive statement regarding our future work.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411751,,,no,,no,yes,no,no,,no
751,Functional Dependency Detection for Integration Test Cases,"This paper presents a natural language processing (NLP) based approach that, given software requirements specification, allows the functional dependency detection between integration test cases. We analyze a set of internal signals to the implemented modules for detecting dependencies between requirements and thereby identifying dependencies between test cases such that: module 2 depends on module 1 if an output internal signal from module 1 enters as an input internal signal to the module 2. Consequently, all requirements (and thereby test cases) for module 2 are dependent on all the designed requirements (and test cases) for module 1. The dependency information between requirements (and thus corresponding test cases) can be utilized for test case prioritization and scheduling. We have implemented our approach as a tool and the feasibility is evaluated through an industrial use case in the railway domain at Bombardier Transportation (BT), Sweden.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8431975,,,yes,,no,,no,no,,no
767,Practical Test Dependency Detection,"Regression tests should consistently produce the same outcome when executed against the same version of the system under test. Recent studies, however, show a different picture: in many cases simply changing the order in which tests execute is enough to produce different test outcomes. These studies also identify the presence of dependencies between tests as one likely cause of this behavior. Test dependencies affect the quality of tests and of the correlated development activities, like regression test selection, prioritization, and parallelization, which assume that tests are independent. Therefore, developers must promptly identify and resolve problematic test dependencies. This paper presents PRADET, a novel approach for detecting problematic dependencies that is both effective and efficient. PRADET uses a systematic, data-driven process to detect problematic test dependencies significantly faster and more precisely than prior work. PRADET scales to analyze large projects with thousands of tests that existing tools cannot analyze in reasonable amount of time, and found 27 previously unknown dependencies.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367031,,,yes,,no,no,no,no,,no
768,Prioritization of Metamorphic Relations Based on Test Case Execution Properties,"A test oracle is essential for software testing. In certain complex systems, it is hard to distinguish between correct and incorrect behavior. Metamorphic testing is one of the solution to solve the test oracle problem. In metamorphic testing, metamorphic relations (MRs) are derived based on the properties exhibited by the program under test (PUT). These MRs play a major role in the generation of test data for conducting MT. The effectiveness of MRs can be determined based on the ability to detect considerable faults for the given PUT. Many metamorphic relations with different fault finding capability can be used to test the PUT and it is important to identify and prioritize the MRs based on its fault finding effectiveness. In order to answer this challenge, we propose to prioritize the MRs based on the diversity in the execution path of the source and follow-up test cases of the MRs. We propose four metrics to capture different levels of diversity in the execution behavior of the test cases for each of the derived MRs. The total weight calculated for each of the MRs using the metrics is used to prioritize the MRs.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539189,,,no,,yes,no,no,no,,no
771,Redefining Prioritization: Continuous Prioritization for Continuous Integration,"Continuous integration (CI) development environments allow soft-ware engineers to frequently integrate and test their code. While CI environments provide advantages, they also utilize non-trivial amounts of time and resources. To address this issue, researchers have adapted techniques for test case prioritization (TCP) to CI environments. To date, however, the techniques considered have operated on test suites, and have not achieved substantial improvements. Moreover, they can be inappropriate to apply when system build costs are high. In this work we explore an alternative: prioritization of commits. We use a lightweight approach based on test suite failure and execution history that is highly efficient; our approach ""continuously"" prioritizes commits that are waiting for execution in response to the arrival of each new commit and the completion of each previously scheduled commit. We have evaluated our approach on three non-trivial CI data sets. Our results show that our approach can be more effective than prior techniques.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453137,,,yes,,yes,,yes,no,no,no
