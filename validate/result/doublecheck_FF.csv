ID,Document Title,Abstract,Year,PDF Link,code
31,Software testability measurements derived from data flow analysis,"The purpose of the research is to develop formulations to measure the testability of a program. Testability is a program's property which is introduced with the intention of predicting efforts required for testing the program. A program with a high degree of testability indicates that a selected testing criterion could be achieved with less effort and the existing faults can be revealed more easily during testing. We propose a new program normalization strategy that makes the measurement of testability more precise and reasonable. If the program testability metric derived from data flow analysis could be applied at the beginning of a software testing phase, much more effective testing of resource allocation and prioritizing is possible.",1998,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=665760,no
32,Scenario based integration testing for object-oriented software development,"The adaptive use case methodology for software development proposed by Carlson and Hurlbutt [1997, 1998] forms the backdrop for this paper. Their methodology integrates the software design, development and testing processes through a series of design preserving, algorithmic transformations. This paper focuses on the software testing metrics used in the generation of object oriented test plans as part of that methodology. During the design phase, interaction diagrams are developed from which use case action matrices are then generated. A use case action matrix contains a collection of related scenarios each describing a specific variant of an executable sequence of use case actions. The proposed software testing metrics are employed to improve the productivity of the testing process through scenario prioritization.",1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=810764,no
55,Test-suite reduction and prioritization for modified condition/decision coverage,"Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique, and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows, and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software, according to some criterion, as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. The paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of a case study of the test-suite reduction algorithm.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972715,yes
59,A history-based test prioritization technique for regression testing in resource constrained environments,"Regression testing is an expensive and frequently executed maintenance process used to revalidate modified software. To improve it, regression test selection (RTS) techniques strive to lower costs without overly reducing effectiveness by carefully selecting a subset of the test suite. Under certain conditions, some can even guarantee that the selected test cases perform no worse than the original test suite. This ignores certain software development realities such as resource and time constraints that may prevent using RTS techniques as intended (e.g., regression testing must be done overnight, but RTS selection returns two days worth of tests). In practice, testers work around this by prioritizing the test cases and running only those that fit within existing constraints. Unfortunately this generally violates key RTS assumptions, voiding RTS technique guarantees and making regression testing performance unpredictable. Despite this, existing prioritization techniques are memoryless, implicitly assuming that local choices can ensure adequate long run performance. Instead, we propose a new technique that bases prioritization on historical execution data. We conducted an experiment to assess its effects on the long run performance of resource constrained regression testing. Our results expose essential tradeoffs that should be considered when using these techniques over a series of software releases.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007961,yes
62,Elimination of crucial faults by a new selective testing method,"Recent software systems contain a lot of functions to provide various services. According to this tendency, software testing becomes more difficult than before and cost of testing increases so much, since many test items are required. In this paper we propose and discuss such a new selective software testing method that is constructed from previous testing method by simplifying testing specification. We have presented, in the previous work, a selective testing method to perform highly efficient software testing. The selective testing method has introduced an idea of functional priority testing and generated test items according to their functional priorities. Important functions with high priorities are tested in detail, and functions with low priorities are tested less intensively. As a result, additional cost for generating testing instructions becomes relatively high. In this paper in order to reduce its cost, we change the way of giving information, with respect to priorities. The new method gives the priority only rather than generating testing instructions to each test item, which makes the testing method quite simple and results in cost reduction. Except for this change, the new method is essentially the same as the previous method. We applied this new method to actual development of software tool and evaluated its effectiveness. From the result of the application experiment, we confirmed that many crucial faults can be detected by using the proposed method.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166937,yes
73,"Generating, selecting and prioritizing test cases from specifications with tool support","The classification-tree method provides a systematic way for software testers to derive test cases by considering important relevant aspects that are identified from the specification. The method has been used in many real-life applications and shown to be effective. This paper presents several enhancements to the method by annotating the classification tree with additional information to reduce manual effort in the generation, selection and prioritization of test cases. A tool for supporting this enhanced process is also described.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319089,yes
83,An approach to generate the thin-threads from the UML diagrams,"Software testing plays a crucial role in assuring software quality. One of the most important issues in software testing research is the generation of the test cases. For scenario-based software testing, the thin-threads, which are the usage scenarios in a software system from the end user's point of view, are frequently used to generate test cases. However, the generation of the thin-threads is not an easy task. A scenario-based business model has to be manually derived or labor-intensive business analysis has to be manually carried out in order to extract the thin-threads from a software system. In this work, we propose an automated approach to directly generate thin-threads from the UML artifacts. The generated thin-threads can be used to generate and to prioritize the test cases for scenario-based software testing.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342893,no
104,Test factoring: focusing test suites for the task at hand,"Frequent execution of a test suite during software maintenance can catch regression errors early, indicate whether progress is being made, and improve productivity. However, if the test suite takes a long time to produce feedback, the developer is slowed down, and the benefit of frequent testing is reduced. After a program is edited, ideally, only changed code would be tested. Any time spent executing previously tested, unchanged parts of the code is wasted. For a large test suite containing many small unit tests, test selection and prioritization can be effective. Test selection runs only those tests that are possibly affected by the most recent change, and test prioritization can run first the tests that are most likely to reveal a recently-introduced error.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553636,no
131,A Novel Approach of Prioritizing Use Case Scenarios,"Modern softwares are very large and complex. As the size and complexity of software increases, software developers feel an urgent need for a better management of different activities during the course of software development. In this paper, we present an approach of use case scenario prioritization suitable for project planning at an early phase of the software development. We consider only use case model in our work. For prioritization, we focus on how critical a scenario path is, which essentially depends on density of overlapping of sub path of a scenario path with other scenario path(s) of a use case. Our proposed approach provides an analytical solution on use case scenario prioritization and is very much effective in project management related activities as substantiated by our experimental results.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425898,no
134,Applying Interface-Contract Mutation in Regression Testing of Component-Based Software,"Regression testing, which plays an important role in software maintenance, usually relies on test adequacy criteria to select and prioritize test cases. However, with the wide use and reuse of black-box components, such as reusable class libraries and COTS components, it is challenging to establish test adequacy criteria for testing software systems built on components whose source code is not available. Without source code or detailed documents, the misunderstanding between the system integrators and component providers has become a main factor of causing faults in component-based software. In this paper, we apply mutation on interface contracts, which can describe the rights and obligations between component users and providers, to simulate the faults that may occur in this way of software development. The mutation adequacy score for killing the mutants of interface contracts can serve as a test adequacy criterion. We performed an experimental study on three subject systems to evaluate the proposed approach together with four other existing criteria. The experimental results show that our adequacy criterion is helpful for both selecting good-quality test cases and scheduling test cases in an order of exposing faults quickly in regression testing of component-based software.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362630,no
140,Enhancing the Efficiency of Regression Testing through Intelligent Agents,"Software testing is indispensable for all software development. As all mature engineering disciplines need to have systematic testing methodologies, software testing is a very important subject of software engineering. In software development practice, testing accounts for as much as 50% of total development efforts. Testing can be manual, automated, or a combination of both.Manual testing is the process of executing the application and manually interacting with the application, specifying inputs and observing outputs. Manually testing the software is inefficient and costly. It is imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process, which contains many testing related activities using various techniques and methods. In order to automate the process, we have to have some ways to generate oracles from the specification, and generate test cases to test the target software against the oracles to decide their correctness. Today we still don't have a full-scale system that has achieved this goal. In general, significant amount of human intervention is still needed in testing. The degree of automation remains at the automated test script level. This paper therefore provides a timely summary and enhancement of agent theory in software testing ,which motivates recent efforts in adapting concepts and methodologies for agent oriented software testing to complex system which has not previously done. Agent technologies facilitate the automated software testing by virtue of their high level decomposition, independency and parallel activation[4]. Usage of agent based regression testing reduces the complexity involved in prioritizing the testcases.With the ability of agents to act autonomously, monitoring code changes and generating test cases for the changed version of the code can be done dynamically. Agent-Oriented Software testing (AOST) is a nascent but active field of research . A comprehensive methodology that plays an essential role in Software testing must be robust but easy-to use. Moreover, it should provide a roadmap to guide engineers in creating Agent-based systems. The agent based regression testing(ABRT) proposed here is to offer a definition for encompassing to cover the regression testing phenomena based on agents, yet sufficiently tight that it can rule out complex systems that are clearly not agent based.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426561,no
147,Prioritizing Coverage-Oriented Testing Process - An Adaptive-Learning-Based Approach and Case Study,"This paper proposes a graph-model-based approach to prioritizing the test process. Tests are ranked according to their preference degrees which are determined indirectly, i.e., through classifying the events. To construct the groups of events, unsupervised neural network is trained by adaptive competitive learning algorithm. A case study demonstrates and validates the approach.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291124,yes
167,An Empirical Study on Bayesian Network-based Approach for Test Case Prioritization,"A cost effective approach to regression testing is to prioritize test cases from a previous version of a software system for the current release. We have previously introduced a new approach for test case prioritization using Bayesian Networks (BN) which integrates different types of information to estimate the probability of each test case finding bugs. In this paper, we enhance our BN-based approach in two ways. First, we introduce a feedback mechanism and a new change information gathering strategy. Second, a comprehensive empirical study is performed to evaluate the performance of the approach and to identify the effects of using different parameters included in the technique. The study is performed on five open source Java objects. The obtained results show relative advantage of using feedback mechanism for some objects in terms of early fault detection. They also provide insight into costs and benefits of the various parameters used in the approach.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539555,yes
183,Prioritizing User-Session-Based Test Cases for Web Applications Testing,"Web applications have rapidly become a critical part of business for many organizations. However, increased usage of Web applications has not been reciprocated with corresponding increases in reliability. Unique characteristics, such as quick turnaround time, coupled with growing popularity motivate the need for efficient and effective Web application testing strategies. In this paper, we propose several new test suite prioritization strategies for Web applications and examine whether these strategies can improve the rate of fault detection for three Web applications and their preexisting test suites. We prioritize test suites by test lengths, frequency of appearance of request sequences, and systematic coverage of parameter-values and their interactions. Experimental results show that the proposed prioritization criteria often improve the rate of fault detection of the test suites when compared to random ordering of test cases. In general, the best prioritization metrics either (1) consider frequency of appearance of sequences of requests or (2) systematically cover combinations of parameter-values as early as possible.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539541,yes
199,A Hybrid Approach to Build Prioritized Pairwise Interaction Test Suites,"Traditional interaction testing aims to build test suites that cover all t-way interactions of inputs. But in many test scenarios, the entire test suites cannot be fully run due to the limited budget. Therefore it is necessary to take the importance of interactions into account and prioritize these tests of the test suite. In the paper, we use the hybrid approach to build prioritized pairwise interaction test suites (PITS). It adopts a one-test-at-a-time strategy to construct final test suites. But to generate a single test it firstly generates a candidate test and then applies a specific metaheuristic search strategy to enhance this test. Here we experiment four different metaheuristic search strategies. In the experiments, we compare our approach to weighted density algorithm (WDA). Meanwhile, we also analyze the effectiveness of four different search strategies and the effectiveness of the increasing iterations. Empirical results demonstrate the effectiveness of our proposed approach.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365886,yes
201,A Survey of Coverage-Based Testing Tools,"Test coverage is sometimes used to measure how thoroughly software is tested and developers and vendors sometimes use it to indicate their confidence in the readiness of their software. This survey studies and compares 17 coverage-based testing tools primarily focusing on, but not restricted to, coverage measurement. We also survey features such as program prioritization for testing, assistance in debugging, automatic generation of test cases and customization of test reports. Such features make tools more useful and practical, especially for large-scale, commercial software applications. Our initial motivations were both to understand the available test coverage tools and to compare them to a tool that we have developed, called eXVantage (a tool suite that includes code coverage testing, debugging, performance profiling and reporting). Our study shows that each tool has some unique features tailored to its application domains. The readers may use this study to help pick the right coverage testing tools for their needs and environment. This paper is also valuable to those who are new to the practice and the art of software coverage testing, as well as those who want to understand the gap between industry and academia.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8130777,no
205,Building Prioritized Pairwise Interaction Test Suites with Ant Colony Optimization,"Interaction testing offers a stable cost-benefit ratio in identifying faults. But in many testing scenarios, the entire test suite cannot be fully executed due to limited time or cost. In these situations, it is essential to take the importance of interactions into account and prioritize these tests. To tackle this issue, the biased covering array is proposed and the Weighted Density Algorithm (WDA) is developed. To find a better solution, in this paper we adopt ant colony optimization (ACO) to build this prioritized pairwise interaction test suite (PITS). In our research, we propose four concrete test generation algorithms based on Ant System, Ant System with Elitist, Ant Colony System and Max-Min Ant System respectively. We also implement these algorithms and apply them to two typical inputs and report experimental results. The results show the effectiveness of these algorithms.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381411,yes
224,Prioritization of Scenarios Based on UML Activity Diagrams,"Increased size and complexity of software requires better methods for different activities in the software development lifecycle. Quality assurance of software is primarily done by means of testing, an activity that faces constraints of both time and resources. Hence, there is need to test effectively within the constraints in order to maximize throughput i.e. rate of fault detection, coverage, etc. Test case prioritization involves techniques aimed at finding the best prioritized test suite. In this paper, we propose a prioritization technique based on UML activity diagrams. The constructs of an activity diagram are used to prioritize scenarios. Preliminary results obtained on a case-study indicate that the technique is effective in extracting the critical scenarios from the activity diagram.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231936,no
225,Prioritized Test Generation Strategy for Pair-Wise Testing,"Pair-wise testing is widely used to detect faults in software systems. In many applications where pair-wise testing is needed, the whole test set can not be run completely due to time or budget constraints. In these situations, it is essential to prioritize the tests. In this paper, we drive weight for each value of each parameter, and adapt UWA algorithm to generate an ordered pair-wise coverage test suite. UWA algorithm is to accord weights set for each value of each parameter of the system, then produce ordered pair-wise coverage test set for having generated but unordered one. Finally, a greedy algorithm is adopted to prioritize generated pair-wise coverage test set with driven weights, so that whenever the testing is interrupted, interactions deemed, most important are tested.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368244,yes
237,Test Selection Prioritization Strategy,"A wide divergence is observed in projects between test activities planned in the test plan and the actual tests that can be executed. Estimates for test execution computed during the planning are inaccurate without test design. The actual time and resources available are usually less than planned. Assuming that time and resources cannot be changed, a dynamic selection of tests for execution that maximizes quality is required.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254084,no
238,Testing Processes in Business-Critical Chain Software Lifecycle,"The business-critical chain lifecycle is an agile software development lifecycle that aims at aligning the software project deliverables to attain the business objectives based on business priorities. The traditional software development projects work on the assumption that all `equal effort consumers' would be treated equally and worked upon. The agile methodology ensures that the delivery cycles are reduced thus introducing agility in the way business is supported by underlying technologies. The proposed lifecycle model introduces new variables pertaining to the business value generation each finished piece of code would produce. Hence the software project processes have to be modified to cater to it. Even the usual agile lifecycle testing strategies need to be modified to suit the proposed model. The test plans, test estimations, test resource management, quality control, regression plans and automation road map and plans have to be customized to cater to the new life cycle model. The secondary project management activities such as risk management, procurement management, etc also may to be modified with respect to the testing processes. My paper aims at using the critical chain principles and proposes a software lifecycle model that can cater to business priorities and aligning the testing processes not only to development cycles but also to the actual business value created. In this paper I would take a case study and compare and contrast when project uses the regular models and this new model. I would also provide guidelines to use this lifecycle model and modify regular project management activities to cater to the new model with emphasis on the testing processes. The paper would try to provide the ideal scenarios; in project teams, in consulting firms and in new customers and expectations; where such a model could provide high impact on the way consulting companies can do successful projects and creating more value to the customers.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319542,no
243,Visualizing the structure of field testing problems,Field testing of a software application prior to general release is an important and essential quality assurance step. Field testing helps identify unforeseen problems. Extensive field testing leads to the reporting of a large number of problems which often overwhelm the allocated resources. Prior efforts focus primarily on studying the reported problems in isolation. We believe that a global view of the interdependencies between these problems will help in rapid understanding and resolution of reported problems. We present a visualization that highlights the commonalities between reported problems. The visualization helps developers identify two patterns that they can use to prioritize and focus their efforts. We demonstrate the applicability of our visualization through a case study on problems reported during field testing efforts for two releases of a large scale enterprise application.,2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306297,no
254,Constructing Prioritized Interaction Test Suite with Interaction Relationship,"Interaction testing has addressed some issues on how to select a small subset of test cases. In many systems where interaction testing is needed, the entire test suite is not executed because of time or budget constraints. It is important to prioritize the test cases in these situations. On the other hand, there are not always interactions among any factors in real systems. Moreover, some factors may need N-way (N&gt;2) testing since there is a closer relationship among them. We present a model for prioritized interaction testing with interaction relationship and propose a greedy algorithm for generating variable strength covering arrays with bias.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459713,yes
255,Design and Analysis of Cost-Cognizant Test Case Prioritization Using Genetic Algorithm with Test History,"During software development, regression testing is usually used to assure the quality of modified software. The techniques of test case prioritization schedule the test cases for regression testing in an order that attempts to increase the effectiveness in accordance with some performance goal. The most general goal is the rate of fault detection. It assumes all test case costs and fault severities are uniform. However, those factors usually vary. In order to produce a more satisfactory order, the cost-cognizant metric that incorporates varying test case costs and fault severities is proposed. In this paper, we propose a cost-cognizant test case prioritization technique based on the use of historical records and a genetic algorithm. We run a controlled experiment to evaluate the proposed technique's effectiveness. Experimental results indicate that our proposed technique frequently yields a higher Average Percentage of Faults Detected per Cost (APFDc). The results also show that our proposed technique is also useful in terms of APFDc when all test case costs and fault severities are uniform.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676289,yes
256,Efficient Reduction of Model-Based Generated Test Suites through Test Case Pair Prioritization,"During the development and maintenance of software, test suites often reach a size that exceeds the costs allocated for test suite execution. In such a case, the test suite needs to be reduced. Many papers are dedicated to the problem of test suite reduction. Most of them consider the removal or merging of test cases. However, less attention has been paid to the identification of test case pairs, which are eminently suitable for merging. In this paper, we fill this gap by presenting a novel approach that helps identifying those test case pairs within a given set of systematically generated test cases which, when merged, have potential for high test suite reduction. As a result, test suites reduced by our approach are considerably smaller in size than those, whose pairs are selected randomly.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772249,yes
266,Prioritization of test case scenarios derived from activity diagram using genetic algorithm,"Software testing involves identifying the test cases which discovers the errors in the program. However, the exhaustive testing is rarely impossible and very time consuming. In this paper, the software testing efficiency is optimized by identifying the critical path clusters. The test case scenarios are derived from the activity diagram and the testing efficiency is optimized by applying the genetic algorithm on the test data. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of activity diagram.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640479,yes
270,Prioritizing Unit Test Creation for Test-Driven Maintenance of Legacy Systems,"Test-Driven Development (TDD) is a software development practice that prescribes writing unit tests before writing implementation code. Recent studies have shown that TDD practices can significantly reduce the number of pre-release defects. However, most TDD research thus far has focused on new development. We investigate the adaptation of TDD-like practices for already implemented code, in particular legacy systems. We call this adaptation of TDD-like practices for already implemented code ``Test-Driven Maintenance'' (TDM). In this paper, we present an approach that assists software development and testing managers, who employ TDM, utilize the limited resources they have for testing legacy systems efficiently. The approach leverages the development history of the project to generate a prioritized list of functions that managers should focus their unit test writing resources on. The list is updated dynamically as the development of the legacy system progresses. To evaluate our approach, we conduct a case study on a large commercial legacy software system. Our findings suggest that heuristics based on the function size, modification frequency and bug fixing frequency should be used to prioritize the unit test writing of legacy systems.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562952,yes
273,Regression Test Generation Approach Based on Tree-Structured Analysis,"Regression test generation is an important process to make sure that changes of program have no unintended side-effects. To achieve full confidence, many projects have to re-run all the test cases for entire program, which makes it a time consuming and expensive activity. In this paper, a code based regression testing approach is proposed to generate selected test suites for unit testing. The framework contains five phases: program change detection phase, logical verification phase, branch pruning phase, test case prioritization phase and test suite generation phase. These five phases can achieve detection of program's modification, coding standard, test case pruning, test case prioritizing and inputs generation for regression test cases respectively. A prototype based on this framework is implemented using logical tree-structured analysis, and the preliminary experiment shows that proposed approach can provide efficient regression test suites.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476640,no
281,Taking Advantage of Service Selection: A Study on the Testing of Location-Based Web Services Through Test Case Prioritization,"Dynamic service compositions pose new verification and validation challenges such as uncertainty in service membership. Moreover, applying an entire test suite to loosely coupled services one after another in the same composition can be too rigid and restrictive. In this paper, we investigate the impact of service selection on service-centric testing techniques. Specifically, we propose to incorporate service selection in executing a test suite and develop a suite of metrics and test case prioritization techniques for the testing of location-aware services. A case study shows that a test case prioritization technique that incorporates service selection can outperform their traditional counterpart - the impact of service selection is noticeable on software engineering techniques in general and on test case prioritization techniques in particular. Further-more, we find that points-of-interest-aware techniques can be significantly more effective than input-guided techniques in terms of the number of invocations required to expose the first failure of a service composition.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552784,yes
283,The Effectiveness of Regression Testing Techniques in Reducing the Occurrence of Residual Defects,"Regression testing is a necessary maintenance activity that can ensure high quality of the modified software system, and a great deal of research on regression testing has been performed. Most of the studies performed to date, however, have evaluated regression testing techniques under the limited context, such as a short-term assessment, which do not fully account for system evolution or industrial circumstances. One important issue associated with a system lifetime view that we have overlooked in past years is the effects of residual defects - defects that persist undetected - across several releases of a system. Depending on an organization's business goals and the type of system being built, residual defects might affect the level of success of the software products. In this paper, we conducted an empirical study to investigate whether regression testing techniques are effective in reducing the occurrence and persistence of residual defects across a system's lifetime, in particular, considering test case prioritization techniques. Our results show that heuristics can be effective in reducing both the occurrence of residual defects and their age. Our results also indicate that residual defects and their age have a strong impact on the cost-benefits of test case prioritization techniques.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477096,no
287,Using Coverage Information to Guide Test Case Selection in Adaptive Random Testing,"Random Testing (RT) is a fundamental software testing technique. Adaptive Random Testing (ART) improves the fault-detection capability of RT by employing the location information of previously executed test cases. Compared with RT, test cases generated in ART are more evenly spread across the input domain. ART has conventionally been applied to programs that have only numerical input types, because the distance between numerical inputs is readily measurable. The vast majority of computer programs, however, involve non-numerical inputs. To apply ART to these programs requires the development of effective new distance measures. Different from those measures that focus on the concrete values of program inputs, in this paper we propose a method to measure the distance using coverage information. The proposed method enables ART to be applied to all kinds of programs regardless of their input types. Empirical studies are further conducted for the branch coverage Manhattan distance measure using the replace and space programs. Experimental results show that, compared with RT, the proposed method significantly reduces the number of test cases required to detect the first failure. This method can be directly applied to prioritize regression test cases, and can also be incorporated into code-based and model-based test case generation tools.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615787,no
294,A genetic algorithm based approach for prioritization of test case scenarios in static testing,"White box testing is a test technique that takes into account program code, code structure and internal design flow. White box testing is primarily of two kinds-static and structural. Whereas static testing requires only the source code of the product, not the binaries or executables, in structural testing tests are actually run by the computer on built products. In this paper, we propose a technique for optimizing static testing efficiency by identifying the critical path clusters using genetic algorithm. The testing efficiency is optimized by applying the genetic algorithm on the test data. The test case scenarios are derived from the source code. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of the control flow graph generated from the source code. This research paper is an extension of our previous research paper [18].",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075160,yes
297,A software safety test approach based on FTA and Bayesian networks,"As an important way to verify software safety, software safety test has caught more attentions in practice. However, it is still an open question that how engineers could make software safety test more efficient. Currently, FTA based method is one of the approaches in software safety test, but it can not utilize the finished software test results, and can not be determined the priorities of all the use cases. In order to solve these problems, this paper gives a quantitative approach of software safety test based on FTA and Bayesian networks. In the approach, top-level events of fault trees are identified from system hazards firstly. Then, fault trees are built using FTA and transferred into Bayesian networks. Finally, test cases of software safety test are determined by the Bayesian networks. Besides, the paper also shows an example using the approach, which could guide software engineers to make software safety test more efficient. The example shows that the approach could take advantage of Bayesian Theorem and FTA methodology together, and give reasonable priorities of use cases in software safety test.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939497,no
299,A Test Case Design Algorithm Based on Priority Techniques,"Testing is an important step of building e-commerce system. In regression testing, it is the key issue that how to reuse the test suite efficiently. This paper presents a dynamic adjustment prioritization based on the design information of test suite which is a new exploration of regression test prioritization. It improves the shortcoming of the existing technologies which failed to use the design information of test cases effectively. It adjusts the priority of test case by collecting running information, gradually optimizes the test suite and makes the test suite adapted to the current test environment to obtain a better error detection results. Experiments show the error-detected efficiency of it has certain advantages than the existing algorithms.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092632,yes
302,Adaptive Regression Testing Strategy: An Empirical Study,"When software systems evolve, different amounts and types of code modifications can be involved in different versions. These factors can affect the costs and benefits of regression testing techniques in different ways, and thus, there may be no single regression testing technique that is the most cost-effective technique to use on every version. To date, many regression testing techniques have been proposed, but no research has been done on the problem of helping practitioners systematically choose appropriate techniques on new versions as systems evolve. To address this problem, we propose adaptive regression testing (ART) strategies that attempt to identify the regression testing techniques that will be the most cost-effective for each regression testing session considering organization's situations and testing environment. To assess our approach, we conducted an experiment focusing on test case prioritization techniques. Our results show that prioritization techniques selected by our approach can be more cost-effective than those used by the control approaches.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132961,yes
304,An Empirical Study on the Relation between Dependency Neighborhoods and Failures,"Changing source code in large software systems is complex and requires a good understanding of dependencies between software components. Modification to components with little regard to dependencies may have an adverse impact on the quality of the latter, i.e., increase their risk to fail. We conduct an empirical study to understand the relationship between the quality of components and the characteristics of their dependencies such as their frequency of change, their complexity, number of past failures and the like. Our study has been conducted on two large software systems: Microsoft VISTA and ECLIPSE. Our results show that components that have outgoing dependencies to components with higher object-oriented complexity tend to have fewer field failures for VISTA, but the opposite relation holds for ECLIPSE. Likewise, other notable observations have been made through our study that (a) confirm that certain characteristics of components increase the risk of their dependencies to fail and (b) some of the characteristics are project specific while some were also found to be common. We expect that such results can be leveraged for use to provide new directions for research in defect prediction, test prioritization and related research fields that utilize code dependencies in their empirical analysis. Additionally, these results provide insights to engineers on the potential reliability impacts of new component dependencies based upon the characteristics of the component.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770624,no
305,An empirical validation of FindBugs issues related to defects,"Background: Effective use of bug finding tools promise to speed up the process of source code verification and to move a portion of discovered defects from testing to coding phase. However, many problems related to their usage, especially the large number of false positives, could easily hinder the potential benefits of such tools. Aims: Assess the percentage and type of issues of a popular bugfinding tool (FindBugs) that are actual defects. Method: We analyzed 301 Java Projects developed at a university with FindBugs, collecting the issues signalled on the source code. Afterwards, we checked the precision of issues with information on changes, we ranked and validated them using both manual inspection and validation with tests failures. Results: We observed that a limited set of issues have high precision and conversely we identified those issues characterized by low precision. We compared findings first with our previous experiment and then to related work: results are consistent with both of them. Conclusions: Since our and other empirical studies demonstrated that few issues are related to real defects with high precision, developers could enable only them (or prioritize), reducing the information overload of FindBugs and having the possibility to discover defects earlier. Furthermore, the technique presented in the paper can be adopted to other tools on a code base with tests to find issues with high precision that can be checked on code in production to find defects earlier.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083173,no
310,"Challenges, benefits and opportunities in operating cabled ocean observatories: Perspectives from NEPTUNE Canada","The advent of the first cabled ocean observatories, with several others being planned, demonstrates the challenges, benefits and opportunities for ocean science and commercial applications. Examples are drawn primarily from NEPTUNE Canada (NC), which completed installation of the subsea infrastructure and 60 diverse instruments in 2009, with 40 more in 2010, thereby establishing the world's first regional cabled ocean observatory, northeast Pacific Ocean, off British Columbia's coast. Initial data flow started in December 2009. Another 30 instruments will be deployed in 2011-12. Introducing abundant power and high bandwidth communications into a range of ocean environments allows discrimination between short and long-term events, interactive experiments, real time data and imagery, and complex multidisciplinary teams interrogating a vast database over the observatory's 25-year design life. Scientific priorities and observatory node sites were identified through workshops. Alcatel-Lucent Submarine Networks designed, manufactured and installed the 800km backbone cable and five nodes (stepping 10kV DC to 400V DC). Node sites are located at the coast (Folger Passage), continental slope (ODP 889; Barkley Canyon), abyssal plain (ODP 1027), and ocean-spreading ridge (Endeavour), in water depths of 100-2660m. Principal scientific themes are: plate tectonic processes and earthquake dynamics; dynamic processes of seabed fluid fluxes and gas hydrates; regional ocean/climate dynamics and effects on marine biota; deep-sea ecosystem dynamics; and engineering and computational research. The Data Management and Archive System (DMAS) provides controls for the observatory network and transparent access to other data providers using interoperability techniques within a Web 2.0 environment. Users can perform data visualization and analysis on-line with either default or custom processing code, as well as simultaneously interacting with each other. Oceans 2.0 is adding tools to perform software-aided feature detection and classification of sounds in acoustic data streams. New knowledge and scientific interpretations are addressing important science applications of the observatory: ocean/climate change, ocean acidification, recognizing and mitigating natural hazards, non-renewable and renewable natural resources. Challenges are considerable: technical innovations, enlarging the user base, management, funding, maximizing educational/outreach activities. Socio-economic benefits are substantial: not only the transformation of ocean sciences but with many applications in sectors such as sovereignty, security, transportation, data services, and public policy. Opportunities for commercialization of technologies and data services/products are being facilitated by the Centre of Enterprise and Engagement (www.onccee.ca) within Ocean Networks Canada (www.networkscanada.ca) that manages the NC and VENUS observatories (www.neptunecanada.ca; www.uvic.venus.ca). Cabled ocean observatories are transforming the ocean sciences and will result in a progressive wiring of the oceans. They are designed to be expandable in footprint, nodes and instruments, and the range of scientific questions, and to provide facilities for testing technology prototypes. They will provide a wealth of new research opportunities and socio-economic benefits.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5774134,no
311,Change Sensitivity Based Prioritization for Audit Testing of Webservice Compositions,"Modern software systems have often the form of Web service compositions. They take advantage of the availability of a variety of external Web services to provide rich and complex functionalities, obtained as the integration of external services. However, Web services change at a fast pace and while syntactic changes are easily detected as interface incompatibilities, other more subtle changes are harder to detect and may give raise to faults. They occur when the interface is compatible with the composition, but the semantics of the service response has changed. This typically involves undocumented or implicit aspects of the service interface. Audit testing of services is the process by which the service integrator makes sure that the service composition continues to work properly with the new versions of the integrated services. Audit testing of services is conducted under strict (sometimes extreme) time and budget constraints. Hence, prioritizing the audit test cases so as to execute the most important ones first becomes of fundamental importance. We propose a test case prioritization method specifically tailored for audit testing of services. Our method is based on the idea that the most important test cases are those that have the highest sensitivity to changes injected into the service responses (mutations). In particular, we consider only changes that do not violate the explicit contract with the service (i.e., the WSDL), but may violate the implicit assumptions made by the service integrator.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954434,yes
312,Code Hot Spot: A tool for extraction and analysis of code change history,"Commercial software development teams have limited time available to focus on improvements to their software. These teams need a way to quickly identify areas of the source code that would benefit from improvement, as well as quantifiable data to defend the selected improvements to management. Past research has shown that mining configuration management systems for change information can be useful in determining faulty areas of the code. We present a tool named Code Hot Spot, which mines change records out of Microsoft's TFS configuration management system and creates a report of hot spots. Hot spots are contiguous areas of the code that have higher values of metrics that are indicators of faulty code. We present a study where we use this tool to study projects at ABB to determine areas that need improvement. The resulting data have been used to prioritize areas for additional code reviews and unit testing, as well as identifying change prone areas in need of refactoring.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080806,no
314,Cost Optimizations in Runtime Testing and Diagnosis of Systems of Systems,"In practically all development processes tests are used to detect the presence of faults. This is not an exception for critical and high-availability systems. However, these systems cannot be taken offline or duplicated for testing in some cases. This makes runtime testing necessary. This paper presents work aimed at optimizing the three main sources of testing cost: preparation, execution and diagnosis. First, preparation cost is optimized by defining a metric of the runtime testability of the system, used to elaborate an implementation plan of preparative work for runtime testing. Second, the interrelated nature of test execution cost and diagnostic cost is highlighted and a new diagnostic test prioritization is introduced.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770638,no
317,Critical components identification and verification for effective software test prioritization,"Nowadays, software complexity increases as the number of components in a component based system (CBS) increases. As the complexity level increases, the testing and verification of components also increases. This in turn rose up the testing time and cost which thus made industries to skip off some of the components due to the hard timeline and resource limitations especially during maintenance. This leads to hazardous effects if some of these missed components are critical in term of their core functionality and dependability with other components. Hence, a regression testing which is usually performed during maintenance phase should be developed meticulously to identify and test these critical components rigorously before releasing the software on to the customer side. This paper proposed a novel regression testing method based on the criticality measure calculated by means of dependability metrics and internal complexity metrics. Also, this paper compares the performance of the proposed approach with existing approaches and concluded that the proposed framework outperforms them.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165171,no
318,Dealing with Test Automation Debt at Microsoft,"At Microsoft, substantial time and resources are expended in test case development, execution and verification. Thousands of new tests are added to existing test suites without any kind of review regarding their unique contribution to test suite effectiveness or impact on test suite efficiency. This talk describes how we leverage existing code coverage data, together with reduction and prioritization techniques, to help each test team analyze its test suite and guide them in improving their suite's effectiveness and efficiency. The analysis focuses on identifying and deprecating/prioritizing groups of tests cases, given specific tactical goals for example, increasing current test suite stability and reliability, better structuring of test suite migration efforts, reducing test suite execution time and testing with limited hardware resources.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032226,yes
322,Developing a Single Model and Test Prioritization Strategies for Event-Driven Software,"Event-Driven Software (EDS) can change state based on incoming events; common examples are GUI and Web applications. These EDSs pose a challenge to testing because there are a large number of possible event sequences that users can invoke through a user interface. While valuable contributions have been made for testing these two subclasses of EDS, such efforts have been disjoint. This work provides the first single model that is generic enough to study GUI and Web applications together. In this paper, we use the model to define generic prioritization criteria that are applicable to both GUI and Web applications. Our ultimate goal is to evolve the model and use it to develop a unified theory of how all EDS should be tested. An empirical study reveals that the GUI and Web-based applications, when recast using the new model, show similar behavior. For example, a criterion that gives priority to all pairs of event interactions did well for GUI and Web applications; another criterion that gives priority to the smallest number of parameter value settings did poorly for both. These results reinforce our belief that these two subclasses of applications should be modeled and studied together.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401169,yes
327,GeTeX: A Tool for Testing Real-Time Embedded Systems Using CAN Applications,"Real-Time Embedded Systems (RTES) have an increasing role in controlling the IT that we use on a day-to-day basis. The behaviour of an RTES is not based solely on the interactions it might have with its surrounding environment, but also on the timing requirements it induces. As a result, ensuring that an RTES behaves correctly is non-trivial, especially after adding time as a new dimension to the complexity of the testing process. In previous research, we introduced a 'priority-based' approach which tested the logical and timing behaviour of an RTES modeled formally as UPPAAL Timed Automata (UTA). The 'priority-based' approach was based on producing sets of timed test traces by achieving timing constraints coverage according to three sets of priorities, namely boundary, out-boundary and in-boundary. In this paper, we introduce a new testing tool 'GeTeX' that deploys the ""priority-based"" testing approach. GeTeX is a complete testing tool which generates timed test-cases from UTA models and executes them on the System Under Test (SUT) to identify faults. In its current version, GeTeX supports Control Area Network (CAN) applications.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5934805,yes
330,Hybrid regression testing technique: A multi layered approach,Software needs to be delivered well in time and within budgets. One way of doing this is performing incremental delivery of the software with each increment being adding new features along with changes requests. This incremental delivery is supported with requirement prioritization and needs to be tested for checking the reliability and quality. Testing of this increment calls for testing of not only of old newly added functionality but also of existing features so as to make sure that old parts that works perfectly well do not malfunctions after new code is added. Thus a new hybrid technique is proposed in this paper that clusters the test cases and prioritizes the clusters on basis of priorities of requirements represented by the clusters and series of selections and prioritizations at levels of test cases reduces the number of test cases to manageable level and execution of these test cases guarantees the testing of highest priority requirements associated with statements that are often associated with failures or has highest number of parents or sibling's statements dependent on it and is likely to be influenced by changes or failures in this statement.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139363,yes
333,Improving Regression Testing Transparency and Efficiency with History-Based Prioritization -- An Industrial Case Study,"Background: History based regression testing was proposed as a basis for automating regression test selection, for the purpose of improving transparency and test efficiency, at the function test level in a large scale software development organization. Aim: The study aims at investigating the current manual regression testing process as well as adopting, implementing and evaluating the effect of the proposed method. Method: A case study was launched including: identification of important factors for prioritization and selection of test cases, implementation of the method, and a quantitative and qualitative evaluation. Results: 10 different factors, of which two are history-based, are identified as important for selection. Most of the information needed is available in the test management and error reporting systems while some is embedded in the process. Transparency is increased through a semi-automated method. Our quantitative evaluation indicates a possibility to improve efficiency, while the qualitative evaluation supports the general principles of history-based testing but suggests changes in implementation details.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770626,yes
335,Increasing test coverage using human-based approach of fault injection testing,"Fault injection testing (FIT) approach validates system's fault tolerance mechanism by actively injecting software faults into the targeted areas in the system in order to accelerate its failure rate. This highly complements other testing approaches such as requirements and regression testing implemented during the same testing phase. During testing, it is impossible to run all possible test scenarios. It is especially difficult to predict how the user might use the system functionality correctly as per design. The human interaction through the system may be varies and will leads to the functionality loophole. It is therefore important to have strategic testing approach for evaluating the dependability of computer systems especially in human errors. This paper proposed on applying Knowledge-Based, Fault Prediction Model and Test Case Prioritization approaches that can be combined to increase the test coverage. The goal of this paper is to highlight the needs and advantages of the selected approaches in performing FIT as one of effective testing techniques in the ongoing quest for increased software quality.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140685,no
338,Making the Case for MORTO: Multi Objective Regression Test Optimization,This paper argues that regression test optimization problems such as selection and prioritization require multi objective optimization in order to adequately cater for real world regression testing scenarios. The paper presents several examples of costs and values that could be incorporated into such a Multi Objective Regression Test Optimization (MORTO) approach.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954399,no
339,Modeling the Diagnostic Efficiency of Regression Test Suites,"Diagnostic performance, measured in terms of the manual effort developers have to spend after faults are detected, is not the only important quality of a diagnosis. Efficiency, i.e., the number of tests and the rate of convergence to the final diagnosis is a very important quality of a diagnosis as well. In this paper we present an analytical model and a simulation model to predict the diagnostic efficiency of test suites when prioritized with the information gain algorithm. We show that, besides the size of the system itself, an optimal coverage density and uniform coverage distribution are needed to achieve an efficient diagnosis. Our models allow us to decide whether using IG with our current test suite will provide a good diagnostic efficiency, and enable us to define criteria for the generation or improvement of test suites.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954476,no
340,On Practical Adequate Test Suites for Integrated Test Case Prioritization and Fault Localization,"An effective integration between testing and debugging should address how well testing and fault localization can work together productively. In this paper, we report an empirical study on the effectiveness of using adequate test suites for fault localization. We also investigate the integration of test case prioritization and statistical fault localization with a postmortem analysis approach. Our results on 16 test case prioritization techniques and four statistical fault localization techniques show that, although much advancement has been made in the last decade, test adequacy criteria are still insufficient in supporting effective fault localization. We also find that the use of branch-adequate test suites is more likely than statement-adequate test suites in the effective support of statistical fault localization.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004308,yes
352,"Security Requirements Analysis, Specification, Prioritization and Policy Development in Cyber-Physical Systems","In recent past, the security of cyber-physical systems (CPSs) has been the subject of major concern. One of the reasons is that, CPSs are often applied to mission-critical processes. Also, the automation CPSs bring in managing physical processes, and the detail of information available to them for carrying out their tasks, make securing them a prime importance. Securing CPSs is a difficult task as systems are interconnected. In order to achieve a continuous secured CPS environment, there is the need for an integrated methodology to analyze, specify and prioritize security requirements and also to develop policies to meet them. First, CPS assets are represented using high-order object models. Second, swim lane diagrams are extended to include malactivities and prevention or mitigation options to decompose use cases. We analyze security threats pertaining to the hardware components, software components and the hardware-software interaction. Security requirements are then specified, and an analytical prioritization approach, based on relative priority analysis is employed to prioritize them. Finally, security policies are then developed to meet the requirements. To demonstrate its effectiveness and evaluate its application, the proposed methodology is applied in a structured approach to a test bed - Ayushman, a Pervasive Health Monitoring System (PHMS).",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004511,no
353,Test Case Generation and Prioritization from UML Models,"This paper proposes a novel approach to generating test cases from UML 2.0 activity diagrams and prioritizing those test cases using model information encapsulated in the activity diagrams. The test cases generated according to our approach are suitable for system level testing of the application. For prioritization of test cases, we propose a method based on coverage of all transitions in the activity diagram and usage probability of a particular flow in the activity model. We also propose an approach for selecting test data based on analysis of the branch conditions of the decision nodes in the activity diagrams.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734956,yes
370,Case based reasoning approach for adaptive test suite optimization,"Case-based reasoning is an approach to problem solving and learning that has got a lot of attention over the last few years. This paper provides an overview of the foundational issues related to case-based reasoning, describing some of the leading methodological approaches within the field, and exemplifying the current state through pointers to some systems. The framework influences the recent methodologies for knowledge level descriptions of intelligent systems. The methods for case retrieval reuse, solution testing, and learning are summarized, and realization is discussed with few example systems that represent different CBR approaches. Regression testing occurs during the maintenance stage of the software life cycle, however, it requires large amounts of test cases to assure the attainment of a certain degree of quality. So, test suite sizes may grow significantly. This paper focuses primarily on application of CBR to test suite optimization.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395870,no
372,Dependency-Based Test Case Selection and Prioritization in Embedded Systems,"Embedded systems in automotive engineering are getting more and more complex due to a higher rate of integration and shared usage of sensor signals. A common solution to testing these systems is deriving test cases from models, so called model-based testing. In practice, generated test suites are typically very huge and have to be reduced by methods of regression-test selection and prioritization. In our field of application, we additionally suffer from the lack of knowledge on system internals like the source code. Therefore, our approach is based on dependences between the components of embedded systems. The model we use is derived from the system architecture and system requirements. We are using graph algorithms for selecting and prioritizing test cases to run on a certain system version. First statistical evaluations and the current implementation already have shown promising reduction of test-cases for regression testing.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200176,yes
376,G-RankTest: Regression testing of controller applications,"Since controller applications must typically satisfy real-time constraints while manipulating real-world variables, their implementation often results in programs that run extremely fast and manipulate numerical inputs and outputs. These characteristics make them particularly suitable for test case generation. In fact a number of test cases can be easily created, due to the simplicity of numerical inputs, and executed, due to the speed of computations. In this paper we present G-RankTest, a technique for test case generation and prioritization. The key idea is that test case generation can run for long sessions (e.g., days) to accurately sample the behavior of a controller application and then the generated test cases can be prioritized according to different strategies, and used for regression testing every time the application is modified. In this work we investigate the feasibility of using the gradient of the output as a criterion for selecting the test cases that activate the most tricky behaviors, which we expect easier to break when a change occurs, and thus deserve priority in regression testing.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228981,no
382,Log-based approach for performance requirements elicitation and prioritization,"Requirements engineering activities are a critical part of a project's lifecycle. Success of subsequent project phases is highly dependent on good requirements definition. However, eliciting and achieving consensus on priority between all stakeholders is a complex task. Considering software development of large scale global applications, the challenges increase by the need of managing discussions between groups of stakeholders with different roles and background. This paper presents a practical approach for requirements elicitation and prioritization based on realistic user behaviors observation. It uses basic statistic analysis and application usage information to automatically identify the most relevant requirements for majority of stakeholders. An industry case illustrates the feasibility and efficiency of our approach.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345818,no
389,Predicting the priority of a reported bug using machine learning techniques and cross project validation,"In bug repositories, we receive a large number of bug reports on daily basis. Managing such a large repository is a challenging job. Priority of a bug tells that how important and urgent it is for us to fix. Priority of a bug can be classified into 5 levels from PI to P5 where PI is the highest and P5 is the lowest priority. Correct prioritization of bugs helps in bug fix scheduling/assignment and resource allocation. Failure of this will result in delay of resolving important bugs. This requires a bug prediction system which can predict the priority of a newly reported bug. Cross project validation is also an important concern in empirical software engineering where we train classifier on one project and test it for prediction on other projects. In the available literature, we found very few papers for bug priority prediction and none of them dealt with cross project validation. In this paper, we have evaluated the performance of different machine learning techniques namely Support Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbors (KNN) and Neural Network (NNet) in predicting the priority of the newly coming reports on the basis of different performance measures. We performed cross project validation for 76 cases of five data sets of open office and eclipse projects. The accuracy of different machine learning techniques in predicting the priority of a reported bug within and across project is found above 70% except Naive Bayes technique.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416595,no
390,Preemptive Regression Test Scheduling Strategies: A New Testing Approach to Thriving on the Volatile Service Environments,"A workflow-based web service may use ultra-late binding to invoke external web services to concretize its implementation at run time. Nonetheless, such external services or the availability of recently used external services may evolve without prior notification, dynamically triggering the workflow-based service to bind to new replacement external services to continue the current execution. Any integration mismatch may cause a failure. In this paper, we propose Preemptive Regression Testing (PRT), a novel testing approach that addresses this adaptive issue. Whenever such a late-change on the service under regression test is detected, PRT preempts the currently executed regression test suite, searches for additional test cases as fixes, runs these fixes, and then resumes the execution of the regression test suite from the preemption point.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340125,no
397,Size-Constrained Regression Test Case Selection Using Multicriteria Optimization,"To ensure that a modified software system has not regressed, one approach is to rerun existing test cases. However, this is a potentially costly task. To mitigate the costs, the testing effort can be optimized by executing only a selected subset of the test cases that are believed to have a better chance of revealing faults. This paper proposes a novel approach for selecting and ordering a predetermined number of test cases from an existing test suite. Our approach forms an Integer Linear Programming problem using two different coverage-based criteria, and uses constraint relaxation to find many close-to-optimal solution points. These points are then combined to obtain a final solution using a voting mechanism. The selected subset of test cases is then prioritized using a greedy algorithm that maximizes minimum coverage in an iterative manner. The proposed approach has been empirically evaluated and the results show significant improvements over existing approaches for some cases and comparable results for the rest. Moreover, our approach provides more consistency compared to existing approaches.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928351,no
398,Software testing suite prioritization using multi-criteria fitness function,"Regression testing is the process of validating modifications introduced in a system during software maintenance. It is an expensive, yet an important process. As the test suite size is very large, system retesting consumes large amount of time and computing resources. Unfortunately, there may be insufficient resources to allow for the re-execution of all test cases during regression testing. Testcase prioritization techniques aim to improve the effectiveness of regression testing, by ordering the testcases so that the most beneficial are executed first with higher priority. The objective of test case prioritization is to detect faults as early as possible. An approach for automating the test case prioritization process using genetic algorithm with Multi-Criteria Fitness function is presented. It uses multiple control flow coverage metrics. These metrics measure the degree of coverage of conditions, multiple conditions and statements that the test case covers. Theses metrics are weighted by the number of faults revealed and their severity. The proposed Multi-criteria technique showed superior results compared to similar work.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523563,yes
406,Value-Based Coverage Measurement in Requirements-Based Testing: Lessons Learned from an Approach Implemented in the TOSCA Testsuite,"Testing is one of the most widely practiced quality assurance measures and also one of the most resource-intensive activities in software development. Still, however, most of the available methods, techniques and tools for software testing are value-neutral and do not realize the potential value contribution of testing. In this paper we present an approach for value-based coverage measurement that can be used to align the testing effort with the achievable value associated with requirements and functional units. It has been implemented as part of a commercial test tool and was successfully applied in real-world projects. The results demonstrated its ability to adequately capture the distribution of the business value and risks involved in different requirements. The paper concludes with sharing important lessons learned from developing value-based coverage measurement in the practical setting of commercial tool development and real-world test projects.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328176,no
422,Developing new Automatic Test Equipments (ATE) using systematic design approaches,"Keeping Automatic Test Equipments (ATE) current with technology is one of the major challenges in automatic testing world. Needs and priorities can quickly evolve throughout the life cycle of ATEs and handling obsolescence via performing upgrades on hardware and software can be impossible after several years. While developing Test Program Sets (TPS), if existing ATE systems cannot meet the necessary requirements without inserting extra test devices or decreasing test coverage, then designing a new ATE can be seen inevitable. If a new ATE system is to be designed, it is very crucial that the requirements for the new ATE system should be identified before design process begins. Determining the requirements is a very critical stage in designing ATE because if enough effort is not focused and extended analysis is not carried out on determining the requirements, then the newly formed ATE system will likely fail to cover the test requirements of the DUTs. Setting up the hardware and software architecture is the next stage after the process of determining the requirements of the ATE system. Practical and cost effective solutions should be considered without compromising performance and capabilities of the test devices. The architectures should be suitable for future enhancements to the system. Throughout the design process, the design requirements, critical design descriptions, verification and validation procedures should be clearly documented and reviewed with relevant engineers. In this paper, the design process of a new ATE system by using systematic design approach is discussed. This process is followed during the design of the ATE system which is under use from the beginning of the year 2013. The challenges in the design process, determining the requirements and the formation of hardware and software architecture are explained in detail benefiting from real experiences.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645035,no
423,Efficient Automated Program Repair through Fault-Recorded Testing Prioritization,"Most techniques for automated program repair use test cases to validate the effectiveness of the produced patches. The validation process can be time-consuming especially when the object programs ship with either lots of test cases or some long-running test cases. To alleviate the cost for testing, we first introduce regression test prioritization insight into the area of automated program repair, and present a novel prioritization technique called FRTP with the goal of reducing the number of test case executions in the repair process. Unlike most existing prioritization techniques frequently requiring additional cost for gathering previous test executions information, FRTP iteratively extracts that information just from the repair process, and thus incurs trivial performance lose. We also built a tool called TrpAutoRepair, which implements our FRTP technique and has the ability of automatically repairing C programs. To evaluate TrpAutoRepair, we compared it with GenProg, a state-of-the-art tool for automated C program repair. The experiment on the 5 subject programs with 16 real-life bugs provides evidence that TrpAutoRepair performs at least as good as GenProg in term of success rate, in most cases (15/16), TrpAutoRepair can significantly improve the repair efficiency by reducing efficiently the test case executions when searching a valid patch in the repair process.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676889,no
429,On Combining Model-Based Analysis and Testing,"Testing a computer system is a challenging task, both due to the large number of possible test cases and the limited resources allocated for testing activities. This means that only a subset of all possible test cases can be chosen to test a system, and therefore the decision on the selection of test cases becomes important. The result of static analysis of a system can be used to help with this decision, in the context of model-based development of systems, this means that the analysis performed on a system model can be used to prioritize and guide the testing efforts. Furthermore, since models allow expression of non-functional requirements (such as performance, timing and security), model-guided testing can be used to direct testing towards specific parts of the system which have large impact on such requirements. In this paper, we focus on modeling and trade-off analysis of non-functional requirements and how static analysis helps to identify problematic parts of a system and thus guide the selection of test cases to target such parts.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614319,no
433,Optimization of test suite-test case in regression test,"Exhaustive product evolution and testing is required to ensure the quality of product. Regression testing is crucial to ensure software excellence. Regression test cases are applied to assure that new or adapted features do not relapse the existing features. As innovative features are included, new test cases are generated to assess the new functionality, and then included in the existing pool of test cases, thus escalating the cost and the time required in performing regression test and this unswervingly impacts the release, laid plan and the quality of the product. Hence there is a need to select minimal test cases that will test all the functionalities of the engineered product and it must rigorously test the functionalities that have high risk exposure. Test Suite-Test Case Refinement Technique will reduce regression test case pool size, reduce regression testing time, cost &amp; effort and also ensure the quality of the engineered product. This technique is a regression test case optimization technique that is a hybrid of Test Case Minimization based on specifications and Test Case Prioritization based on risk exposure. This approach will facilitate achievement of quality product with decreased regression testing time and cost yet uncover same amount of errors as the original test cases.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724206,yes
434,Prioritizing Variable-Strength Covering Array,"Combinatorial interaction testing is a well-studied testing strategy, and has been widely applied in practice. Combinatorial interaction test suite, such as fixed-strength and variable-strength interaction test suite, is widely used for combinatorial interaction testing. Due to constrained testing resources in some applications, for example in combinatorial interaction regression testing, prioritization of combinatorial interaction test suite has been proposed to improve the efficiency of testing. However, nearly all prioritization techniques may only support fixed-strength interaction test suite rather than variable-strength interaction test suite. In this paper, we propose two heuristic methods in order to prioritize variable-strength interaction test suite by taking advantage of its special characteristics. The experimental results show that our methods are more effective for variable-strength interaction test suite by comparing with the technique of prioritizing combinatorial interaction test suites according to test case generation order, the random test prioritization technique, and the fixed-strength interaction test suite prioritization technique. Besides, our methods have additional advantages compared with the prioritization techniques for fixed-strength interaction test suite.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649874,yes
436,Regression Testing Prioritization Based on Model Checking for Safety-Crucial Embedded Systems,"The order in which test-cases are executed has an influence on the rate at which faults can be detected. In this paper we demonstrate how test-case prioritization can be performed with the use of model-checkers. For this, different well known prioritization techniques are adapted for model-based use. New property based prioritization techniques are introduced. In addition it is shown that prioritization can be done at test-case generation time, thus removing the need for test-suite post-processing. Several experiments for safety-crucial embedded systems are used to show the validity of these ideas.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598153,yes
437,Research on optimization scheme of regression testing,"Regression testing is an important process during software development. In order to reduce costs of regression testing, research on optimization of scheme of regression testing have been done in this paper. For the purpose of reducing the number of test cases and detecting faults of programs early, this paper proposed to combine test case selection with test case prioritization. Regression testing process has been designed and optimization of testing scheme has been implemented. The criterion of test case selection is modify impact of programs, finding programs which are impacted by program modification according to modify information of programs and dependencies between programs. Test cases would be selected during test case selection. The criterion of test case prioritization is coverage ability and troubleshooting capabilities of test case. Test cases which have been selected during test case selection would be ordering in test case prioritization. Finally, the effectiveness of the new method is discussed.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818242,yes
438,Reusing black box test paths for white box testing of websites,"As the numbers of web users are increasing exponentially, the software complexity is increasing exponentially and the malwares are increasing exponentially, so exhaustive and extensive testing of websites has become a necessity today. But testing of a website is not 100% exhaustive as the page explosion problem is also very usual. In this paper, we propose to reuse the basis test paths as obtained from the Page-Test-Trees (PTTs) for white box testing of websites. We traverse the same set of paths (obtained above) and test for the source code at these nodes. This saves significant amount of time required to generate test paths and hence test cases as compared to the existing approaches of white box testing. The cost and efforts are also minimized. The proposed technique ensures better website testing coverage as white box testing provides better results than black box testing. Then we validate the proposed reusability testing with two web navigational structures. The results show that doing regression testing can save several billion dollars. These test cases can be further minimized by using prioritization techniques of regression testing.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514424,no
439,RisCal -- A Risk Estimation Tool for Software Engineering Purposes,"Decision making in software engineering requires the consideration of risk information. The reliability of risk information is strongly influenced by the underlying risk estimation process which consists of the steps risk identification, risk analysis and risk prioritization. In this paper we present a novel risk estimation tool for software engineering pruposes called RisCal. RisCal is based on a generic risk model and supports the integration of manually and automatically determined metrics into the risk estimation. This makes the tool applicable for arbitrary software engineering activities like risk-based testing or release planning. We show how RisCal supports risk identification, analysis and prioritizations, provide an estimation example, and discuss its application to risk-based testing and release planning.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619524,no
451,A distributed algorithm for virtual traffic lights with IEEE 802.11p,"A virtual traffic light (VTL) is a mechanism that allows vehicles to autonomously solve priorities at road junctions in the absence of fixed infrastructures (i.e., conventional traffic lights). To develop an effective VTL system, communication between vehicles is a crucial factor and can be handled either using cellular infrastructure or adopting a vehicle-to-vehicle (V2V) communication paradigm. In this paper we present the design, the implementation, and the field trial of a VTL which exploits V2V communications based on IEEE 802.11p. Specif-ically, we propose a decentralized algorithm, that adopts both broadcast signaling and unicast messages to assign priorities to the vehicles approaching intersections, thus preventing accidents and reducing traffic congestions. The algorithm has been tested both in a controlled laboratory environment and in a field trial with equipped vehicles.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6882621,no
454,A study of applying severity-weighted greedy algorithm to software test case prioritization during testing,"Regression testing is a very useful technique for software testing. Traditionally, there are several techniques for test case prioritization; two of the most used techniques are Greedy and Additional Greedy Algorithm (GA and AGA). However, it can be found that they may not consider the severity while prioritizing test cases. In this paper, an Enhanced Additional Greedy Algorithm (EAGA) is proposed for test case prioritization. Experiments with eight subject programs are performed to investigate the effects of different techniques under different criteria and fault severity. Experimental results show that proposed EAGA perform well than other techniques.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058806,yes
456,Applying Parameter Value Weighting to a Practical Application,"This paper reports a case study where pair-wise testing was applied to a real-world program. In particular we focus on weighting, an added feature which allows the tester to prioritize particular parameter values. In our previous work we proposed a weighting method that can reflect given weights in the resulting test suite more directly than can existing methods. To asses the effects of weighting in a practical testing process, we compare the number of execution times of the program's methods among three pair-wise test suites, including the test suite generated by our weighting method and those generated by an existing test case generation tool with and without the weighting option. The results show that the effects of weighting were most clearly observed when our weighting method was used.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983821,no
457,Athena: A Visual Tool to Support the Development of Computational Intelligence Systems,"Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984580,no
458,Automated Prioritization of Metrics-Based Design Flaws in UML Class Diagrams,"The importance of software architecture in software development prolongs throughout the entire software life cycle. This is because quality of the architectural design defines the structural aspects of the system that are difficult to change, and hence will affect most of the subsequent development and maintenance activities. This paper considers software design flaws (related to the system structure) and not flaws identified at run time (by testing). These design flaws are akin to what is described in the literature as anti-patterns, bad smells or rotting design. Recently, two tools that have been developed for quality assurance of software designs represented in the UML notation: SDMetrics and Metric View. However these tools are not considered practical because they report many design flaws which are not considered by developers (false positives). This paper explores an approach that tries to identify which design flaws should be considered important and which are not. To this end, we propose an approach for automated prioritization of software design flaws (BX approach), to facilitate developers to focus on important design flaws more effectively. We designed and implemented a tool (PoSDef) that implements this approach. The BX approach and the PoSDef tool have been validated using two open source projects and one large industrial system. Our validation consists of comparing our approach and tool with the existing design flaw tools. The evaluation has shown that the proposed approach could facilitate developers to identify and prioritize important design flaws effectively.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928837,no
460,Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines,"Large Software Product Lines (SPLs) are common in industry, thus introducing the need of practical solutions to test them. To this end, t-wise can help to drastically reduce the number of product configurations to test. Current t-wise approaches for SPLs are restricted to small values of t. In addition, these techniques fail at providing means to finely control the configuration process. In view of this, means for automatically generating and prioritizing product configurations for large SPLs are required. This paper proposes (a) a search-based approach capable of generating product configurations for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of product configurations. Both these techniques employ a similarity heuristic. The ability of the proposed techniques is assessed in an empirical study through a comparison with state of the art tools. The comparison focuses on both the product configuration generation and the prioritization aspects. The results demonstrate that existing t-wise tools and prioritization techniques fail to handle large SPLs. On the contrary, the proposed techniques are both effective and scalable. Additionally, the experiments show that the similarity heuristic can be used as a viable alternative to t-wise.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823132,no
466,Effective Regression Testing Using Requirements and Risks,"The use of system requirements and their risks enables software testers to identify more important test cases that can reveal faults associated with risky components. Having identified those test cases, software testers can manage the testing schedule more effectively by running such test cases earlier so that they can fix faults sooner. Some work in this area has been done, but the previous approaches and studies have some limitations, such as an improper use of requirements risks in prioritization and an inadequate evaluation method. To address the limitations, we implemented a new requirements risk-based prioritization technique and evaluated it considering whether the proposed approach can detect faults earlier overall. It can also detect faults associated with risky components earlier. Our results indicate that the proposed approach is effective for detecting faults early and even better for finding faults associated with risky components of the system earlier than the existing techniques.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6895426,yes
467,Enabling Prioritized Cloud I/O Service in Hadoop Distributed File System,"Cloud computing has become more and more popular nowadays. Both governments and enterprises provide service through the construction of public and private clouds accordingly. Among the platforms used in cloud computing, Hadoop is considered one of the most practical and stable systems. Nevertheless, as with other regular software, Hadoop still needs to rely on the underlying operating system to communicate with hardware to function appropriately. For modern computer systems, CPUs excessively outrun hard drives (hard disks). The computer hard disk has become a major bottleneck to the overall system performance. Consequently, computer programs can execute faster if their corresponding I/O operation can be completed sooner. This is important in particular when we want to expedite the execution of urgent programs in a busy system. Unfortunately, under the current Hadoop environment, users cannot prioritize operation of disk and memory for programs which they would like them to run faster. With the help of prioritized I/O service we developed earlier, we proposed and implemented a Hadoop environment with the ability of providing prioritized I/O service. Our Hadoop environment could accelerate the execution of programs with high priority assigned by users. We evaluated our design by executing prioritized programs in environments with different busy levels. Experimental results show that programs can improve their performance by up to 33.79% if executed with high priority.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056748,no
468,Exploring Model-Based Repositories for a Broad Range of Industrial Applications and Challenges,"Nowadays, systems are becoming increasingly complex and large and the process of developing such large-scale systems is becoming complicated with high cost and enormous effort required. Such a complicated process has a prominent challenge to ensure the quality of delivered artifacts. Therefore there is clearly a need to facilitate reuse of developed artifacts (e.g., requirements, architecture, tests) and enable automated analyses such as risk analyses, prioritizing test cases, change impact analysis, with the objective to reduce cost, effort and improve quality. Model-based engineering provides a promising mechanism to facilitate reuse and enable automation. The key idea is to use models as the backbone of structuring repositories that contain reusable artifacts (e.g., test cases, requirements). Such a backbone model is subse-quently used to enable various types of automation such as model-based testing and automated rule verification. In this paper, we report 12 industrial projects from five different industry domains that all require the construction of model-based repositories to enable various types of automation. We believe using models as the backbone to structure repositories for the purpose of enabling different types of automation in different contexts is a new and non-conventional model-based development research approach. This exploratory paper will serve the basis for future research to derive a generic model-based repository.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958385,no
469,Fair and delay adaptive scheduler for UC and NGN networks,"Fair bandwidth allocation while conforming to stringent end-to-end delay constraints is a major requirement for the successful delivery of next generation QoS demanding traffic. Research has been carried out in the area of processor and scheduler sharing for decades to try to achieve the QoS requirements of network traffic. Fairness and traffic prioritization are two main objectives that many schedulers were originally designed to meet. Another important issue is how schedulers treat the delay sensitive traffic. Although the combination of fairness and prioritization is implemented in several schedulers but, to our knowledge, incorporating adaptive traffic delay treatment in fair and prioritized schedulers has not yet been successfully implemented. In this paper, we introduce a new scheduler that balances between the fairness of bandwidth allocation between flows while implementing prioritization and minimizes the number of end-to-end delay bound breaches. The scheduler combines the virtual clock concept used in well-known fair schedulers together with schedulability testing and evaluation implemented in delay sensitive schedulers. The scheduler is designed to achieve the fairness of bandwidth allocation, such as in fair schedulers, while minimizing the number of possible violation of end-to-end QoS delay of individual flows' packets.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900970,no
471,How Does the UML Testing Profile Support Risk-Based Testing,"The increasing complexity of software-intensive systems raises a lot of challenges demanding new techniques for ensuring their overall quality. The risk of not meeting the expected level of quality has negative impact on business, customers, environment and people, especially in the context of safety/security-critical systems. The importance of risk assessment, analysis and management has been well understood both in the literature and practice, which has led to the definition of a number of well-known standards. In the recent years, Risk-Based Testing (RBT) is gaining more attention, especially focusing on test prioritization and selection based on risks. On the other hand, model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software-intensive systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in literature and practice in the last decade. In this paper, we study the feasibility of combining RBT with MBT by using the upcoming version of UML Testing Profile (UTP 2) as the mechanism. We present potential traceability between RBT and UTP 2 concepts.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983859,no
473,Implementing test case selection and reduction techniques using meta-heuristics,"Regression Testing is an inevitable and very costly maintenance activity that is implemented to make sure the validity of modified software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization to select and prioritize a minimum set of test cases, fulfilling some chosen criteria, that is, covering all possible faults in minimum time and other. In this paper a test case reduction hybrid Particle Swarm Optimization (PSO) algorithm has been proposed. This PSO algorithm uses GA mutation operator while processing. PSO is a swarm intelligence algorithm based on particles behavior. GA is an evolutionary algorithm (EA). The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949377,no
481,Reverse Engineering Complex Feature Correlations for Product Line Configuration Improvement,"As a Software Product Line (SPL) evolves with increasing number of variant features and feature values, the feature correlations become extremely intricate. However, these correlations are often incompletely documented (e.g., In feature models) so that most features can only be configured manually. In order to make product configuration processes more efficient, we present an approach to extracting complex feature correlations from existing product configurations using association mining techniques. Then these correlations are pruned and prioritized in order to minimize the effort of correlation validation. Our approach is conducted on an industrial SPL with 100 product configurations across 480 features. While 80 out of the 100 configurations are used as training data to automatically extract 4834 complex feature correlations, the rest 20 configurations are used as test data to evaluate the improvement potential of configuration efficiency. In the end, avg. 25% features in each of the 20 products can be configured automatically.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928830,no
489,Test Suite Reduction by Combinatorial-Based Coverage of Event Sequences,"Combinatorial-based criteria are useful in several studies for test suite generation, prioritization, and minimization. In this paper, we extend previous work by using combinatorial-based criteria for test suite reduction. We use criteria that are based on combinatorial coverage of events and consider the order in which events occur. A simple combinatorial-based criterion covers t-way events and does not differentiate between the order of events. The event pair (e<sub>1</sub>, e<sub>2</sub>) is counted the same as if it occurs in the order (e<sub>2</sub>, e<sub>1</sub>). We also use two criteria that consider the order in which events occur since different orderings of events may be valuable during testing. First, the consecutive sequence-based criterion counts all event sequences in different orders, but they must occur adjacent to each other. The sequence-based criterion counts pairs in all orders without the requirement that events must be adjacent. We evaluate the new criteria on three GUI applications. We use 2way inter-window coverage in our studies. All of the 2way combinatorial-based criteria are effective in reducing the test suites and maintaining close to 100% fault finding effectiveness. Our future work examines larger test suites, higher strength coverage, techniques to partition event data, and further empirical studies.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825647,no
490,Testability of object-oriented systems: An AHP-based approach for prioritization of metrics,This paper investigates testability from the perspective of metrics used in an object-oriented system. The idea is to give an overview of object oriented design metrics with the prioritization of same keeping testability as the overall goal. We have used Analytic Hierarchy Process (AHP) method to attain which metric is mostly used and is best for testability.,2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019595,no
493,Using Dual Priority scheduling to improve the resource utilization in the nMPRA microcontrollers,"The current practice in most of the safety-critical areas, including automotive, avionics systems and factory automation, encouraging the use of real-time time-trigger schedulers that does not allow interference to take place between safety-critical components and non-critical. Furthermore, in these systems the lack of interference between safety-critical components and non-critical components is achieved by a strict isolation between components with different degrees of severity. This approach can assure, easily, the certification of the safety-critical functionality, but leads to very low resource utilization. For this purpose it will be presented a solution that when the system enters into a state that is different from the normal running state (test service), allowing relaxation and a change in the activation time of tasks (release) violating the fixed priorities scheduling, but avoiding starvation of the system tasks. The proposed solution modifies a static scheduler in a dynamic scheduler depending on the system status using Dual Priority scheduling. The algorithm has been proposed to be implemented on a nMPRA processor, by multiplying hardware resources (PC, pipeline registers and file registers) and other facilities (events, mutexes, interrupts, IPC communication, timer's, the static scheduler and support for dynamic scheduler) provides a switching and response time for events within 1 to 3 machine cycles.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842431,no
498,A defect dependency based approach to improve software quality in integrated software products,"Integrated software products are complex in design. They are prone to defects caused by integrated and non-integrated modules of the entire integrated software suite. In such software products, a small proportion of defects are fixed as soon as they are reported. Rest of the defects are targeted for fixes in future product release cycles. Among such targeted defects, most of them seem to be insignificant and innocuous in the current version but have the potential to become acute in future versions. In this paper, we propose an approach to study defect dependency of the reported defect using a dependency metric. Identifying the dependency of a defect in an integrated product suite can help the product stake-owners to prioritize them and help improve software quality.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320343,no
506,AEGIS autonomous targeting for the Curiosity rover's ChemCam instrument,"AEGIS (Autonomous Exploration for Gathering Increased Science) is a software suite that will imminently be operational aboard NASA's Curiosity Mars rover, allowing the rover to autonomously detect and prioritize targets in its surroundings, and acquire geochemical spectra using its ChemCam instrument. ChemCam, a Laser-Induced Breakdown Spectrometer (LIBS), is normally used to study targets selected by scientists using images taken by the rover on a previous sol and relayed by Mars orbiters to Earth. During certain mission phases, ground-based target selection entails significant delays and the use of limited communication bandwidth to send the images. AEGIS will allow the science team to define the properties of preferred targets, and obtain geochemical data more quickly, at lower data penalty, without the extra ground-inthe-loop step. The system uses advanced image analysis techniques to find targets in images taken by the rover's stereo navigation cameras (NavCam), and can rank, filter, and select targets based on properties selected by the science team. AEGIS can also be used to analyze images from ChemCam's Remote Micro Imager (RMI) context camera, allowing it to autonomously target very fine-scale features - such as veins in a rock outcrop - which are too small to detect with the range and resolution of NavCam. AEGIS allows science activities to be conducted in a greater range of mission conditions, and saves precious time and command cycles during the rover's surface mission. The system is currently undergoing initial tests and checkouts aboard the rover, and is expected to be operational by late 2015. Other current activities are focused on science team training and the development of target profiles for the environments in which AEGIS is expected to be used on Mars.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444544,no
510,Approximating Attack Surfaces with Stack Traces,"Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202964,no
511,Architecting to Ensure Requirement Relevance: Keynote TwinPeaks Workshop,"Research has shown that up to two thirds of features in software systems are hardly ever used or not even used at all. This represents a colossal waste of R&amp;D resources and occurs across the industry. On the other hand, product management and many others work hard at interacting with customers, building business cases and prioritizing requirements. A fundamentally different approach to deciding what to build is required: requirements should be treated as hypothesis throughout the development process and constant feedback from users and systems in the field should be collected to dynamically reprioritize and change requirements. This requires architectural support beyond the current state of practice as continuous deployment, split testing and data collection need to be an integral part of the architecture. In this paper, we present a brief overview of our research and industry collaboration to address this challenge.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184704,no
514,Communication and collaboration of heterogeneous unmanned systems using the joint architecture for Unmanned Systems (JAUS) standards,"The Naval Undersea Warfare Center Division Newport (NUWCDIVNPT) and Georgia Tech Research Institute (GTRI) completed a successful at-sea exercise with autonomous UAS and UUV systems demonstrating cross-domain unmanned system communication and collaboration. The exercise was held at the NUWC Narragansett Bay Shallow Water Test Facility (NBSWTF) range, and it represented for the first time the use of standard protocols and formats that effectively support cross-domain unmanned system operations. Four man-portable Iver2 UUVs operating in coordinated missions autonomously collected environmental data, which was compressed in-stride, re-formatted, and exfiltrated via UAS relay for display and tactical decision making. Two UAS with autonomous flight take-off and mission execution were sequenced to serve as ISR platforms and to support communications as RF relays for the UUVs performing Intelligence Preparation of the Environment missions. Two Command and Control nodes ashore provided unmanned system tasking and re-tasking, and they served to host and display both geo-positional data and status for UAS and UUV vehicles during the operational scenarios run during the exercise. The SAE Joint Architecture for Unmanned Systems (JAUS) standards were used for all message traffic between shore-based C2 nodes, UAS, and UUVs active in the NBSWTF exercise area. Exercise goals focused on CNO priorities expressed in the Undersea Domain Operating Concept of AUG 2013 which emphasized protocols essential to effective command and control of networked unmanned systems with decentralization and flexibility of command structures. Development for this project highlighted both the strengths and shortfalls of JAUS and captured the requirements for moving forward in effective cross-domain communications that support distributed, agile C2 nodes to meet evolving CONOPS for growing unmanned system presence and mission roles. The scenario employed operating parameters for UAS and UUV that have been established in real-world operations and ongoing unmanned system programs. The tactical information from unmanned systems was displayed in real-time on shore-based C2 displays: the tactical FalconView display and the developmental TOPSIDE command and control station. This work represents a critical step in communications for networking of heterogeneous unmanned systems and establishes a solid platform for alignment of development and ongoing programs. The evaluation of JAUS suitability for near-term operational applications provides significant value as Concepts of Operation that rely on netted heterogeneous systems are being targeted. The focus on affordable commercial unmanned systems for this experimentation establishes the value of highly capable, portable systems to provide economical development and test opportunities with low-cost and low-risk alternatives to many planned and fielded systems. The JAUS architecture was introduced to the NUWC and GTRI unmanned systems though an instantiation of the Mission Oriented Operating Suite (MOOS) autonomy framework on secondary CPUs integrated into the Iver2 UUVs and the GTRI UAS. Since the GTRI UASs already had ROS installed, a MOOS-ROS bridge was employed to support use of the developed JAUS messaging capability. Established JAUS services were employed where the required functions could be met. New JAUS services were developed to meet functionality required for the operational scenarios in this exercise but not yet supported in the existing releases of SAE JAUS. Independent C++ header libraries that could be compiled at run time for specific autonomy frameworks, such as MOOS, were employed to support a software-agnostic approach. Immediate targets for broadening the influence of this work to coalition partners include the NATO Recognized Environmental Picture (REP) 2015 and The Technical Cooperation Program (TTCP) 2015 exercises. This project and demonstration was funded under a NUWC Strategic Initiative and GTRI program support.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271613,no
515,Component based reliability assessment from UML models,"Model based development and testing techniques have ventured diverse research directions to assure quality of the software product. Models developed during architecture and design phases are efficient tools to assess quality at an early development stage. However, testing the extra-functional or non-functional properties of software systems is not frequently practised eg. reliability. The motivation to our work is to model the context of execution which is significant in system reliability analysis. In this paper we visualize the components of complex software systems and their interactions in the form of Functional Flow Diagram (FFD). This notation specifies the dynamic aspect of system behavior as the context of execution. To further asses reliability, the FFD is translated into Reliability Block Diagram (RBD). The relative importance of the components in terms of reliability is evaluated and is associated with prioritization of the component. The model is simple but significant for system maintenance, improvisation and modification. This model supports analysis and testing through better understanding of the interacting components and their reliabilities.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275704,no
517,Considerations on application of selective hardening based on software fault tolerance techniques,"This paper analyses the nature of fault tolerance software-based techniques and the influence of their overheads to determine an efficient strategy for applying those techniques in a selective way. Several considerations that have to be taken into account are presented in this work. These include an analysis of fault coverage and overheads when selective hardening is adopted; side effects of selective protection based on software; and the need of new criticality metrics, apart from those used for hardware-based techniques (e.g., AVF), to facilitate and prioritize the selection of resources to be protected.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102509,no
519,Detecting Display Energy Hotspots in Android Apps,"Energy consumption of mobile apps has become an important consideration as the underlying devices are constrained by battery capacity. Display represents a significant portion of an app's energy consumption. However, developers lack techniques to identify the user interfaces in their apps for which energy needs to be improved. In this paper, we present a technique for detecting display energy hotspots - user interfaces of a mobile app whose energy consumption is greater than optimal. Our technique leverages display power modeling and automated display transformation techniques to detect these hotspots and prioritize them for developers. In an evaluation on a set of popular Android apps, our technique was very accurate in both predicting energy consumption and ranking the display energy hotspots. Our approach was also able to detect display energy hotspots in 398 Android market apps, showing its effectiveness and the pervasiveness of the problem. These results indicate that our approach represents a potentially useful technique for helping developers to detect energy related problems and reduce the energy consumption of their mobile apps.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102585,no
520,Detecting hardware Trojans in unspecified functionality using mutation testing,"Existing functional Trojan detection methodologies assume Trojans violate the design specification under carefully crafted rare triggering conditions. We present a new type of Trojan that leaks secret information from the design by only modifying unspecified functionality, meaning the Trojan is no longer restricted to being active only under rare conditions. We provide a method based on mutation testing for detecting this new Trojan type along with mutant ranking heuristics to prioritize analysis of the most dangerous functionality. Applying our method to a UART controller design, we discover unspecified and untested bus functionality with the potential to leak 32 bits of information during hundreds of cycles without being detected! Our method also reveals poorly tested interrupt functionality with information leakage potential. After modifying the specification and test bench to remove the discovered vulnerabilities, we close the verification loop by re-analyzing the design using our methodology and observe the functionality is no longer flagged as dangerous.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372619,no
523,Feasibility Analysis of Engine Control Tasks under EDF Scheduling,"Engine control applications include software tasks that are triggered at predetermined angular values of the crankshaft, thus generating a computational workload that varies with the engine speed. To avoid overloads at high rotation speeds, these tasks are implemented to self adapt and reduce their computational demand by switching mode at given rotation speeds. For this reason, they are referred to as adaptive variable rate (AVR) tasks. Although a few works have been proposed in the literature to model and analyze the schedulability of such a peculiar type of tasks, an exact analysis of engine control applications has been derived only for fixed priority systems, under a set of simplifying assumptions. The major problem of scheduling AVR tasks with fixed priorities, however, is that, due to engine accelerations, the interarrival period of an AVR task is subject to large variations, therefore there will be several speeds at which any fixed priority assignment is far from being optimal, significantly penalizing the schedulability of the system. This paper proposes for the first time an exact feasibility test under the Earliest Deadline First scheduling algorithm for tasks sets including regular periodic tasks and AVR tasks triggered by a common rotation source. In addition, a set of simulation results are reported to evaluate the schedulability gain achieved in this context by EDF over fixed priority scheduling.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176033,no
524,Fusion of LIDAR and video cameras to augment medical training and assessment,"The Mobile Medical Lane Trainer (MMLT) is a multi-sensor rapidly deployed After-Action Review (AAR) system for Army medical lane training. Current AAR systems have two main drawbacks: 1) video does not provide a complete view of the medical and tactical situation, and 2) the video is not readily available for effective evaluation. The MMLT program is developing a ??smarter? AAR system by using 3D LIDAR (LIght Detection And Ranging), a camera array, People Tracking software and Medical Training Evaluation and Review (MeTER) software. This system can be brought to the field and deployed in less than an hour to provide hands-off data collection for the exercise. MMLT supplements existing evaluation systems deployed at the Medical Simulation Training Centers (MSTCs) by providing a 3-D perspective of the training event for tactical evaluation with synchronized video technology to capture both tactical and clinical skills and instructor scoring. This capability is used in conjunction with the MeTER system's skill assessment checklists for automated performance review. An immediate synchronized playback capability has been developed, ultimately resulting in a rapid AAR for debriefing. This paper will discuss the technical components of the system, including hardware components, data fusion technique, tracking algorithms, and camera prioritization approaches, and will conclude with operational test results and lessons learned.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7295832,no
527,Improving Multi-Objective Test Case Selection by Injecting Diversity in Genetic Algorithms,"A way to reduce the cost of regression testing consists of selecting or prioritizing subsets of test cases from a test suite according to some criteria. Besides greedy algorithms, cost cognizant additional greedy algorithms, multi-objective optimization algorithms, and multi-objective genetic algorithms (MOGAs), have also been proposed to tackle this problem. However, previous studies have shown that there is no clear winner between greedy and MOGAs, and that their combination does not necessarily produce better results. In this paper we show that the optimality of MOGAs can be significantly improved by diversifying the solutions (sub-sets of the test suite) generated during the search process. Specifically, we introduce a new MOGA, coined as DIversity based Genetic Algorithm (DIV-GA), based on the mechanisms of orthogonal design and orthogonal evolution that increase diversity by injecting new orthogonal individuals during the search process. Results of an empirical study conducted on eleven programs show that DIV-GA outperforms both greedy algorithms and the traditional MOGAs from the optimality point of view. Moreover, the solutions (sub-sets of the test suite) provided by DIV-GA are able to detect more faults than the other algorithms, while keeping the same test execution cost.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936894,no
528,Improving prioritization of software weaknesses using security models with AVUS,"Testing tools for application security have become an integral part of secure development life-cycles. Despite their ability to spot important software weaknesses, the high number of findings require rigorous prioritization. Most testing tools provide generic ratings to support prioritization. Unfortunately, ratings from established tools lack context information especially with regard to the security requirements of respective components or source code. Thus experts often spend a great deal of time re-assessing the prioritization provided by these tools. This paper introduces our lightweight tool AVUS that adjusts context-free ratings of software weaknesses according to a user-defined security model. We also present a first evaluation applying AVUS to a well-known open source project and the findings of a popular, commercially available application security testing tool.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335423,no
539,Practical Combinatorial Interaction Testing: Empirical Findings on Efficiency and Early Fault Detection,"Combinatorial interaction testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. Simulated Annealing (SA) and Greedy Algorithms have been widely used to find CIT test suites. From the literature, there is a widely-held belief that SA is slower, but produces more effective tests suites than Greedy and that SA cannot scale to higher strength coverage. We evaluated both algorithms on seven real-world subjects for the well-studied two-way up to the rarely-studied six-way interaction strengths. Our findings present evidence to challenge this current orthodoxy: real-world constraints allow SA to achieve higher strengths. Furthermore, there was no evidence that Greedy was less effective (in terms of time to fault revelation) compared to SA; the results for the greedy algorithm are actually slightly superior. However, the results are critically dependent on the approach adopted to constraint handling. Moreover, we have also evaluated a genetic algorithm for constrained CIT test suite generation. This is the first time strengths higher than 3 and constraint handling have been used to evaluate GA. Our results show that GA is competitive only for pairwise testing for subjects with a small number of constraints.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081752,no
540,Preemptive Regression Testingof Workflow-Based Web Services,"An external web service may evolve without prior notification. In the course of the regression testing of a workflow-based web service, existing test case prioritization techniques may only verify the latest service composition using the not-yet-executed test cases, overlooking high-priority test cases that have already been applied to the service composition before the evolution. In this paper, we propose Preemptive Regression Testing (PRT), an adaptive testing approach to addressing this challenge. Whenever a change in the coverage of any service artifact is detected, PRT recursively preempts the current session of regression test and creates a sub-session of the current test session to assure such lately identified changes in coverage by adjusting the execution priority of the test cases in the test suite. Then, the sub-session will resume the execution from the suspended position. PRT terminates only when each test case in the test suite has been executed at least once without any preemption activated in between any test case executions. The experimental result confirms that testing workflow-based web service in the face of such changes is very challenging; and one of the PRT-enriched techniques shows its potential to overcome the challenge.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6812226,yes
545,Priority Integration for Weighted Combinatorial Testing,"Priorities (weights) for parameter values can improve the effectiveness of combinatorial testing. Previous approaches have employed weights to derive high-priority test cases either earlier or more frequently. Our approach integrates these order-focused and frequency-focused prioritizations. We show that our priority integration realizes a small test suite providing high-priority test cases early and frequently in a good balance. We also propose two algorithms that apply our priority integration to existing combinatorial test generation algorithms. Experimental results using numerous test models show that our approach improves the existing approaches w.r.t. Order-focused and frequency-focused metrics, while overheads in the size and generation time of test suites are small.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273624,no
547,Quantifying security risk by measuring network risk conditions,"Software vulnerabilities are the weaknesses in the software that inadvertently allow dangerous operations. If the vulnerability is in a network service, it poses serious security threats because a cyber-attacker can exploit it to gain unauthorized access to the system. Hence, rapid discovery and remediation of network vulnerabilities is critical issues in network security. In today's dynamic IT environment, it is common practice that an organization prioritizes the mitigation of discovered vulnerabilities according to their risk levels. Currently available technologies, however, associate each vulnerability to the static risk level which does not take the unique characteristics of the target network into account. This often leads to inaccurate risk prioritization and less-than-optimal resource allocation. In this research, we introduce a novel way of quantifying the risk of network vulnerability by augmenting the static risk level with conditions specific to the target network. The method calculates the risk value of each vulnerability by measuring the proximity to the untrusted network and risk of the neighboring hosts. The resulting risk value, RCR is a composite index of the individual risk, network location and neighborhood risk conditions. Thus, it can be effectively used for prioritization, comparison and trending. We tested the methodology through the network intrusion simulation. The results shows average 88.9% the correlation between RCR and number of successful attacks on each vulnerability.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166562,no
549,Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling,"Mixed-criticality real-time scheduling has been developed to improve resource utilization while guaranteeing safe execution of critical applications. These studies use optimistic resource reservation for all the applications to improve utilization, but prioritize critical applications when the reservations become insufficient at runtime. Many of them however share an impractical assumption that all the critical applications will simultaneously demand additional resources. As a consequence, they under-utilize resources by penalizing all the low-criticality applications. In this paper we overcome this shortcoming using a novel mechanism that comprises a parameter to model the expected number of critical applications simultaneously demanding more resources, and an execution strategy based on the parameter to improve resource utilization. Since most mixed criticality systems in practice are component-based, we design our mechanism such that the component boundaries provide the isolation necessary to support the execution of low-criticality applications, and at the same time protect the critical ones. We also develop schedulability tests for the proposed mechanism under both a flat as well as a hierarchical scheduling framework. Finally, through simulations, we compare the performance of the proposed approach with existing studies in terms of schedulability and the capability to support low-criticality applications.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176022,no
551,Supporting Continuous Integration by Code-Churn Based Test Selection,"Continuous integration promises advantages in large-scale software development by enabling software development organizations to deliver new functions faster. However, implementing continuous integration in large software development organizations is challenging because of organizational, social and technical reasons. One of the technical challenges is the ability to rapidly prioritize the test cases which can be executed quickly and trigger the most failures as early as possible. In our research we propose and evaluate a method for selecting a suitable set of functional regression tests on system level. The method is based on analysis of correlations between test-case failures and source code changes and is evaluated by combining semi-structured interviews and workshops with practitioners at Ericsson and Axis Communications in Sweden. The results show that using measures of precision and recall, the test cases can be prioritized. The prioritization leads to finding an optimal test suite to execute before the integration.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167168,no
557,Test case selection for networked production systems,"This paper provides a discussion on the coming technological changes in process automation of networked production systems, which will change the testing procedure. In the smart factory of the future there will be no possibility to reach a test coverage of 100%, assuming a flexible automation with continuous reconfiguration and dynamic changes during runtime. Consequently, large amounts of test cases and powerful algorithms for their prioritization are needed in order to certify the correct functionality of the production systems in the network. A concept is presented on how to analyze and prioritize the enormous amount of test cases resulting from the changes during runtime. The proposed approach for test case selection utilizes information of the product, the process and the status of the for the prioritization and selection.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301604,yes
565,Tracking down high coverage configuration using clustering and fault detection,"Mostly all software systems are highly configured, it has many benefits but there is a difficulty of software testing because there will be unique errors could be hidden in any of the configurations and undergoing testing for each of the configurations will lead to expensive testing and it is also impractical. The dependable systems will have some mechanism for fault tolerance in software testing. If the rate of the fault detection is calculated then the coverage of the configuration can be easily generated. First load the application for which it is going to be tested by using our test case prioritization approach and loading the dataset for the test case for the given application. After this process, need to assign the individual ids for all the test cases in the test case dataset. Also it is able to add the test cases in the dynamic nature. Then to compute the test case prioritization, first built the dependency structure for the test cases. Through the approach get the height and weight matrix for the test cases after this computation the test cases. The cosine similarity values between the test cases. In the similarity values it will show how it is highly related with the other test cases. Thus the clustering approach is introduced for grouping the test cases. These test cases are analyzed for measuring their relevancy and relationship between the test cases using their constrains and the clustering of the test cases is done for the better result in the rate of fault detect. With the Average percentage fault detection the graph is drawn and it shown the high coverage configurations.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453839,yes
576,A study of critical success factors on software quality assurance of cloud networking devices,"In recent years, many more enterprises and organizations are embracing cloud computing for its commercial benefits and competitive advantages. However, these companies have prioritized hardware specifications when evaluating cloud networking devices, often neglecting the importance of software quality as a result. Most cloud networking devices currently offered on the market include embedded software systems, which mean that software and hardware compatibility can be one of the most essential characteristics to look for. In order to investigate the critical success factors (CSFs) for the software quality assurance (QA) of cloud networking devices, this study employed a Plan-Do-Check-Act (PDCA) research framework. Important Software QA factors from relevant IEEE standards and hardware factors related to software quality were then employed in the 4 facets of the PDCA process to conduct a 2-stage analysis. For the first stage of this study, 5 experts were interviewed and asked to complete a survey form and select CSFs that were then used to generate a questionnaire for the second stage where a total of 15 experts and QA engineers were invited to fill out questionnaires. Completed questionnaires were then subject to an analytic hierarchy process (AHP) to calculate the weight and priority orders for each of the CSFs. The overall results of Stage 2 indicate that Cloud devices security testing, Security of cloud networking devices, and Resources and their allocation are the top three orders of all CSFs when experts take a real concern in software QA of cloud networking devices.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811054,no
577,A tool for constrained pairwise test case generation using statistical user profile based prioritization,"Pairwise testing is a wildly used approach in order to reduce the size of test suite and take steps to combinatorial testing problems because of an extensively large number of possible combinations between input parameters and values. In some cases, there will be invalid combinations between input parameters and values if constraints have not been handled. In this paper, we present a pairwise test generation tool called CPTG, a tool to generate test cases for pairwise testing by applying user profile for guiding and prioritizing in order to select optimal input parameters and values which do not depend on individual tester skills and also providing constraint handling solution between input parameters and values. We performed experiments and comparison with other tools. The experimental results of our tool demonstrated that our tool becomes particularly valuable in guiding testing with a maximized reliability by testing the most frequently used of the system and can generate comparable results of the size of the test case set.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748881,no
578,ACO based embedded system testing using UML Activity Diagram,"This paper proposed a model-based technique for test scenario generation using Activity Diagram (AD). We transform an AD specification into an intermediate graph called Activity Interaction Graph (AIG) using the proposed parser. After that, we apply combination of BFS and DFS algorithms for generating test scenarios. Then, we apply an algorithm called ACOToTSP (Ant Colony Optimization for Test Scenarios Prioritization) algorithm on the generated test scenarios with respect to some decision and concurrent criteria, for prioritizing the test scenarios. This approach generates test scenarios according to forks, Joins, and merge point's strength in the activity diagram.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847997,yes
581,Application of Mahalanobis-Taguchi Method and 0-1 Programming Method to Cost-Effective Regression Testing,"To enhance the cost effectiveness of regression testing, this paper proposes a method for prioritizing test cases. In general, a test case can be evaluated from various different points of view, therefore whether it is worth it to re-run should be discussed using multi criteria. This paper shows that the Mahalanobis-Taguchi (MT) method is a useful way to successfully integrate different evaluations of a test case. Moreover, this paper proposes to use the 0-1 programming method together with the MT method in order to take into account not only the priority of a test case but also its cost to run. The empirical study with 300 test cases for an industrial software system shows that the combination of the MT method and the 0-1 programming method is more cost-effective than other conventional methods.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592803,no
583,Automated comparison of X-ray images for cargo scanning,"Customs administrations are responsible for the enforcement of fiscal integrity and security of movements of goods across land and sea borders. In order to verify whether the transported goods match the transport declaration, X-ray imaging of containers is used at many customs site worldwide. The main objective of the research and development project ??Automated Comparison of X-ray Images for Cargo Scanning (ACXIS)?, which is funded by the European 7<sup>th</sup>Framework Program, is to improve the efficiency and effectiveness of the inspection procedures of cargo at customs using X-ray technology. The current inspection procedures are reviewed to identify risks, catalogue illegal cargo, and prioritize detection scenarios. Based on these results, we propose an integrated solution that provides automation, information exchange between customs administrations, and computer-based training modules for customs officers. Automated target recognition (ATR) functions analyze the X-ray image after a scan is made to detect certain types of goods such as cigarettes, weapons and drugs in the freight or container. Other helpful information can also be provided, such as the load homogeneity, total or partial weight, or the number of similar items. The ATR functions are provided as an option to the user. The X-ray image is transformed into a manufacturer-independent format through geometrical and spectral corrections and stored into a database along with the user feedback and other related data. This information can be exchanged with similar systems at other sites, thus facilitating information exchange between customs administrations. The database is seeded with over 30'000 examples of legitimate and illegal goods. These examples are used by the ATR functions through machine learning techniques, which are further strengthened by the information exchange. In order to improve X-ray image interpretation competency of human operators (customs officers), a computer-based training software is developed that simulates these new inspection procedures. A study is carried out to validate the effectiveness and efficiency of the computer-based training as well as the implemented procedures. Officers from the Dutch and Swiss Customs administrations partake in the study, covering both land and sea borders.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815714,no
588,AVISAR - a three tier architectural framework for the testing of Object Oriented Programs,"In this research paper, we have proposed a three tier conceptual framework AVISAR for the testing of Object Oriented Programs. We have devised the proposed framework into three levels: Requirements Modeling, Test Case Generation and Effort Estimation. At the first level, we have used the Extend relationship of the Use Case to capture the events generated by the Classes during message passing between them through their objects. These events were captured by an Object Instantiator using Event Templates and further given to a Test case scenario generator, which generates the test cases. At the second level, the test cases will be generated based on the event templates that are used for capturing the events generated by the Extend relationship of the Use cases. Further, we have proposed a Genetic Algorithm (GA) for the effective test case prioritization to ensure maximum code coverage. At the third level, effort estimation, we have used the Cyclometric Complexity and Token count to perform the effort estimation for the Software under Test (SUT).",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918730,yes
591,Concolic test generation for PLC programs using coverage metrics,"This paper presents a technique for fully automated generation of test cases for PLC programs adhering to the IEC 61131-3 standard. While previous methods strive for completeness and therefore struggle with the state explosion we pursue a symbolic execution based approach, dropping completeness but nevertheless achieving similar or even better results in practice. The core component is a symbolic execution engine which chooses the next state to execute, handles constraints emerging during the execution and derives respective test vectors leading to a state. To make for a high coverage of the generated tests, we adopt techniques from concolic testing, allow for use of heuristics to prioritise promising states but also merge states to alleviate the path explosion. We exploit peculiarities of PLC semantics to determine reasonable merge-points and unlike similar approaches even handle unreachable code. To examine the feasibility of our technique we evaluate it on function blocks used in industry.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497884,no
593,Data flow based quality testing approach using ACO for component based software development,"Component-based Software Engineering (CBSE) has been centered around advancements identified with configuration and implementation of software components and frameworks assembled from software components. Quality Assurance (QA) for CBSE is a new subject in the software development research area. In this paper an enhanced data flow based QA model is presented for CBSE by employing the Ant Colony Optimization (ACO)algorithm to optimize the given code for automatic generation and prioritization of optimal path in decision to decision Control Flow Graph (CFG), which results an enhanced testing phase for QA model with reduced complexity. After that, proposed ACO based approach is also utilized for the generation of test data to satisfy the generated set of paths. This paper additionally exhibits the proposed approach applying it in a program module. Results show that a better testing is achieved by applying proposed ACO based scheme on component based software. Proposed approach ensures full software coverage with least redundancy.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813850,no
594,Database design for error searching system based on keyword priority,"Because there are too many types of errors occurred in a multi-software integrated platform, such as Integrated Decision Information System (IDIS). It is urgent to design an error searching system to solve all different problems. However, those errors belong to different stages like setup, configuration, and operation, or those errors may occurred in different services, applications, or IP ports, or may be happened in different system software, different version of software, and those errors are also can be classified into different types. The new requirement on the design of database has been announced, that it has to locate the error, find out the reason of this error, as well as the corresponding solution. Also, it requires to provide the location, phase, etc. of an error. For a multi-software integrated platform, this paper proposed a database design for error searching system based on keyword priority. This DB made the correspondence among error, the reason of the error, and corresponding solution, and put them to different categories in terms of their characteristics, such that it is easy to manage, search, and use. This method treats those characteristics as keywords with higher priority, which are correspondent with errors, reasons, and solutions, respectively. While the keywords extracted from errors, reasons, and solutions are treated as keywords with lower priority. Keywords with different priorities can all be used as index to search errors, reasons, and solutions, while keywords with higher priority can be used to filter the searching results finer, and make the searching results more accurate. On one hand, the database design method based on keyword priority has simplified the logical structure of the database. On the other hand, users do not only search the reason and solution of an error, but also can find out the accurate information of the error, such as in which stage, layer, software, or categories, etc. the error has happened. The data of the database is complete, which has been provided by 500 technicians who had found thousands of errors. Those errors haven been added to the DB after tests to make the DB more complete.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811011,no
595,Diversity-Aware Mutation Adequacy Criterion for Improving Fault Detection Capability,"Many existing testing techniques adopt diversity as an important criterion for the selection and prioritization of tests. However, mutation adequacy has been content with simply maximizing the number of mutants that have been killed. We propose a novel mutation adequacy criterion that considers the diversity in the relationship between tests and mutants, as well as whether mutants are killed. Intuitively, the proposed criterion is based on the notion that mutants can be distinguished by the sets of tests that kill them. A test suite is deemed adequate by our criterion if the test suite distinguishes all mutants in terms of their kill patterns. Our hypothesis is that, simply by using a stronger adequacy criterion, it is possible to improve fault detection capabilities of mutation-adequate test suites. The empirical evaluation selects tests for real world applications using the proposed mutation adequacy criterion to test our hypothesis. The results show that, for real world faults, test suites adequate to our criterion can increase the fault detection success rate by up to 76.8 percentage points compared to test suites adequate to the traditional criterion.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528954,no
596,Do We Have a Chance to Fix Bugs When Refactoring Code Smells?,"Code smells are used to describe code structures that may cause detrimental effects on software and should be refactored. Previous studies show that some code smells have significant effect on faults. However, how to refactor code smells to reduce bugs still needs more concern. We investigate the possibility of prioritizing code smell refactoring with the help of fault prediction results. We also investigate the possibility of improving the performance of fault prediction by using code smell detection results. We use Cohen's Kappa statistic to report agreements between results of code smell detections and fault predictions. We use fault prediction result as an indicator to guide code smell refactoring. Our results show that refactoring Blob, Long Parameter List, and Refused Parent Be Request may have a good chance to detect and fix bugs, and some code smells are particularly useful for improving the recall of fault prediction.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780189,no
598,Effect of Time Window on the Performance of Continuous Regression Testing,"Test prioritization is an effective technique used to reduce the amount of work required to support regression testing in continuous integration development. It aims at finding an optimal order of tests that can detect regressions faster, potentially increasing the frequency of software releases. Prioritization techniques based on test execution history use the results of preceding executions to determine an optimal order of regression tests in the succeeding test executions. In this paper, we investigate how can execution history be optimally used to increase the effectiveness of regression test prioritization. We analyze the effect of history time window on the fault detection effectiveness of prioritized regression tests. We report an experimental study using a data set from Cisco. The results suggest that varying the size of the window can considerably change the performance of regression testing. Our findings will potentially help developers and test teams in adjusting test prioritization techniques for achieving higher cost-effectiveness in continuous regression testing.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816510,yes
600,Empirical Evaluation of Cross-Release Effort-Aware Defect Prediction Models,"To prioritize quality assurance efforts, various fault prediction models have been proposed. However, the best performing fault prediction model is unknown due to three major drawbacks: (1) comparison of few fault prediction models considering small number of data sets, (2) use of evaluation measures that ignore testing efforts and (3) use of n-fold cross-validation instead of the more practical cross-release validation. To address these concerns, we conducted cross-release evaluation of 11 fault density prediction models using data sets collected from 2 releases of 25 open source software projects with an effort-aware performance measure known as Norm(P<sub>opt</sub>). Our result shows that, whilst M5 and K* had the best performances, they were greatly influenced by the percentage of faulty modules present and size of data set. Using Norm(P<sub>opt</sub>) produced an overall average performance of more than 50% across all the selected models clearly indicating the importance of considering testing efforts in building fault-prone prediction models.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589801,no
604,Experience Report: Understanding Cross-Platform App Issues from User Reviews,"App developers publish apps on different platforms, such as Google Play, App Store, and Windows Store, to maximize the user volumes and potential revenues. Due to the different characteristics of the platforms and the different user preference (e.g., Android is more customized than iOS), app testing cases on these three platforms should also be designed differently. Comprehensive app testing can be time-consuming for developers. Therefore, understanding the differences of the app issues on these platforms can facilitate the testing process. In this paper, we propose a novel framework named CrossMiner to analyze the essential app issues and explore whether the app issues exhibit differently on the three platforms. Based on five million user reviews, the framework automatically captures the distributions of seven app issues, i.e., ""battery"", ""crash"", ""memory"", ""network"", ""privacy"", ""spam"", and ""UI"". We discover that the apps for different platforms indeed generate different issue distributions, which can be employed by app developers to schedule and design the testing cases. The verification based on the official user forums also demonstrates the effectiveness of our framework. Furthermore, we also identify that the issues related to ""crash"" and ""network"" are more concerned by users than the other issues on these three platforms. To assist developers in gaining a deep insight on the user issues, we also prioritize the user reviews corresponding to the issues. Overall, we aim at understanding the differences of issues on different platforms and facilitating the testing process for app developers.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774515,no
606,Functional stratification of biomarkers selected from microarray data for understanding oral leukoplakia associated carcinogenesis,"In this research paper, we have proposed a three tier conceptual framework AVISAR for the testing of Object Oriented Programs. We have devised the proposed framework into three levels: Requirements Modeling, Test Case Generation and Effort Estimation. At the first level, we have used the Extend relationship of the Use Case to capture the events generated by the Classes during message passing between them through their objects. These events were captured by an Object Instantiator using Event Templates and further given to a Test case scenario generator, which generates the test cases. At the second level, the test cases will be generated based on the event templates that are used for capturing the events generated by the Extend relationship of the Use cases. Further, we have proposed a Genetic Algorithm (GA) for the effective test case prioritization to ensure maximum code coverage. At the third level, effort estimation, we have used the Cyclometric Complexity and Token count to perform the effort estimation for the Software under Test (SUT).",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7915096,yes
607,Guiding Dynamic Symbolic Execution toward Unverified Program Executions,"Most techniques to detect program errors, such as testing, code reviews, and static program analysis, do not fully verify all possible executions of a program. They leave executions unverified when they do not check certain properties, fail to verify properties, or check properties under certain unsound assumptions such as the absence of arithmetic overflow. In this paper, we present a technique to complement partial verification results by automatic test case generation. In contrast to existing work, our technique supports the common case that the verification results are based on unsound assumptions. We annotate programs to reflect which executions have been verified, and under which assumptions. These annotations are then used to guide dynamic symbolic execution toward unverified program executions. Our main technical contribution is a code instrumentation that causes dynamic symbolic execution to abort tests that lead to verified executions, to prune parts of the search space, and to prioritize tests that cover more properties that are not fully verified. We have implemented our technique for the .NET static analyzer Clousot and the dynamic symbolic execution tool Pex. It produces smaller test suites (by up to 19.2%), covers more unverified executions (by up to 7.1%), and reduces testing time (by up to 52.4%) compared to combining Clousot and Pex without our technique.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886899,no
612,Improving Testing in an Enterprise SOA with an Architecture-Based Approach,"High resource demand for system testing is a major obstacle for continuous delivery. This resource demand can be reduced by prioritizing test cases, e.g., by focusing on tests that cover a lot of functionality. For large-scale systems, like an enterprise SOA, defining such test cases can be difficult for the tester because of the lack of relevant knowledge about the system. We propose an approach for test case prioritization and selection that is based on architectural viewpoint that provides software testers with the required architectural information. We outline how architectural information is used for defining and selecting prioritized test cases. The approach has been developed in close cooperation with the provider of an enterprise SOA in the banking domain in Austria following an action research approach. In addition, the approach has been validated in an industrial case study. Validation showed that there is no further need for manual architectural analysis to be able to prioritize and select test cases. We also show the limitations of our approach as it is based on static code analysis.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516833,no
613,Improvising the effectiveness of test suites using differential evolution technique,"The process of testing any software system is an atrocious task which indeed consumes a ton of effort, and expensive also. Required effort and time to do adequate as well as effective testing get bigger, as the software gets more complexed that can lead to swarm over the project budget or some test cases left uncovered or delay in completion. A suitably generated test suite does not only locate errors but also aid in reducing cost investment associated with the testing process. This paper implements an optimizing technique called as Differential Evolution to improve the effectiveness of test cases using Average Percentage of Fault Detection (APFD) metric. APFD is taken as the fitness function which is to be optimized. In this work, We have performed comparison of our approach with other existing prioritizing approaches and Experimental computations show that Differential Evolution technique achieve better APFD values than other techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7784924,yes
616,MACKE: Compositional analysis of low-level vulnerabilities with symbolic execution,"Concolic (concrete+symbolic) execution has recently gained popularity as an effective means to uncover non-trivial vulnerabilities in software, such as subtle buffer overflows. However, symbolic execution tools that are designed to optimize statement coverage often fail to cover potentially vulnerable code because of complex system interactions and scalability issues of constraint solvers. In this paper, we present a tool (MACKE) that is based on the modular interactions inferred by static code analysis, which is combined with symbolic execution and directed inter-procedural path exploration. This provides an advantage in terms of statement coverage and ability to uncover more vulnerabilities. Our tool includes a novel feature in the form of interactive vulnerability report generation that helps developers prioritize bug fixing based on severity scores. A demo of our tool is available at https://youtu.be/icC3jc3mHEU.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582815,no
617,Minimum complexity APP prioritization by bandwidth apportioning in smart phones,"The volume of best effort traffic is exploded by rapid adoption of peer-to-peer and content applications. Smart phone consumers are spending more times on applications which include video and music streaming, playing games, video chatting, social media like uploading photos to Facebook, Twitter etc. Many such applications are always running in background and sometimes come in foreground based on user preferences. In this work we propose an approach to improve the user experience by giving more bandwidth to preferred applications. We describe a preliminary model explaining our technique in detail. Further, we validate our proposal using real time test setup with Wireshark traffic analyzer, and results are detailed with respect to (1) Percentage of network share (2) Jitter experience and (3) Time taken for the algorithm to adapt. Proposed algorithm has been tested in two different platforms such as Android and Tizen. Our preliminary observations show that our proposed algorithm allocates more bandwidth to high priority applications while maintaining the low priority APPs are intact with above minimum bandwidth. Our approach gives users better jitter free experience for video streaming (high priority) applications in both Android (KitKat) and Tizen (Z1) platforms.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7564972,no
621,Multi-objective test report prioritization using image understanding,"In crowdsourced software testing, inspecting the large number of test reports is an overwhelming but inevitable software maintenance task. In recent years, to alleviate this task, many text-based test-report classification and prioritization techniques have been proposed. However in the mobile testing domain, test reports often consist of more screenshots and shorter descriptive text, and thus text-based techniques may be ineffective or inapplicable. The shortage and ambiguity of natural-language text information and the well defined screenshots of activity views within mobile applications motivate our novel technique based on using image understanding for multi-objective test-report prioritization. In this paper, by taking the similarity of screenshots into consideration, we present a multi-objective optimization-based prioritization technique to assist inspections of crowdsourced test reports. In our technique, we employ the Spatial Pyramid Matching (SPM) technique to measure the similarity of the screenshots, and apply the natural-language processing technique to measure the distance between the text of test reports. Furthermore, to validate our technique, an experiment with more than 600 test reports and 2500 images is conducted. The experimental results show that image-understanding techniques can provide benefit to test-report prioritization for most applications.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582758,no
623,Neuro-fuzzy based approach to event driven software testing: A new opportunity,"Event Driven Software (EDS) testing is a very challenging task as a large number of events can be invoked by users. So far it is difficult to test all the user inputs invoked, therefore, test case prioritization is essentially required for giving more priority to test cases which reveal higher faults comparatively. We have proposed test case prioritization for EDS: as the Event Type, Interaction of Event, and Coverage of Event. Priority assigned in the proposed model uses these factors in Adaptive Neuro-Fuzzy Inference System (ANFIS) MATLAB Toolbox based on Neuro-Fuzzy logic model. Evaluation and validation will be done using Average Percentage of Fault Detection (APFD). APFD rate for prioritized sequence using the proposed Neuro-Fuzzy logic model exhibited 81% rate, whereas, non-prioritized test sequences showed70% suggesting, thereby, that after prioritization; rate of fault detection has improved considerably. Data shows that proposed Neuro-Fuzzy logic model is apt for Test Case Prioritization of EDS Testing.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975349,yes
627,Prioritization techniques in combinatorial testing: A survey,"Prioritization techniques have become an indispensable part of the software testing process. They are highly beneficial either due to resource or time constraints or when tester cannot execute the complete test set. Combinatorial test sets are generated with the aim to cover all the possible t-way interactions. In this paper, techniques to prioritize t-way test sets are studied. A comparative study is done of prioritization criteria used by researchers to prioritize t-way test sets. Different evaluation methods used by them are also discussed.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975314,yes
628,Prioritizing Interaction Test Suites Using Repeated Base Choice Coverage,"Combinatorial interaction testing is a well-studied testing strategy that aims at constructing an effective interaction test suite (ITS) of a specific generation strength to identify interaction faults caused by the interactions among factors. Due to limited testing resources in practice, for example in combinatorial interaction regression testing, interaction test suite prioritization (ITSP) has been proposed to improve the efficiency of testing. An intuitive ITSP strategy that has been widely used in practice is fixed-strength interaction coverage based prioritization (FICBP). FICBP makes use of a property of the ITS: interaction coverage at a fixed prioritization strength. However, a challenge facing FICBP is that, when the ITS is large, the prioritization cost can be very high. In this paper, we propose a new FICBP method that, by repeatedly using base choice coverage (i.e., one-wise coverage) during the prioritization process, improves testing efficiency while maintaining testing effectiveness. The empirical studies show that our method has fault detection capability comparable to current FICBP methods, but obtains more stable results in many cases. Additionally, our method requires considerably less prioritization time than other FICBP methods at different prioritization strengths.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552005,yes
629,Product feature prioritization using the Hidden Structure method: A practical case at Ericsson,"In this paper, we present a case were we employ the Hidden Structure method to product feature prioritization at Ericsson. The method extends the more common Design Structure Matrix (DSM) approach that has been used in technology management (e.g. project management and systems engineering) for quite some time in order to model complex systems and processes. The hidden structure method focuses on analyzing a DSM based on coupling and modularity theory, and it has been used in a number of software architecture and software portfolio cases. In previous work by the authors the method was tested on organization transformation at Ericsson, however this is the first time it has been employed in the domain of product feature prioritization. Today, at Ericsson, features are prioritized based on a business case approach where each feature is handled isolated from other features and the main focus is customer or market-based requirements. By employing the hidden structure method we show that features are heavily dependent on each other in a complex network, thus they should not be treated as isolated islands. These dependencies need to be considered when prioritizing features in order to save time and money, as well as increase end customer satisfaction.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7806519,no
635,Targeted Scrum: Applying Mission Command to Agile Software Development,"Software engineering and mission command are two separate but similar fields, as both are instances of complex problem solving in environments with ever changing requirements. Our research hypothesis is that modifications to agile software development based on inspirations from mission command can improve the software engineering process in terms of planning, prioritizing, and communication of software requirements and progress, as well as improving the overall software product. Targeted Scrum is a modification of Traditional Scrum based on three inspirations from Mission Command: End State, Line of Effort, and Targeting. These inspirations have led to the introduction of the Product Design Meeting and modifications of some current Scrum meetings and artifacts. We tested our research hypothesis using a semester-long undergraduate level software engineering class. Students developed two software projects, one using Traditional Scrum and the other using Targeted Scrum. We then assessed how well both methodologies assisted the software development teams in planning and developing the software architecture, prioritizing requirements, and communicating progress. We also evaluated the software product produced by both methodologies. We found that Targeted Scrum did better in assisting the software development teams in the planning and prioritization of the requirements. However, Targeted Scrum had a negligible effect on improving the software development teams external and internal communications. Finally, Targeted Scrum did not have an impact on the product quality by the top performing and worst performing teams. Targeted Scrum did assist the product quality of the teams in the middle of the performance spectrum.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7296686,no
644,Test Effectiveness Evaluation of Prioritized Combinatorial Testing: A Case Study,"Combinatorial testing is a widely-used technique to detect system interaction failures. To improve test effectiveness with given priority weights of parameter values in a system under test, prioritized combinatorial testing constructs test suites where highly weighted parameter values appear earlier or more frequently. Such order-focused and frequency-focused combinatorial test generation algorithms have been evaluated using metrics called weight coverage and KL divergence but not sufficiently with fault detection effectiveness so far. We evaluate the fault detection effectiveness on a collection of open source utilities, applying prioritized combinatorial test generation and investigating its correlation with weight coverage and KL divergence.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589785,yes
645,Testing and Debugging in Continuous Integration with Budget Quotas on Test Executions,"In Continuous Integration, a software application is developed through a series of development sessions, each with limited time allocated to testing and debugging on each of its modules. Test Case Prioritization can help execute test cases with higher failure estimate earlier in each session. When the testing time is limited, executing such prioritized test cases may only produce partial and prioritized execution coverage data. To identify faulty code, existing Spectrum-Based Fault Localization techniques often use execution coverage data but without the assumption of execution coverage priority. Is it possible to decompose these two steps for optimization within individual steps? In this paper, we study to what extent the selection of test case prioritization techniques may reduce its influence on the effectiveness of spectrum-based fault localization, thereby showing the possibility to decompose the process of continuous integration for optimization in workflow steps. We present a controlled experiment using the Siemens suite as subjects, nine test case prioritization techniques and four spectrum-based fault localization techniques. The findings showed that the studied test cases prioritization and spectrum-based fault localization can be customized separately, and, interestingly, prioritization over a smaller test suite can enable spectrum-based fault localization to achieve higher accuracy by assigning faulty statements with higher ranks.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589824,yes
646,Test-Suite Prioritisation by Application Navigation Tree Mining,"Software tend to evolve over time and so does the test-suite. Regression testing is aimed at assessing that the software evolution did not compromise the working of the existing software components. However, as the software and consequently the test-suite grow in size, the execution of the entire test-suite for each new build becomes infeasible. Techniques like test-suite selection, test-suite minimisation and test-suite prioritisation have been proposed in literature for regression testing. Whilst all of these techniques are essentially an attempt to reduce the testing effort, test-suite selection and minimisation reduce the test-suite size whereas test-suite prioritisation provides a priority order of the test cases without changing the test-suite size. In this work, we focus on test-suite prioritisation. Recently, techniques from data mining have been used for test-suite prioritisation which consider the frequent pairs of interaction among the application interaction patterns. We propose test-Suite prioritisation by Application Navigation Tree mining (t-SANT). First, we construct an application navigation tree by way of extracting both tester and user interaction patterns. Next, we extract frequent sequences of interaction using a sequence mining algorithm inspired from sequential pattern mining. The most frequent longest sequences are assumed to model complex and most frequently used work-flows and hence a prioritisation algorithm is proposed that prioritises the test cases based on the most frequent and longest sequences. We show the usefulness of the proposed scheme with the help of two case studies, an online book store and calculator.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7866754,yes
650,Tolerance to complexity: Measuring capacity of development teams to handle source code complexity,"A well defined testing strategy is essential for any software development project. Testing efforts need to be carefully planed and executed in order to ensure effectiveness. Programming failures can represent a high risk for business. In order to mitigate such risk, companies have been increasingly investing more resources on software testing. In despite of massive investments on software testing and extensive collection of static analysis techniques and tools, there are still few conclusive explanations for what causes human programming failures on software. The hypothesis investigated in this paper is that a metric based on development teams characteristics can be more effective to predict defective source code than metrics purely focused on information about source code, alone. Aiming to assist software engineers during testing initiatives, this article presents a new approach to systematically measure capacity of development teams to handle source code complexity. The proposed metric can be effective for raising information and comparing multiple development teams, planning training initiatives and prioritising testing efforts. Experiments were carried out with the entire source code base of device drivers for Linux Operating System. Our approach was able to predict, with 80% of accuracy rate, which development teams introduced more issues from 2010 to 2014.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844689,no
655,Visualization of combinatorial models and test plans,"Combinatorial test design (CTD) is an effective and widely used test design technique. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. One challenge for successful application of CTD in practice relates to this manual model definition and maintenance process. Another challenge relates to the comprehension and use of the test plan generated by CTD for prioritization purposes. In this work we introduce the use of visualizations as a means to address these challenges. We apply three different forms of visualization, matrices, graphs, and treemaps, to visualize the relationships between the different elements of the model, and to visualize the strength of each test in the test plan and the relationships between the different tests in terms of combinatorial coverage. We evaluate our visualizations via a user survey with 19 CTD practitioners, as well as via two industrial projects in which our visualization was used and allowed test designers to get vital insight into their models and into the coverage provided through CTD generated test plans.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582753,no
658,A Framework for Combining and Ranking Static Analysis Tool Findings Based on Tool Performance Statistics,"This paper proposes a conceptual, performance-based ranking framework that prioritises the output of multiple Static Analysis Tools, to improve the tool effectiveness and usefulness. The framework weights the performance of Static Analysis Tools per defect type and cross-validates the findings between different Static Analysis Tools' reports. An initial validation shows the potential benefits of the proposed framework.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004389,no
660,A Greedy-Based Method for Modified Condition/Decision Coverage Testing Criterion,"During software regression testing, the code coverage of target program is a crucial factor while we perform test case reduction and prioritization. Modified Condition/ Decision Coverage (MC/DC) is one of the most strict and high-accuracy criterion in code coverage and it is usually considered necessary for adequate testing of critical software. In the past, Hayhurst et al proposed a method to implement the MC/DC criterion that complies with regulatory guidance for DO-178B level A software. Hayhurst's MC/DC approach was to find some test cases which are satisfied by MC/DC criterion for each operator (and, or, not, or xor) in the Boolean expression. However, there could be some problems when using Hayhurst's MC/DC approach to select test cases. In this paper, we discuss how to improve and/or enhance Hayhurst's MC/DC approach by using a greedy-based method. Some experiments are performed based on real programs to evaluate as well as compare the performance of our proposed and Hayhurst's approaches.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8069087,no
666,Adapting code maintainability to bat-inspired test case prioritization,"Time and budget constraints in developing a software create an adverse effect in terms of the adequacy of maintenance and test processes. This case can be considered as a burden for persons who account for test processes. In order to alleviate this burden, test case prioritization is one of the solutions. A nature-inspired method namely BITCP, which was developed based on bat algorithm, produced promising results. However, this method does not involve test case elements with respect to the code maintainability. In this work, the correlation between some code maintainability indicators including WMC, LCOM, and Coupling and cyclomatic complexity is investigated. IMPBITCP appears after adapting the results of the investigation to BITCP. The method is then compared with well known alternatives such as greedy-search, particle swarm optimization, and BITCP. The experiment involving six open source project showed that IMPCBITCP outperformed the others with respect to the APFD. The findings of the work indicates that if the factors affecting code maintenance are considered while developing test case prioritization techniques, APFD results becomes high and stable.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001134,yes
673,An extended adaptive process model for agile software development methodology,"Agile methodologies focus on the agility for the development of software. Among various agile methods, eXtreme Programming (XP) is the most adopted agile method. XP has been used for the development of other agile methods, for example, ??adaptive software development process model? (ASDPM), which is the modified approach of XP. ASDPM was proposed to support the following activities: ??(a) communication and planning, (b) analysis, (c) design and development, and (d) testing?. Based on our literature review of ASDPM, we identify that ASDPM does not support the following: (i) how to identify the different types of agile team members who will participate during the communication and planning phase?; and (ii) how to deliver the most important requirements of the software during analysis phase?. Therefore, in order to address this issue and to strengthen the analysis phase of agile process models, in this paper we propose an extended adaptive process model (APM) for agile software development methodology. This method includes the following steps: (1) identification of agile team members, (2) communication and planning, (3) analysis includes the computation of function point of each requirement; and the selection and prioritization of the requirements, (4) design and development, (5) testing. Finally, the utilization of the proposed method is demonstrated with the help of Institute Examination System, as a case study.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342770,no
675,An Industrial Study of Natural Language Processing Based Test Case Prioritization,"In mobile application development, the frequentsoftware release limits the testing time resource. In order todetect bugs in early phases, researchers proposed various testcase prioritization (TCP) techniques in past decades. In practice, considering that some test case is described or contains text, theresearchers also employed Natural Language Processing (NLP)to assist the TCP techniques. This paper conducted an extensiveempirical study to analyze the performance of three NLP basedTCP technologies, which is based on 15059 test cases from 30industrial projects. The result shows that all of these threestrategies can help to improve the efficiency of software testing, and the Risk strategy achieved the best performance across thesubject programs.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7928016,yes
684,Cost-Effective Regression Testing Using Bloom Filters in Continuous Integration Development Environments,"Regression testing in continuous integration development environments must be cost-effective and should provide fast feedback on test suite failures to the developers. In order to provide faster feedback on failures to developers while using computing resources efficiently, two types of regression testing techniques have been developed: Regression Testing Selection (RTS) and Test Case Prioritization (TCP). One of the factors that reduces the effectiveness of the RTS and TCP techniques is the inclusion of test suites that fail only once over a period. We propose an approach based on Bloom filtering to exclude such test suites during the RTS process, and to assign such test suites with a lower priority during the TCP process. We experimentally evaluate our approach using a Google dataset, and demonstrate that cost-effectiveness of the proposed RTS and TCP techniques outperforms the state-of-the-art techniques.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305938,yes
685,Coverage-Based Reduction of Test Execution Time: Lessons from a Very Large Industrial Project,"There exist several coverage-based approaches to reduce time and resource costs of test execution. While these methods are well-investigated and evaluated for smaller to medium-size projects, we faced several challenges in applying them in the context of a very large industrial software project, namely SAP HANA. These issues include: varying effectiveness of algorithms for test case selection/prioritization, large amounts of shared (non-specific) coverage between different tests, high redundancy of coverage data, and randomness of test results (i.e. flaky tests), as well as of the coverage data (e.g. due to concurrency issues). We address these issues by several approaches. First, our study shows that compared to standard algorithms, so-called overlap-aware solvers can achieve up to 50% higher code coverage in a fixed time budget, significantly increasing the effectiveness of test case prioritization and selection. We also detected in our project high redundancy of line coverage data (up to 97%), providing opportunities for data size reduction. Finally, we show that removal of coverage shared by tests can significantly increase test specificity. Our analysis and approaches can help to narrow the gap between research and practice in context of coverage-based testing approaches, especially in case of very large software projects.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899023,no
686,d(mu)Reg: A Path-Aware Mutation Analysis Guided Approach to Regression Testing,"Regression testing re-runs some previously executed test cases, with the purpose of checking whether previously fixed faults have re-emerged and ensuring that the changes do not negatively affect the existing behaviors of the software under development. Today's software is rapidly developed and evolved, and thus it is critical to implement regression testing quickly and effectively. In this paper, we propose a novel technique for regression testing, based on a family of mutant selection strategies. The preliminary results show that the proposed technique can significantly improve the efficiency of different regression testing activities, including test case reduction and prioritization. Our work also makes it possible to develop a unified framework that effectively implements various activities in regression testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962333,no
687,Delta-Oriented Product Prioritization for Similarity-Based Product-Line Testing,"Testing every product of a software product line (SPL) is often not feasible due to the exponential number of products in the number of features. Thus, the order in which products are tested matters, because it can increase the early rate of fault detection. Several approaches have been proposed to prioritize products based on configuration similarity. However, current approaches are oblivious to solution-space differences among products, because they consider only problem-space information. With delta modeling, we incorporate solution-space information in product prioritization to improve the effectiveness of SPL testing. Deltas capture the differences between products facilitating the reasoning about product similarity. As a result, we select the most dissimilar product to the previously tested ones, in terms of deltas, to be tested next. We evaluate the effectiveness of our approach using an SPL from the automotive domain showing an improvement in the effectiveness of SPL testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968061,no
688,Deriving high-priority acceptance test cases using utility trees: A case study,"Even though software testing is considered a mature field in software engineering, deriving test cases is still an important issue and even more when related to quality requirements. Utility Trees are used to evaluate software architectures, organizing requirements as scenarios associated to quality attributes and decorating them with stakeholder-given priority and developer- given difficulty. In this article, we propose an approach to use Utility Trees to derive prioritized acceptance test cases allowing to focus in high-value tests. The technique has been tried in two medium-sized projects for a Chilean public agency, with positive results. This innovative use of Utility Trees offers a simple, collaborative way to focus testing resources.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8405142,no
692,Efficient Product-Line Testing Using Cluster-Based Product Prioritization,"A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962326,yes
694,Epistasis Based ACO for Regression Test Case Prioritization,"Metaheuristics that are inspired by natural systems have been widely applied into search-based software engineering. It has been shown that combining knowledge of the application domain with a biological theory for metaheuristics can narrow down the search space and speed up the convergence for metaheuristics based algorithms. This paper introduces Epistatic Test case Segment (ETS) for multiobjective search-based regression Test Case Prioritization (MoTCP), based on epistasis theory that reflects the correlation between genes in evolution process. An ETS-based pheromone update strategy for ant colony optimization (ACO) algorithm is proposed. The experiments with three benchmarks and a real industrial program V8 illustrate that proposed pheromone update strategy guided by the epistasis theory can significant improve the performance of ACO in terms of effectiveness and efficiency for search-based MoTCP.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7935487,yes
696,Exniffer: Learning to Prioritize Crashes by Assessing the Exploitability from Memory Dump,"An important component of software reliability is the assurance of certain security guarantees, such as absence of low-level bugs that may result in code exploitation, for example. A program crash is an early indicator of possible errors in the program like memory corruption, access violation or division by zero. In particular, a crash may indicate the presence of safety or security critical errors. A safety-error crash does not result in any exploitable condition, whereas a security-error crash allows an attacker to exploit a vulnerability. However, distinguishing one from the other is a non-trivial task. This exacerbates the problem in cases where we get hundreds of crashes and programmers have to make choices which crash to patch first! In this work, we present a technique to identify security critical crashes by applying machine learning on a set of features derived from core-dump files and runtime information obtained from hardware assisted monitoring such as the last branch record (LBR) register. We implement the proposed technique in a prototype called Exniffer. Our empirical results, obtained by experimenting Exniffer on several crashes on real-world applications show that proposed technique is able to classify a given crash as exploitable or not-exploitable with high accuracy.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305946,no
699,Formal Methods for Validation and Test Point Prioritization in Railway Signaling Logic,"The EN50128 Railway Safety Standard recommends the use of formal methods for proving the correctness of the yard-specific logic, which was developed for electronic signaling and interlocking systems. We present a tool flow, which consists of three components. The core component uses a novel method for automatically generating the relevant safety properties for a yard from its control table. The second component proves the validity of the properties on the application logic by using a new theory of invariant checking. The third component leverages the suite of formal properties to prioritize site acceptance test points. Experimental results are presented on real application data for the yards in India that are demonstrating the performance of the proposed methods.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529152,no
704,Improving the Cooperation of Fuzzing and Symbolic Execution by Test-cases Prioritizing,"Nowadays much attention is paid to the threat of vulnerabilities on the software security. Fuzzing and symbolic execution, complementary to each other, are two effective techniques in software testing. In this paper, we develop the prototype called FAS(Fuzzing and Symbolic) for software testing under both fuzzing and symbolic execution. In our method, the test cases are prioritized through deep-oriented strategy and large-distance-first strategy, in order to get a higher path-coverage with the condition of limited resource.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288548,yes
705,Knowledge Transfer for Global Roles in GSE,"This practice paper presents how a software engineering organization spread across three countries successfully transferred the knowledge of a few identified roles for a large mission-critical software system that had to conform to regulatory requirements. Multiple releases of the system have been delivered to customers over the 15 years it has been in the market. Each release of the product had a focus area. The competence availability for these focus areas was distributed. As a natural evolution of the globally distributed team, greater responsibility is devolved to a particular location, based on the availability of the competence at that location. Moving the increased responsibility to a location, created a global role, which did not exist earlier. Building the new role required a new skill, what is unique about a global role. Equipping the team members in the new skill was necessary to take up the roles effectively and quickly. The first step was the identification of the competence for a function/role, training for which may be imparted to another person, who will take over the function/role. This is followed by a process of knowledge transfer, which ensured that a person can take up a new global role from another location. Prioritization based on ease of knowledge transfer for different areas of work, that was found to be effective is described. This helped reduce possible problems that could occur due to incorrect or incomplete transfer of knowledge. The advantages by such knowledge transfer that resulted in new persons taking up global roles have outweighed its disadvantages. The practices described are generic and can be applied to any organization of similar size and complexity.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976692,no
707,Learning to Prioritize Test Programs for Compiler Testing,"Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985706,yes
723,Test Optimization from Release Insights: An Analytical Hierarchy Approach,"Software Testing is an essential aspect to ensure software quality, reliability and consistent user experience. Digital applications such as mobile app usually follow rapid software delivery which consists of various releases. It typically uses insights from the development data such as defects, test logs for test execution optimization. Once the application is released and deployed, there is rich availability of untapped heterogeneous data which can also be effectively utilized for the next release test execution optimization. The data from the release includes direct customer feedback, application monitoring data such as user behavioral traces, device usages, release logs. In this position paper, we discuss about the various data sources and the multiple insights which can be derived from them. We also propose a framework which uses Analytical Hierarchy Process to prioritize the tests based on these insights available from the release data. The framework also recommends the prioritized and missed device configurations for next release test planning.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967947,yes
727,The Significant Effects of Data Sampling Approaches on Software Defect Prioritization and Classification,"Context: Recent studies have shown that performance of defect prediction models can be affected when data sampling approaches are applied to imbalanced training data for building defect prediction models. However, the magnitude (degree and power) of the effect of these sampling methods on the classification and prioritization performances of defect prediction models is still unknown. Goal: To investigate the statistical and practical significance of using resampled data for constructing defect prediction models. Method: We examine the practical effects of six data sampling methods on performances of five defect prediction models. The prediction performances of the models trained on default datasets (no sampling method) are compared with that of the models trained on resampled datasets (application of sampling methods). To decide whether the performance changes are significant or not, robust statistical tests are performed and effect sizes computed. Twenty releases of ten open source projects extracted from the PROMISE repository are considered and evaluated using the AUC, pd, pf and G-mean performance measures. Results: There are statistical significant differences and practical effects on the classification performance (pd, pf and G-mean) between models trained on resampled datasets and those trained on the default datasets. However, sampling methods have no statistical and practical effects on defect prioritization performance (AUC) with small or no effect values obtained from the models trained on the resampled datasets. Conclusions: Existing sampling methods can properly set the threshold between buggy and clean samples, while they cannot improve the prediction of defect-proneness itself. Sampling methods are highly recommended for defect classification purposes when all faulty modules are to be considered for testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170123,no
729,TITAN: Test Suite Optimization for Highly Configurable Software,"Exhaustive testing of highly configurable software developed in continuous integration is rarely feasible in practice due to the configuration space of exponential size on the one hand, and strict time constraints on the other. This entails using selective testing techniques to determine the most failure-inducing test cases, conforming to highly-constrained time budget. These challenges have been well recognized by researchers, such that many different techniques have been proposed. In practice, however, there is a lack of efficient tools able to reduce high testing effort, without compromising software quality. In this paper we propose a test suite optimization technology TITAN, which increases the time-and cost-efficiency of testing highly configurable software developed in continuous integration. The technology implements practical test prioritization and minimization techniques, and provides test traceability and visualization for improving the quality of testing. We present the TITAN tool and discuss a set of methodological and technological challenges we have faced during TITAN development. We evaluate TITAN in testing of Cisco's highly configurable software with frequent high quality releases, and demonstrate the benefit of the approach in such a complex industry domain.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7928010,yes
733,Value-Based Decision-Making Using a Web-Based Tool: A Multiple Case Study,"[Context]: To remain competitive, innovative and to grow, companies should use a value-based decision-making where decisions are the best for that company's overall value creation. However, without tool support, the use of explicit value propositions and aggregation of different key stakeholders' decisions during decision-making may be a challenge for many companies. [Goal]: The goal of this paper is to investigate the extent to which a Web-based tool for value-based decision-making can successfully support stakeholders' decision-making process. [Method]: We conducted three case studies across four software projects, during six weeks, in the contexts of feature selection, test cases execution prioritization and user interfaces design selection. Prior to using the tool, stakeholders' value propositions were elicited via focus-group meetings; later, during a post-mortem phase, data was gathered via observation, semi-structured interviews and structured questionnaires. [Results]: Participants reported an improvement of their decision-making process and quality of decisions; further, they also felt confident about using the tool, and that it can be useful to their work. [Conclusions]: Results suggested that the use of tool support by the stakeholders in the investigated company for value-based decision-making improved their decision-making process and the quality of decisions.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305950,no
734,Weighting for Combinatorial Testing by Bayesian Inference,"Combinatorial testing (CT) is a widely-used technique to detect system interaction failures. To improve the test effectiveness of CT, prioritized combinatorial testing inputs priority weights of parameter-values, and generates combinatorial test suites based on the weights. This paper proposes a method to automatically determine the weights of parameter-values by Bayesian inference using previous testing results. Using two open source projects, we evaluate the fault detection effectiveness of the proposed weighting based prioritized combinatorial testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899090,yes
736,Aggregation process for implementation of application security management based on risk assessment,This article is devoted to the review and analysis of existing methods of ensuring information security based on risk models. The strengths and weaknesses of the model are investigated on the basis of reliability theory. The article discusses potential obstacle to managing application security effectively and describes five steps for managing security. Create inventory of application and their attributes and evaluating their role in business impact (Create a profile for each application and conduction analysis of date processed in the application). Software vulnerability search (Static Analysis (??white-box?); Dynamic Analysis (??black-box?); Interactive Analysis (??glass-box?); Mobile Application Analysis); Risk assessment and prioritization of vulnerabilities (Setting priorities for applications; Setting priorities for types of vulnerabilities; Setting priorities for the development team; Changing vulnerability priorities and reassessing risks). Elimination of vulnerabilities and minimization of risks (security manager sets priorities and firmed tasks for the development team.,2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8317039,no
740,BP: Profiling Vulnerabilities on the Attack Surface,"Security practitioners use the attack surface of software systems to prioritize areas of systems to test and analyze. To date, approaches for predicting which code artifacts are vulnerable have utilized a binary classification of code as vulnerable or not vulnerable. To better understand the strengths and weaknesses of vulnerability prediction approaches, vulnerability datasets with classification and severity data are needed. The goal of this paper is to help researchers and practitioners make security effort prioritization decisions by evaluating which classifications and severities of vulnerabilities are on an attack surface approximated using crash dump stack traces. In this work, we use crash dump stack traces to approximate the attack surface of Mozilla Firefox. We then generate a dataset of 271 vulnerable files in Firefox, classified using the Common Weakness Enumeration (CWE) system. We use these files as an oracle for the evaluation of the attack surface generated using crash data. In the Firefox vulnerability dataset, 14 different classifications of vulnerabilities appeared at least once. In our study, 85.3% of vulnerable files were on the attack surface generated using crash data. We found no difference between the severity of vulnerabilities found on the attack surface generated using crash data and vulnerabilities not occurring on the attack surface. Additionally, we discuss lessons learned during the development of this vulnerability dataset.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543394,no
744,Context-Aware Patch Generation for Better Automated Program Repair,"The effectiveness of search-based automated program repair is limited in the number of correct patches that can be successfully generated. There are two causes of such limitation. First, the search space does not contain the correct patch. Second, the search space is huge and therefore the correct patch cannot be generated (ie correct patches are either generated after incorrect plausible ones or not generated within the time budget). To increase the likelihood of including the correct patches in the search space, we propose to work at a fine granularity in terms of AST nodes. This, however, will further enlarge the search space, increasing the challenge to find the correct patches. We address the challenge by devising a strategy to prioritize the candidate patches based on their likelihood of being correct. Specifically, we study the use of AST nodes' context information to estimate the likelihood. In this paper, we propose CapGen, a context-aware patch generation technique. The novelty which allows CapGen to produce more correct patches lies in three aspects: (1) The fine-granularity design enables it to find more correct fixing ingredients; (2) The context-aware prioritization of mutation operators enables it to constrain the search space; (3) Three context-aware models enable it to rank correct patches at high positions before incorrect plausible ones. We evaluate CapGen on Defects4J and compare it with the state-of-the-art program repair techniques. Our evaluation shows that CapGen outperforms and complements existing techniques. CapGen achieves a high precision of 84.00% and can prioritize the correct patches before 98.78% of the incorrect plausible ones.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453055,no
745,DevOps Improvements for Reduced Cycle Times with Integrated Test Optimizations for Continuous Integration,"DevOps, as a growing development practice that aims to enable faster development and efficient deployment of applications without compromising on quality, is often hampered by long cycle times. One contributing factor to long cycle times in DevOps is long build time. Automated testing in continuous integration is one of the build stages that is highly prone to long run-time due to software complexity and evolution, and inefficient due to unoptimized testing approaches. To be cost-effective, testing in continuous integration needs to use only a fast-running set of comprehensive tests that are able to ensure the level of quality needed for deployment to production. Known approaches use time-aware test selection methods to improve time-efficiency of continuous integration testing by providing optimized combinations and order of tests with respect to decreased run-time. However, focusing on time-efficiency as the sole criterion in DevOps often jeopardizes the quality of software deliveries. This paper proposes a technique that integrates fault-based and risk-based test selection and prioritization optimized for low run-time, to improve time-effectiveness of continuous integration testing, and thus reduce long cycle times in DevOps, without compromising on quality. The technique has been evaluated in testing of a large-scale configurable software in continuous integration, and has shown considerable improvement over industry practice with respect to time-efficiency.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377636,yes
751,Functional Dependency Detection for Integration Test Cases,"This paper presents a natural language processing (NLP) based approach that, given software requirements specification, allows the functional dependency detection between integration test cases. We analyze a set of internal signals to the implemented modules for detecting dependencies between requirements and thereby identifying dependencies between test cases such that: module 2 depends on module 1 if an output internal signal from module 1 enters as an input internal signal to the module 2. Consequently, all requirements (and thereby test cases) for module 2 are dependent on all the designed requirements (and test cases) for module 1. The dependency information between requirements (and thus corresponding test cases) can be utilized for test case prioritization and scheduling. We have implemented our approach as a tool and the feasibility is evaluated through an industrial use case in the railway domain at Bombardier Transportation (BT), Sweden.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8431975,no
754,Identifying Technical Debt in Database Normalization Using Association Rule Mining,"In previous work, we explored a new context of technical debt that relates to database normalization design decisions. We claimed that database normalization debts are likely to be incurred for tables below the fourth normal form. We proposed a method to prioritize the tables that should be normalized based on their impact on data quality and performance. In this study, we propose a framework to identify normalization debt items (i.e. tables below the fourth normal form) by mining the data stored in each table. Our framework makes use of association rule mining to discover functional dependencies between attributes in a table, which will help determine the current normal form of that table and reveal debt tables. To illustrate our method, we use a case study from Microsoft, AdventureWorks database. The results revealed the applicability of our framework to identify debt tables.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8498244,no
755,Industrially Applicable System Regression Test Prioritization in Production Automation,"When changes are performed on an automated production system (aPS), new faults can be accidentally introduced into the system, which are called regressions. A common method for finding these faults is regression testing. In most cases, this regression testing process is performed under high time pressure and onsite in a very uncomfortable environment. Until now, there has been no automated support for finding and prioritizing system test cases regarding the fully integrated aPS that are suitable for finding regressions. Thus, the testing technician has to rely on personal intuition and experience, possibly choosing an inappropriate order of test cases, finding regressions at a very late stage of the test run. Using a suitable prioritization, this iterative process of finding and fixing regressions can be streamlined and a lot of time can be saved by executing test cases likely to identify new regressions earlier. Thus, an approach is presented in this paper that uses previously acquired runtime data from past test executions and performs a change identification and impact analysis to prioritize test cases that have a high probability to unveil regressions caused by side effects of a system change. The approach was developed in cooperation with reputable industrial partners active in the field of aPS engineering, ensuring a development in line with industrial requirements. An industrial case study and an expert evaluation were performed, showing promising results.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320514,yes
759,Learning to Accelerate Compiler Testing,"Compilers are one of the most important software infrastructures. Compiler testing is an effective and widely-used way to assure the quality of compilers. While many compiler testing techniques have been proposed to detect compiler bugs, these techniques still suffer from the serious efficiency problem. This is because these techniques need to run a large number of randomly generated test programs on the fly through automated test-generation tools (e.g., Csmith). To accelerate compiler testing, it is desirable to schedule the execution order of the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. Since different test programs tend to trigger the same compiler bug, the ideal goal of accelerating compiler testing is to execute the test programs triggering different compiler bugs in the beginning. However, such perfect goal is hard to achieve, and thus in this work, we design four steps to approach the ideal goal through learning, in order to largely accelerate compiler testing.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449630,no
762,MS-guided many-objective evolutionary optimisation for test suite minimisation,"Test suite minimisation is a process that seeks to identify and then eliminate the obsolete or redundant test cases from the test suite. It is a trade-off between cost and other value criteria and is appropriate to be described as a many-objective optimisation problem. This study introduces a mutation score (MS)-guided many-objective optimisation approach, which prioritises the fault detection ability of test cases and takes MS, cost and three standard code coverage criteria as objectives for the test suite minimisation process. They use six classical evolutionary many-objective optimisation algorithms to identify efficient test suite, and select three small programs from the Software-Artefact Infrastructure Repository (SIR) and two larger program space and gzip for experimental evaluation as well as statistical analysis. The experiment results of the three small programs show non-dominated sorting genetic algorithm II (NSGA-II) with tuning was the most effective approach. However, MOEA/D-PBI and MOEA/D-WS outperform NSGA-II in the cases of two large programs. On the other hand, the test cost of the optimal test suite obtained by their proposed MS-guided many-objective optimisation approach is much lower than the one without it in most situation for both small programs and large programs.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8572623,no
766,Poster: Identification of Methods with Low Fault Risk,"Test resources are usually limited and therefore it is often not possible to completely test an application before a release. Therefore, testers need to focus their activities on the relevant code regions. In this paper, we introduce an inverse defect prediction approach to identify methods that contain hardly any faults. We applied our approach to six Java open-source projects and show that on average 31.6% of the methods of a project have a low fault risk; they contain in total, on average, only 5.8% of all faults. Furthermore, the results suggest that, unlike defect prediction, our approach can also be applied in cross-project prediction scenarios. Therefore, inverse defect prediction can help prioritize untested code areas and guide testers to increase the fault detection probability.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449591,no
767,Practical Test Dependency Detection,"Regression tests should consistently produce the same outcome when executed against the same version of the system under test. Recent studies, however, show a different picture: in many cases simply changing the order in which tests execute is enough to produce different test outcomes. These studies also identify the presence of dependencies between tests as one likely cause of this behavior. Test dependencies affect the quality of tests and of the correlated development activities, like regression test selection, prioritization, and parallelization, which assume that tests are independent. Therefore, developers must promptly identify and resolve problematic test dependencies. This paper presents PRADET, a novel approach for detecting problematic dependencies that is both effective and efficient. PRADET uses a systematic, data-driven process to detect problematic test dependencies significantly faster and more precisely than prior work. PRADET scales to analyze large projects with thousands of tests that existing tools cannot analyze in reasonable amount of time, and found 27 previously unknown dependencies.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367031,no
768,Prioritization of Metamorphic Relations Based on Test Case Execution Properties,"A test oracle is essential for software testing. In certain complex systems, it is hard to distinguish between correct and incorrect behavior. Metamorphic testing is one of the solution to solve the test oracle problem. In metamorphic testing, metamorphic relations (MRs) are derived based on the properties exhibited by the program under test (PUT). These MRs play a major role in the generation of test data for conducting MT. The effectiveness of MRs can be determined based on the ability to detect considerable faults for the given PUT. Many metamorphic relations with different fault finding capability can be used to test the PUT and it is important to identify and prioritize the MRs based on its fault finding effectiveness. In order to answer this challenge, we propose to prioritize the MRs based on the diversity in the execution path of the source and follow-up test cases of the MRs. We propose four metrics to capture different levels of diversity in the execution behavior of the test cases for each of the derived MRs. The total weight calculated for each of the MRs using the metrics is used to prioritize the MRs.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539189,no
782,Varying defect prediction approaches during project evolution: A preliminary investigation,"Defect prediction approaches use various features of software product or process to prioritize testing, analysis and general quality assurance activities. Such approaches require the availability of project's historical data, making them inapplicable in early phase. To cope with this problem, researchers have proposed cross-project and even cross-company prediction models, which use training material from other projects to build the model. Despite such advances, there is limited knowledge of how, as the project evolves, it would be convenient to still keep using data from other projects, and when, instead, it might become convenient to switch towards a local prediction model. This paper empirically investigates, using historical data from four open source projects, on how the performance of various kinds of defect prediction approaches - within-project prediction, local and global cross-project prediction, and mixed (injected local cross) prediction - varies over time. Results of the study are part of a long-term investigation towards supporting the customization of defect prediction models over projects' history.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368451,no
