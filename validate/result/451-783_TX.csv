ID,Document Title,Abstract,Year,PDF Link,code
451,A distributed algorithm for virtual traffic lights with IEEE 802.11p,"A virtual traffic light (VTL) is a mechanism that allows vehicles to autonomously solve priorities at road junctions in the absence of fixed infrastructures (i.e., conventional traffic lights). To develop an effective VTL system, communication between vehicles is a crucial factor and can be handled either using cellular infrastructure or adopting a vehicle-to-vehicle (V2V) communication paradigm. In this paper we present the design, the implementation, and the field trial of a VTL which exploits V2V communications based on IEEE 802.11p. Specif-ically, we propose a decentralized algorithm, that adopts both broadcast signaling and unicast messages to assign priorities to the vehicles approaching intersections, thus preventing accidents and reducing traffic congestions. The algorithm has been tested both in a controlled laboratory environment and in a field trial with equipped vehicles.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6882621,yes
452,A hierarchical test case prioritization technique for object oriented software,"Software reuse is the use of existing artifacts to create new software. Inheritance is the foremost technique of reuse. But the inherent complexity due to inheritance hierarchy found in object - oriented paradigm also affect testing. Every time any change occurs in the software, new test cases are added in addition to the existing test suite. So there is need to conduct effective regression testing having less number of test cases to reduce cost and time. In this paper a hierarchical test case prioritization technique is proposed wherein various factors have been considered that affect error propagation in the inheritance. In this paper prioritization of test cases take place at two levels. In the first level the classes are prioritized and in the second level the test cases of prioritized classes are ordered. To show the effectiveness of proposed technique it was applied and analyze on a C++ program.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019794,yes
453,A Method to Test the Information Quality of Technical Documentation on Websites,"In software engineering, testing is one of the corner-stones of quality assurance. The idea of software testing can be applied to information quality as well. Technical documentation has a set of intended uses that correspond to use cases in a software system. Documentation is, in many cases, presented via software systems, e.g., web servers and browsers, and contains software, e.g., Javascript for user interaction, animation, and customization, etc. This makes it difficult to find a clear-cut distinction between a software system and technical documentation. However, we can assume that each use case of a technical documentation involves retrieval of some sort of information that helps a user answer a specific questions. To assess information testing as a method, we implemented QAnalytics, a tool to assess the information quality of documentation that is provided by a website. The tool is web-based and allows test managers and site owners to define test cases and success criteria, disseminate the test cases to testers, and to analyze the test results. This way, information testing is easily manageable even for non-technical stakeholders. We applied our testing method and tool in a case study. According to common perception, the website of Linnaeus University needs to be re-engineered. Our method and tool helped the stakeholders identify what information is presented well and which parts of the website that need to change. The test results allowed the design and development effort to prioritize actual quality issues and potentially save time and resources.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958417,no
454,A study of applying severity-weighted greedy algorithm to software test case prioritization during testing,"Regression testing is a very useful technique for software testing. Traditionally, there are several techniques for test case prioritization; two of the most used techniques are Greedy and Additional Greedy Algorithm (GA and AGA). However, it can be found that they may not consider the severity while prioritizing test cases. In this paper, an Enhanced Additional Greedy Algorithm (EAGA) is proposed for test case prioritization. Experiments with eight subject programs are performed to investigate the effects of different techniques under different criteria and fault severity. Experimental results show that proposed EAGA perform well than other techniques.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058806,yes
455,An improved genetic approach for test path generation,"Quality of a software system depends on testing approaches adopted to analyze the software product. Testing process itself depends on two main vectors called test sequence generation and test data generation. Test sequence generation is about to identify the order in which the particular test cases will be executed and the test data defines the various checks performed on each test case. In this present work, a fuzzy improved genetic approach is suggested for test case generation. The sequence on these test cases is here dependent on module interaction analysis. Based on this analysis, the test case prioritization will be defined. Once the test cases will be prioritized, the next work is to apply fuzzy improved genetic approach for test path generation. The work is analyzed under different prioritization vectors. Analysis of work is defined in terms of test cost estimation under different prioritization scenarios.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012823,yes
456,Applying Parameter Value Weighting to a Practical Application,"This paper reports a case study where pair-wise testing was applied to a real-world program. In particular we focus on weighting, an added feature which allows the tester to prioritize particular parameter values. In our previous work we proposed a weighting method that can reflect given weights in the resulting test suite more directly than can existing methods. To asses the effects of weighting in a practical testing process, we compare the number of execution times of the program's methods among three pair-wise test suites, including the test suite generated by our weighting method and those generated by an existing test case generation tool with and without the weighting option. The results show that the effects of weighting were most clearly observed when our weighting method was used.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983821,yes
457,Athena: A Visual Tool to Support the Development of Computational Intelligence Systems,"Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984580,yes
458,Automated Prioritization of Metrics-Based Design Flaws in UML Class Diagrams,"The importance of software architecture in software development prolongs throughout the entire software life cycle. This is because quality of the architectural design defines the structural aspects of the system that are difficult to change, and hence will affect most of the subsequent development and maintenance activities. This paper considers software design flaws (related to the system structure) and not flaws identified at run time (by testing). These design flaws are akin to what is described in the literature as anti-patterns, bad smells or rotting design. Recently, two tools that have been developed for quality assurance of software designs represented in the UML notation: SDMetrics and Metric View. However these tools are not considered practical because they report many design flaws which are not considered by developers (false positives). This paper explores an approach that tries to identify which design flaws should be considered important and which are not. To this end, we propose an approach for automated prioritization of software design flaws (BX approach), to facilitate developers to focus on important design flaws more effectively. We designed and implemented a tool (PoSDef) that implements this approach. The BX approach and the PoSDef tool have been validated using two open source projects and one large industrial system. Our validation consists of comparing our approach and tool with the existing design flaw tools. The evaluation has shown that the proposed approach could facilitate developers to identify and prioritize important design flaws effectively.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928837,yes
459,Bench level automotive EMC validation test laboratory challenges and preferences,"Automotive original equipment manufacturers (OEMs) and suppliers of electrical and electronic subsystems are required to perform bench level electrical and electromagnetic compatibility (EMC) validation testing. This is an important process that requires a significant investment in time and money. In an effort to improve the efficiency of testing, a survey was developed to gain an understanding of the challenges faced by test laboratories and also their preferences. This paper summarizes the results of the survey. Given the amount of time and money spent annually for the type of testing considered, the results suggest that pursuing improvements will result in a long term savings for the original equipment manufacturers (OEMs), suppliers, and labs involved. In order to increase test efficiency, the OEMs, suppliers, and laboratories will need to work together to better prepare test plans and test setups. It is suggested that the results of this survey are used to prioritize the improvement activities.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898989,no
460,Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines,"Large Software Product Lines (SPLs) are common in industry, thus introducing the need of practical solutions to test them. To this end, t-wise can help to drastically reduce the number of product configurations to test. Current t-wise approaches for SPLs are restricted to small values of t. In addition, these techniques fail at providing means to finely control the configuration process. In view of this, means for automatically generating and prioritizing product configurations for large SPLs are required. This paper proposes (a) a search-based approach capable of generating product configurations for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of product configurations. Both these techniques employ a similarity heuristic. The ability of the proposed techniques is assessed in an empirical study through a comparison with state of the art tools. The comparison focuses on both the product configuration generation and the prioritization aspects. The results demonstrate that existing t-wise tools and prioritization techniques fail to handle large SPLs. On the contrary, the proposed techniques are both effective and scalable. Additionally, the experiments show that the similarity heuristic can be used as a viable alternative to t-wise.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823132,yes
461,Competence transfer through enterprise mobile application development,"Large world corporations need corresponding information technology (IT) support as well as constant improvement of software tools, which should enable further business development and more efficient work to operational organizations. The main interest of IT support organizations is currently more and more connected with mobile IT equipment of the employees. Specific business mobile applications improve the efficiency and result in new operating possibilities. The cooperation between Ericsson Nikola Tesla (ENT) and Faculty of Electrical Engineering and Computing (FER) of the Zagreb University, has enabled a fast development of competencies in ENT, necessary for mobile application development and quality realization of innovative solution for platforms iOS and Android. In this cooperation at the project realization the methodology of application development was defined, the corresponding competencies were developed, the system architecture was designed together with the communication of mobile application and back-end IT systems. In the project the iterative development approach, tools for software code versioning and project control have been used thus enabling continuous insight in project progressing. At any moment it was possible to determine priorities of functional development and solution elements, as well as to realize the necessary additional transfer of knowledge. The project has resulted in an enterprise mobile application for iOS and Android platforms, which had been implemented and tested in several countries. The complete solution enables data management by means of the portal, thus decreasing the frequent changes of users' mobile application and significantly accelerating the process of new functionalities introduction.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859609,no
462,Development and design of a platform for arbitration and sharing control applications,"In this paper, a description of the ADAS development platform in DESERVE project framework is presented. This work is framed within the Sub Project 2 (Development platform) of DESERVE project, and it is divided in 6 different and complementary lines of work. Most of the functions described in the tools and development systems, perception layer and the platform system architecture show the modularity and scalability of our proposal. Moreover, based on vehicle modelling, driver behaviour and intention, a first approach for arbitration and control strategies, which can anticipate the priorities on the control in emergency situations, is described. Furthermore, some simulations will allow the virtual testing for the future implementation in demonstrators. The presented work is the core of DESERVE project, and it is developed in parallel with Driver behaviour and HMI activities (SP3). This work presents some of the achievements in SP2, mainly the application platform integration in one of the demonstrators, along with the arbitration and sharing control, based on intelligent techniques (Fuzzy logic). Simulation shows the feasibility of proposal. This approach will be tested, integrated and validated in a real vehicle in the next stages of the project.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6893228,no
463,Development test case prioritization technique in regression testing based on hybrid criteria,"Test case prioritization techniques improve the performance of regression testing, and arrange test cases in order to obtain maximum available fault that is going to be detected in a shorter time. In this research the priority is given to test cases that are performed based on multiple criteria and hybrid criteria to enhance the effectiveness of time and cost for proposed technique. This paper shows that our prioritization technique is appropriate for regression testing environment and show that our prioritization approach frequently produces a higher average percentage of fault detection rate value, for web application. The experiments also reveal fundamental tradeoffs in the performance of time aware prioritization. In this technique some fault will be seeded in subject application, then applying the prioritization criteria on test cases to obtain the effective time of average percentage fault detection rate.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986033,yes
464,DITEC (DoD-Centric and Independent Technology Evaluation Capability): A Process for Testing Security,"Information Assurance (IA) is one of the Department of Defense's (DoD) top priorities today. IA technologies are constantly evolving to protect critical information from the growing number of cyber threats. Furthermore, DoD spends millions of dollars each year procuring, maintaining, and discontinuing various IA and Cyber technologies. Today, there is no process and/or standardized method for making informed decisions about which IA technologies are better/best. Due to this, efforts for selecting technologies go through very disparate evaluations that are often times non-repeatable and very subjective. DITEC (DoD-centric and Independent Technology Evaluation Capability) is a new capability that streamlines IA technology evaluation. DITEC defines a Process for evaluating whether or not a product meets DoD needs, Security Metrics for measuring how well needs are met, and a Framework for comparing various products that address the same IA technology area. DITEC seeks to reduce the time and cost of creating a test plan and expedite the test and evaluation effort for considering new IA technologies, consequently streamlining the deployment of IA products across DoD and increasing the potential to meet its needs.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825634,no
465,Dynamic test case prioritization based on multi-objective,"Test case prioritization technology is to sort the test cases before the software testing designed to improve test efficiency. This paper presents a dynamic test case prioritization technique based on multi-objective. It integrates several traditional single-objective technologies so that makes it more flexible. This technology, from five dimensions, calculates prioritization values of test cases separately. Then a weighted sum is made to the values and it sorts the test cases according to the values. The results return to the storage in order to dynamically adjust the sort of test cases. This technology not only meets the high demands of regression testing, but also ensures the high efficiency of the test results.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888744,yes
466,Effective Regression Testing Using Requirements and Risks,"The use of system requirements and their risks enables software testers to identify more important test cases that can reveal faults associated with risky components. Having identified those test cases, software testers can manage the testing schedule more effectively by running such test cases earlier so that they can fix faults sooner. Some work in this area has been done, but the previous approaches and studies have some limitations, such as an improper use of requirements risks in prioritization and an inadequate evaluation method. To address the limitations, we implemented a new requirements risk-based prioritization technique and evaluated it considering whether the proposed approach can detect faults earlier overall. It can also detect faults associated with risky components earlier. Our results indicate that the proposed approach is effective for detecting faults early and even better for finding faults associated with risky components of the system earlier than the existing techniques.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6895426,yes
467,Enabling Prioritized Cloud I/O Service in Hadoop Distributed File System,"Cloud computing has become more and more popular nowadays. Both governments and enterprises provide service through the construction of public and private clouds accordingly. Among the platforms used in cloud computing, Hadoop is considered one of the most practical and stable systems. Nevertheless, as with other regular software, Hadoop still needs to rely on the underlying operating system to communicate with hardware to function appropriately. For modern computer systems, CPUs excessively outrun hard drives (hard disks). The computer hard disk has become a major bottleneck to the overall system performance. Consequently, computer programs can execute faster if their corresponding I/O operation can be completed sooner. This is important in particular when we want to expedite the execution of urgent programs in a busy system. Unfortunately, under the current Hadoop environment, users cannot prioritize operation of disk and memory for programs which they would like them to run faster. With the help of prioritized I/O service we developed earlier, we proposed and implemented a Hadoop environment with the ability of providing prioritized I/O service. Our Hadoop environment could accelerate the execution of programs with high priority assigned by users. We evaluated our design by executing prioritized programs in environments with different busy levels. Experimental results show that programs can improve their performance by up to 33.79% if executed with high priority.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056748,yes
468,Exploring Model-Based Repositories for a Broad Range of Industrial Applications and Challenges,"Nowadays, systems are becoming increasingly complex and large and the process of developing such large-scale systems is becoming complicated with high cost and enormous effort required. Such a complicated process has a prominent challenge to ensure the quality of delivered artifacts. Therefore there is clearly a need to facilitate reuse of developed artifacts (e.g., requirements, architecture, tests) and enable automated analyses such as risk analyses, prioritizing test cases, change impact analysis, with the objective to reduce cost, effort and improve quality. Model-based engineering provides a promising mechanism to facilitate reuse and enable automation. The key idea is to use models as the backbone of structuring repositories that contain reusable artifacts (e.g., test cases, requirements). Such a backbone model is subse-quently used to enable various types of automation such as model-based testing and automated rule verification. In this paper, we report 12 industrial projects from five different industry domains that all require the construction of model-based repositories to enable various types of automation. We believe using models as the backbone to structure repositories for the purpose of enabling different types of automation in different contexts is a new and non-conventional model-based development research approach. This exploratory paper will serve the basis for future research to derive a generic model-based repository.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958385,yes
469,Fair and delay adaptive scheduler for UC and NGN networks,"Fair bandwidth allocation while conforming to stringent end-to-end delay constraints is a major requirement for the successful delivery of next generation QoS demanding traffic. Research has been carried out in the area of processor and scheduler sharing for decades to try to achieve the QoS requirements of network traffic. Fairness and traffic prioritization are two main objectives that many schedulers were originally designed to meet. Another important issue is how schedulers treat the delay sensitive traffic. Although the combination of fairness and prioritization is implemented in several schedulers but, to our knowledge, incorporating adaptive traffic delay treatment in fair and prioritized schedulers has not yet been successfully implemented. In this paper, we introduce a new scheduler that balances between the fairness of bandwidth allocation between flows while implementing prioritization and minimizes the number of end-to-end delay bound breaches. The scheduler combines the virtual clock concept used in well-known fair schedulers together with schedulability testing and evaluation implemented in delay sensitive schedulers. The scheduler is designed to achieve the fairness of bandwidth allocation, such as in fair schedulers, while minimizing the number of possible violation of end-to-end QoS delay of individual flows' packets.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900970,yes
470,Genetic algorithm secure procedures algorithm to manage data integrity of test case prioritization methodology,"The focus of present research paper is to manage data integrity and trustworthiness of issues involved in software testing phase of software development life cycle. Many times, it seems that, data integration left behind the software testing phase and it is directly focused at software deployment or delivery time. To avoid issues due to lack of data integration, we developed algorithm which can track integration of test case prioritization along with trustworthiness of test case execution. This paper demonstrates how software testing can be efficient with management of data integrity factor to avoid major security issues. One of the main advantages of our approach is that domain specific semantics can be integrated with the data quality test cases prioritization, thus being able to discover test feed data quality problems beyond conventional quality measures.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030880,yes
471,How Does the UML Testing Profile Support Risk-Based Testing,"The increasing complexity of software-intensive systems raises a lot of challenges demanding new techniques for ensuring their overall quality. The risk of not meeting the expected level of quality has negative impact on business, customers, environment and people, especially in the context of safety/security-critical systems. The importance of risk assessment, analysis and management has been well understood both in the literature and practice, which has led to the definition of a number of well-known standards. In the recent years, Risk-Based Testing (RBT) is gaining more attention, especially focusing on test prioritization and selection based on risks. On the other hand, model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software-intensive systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in literature and practice in the last decade. In this paper, we study the feasibility of combining RBT with MBT by using the upcoming version of UML Testing Profile (UTP 2) as the mechanism. We present potential traceability between RBT and UTP 2 concepts.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983859,yes
472,Identifying usability risk: A survey study,"As defined in various quality models, usability is recognized as an important attribute of software quality. Failing to address usability requirements in a software product could lead to poor quality and high usability problems in software product. Research is still in progress to introduce the best methods for reducing usability problems and increase the rate of successful usable software products. Studies have shown that problems in software products can also be controlled using Software Risk Management methods, even though these problems cannot be eliminated totally. Using Software Risk Management, problems in software products are dealt before it occurs. This paper presents usability problems as a risk factor and by managing usability risk at earlier phases of the development process, successful and high usability software products can be developed. Unfortunately, currently there is little effort in identifying, analyzing and prioritizing potential usability risks at earlier phases of the development process. This paper focuses on usability risk identification as it is the first stage in usability risk management. This paper presents the results of an industry survey based on the opinion of Malaysian Public Sector involving sample size of 330 software developers and software projects managers regarding potential usability risk that could occur during Software Development Life Cycle (SDLC). Our finding has identified 42 potential usability risks, defined as a list for further risk analysis in future.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986005,no
473,Implementing test case selection and reduction techniques using meta-heuristics,"Regression Testing is an inevitable and very costly maintenance activity that is implemented to make sure the validity of modified software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization to select and prioritize a minimum set of test cases, fulfilling some chosen criteria, that is, covering all possible faults in minimum time and other. In this paper a test case reduction hybrid Particle Swarm Optimization (PSO) algorithm has been proposed. This PSO algorithm uses GA mutation operator while processing. PSO is a swarm intelligence algorithm based on particles behavior. GA is an evolutionary algorithm (EA). The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949377,no
474,Parallelized ACO algorithm for regression testing prioritization in hadoop framework,"Regression testing is an important strategy in the software maintenance phase to produce a high quality software product. This testing ensures that the modified system code does not have an effect on the original software system. Initially, the test suite is generated for the existing software system. After the system undergoes changes the test suite contains both the original test cases and the modified test cases. Regression test prioritization method helps to separate the optimal test cases from the modified test suite. In the existing work, the multi-criteria optimization was applied for generating optimal regression test cases and it was carried out in a non-parallelized environment. The proposed solution is to extend the existing work by generating an optimized test suite using Ant Colony Optimization (ACO) technique on Hadoop Map reduce framework in a parallelized environment.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019371,yes
475,"Planning of Prioritized Test Procedures in Large Integrated Systems: Best Strategy of Defect Discovery and Early Stop of Testing Session, The Selex-ES Experience","Large integrated systems verification can be made more efficient if driven by specific strategy, classification and prioritization of test procedures is the Selex ES way to speed up important defects discovery and cost of testing activities.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983814,yes
476,Practical Software Quality Prediction,"Software systems continue to play an increasingly important role in our daily lives, making the quality of software systems an extremely important issue. Therefore, a significant amount of recent research focused on the prioritization of software quality assurance efforts. One line of work that has been receiving an increasing amount of attention is Software Defect Prediction (SDP), where predictions are made to determine where future defects might appear. Our survey showed that in the past decade, more than 100 papers were published on SDP. Nevertheless, the practical adoption of SDP to date is limited. In this paper, we highlight the findings of our thesis, which identifies the challenges that hinder the adoption of SDP in practice. These challenges include the fact that the majority of SDP research rarely considers the impact of defects when performing their predictions, seldom provides guidance on how to use the SDP results, and is too reactive and defect-centric in nature. Therefore, we propose approaches that tackle these challenges. First, we present approaches that predict high-impact defects. Our approaches illustrate how SDP research can be tailored to consider the impact of defects when making their predictions. Second, we present approaches that simplify SDP models so they can be easily understood and illustrates how these simple models can be used to assist practitioners in prioritizing the creation of unit tests in large software systems. These approaches illustrate how SDP research can provide guidance to practitioners using SDP. Then, we argue that organizations are interested in proactive risk management, which covers more than just defects. For example, risky changes may not introduce defects but they could delay the release of projects. Therefore, we present an approach that predicts risky changes, illustrating how SDP can be more encompassing (i.e., by predicting risk, not only defects) and proactive (i.e., by predicting changes before they are incorporated into the code base).",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976158,no
477,Prioritization of Unit Testing on non-object oriented using a top-down based approach,"The issue which makes Unit Testing so tough is the ambiguous ways that the software world keeps moving forward. Although sometimes by implementing simple unit testing methods, this task become easy to handle. However to achieve a comprehensive unit testing it is supposed to test all corners of software such as database, devices, communication etc. This paper proposes an orthogonal software testing approach based on Top-Down technique which treats the input parameters of a software unit in an orthogonal partitioning to dynamic level of testing. Describes how test cases are statistically for each trial of software testing steps and makes a dynamic partitioning approach on non-object oriented experiments. The adequacy of the generated test cases can be validated by examining testing coverage metrics. The authors have considered using of different partitioning and mock objects help to make an isolated testing, improve code's structure and automated testing possibility. The results of the test case executions can be analyzed in order to find the ƒ??IFƒ? metrics for partitioning the traceable ways and detecting defects, to generate more effective test cases in future testing, and to help locate and correct defects in the early stage of testing.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6985991,yes
478,Quality of Service (QoS)-Guaranteed Network Resource Allocation via Software Defined Networking (SDN),"Quality of Service (QoS) -- based bandwidth allocation plays a key role in real-time computing systems and applications such as voice IP, teleconferencing, and gaming. Likewise, customer services often need to be distinguished according to their service priorities and requirements. In this paper, we consider bandwidth allocation in the networks of a cloud carrier in which cloud users' requests are processed and transferred by a cloud provider subject to QoS requirements. We present a QoS-guaranteed approach for bandwidth allocation that satisfies QoS requirements for all priority cloud users by using Open vSwitch, based on software defined networking (SDN). We implement and test the proposed approach on the Global Environment for Networking Innovations (GENI). Experimental results show the effectiveness of the proposed approach.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945296,no
479,"RDCC: An effective test case prioritization framework using software requirements, design and source code collaboration","Test case prioritization is a technique for selecting those test cases, which are expected to outperform for determining faulty modules earlier. Different phases of software development lifecycle represent the total software from different point of views, where priority module may vary from phase to phase. However, information from different phases of software development lifecycle is rarely introduced and no one integrates that information to prioritize test cases. This paper presents an effective test case prioritization framework, which takes software requirements specification, design diagrams, source codes and test cases as input and provides a prioritized order of test cases using their collaborative information as output. Requirement IDs are split into words or terms excluding stop words to calculate requirements relativity. Design diagrams are extracted as readable XML format to calculate the degree of interconnectivity among the activities. Source codes are parsed as call graphs where vertices and edges represent classes, and calls between two classes respectively. Requirements relativity, design interconnectivity and class dependencies are multiplied by their assigned weight to calculate final weight and select test cases by mapping the customers' requirements and test cases using that weight. The proposed framework is validated with an academic project and the results show that use of collaborative information during prioritization process can be beneficial.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7073072,yes
480,Regression Testing Approach for Large-Scale Systems,"Regression testing is an important and expensive activity that is undertaken every time a program is modified to ensure that the changes do not introduce new bugs into previously validated code. Instead of re-running all test cases, different approaches were studied to solve regression testing problems. Data mining techniques are introduced to solve regression testing problems with large-scale systems containing huge sets of test cases, as different data mining techniques were studied to group test cases with similar features. Dealing with groups of test cases instead of each test case separately helped to solve regression testing scalability issues. In this paper, we propose a new methodology for regression testing of large-scale systems using data mining techniques to prioritize and select test cases based on their coverage criteria and fault history.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983822,yes
481,Reverse Engineering Complex Feature Correlations for Product Line Configuration Improvement,"As a Software Product Line (SPL) evolves with increasing number of variant features and feature values, the feature correlations become extremely intricate. However, these correlations are often incompletely documented (e.g., In feature models) so that most features can only be configured manually. In order to make product configuration processes more efficient, we present an approach to extracting complex feature correlations from existing product configurations using association mining techniques. Then these correlations are pruned and prioritized in order to minimize the effort of correlation validation. Our approach is conducted on an industrial SPL with 100 product configurations across 480 features. While 80 out of the 100 configurations are used as training data to automatically extract 4834 complex feature correlations, the rest 20 configurations are used as test data to evaluate the improvement potential of configuration efficiency. In the end, avg. 25% features in each of the 20 products can be configured automatically.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928830,yes
482,RSUs placement using overlap based greedy method for urban and rural roads,"The deployment of efficient roadside networks is a necessity for ITS deployment. The main challenge for the roadside deployment is to find a satisfying or best distribution of RSUs on the roads network according to the given conditions in order to meet the requested requirements of the roads operator. Additionally, various factors affect this process such as traffic, infrastructure and topological characteristics of the roads. This paper introduces an Overlap based Greedy Method (OGM) as a basis for RSUs placement and can be applied on urban and rural roads. This method in its current development mainly considers the RSU coverage radius and the overlap rate into the RSUs distribution process. Moreover, a wide range of influencing factors is considered through the prioritization of Sites of Interest (SoIs). The OGM method is developed within the recently started PRONET project and is integrated into its software platform. The tests conducted on selected roads environment in the city of Rostock (Germany) ensure: (1) decrease in number of chosen SoIs for placement compared to the original scanned number of SoIs, and (2) full (continuous) coverage cannot be guaranteed for a roads network using only the available junctions.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000905,no
483,SimLatte: A Framework to Support Testing for Worst-Case Interrupt Latencies in Embedded Software,"Embedded systems tend to be interrupt-driven, yet the presence of interrupts can affect system dependability because there can be delays in servicing interrupts. Such delays can occur when multiple interrupt service routines and interrupts of different priorities compete for resources on a given CPU. For this reason, researchers have sought approaches by which to estimate worst-case interrupt latencies (WCILs) for systems. Most existing approaches, however, are based on static analysis. In this paper, we present SIMLATTE, a testing-based approach for finding WCILs. SIMLATTE uses a genetic algorithm for test case generation that converges on a set of inputs and interrupt arrival points that are likely to expose WCILs. It also uses an opportunistic interrupt invocation approach to invoke interrupts at a variety of feasible locations. Our evaluation of SIMLATTE on several non-trivial embedded systems reveals that it is considerably more effective and efficient than random testing. We also determine that the combination of the genetic algorithm and opportunistic interrupt invocation allows SIMLATTE to perform better than it can when using either one in isolation.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823893,no
484,Test Case Prioritization Based on Information Retrieval Concepts,"In regression testing, running all a system's test cases can require a great deal of time and resources. Test case prioritization (TCP) attempts to schedule test cases to achieve goals such as higher coverage or faster fault detection. While code coverage-based approaches are typical in TCP, recent work has explored the use of additional information to improve effectiveness. In this work, we explore the use of Information Retrieval (IR) techniques to improve the effectiveness of TCP, particularly for testing infrequently tested code. Our approach considers the frequency at which elements have been tested, in additional to traditional coverage information, balancing these factors using linear regression modeling. Our empirical study demonstrates that our approach is generally more effective than both random and traditional code coverage-based approaches, with improvements in rate of fault detection of up to 4.7%.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091286,yes
485,Test case prioritization techniques ƒ??an empirical studyƒ?,"Regression testing is an expensive process. A number of methodologies of regression testing are used to improve its effectiveness. These are retest all, test case selection, test case reduction and test case prioritization. Retest all technique involves re-execution of all available test suites, which are critical moreover cost effective. In order to increase efficiency, test case prioritization is being utilized for rearranging the test cases. A number of algorithms has been stated in the literature survey such as Greedy Algorithms and Metaheuristic search algorithms. A simple greedy algorithm focuses on test case prioritization but results in less efficient manner, due to which researches moved towards the additional greedy and 2-Optimal algorithms. Forthcoming metaheuristic search technique (Hill climbing and Genetic Algorithm) produces a much better solution to the test case prioritization problem. It implements stochastic optimization while dealing with problem concern. The genetic algorithm is an evolutionary algorithm which gives an exact mathematical fitness value for the test cases on which prioritization is done. This paper focuses on the comparison of metaheuristic genetic algorithm with other algorithms and proves the efficiency of genetic algorithm over the remaining ones.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045344,yes
486,Test case prioritization using multi objective particle swarm optimizer,"The goal of regression testing is to validate the modified software. Due to the resource and time constraints, it becomes necessary to develop techniques to minimize existing test suites by eliminating redundant test cases and prioritizing them. This paper proposes a 3-phase approach to solve test case prioritization. In the first phase, we are removing redundant test cases by simple matrix operation. In the second phase, test cases are selected from the test suite such that selected test cases represent the minimal set which covers all faults and also at the minimum execution time. For this phase, we are using multi objective particle swarm optimization (MOPSO) which optimizes fault coverage and execution time. In the third phase, we allocate priority to test cases obtained from the second phase. Priority is obtained by calculating the ratio of fault coverage to the execution time of test cases, higher the value of the ratio higher will be the priority and the test cases which are not selected in phase 2 are added to the test suite in sequential order. We have also performed experimental analysis based on maximum fault coverage and minimum execution time. The proposed MOPSO approach is compared with other prioritization techniques such as No Ordering, Reverse Ordering and Random Ordering by calculating Average Percentage of fault detected (APFD) for each technique and it can be concluded that the proposed approach outperformed all techniques mentioned above.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6884931,yes
487,Test case prioritization with improved genetic algorithm,"In software development, the most time consuming phase is maintenance. Regression testing, which is a part of maintenance, deals with test case prioritization that aims to increase rate of fault detection with less number of tests. In our study, we used 100 tests and 1000 faults; however, faults are detected by tests using genetic algorithm and improved genetic algorithm. After test case prioritization, we may detect all faults with less number of tests so there'll no need to apply all 100 tests (re-test).",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6830456,yes
488,Test Suite Prioritization by Switching Cost,"Test suite generation and prioritization are two main research fields to improve testing efficiency. Combinatorial testing has been proven as an effective method to generate test suite for highly configurable software systems, while test suites are often prioritized by interaction coverage to detect faults as early as possible. However, for some cases, there exists reasonable cost of reconfiguring parameter settings when switching test cases in different orders. Surprisingly, only few studies paid attention to it. In this paper, by proposing greedy algorithms and graph-based algorithms, we aim to prioritize a given test suite to minimize its total switching cost. We also compare two different prioritization strategies by a series of experiments, and discuss the advantages of our prioritization strategy and the selection of prioritization techniques. The results show that prioritization by switching cost can improve testing efficiency and our prioritization strategy can produce a small test suite with a reasonably low switching cost. This prioritization can be used widely and help locate fault causing interactions. The results also suggest that when testing highly configurable software systems and no knowledge of fault detection can be used, prioritization by switching cost is a good choice to detect faults earlier.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825648,yes
489,Test Suite Reduction by Combinatorial-Based Coverage of Event Sequences,"Combinatorial-based criteria are useful in several studies for test suite generation, prioritization, and minimization. In this paper, we extend previous work by using combinatorial-based criteria for test suite reduction. We use criteria that are based on combinatorial coverage of events and consider the order in which events occur. A simple combinatorial-based criterion covers t-way events and does not differentiate between the order of events. The event pair (e<sub>1</sub>, e<sub>2</sub>) is counted the same as if it occurs in the order (e<sub>2</sub>, e<sub>1</sub>). We also use two criteria that consider the order in which events occur since different orderings of events may be valuable during testing. First, the consecutive sequence-based criterion counts all event sequences in different orders, but they must occur adjacent to each other. The sequence-based criterion counts pairs in all orders without the requirement that events must be adjacent. We evaluate the new criteria on three GUI applications. We use 2way inter-window coverage in our studies. All of the 2way combinatorial-based criteria are effective in reducing the test suites and maintaining close to 100% fault finding effectiveness. Our future work examines larger test suites, higher strength coverage, techniques to partition event data, and further empirical studies.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825647,no
490,Testability of object-oriented systems: An AHP-based approach for prioritization of metrics,This paper investigates testability from the perspective of metrics used in an object-oriented system. The idea is to give an overview of object oriented design metrics with the prioritization of same keeping testability as the overall goal. We have used Analytic Hierarchy Process (AHP) method to attain which metric is mostly used and is best for testability.,2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019595,yes
491,The research of the test case prioritization algorithm for black box testing,"In order to improve the efficiency of software test case execution, this paper analyzed the impact of some factors to test cases prioritization and presented two adjustment algorithms. These factors included software requirement prioritization, software failure severity and software failure probability level. Firstly, gave the definition of software requirement prioritization, the ranking methods of software failure severity and software failure probability level, the description of the relationship between test cases and test requirements. Then, presented an initial test case prioritization method based on the analysis. And then, proposed a dynamic adjustment algorithm using of software requirement prioritization and software failure probability level when software failure occurred. Experimental data show that the two test case prioritization algorithms can improve the efficiency of software testing and are helpful to find more software defects in a short period.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933509,yes
492,Toolset and Program Repository for Code Coverage-Based Test Suite Analysis and Manipulation,"Code coverage is often used in academic and industrial practice of white-box software testing. Various test optimization methods, e.g. Test selection and prioritization, rely on code coverage information, but other related fields benefit from it as well, such as fault localization. These methods require access to the fine details of coverage information and efficient ways of processing this data. The purpose of the (free) SoDA library and toolset is to provide an efficient set of data structures and algorithms which can be used to prepare, store and analyze in various ways data related to code coverage. The focus of SoDA is not on the calculation of coverage data (such as instrumentation and test execution) but on the analysis and manipulation of test suites based on such information. An important design goal of the library was to be usable on industrial-size programs and test suites. Furthermore, there is no limitation on programming language, analysis granularity and coverage criteria. In this paper, we demonstrate the purpose and benefits of the library, the associated toolset, which also includes a graphical user interface, as well as possible usage scenarios. SoDA also includes a repository of prepared programs, which are from small to large sizes and can be used for experimentation and as a benchmark for code coverage related research.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975635,no
493,Using Dual Priority scheduling to improve the resource utilization in the nMPRA microcontrollers,"The current practice in most of the safety-critical areas, including automotive, avionics systems and factory automation, encouraging the use of real-time time-trigger schedulers that does not allow interference to take place between safety-critical components and non-critical. Furthermore, in these systems the lack of interference between safety-critical components and non-critical components is achieved by a strict isolation between components with different degrees of severity. This approach can assure, easily, the certification of the safety-critical functionality, but leads to very low resource utilization. For this purpose it will be presented a solution that when the system enters into a state that is different from the normal running state (test service), allowing relaxation and a change in the activation time of tasks (release) violating the fixed priorities scheduling, but avoiding starvation of the system tasks. The proposed solution modifies a static scheduler in a dynamic scheduler depending on the system status using Dual Priority scheduling. The algorithm has been proposed to be implemented on a nMPRA processor, by multiplying hardware resources (PC, pipeline registers and file registers) and other facilities (events, mutexes, interrupts, IPC communication, timer's, the static scheduler and support for dynamic scheduler) provides a switching and response time for events within 1 to 3 machine cycles.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842431,yes
494,Using TTCN-3 to Test SPDY Protocol Interaction Property,"SPDY protocol is a new application-layer communication protocol which was proposed by Google in order to overcome the defects of HTTP. On the basis of HTTP protocol, SPDY offered four improvements to shorten the page loading time such as multiplexed requests, prioritized requests, server pushed streams and compressed headers. However, there is no much test work of the SPDY especially treating it as a black box, focusing on its interaction property, or testing it by using TTCN-3. In this paper, the interaction property of SPDY protocol is analyzed according to the SPDY protocol draft specification, and a novel test work designed in view of SPDY interaction property is implemented. During the test work, the interaction granularity of the SPDY peers is divided into three kinds of granularity from different levels, meanwhile the test cases were generated in accordance with the draft specification and encoded by utilizing TTCN-3 language for executing on the TT work bench Professional software. Above all we must ensure the SPDY server-side can support the SPDY protocol and the test host installs TT work bench Professional software to complete TTCN-3 testing. Finally, the interaction property of SPDY protocol was tested and the test results were reported. Moreover, some failed test cases were analyzed in detail.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903186,no
495,8th International Workshop on Search-Based Software Testing (SBST 2015),"This paper is a report on the 8th International Workshop on Search-Based Software Testing at the 37th International Conference on Sofrware Engineering (ICSE). Search-Based Software Testing (SBST) is a form of Search-Based Software Engineering (SBSE) that optimizes testing through the use of computational search. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service-orientated architectures, construct test suites for interaction testing, and validate real time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.Three full research papers, three short papers, and threeposition papers will be presented in the two-day workshop. Additionally, six development groups have pitted their test generation tools against a common set of programs and benchmarks, and will present their techniques and results. This report will give the background of the workshop and detail the provisional program.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203148,no
496,A Clustering-Bayesian Network Based Approach for Test Case Prioritization,"Test case prioritization can effectively reduce the cost of regression testing by executing test cases with respect to their contributions to testing goals. Previous research has proved that the Bayesian Networks based technique which uses source code change information, software quality metrics and test coverage data has better performance than those methods merely depending on only one of the items above. Although the former Bayesian Networks based Test Case Prioritization (BNTCP) focusing on assessing the fault detection capability of each test case can utilize all three items above, it still has a deficiency that ignores the similarity between test cases. For mitigating this problem, this paper proposes a hybrid regression test case prioritization technique which aims to achieve better prioritization by incorporating code coverage based clustering approach with BNTCP to depress the impact of those similar test cases having common code coverage. Experiments on two Java projects with mutation faults and one Java project with hand-seeded faults have been conducted to evaluate the fault detection performance of the proposed approach against Additional Greedy approach, Bayesian Networks based approach (BNTCP), Bayesian Networks based approach with feedback (BNA) and code coverage based clustering approach. The experimental results showed that the proposed approach is promising.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273420,yes
497,A coupling effect based test case prioritization technique,"Regression testing is a process that executes subset of tests that have already been conducted to ensure that changes have not propagated unintended side effects. Test case prioritization aims at reordering the regression test suit based on certain criteria, so that the test cases with higher priority can be executed first rather than those with lower priority. In this paper, a new approach for test case prioritization has been proposed which is based on a module-coupling effect that considers the module-coupling value for the purpose of prioritizing the modules in the software so that critical modules can be identified which in turn will find the prioritized set of test cases. In this way there will be high percentage of detecting critical errors that have been propagated to other modules due to any change in a module. The proposed approach has been evaluated with the case study of software consisting of ten modules.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100468,yes
498,A defect dependency based approach to improve software quality in integrated software products,"Integrated software products are complex in design. They are prone to defects caused by integrated and non-integrated modules of the entire integrated software suite. In such software products, a small proportion of defects are fixed as soon as they are reported. Rest of the defects are targeted for fixes in future product release cycles. Among such targeted defects, most of them seem to be insignificant and innocuous in the current version but have the potential to become acute in future versions. In this paper, we propose an approach to study defect dependency of the reported defect using a dependency metric. Identifying the dependency of a defect in an integrated product suite can help the product stake-owners to prioritize them and help improve software quality.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320343,yes
499,A methodology for regression testing reduction and prioritization of agile releases,"Regression testing is the type of software testing that seeks to uncover new software bugs in existing areas of a system after changes have been made to them. The significance of regression testing have grown in the past decade with the amplified adoption of agile development methodologies, which requires the execution of regression testing at the end of each release. In this paper, we present an automated agile regression testing approach that reduces the number of test cases to be used at regression phase depending on the similarity of issues exposed from the different test cases, taking into consideration the user story coverage. It then prioritizes the reduced test cases using user-provided weighted agile parameters. The proposed approach achieves enhancement for both the reduction and prioritization of test cases for agile regression testing.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426903,yes
500,A novel dynamic analysis of test cases to improve testing efficiency in object-oriented systems,"In this paper, we present a series of methods to improve testing efficiency especially for regression testing from a novel view, namely dynamic analysis of test cases suitable for class testing in object-oriented systems. We mine static call graphs and dynamic call trees to represent the static features and dynamic tests of the program. By graph analysis, we present a series of methods and testing criteria to evaluate test cases from the view of code coverage. These methods improve testing efficiency for class testing from the following aspects: automation; multi-angle evaluations of test cases; improvement and management of test cases; providing different prioritization criteria and optimization criteria for regression testing to meet different testing requirements etc. What's more, they can be used in large-scale OO systems, and the test results are quantifiable.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490789,yes
501,A Schema Support for Selection of Test Case Prioritization Techniques,"Regression testing is a vast field of research. It is very costly and time consuming process but on the other hand very important process in software testing. Retest all, Test case Selection, Hybrid and Test Case Prioritization are its various techniques which are used to reduce the efforts in maintenance phase. In technical literature several techniques are present with their different and vast number of goals which can be applied in software projects despite of that they have not proven their true efficiency in the testing process. The major problem in regression testing area is to select the test case prioritization technique/s that is effective in such a way that maximum project characteristics should be cover in a minimum time span. However, consideration of this decision be carefully done so that loss of resources can be avoided in a software project. Based on the above scenario, author proposes a selection schema to support the selection of TCP techniques for a given software project aiming at maximizing the coverage of software project characteristics considering aspect of prioritization of software project characteristics. At the end, preliminary results of an experimental evaluation are presented. The purpose of this research is decision should be based on the objective knowledge of the techniques rather than considering some perception and assumptions.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079143,yes
502,A similarity-based approach for test case prioritization using historical failure data,"Test case prioritization is a crucial element in software quality assurance in practice, specially, in the context of regression testing. Typically, test cases are prioritized in a way that they detect the potential faults earlier. The effectiveness of test cases, in terms of fault detection, is estimated using quality metrics, such as code coverage, size, and historical fault detection. Prior studies have shown that previously failing test cases are highly likely to fail again in the next releases, therefore, they are highly ranked, while prioritizing. However, in practice, a failing test case may not be exactly the same as a previously failed test case, but quite similar, e.g., when the new failing test is a slightly modified version of an old failing one to catch an undetected fault. In this paper, we define a class of metrics that estimate the test cases quality using their similarity to the previously failing test cases. We have conducted several experiments with five real world open source software systems, with real faults, to evaluate the effectiveness of these quality metrics. The results of our study show that our proposed similarity-based quality measure is significantly more effective for prioritizing test cases compared to existing test case quality measures.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381799,yes
503,A simple statically reconfigurable processor architecture,"A reconfigurable Processor architecture is presented that has been designed prioritizing modularity, scalability and simplicity. Modular design enables swapping of functional units within the main processing core while maintaining the same programming model. This ensures that the associated software tools chain such as Assembler and Compiler need not be redesigned. Scalable design enables reconfiguring the datapath width to suite application requirements without redesigning the processor architecture or making changes to the software program already written. Applications for such design range from academia where real world performance of many proposed Adder/Multiplier structures may be tested; to data centers where the nature of operation to be performed on massive chunks of data changes regularly requiring ASIC like performance.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226112,no
504,A Subsumption Hierarchy of Test Case Prioritization for Composite Services,"Many composite workflow services utilize non-imperative XML technologies such as WSDL, XPath, XML schema, and XML messages. Regression testing should assure the services against regression faults that appear in both the workflows and these artifacts. In this paper, we propose a refinement-oriented level-exploration strategy and a multilevel coverage model that captures progressively the coverage of different types of artifacts by the test cases. We show that by using them, the test case prioritization techniques initialized on top of existing greedy-based test case prioritization strategy form a subsumption hierarchy such that a technique can produce more test suite permutations than a technique that subsumes it. Our experimental study of a model instance shows that a technique generally achieves a higher fault detection rate than a subsumed technique, which validates that the proposed hierarchy and model have the potential to improve the cost-effectiveness of test case prioritization techniques.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839018,yes
505,A Test Framework for Communications-Critical Large-Scale Systems,"Today's large-scale systems couldn't function without the reliable availability of a range of network communications capabilities. Software, hardware, and communications technologies have been advancing throughout the past two decades. However, the methods that industry commonly uses to test large-scale systems that incorporate critical communications interfaces haven't kept pace. The need exists for a specifically tailored framework to achieve effective, precise testing of communications-critical large-scale systems. A proposed test framework offers an alternative to the current generic approaches that lead to inefficient, costly testing in industry. A case study illustrates its benefits, which can also be realized with other comparable systems.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6785925,no
506,AEGIS autonomous targeting for the Curiosity rover's ChemCam instrument,"AEGIS (Autonomous Exploration for Gathering Increased Science) is a software suite that will imminently be operational aboard NASA's Curiosity Mars rover, allowing the rover to autonomously detect and prioritize targets in its surroundings, and acquire geochemical spectra using its ChemCam instrument. ChemCam, a Laser-Induced Breakdown Spectrometer (LIBS), is normally used to study targets selected by scientists using images taken by the rover on a previous sol and relayed by Mars orbiters to Earth. During certain mission phases, ground-based target selection entails significant delays and the use of limited communication bandwidth to send the images. AEGIS will allow the science team to define the properties of preferred targets, and obtain geochemical data more quickly, at lower data penalty, without the extra ground-inthe-loop step. The system uses advanced image analysis techniques to find targets in images taken by the rover's stereo navigation cameras (NavCam), and can rank, filter, and select targets based on properties selected by the science team. AEGIS can also be used to analyze images from ChemCam's Remote Micro Imager (RMI) context camera, allowing it to autonomously target very fine-scale features - such as veins in a rock outcrop - which are too small to detect with the range and resolution of NavCam. AEGIS allows science activities to be conducted in a greater range of mission conditions, and saves precious time and command cycles during the rover's surface mission. The system is currently undergoing initial tests and checkouts aboard the rover, and is expected to be operational by late 2015. Other current activities are focused on science team training and the development of target profiles for the environments in which AEGIS is expected to be used on Mars.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444544,yes
507,An effective test case prioritization method based on fault severity,"In regression testing area, test case prioritization is one of the main techniques to improve the test validity and test effectiveness. However, when the test cases have the same maximum coverage rate, the random selection of the additional statement will influence the effect of sorting. For dealing with this problem, a new method is proposed to optimize test case prioritization based on fault severity, referred to as additional-statement-on-fault-severity. Facing those same maximum coverage rate, the new technique main consider a factor, fault severity, to sort test cases, it figures out the value of test case based on the algorithm of the new technique and order the sequence from high to low. Experiment results show that the improved technique of test case prioritizaftion can improve the efficiency of regression testing.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339162,yes
508,An Information Retrieval Approach for Regression Test Prioritization Based on Program Changes,"Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight open-source Java projects shows that REPiR is computationally efficient and performs better than existing (dynamic or static) techniques for the majority of subject systems.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194580,yes
509,Applying Ant Colony Optimization in software testing to generate prioritized optimal path and test data,"Software testing is one of the most important parts of software development lifecycle. Among various types of software testing approaches structural testing is widely used. Structural testing can be improved largely by traversing all possible code paths of the software. Genetic algorithm is the most used search technique to automate path testing and test case generation. Recently, different novel search based optimization techniques such as Ant Colony Optimization (ACO), Artificial Bee Colony (ABC), Artificial Immune System (AIS), Particle Swarm Optimization (PSO) have been applied to generate optimal path to complete software coverage. In this paper, ant colony optimization (ACO) based algorithm has been proposed which will generate set of optimal paths and prioritize the paths. Additionally, the approach generates test data sequence within the domain to use as inputs of the generated paths. Proposed approach guarantees full software coverage with minimum redundancy. This paper also demonstrates the proposed approach applying it in a program module.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307500,yes
510,Approximating Attack Surfaces with Stack Traces,"Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202964,yes
511,Architecting to Ensure Requirement Relevance: Keynote TwinPeaks Workshop,"Research has shown that up to two thirds of features in software systems are hardly ever used or not even used at all. This represents a colossal waste of R&amp;D resources and occurs across the industry. On the other hand, product management and many others work hard at interacting with customers, building business cases and prioritizing requirements. A fundamentally different approach to deciding what to build is required: requirements should be treated as hypothesis throughout the development process and constant feedback from users and systems in the field should be collected to dynamically reprioritize and change requirements. This requires architectural support beyond the current state of practice as continuous deployment, split testing and data collection need to be an integral part of the architecture. In this paper, we present a brief overview of our research and industry collaboration to address this challenge.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184704,yes
512,Challenges and Issues of Mining Crash Reports,"Automatic crash reporting tools built in many software systems allow software practitioners to understand the origin of field crashes and help them prioritise field crashes or bugs, locate erroneous files, and/or predict bugs and crash occurrences in subsequent versions of the software systems. In this paper, after illustrating the structure of crash reports in Mozilla, we discuss some techniques for mining information from crash reports, and highlight the challenges and issues of these techniques. Our aim is to raise the awareness of the research community about issues that may bias research results obtained from crash reports and provide some guidelines to address certain challenges related to mining crash reports.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070480,no
513,Clustering based novel test case prioritization technique,"Regression testing is an activity during the maintenance phase to validate the changes made to the software and to ensure that these changes would not affect the previously verified code or functionality. Often, regression testing is performed with limited computing resources and time budget. So in this phase, it is infeasible to run the complete test suite Thus, test-case prioritization approaches are applied to ensure the execution of test cases in some prioritized order and to achieve some specific goals like, increasing the rate of bug detection, identifying the most critical bugs as early as possible etc. In this research work, we are going to propose a new and more effective clustering based prioritization technique that uses various metrics and execution time of test cases to reorder them. The results of implementation will prove that the suggested approach is more productive than the existing coverage and clustering based prioritization techniques.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506447,yes
514,Communication and collaboration of heterogeneous unmanned systems using the joint architecture for Unmanned Systems (JAUS) standards,"The Naval Undersea Warfare Center Division Newport (NUWCDIVNPT) and Georgia Tech Research Institute (GTRI) completed a successful at-sea exercise with autonomous UAS and UUV systems demonstrating cross-domain unmanned system communication and collaboration. The exercise was held at the NUWC Narragansett Bay Shallow Water Test Facility (NBSWTF) range, and it represented for the first time the use of standard protocols and formats that effectively support cross-domain unmanned system operations. Four man-portable Iver2 UUVs operating in coordinated missions autonomously collected environmental data, which was compressed in-stride, re-formatted, and exfiltrated via UAS relay for display and tactical decision making. Two UAS with autonomous flight take-off and mission execution were sequenced to serve as ISR platforms and to support communications as RF relays for the UUVs performing Intelligence Preparation of the Environment missions. Two Command and Control nodes ashore provided unmanned system tasking and re-tasking, and they served to host and display both geo-positional data and status for UAS and UUV vehicles during the operational scenarios run during the exercise. The SAE Joint Architecture for Unmanned Systems (JAUS) standards were used for all message traffic between shore-based C2 nodes, UAS, and UUVs active in the NBSWTF exercise area. Exercise goals focused on CNO priorities expressed in the Undersea Domain Operating Concept of AUG 2013 which emphasized protocols essential to effective command and control of networked unmanned systems with decentralization and flexibility of command structures. Development for this project highlighted both the strengths and shortfalls of JAUS and captured the requirements for moving forward in effective cross-domain communications that support distributed, agile C2 nodes to meet evolving CONOPS for growing unmanned system presence and mission roles. The scenario employed operating parameters for UAS and UUV that have been established in real-world operations and ongoing unmanned system programs. The tactical information from unmanned systems was displayed in real-time on shore-based C2 displays: the tactical FalconView display and the developmental TOPSIDE command and control station. This work represents a critical step in communications for networking of heterogeneous unmanned systems and establishes a solid platform for alignment of development and ongoing programs. The evaluation of JAUS suitability for near-term operational applications provides significant value as Concepts of Operation that rely on netted heterogeneous systems are being targeted. The focus on affordable commercial unmanned systems for this experimentation establishes the value of highly capable, portable systems to provide economical development and test opportunities with low-cost and low-risk alternatives to many planned and fielded systems. The JAUS architecture was introduced to the NUWC and GTRI unmanned systems though an instantiation of the Mission Oriented Operating Suite (MOOS) autonomy framework on secondary CPUs integrated into the Iver2 UUVs and the GTRI UAS. Since the GTRI UASs already had ROS installed, a MOOS-ROS bridge was employed to support use of the developed JAUS messaging capability. Established JAUS services were employed where the required functions could be met. New JAUS services were developed to meet functionality required for the operational scenarios in this exercise but not yet supported in the existing releases of SAE JAUS. Independent C++ header libraries that could be compiled at run time for specific autonomy frameworks, such as MOOS, were employed to support a software-agnostic approach. Immediate targets for broadening the influence of this work to coalition partners include the NATO Recognized Environmental Picture (REP) 2015 and The Technical Cooperation Program (TTCP) 2015 exercises. This project and demonstration was funded under a NUWC Strategic Initiative and GTRI program support.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271613,no
515,Component based reliability assessment from UML models,"Model based development and testing techniques have ventured diverse research directions to assure quality of the software product. Models developed during architecture and design phases are efficient tools to assess quality at an early development stage. However, testing the extra-functional or non-functional properties of software systems is not frequently practised eg. reliability. The motivation to our work is to model the context of execution which is significant in system reliability analysis. In this paper we visualize the components of complex software systems and their interactions in the form of Functional Flow Diagram (FFD). This notation specifies the dynamic aspect of system behavior as the context of execution. To further asses reliability, the FFD is translated into Reliability Block Diagram (RBD). The relative importance of the components in terms of reliability is evaluated and is associated with prioritization of the component. The model is simple but significant for system maintenance, improvisation and modification. This model supports analysis and testing through better understanding of the interacting components and their reliabilities.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275704,yes
516,Component-Based Software System Test Case Prioritization with Genetic Algorithm Decoding Technique Using Java Platform,"Test case prioritization includes testing experiments in a request that builds the viability in accomplishing some execution objectives. The importance amongst the most imperative testing objectives is the fast rate of fault recognition. Test case ought to run in a request that extends the likelihood of fault discovery furthermore that detects the most serious issues at the early stage of testing life cycle. In this paper, we develop and prove the necessity of Component-Based Software testing prioritization framework which plans to uncover more extreme bugs at an early stage and to enhance software product deliverable quality utilizing Genetic Algorithm (GA) with java decoding technique. For this, we propose a set of prioritization keys to plan the proposed Component-Based Software java framework. In our proposed method, we allude to these keys as Prioritization Keys (PK). These keys may be project size, scope of the code, information stream, and bug inclination and impact of fault or bug on overall system, which prioritizes the Component-Based Software framework testing. The integrity of these keys was measured with implementation of key assessment metric called KAM that will likewise be ascertained. This paper demonstrates how software testing can be efficient with management of data integrity factor to avoid major security issues. One of the main advantages of our approach is that domain specific semantics can be integrated with the data quality test cases prioritization, thus being able to discover test feed data quality problems beyond conventional quality measures.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155967,yes
517,Considerations on application of selective hardening based on software fault tolerance techniques,"This paper analyses the nature of fault tolerance software-based techniques and the influence of their overheads to determine an efficient strategy for applying those techniques in a selective way. Several considerations that have to be taken into account are presented in this work. These include an analysis of fault coverage and overheads when selective hardening is adopted; side effects of selective protection based on software; and the need of new criticality metrics, apart from those used for hardware-based techniques (e.g., AVF), to facilitate and prioritize the selection of resources to be protected.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102509,yes
518,Decision support system prototype on obstetrics ultrasonography for primary service physicians,"Introduction. The National Social Security System (SJSN) prioritizes primary service as a spearhead to assist Primary Care Physicians to make medical decisions. The purpose of this research is to develop computer software that will assist primary care physicians in the fields of Obstetrics Ultrasonography, related to referral decision-making abilities. Methods. A quasi-experimental post-test only design without a control group. The stages of the research process: Systems Analysis and Design, Prototyping and Testing by Lecture, Students, Programmers and Doctors. Results. From Analysis and Systems design document, has been produced prototype of software, and a test run has been proven successful Decision Support System software helping doctors develop diagnosis and specialty referrals. Conclusion: The Decision Support System software can be used in Obstetrics Ultrasonography by Primary Care Physicians to provide aid in their diagnosis and referrals. Before it is used, it is recommended for trainings and application tests.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401367,no
519,Detecting Display Energy Hotspots in Android Apps,"Energy consumption of mobile apps has become an important consideration as the underlying devices are constrained by battery capacity. Display represents a significant portion of an app's energy consumption. However, developers lack techniques to identify the user interfaces in their apps for which energy needs to be improved. In this paper, we present a technique for detecting display energy hotspots - user interfaces of a mobile app whose energy consumption is greater than optimal. Our technique leverages display power modeling and automated display transformation techniques to detect these hotspots and prioritize them for developers. In an evaluation on a set of popular Android apps, our technique was very accurate in both predicting energy consumption and ranking the display energy hotspots. Our approach was also able to detect display energy hotspots in 398 Android market apps, showing its effectiveness and the pervasiveness of the problem. These results indicate that our approach represents a potentially useful technique for helping developers to detect energy related problems and reduce the energy consumption of their mobile apps.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102585,yes
520,Detecting hardware Trojans in unspecified functionality using mutation testing,"Existing functional Trojan detection methodologies assume Trojans violate the design specification under carefully crafted rare triggering conditions. We present a new type of Trojan that leaks secret information from the design by only modifying unspecified functionality, meaning the Trojan is no longer restricted to being active only under rare conditions. We provide a method based on mutation testing for detecting this new Trojan type along with mutant ranking heuristics to prioritize analysis of the most dangerous functionality. Applying our method to a UART controller design, we discover unspecified and untested bus functionality with the potential to leak 32 bits of information during hundreds of cycles without being detected! Our method also reveals poorly tested interrupt functionality with information leakage potential. After modifying the specification and test bench to remove the discovered vulnerabilities, we close the verification loop by re-analyzing the design using our methodology and observe the functionality is no longer flagged as dangerous.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372619,yes
521,Effective test strategy for testing automotive software,"Electronic content is increasing in automobiles day by day. Functionalities like Air Bags, Anti-lock Braking, Driver Assistance Systems, Body Controllers, Passive entry Passive Start, Electronic Power Steering etc. are realized electronically with complex software. These functionalities are related to automobile system safety. Hence, safety is one of the key issues of future automobile development. Risk of system failure is high due to increasing technological complexity and software content. The software shall be tested well to arrest almost all the defects. This paper explains a test case development and execution strategy based on practical implementation. It explains how test case reduction using Taguchi method, prioritization of test execution and automation help to make testing effective. It also demonstrates how maximum defects are discovered in short time.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150821,no
522,Effective verification of low-level software with nested interrupts,"Interrupt-driven software is difficult to test and debug, especially when interrupts can be nested and subject to priorities. Interrupts can arrive at arbitrary times, leading to an explosion in the number of cases to be considered. We present a new formal approach to verifying interrupt-driven software based on symbolic execution. The approach leverages recent advances in the encoding of the execution traces of interacting, concurrent threads. We assess the performance of our method on benchmarks drawn from embedded systems code and device drivers, and experimentally compare it to conventional formal approaches that use source-to-source transformations. Our experimental results show that our method significantly outperforms conventional techniques. To the best of our knowledge, our technique is the first to demonstrate effective formal verification of low-level embedded software with nested interrupts.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092387,no
523,Feasibility Analysis of Engine Control Tasks under EDF Scheduling,"Engine control applications include software tasks that are triggered at predetermined angular values of the crankshaft, thus generating a computational workload that varies with the engine speed. To avoid overloads at high rotation speeds, these tasks are implemented to self adapt and reduce their computational demand by switching mode at given rotation speeds. For this reason, they are referred to as adaptive variable rate (AVR) tasks. Although a few works have been proposed in the literature to model and analyze the schedulability of such a peculiar type of tasks, an exact analysis of engine control applications has been derived only for fixed priority systems, under a set of simplifying assumptions. The major problem of scheduling AVR tasks with fixed priorities, however, is that, due to engine accelerations, the interarrival period of an AVR task is subject to large variations, therefore there will be several speeds at which any fixed priority assignment is far from being optimal, significantly penalizing the schedulability of the system. This paper proposes for the first time an exact feasibility test under the Earliest Deadline First scheduling algorithm for tasks sets including regular periodic tasks and AVR tasks triggered by a common rotation source. In addition, a set of simulation results are reported to evaluate the schedulability gain achieved in this context by EDF over fixed priority scheduling.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176033,yes
524,Fusion of LIDAR and video cameras to augment medical training and assessment,"The Mobile Medical Lane Trainer (MMLT) is a multi-sensor rapidly deployed After-Action Review (AAR) system for Army medical lane training. Current AAR systems have two main drawbacks: 1) video does not provide a complete view of the medical and tactical situation, and 2) the video is not readily available for effective evaluation. The MMLT program is developing a ƒ??smarterƒ? AAR system by using 3D LIDAR (LIght Detection And Ranging), a camera array, People Tracking software and Medical Training Evaluation and Review (MeTER) software. This system can be brought to the field and deployed in less than an hour to provide hands-off data collection for the exercise. MMLT supplements existing evaluation systems deployed at the Medical Simulation Training Centers (MSTCs) by providing a 3-D perspective of the training event for tactical evaluation with synchronized video technology to capture both tactical and clinical skills and instructor scoring. This capability is used in conjunction with the MeTER system's skill assessment checklists for automated performance review. An immediate synchronized playback capability has been developed, ultimately resulting in a rapid AAR for debriefing. This paper will discuss the technical components of the system, including hardware components, data fusion technique, tracking algorithms, and camera prioritization approaches, and will conclude with operational test results and lessons learned.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7295832,yes
525,GUI Test Case Prioritization by State-Coverage Criterion,"Graphical User Interface (GUI) application is a kind of typical event-driven software (EDS) that transforms state according to input events invoked through a user interface. It is time consuming to test a GUI application since there are a large number of possible event sequences generated by the permutations and combinations of user operations. Although some GUI test case prioritization techniques have been proposed to determine ""which test case to execute next"" for early fault detection, most of them use random ordering to break tie cases, which has been proved to be ineffective. Recent research presents the opinion that using hybrid criteria can be an effective way for tie-breaking, but few studies focus on seeking a new criterion cooperating well with other criteria when breaking tie cases. In this paper, we propose a state-distance-based method using state coverage as a new criterion to prioritize GUI test cases. An empirical study on three GUI programs reveals that the state-distance-based method is really suitable for GUI test case prioritization and can cooperate well with the (additional) event length criterion.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166260,yes
526,History-Based Test Case Prioritization for Black Box Testing Using Ant Colony Optimization,"Test case prioritization is a technique to improve software testing. Although a lot of work has investigated test case prioritization, they focus on white box testing or regression testing. However, software testing is often outsourced to a software testing company, in which testers are rarely able to access to source code due to a contract. Herein a framework is proposed to prioritize test cases for black box testing on a new product using the test execution history collected from a similar prior product and the Ant Colony Optimization. A simulation using two actual products shows the effectiveness and practicality of our proposed framework.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102622,yes
527,Improving Multi-Objective Test Case Selection by Injecting Diversity in Genetic Algorithms,"A way to reduce the cost of regression testing consists of selecting or prioritizing subsets of test cases from a test suite according to some criteria. Besides greedy algorithms, cost cognizant additional greedy algorithms, multi-objective optimization algorithms, and multi-objective genetic algorithms (MOGAs), have also been proposed to tackle this problem. However, previous studies have shown that there is no clear winner between greedy and MOGAs, and that their combination does not necessarily produce better results. In this paper we show that the optimality of MOGAs can be significantly improved by diversifying the solutions (sub-sets of the test suite) generated during the search process. Specifically, we introduce a new MOGA, coined as DIversity based Genetic Algorithm (DIV-GA), based on the mechanisms of orthogonal design and orthogonal evolution that increase diversity by injecting new orthogonal individuals during the search process. Results of an empirical study conducted on eleven programs show that DIV-GA outperforms both greedy algorithms and the traditional MOGAs from the optimality point of view. Moreover, the solutions (sub-sets of the test suite) provided by DIV-GA are able to detect more faults than the other algorithms, while keeping the same test execution cost.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936894,no
528,Improving prioritization of software weaknesses using security models with AVUS,"Testing tools for application security have become an integral part of secure development life-cycles. Despite their ability to spot important software weaknesses, the high number of findings require rigorous prioritization. Most testing tools provide generic ratings to support prioritization. Unfortunately, ratings from established tools lack context information especially with regard to the security requirements of respective components or source code. Thus experts often spend a great deal of time re-assessing the prioritization provided by these tools. This paper introduces our lightweight tool AVUS that adjusts context-free ratings of software weaknesses according to a user-defined security model. We also present a first evaluation applying AVUS to a well-known open source project and the findings of a popular, commercially available application security testing tool.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335423,yes
529,Improving reliability using software operational profile and testing profile,"Software testing has ever remained a challenge particularly when testing is done with intention in enhancing the reliability. Conventional testing is increasing the testing in an unpredictable way by reducing the number of faults. There is a need to enhance the reliability by assigning probabilistic priorities to testing mechanism, which is done through software operational profile. This study adopts a case study to generate test cases and test suites with perspective of probabilistic reliability using the proposed framework based on software operational profile and testing profile.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219603,no
530,Introducing Continuous Delivery of Mobile Apps in a Corporate Environment: A Case Study,"Software development is conducted in increasingly dynamic business environments. Organizations need the capability to develop, release and learn from software in rapid parallel cycles. The abilities to continuously deliver software, to involve users, and to collect and prioritize their feedback are necessary for software evolution. In 2014, we introduced Rugby, an agile process model with workflows for continuous delivery and feedback management, and evaluated it in university projects together with industrial clients. Based on Rugby's release management workflow we identified the specific needs for project-based organizations developing mobile applications. Varying characteristics and restrictions in projects teams in corporate environments impact both process and infrastructure. We found that applicability and acceptance of continuous delivery in industry depend on its adaptability. To address issues in industrial projects with respect to delivery process, infrastructure, neglected testing and continuity, we extended Rugby's workflow and made it tailor able. Eight projects at Capgemini, a global provider of consulting, technology and outsourcing services, applied a tailored version of the workflow. The evaluation of these projects shows anecdotal evidence that the application of the workflow significantly reduces the time required to build and deliver mobile applications in industrial projects, while at the same time increasing the number of builds and internal deliveries for feedback.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167166,no
531,Investigating the Correspondence between Mutations and Static Warnings,"This paper provides evidences on the correspondence between mutations and static warnings. We used mutation operators as a fault model to evaluate the direct correspondence between mutations and static warnings. The main advantage of using mutation operators is that they generate a large number of programs containing faults of different types, which can be used to decide the ones most probable to be detected by static analyzers. Since static analyzers, in general, report a substantial number of false positive warnings, the intention of this study is to define a prioritization approach of static warnings based on the probability they correspond to a true positive and lead to detect software faults. The results obtained for a set of open-source programs indicate that a correspondence exist when considering specific mutation operators such that static warnings may be prioritized based on their correspondence level with mutations.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328004,no
532,Kvazaar HEVC encoder for efficient intra coding,"This paper presents an open-source Kvazaar encoder for HEVC intra coding. This academic software encoder has been developed from the scratch using C as an implementation language by prioritizing modularity, portability, and readability of the source code. Kvazaar implements almost the same intra coding functionality as HEVC reference encoder (HM) but its rewritten source code makes it significantly faster. In all-intra (AI) coding, a single-threaded C implementation of Kvazaar is 2.3 times faster than HM at a cost of 1.7% bit rate increase. The respective values with a high speed preset of Kvazaar are 10.6 and 8.8%. Compared to a single-threaded C++ implementation of x265, Kvazaar improves rate-distortion performance and increases encoding speed in both high-quality and high-speed test cases. Kvazaar has a particular edge in the high-speed test case where it almost halves the BD-rate loss and more than doubles the performance.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168970,no
533,Lexical Parsing Expression Recognition Schemata,"Parsing expression grammars (PEGs) have emerged as a promising substitute for context-free grammars (CFGs) and regular expressions (REs) in programming language specification. The benefits of PEGs are twofold. First, parsing expression grammars replace unordered choice between alternatives by prioritized choice, which naturally solves the ubiquitous ""dangling else"" problem in grammar definitions. Second, PEGs employ ""character-level syntax"" specifications that eliminate the need to separate the lexical and hierarchical components of a language specification. However, there is ""no free lunch"" in PEGs. PEGs capture only syntactic relationships, but many language constructs cannot be parsed without additional semantic information. Moreover, character-level specifications can become unwieldy, as every aspect of the language, including spacing, has to be accounted for. To overcome these issues, we extend the original PEG formalism to incorporate semantic predicates that yield a programmatic means for state-based token recognition control. Furthermore, rather than requiring a single complete specification, we capture lexical components as PEG closures that provide a self-contained token recognition mechanism to reduce the clutter associated with purely character-level PEGs. To test the effectiveness of our approach, we use it for the construction of a Delphi language front-end and practically confirm that Ford's theoretical linear-time result also holds for PEG closures.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365805,no
534,METEOR: An Enterprise Health Informatics Environment to Support Evidence-Based Medicine,"Goal: The aim of this paper is to propose the design and implementation of next-generation enterprise analytics platform developed at the Houston Methodist Hospital (HMH) system to meet the market and regulatory needs of the healthcare industry. Methods: For this goal, we developed an integrated clinical informatics environment, i.e., Methodist environment for translational enhancement and outcomes research (METEOR). The framework of METEOR consists of two components: the enterprise data warehouse (EDW) and a software intelligence and analytics (SIA) layer for enabling a wide range of clinical decision support systems that can be used directly by outcomes researchers and clinical investigators to facilitate data access for the purposes of hypothesis testing, cohort identification, data mining, risk prediction, and clinical research training. Results: Data and usability analysis were performed on METEOR components as a preliminary evaluation, which successfully demonstrated that METEOR addresses significant niches in the clinical informatics area, and provides a powerful means for data integration and efficient access in supporting clinical and translational research. Conclusion: METEOR EDW and informatics applications improved outcomes, enabled coordinated care, and support health analytics and clinical research at HMH. Significance: The twin pressures of cost containment in the healthcare market and new federal regulations and policies have led to the prioritization of the meaningful use of electronic health records in the United States. EDW and SIA layers on top of EDW are becoming an essential strategic tool to healthcare institutions and integrated delivery networks in order to support evidence-based medicine at the enterprise level.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7137654,no
535,Modification Impact Analysis Based Test Case Prioritization for Regression Testing of Service-Oriented Workflow Applications,"Test case prioritization for regression testing is an approach that schedules test cases to improve the efficiency of service-oriented workflow application testing. Most of existing prioritization approaches range test cases according to various metrics (e.g., Statement coverage, path coverage) in different application context. Service-oriented workflow applications orchestrate web services to provide value-added service and typically are long-running and time-consuming processes. Therefore, these applications need more precise prioritization to execute earlier those test cases that may detect failures. Surprisingly, most of current regression test case prioritization researches neglect to use internal structure information of software, which is a significant factor influencing the prioritization of test cases. Considering the internal structure information and fault propagation behavior of modifications respect to modified version for service-oriented workflow applications, we present in this paper a new regression test case prioritization approach. Our prioritization approach schedules test cases based on dependence analysis of internal activities in service-oriented workflow applications. Experimental results show that test case prioritization using our approach is more effective than conventional coverage-based techniques.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273631,yes
536,Multi-perspective Regression Test Prioritization for Time-Constrained Environments,"Test case prioritization techniques are widely used to enable reaching certain performance goals during regression testing faster. A commonly used goal is high fault detection rate, where test cases are ordered in a way that enables detecting faults faster. However, for optimal regression testing, there is a need to take into account multiple performance indicators, as considered by different project stakeholders. In this paper, we introduce a new optimal multi-perspective approach for regression test case prioritization. The approach is designed to optimize regression testing for faster fault detection integrating three different perspectives: business perspective, performance perspective, and technical perspective. The approach has been validated in regression testing of industrial mobile device systems developed in continuous integration. The results show that our proposed framework efficiently prioritizes test cases for faster and more efficient regression fault detection, maximizing the number of executed test cases with high failure frequency, high failure impact, and cross-functional coverage, compared to manual practice.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272927,yes
537,Mutation-based test-case prioritization in software evolution,"During software evolution, to assure the software quality, test cases for an early version tend to be reused by its latter versions. As a large number of test cases may aggregate during software evolution, it becomes necessary to schedule the execution order of test cases so that the faults in the latter version may be detected as early as possible, which is test-case prioritization in software evolution. In this paper, we proposed a novel test-case prioritization approach for software evolution, which first uses mutation faults on the difference between the early version and the latter version to simulate real faults occurred in software evolution, and then schedules the execution order of test cases based on their fault-detection capability, which is defined based on mutation faults. In particular, we present two models on calculating fault-detection capability, which are statistics-based model and probability-based model. Moreover, we conducted an experimental study and found that our approach with the statistics-based model outperforms our approach with the probability-based model and the total statement coverage-based approach, and slightly outperforms the additional statement-coverage based approach in many cases. Furthermore, compared with the total or additional statement coverage-based approach, our approach with either the statistics-based model or the probability-based model tends to be stably effective when the difference on the source code between the early version and the latter version is non-trivial.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381798,yes
538,PORA: Proportion-Oriented Randomized Algorithm for Test Case Prioritization,"Effective testing is essential for assuring software quality. While regression testing is time-consuming, the fault detection capability may be compromised if some test cases are discarded. Test case prioritization is a viable solution. To the best of our knowledge, the most effective test case prioritization approach is still the additional greedy algorithm, and existing search-based algorithms have been shown to be visually less effective than the former algorithms in previous empirical studies. This paper proposes a novel Proportion-Oriented Randomized Algorithm (PORA) for test case prioritization. PORA guides test case prioritization by optimizing the distance between the prioritized test suite and a hierarchy of distributions of test input data. Our experiment shows that PORA test case prioritization techniques are as effective as, if not more effective than, the total greedy, additional greedy, and ART techniques, which use code coverage information. Moreover, the experiment shows that PORA techniques are more stable in effectiveness than the others.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272924,yes
539,Practical Combinatorial Interaction Testing: Empirical Findings on Efficiency and Early Fault Detection,"Combinatorial interaction testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. Simulated Annealing (SA) and Greedy Algorithms have been widely used to find CIT test suites. From the literature, there is a widely-held belief that SA is slower, but produces more effective tests suites than Greedy and that SA cannot scale to higher strength coverage. We evaluated both algorithms on seven real-world subjects for the well-studied two-way up to the rarely-studied six-way interaction strengths. Our findings present evidence to challenge this current orthodoxy: real-world constraints allow SA to achieve higher strengths. Furthermore, there was no evidence that Greedy was less effective (in terms of time to fault revelation) compared to SA; the results for the greedy algorithm are actually slightly superior. However, the results are critically dependent on the approach adopted to constraint handling. Moreover, we have also evaluated a genetic algorithm for constrained CIT test suite generation. This is the first time strengths higher than 3 and constraint handling have been used to evaluate GA. Our results show that GA is competitive only for pairwise testing for subjects with a small number of constraints.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081752,no
540,Preemptive Regression Testingof Workflow-Based Web Services,"An external web service may evolve without prior notification. In the course of the regression testing of a workflow-based web service, existing test case prioritization techniques may only verify the latest service composition using the not-yet-executed test cases, overlooking high-priority test cases that have already been applied to the service composition before the evolution. In this paper, we propose Preemptive Regression Testing (PRT), an adaptive testing approach to addressing this challenge. Whenever a change in the coverage of any service artifact is detected, PRT recursively preempts the current session of regression test and creates a sub-session of the current test session to assure such lately identified changes in coverage by adjusting the execution priority of the test cases in the test suite. Then, the sub-session will resume the execution from the suspended position. PRT terminates only when each test case in the test suite has been executed at least once without any preemption activated in between any test case executions. The experimental result confirms that testing workflow-based web service in the face of such changes is very challenging; and one of the PRT-enriched techniques shows its potential to overcome the challenge.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6812226,yes
541,Prioritization and ranking of ERP testing components,"Software Testing is one of the most important activities but more often than not attracts less attention than it deserves in software development and implementation. It often takes twenty to sometimes even more than fifty percent of the total software development time. Enterprise Resource Planning (ERP) Systems provide synergy by integrating all operations of an enterprise. So implementation of ERP systems need even more rigorous testing than that employed in stand-alone software development. Software testing is a well-researched area but software testing as employed on ERP systems albeit is droughted with respect to research. This research paper is an extension of the patent by Kapur et al.[8] that identified the ERP Testing Components to measure ERP testing efficiency. Here, the ERP Testing Components have been accumulated and categorized under five heads. Thereafter, these testing components have been prioritized and ranked with the help of Analytic Hierarchy Process (AHP), as given by Saaty [17].",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359245,yes
542,Prioritization of test scenarios using hybrid genetic algorithm based on UML activity diagram,"Software testing is an essential part of the SDLC(Software Development Life Cycle). Test scenarios are used to derive test cases for model based testing. However, with the software rapidly growing in size and complexity, the cost of software will be too high if we want to test all the test cases. So this paper presents an approach using Hybrid Genetic Algorithm(HGA) to prioritize test scenarios, which improves efficiency and reduces cost as well. The algorithm combines Genetic Algorithm(GA) with Particle Swarm Optimization(PSO) algorithm and uses Local Search Strategy to update the local and global best information of the PSO. The proposed algorithm can prioritize test scenarios so as to find a critical scenario. Finally, the proposed method is applied to several typical UML activity diagrams, and compared with the Simple Genetic Algorithm(SGA). The experimental results show that the proposed method not only prioritizes test scenarios, but also improves the efficiency, and further saves effort, time as well as cost.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339189,yes
543,Prioritized test-driven reverse engineering process: A case study,"In this study we empirically investigate the adaptation of Test-Driven Development (TDD) practice into software Reverse Engineering (RE) process. We call this adaptation as Test-Driven Reverse Engineering (TDRE) process. We propose a two-layer prioritization process, which firstly prioritizes the already-implemented functionalities using the Cumulative Voting (CV) method and three prioritization criteria (importance, complexity and dependency), and secondly prioritizes test-cases for each prioritized functionality, using the same criteria. We conducted a case study in academia with students to empirically evaluate the usability and effectiveness of the prioritization process and the TDD adaptation into RE process. The results have shown that students with a good performance in testing had also good performance in designing UML class-diagrams. Moreover, the implementation of hierarchical test-cases for the already prioritized functionalities, improves code comprehension and redesigning in the RE process in terms of better total grades obtained.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7388099,yes
544,Prioritizing Manual Test Cases in Traditional and Rapid Release Environments,"Test case prioritization is one of the most practically useful activities in testing, specially for large scale systems. The goal is ranking the existing test cases in a way that they detect faults as soon as possible, so that any partial execution of the test suite detects maximum number of defects for the given budget. Test prioritization becomes even more important when the test execution is time consuming, e.g., manual system tests vs. automated unit tests. Most existing test case prioritization techniques are based on code coverage, which requires access to source code. However, manual testing is mainly done in a black- box manner (manual testers do not have access to the source code). Therefore, in this paper, we first examine the existing test case prioritization techniques and modify them to be applicable on manual black-box system testing. We specifically study a coverage- based, a diversity-based, and a risk driven approach for test case prioritization. Our empirical study on four older releases of Mozilla Firefox shows that none of the techniques are strongly dominating the others in all releases. However, when we study nine more recent releases of Firefox, where the development has been moved from a traditional to a more agile and rapid release environment, we see a very signifiant difference (on average 65% effectiveness improvement) between the risk-driven approach and its alternatives. Our conclusion, based on one case study of 13 releases of an industrial system, is that test suites in rapid release environments, potentially, can be very effectively prioritized for execution, based on their historical riskiness; whereas the same conclusions do not hold in the traditional software development environments.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102602,yes
545,Priority Integration for Weighted Combinatorial Testing,"Priorities (weights) for parameter values can improve the effectiveness of combinatorial testing. Previous approaches have employed weights to derive high-priority test cases either earlier or more frequently. Our approach integrates these order-focused and frequency-focused prioritizations. We show that our priority integration realizes a small test suite providing high-priority test cases early and frequently in a good balance. We also propose two algorithms that apply our priority integration to existing combinatorial test generation algorithms. Experimental results using numerous test models show that our approach improves the existing approaches w.r.t. Order-focused and frequency-focused metrics, while overheads in the size and generation time of test suites are small.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273624,yes
546,Pushing to the top,"IC3 is undoubtedly one of the most successful and important recent techniques for unbounded model checking. Understanding and improving IC3 has been a subject of a lot of recent research. In this regard, the most fundamental questions are how to choose Counterexamples to Induction (CTIs) and how to generalize them into (blocking) lemmas. Answers to both questions influence performance of the algorithm by directly affecting the quality of the lemmas learned. In this paper, we present a new IC3-based algorithm, called QUIP1, that is designed to more aggressively propagate (or push) learned lemmas to obtain a safe inductive invariant faster. QUIP modifies the recursive blocking procedure of IC3 to prioritize pushing already discovered lemmas over learning of new ones. However, a naive implementation of this strategy floods the algorithm with too many useless lemmas. In QUIP, we solve this by extending IC3 with may-proof-obligations (corresponding to the negations of learned lemmas), and by using an under-approximation of reachable states (i.e., states that witness why a may-proof-obligation is satisfiable) to prune non-inductive lemmas. We have implemented QUIP on top of an industrial-strength implementation of IC3. The experimental evaluation on HWMCC benchmarks shows that the QUIP is a significant improvement (at least 2x in runtime and more properties solved) over IC3. Furthermore, the new reasoning capabilities of QUIP naturally lead to additional optimizations and new techniques that can lead to further improvements in the future.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542254,no
547,Quantifying security risk by measuring network risk conditions,"Software vulnerabilities are the weaknesses in the software that inadvertently allow dangerous operations. If the vulnerability is in a network service, it poses serious security threats because a cyber-attacker can exploit it to gain unauthorized access to the system. Hence, rapid discovery and remediation of network vulnerabilities is critical issues in network security. In today's dynamic IT environment, it is common practice that an organization prioritizes the mitigation of discovered vulnerabilities according to their risk levels. Currently available technologies, however, associate each vulnerability to the static risk level which does not take the unique characteristics of the target network into account. This often leads to inaccurate risk prioritization and less-than-optimal resource allocation. In this research, we introduce a novel way of quantifying the risk of network vulnerability by augmenting the static risk level with conditions specific to the target network. The method calculates the risk value of each vulnerability by measuring the proximity to the untrusted network and risk of the neighboring hosts. The resulting risk value, RCR is a composite index of the individual risk, network location and neighborhood risk conditions. Thus, it can be effectively used for prioritization, comparison and trending. We tested the methodology through the network intrusion simulation. The results shows average 88.9% the correlation between RCR and number of successful attacks on each vulnerability.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166562,yes
548,Replicating and Re-Evaluating the Theory of Relative Defect-Proneness,"A good understanding of the factors impacting defects in software systems is essential for software practitioners, because it helps them prioritize quality improvement efforts (e.g., testing and code reviews). Defect prediction models are typically built using classification or regression analysis on product and/or process metrics collected at a single point in time (e.g., a release date). However, current defect prediction models only predict if a defect will occur, but not when, which makes the prioritization of software quality improvements efforts difficult. To address this problem, Koru et al. applied survival analysis techniques to a large number of software systems to study how size (i.e., lines of code) influences the probability that a source code module (e.g., class or file) will experience a defect at any given time. Given that 1) the work of Koru et al. has been instrumental to our understanding of the size-defect relationship, 2) the use of survival analysis in the context of defect modelling has not been well studied and 3) replication studies are an important component of balanced scholarly debate, we present a replication study of the work by Koru et al. In particular, we present the details necessary to use survival analysis in the context of defect modelling (such details were missing from the original paper by Koru et al.). We also explore how differences between the traditional domains of survival analysis (i.e., medicine and epidemiology) and defect modelling impact our understanding of the size-defect relationship. Practitioners and researchers considering the use of survival analysis should be aware of the implications of our findings.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914599,no
549,Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling,"Mixed-criticality real-time scheduling has been developed to improve resource utilization while guaranteeing safe execution of critical applications. These studies use optimistic resource reservation for all the applications to improve utilization, but prioritize critical applications when the reservations become insufficient at runtime. Many of them however share an impractical assumption that all the critical applications will simultaneously demand additional resources. As a consequence, they under-utilize resources by penalizing all the low-criticality applications. In this paper we overcome this shortcoming using a novel mechanism that comprises a parameter to model the expected number of critical applications simultaneously demanding more resources, and an execution strategy based on the parameter to improve resource utilization. Since most mixed criticality systems in practice are component-based, we design our mechanism such that the component boundaries provide the isolation necessary to support the execution of low-criticality applications, and at the same time protect the critical ones. We also develop schedulability tests for the proposed mechanism under both a flat as well as a hierarchical scheduling framework. Finally, through simulations, we compare the performance of the proposed approach with existing studies in terms of schedulability and the capability to support low-criticality applications.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176022,yes
550,Strategies for Prioritizing Test Cases Generated Through Model-Based Testing Approaches,"Software testing is expensive and time consuming,especially for complex software. In order to deal with the costof testing, researchers develop Model-Based Testing (MBT). InMBT, test cases are generated automatically and a drawback isa huge generated test suite. Our research aims at studying the Test Case Prioritization problem in MBT context. So far, we already evaluated the influence of the model structure and the characteristics of the test cases that fail. Results suggest that the former does not affect significantly the performance of techniques, however, the latter indeed represents a major impact. Therefore, a worthy information in this context might be an expert who knows the crucial parts of the software, thus we propose the first version of a prioritization technique that considers hints from the expert and the distance notion in order to prioritize test cases. Evaluation and tuning of the technique are ongoing, but preliminary evaluation reveals promising results.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203104,yes
551,Supporting Continuous Integration by Code-Churn Based Test Selection,"Continuous integration promises advantages in large-scale software development by enabling software development organizations to deliver new functions faster. However, implementing continuous integration in large software development organizations is challenging because of organizational, social and technical reasons. One of the technical challenges is the ability to rapidly prioritize the test cases which can be executed quickly and trigger the most failures as early as possible. In our research we propose and evaluate a method for selecting a suitable set of functional regression tests on system level. The method is based on analysis of correlations between test-case failures and source code changes and is evaluated by combining semi-structured interviews and workshops with practitioners at Ericsson and Axis Communications in Sweden. The results show that using measures of precision and recall, the test cases can be prioritized. The prioritization leads to finding an optimal test suite to execute before the integration.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167168,yes
552,Test case analytics: Mining test case traces to improve risk-driven testing,"In risk-driven testing, test cases are generated and/or prioritized based on different risk measures. For example, the most basic risk measure would analyze the history of the software and assigns higher risk to the test cases that used to detect bugs in the past. However, in practice, a test case may not be exactly the same as a previously failed test, but quite similar. In this study, we define a new risk measure that assigns a risk factor to a test case, if it is similar to a failing test case from history. The similarity is defined based on the execution traces of the test cases, where we define each test case as a sequence of method calls. We have evaluated our new risk measure by comparing it to a traditional risk measure (where the risk measure would be increased only if the very same test case, not a similar one, failed in the past). The results of our study, in the context of test case prioritization, on two open source projects show that our new risk measure is by far more effective in identifying failing test cases compared to the traditional risk measure.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070482,yes
553,Test case prioritization for regression testing based on ant colony optimization,"Test case prioritization technique is an efficient method to improve regression testing activities. It orders a regression test suite to execute the test cases with higher priority earlier than those with lower priority, and the problem is how to optimize the test case ordering according to some criterion. In this paper, we have proposed an algorithm which prioritizes the test cases based on ant colony optimization (ACO), considering three factors: number of faults detected, execution time and fault severity, and these three factors are used in ant colony optimization algorithm to help to reveal more severe faults at earlier stage of the regression testing process. The effectiveness of the algorithm is demonstrated using the metric named APFD, and the results of experiment show the algorithm optimizes the test case orderings effectively.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339054,yes
554,Test case prioritization with textual comparison metrics,"Regression testing of a large test pool consistently needs a prioritization technique that caters requirements changes. Conventional prioritization techniques cover only the methods to find the ideal ordering of test cases neglecting requirement changes. In this paper, we propose string dissimilarity-based priority assignment to test cases through the combination of classical and non-classical textual comparison metrics and elaborate a prioritization algorithm considering requirement changes. The proposed technique is suitable to be used as a preliminary testing when the information of the entire program is not in possession. We performed evaluation on random permutations and three textual comparison metrics and concluded the findings of the experiment.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475187,yes
555,Test case prioritization: An approach based on modified ant colony optimization (m-ACO),"Intense and widespread usage of software in every field of life has attracted the researchers to focus their attention on developing the methods to improve the efficiency of software testing; which is the most crucial and cost intensive phase of software development. Software testing aims to uncover the potential faults in Application Under Test by running the test cases on software code. Software code keeps on changing as the uncovered faults during testing are fixed by the developers. Regression testing is concerned with verifying the modified software code to ensure that changes in software code does not induce any undesired effect on rest of the code. Test Case Prioritization is a regression testing technique which re-schedule the execution sequence of test cases to improve the fault detection rate and enhance the performance of regression test suite. This paper focuses on proposing a novel method ""m-ACO"" for test case prioritization and the performance evaluation of the proposed method using Average Percentage of faults Detected.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375627,yes
556,Test case selection and prioritization using cuckoos search algorithm,"Regression Testing is an inevitable and very costly activity that is implemented to ensure the validity of new version of software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization for proper selection and schedule of test cases in a specific sequence, fulfilling some chosen criteria. Cuckoo search (CS) algorithm is an optimization algorithm proposed by Yang and Deb [13]. It is inspired by the obligate brood parasitism of some cuckoo species by laying their eggs in the nests of other host birds. Cuckoo Search is very easy to implement as it depends on single parameter only unlike other optimization algorithms. In this paper a test case selection and prioritization algorithm has been proposed using Cuckoo Search. This algorithm selects and prioritizes the test cases based on the number of faults covered in minimum time. The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155012,yes
557,Test case selection for networked production systems,"This paper provides a discussion on the coming technological changes in process automation of networked production systems, which will change the testing procedure. In the smart factory of the future there will be no possibility to reach a test coverage of 100%, assuming a flexible automation with continuous reconfiguration and dynamic changes during runtime. Consequently, large amounts of test cases and powerful algorithms for their prioritization are needed in order to certify the correct functionality of the production systems in the network. A concept is presented on how to analyze and prioritize the enormous amount of test cases resulting from the changes during runtime. The proposed approach for test case selection utilizes information of the product, the process and the status of the for the prioritization and selection.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301604,yes
558,Test cases prioritization for software regression testing using analytic hierarchy process,"Test cases are considered an important asset in the software testing process since they are used to detect defects in the software. In order to produce quality software covering all of the requirements, the test case designer requires much time and effort in designing test cases to cover all requirements and conditions according to the test case structure. This research proposes a method for storing and retrieving of test cases affected by software requirements changes, as well as ranking the retrieved test cases using the AHP method to improve the quality of the ranking. There are to assist system testers in identifying test cases for complete regression testing. An example application of the proposed method will also be presented.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219790,yes
559,Testing analytics on software variability,"Software testing is a tool-driven process. However, there are many situations in which different hardware/software components are tightly integrated. Thus system integration testing has to be manually executed to evaluate the system's compliance with its specified requirements and performance. There could be many combinations of changes as different versions of hardware and software components could be upgraded and/or substituted. Occasionally, some software components could even be replaced by clones. The whole system after each component change demands to be re-tested to ensure proper system behavior. For better utilization of resources, there is a need to prioritize the past test cases to test the newly integrated systems. We propose a way to facilitate the use of historical testing records of the previous systems so that a testcase portfolio can be developed, which intends to maximize testing resources for the same integrated product family. As the proposed framework does not consider much of internal software complexity, the implementation costs are relatively low.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070483,no
560,The Effect of GoF Design Patterns on Stability: A Case Study,"Stability refers to a software system's resistance to the ƒ??ripple effectƒ?, i.e., propagation of changes. In this paper, we investigate the stability of classes that participate in instances/occurrences of GoF design patterns. We examine whether the stability of such classes is affected by (a) the pattern type, (b) the role that the class plays in the pattern, (c) the number of pattern occurrences in which the class participates, and (d) the application domain. To this end, we conducted a case study on about 65.000 Java open-source classes, where we performed change impact analysis on classes that participate in zero, one (single pattern), or more than one (coupled) pattern occurrences. The results suggest that, the application of design patterns can provide the expected ƒ??shieldingƒ? of certain pattern-participating classes against changes, depending on their role in the pattern. Moreover, classes that participate in coupled pattern occurrences appear to be the least stable. The results can be used for assessing the benefits and liabilities of the use of patterns and for testing and refactoring prioritization, because less stable classes are expected to require more effort while testing, and urge for refactoring activities that would make them more resistant to change propagation.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7066925,no
561,The evaluation of the results of an eye tracking based usability tests of the so called Instructor's Portal framework (http://tanitlap.ektf.hu/csernaiz),"The research discussed in this paper can be positioned at the cross-section of Applied Computer Science, Didactics and Human-Computer Interaction. Accordingly, an Instructor's Portal (IPo) framework system was developed at the Department of Human Informatics of the Eszterh?­zy K?­roly College (EKC) in 2015. The aim of the framework system is to enable instructors working in higher education institutions to establish, customize, and update their own webpages independently without any help from informatics professionals. Said system not only fulfills a gap filling function in the higher education sphere, but performs complex tasks while serving a wide range of users. In order to establish a logically arranged content structure and user interface the respective development process observed several developmental principles, methods, and web-ergonomic rules. This paper introduces the results of usability tests and examinations pertaining to the system. The examination utilized, an eye-tracking hardware device and a mouse movement recording software along with a special software facilitating the evaluation and presentation of the respective results. This essay introduces and highlight how the respective apparatus complements the traditional human observation-based usability tests while identifying the cognitive skills to be ascertained with such devices, one of the main priorities of Cognitive Infocommunications (CogInfoCom).",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390637,no
562,Total coverage based regression test case prioritization using genetic algorithm,"Regression Testing is a test to ensure that a program that was changed is still working. Changes introduced to a software product often come with defects. Additional test cases are, this could reduce the main challenges of regression testing is test case prioritization. Time, effort and budget needed to retest the software. Former studies in test case prioritization confirm the benefits of prioritization techniques. Most prioritization techniques concern with choosing test cases based on their ability to cover more faults. Other techniques aim to maximize code coverage. Thus, the test cases selected should secure the total coverage to assure the adequacy of software testing. In this paper, we present an algorithm to prioritize test cases based on total coverage using a modified genetic algorithm. Its performance on the average percentage of condition covered and execution time are compared with five other approaches.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207103,yes
563,Towards a framework for automatic correction of anti-patterns,"One of the biggest concerns in software maintenance is design quality; poor design hinders software maintenance and evolution. One way to improve design quality is to detect and correct anti-patterns (i.e., poor solutions to design and implementation problems), for example through refactorings. There are several approaches to detect anti-patterns, that rely on metrics and structural properties. However, finding a specific solution to remove anti-patterns is a challenging task as candidate refactorings can be conflicting and their number very large, making it costly. Hence, development teams often have to prioritize the refactorings to be applied on a system. In addition to this, refactoring is risky, since non-experienced developers can change the behaviour of a system, without a comprehensive test suite. Therefore, there is a need for tools that can automatically remove anti-patterns. We will apply meta-heuristics to propose a technique for automated refactoring that improves design quality.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081891,no
564,Towards testing variability intensive systems using user reviews,"Variability intensive systems are software systems that describe a large set of diverse and different configurations that share some characteristics. This high number of configurations makes testing such systems an expensive and error-prone task. For example, in the Android ecosystem we can find up to 24 different valid configurations, thus, making it impossible to test an application on all of them. To alleviate this problem, previous research suggest the selection of a subset of test cases that maximize the changes of finding errors while maximizing the diversity of configurations. Concretely, the proposals focus on the prioritization and selection of tests, so only relevant configurations are tested according to some criterion. In this paper, we envision the use of user reports to prioritize and select meaningful tests. To do this, we explore the use of recommender systems as a possible improvement to the selection of test cases in intensive variability systems.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333410,yes
565,Tracking down high coverage configuration using clustering and fault detection,"Mostly all software systems are highly configured, it has many benefits but there is a difficulty of software testing because there will be unique errors could be hidden in any of the configurations and undergoing testing for each of the configurations will lead to expensive testing and it is also impractical. The dependable systems will have some mechanism for fault tolerance in software testing. If the rate of the fault detection is calculated then the coverage of the configuration can be easily generated. First load the application for which it is going to be tested by using our test case prioritization approach and loading the dataset for the test case for the given application. After this process, need to assign the individual ids for all the test cases in the test case dataset. Also it is able to add the test cases in the dynamic nature. Then to compute the test case prioritization, first built the dependency structure for the test cases. Through the approach get the height and weight matrix for the test cases after this computation the test cases. The cosine similarity values between the test cases. In the similarity values it will show how it is highly related with the other test cases. Thus the clustering approach is introduced for grouping the test cases. These test cases are analyzed for measuring their relevancy and relationship between the test cases using their constrains and the clustering of the test cases is done for the better result in the rate of fault detect. With the Average percentage fault detection the graph is drawn and it shown the high coverage configurations.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453839,yes
566,UPMOA: An improved search algorithm to support user-preference multi-objective optimization,"Multi-objective search algorithms (e.g., non-dominated sorting genetic algorithm II (NSGA-II)) have been applied extensively to solve various multi-objective optimization problems in software engineering such as problems in testing. However, existing multi-objective algorithms usually treat all the objectives with equivalent priorities and do not provide a mechanism to reflect various user preferences when guiding search. The need to have such a mechanism was observed in one of our industrial projects on applying search algorithms for test optimization of a product line of Videoconferencing Systems (VCSs) called Saturn, where user preferences must be incorporated into optimization objectives, based on domain knowledge of test engineers for VCS testing. To address this, we propose an extension to the most commonly-used multi-objective search algorithm NSGA-II, which has shown promising results with user preferences. We name the extension as User-Preference Multi-Objective Optimization Algorithm (UPMOA), which includes a user preference indicator p and is based on existing weight assignment strategies. We empirically evaluated UPMOA with two industrial problems focusing on optimizing the test execution system for Saturn in Cisco. To assess the performance and scalability of UPMOA, inspired by the two industrial problems, in total we created 64000 artificial problems with 128 different sets of user preferences. The evaluation includes two aspects: 1) Three weight assignment strategies together with UPMOA were empirically evaluated to identify a best weight assignment strategy for p. Results show that the Uniformly Distributed Weights (UDW) strategy can assist UPMOA in achieving the best performance; 2) UPMOA was compared with three representative multi-objective search algorithms (including NSGA-II) and results show that UPMOA significantly outperformed the others and has the ability to solve problems with a wide range of complexity.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381833,no
567,Using Artificial Bee Colony for Code Coverage Based Test Suite Prioritization,"The goal of test suite prioritization is maximizing fault detection and code coverage rate. Several nature inspired optimization algorithms such as Swarm Intelligence (SI) have been studied for the optimization of such problems. The studies revealed the benefits of Artificial Bee Colony (ABC) over other algorithms. ABC and its variations were implemented in software testing areas, test suite prioritization in particular. However, most SI based approaches focus on fault detection ability which is difficult to predict. In this paper, the standard ABC algorithm is used to prioritize test suites based on code coverage. The results reveal that ABC shows promising results and, hence, is a great candidate for prioritizing test suites. It also suggests that a modification to the standard ABC algorithm or combination of ABC and another SI algorithm should yield an even better result.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371038,yes
568,Using Defect Taxonomies for Testing Requirements,"Systematic defect management based on bug-tracking systems such as Bugzilla is well established and has been successfully used in many software organizations. Defect management weights the failures observed during test execution according to their severity and forms the basis for effective defect taxonomies. In practice, most defect taxonomies are used only for the a posteriori allocation of testing resources to prioritize failures for debugging. Thus, these taxonomies' full potential to control and improve all the steps of testing has remained unexploited. This is especially the case for testing a system's user requirements. System-level defect taxonomies can improve the design of requirements-based tests, the tracing of defects to requirements, the quality assessment of requirements, and the control of the relevant defect management. So, we developed requirements-based testing with defect taxonomies (RTDT). This approach is aligned with the standard test process and uses defect taxonomies to support all phases of testing requirements. To illustrate this approach and its benefits, we use an example project (which we call Project A) from a public health insurance institution.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799150,no
569,Using Fuzzy Logic and Symbolic Execution to Prioritize UML-RT Test Cases,"The relative ease of test case generation associated with model-based testing can lead to an increased number of test cases being identified for any given system; this is problematic as it is becoming near impossible to run (or even generate) all of the possible tests in available time frames. Test case prioritization is a method of ranking the tests in order of importance, or priority based on criteria specific to a domain or implementation, and selecting some subset of tests to generate and run. Some approaches require the generation of all tests, and simply prioritize the ones to be run, however we propose an approach that would prevent unnecessary generation of tests through the use of symbolic execution trees to determine which tests provide the most benefit to coverage of execution. Our approach makes use of fuzzy logic, specifically fuzzy control systems, to prioritize test cases generated from these execution; the prioritization is based on natural language rules about testing priority. Within this paper we present our motivation, some background research, our methodology and implementation, results, and conclusions.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102610,yes
570,Using Partition Information to Prioritize Test Cases for Fault Localization,"Fault Localization Prioritization (FLP) aims at reordering existing test cases so that the location of detected faulty components can be identified earlier, using certain fault localization techniques. Although some researchers have proposed adaptive prioritization strategies with white-box code coverage information, such information may not always be available. In this paper, we address the FLP problem using black-box information derived from partitioning the input domain. Based on the well-known technique of Spectra-Based Fault Localization (SBFL), three test case prioritization strategies are designed following some basic SBFL heuristics. The implementation of these proposed strategies relies only on the partition information, and does not require any test case execution history. Experiments show that our strategies, when compared with pure random selection, result in a faster localization of faulty statements, reducing the number of test case executions required. Here, we analyze the characteristics and merits of the three proposed strategies.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273609,yes
571,A hybrid approach for test case prioritization and optimization using meta-heuristics techniques,"Software testing is a very crucial and important phase for (SDLC) software development life cycle. Software is being tested on its effectiveness for generating good quality software. Regression testing is done by considering the constraints of resources and in this phase optimization of test suite is very important and crucial. This paper mainly aims to make use of hybrid approach of meta-heuristics, It comprises of two algorithms first is genetic algorithm and second is particle swarm optimization. In addition to algorithm the comparison of proposed algorithm hybrid GA_PSO with other optimization algorithms are been done. To validate the research Average Percentage Fault Detection (APFD) metric is used for comparison and fitness evaluation of the proposed algorithm.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975319,yes
572,A hybrid approach for test case prioritization and selection,"Software testing consists in the dynamic verification of the behavior of a program on a set of test cases. When a program is modified, it must be tested to verify if the changes did not imply undesirable effects on its functionality. The rerunning of all test cases can be impossible, due to cost, time and resource constraints. So, it is required the creation of a test cases subset before the test execution. This is a hard problem and the use of standard Software Engineering techniques could not be suitable. This work presents an approach for test case prioritization and selection, based in relevant inputs obtained from a software development environment. The approach uses Software Quality Function Deployment (SQFD) to deploy the features relevance among the system components, Mamdani fuzzy inference systems to infer the criticality of each class and Ant Colony Optimization to select test cases. An evaluation of the approach is presented, using data from simulations with different number of tests.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744363,yes
573,A Multi-Objective Technique to Prioritize Test Cases,"While performing regression testing, an appropriate choice for test case ordering allows the tester to early discover faults in source code. To this end, test case prioritization techniques can be used. Several existing test case prioritization techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test case prioritization technique that determines the ordering of test cases that maximize the number of discovered faults that are both technical and business critical. In other words, our new technique aims at both early discovering faults and reducing the execution cost of test cases. To this end, we automatically recover links among software artifacts (i.e., requirements specifications, test cases, and source code) and apply a metric-based approach to automatically identify critical and fault-prone portions of software artifacts, thus becoming able to give them more importance during test case prioritization. We experimentally evaluated our technique on 21 Java applications. The obtained results support our hypotheses on efficiency and effectiveness of our new technique and on the use of automatic artifacts analysis and weighting in test case prioritization.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362042,yes
574,A novel approach for selecting an effective regression testing technique,"All software systems need modifications with time, these modifications involve different types or amounts of code modifications in different versions. To validate these modifications many regression testing sessions are needed. But researchers do not have a single regression testing technique that can be used on every version. The objective of this scrutiny is to evolve a methodology that attempts to determine the re-testing technique that would be effective for every re-testing period accounting testing domain and conditions. This methodology is based on Revised Analytical Hierarchy Process (Revised AHP). There are numerous regression testing techniques. But this investigation is limited to test case prioritization techniques only. The result showed that prioritization techniques selected by proposed technique are more efficacious than those used by the forgoing techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724438,yes
575,A random and coverage-based approach for fault localization prioritization,"Fault Localization Prioritization (FLP) aims to order the execution sequence of test cases so that faulty statements in a faulty program can be localized faster. FLP is an important part of the automation of testing and fault localization in software engineering. The key issue is to identify which test cases can provide most useful information to help locate the faulty statement. Assuming the well-known technique of Spectra-Based Fault Localization (SBFL) is applied, this paper evaluates the quality of a test case based on the characteristics of its statement coverage information. We propose the COverage-based Random (COR) approach to address the FLP problem. Two statement coverage characteristics, the diversity characteristic and the failure-like characteristic, are analyzed and identified as having significant impacts on the effectiveness of fault localization. When using the COR approach, each test case is examined and the degree of each characteristic is measured, with test cases showing high degrees of the characteristics being assigned higher priority for execution. Because of the power of random strategies to improve the robustness of the approach, some random factors in the selection of test cases are included. Empirical studies show that, compared with existing approaches, the COR approach results in a faster localization of faulty statements, reducing the number of necessary test case executions.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7531562,yes
576,A study of critical success factors on software quality assurance of cloud networking devices,"In recent years, many more enterprises and organizations are embracing cloud computing for its commercial benefits and competitive advantages. However, these companies have prioritized hardware specifications when evaluating cloud networking devices, often neglecting the importance of software quality as a result. Most cloud networking devices currently offered on the market include embedded software systems, which mean that software and hardware compatibility can be one of the most essential characteristics to look for. In order to investigate the critical success factors (CSFs) for the software quality assurance (QA) of cloud networking devices, this study employed a Plan-Do-Check-Act (PDCA) research framework. Important Software QA factors from relevant IEEE standards and hardware factors related to software quality were then employed in the 4 facets of the PDCA process to conduct a 2-stage analysis. For the first stage of this study, 5 experts were interviewed and asked to complete a survey form and select CSFs that were then used to generate a questionnaire for the second stage where a total of 15 experts and QA engineers were invited to fill out questionnaires. Completed questionnaires were then subject to an analytic hierarchy process (AHP) to calculate the weight and priority orders for each of the CSFs. The overall results of Stage 2 indicate that Cloud devices security testing, Security of cloud networking devices, and Resources and their allocation are the top three orders of all CSFs when experts take a real concern in software QA of cloud networking devices.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811054,yes
577,A tool for constrained pairwise test case generation using statistical user profile based prioritization,"Pairwise testing is a wildly used approach in order to reduce the size of test suite and take steps to combinatorial testing problems because of an extensively large number of possible combinations between input parameters and values. In some cases, there will be invalid combinations between input parameters and values if constraints have not been handled. In this paper, we present a pairwise test generation tool called CPTG, a tool to generate test cases for pairwise testing by applying user profile for guiding and prioritizing in order to select optimal input parameters and values which do not depend on individual tester skills and also providing constraint handling solution between input parameters and values. We performed experiments and comparison with other tools. The experimental results of our tool demonstrated that our tool becomes particularly valuable in guiding testing with a maximized reliability by testing the most frequently used of the system and can generate comparable results of the size of the test case set.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748881,yes
578,ACO based embedded system testing using UML Activity Diagram,"This paper proposed a model-based technique for test scenario generation using Activity Diagram (AD). We transform an AD specification into an intermediate graph called Activity Interaction Graph (AIG) using the proposed parser. After that, we apply combination of BFS and DFS algorithms for generating test scenarios. Then, we apply an algorithm called ACOToTSP (Ant Colony Optimization for Test Scenarios Prioritization) algorithm on the generated test scenarios with respect to some decision and concurrent criteria, for prioritizing the test scenarios. This approach generates test scenarios according to forks, Joins, and merge point's strength in the activity diagram.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847997,yes
579,An Adaptive Sequence Approach for OOS Test Case Prioritization,"Test case prioritization (TCP) attempts to improve fault detection effectiveness by scheduling important test cases earlier, where important is determined by some criteria and strategy. Adaptive random sequences (ARSs) may be applied to improve the effectiveness of TCP in black-box testing. In this paper, to improve the effectiveness of TCP for object-oriented software, we present an ARS approach based on clustering techniques. In the proposed approach, test cases are clustered according to the number of objects and methods, using two clustering algorithms - K-means and K-medoids. Our proposed sampling strategy can construct ARSs within the clustering framework, constructing two ARS sequences based on the two clustering algorithms, which results in generated test cases with different execution sequences. We also report on experimental studies to verify the proposed approach, with the results showing that our approach can enhance the probability of earlier fault detection, and deliver higher effectiveness than random prioritization.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789402,yes
580,An exploration of various quality of service mechanisms in an OpenFlow and software defined networking environment,"Technology prevalent in networks today logically detaches the control from the data plane yet physically deploys them on the same device on all network devices. This has negatively resulted in expensive management of disparate and closed network devices and the inability to implement coherent, synchronized, and wide-ranged network policies. Software Defined Networking (SDN) solves these issues by providing an open and virtually centralized network system through cost-effective and gradual network introduction that minimizes investment in infrastructure. The OpenFlow protocol defines communications from the control plane where the logical center of the network, the controller, resides to the packet-routing devices in the data plane and vice-versa. The Mininet SDN emulator mimics real-world SDN network environments for rapid system testing and proof-of-concept experimentation. Quality Of Service (QoS) represents a ripe and proven technology that allows for the enforcement of priorities and assured performance levels on network links and objects. This study examined the application of QoS-based methods to an SDN OpenFlow environment. QoS parameters were utilized to implement rudimentary Class-Based Queuing (CBQ) scheduling algorithms on a custom OpenFlow controller within a Mininet-emulated SDN network topology. Two CBQ algorithms were used in the experiments contrasted by the classes on which they were based. One algorithm is Basic CBQ which modeled scheduling based on the ƒ??Streamingƒ?, ƒ??Burstyƒ?, and ƒ??Catch-Allƒ? traffic types. The other algorithm, Source CBQ, was based on predefined Source-IP address groupings of client hosts. Each Class Profile was broken down in terms of QoS enforcement points at the Leaf Switches and at the Core Switch. Tests were conducted on the topology with these CBQ-based Class Profiles to gather performance data and observe any issues encountered. Experiment results showed that CBQs at the Leaves yielded better overall average bandwidth than CBQs at the Core with Basic CBQ at the Leaves at 2% higher than Basic CBQ at the Core and Source CBQ at the Leaves at 30% higher than Source CBQ at the Core. This indicated that QoS implemented at the Client Leaf Switches rather than at the Core Switch yielded better performance as they were not prone to the same bottlenecks as the Core Switch. Additionally, experiment results seemingly showed that Basic CBQ, whose classes are based on traffic types, yielded optimum performance at less variability as it managed bandwidth better by segmenting traffic and limiting errors.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811055,no
581,Application of Mahalanobis-Taguchi Method and 0-1 Programming Method to Cost-Effective Regression Testing,"To enhance the cost effectiveness of regression testing, this paper proposes a method for prioritizing test cases. In general, a test case can be evaluated from various different points of view, therefore whether it is worth it to re-run should be discussed using multi criteria. This paper shows that the Mahalanobis-Taguchi (MT) method is a useful way to successfully integrate different evaluations of a test case. Moreover, this paper proposes to use the 0-1 programming method together with the MT method in order to take into account not only the priority of a test case but also its cost to run. The empirical study with 300 test cases for an industrial software system shows that the combination of the MT method and the 0-1 programming method is more cost-effective than other conventional methods.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592803,yes
582,Applying Assemble Clustering Algorithm and Fault Prediction to Test Case Prioritization,"Cluster application is proposed as an efficient approach to improve test case prioritization, Test case in a same cluster are considered to have similar behaviors. In the process of cluster test case, the selection of test case feature and the clusters number have great influence on the clustering results. but to date almost clustering algorithm to improve test case prioritization are selected random clusters number and clustering result are based on one or a few of the code features, the paper propose a new prioritization techniques that not only consider the best clusters number but also produce the best clustering result based on test case multidimensional feature. After clustering, considering the inter-cluster prioritization and intra-cluster prioritization,in order to improve the effectiveness of our approach, the fault prediction value of code corresponding to the test case is used as one of a prioritization weight. Finally,we implemented an empirical studies using an industrial software to illustrate the effectiveness of the test case prioritization techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780203,yes
583,Automated comparison of X-ray images for cargo scanning,"Customs administrations are responsible for the enforcement of fiscal integrity and security of movements of goods across land and sea borders. In order to verify whether the transported goods match the transport declaration, X-ray imaging of containers is used at many customs site worldwide. The main objective of the research and development project ƒ??Automated Comparison of X-ray Images for Cargo Scanning (ACXIS)ƒ?, which is funded by the European 7<sup>th</sup>Framework Program, is to improve the efficiency and effectiveness of the inspection procedures of cargo at customs using X-ray technology. The current inspection procedures are reviewed to identify risks, catalogue illegal cargo, and prioritize detection scenarios. Based on these results, we propose an integrated solution that provides automation, information exchange between customs administrations, and computer-based training modules for customs officers. Automated target recognition (ATR) functions analyze the X-ray image after a scan is made to detect certain types of goods such as cigarettes, weapons and drugs in the freight or container. Other helpful information can also be provided, such as the load homogeneity, total or partial weight, or the number of similar items. The ATR functions are provided as an option to the user. The X-ray image is transformed into a manufacturer-independent format through geometrical and spectral corrections and stored into a database along with the user feedback and other related data. This information can be exchanged with similar systems at other sites, thus facilitating information exchange between customs administrations. The database is seeded with over 30'000 examples of legitimate and illegal goods. These examples are used by the ATR functions through machine learning techniques, which are further strengthened by the information exchange. In order to improve X-ray image interpretation competency of human operators (customs officers), a computer-based training software is developed that simulates these new inspection procedures. A study is carried out to validate the effectiveness and efficiency of the computer-based training as well as the implemented procedures. Officers from the Dutch and Swiss Customs administrations partake in the study, covering both land and sea borders.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815714,yes
584,Automated testcase generation for software quality assurance,"The overall venture of the software engineering is to guarantee the delivery of high quality software to the client. To certify high quality software, it is required to test software. Testing is a decisive constituent of software engineering. In software testing there are number of underlying issues like effective generation of test cases, prioritisation of test cases which need to be tackled. This automated test case generation mainly depends on these four aspects: test strategy, test case generation, test execution and test evaluation. Test strategy is a collection of events that determines the testing approach to be followed by the testing team. The test case generation refers to the generation of testcases based on the certain application. The test execution briefs about the execution of those tests then comparing the expected result with actual result. The test evaluation investigates the test cases and helps us to generate test report and software quality assurance report automatically. The intention of producing this tool is to generate test cases automatically and to decrease the cost of testing in addition to accumulate the time of deriving test cases physically. Hence this system helps to improve overall quality of the software.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727003,no
585,Automatic Reproducible Crash Detection,"Crash reproduction, which spends much time of developers in reading and understanding source code, is a crucial yet time-consuming task in program debugging. To reduce the time and resource cost, automatic techniques of test generation have been proposed. These techniques aim to automatically generate test cases to reproduce the scenario of a crashed project. Unfortunately, due to the lack of a detailed comprehension of the source code, a generated test case may fail in reproducing an expected crash. In this paper, we propose an automatic approach to reproducible bug detection. This approach predicts whether a crash is difficult to reproduce or not via training a classifier based on historical reproducible crash data. If a crash is difficult to reproduce, it is better to assign the crash to a developer, instead of using an automatic technique of test generation. Our work can help to prioritize crashes and to save the cost of developers. Preliminary experiments show that our approach effectively detects reproducible crashes via evaluating 45 crashes.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780193,no
586,Automatically Learning Semantic Features for Defect Prediction,"Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886912,no
587,Automation Architecture for Bayesian Network Based Test Case Prioritization and Execution,"An automation architecture for Bayesian Network based test case prioritization is designed for software written in Java programming language following the approach proposed by Mirarab and Tahvildari [2]. The architecture is implemented as an integration of a series of tools and called Bayesian Network based test case prioritization and execution platform. The platform is triggered by a change in the source code, then it collects necessary information to be supplied to Bayesian Network and uses Bayesian Network evaluation results to run high priority unit tests.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552177,yes
588,AVISAR - a three tier architectural framework for the testing of Object Oriented Programs,"In this research paper, we have proposed a three tier conceptual framework AVISAR for the testing of Object Oriented Programs. We have devised the proposed framework into three levels: Requirements Modeling, Test Case Generation and Effort Estimation. At the first level, we have used the Extend relationship of the Use Case to capture the events generated by the Classes during message passing between them through their objects. These events were captured by an Object Instantiator using Event Templates and further given to a Test case scenario generator, which generates the test cases. At the second level, the test cases will be generated based on the event templates that are used for capturing the events generated by the Extend relationship of the Use cases. Further, we have proposed a Genetic Algorithm (GA) for the effective test case prioritization to ensure maximum code coverage. At the third level, effort estimation, we have used the Cyclometric Complexity and Token count to perform the effort estimation for the Software under Test (SUT).",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918730,yes
589,Comparing White-Box and Black-Box Test Prioritization,"Although white-box regression test prioritization has been well-studied, the more recently introduced black-box prioritization approaches have neither been compared against each other nor against more well-established white-box techniques. We present a comprehensive experimental comparison of several test prioritization techniques, including well-established white-box strategies and more recently introduced black-box approaches. We found that Combinatorial Interaction Testing and diversity-based techniques (Input Model Diversity and Input Test Set Diameter) perform best among the black-box approaches. Perhaps surprisingly, we found little difference between black-box and white-box performance (at most 4% fault detection rate difference). We also found the overlap between black- and white-box faults to be high: the first 10% of the prioritized test suites already agree on at least 60% of the faults found. These are positive findings for practicing regression testers who may not have source code available, thereby making white-box techniques inapplicable. We also found evidence that both black-box and white-box prioritization remain robust over multiple system releases.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886931,yes
590,Conc-iSE: Incremental symbolic execution of concurrent software,"Software updates often introduce new bugs to existing code bases. Prior regression testing tools focus mainly on test case selection and prioritization whereas symbolic execution tools only handle code changes in sequential software. In this paper, we propose the first incremental symbolic execution method for concurrent software to generate new tests by exploring only the executions affected by code changes between two program versions. Specifically, we develop an inter-thread and inter-procedural change-impact analysis to check if a statement is affected by the changes and then leverage the information to choose executions that need to be re-explored. We also check if execution summaries computed in the previous program can be used to avoid redundant explorations in the new program. We have implemented our method in an incremental symbolic execution tool called Conc-iSE and evaluated it on a large set of multithreaded C programs. Our experiments show that the new method can significantly reduce the overall symbolic execution time when compared with state-of-the-art symbolic execution tools such as KLEE.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582788,no
591,Concolic test generation for PLC programs using coverage metrics,"This paper presents a technique for fully automated generation of test cases for PLC programs adhering to the IEC 61131-3 standard. While previous methods strive for completeness and therefore struggle with the state explosion we pursue a symbolic execution based approach, dropping completeness but nevertheless achieving similar or even better results in practice. The core component is a symbolic execution engine which chooses the next state to execute, handles constraints emerging during the execution and derives respective test vectors leading to a state. To make for a high coverage of the generated tests, we adopt techniques from concolic testing, allow for use of heuristics to prioritise promising states but also merge states to alleviate the path explosion. We exploit peculiarities of PLC semantics to determine reasonable merge-points and unlike similar approaches even handle unreachable code. To examine the feasibility of our technique we evaluate it on function blocks used in industry.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497884,yes
592,Customized Regression Testing Using Telemetry Usage Patterns,"Pervasive telemetry in modern applications is providing new possibilities in the application of regression testing techniques. Similar to how research in bioinformatics is leading to personalized medicine, tailored to individuals, usage telemetry in modern software allows for custom regression testing, tailored to the usage patterns of an installation. By customizing regression testing based on software usage, the effectiveness of regression testing techniques can be greatly improved, leading to reduced testing costs and enhanced detection of defects that are most important to that customer. In this research, we introduce the concept of fingerprinting software usage patterns through telemetry. We provide various algorithms tocompute fingerprints and conduct an empirical study that shows that fingerprints are effective in identifying distinct usage patterns. Further, we discuss how usage fingerprints can be used to improve regression test prioritization run time by over 30 percent compared to traditional prioritization techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816511,no
593,Data flow based quality testing approach using ACO for component based software development,"Component-based Software Engineering (CBSE) has been centered around advancements identified with configuration and implementation of software components and frameworks assembled from software components. Quality Assurance (QA) for CBSE is a new subject in the software development research area. In this paper an enhanced data flow based QA model is presented for CBSE by employing the Ant Colony Optimization (ACO)algorithm to optimize the given code for automatic generation and prioritization of optimal path in decision to decision Control Flow Graph (CFG), which results an enhanced testing phase for QA model with reduced complexity. After that, proposed ACO based approach is also utilized for the generation of test data to satisfy the generated set of paths. This paper additionally exhibits the proposed approach applying it in a program module. Results show that a better testing is achieved by applying proposed ACO based scheme on component based software. Proposed approach ensures full software coverage with least redundancy.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813850,yes
594,Database design for error searching system based on keyword priority,"Because there are too many types of errors occurred in a multi-software integrated platform, such as Integrated Decision Information System (IDIS). It is urgent to design an error searching system to solve all different problems. However, those errors belong to different stages like setup, configuration, and operation, or those errors may occurred in different services, applications, or IP ports, or may be happened in different system software, different version of software, and those errors are also can be classified into different types. The new requirement on the design of database has been announced, that it has to locate the error, find out the reason of this error, as well as the corresponding solution. Also, it requires to provide the location, phase, etc. of an error. For a multi-software integrated platform, this paper proposed a database design for error searching system based on keyword priority. This DB made the correspondence among error, the reason of the error, and corresponding solution, and put them to different categories in terms of their characteristics, such that it is easy to manage, search, and use. This method treats those characteristics as keywords with higher priority, which are correspondent with errors, reasons, and solutions, respectively. While the keywords extracted from errors, reasons, and solutions are treated as keywords with lower priority. Keywords with different priorities can all be used as index to search errors, reasons, and solutions, while keywords with higher priority can be used to filter the searching results finer, and make the searching results more accurate. On one hand, the database design method based on keyword priority has simplified the logical structure of the database. On the other hand, users do not only search the reason and solution of an error, but also can find out the accurate information of the error, such as in which stage, layer, software, or categories, etc. the error has happened. The data of the database is complete, which has been provided by 500 technicians who had found thousands of errors. Those errors haven been added to the DB after tests to make the DB more complete.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811011,yes
595,Diversity-Aware Mutation Adequacy Criterion for Improving Fault Detection Capability,"Many existing testing techniques adopt diversity as an important criterion for the selection and prioritization of tests. However, mutation adequacy has been content with simply maximizing the number of mutants that have been killed. We propose a novel mutation adequacy criterion that considers the diversity in the relationship between tests and mutants, as well as whether mutants are killed. Intuitively, the proposed criterion is based on the notion that mutants can be distinguished by the sets of tests that kill them. A test suite is deemed adequate by our criterion if the test suite distinguishes all mutants in terms of their kill patterns. Our hypothesis is that, simply by using a stronger adequacy criterion, it is possible to improve fault detection capabilities of mutation-adequate test suites. The empirical evaluation selects tests for real world applications using the proposed mutation adequacy criterion to test our hypothesis. The results show that, for real world faults, test suites adequate to our criterion can increase the fault detection success rate by up to 76.8 percentage points compared to test suites adequate to the traditional criterion.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528954,no
596,Do We Have a Chance to Fix Bugs When Refactoring Code Smells?,"Code smells are used to describe code structures that may cause detrimental effects on software and should be refactored. Previous studies show that some code smells have significant effect on faults. However, how to refactor code smells to reduce bugs still needs more concern. We investigate the possibility of prioritizing code smell refactoring with the help of fault prediction results. We also investigate the possibility of improving the performance of fault prediction by using code smell detection results. We use Cohen's Kappa statistic to report agreements between results of code smell detections and fault predictions. We use fault prediction result as an indicator to guide code smell refactoring. Our results show that refactoring Blob, Long Parameter List, and Refused Parent Be Request may have a good chance to detect and fix bugs, and some code smells are particularly useful for improving the recall of fault prediction.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780189,yes
597,Dynamic Integration Test Selection Based on Test Case Dependencies,"Prioritization, selection and minimization of test cases are well-known problems in software testing. Test case prioritization deals with the problem of ordering an existing set of test cases, typically with respect to the estimated likelihood of detecting faults. Test case selection addresses the problem of selecting a subset of an existing set of test cases, typically by discarding test cases that do not add any value in improving the quality of the software under test. Most existing approaches for test case prioritization and selection suffer from one or several drawbacks. For example, they to a large extent utilize static analysis of code for that purpose, making them unfit for higher levels of testing such as integration testing. Moreover, they do not exploit the possibility of dynamically changing the prioritization or selection of test cases based on the execution results of prior test cases. Such dynamic analysis allows for discarding test cases that do not need to be executed and are thus redundant. This paper proposes a generic method for prioritization and selection of test cases in integration testing that addresses the above issues. We also present the results of an industrial case study where initial evidence suggests the potential usefulness of our approach in testing a safety-critical train control management subsystem.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528974,yes
598,Effect of Time Window on the Performance of Continuous Regression Testing,"Test prioritization is an effective technique used to reduce the amount of work required to support regression testing in continuous integration development. It aims at finding an optimal order of tests that can detect regressions faster, potentially increasing the frequency of software releases. Prioritization techniques based on test execution history use the results of preceding executions to determine an optimal order of regression tests in the succeeding test executions. In this paper, we investigate how can execution history be optimally used to increase the effectiveness of regression test prioritization. We analyze the effect of history time window on the fault detection effectiveness of prioritized regression tests. We report an experimental study using a data set from Cisco. The results suggest that varying the size of the window can considerably change the performance of regression testing. Our findings will potentially help developers and test teams in adjusting test prioritization techniques for achieving higher cost-effectiveness in continuous regression testing.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816510,yes
599,Effectiveness of prioritization of test cases based on Faults,"Regression testing (RT) is an expensive activity. It is applied on a modified program to enhance confidence and reliability by ensuring that the changes are accurately true and have not affected the unmodified portions of the SUT. Due to limited resources, it is not practical to re-run each test cases (TC). To improve the regression testing's effectiveness, the TCs should be arranged according to some objective function or criteria. Test case prioritization (TCP) arranges TCs in an order for execution that enhances their effectiveness by satisfying some testing goals. The highest priority assigned to TCs must execute before the TCs with low priority by virtue of some performance goal. Numerous goals are possible to achieve of which one such goal is rate of fault detection (RFT) in which the faults are surfaced as quickly as possible within the testing process. In this paper, a novel technique is suggested to prioritize the TCs that increase its effectiveness in detecting faults. The effectiveness of the proposed method is compared and matched with other prioritization approaches with the help of Average Percentage of Fault Detection (APFD) metric from which charts have been prepared.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507977,yes
600,Empirical Evaluation of Cross-Release Effort-Aware Defect Prediction Models,"To prioritize quality assurance efforts, various fault prediction models have been proposed. However, the best performing fault prediction model is unknown due to three major drawbacks: (1) comparison of few fault prediction models considering small number of data sets, (2) use of evaluation measures that ignore testing efforts and (3) use of n-fold cross-validation instead of the more practical cross-release validation. To address these concerns, we conducted cross-release evaluation of 11 fault density prediction models using data sets collected from 2 releases of 25 open source software projects with an effort-aware performance measure known as Norm(P<sub>opt</sub>). Our result shows that, whilst M5 and K* had the best performances, they were greatly influenced by the percentage of faulty modules present and size of data set. Using Norm(P<sub>opt</sub>) produced an overall average performance of more than 50% across all the selected models clearly indicating the importance of considering testing efforts in building fault-prone prediction models.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589801,yes
601,Enhanced Weighted Method for Test Case Prioritization in Regression Testing Using Unique Priority Value,"Regression testing is an integral and expensive part in software testing. To reduce its effort, test case prioritization approaches were proposed. The problem with most of the existing approaches is the random ranking of test cases with equal weight. In this paper, an enhanced weighted method to prioritize the full test suite without using random ranking is presented. In addition, a controlled experiment was executed to evaluate the effectiveness of the proposed method. The results show an improved performance in terms of prioritizing test cases and recording higher APFD values over the original weighted method. In future, a larger experiment would be executed to generalize the results.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885851,yes
602,Enhancing Test Case Prioritization in an Industrial Setting with Resource Awareness and Multi-objective Search,"Test case prioritization is an essential part of test execution systems for large organizations developing software systems in the context that their software versions are released very frequently. They must be tested on a variety of compatible hardware with different configurations to ensure correct functioning of a software version on a compatible hardware. In practice, test case execution must not only execute cost-effective test cases in an optimal order, but also optimally allocate required test resources, in order to deliver high quality software releases. To optimize the current test execution system for testing software releases developed for Videoconferencing Systems (VCSs) at Cisco, Norway, in this paper, we propose a resource- aware multi-objective optimization solution with a fitness function defined based on four cost-effectiveness measures. In this context, a set of software releases must be tested on a set of compatible VCS hardware (test resources) by executing a set of cost-effective test cases in an optimal order within a given test cycle constrained by maximum allowed time budget and maximum available test resources. We empirically evaluated seven search algorithms regarding their performance and scalability by comparing with the current practice (random ordering (RO)). The results show that the proposed solution with the best search algorithm (i.e., Random-Weighted Genetic Algorithm) improved the current practice by reducing on average 40.6% of time for test resource allocation and test case execution, improved test resource usage on average by 37.9% and fault detection on average by 60%.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883302,yes
603,Experience Report: Automated System Level Regression Test Prioritization Using Multiple Factors,"We propose a new method of determining an effective ordering of regression test cases, and describe its implementation as an automated tool called SuiteBuilder developed by Westermo Research and Development AB. The tool generates an efficient order to run the cases in an existing test suite by using expected or observed test duration and combining priorities of multiple factors associated with test cases, including previous fault detection success, interval since last executed, and modifications to the code tested. The method and tool were developed to address problems in the traditional process of regression testing, such as lack of time to run a complete regression suite, failure to detect bugs in time, and tests that are repeatedly omitted. The tool has been integrated into the existing nightly test framework for Westermo software that runs on large-scale data communication systems. In experimental evaluation of the tool, we found significant improvement in regression testing results. The re-ordered test suites finish within the available time, the majority of fault-detecting test cases are located in the first third of the suite, no important test case is omitted, and the necessity for manual work on the suites is greatly reduced.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774503,yes
604,Experience Report: Understanding Cross-Platform App Issues from User Reviews,"App developers publish apps on different platforms, such as Google Play, App Store, and Windows Store, to maximize the user volumes and potential revenues. Due to the different characteristics of the platforms and the different user preference (e.g., Android is more customized than iOS), app testing cases on these three platforms should also be designed differently. Comprehensive app testing can be time-consuming for developers. Therefore, understanding the differences of the app issues on these platforms can facilitate the testing process. In this paper, we propose a novel framework named CrossMiner to analyze the essential app issues and explore whether the app issues exhibit differently on the three platforms. Based on five million user reviews, the framework automatically captures the distributions of seven app issues, i.e., ""battery"", ""crash"", ""memory"", ""network"", ""privacy"", ""spam"", and ""UI"". We discover that the apps for different platforms indeed generate different issue distributions, which can be employed by app developers to schedule and design the testing cases. The verification based on the official user forums also demonstrates the effectiveness of our framework. Furthermore, we also identify that the issues related to ""crash"" and ""network"" are more concerned by users than the other issues on these three platforms. To assist developers in gaining a deep insight on the user issues, we also prioritize the user reviews corresponding to the issues. Overall, we aim at understanding the differences of issues on different platforms and facilitating the testing process for app developers.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774515,yes
605,Foreword of the Thematic Track Quality Aspects in Agile Methods,"There is no doubt that agile methods have become mainstream and with their increased use unanswered questions start to appear: How do we address cross-cutting concerns when software is developed vertically? Does value prioritization lead to increases in technical debt by promoting feature development over refactoring? Isnƒ??t the reticence to write initial specifications on the premise of change an invitation to unnecessary change? As agile development matures answers, albeit partial, responses start to appear. The recurring themes in this year presentations are not whether agile is good or bad, better or worse,",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814524,no
606,Functional stratification of biomarkers selected from microarray data for understanding oral leukoplakia associated carcinogenesis,"In this research paper, we have proposed a three tier conceptual framework AVISAR for the testing of Object Oriented Programs. We have devised the proposed framework into three levels: Requirements Modeling, Test Case Generation and Effort Estimation. At the first level, we have used the Extend relationship of the Use Case to capture the events generated by the Classes during message passing between them through their objects. These events were captured by an Object Instantiator using Event Templates and further given to a Test case scenario generator, which generates the test cases. At the second level, the test cases will be generated based on the event templates that are used for capturing the events generated by the Extend relationship of the Use cases. Further, we have proposed a Genetic Algorithm (GA) for the effective test case prioritization to ensure maximum code coverage. At the third level, effort estimation, we have used the Cyclometric Complexity and Token count to perform the effort estimation for the Software under Test (SUT).",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7915096,yes
607,Guiding Dynamic Symbolic Execution toward Unverified Program Executions,"Most techniques to detect program errors, such as testing, code reviews, and static program analysis, do not fully verify all possible executions of a program. They leave executions unverified when they do not check certain properties, fail to verify properties, or check properties under certain unsound assumptions such as the absence of arithmetic overflow. In this paper, we present a technique to complement partial verification results by automatic test case generation. In contrast to existing work, our technique supports the common case that the verification results are based on unsound assumptions. We annotate programs to reflect which executions have been verified, and under which assumptions. These annotations are then used to guide dynamic symbolic execution toward unverified program executions. Our main technical contribution is a code instrumentation that causes dynamic symbolic execution to abort tests that lead to verified executions, to prune parts of the search space, and to prioritize tests that cover more properties that are not fully verified. We have implemented our technique for the .NET static analyzer Clousot and the dynamic symbolic execution tool Pex. It produces smaller test suites (by up to 19.2%), covers more unverified executions (by up to 7.1%), and reduces testing time (by up to 52.4%) compared to combining Clousot and Pex without our technique.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886899,yes
608,History-Based Dynamic Test Case Prioritization for Requirement Properties in Regression Testing,"Regression testing is an important but extremely costly and time-consuming process. Because of limited resources in practice, test case prioritization focuses on the improvement of testing efficiency. However, traditional test case prioritization techniques emphasize only one-time testing without considering huge historical data generated in regression testing. This paper proposes an approach to prioritizing test cases based on historical data. Requirements are a significant factor in the testing process, the priorities of test cases are initialized based on requirement priorities in our history-based approach, and then are calculated dynamically according to historical data in regression testing. To evaluate our approach, an empirical study on an industrial system is conducted. Experimental results show an improved performance for our proposed method using measurements of Average Percentage of Faults Detected and Fault Detection Rate.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809434,yes
609,History-Based Test Case Prioritization for Failure Information,"From regression tests, developers seek to determine not only the existence of faults, but also failure information such as what test cases failed. Failure information can assist in identifying suspicious modules or functions in order to fix the detected faults. In continuous integration environments, this can also help managers of the source code repository address unexpected situations caused by regression faults. We introduce an approach, referred to as AFSAC, which is a test case prioritization technique based on history data, that can be used to effectively obtain failure information. Our approach is composed of two stages. First, we statistically analyze the failure history for each test case to order the test cases. Next, we reorder the test cases utilizing the correlation data of test cases acquired by previous test results. We performed an empirical study on two open-source Apache software projects (i.e., Tomcat and Camel) to evaluate our approach. The results of the empirical study show that our approach provides failure information to testers and developers more effectively than other prioritization techniques, and each prioritizing method of our approach improves the ability to obtain failure information.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890618,yes
610,How Does Regression Test Prioritization Perform in Real-World Software Evolution?,"In recent years, researchers have intensively investigated various topics in test prioritization, which aims to re-order tests to increase the rate of fault detection during regression testing. While the main research focus in test prioritization is on proposing novel prioritization techniques and evaluating on more and larger subject systems, little effort has been put on investigating the threats to validity in existing work on test prioritization. One main threat to validity is that existing work mainly evaluates prioritization techniques based on simple artificial changes on the source code and tests. For example, the changes in the source code usually include only seeded program faults, whereas the test suite is usually not augmented at all. On the contrary, in real-world software development, software systems usually undergo various changes on the source code and test suite augmentation. Therefore, it is not clear whether the conclusions drawn by existing work in test prioritization from the artificial changes are still valid for real-world software evolution. In this paper, we present the first empirical study to investigate this important threat to validity in test prioritization. We reimplemented 24 variant techniques of both the traditional and time-aware test prioritization, and investigated the impacts of software evolution on those techniques based on the version history of 8 real-world Java programs from GitHub. The results show that for both traditional and time-aware test prioritization, test suite augmentation significantly hampers their effectiveness, whereas source code changes alone do not influence their effectiveness much.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886932,yes
611,Improving test efficiency through multiple criteria coverage based test case prioritization using Modified heuristic algorithm,"Test case prioritization involves reordering the test cases in an order that helps in attaining certain performance goals. The rate of fault detection is one of the prime goals that we tend to achieve while doing prioritization. Test cases should run in an order to increase the possibility of fault detection and it should be achieved early during the test life cycle. To reduce the cost and time of regression testing, test case prioritization should be done with the intention of periodically modifying the test suite. The humongous set of test cases makes it redundant and cumbersome for the testers who ensure quality for an end application. The fault detection capability of a prioritized test suite is improved up to 15% using Modified PSO which forms the base algorithms for prioritization. The algorithm illustrated detects serious errors at earlier phases of testing process and effectiveness between prioritized and unprioritized test cases.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783254,yes
612,Improving Testing in an Enterprise SOA with an Architecture-Based Approach,"High resource demand for system testing is a major obstacle for continuous delivery. This resource demand can be reduced by prioritizing test cases, e.g., by focusing on tests that cover a lot of functionality. For large-scale systems, like an enterprise SOA, defining such test cases can be difficult for the tester because of the lack of relevant knowledge about the system. We propose an approach for test case prioritization and selection that is based on architectural viewpoint that provides software testers with the required architectural information. We outline how architectural information is used for defining and selecting prioritized test cases. The approach has been developed in close cooperation with the provider of an enterprise SOA in the banking domain in Austria following an action research approach. In addition, the approach has been validated in an industrial case study. Validation showed that there is no further need for manual architectural analysis to be able to prioritize and select test cases. We also show the limitations of our approach as it is based on static code analysis.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516833,yes
613,Improvising the effectiveness of test suites using differential evolution technique,"The process of testing any software system is an atrocious task which indeed consumes a ton of effort, and expensive also. Required effort and time to do adequate as well as effective testing get bigger, as the software gets more complexed that can lead to swarm over the project budget or some test cases left uncovered or delay in completion. A suitably generated test suite does not only locate errors but also aid in reducing cost investment associated with the testing process. This paper implements an optimizing technique called as Differential Evolution to improve the effectiveness of test cases using Average Percentage of Fault Detection (APFD) metric. APFD is taken as the fitness function which is to be optimized. In this work, We have performed comparison of our approach with other existing prioritizing approaches and Experimental computations show that Differential Evolution technique achieve better APFD values than other techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7784924,yes
614,Integrating risk assessment and threat modeling within SDLC process,"Risk assessment and threat modeling are conducted for different purpose. The integration of risk assessment and threat modeling process limit the risk of software-based system. Incorporating security in all phases of software development life cycle is a tedious task in many organizations. In design phase of SDLC, the 50 % software defects are identified and detected. Most of the security attacks are happen in application layer. This paper explains the combined use of risk assessment and threat model to understand the security risk of an application. We also discuss how the model may be identifying threats and how to frame threat prioritization for threat category. Finally, we recommend understanding of risk of detection and creating a fair environment to reduce the likelihood of committing criminal acts by attackers.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823275,no
615,Investigating the Effects of Balanced Training and Testing Datasets on Effort-Aware Fault Prediction Models,"To prioritize software quality assurance efforts, fault prediction models have been proposed to distinguish faulty modules from clean modules. The performances of such models are often biased due to the skewness or class imbalance of the datasets considered. To improve the prediction performance of these models, sampling techniques have been employed to rebalance the distribution of fault-prone and non-fault-prone modules. The effect of these techniques have been evaluated in terms of accuracy/geometric mean/F1-measure in previous studies; however, these measures do not consider the effort needed to fix faults. To empirically investigate the effect of sampling techniques on the performance of software fault prediction models in a more realistic setting, this study employs Norm(P<sub>opt</sub>), an effort-aware measure that considers the testing effort. We performed two sets of experiments aimed at (1) assessing the effects of sampling techniques on effort-aware models and finding the appropriate class distribution for training datasets (2) investigating the role of balanced training and testing datasets on performance of predictive models. Of the four sampling techniques applied, the over-sampling techniques outperformed the under-sampling techniques with Random Over-sampling performing best with respect to the Norm(P<sub>opt</sub>) evaluation measure. Also, performance of all the prediction models improved when sampling techniques were applied between the rates of (20-30)% on the training datasets implying that a strictly balanced dataset (50% faulty modules and 50% clean modules) does not result in the best performance for effort-aware models. Our results also indicate that performances of effort-aware models are significantly dependent on the proportions of the two types of the classes in the testing dataset. Models trained on moderately balanced datasets are more likely to withstand fluctuations in performance as the class distribution in the testing data varies.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552003,no
616,MACKE: Compositional analysis of low-level vulnerabilities with symbolic execution,"Concolic (concrete+symbolic) execution has recently gained popularity as an effective means to uncover non-trivial vulnerabilities in software, such as subtle buffer overflows. However, symbolic execution tools that are designed to optimize statement coverage often fail to cover potentially vulnerable code because of complex system interactions and scalability issues of constraint solvers. In this paper, we present a tool (MACKE) that is based on the modular interactions inferred by static code analysis, which is combined with symbolic execution and directed inter-procedural path exploration. This provides an advantage in terms of statement coverage and ability to uncover more vulnerabilities. Our tool includes a novel feature in the form of interactive vulnerability report generation that helps developers prioritize bug fixing based on severity scores. A demo of our tool is available at https://youtu.be/icC3jc3mHEU.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582815,yes
617,Minimum complexity APP prioritization by bandwidth apportioning in smart phones,"The volume of best effort traffic is exploded by rapid adoption of peer-to-peer and content applications. Smart phone consumers are spending more times on applications which include video and music streaming, playing games, video chatting, social media like uploading photos to Facebook, Twitter etc. Many such applications are always running in background and sometimes come in foreground based on user preferences. In this work we propose an approach to improve the user experience by giving more bandwidth to preferred applications. We describe a preliminary model explaining our technique in detail. Further, we validate our proposal using real time test setup with Wireshark traffic analyzer, and results are detailed with respect to (1) Percentage of network share (2) Jitter experience and (3) Time taken for the algorithm to adapt. Proposed algorithm has been tested in two different platforms such as Android and Tizen. Our preliminary observations show that our proposed algorithm allocates more bandwidth to high priority applications while maintaining the low priority APPs are intact with above minimum bandwidth. Our approach gives users better jitter free experience for video streaming (high priority) applications in both Android (KitKat) and Tizen (Z1) platforms.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7564972,yes
618,Mixed-Criticality Systems as a Service for Non-critical Tasks,"Mixed-Criticality Systems are capable of accommodating tasks of varying criticality. In this paper, these are [life, mission, and non-critical]. Tasks usually have an overestimated execution time to allow for the Worst Case Execution Time (WCET). When these tasks finish execution prior to their allotted execution time due to pessimistic assumptions present in the static analysis of the system. The surplus time is used to accommodate tasks with tolerance for deadline-misses. Non-critical tasks are often treated in a ƒ?best-effortƒ? capacity where no quality of service is considered. When processor utilisation is not overconstrained, all deadlines will be met. However, in cases where not enough processing resources exist to meet all deadlines for non-critical tasks, the allotted time for the non-critical tasks must be rationed between non-critical tasks. This paper proposes a novel method of prioritising non-critical tasks. By treating task execution as a service, non-critical tasks with unbounded deadline miss tolerance are given a Grade of Service for their met deadlines. This Grade of Service is used for the dynamic scheduling of non-critical tasks. Four different scheduling algorithms were tested with the proposed Highest Penalty First algorithm for distributing the effort of task execution amongst non-critical tasks in a proportionate manner and showing superior fairness of task execution compared to all other tested algorithms.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515632,no
619,Model-based test case prioritization using ACO: A review,"Regression testing is very costly and inevitable activity of maintenance that is performed to ensure whether the modified software is valid or not. Running all the test cases of a test suit within given limited time and cost constraints is not possible. So, to cover the maximum number of faults in comparatively less time, it is necessary to prioritize the test cases. To solve the time constraint test case prioritization problems Ant Colony optimization (ACO) is a better approach. This paper presents a review on test case prioritization from a given test suite using ACO.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7913140,yes
620,Modified ACO to maintain diversity in regression test optimization,"Regression testing is unavoidable maintenance activity that is performed several times in software development life cycle. Optimization of regression test case is required to minimize the test case (which will in-turn reduce the time and cost of testing) and to find the fault in early testing activity. The two widely used regression test case optimization techniques, namely, selection and prioritization are recently found to be integrated with different metaheuristic algorithms for fruitful regression test cases. Among the various meta-heuristic algorithms, Ant colony optimization (ACO) algorithm is most popularly used. ACO will try to find the smallest path out all the test cases and it is not sufficient because it will not cover all the test cases which are needed. In this paper we have proposed a modified ant colony optimization to solve test cases in huge search space. The modified algorithm selects the best test cases that find the maximum fault in minimum time.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507970,yes
621,Multi-objective test report prioritization using image understanding,"In crowdsourced software testing, inspecting the large number of test reports is an overwhelming but inevitable software maintenance task. In recent years, to alleviate this task, many text-based test-report classification and prioritization techniques have been proposed. However in the mobile testing domain, test reports often consist of more screenshots and shorter descriptive text, and thus text-based techniques may be ineffective or inapplicable. The shortage and ambiguity of natural-language text information and the well defined screenshots of activity views within mobile applications motivate our novel technique based on using image understanding for multi-objective test-report prioritization. In this paper, by taking the similarity of screenshots into consideration, we present a multi-objective optimization-based prioritization technique to assist inspections of crowdsourced test reports. In our technique, we employ the Spatial Pyramid Matching (SPM) technique to measure the similarity of the screenshots, and apply the natural-language processing technique to measure the distance between the text of test reports. Furthermore, to validate our technique, an experiment with more than 600 test reports and 2500 images is conducted. The experimental results show that image-understanding techniques can provide benefit to test-report prioritization for most applications.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582758,yes
622,Negative Effects of Bytecode Instrumentation on Java Source Code Coverage,"Code coverage measurement is an important element in white-box testing, both in industrial practice and academic research. Other related areas are highly dependent on code coverage as well, including test case generation, test prioritization, fault localization, and others. Inaccuracies of a code coverage tool sometimes do not matter that much but in certain situations they can lead to serious confusion. For Java, the prevalent approach to code coverage measurement is to use bytecode instrumentation due to its various benefits over source code instrumentation. However, if the results are to be mapped back to source code this may lead to inaccuracies due to the differences between the two program representations. In this paper, we systematically investigate the amount of differences in the results of these two Java code coverage approaches, enumerate the possible reasons and discuss the implications on various applications. For this purpose, we relied on two widely used tools to represent the two approaches and a set of benchmark programs from the open source domain.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476645,no
623,Neuro-fuzzy based approach to event driven software testing: A new opportunity,"Event Driven Software (EDS) testing is a very challenging task as a large number of events can be invoked by users. So far it is difficult to test all the user inputs invoked, therefore, test case prioritization is essentially required for giving more priority to test cases which reveal higher faults comparatively. We have proposed test case prioritization for EDS: as the Event Type, Interaction of Event, and Coverage of Event. Priority assigned in the proposed model uses these factors in Adaptive Neuro-Fuzzy Inference System (ANFIS) MATLAB Toolbox based on Neuro-Fuzzy logic model. Evaluation and validation will be done using Average Percentage of Fault Detection (APFD). APFD rate for prioritized sequence using the proposed Neuro-Fuzzy logic model exhibited 81% rate, whereas, non-prioritized test sequences showed70% suggesting, thereby, that after prioritization; rate of fault detection has improved considerably. Data shows that proposed Neuro-Fuzzy logic model is apt for Test Case Prioritization of EDS Testing.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975349,yes
624,Opportunistic Competition Overhead Reduction for Expediting Critical Section in NoC Based CMPs,"With the degree of parallelism increasing, performance of multi-threaded shared variable applications is not only limited by serialized critical section execution, but also by the serialized competition overhead for threads to get access to critical section. As the number of concurrent threads grows, such competition overhead may exceed the time spent in critical section itself, and become the dominating factor limiting the performance of parallel applications. In modern operating systems, queue spinlock, which comprises a low-overhead spinning phase and a high-overhead sleeping phase, is often used to lock critical sections. In the paper, we show that this advanced locking solution may create very high competition overhead for multithreaded applications executing in NoC-based CMPs. Then we propose a software-hardware cooperative mechanism that can opportunistically maximize the chance that a thread wins the critical section access in the low-overhead spinning phase, thereby reducing the competition overhead. At the OS primitives level, we monitor the remaining times of retry (RTR) in a thread's spinning phase, which reflects in how long the thread must enter into the high-overhead sleep mode. At the hardware level, we integrate the RTR information into the packets of locking requests, and let the NoC prioritize locking request packets according to the RTR information. The principle is that the smaller RTR a locking request packet carries, the higher priority it gets and thus quicker delivery. We evaluate our opportunistic competition overhead reduction technique with cycle-accurate full-system simulations in GEM5 using PARSEC (11 programs) and SPEC OMP2012 (14 programs) benchmarks. Compared to the original queue spinlock implementation, experimental results show that our method can effectively increase the opportunity of threads entering the critical section in low-overhead spinning phase, reducing the competition overhead averagely by 39.9% (maximally by 61.8%) and accelerating the execution of the Region-of-Interest averagely by 14.4% (maximally by 24.5%) across all 25 benchmark programs.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7551400,no
625,Optimal test sequence generation using River Formation Dynamics,"Software testing is a complex and exhaustive process, often limited by the resources. Although many approaches for test sequence generation exist in the literature, but none of it is ideal as far as coverage and redundancy is concerned. This paper aims at improving the efficiency of software testing process by generating the optimal test sequences in the control flow graph (CFG) of the program under test (PUT) by using a novel swarm intelligence method called River Formation Dynamics(RFD). RFD is inspired by a natural phenomenon of how drops transformed into river and river into sea. It provides full path coverage with zero edge/transition redundancy. It also tries to prioritize the paths based on their strength, calculated in terms of their traversal by the drops.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894483,no
626,PRADA: Prioritizing Android Devices for Apps by Mining Large-Scale Usage Data,"Selecting and prioritizing major device models are critical for mobile app developers to select testbeds and optimize resources such as marketing and quality-assurance resources. The heavily fragmented distribution of Android devices makes it challenging to select a few major device models out of thousands of models available on the market. Currently app developers usually rely on some reported or estimated general market share of device models. However, these estimates can be quite inaccurate, and more problematically, can be irrelevant to the particular app under consideration. To address this issue, we propose PRADA, the first approach to prioritizing Android device models for individual apps, based on mining large-scale usage data. PRADA adapts the concept of operational profiling (popularly used in software reliability engineering) for mobile apps - the usage of an app on a specific device model reflects the importance of that device model for the app. PRADA includes a collaborative filtering technique to predict the usage of an app on different device models, even if the app is entirely new (without its actual usage in the market yet), based on the usage data of a large collection of apps. We empirically demonstrate the effectiveness of PRADA over two popular app categories, i.e., Game and Media, covering over 3.86 million users and 14,000 device models collected through a leading Android management app in China.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886887,no
627,Prioritization techniques in combinatorial testing: A survey,"Prioritization techniques have become an indispensable part of the software testing process. They are highly beneficial either due to resource or time constraints or when tester cannot execute the complete test set. Combinatorial test sets are generated with the aim to cover all the possible t-way interactions. In this paper, techniques to prioritize t-way test sets are studied. A comparative study is done of prioritization criteria used by researchers to prioritize t-way test sets. Different evaluation methods used by them are also discussed.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975314,yes
628,Prioritizing Interaction Test Suites Using Repeated Base Choice Coverage,"Combinatorial interaction testing is a well-studied testing strategy that aims at constructing an effective interaction test suite (ITS) of a specific generation strength to identify interaction faults caused by the interactions among factors. Due to limited testing resources in practice, for example in combinatorial interaction regression testing, interaction test suite prioritization (ITSP) has been proposed to improve the efficiency of testing. An intuitive ITSP strategy that has been widely used in practice is fixed-strength interaction coverage based prioritization (FICBP). FICBP makes use of a property of the ITS: interaction coverage at a fixed prioritization strength. However, a challenge facing FICBP is that, when the ITS is large, the prioritization cost can be very high. In this paper, we propose a new FICBP method that, by repeatedly using base choice coverage (i.e., one-wise coverage) during the prioritization process, improves testing efficiency while maintaining testing effectiveness. The empirical studies show that our method has fault detection capability comparable to current FICBP methods, but obtains more stable results in many cases. Additionally, our method requires considerably less prioritization time than other FICBP methods at different prioritization strengths.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552005,yes
629,Product feature prioritization using the Hidden Structure method: A practical case at Ericsson,"In this paper, we present a case were we employ the Hidden Structure method to product feature prioritization at Ericsson. The method extends the more common Design Structure Matrix (DSM) approach that has been used in technology management (e.g. project management and systems engineering) for quite some time in order to model complex systems and processes. The hidden structure method focuses on analyzing a DSM based on coupling and modularity theory, and it has been used in a number of software architecture and software portfolio cases. In previous work by the authors the method was tested on organization transformation at Ericsson, however this is the first time it has been employed in the domain of product feature prioritization. Today, at Ericsson, features are prioritized based on a business case approach where each feature is handled isolated from other features and the main focus is customer or market-based requirements. By employing the hidden structure method we show that features are heavily dependent on each other in a complex network, thus they should not be treated as isolated islands. These dependencies need to be considered when prioritizing features in order to save time and money, as well as increase end customer satisfaction.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7806519,yes
630,Radius aware probabilistic testing of deadlocks with guarantees,"Concurrency bugs only occur under certain interleaving. Existing randomized techniques are usually ineffective. PCT innovatively generates scheduling, before executing a program, based on priorities and priority change points. Hence, it provides a probabilistic guarantee to trigger concurrency bugs. PCT randomly selects priority change points among all events, which might be effective for non-deadlock concurrency bugs. However, deadlocks usually involve two or more threads and locks, and require more ordering constraints to be triggered. We interestingly observe that, every two events of a deadlock usually occur within a short range. We generally formulate this range as the bug Radius, to denote the max distance of every two events of a concurrency bug. Based on the bug radius, we propose RPro (Radius aware Probabilistic testing) for triggering deadlocks. Unlike PCT, RPro selects priority change points within the radius of the targeted deadlocks but not among all events. Hence, it guarantees larger probabilities to trigger deadlocks. We have implemented RPro and PCT and evaluated them on a set of real-world benchmarks containing 10 unique deadlocks. The experimental results show that RPro triggered all deadlocks with higher probabilities (i.e., &gt;7.7x times larger on average) than that by PCT. We also evaluated RPro with radius varying from 1 to 150 (or 300). The result shows that the radius of a deadlock is much smaller (i.e., from 2 to 114 in our experiment) than the number of all events. This further confirms our observation and makes RPro meaningful in practice.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582772,no
631,Signature limits: an entire map of clone features and their discovery in nearly linear time,"We address an increasingly critical problem of identifying the potential signatures for identifying a given family of malware or unwanted software (i.e., or generally any corpus of artifacts of unknown provenance). We address this with a novel methodology designed to create an entire and complete maps of software code clones (copy features in data). We report on a practical methodology, which employs enhanced suffix data structures and partial orderings of clones to compute a compact representation of most interesting clones features in data. The enumeration of clone features is useful for malware triage and prioritization when human exploration, testing and verification is the most costly factor. We further show that the enhanced arrays may be used for discovery of provenance relations in data and we introduce two distinct Jaccard similarity coefficients to measure code similarity in binary artifacts. We illustrate the use of these tools on real malware data including a retro-diction experiment for measuring and enumerating evidence supporting common provenance in Stuxnet and Duqu. The results indicate the practicality and efficacy of mapping completely the clone features in data.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888740,no
632,Solving exercise generation problems by diversity oriented meta-heuristics,"Evolutionary algorithms used for multi-objective optimization mostly prioritize fitness over diversity to achieve a single optimum fast, or a region in the Pareto-front. In this paper, we argue on that diversity should be a primary objective as well, and we propose a novel approach called EGAL to solve a well-known problem: to generate very different exercises to test students' knowledge in a specific range of topics. We show that focusing on diversity and fitness at the same time result in a better quality of solutions in the resulting population.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916196,no
633,Supporting the regression test of multi-variant systems in distributed production scenarios,"Modern manufacturing systems based on cyber-physical systems with a growing amount of software allow frequent updates and reconfigurations to adapt the systems to volatile usage scenarios in the production. A diverse system environment arises even for similar or equal subsystems based on the same platform used at different locations. A major challenge for such systems is the regression test after changes or updates. The resources for the regression test, in a dedicated test environment or deployed to the assembly lines, are limited. To plan the test in the best possible way, a lot of dependencies, relationships and experiences from former tests and tests from other locations have to be considered. This paper describes an assistance system which supports the planning of the regression test in such distributed manufacturing scenarios by combining manual modeling with automated data processing. Therefore the system calculates a cross-location test progress and suggests a prioritized test case sequence.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733652,no
634,System-Level Test Case Prioritization Using Machine Learning,"Regression testing is the common task of retesting software that has been changed or extended (e.g., by new features) during software evolution. As retesting the whole program is not feasible with reasonable time and cost, usually only a subset of all test cases is executed for regression testing, e.g., by executing test cases according to test case prioritization. Although a vast amount of methods for test case prioritization exist, they mostly require access to source code (i.e., white-box). However, in industrial practice, system-level testing is an important task that usually grants no access to source code (i.e., black-box). Hence, for an effective regression testing process, other information has to be employed. In this paper, we introduce a novel technique for test case prioritization for manual system-level regression testing based on supervised machine learning. Our approach considers black-box meta-data, such as test case history, as well as natural language test case descriptions for prioritization. We use the machine learning algorithm SVM Rank to evaluate our approach by means of two subject systems and measure the prioritization quality. Our results imply that our technique improves the failure detection rate significantly compared to a random order. In addition, we are able to outperform a test case order given by a test expert. Moreover, using natural language descriptions improves the failure finding rate.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838169,yes
635,Targeted Scrum: Applying Mission Command to Agile Software Development,"Software engineering and mission command are two separate but similar fields, as both are instances of complex problem solving in environments with ever changing requirements. Our research hypothesis is that modifications to agile software development based on inspirations from mission command can improve the software engineering process in terms of planning, prioritizing, and communication of software requirements and progress, as well as improving the overall software product. Targeted Scrum is a modification of Traditional Scrum based on three inspirations from Mission Command: End State, Line of Effort, and Targeting. These inspirations have led to the introduction of the Product Design Meeting and modifications of some current Scrum meetings and artifacts. We tested our research hypothesis using a semester-long undergraduate level software engineering class. Students developed two software projects, one using Traditional Scrum and the other using Targeted Scrum. We then assessed how well both methodologies assisted the software development teams in planning and developing the software architecture, prioritizing requirements, and communicating progress. We also evaluated the software product produced by both methodologies. We found that Targeted Scrum did better in assisting the software development teams in the planning and prioritization of the requirements. However, Targeted Scrum had a negligible effect on improving the software development teams external and internal communications. Finally, Targeted Scrum did not have an impact on the product quality by the top performing and worst performing teams. Targeted Scrum did assist the product quality of the teams in the middle of the performance spectrum.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7296686,yes
636,Test case design based technique for the improvement of test case selection in software maintenance,"Test case selection is the maintenance technique regarding the concept of choosing the appropriate representative of the modified program that has to be changed depending upon the new requirements added for the next modification. The main problem of updating software is the amounts of the test suite that is generated for the tests increases, which can drop the performance of using the new code that added to the previous program. According to this, execution and testing time will increase, including the complexity of integrating the codes also increase. Therefore, the test suite minimization approaches are offered to solve these problems. The objective of proposing the design based technique is to refine the competency of using test case selection to control the large programs by applying four algorithms, which are testing test case, classification, deletion, and selection. The result of determining the size reduction is greater than random selection, general regression technique, and test case prioritization as about 28.00%, 17.00%, and 11.00% approximately.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749202,no
637,Test Case Prioritization Approach to Improving the Effectiveness of Fault Localization,"Fault localization aims to use testing information from executed test cases to help locate the fault position. However, obtaining testing information (including test results and coverage information) is expensive because it needs much manual effort. How to orderly choose and execute a small number of test cases, and in the meantime achieve a good effectiveness of fault localization in the case of unknowing testing information is still a challenge for us. In this paper, we propose a new test case prioritization algorithm for fault localization, which is based on the rank changes of suspicious values of program elements. Test case which can maximize the improvements of suspicious ranks of program elements may assign the highest priority for execution. A set of empirical studies have been designed and conducted on Siemens programs and four medium-sized programs. The results show that our algorithm can help reduce the debugging effort in terms of the percentage of statements needed to be inspected to locate faults in both single-fault and multi-fault programs.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780195,yes
638,Test case prioritization based on requirement correlations,"Test case prioritization technique aims to improve test efficiency rate by sorting test cases according to some specific criteria. Requirements play an important role throughout software testing. This paper proposes a test case prioritization method based on requirement correlations. Prioritization of requirements is defined by the users and the developers. This technique focuses on requirements with detected faults after the last regression testing. By readjusting prioritization of fault-related requirements, it can optimize the order of test cases. Experimental results show that this technique exactly contributes to achieving high testing efficiency.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515934,yes
639,Test Case Prioritization for Compilers: A Text-Vector Based Approach,"Test case prioritization aims to schedule the execution order of test cases so as to detect bugs as early as possible. For compiler testing, the demand for both effectiveness and efficiency imposes challenge to test case prioritization. In the literature, most existing approaches prioritize test cases by using some coverage information (e.g., statement coverage or branch coverage), which is collected with considerable extra effort. Although input-based test case prioritization relies only on test inputs, it can hardly be applied when test inputs are programs. In this paper we propose a novel text-vector based test case prioritization approach, which prioritizes test cases for C compilers without coverage information. Our approach first transforms each test case into a text-vector by extracting its tokens which reflect fault-relevant characteristics and then prioritizes test cases based on these text-vectors. In particular, in our approach we present three prioritization strategies: greedy strategy, adaptive random strategy, and search strategy. To investigate the efficiency and effectiveness of our approach, we conduct an experiment on two C compilers (i.e., GCC and LLVM), and find that our approach is much more efficient than the existing approaches and is effective in prioritizing test cases.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515478,yes
640,Test case prioritization technique based on early fault detection using fuzzy logic,"Regression testing is time consuming and expensive activity in software testing. In Regression testing when any changes made to already tested program it should not affect to other part of program. Regression testing is crucial activities in software testing and maintenance phases. If some part of code is altered then it is mandatory to validate the modified code. Throughout regression testing test case from test suite will be re-executed and re-execution of all the test case will be very expensive. In this paper we present regression test case prioritization for object oriented program. The most important research is how to select efficient and suitable test cases during regression testing from the test suite. To minimize the regression testing cost we have applied prioritization technique. In this paper prioritization is done based on fault detection rate of program, execution time and requirement coverage using fuzzy logic.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724418,yes
641,Test case prioritization techniques for software product line: A survey,"Software product line (SPL) testing is a tougher work than testing of single systems. Still testing of each individual SPL product would be perfect but it is too costly in practice. In fact, when the number of features increases then the number of possible products also increases exponentially usually derived from a feature model. Number of features is leading to thousands of different products. Due to cost and time constraints, it is infeasible or large number of effort to run all the test cases in an existing test suite. To decrease the cost of testing, various techniques have been proposed. One of them is test case prioritization (TCP) techniques. Here we presented a survey for TCP techniques for software SPL.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813841,yes
642,Test Case Prioritization Using Adaptive Random Sequence with Category-Partition-Based Distance,"Test case prioritization schedules test cases in a certain order aiming to improve the effectiveness of regression testing. Random sequence is a basic and simple prioritization technique, while Adaptive Random Sequence (ARS) makes use of extra information to improve the diversity of random sequence. Some researchers have proposed prioritization techniques using ARS with white-box information, such as code coverage information, or with black-box information, such as string distances of the input data. In this paper, we propose new black-box test case prioritization techniques using ARS, and the diversity of test cases is assessed by category-partition-based distance. Our experimental studies show that these new techniques deliver higher fault-detection effectiveness than random prioritization, especially in the case of smaller ratio of failed test cases. In addition, in the comparison of different distance metrics, techniques with category-partition-based distance generally deliver better fault-detection effectiveness and efficiency, meanwhile in the comparison of different ordering algorithms, our ARS-based ordering algorithms usually have comparable fault-detection effectiveness but much lower computation overhead, and thus are much more cost-effective.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589817,yes
643,Test Case Prioritization Using Lexicographical Ordering,"Test case prioritization aims at ordering test cases to increase the rate of fault detection, which quantifies how fast faults are detected during the testing phase. A common approach for test case prioritization is to use the information of previously executed test cases, such as coverage information, resulting in an iterative (greedy) prioritization algorithm. Current research in this area validates the fact that using coverage information can improve the rate of fault detection in prioritization algorithms. The performance of such iterative prioritization schemes degrade as the number of ties encountered in prioritization steps increases. In this paper, using the notion of lexicographical ordering, we propose a new heuristic for breaking ties in coverage based techniques. Performance of the proposed technique in terms of the rate of fault detection is empirically evaluated using a wide range of programs. Results indicate that the proposed technique can resolve ties and in turn noticeably increases the rate of fault detection.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456343,yes
644,Test Effectiveness Evaluation of Prioritized Combinatorial Testing: A Case Study,"Combinatorial testing is a widely-used technique to detect system interaction failures. To improve test effectiveness with given priority weights of parameter values in a system under test, prioritized combinatorial testing constructs test suites where highly weighted parameter values appear earlier or more frequently. Such order-focused and frequency-focused combinatorial test generation algorithms have been evaluated using metrics called weight coverage and KL divergence but not sufficiently with fault detection effectiveness so far. We evaluate the fault detection effectiveness on a collection of open source utilities, applying prioritized combinatorial test generation and investigating its correlation with weight coverage and KL divergence.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589785,yes
645,Testing and Debugging in Continuous Integration with Budget Quotas on Test Executions,"In Continuous Integration, a software application is developed through a series of development sessions, each with limited time allocated to testing and debugging on each of its modules. Test Case Prioritization can help execute test cases with higher failure estimate earlier in each session. When the testing time is limited, executing such prioritized test cases may only produce partial and prioritized execution coverage data. To identify faulty code, existing Spectrum-Based Fault Localization techniques often use execution coverage data but without the assumption of execution coverage priority. Is it possible to decompose these two steps for optimization within individual steps? In this paper, we study to what extent the selection of test case prioritization techniques may reduce its influence on the effectiveness of spectrum-based fault localization, thereby showing the possibility to decompose the process of continuous integration for optimization in workflow steps. We present a controlled experiment using the Siemens suite as subjects, nine test case prioritization techniques and four spectrum-based fault localization techniques. The findings showed that the studied test cases prioritization and spectrum-based fault localization can be customized separately, and, interestingly, prioritization over a smaller test suite can enable spectrum-based fault localization to achieve higher accuracy by assigning faulty statements with higher ranks.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589824,yes
646,Test-Suite Prioritisation by Application Navigation Tree Mining,"Software tend to evolve over time and so does the test-suite. Regression testing is aimed at assessing that the software evolution did not compromise the working of the existing software components. However, as the software and consequently the test-suite grow in size, the execution of the entire test-suite for each new build becomes infeasible. Techniques like test-suite selection, test-suite minimisation and test-suite prioritisation have been proposed in literature for regression testing. Whilst all of these techniques are essentially an attempt to reduce the testing effort, test-suite selection and minimisation reduce the test-suite size whereas test-suite prioritisation provides a priority order of the test cases without changing the test-suite size. In this work, we focus on test-suite prioritisation. Recently, techniques from data mining have been used for test-suite prioritisation which consider the frequent pairs of interaction among the application interaction patterns. We propose test-Suite prioritisation by Application Navigation Tree mining (t-SANT). First, we construct an application navigation tree by way of extracting both tester and user interaction patterns. Next, we extract frequent sequences of interaction using a sequence mining algorithm inspired from sequential pattern mining. The most frequent longest sequences are assumed to model complex and most frequently used work-flows and hence a prioritisation algorithm is proposed that prioritises the test cases based on the most frequent and longest sequences. We show the usefulness of the proposed scheme with the help of two case studies, an online book store and calculator.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7866754,yes
647,The drawbacks of statement code coverage test case prioritization related to domain testing,"In this paper we study the weaknesses of the test case prioritization algorithms based on statement code coverage and applied to the domain test cases. We present the inconsistency between the principles of domain testing and the selection and prioritization practices over domain test cases on criteria unrelated to the scope of the domain testing. We continue the study by discussing the impact of 100% statement code coverage over the suites of domain test cases, studying why this type of code coverage should not produce effects over the domain test cases. Statement code coverage prioritization techniques related to unit testing, integration testing and regression testing phases are discussed, emphasizing the incompatibility between statement code coverage, domain testing and test case prioritization all at once.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507373,yes
648,The Perception of Technical Debt in the Embedded Systems Domain: An Industrial Case Study,"Technical Debt Management (TDM) has drawn the attention of software industries during the last years, including embedded systems. However, we currently lack an overview of how practitioners from this application domain perceive technical debt. To this end, we conducted a multiple case study in the embedded systems industry, to investigate: (a) the expected life-time of components that have TD, (b) the most frequently occurring types of TD in them, and (c) the significance of TD against run-time quality attributes. The case study was performed on seven embedded systems industries (telecommunications, printing, smart manufacturing, sensors, etc.) from five countries (Greece, Netherlands, Sweden, Austria, and Finland). The results of the case study suggest that: (a) maintainability is more seriously considered when the expected lifetime of components is larger than ten years, (b) the most frequent types of debt are test, architectural, and code debt, and (c) in embedded systems the run-time qualities are prioritized compared to design-time qualities that are usually associated with TD. The obtained results can be useful for both researchers and practitioners: the former can focus their research on the most industrially-relevant aspects of TD, whereas the latter can be informed about the most common types of TD and how to focus their TDM processes.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776447,no
649,To Be Optimal or Not in Test-Case Prioritization,"Software testing aims to assure the quality of software under test. To improve the efficiency of software testing, especially regression testing, test-case prioritization is proposed to schedule the execution order of test cases in software testing. Among various test-case prioritization techniques, the simple additional coverage-based technique, which is a greedy strategy, achieves surprisingly competitive empirical results. To investigate how much difference there is between the order produced by the additional technique and the optimal order in terms of coverage, we conduct a study on various empirical properties of optimal coverage-based test-case prioritization. To enable us to achieve the optimal order in acceptable time for our object programs, we formulate optimal coverage-based test-case prioritization as an integer linear programming (ILP) problem. Then we conduct an empirical study for comparing the optimal technique with the simple additional coverage-based technique. From this empirical study, the optimal technique can only slightly outperform the additional coverage-based technique with no statistically significant difference in terms of coverage, and the latter significantly outperforms the former in terms of either fault detection or execution time. As the optimal technique schedules the execution order of test cases based on their structural coverage rather than detected faults, we further implement the ideal optimal test-case prioritization technique, which schedules the execution order of test cases based on their detected faults. Taking this ideal technique as the upper bound of test-case prioritization, we conduct another empirical study for comparing the optimal technique and the simple additional technique with this ideal technique. From this empirical study, both the optimal technique and the additional technique significantly outperform the ideal technique in terms of coverage, but the latter significantly outperforms the former two techniques in terms of fault detection. Our findings indicate that researchers may need take cautions in pursuing the optimal techniques in test-case prioritization with intermediate goals.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314957,yes
650,Tolerance to complexity: Measuring capacity of development teams to handle source code complexity,"A well defined testing strategy is essential for any software development project. Testing efforts need to be carefully planed and executed in order to ensure effectiveness. Programming failures can represent a high risk for business. In order to mitigate such risk, companies have been increasingly investing more resources on software testing. In despite of massive investments on software testing and extensive collection of static analysis techniques and tools, there are still few conclusive explanations for what causes human programming failures on software. The hypothesis investigated in this paper is that a metric based on development teams characteristics can be more effective to predict defective source code than metrics purely focused on information about source code, alone. Aiming to assist software engineers during testing initiatives, this article presents a new approach to systematically measure capacity of development teams to handle source code complexity. The proposed metric can be effective for raising information and comparing multiple development teams, planning training initiatives and prioritising testing efforts. Experiments were carried out with the entire source code base of device drivers for Linux Operating System. Our approach was able to predict, with 80% of accuracy rate, which development teams introduced more issues from 2010 to 2014.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844689,yes
651,UML-based reconfigurable middleware for design-level timing verification in model-based approach,"Model-based approaches for the development of software intensive real-time embedded systems allow early verification of timing properties at the design phase. In order to perform such verification, some aspects of the target software platform (i.e. the Real-Time Operating System (RTOS)) need to be considered such as priorities, scheduling policies, etc. However, one of the basic principles of model-based approaches, is to keep RTOS-independence of the design model. Hence, some assumptions on the software platform are implicitly made to achieve timing verification. This approach may lead to a mismatch between the design model and the RTOS-specific model describing the real-time application and thus, at the implementation level, timing properties may be affected. To tackle this issue, we define in this paper a reconfigurable middleware called RT-Mw. This middleware aims to explicitly describe the software assumptions at the design level for timing verification. Such approach allows early verification of these assumptions before the effective deployment which may prevents the mismatch between the design and the RTOS-Specific models. RT-Mw is described using UML modeling language together with the MARTE Standard.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843037,no
652,User-Centric Network Provisioning in Software Defined Data Center Environment,"Present data center (DC) network provisioning schemes primarily utilize conventional load-balancing technologies, offering individual application performance improvement. Diversity in application usage however, makes isolated application prioritization a performance caveat for users with varying application trends. The present paper proposes a user profiling approach to capture application trends based on generic flow measurements (NetFlow) and employs the extracted profiles to create DC traffic forwarding policies. The scheme allows operators to define a global profile and application hierarchy based on extracted profiles to prioritize traffic for individual user classes. The proposed design was tested by extracting user profiles from a realistic enterprise network, and further simulated to dynamically manage DC traffic using the software defined networking paradigm. Compared to conventional traffic management schemes, the frame delivery ratio and effective throughput of our design was significantly higher for high priority north-south user traffic as well as the inter-server east-west application traffic.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796801,no
653,Using memetic algorithms for test case prioritization in model based software testing,"Building high quality software is one of the main goals in software industry. Software testing is a critical step in confirming the quality of software. Testing is an expensive activity because it consumes about 30% to 50% of all software developing cost. Today much research has been done in generating and prioritizing tests. First, tester should find the most important and critical path in software. They can reduce cost by finding errors and preventing to propagate it in design step. In this paper, a model based testing method is introduced. This method can prioritize tests using activity diagram, control flow graph, genetic and memetic algorithm. Different version of memetic algorithm has been made by stochastic local search, randomize iterative improvement, hill climbing and simulated annealing algorithms. The results show that the using local search methods with genetic algorithm (GA) provide efficiency and produce competitive results in comparison with GA.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482129,yes
654,Using Software Metrics Thresholds to Predict Fault-Prone Classes in Object-Oriented Software,"Most code-based quality measurement approaches are based, at least partially, on values of multiple source code metrics. A class will often be classified as being of poor quality if the values of its metrics are above given thresholds, which are different from one metric to another. The metrics thresholds are calculated using various techniques. In this paper, we investigated two specific techniques: ROC curves and Alves rankings. These techniques are supposed to give metrics thresholds which are practical for code quality measurements or even for fault-proneness prediction. However, Alves Rankings technique has not been validated as being a good choice for fault-proneness prediction, and ROC curves only partially on few datasets. Fault-proneness prediction is an important field of software engineering, as it can be used by developers and testers as a test effort indication to prioritize tests. This will allow a better allocation of resources, reducing therefore testing time and costs, and an improvement of the effectiveness of testing by testing more intensively the components that are likely more fault-prone. In this paper, we wanted to compare empirically the selected threshold calculation methods used as part of fault-proneness prediction techniques. We also used a machine learning technique (Bayes Network) as a baseline for comparison. Thresholds have been calculated for different object-oriented metrics using four different datasets obtained from the PROMISE Repository and another one based on the Eclipse project.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916977,no
655,Visualization of combinatorial models and test plans,"Combinatorial test design (CTD) is an effective and widely used test design technique. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. One challenge for successful application of CTD in practice relates to this manual model definition and maintenance process. Another challenge relates to the comprehension and use of the test plan generated by CTD for prioritization purposes. In this work we introduce the use of visualizations as a means to address these challenges. We apply three different forms of visualization, matrices, graphs, and treemaps, to visualize the relationships between the different elements of the model, and to visualize the strength of each test in the test plan and the relationships between the different tests in terms of combinatorial coverage. We evaluate our visualizations via a user survey with 19 CTD practitioners, as well as via two industrial projects in which our visualization was used and allowed test designers to get vital insight into their models and into the coverage provided through CTD generated test plans.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582753,yes
656,10th International Workshop on Search-Based Software Testing (SBST 2017),"Summary form only given, as follows. SBST 2017 Workshop Summary. Search-Based Software Testing (SBST) is the application of optimizing search techniques (for example, Genetic Algorithms) to solve problems in software testing. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service- orientated architectures, construct test suites for interaction testing, and validate real-time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967910,no
657,A Fault Based Approach to Test Case Prioritization,"Regression testing is performed to ensure that the no new faults have been introduced in the software after modification and the software continues to work correctly. Regression testing is an expensive process because the test suite might be too large to execute in full. Thus to reduce the cost of such testing, regression testing techniques are used. One such technique is test case prioritization. Software testers assign priority to each test case to make sure that the test cases with higher priorities are executed first, in case of not having enough resources to execute the whole test suite. Test case prioritization is mainly used to increase fault detection rate of test suite which is the measure of how early faults are detected. In this paper, we propose an approach which exploits mutation testing in order to assign priorities to test cases. Using mutation testing, we introduce different faults in original program thus creating a number of mutated copies of the program and test case that exposes maximum number of these faults is given the highest priority. We report the outcomes of our experiments in which we applied our technique to test suites and calculated the fault detection rates produced by the prioritized test suites, comparing those rates of fault detection to the rates achieved by existing prioritization technique. The resulting data shows that prioritization technique proposed improved the fault detection rate of test suites.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8261011,yes
658,A Framework for Combining and Ranking Static Analysis Tool Findings Based on Tool Performance Statistics,"This paper proposes a conceptual, performance-based ranking framework that prioritises the output of multiple Static Analysis Tools, to improve the tool effectiveness and usefulness. The framework weights the performance of Static Analysis Tools per defect type and cross-validates the findings between different Static Analysis Tools' reports. An initial validation shows the potential benefits of the proposed framework.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004389,yes
659,A General Framework for Dynamic Stub Injection,"Stub testing is a standard technique to simulate the behavior of dependencies of an application under test such as the file system. Even though existing frameworks automate the actual stub injection, testers typically have to implement manually where and when to inject stubs, in addition to the stub behavior. This paper presents a novel framework that reduces this effort. The framework provides a domain specific language to describe stub injection strategies and stub behaviors via declarative rules, as well as a tool that automatically injects stubs dynamically into binary code according to these rules. Both the domain specific language and the injection are language independent, which enables the reuse of stubs and injection strategies across applications. We implemented this framework for both unmanaged (assembly) and managed (.NET) code and used it to perform fault injection for twelve large applications, which revealed numerous crashes and bugs in error handling code. We also show how to prioritize the analysis of test failures based on a comparison of the effectiveness of stub injection rules across applications.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985696,no
660,A Greedy-Based Method for Modified Condition/Decision Coverage Testing Criterion,"During software regression testing, the code coverage of target program is a crucial factor while we perform test case reduction and prioritization. Modified Condition/ Decision Coverage (MC/DC) is one of the most strict and high-accuracy criterion in code coverage and it is usually considered necessary for adequate testing of critical software. In the past, Hayhurst et al proposed a method to implement the MC/DC criterion that complies with regulatory guidance for DO-178B level A software. Hayhurst's MC/DC approach was to find some test cases which are satisfied by MC/DC criterion for each operator (and, or, not, or xor) in the Boolean expression. However, there could be some problems when using Hayhurst's MC/DC approach to select test cases. In this paper, we discuss how to improve and/or enhance Hayhurst's MC/DC approach by using a greedy-based method. Some experiments are performed based on real programs to evaluate as well as compare the performance of our proposed and Hayhurst's approaches.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8069087,yes
661,A novel approach to multiple criteria based test case prioritization,"When software is modified, it is retested to ensure that no new faults have been introduced in the previously tested code and it still works correctly. Such testing is known as regression testing. The cost of regression testing is high because the original program has large number of test cases. It is not feasible to execute all test cases for regression testing. Test suite minimization, test case selection and test case prioritization are cost commonly used techniques in regression testing to reduce the cost of regression testing. While test suite minimization and test case selection techniques select a subset of test cases, test case prioritization does not eliminate any test case, it only orders the test cases with the objective of increasing the fault detection rate. Prioritization is usually preferred over other two approaches because it does not involve the risk of losing useful test cases. Prioritization techniques assign priority to each test case on the basis of some coverage criteria. A number of different single criterion and multiple criteria based prioritization techniques have been proposed in the literature. Multiple criteria based prioritization techniques perform better than single criterion based prioritization techniques. The existing multiple criteria based prioritization techniques combine the criteria in such a way that ƒ??Additionalƒ? strategy cannot be applied on them. In this paper, we propose a new multiple criteria based test case prioritization algorithm that considers two criteria to prioritize test cases using ƒ??Additionalƒ? strategy. One criterion is considered as primary and other is considered as secondary. Primary criterion is used to prioritize the test cases whereas secondary criterion is used to break the tie among test cases when two or more test cases provide equal coverage of entities of first criterion. Our proposed multiple criteria based prioritization algorithm performs better than the existing prioritization techniques.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8281742,yes
662,A QoS Guaranteed Technique for Cloud Applications Based on Software Defined Networking,"Due to the centralized control, network-wide monitoring and flow-level scheduling of software-defined-networking (SDN), it can be utilized to achieve quality of service (QoS) for cloud applications and services, such as voice over IP, video conference, and online games. However, most existing approaches stay at the QoS framework design and test level, while few works focus on studying the basic QoS techniques supported by SDN. In this paper, we enable SDN with QoS guaranteed abilities, which could provide end-to-end QoS routing for each cloud user service. First of all, we implement an application identification technique on SDN controller to determine required QoS levels for each application type. Then, we implement a queue scheduling technique on SDN switch. It queues the application flows into different queues and schedules the flows out of the queues with different priorities. At last, we evaluate the effectiveness of the proposed SDN-based QoS technique through both theoretical and experimental analysis. Theoretical analysis shows that our methods can provide differentiated services for the application flows mapped to different QoS levels. Experiment results show that when the output interface has sufficiently available bandwidth, the delay can be reduced by 28% on average. In addition, for the application flow with the highest priority, our methods can reduce 99.99% delay and increase 90.17% throughput on average when the output interface utilization approaches to the maximum bandwidth limitation.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048495,no
663,A Regression Test Case Prioritization Algorithm Based on Program Changes and Method Invocation Relationship,"Regression testing is essential for assuring the quality of a software product. Because rerunning all test cases in regression testing may be impractical under limited resources, test case prioritization is a feasible solution to optimize regression testing by reordering test cases for the current testing version. In this paper, we propose a new test case prioritization algorithm based on program changes and method (function) invocation relationship. Combining the estimated risk value of each program method (function) and the method (function) coverage information, the fault detection capability of each test case can be calculated. The algorithm reduces the prioritization problem to an integer linear programming (ILP) problem, and finally prioritizes test cases according to their fault detection capabilities. Experiments are conducted on 11 programs to validate the effectiveness of our proposed algorithm. Experimental results show that our approach is more effective than some well studied test case prioritization techniques in terms of average percentage of fault detected (APFD) values.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305939,yes
664,A survey on prioritization regression testing test case,"Regression testing is a process used to measure the validity of the system during software maintenance. Regression testing process is very expensive and must be introduced each time a modification occurs in software to ensure that the system still work and that the new modification doesn't cause any bugs, this process depends on selecting test cases from a test suite. Selection of test cases is very critical since it affect the regression testing time and effort, so that many algorithms exist to enhance regression testing process. One of the methods used to make enhancements is to select test cases using prioritization testing techniques. Prioritization techniques find the bugs early to improve regression testing efficiency by prioritizing the test cases. In this paper many regression testing prioritization techniques were reviewed and analyzed.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8079958,yes
665,"A Test Case Recommendation Method Based on Morphological Analysis, Clustering and the Mahalanobis-Taguchi Method","This paper focuses on the content of test cases, and categorizes test cases into clusters using the similarity between test cases, their degree of similarity is obtained through a morphological analysis. If there are two similar test cases, they would test the same or similar functionalities in similar but different conditions. Thus, when one of them is run for a regression testing, the remaining one should be run as well, in order to reduce a risk of overlooking regressions. Once a test engineer decides to run a set of test cases, the method proposed in this paper can recommend adding similar test cases to their candidate set. The proposed method also considers the priorities of recommended test cases by using the Mahalanobis-Taguchi method. This paper reports on an empirical study with an industrial software product. The results show that the proposed method is useful to prevent overlooking regressions.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899026,yes
666,Adapting code maintainability to bat-inspired test case prioritization,"Time and budget constraints in developing a software create an adverse effect in terms of the adequacy of maintenance and test processes. This case can be considered as a burden for persons who account for test processes. In order to alleviate this burden, test case prioritization is one of the solutions. A nature-inspired method namely BITCP, which was developed based on bat algorithm, produced promising results. However, this method does not involve test case elements with respect to the code maintainability. In this work, the correlation between some code maintainability indicators including WMC, LCOM, and Coupling and cyclomatic complexity is investigated. IMPBITCP appears after adapting the results of the investigation to BITCP. The method is then compared with well known alternatives such as greedy-search, particle swarm optimization, and BITCP. The experiment involving six open source project showed that IMPCBITCP outperformed the others with respect to the APFD. The findings of the work indicates that if the factors affecting code maintenance are considered while developing test case prioritization techniques, APFD results becomes high and stable.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001134,yes
667,ALOJA: A Framework for Benchmarking and Predictive Analytics in Hadoop Deployments,"This article presents the ALOJA project and its analytics tools, which leverages machine learning to interpret big data benchmark performance data and tuning. ALOJA is part of a long-term collaboration between Barcelona Supercomputing Center and Microsoft to automate the characterization of cost-effectiveness on big data deployments, currently focusing on Hadoop. Hadoop presents a complex run-time environment, where costs and performance depend on a large number of configuration choices. The ALOJA project has created an open, vendor-neutral repository, featuring over 40000 Hadoop job executions and their performance details. The repository is accompanied by a test bed and tools to deploy and evaluate the cost-effectiveness of different hardware configurations, parameters, and cloud services. Despite early success within ALOJA, a comprehensive study requires automation of modeling procedures to allow an analysis of large and resource-constrained search spaces. The predictive analytics extension, ALOJA-ML, provides an automated system allowing knowledge discovery by modeling environments from observed executions. The resulting models can forecast execution behaviors, predicting execution times for new configurations and hardware choices. That also enables model-based anomaly detection or efficient benchmark guidance by prioritizing executions. In addition, the community can benefit from ALOJA data sets and framework to improve the design and deployment of big data applications.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312954,no
668,An Empirical Comparison of Similarity Measures for Abstract Test Case Prioritization,"Test case prioritization (TCP) attempts to order test cases such that those which are more important, according to some criterion or measurement, are executed earlier. TCP has been applied in many testing situations, including, for example, regression testing. An abstract test case (also called a model input) is an important type of test case, and has been widely used in practice, such as in configurable systems and software product lines. Similarity-based test case prioritization (STCP) has been proven to be cost-effective for abstract test cases (ATCs), but because there are many similarity measures which could be used to evaluate ATCs and to support STCP, we face the following question: How can we choose the similarity measure(s) for prioritizing ATCs that will deliver the most effective results? To address this, we studied fourteen measures and two popular STCP algorithms - local STCP (LSTCP), and global STCP (GSTCP). We also conducted an empirical study of five realworld programs, and investigated the efficacy of each similarity measure, according to the interaction coverage rate and fault detection rate. The results of these studies show that GSTCP outperforms LSTCP - in 61% to 84% of the cases, in terms of interaction coverage rates; and in 76% to 78% of the cases with respect to fault detection rates. Our studies also show that Overlap, the simplest similarity measure examined in this study, could obtain the overall best performance for LSTCP; and that Goodall3 has the best performance for GSTCP.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029584,yes
669,An Empirical Examination of Abstract Test Case Prioritization Techniques,"Abstract test case prioritization (ATCP) aims at ordering abstract test case in order to increase the speed at which faults are detected, potentially increasing the fault detection rate. This paper empirically examines possible ATCP techniques, according to the following four categories: non-information-guided prioritization (NIGP), interaction coverage based prioritization (ICBP), input-model mutation based prioritization (IMBP), and similarity based prioritization (SBP). We found that the ICBP category has better testing effectiveness than others, according to fault detection rates. Surprisingly, we found that NIGP can achieve similar performance to IMBP, and that SBP can sometimes achieve even better rates of fault detection than some ICBP techniques.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965282,yes
670,An empirical study on clustering approach combining fault prediction for test case prioritization,"Using Clustering algorithm to improve the effectiveness of test case prioritization has been well recognized by many researchers. Software fault prediction has been one of the active parts of software engineering, but to date, there are few test cases prioritization technique using fault prediction. We conjecture that if the code has a fault-proneness, the test cases covering the code will find fault with higher probability. In addition, most of the existing test cases prioritization techniques using clustering algorithm don't consider the number of clusters. Thus, in this paper, we design a test case prioritization based on clustering approach combining fault prediction. We consider the method to obtain the best number of clusters and the clustering prioritization based on the results of fault prediction. To investigate the effectiveness of our approach, we perform an empirical study using an object which contains test cases and faults. The experiment results indicate that our techniques can improve the effectiveness of test case prioritization.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960104,yes
671,An empirical study on clustering approach combining fault prediction for test case prioritization,"Using Clustering algorithm to improve the effectiveness of test case prioritization has been well recognized by many researchers. Software fault prediction has been one of the active parts of software engineering, but to date, there are few test cases prioritization technique using fault prediction. We conjecture that if the code has a fault-proneness, the test cases covering the code will findfault with higher probability. In addition, most of the existing test cases prioritization techniques using clustering algorithm don't consider the number of clusters. Thus, in this paper, we design a test case prioritization based on clustering approach combining fault prediction. We consider the method to obtain the best number of clusters and the clustering prioritization based on the results of fault prediction. To investigate the effectiveness of our approach, we perform an empirical study using an object which contains test cases and faults. The experiment results indicate that our techniques can improve the effectiveness of test case prioritization.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960105,yes
672,An Empirical Study on the Effect of Testing on Code Quality Using Topic Models: A Case Study on Software Development Systems,"Previous research in defect prediction has proposed approaches to determine which files require additional testing resources. However, practitioners typically create tests at a higher level of abstraction, which may span across many files. In this paper, we study software testing, especially test resource prioritization, from a different perspective. We use topic models to generate topics that provide a high-level view of a system, allowing developers to look at the test case coverage from a different angle. We propose measures of how well tested and defect prone a topic is, allowing us to discover which topics are well tested and which are defect prone. We conduct case studies on the histories of Mylyn, Eclipse, and NetBeans. We find that 34-78% of topics are shared between source code and test files, indicating that we can use topic models to study testing; well-tested topics are usually less defect prone, defect-prone topics are usually undertested; we can predict which topics are defect prone but not well tested with an average precision and recall of 75% and 77%, respectively; our approach complements traditional prediction-based approaches by saving testing and code inspection effort; and our approach is not particularly sensitive to the parameters that we use.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949108,yes
673,An extended adaptive process model for agile software development methodology,"Agile methodologies focus on the agility for the development of software. Among various agile methods, eXtreme Programming (XP) is the most adopted agile method. XP has been used for the development of other agile methods, for example, ƒ??adaptive software development process modelƒ? (ASDPM), which is the modified approach of XP. ASDPM was proposed to support the following activities: ƒ??(a) communication and planning, (b) analysis, (c) design and development, and (d) testingƒ?. Based on our literature review of ASDPM, we identify that ASDPM does not support the following: (i) how to identify the different types of agile team members who will participate during the communication and planning phase?; and (ii) how to deliver the most important requirements of the software during analysis phase?. Therefore, in order to address this issue and to strengthen the analysis phase of agile process models, in this paper we propose an extended adaptive process model (APM) for agile software development methodology. This method includes the following steps: (1) identification of agile team members, (2) communication and planning, (3) analysis includes the computation of function point of each requirement; and the selection and prioritization of the requirements, (4) design and development, (5) testing. Finally, the utilization of the proposed method is demonstrated with the help of Institute Examination System, as a case study.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342770,yes
674,An Ilities-Driven Methodology for the Analysis of Gaps of Stakeholders Needs in Space Systems Conceptual Design,"The new generation of space-based services includes large-scale, integrated, and distributed informational systems for which traditional system engineering approaches show some limits in delivering the ƒ??big picture.ƒ? Missing the view of the full range of design options, or prematurely translating the perceived stakeholders needs into design requirements, is often a consequence of insufficient regard to the end-users priorities. Objective of the present research is to bring to light the gaps extant between what system architects prioritize, and the preferences of potential system users. To this purpose, the proposed method aims at incorporating lifecycle properties (-ilities) in the concept design phase, by submitting attributes of these properties for the evaluation of two stakeholders representative groups. The case study refers to the integration of environmental measurements, coming from a global-navigation-satellite-systems-based remote sensing satellite constellation, as complementary data to the traditional weather-forecasting service, resulting in a new system of systems. The method runs through an interview-based quality function deployment process and collaborative sessions of teams of stakeholders. The strength of the formulation relies on the ability to treat a quantitative measure of the gaps extant between system desired capabilities as perceived by architects, and real end-user needs. The method can be potentially tested in a concurrent design environment as a complementary tool for eliciting requirements and suggesting the areas where investments and resources should be preferably allocated. Results can be used by researchers as pieces of knowledge to be further investigated, and by practitioners in development projects, taking into account that they are preliminary findings.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498637,no
675,An Industrial Study of Natural Language Processing Based Test Case Prioritization,"In mobile application development, the frequentsoftware release limits the testing time resource. In order todetect bugs in early phases, researchers proposed various testcase prioritization (TCP) techniques in past decades. In practice, considering that some test case is described or contains text, theresearchers also employed Natural Language Processing (NLP)to assist the TCP techniques. This paper conducted an extensiveempirical study to analyze the performance of three NLP basedTCP technologies, which is based on 15059 test cases from 30industrial projects. The result shows that all of these threestrategies can help to improve the efficiency of software testing, and the Risk strategy achieved the best performance across thesubject programs.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7928016,yes
676,Automated System-Level Regression Test Prioritization in a Nutshell,"Westermo Research and Development has developed SuiteBuilder, an automated tool to determine an effective ordering of regression test cases. The ordering is based on factors such as fault detection success, the interval since the last execution, and code modifications. SuiteBuilder has enabled Westermo to overcome numerous regression-testing problems, including lack of time to run a complete regression suite, failure to detect bugs in a timely manner, and repeatedly omitted tests. In the tool's first two years of use, reordered test suites finished in the available time, most fault-detecting test cases were located in the first third of suites, no important test case was omitted, and the necessity for manual work on the suites decreased greatly.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7974685,yes
677,Autonomous observation of multiple USVs from UAV while prioritizing camera tilt and yaw over UAV motion,"This paper proposes a scheme for observing cooperative Unmanned Surface Vehicles (USV), using a rotorcraft Unmanned Aerial Vehicle (UAV) with camera movements (tilt and yaw) prioritized over UAV movements. Most of the current researches consider a fixed-wing type UAV for surveillance of multiple moving targets (MMT), whose functionality is limited to just UAV movements. Experiments in simulation are conducted and verified that, prioritizing camera movements increased the number of times each USV is visited (on an average by 5.68 times more), decreased the percentage of the duration that the UAV is not observing any USV (on an average by 19.8%) and increased the efficiency by decreasing the distance traveled by the UAV (on an average by 747 pixels) for the six test cases. Autonomous repositioning of the UAV at regular intervals to observe USVs during a disaster scenario will provide the operator with better situational awareness. Using a rotorcraft over a fixed-wing type UAV provides the operator with a flexibility of observing the target for the required duration by hovering and freedom of unrestricted movements, which help improve the efficiency of target observation.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8088154,no
678,Big RF Data Assisted Cognitive Radio Network Coexistence in 3.5GHz Band,"In this paper, big Radio Frequency (RF) data assisted optimization is considered for future wireless networks employing cognitive radio technology with machine learning capability. A cognitive radio network (CRN) with multiple Secondary Users (SUs) may coexist with other wireless systems such as Small Cells (SC) and Radar systems, both Primary Users (PUs) with different level of priorities. Traditional spectrum sensing typically only gives information about the presence or absence of a PU. However, when multiple heterogeneous systems coexist, it becomes imperative to acquire the knowledge of the systems operating in a specific band at a particular time so as to choose an appropriate transmission strategy. In this work, we take advantage of the learning capability of a Neural Network Predictor (NNP) to obtain the statistics of the coexisted wireless systems from the RF traces collected in our Universal Software Radio Peripheral (USRP) based test bed. The NNP is able to learn the features of the RF traces and make accurate prediction of the signals prevalent in the wireless environment. Because of the augmented information learned from the RF traces, a novel optimization problem incorporating the outputs from the NNP is formulated to maximize the throughput of the CRN. The solution is derived using Karush- Kuhn-Tucker (KKT) and extensive simulations using the real RF traces are carried out. It is demonstrated that the NNP can detect the type and number of coexisted users reliably and the proposed scheme will improve the performance of the coexisted CRN.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038357,no
679,CBGA-ES: A Cluster-Based Genetic Algorithm with Elitist Selection for Supporting Multi-Objective Test Optimization,"Multi-objective search algorithms (e.g., non-dominated sorting genetic algorithm II (NSGA-II)) have been frequently applied to address various testing problems requiring multi-objective optimization such as test case selection. However, existing multi-objective search algorithms have certain randomness when selecting parent solutions for producing offspring solutions. In the worse case, suboptimal parent solutions may result in offspring solutions with bad quality, and thus affect the overall quality of the next generation. To address such a challenge, we propose a cluster-based genetic algorithm with elitist selection (CBGA-ES) with the aim to reduce such randomness for supporting multi-objective test optimization. We empirically compared CBGA-ES with random search, greedy (as baselines) and four commonly used multi-objective search algorithms (e.g., NSGA-II) using two industrial and one real world test optimization problem, i.e., test suite minimization, test case prioritization, and test case selection. The results showed that CBGA-ES significantly outperformed the baseline algorithms (e.g., greedy), and the four selected search algorithms for all the three test optimization problems. CBGA-ES managed to outperform more than 75% of the objectives for all the four algorithms in each test optimization problem. Moreover, CBGA-ES was able to improve the quality of the solutions for an average of 32.5% for each objective as compared to the four algorithms for the three test optimization problems.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927990,no
680,Classification model for test case prioritization techniques,"Regression Testing is mainly done in software maintenance aiming to assure that the changes made in the software have correctly been implemented and also to achieve the confidence that the modifications have not affected the other parts of the software. It is very costly and expensive technique. There are number of techniques present in literature that focus on achieving various testing objectives early in the process and hence reduces its cost. Despite of that, testers usually prefer only few already known techniques for test case prioritization. The main reason behind is the absence of guidelines for the selection of TCP techniques. Hence, this piece of research introduces a novel approach for classification of TCP techniques using fuzzy logic to support the efficient selection of test case prioritization techniques. This work is an extension of already proposed selection schema for test case prioritization techniques. To perform the validation of proposed approach results are compared with other classification techniques using Weka tool. The analysis clearly shows the effectiveness of proposed approach as compared to others in terms of its accuracy.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8229925,yes
681,Cloud-based parallel concolic execution,"Path explosion is one of the biggest challenges hindering the wide application of concolic execution. Although several parallel approaches have been proposed to accelerate concolic execution, they neither scale well nor properly handle resource fluctuations and node failures, which often happen in practice. In this paper, we propose a novel approach, named PACCI, which parallelizes concolic execution and adapts to the drastic changes of computing resources by leveraging cloud infrastructures. PACCI tailors concolic execution to the MapReduce programming model and takes into account the features of cloud infrastructures. In particular, we tackle several challenging issues, such as making the exploration of different program paths independently and constructing an extensible path exploration module to support the prioritization of test inputs from a global perspective. Preliminary experimental results show that PACCI is scalable (e.g., gaining about 20?? speedup using 24 nodes) and its efficiency declines slightly about 5% and 6.1% under resource fluctuations and node failures, respectively.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884649,no
682,Comparison analysis of two test case prioritization approaches with the core idea of adaptive,"Test case prioritization problem (TCP) has been widely discussed. It aims to controlling the test case execution sequence to improve the effectiveness of software testing. The key issue of TCP is to identify which test cases can provide useful information for failure detection and fault localization. So far, many TCP approaches have been proposed. Among them, Adaptive Random Testing (ART) and Dynamic Random Testing (DRT) are two of the most popular approaches to solve TCP with a basic idea borrowed from Cybernetics: adaptive. Both ART and DRT has been widely explored and observed with good performances in experimental studies. Nevertheless, although they are proposed by two related research groups, they are developed independently and in parallel. In fact, their mechanisms have many similarities and differences and, for the completeness of the domains of Adaptive Testing and Software Cybernetics, many issues concerning the comparison between these two approaches should be further explored. In this paper, we specifically explores the relationship between these two adaptive TCP approaches. Their mechanisms are described respectively with explorations of their distinctions, similarities, and respective characteristics. Moreover, based on these explorations, we analyse their advantages from the aspects of failure detection and fault understanding. During the analysis, a symbolic-graphic combination method is applied. Finally simulation based on real-life programs is conducted to observe our analysis. Our comparison analysis can support the selection of a proper testing approach according to various practical environments with different targets. Furthermore, the clarification of the two easily confused concepts is also a complement for the framework of Adaptive Testing and Software Cybernetics.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7978795,yes
683,Cost aware test suite reduction algorithm for regression testing,"Regression testing is the process that a recent code change has not adversely affect the existing features. The re-running of all the test cases during regression testing is very expensive as it requires huge time and resources. Test case prioritization techniques are to schedule the test cases in accordance with some criteria such that important test cases are executed with that given period. This study presents test case prioritization using genetic algorithm and their effectiveness is measured using APFD. Then the prioritized test cases are reduced. Test suite reduction techniques aim at identifying and eliminating redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of the software testing activity. Our aim is to reduce the cost by reducing the number of test suite after prioritization. MFTS algorithm is used to reduce the given test suite with maximum coverage and it improves the rate of fault detection effectiveness.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300829,yes
684,Cost-Effective Regression Testing Using Bloom Filters in Continuous Integration Development Environments,"Regression testing in continuous integration development environments must be cost-effective and should provide fast feedback on test suite failures to the developers. In order to provide faster feedback on failures to developers while using computing resources efficiently, two types of regression testing techniques have been developed: Regression Testing Selection (RTS) and Test Case Prioritization (TCP). One of the factors that reduces the effectiveness of the RTS and TCP techniques is the inclusion of test suites that fail only once over a period. We propose an approach based on Bloom filtering to exclude such test suites during the RTS process, and to assign such test suites with a lower priority during the TCP process. We experimentally evaluate our approach using a Google dataset, and demonstrate that cost-effectiveness of the proposed RTS and TCP techniques outperforms the state-of-the-art techniques.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305938,yes
685,Coverage-Based Reduction of Test Execution Time: Lessons from a Very Large Industrial Project,"There exist several coverage-based approaches to reduce time and resource costs of test execution. While these methods are well-investigated and evaluated for smaller to medium-size projects, we faced several challenges in applying them in the context of a very large industrial software project, namely SAP HANA. These issues include: varying effectiveness of algorithms for test case selection/prioritization, large amounts of shared (non-specific) coverage between different tests, high redundancy of coverage data, and randomness of test results (i.e. flaky tests), as well as of the coverage data (e.g. due to concurrency issues). We address these issues by several approaches. First, our study shows that compared to standard algorithms, so-called overlap-aware solvers can achieve up to 50% higher code coverage in a fixed time budget, significantly increasing the effectiveness of test case prioritization and selection. We also detected in our project high redundancy of line coverage data (up to 97%), providing opportunities for data size reduction. Finally, we show that removal of coverage shared by tests can significantly increase test specificity. Our analysis and approaches can help to narrow the gap between research and practice in context of coverage-based testing approaches, especially in case of very large software projects.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899023,yes
686,d(mu)Reg: A Path-Aware Mutation Analysis Guided Approach to Regression Testing,"Regression testing re-runs some previously executed test cases, with the purpose of checking whether previously fixed faults have re-emerged and ensuring that the changes do not negatively affect the existing behaviors of the software under development. Today's software is rapidly developed and evolved, and thus it is critical to implement regression testing quickly and effectively. In this paper, we propose a novel technique for regression testing, based on a family of mutant selection strategies. The preliminary results show that the proposed technique can significantly improve the efficiency of different regression testing activities, including test case reduction and prioritization. Our work also makes it possible to develop a unified framework that effectively implements various activities in regression testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962333,yes
687,Delta-Oriented Product Prioritization for Similarity-Based Product-Line Testing,"Testing every product of a software product line (SPL) is often not feasible due to the exponential number of products in the number of features. Thus, the order in which products are tested matters, because it can increase the early rate of fault detection. Several approaches have been proposed to prioritize products based on configuration similarity. However, current approaches are oblivious to solution-space differences among products, because they consider only problem-space information. With delta modeling, we incorporate solution-space information in product prioritization to improve the effectiveness of SPL testing. Deltas capture the differences between products facilitating the reasoning about product similarity. As a result, we select the most dissimilar product to the previously tested ones, in terms of deltas, to be tested next. We evaluate the effectiveness of our approach using an SPL from the automotive domain showing an improvement in the effectiveness of SPL testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968061,yes
688,Deriving high-priority acceptance test cases using utility trees: A case study,"Even though software testing is considered a mature field in software engineering, deriving test cases is still an important issue and even more when related to quality requirements. Utility Trees are used to evaluate software architectures, organizing requirements as scenarios associated to quality attributes and decorating them with stakeholder-given priority and developer- given difficulty. In this article, we propose an approach to use Utility Trees to derive prioritized acceptance test cases allowing to focus in high-value tests. The technique has been tried in two medium-sized projects for a Chilean public agency, with positive results. This innovative use of Utility Trees offers a simple, collaborative way to focus testing resources.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8405142,yes
689,Design and Implementation of Combinatorial Testing Tools,"As an effective software testing technique, combinatorial testing has been gradually applied in various types of test practice. In this case, it is necessary to provide useful combinatorial testing tools to support the application of combinatorial testing technique on industrial scenarios, as well as the academic research for combinatorial testing technique. To this end, on the basis of the research results of this group, a suite of combinatorial testing tools has been developed, whose functions include test case generation, test case optimization, and etc. For the requirements from both industrial and academic scenarios, the tools should be configurable, scalable, modular, and etc. This paper gives a brief introduction to the design and implementation of these tools. Keywords-combinatorial testing, combinatorial testing tools, test generation, test prioritization.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004338,yes
690,Digital learning as a tool to overcome school failure in minority groups,"In the European Union development strategy formulated in the Europe 2020 document (European Commission, 2015) it was indicated, that the smart growth of the EU as a whole should be reached through the realization of three priorities: the increase in employment, the increase of productiveness and the social cohesion and specialized agendas: Digital Agenda, Education and Learning, E-skills and Employment. The main documents identify main weaknesses and risk areas. One of the most significant was described as the early school leaving of Roma minority members. Roma constitute Europe's largest transnational ethnic minority with an estimate of ten million people. Learning outcomes of this minority are significantly lower than outcomes of the majority. As one of the reasons for early school leaving of Roma, insufficient understanding of learning materials is identified. The result is that most of the Roma community members drop out of education before attending a secondary school and continue their lives as unemployed or enter the labor market as unskilled workers. Within the paper will be presented the CloudLearning project that represents an alternative and innovative educational method: the way of the SOLE method implemented in their education. This paper will include partial results from the pilot tests realized these days.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973525,no
691,Educational prototype demonstrating frequency spectrum sharing through channel borrowing and priority assignment,"The purpose of this research is to build an educational prototype for attracting high school seniors and college students to pursue university degrees. The prototype entails demonstrable hardware and software comprising of a set of communication nodes with call priorities, which are used to help educate students on future and practical implications of spectrum sharing. Two objectives are achieved by building this hands-on prototype: (1) Students learn firsthand basics of communication systems and (2) Students are taught the concept and feasibility of ƒ??priorityƒ? in RF device communication. In crafting this easy-to-use prototype, integrated Arduinos, RF modules, and open-source libraries are utilized. Eight simplex devices (or nodes) that can communicate over just three channels in the 2.4GHz ISM band are built to articulate the spectrum scarcity challenge. When placing a call, a device requests a channel from the base station, which notifies the device about available channels, taking into account both device priorities (e.g. some devices have higher priorities) and channels that are in use by other calls. The base station has the authority to remove devices from a channel or transfer them to another channel when necessary. The paradigm of ƒ??priority assignmentƒ? is implemented keeping in mind futuristic trends in communication systems aimed at optimal use of the transmission spectrum. The prototype is validated via many test cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053422,no
692,Efficient Product-Line Testing Using Cluster-Based Product Prioritization,"A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962326,yes
693,Enabling Software Defined Networking with QoS Guarantee for Cloud Applications,"Due to the centralized control, network-wide monitoring and flow-level scheduling of Software-Defined-Networking (SDN), it can be utilized to achieve Quality of Service (QoS) for cloud applications and services, such as voice over IP, video conference and online games, etc. However, most existing approaches stay at the QoS framework design and test level, while few works focus on studying the basic QoS techniques supported by SDN. In this paper, we enable SDN with QoS guaranteed abilities, which could provide end-to-end QoS routing for each cloud user service. First of all, we implement an application identification technique on SDN controller to determine required QoS levels for each application type. Then, we implement a queue scheduling technique on SDN switch. It queues the application flows into different queues and schedules the flows out of the queues with different priorities. At last, we evaluate the effectiveness of the proposed SDN-based QoS technique through an experimental analysis. Results show that when the output interface has sufficiently available bandwidth, the delay can be reduced by 28% on average. In addition, for the application flow with the highest priority, our methods can reduce 99.99% delay and increase 90.17% throughput on average when the output interface utilization approaches to the maximum bandwidth limitation.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030581,no
694,Epistasis Based ACO for Regression Test Case Prioritization,"Metaheuristics that are inspired by natural systems have been widely applied into search-based software engineering. It has been shown that combining knowledge of the application domain with a biological theory for metaheuristics can narrow down the search space and speed up the convergence for metaheuristics based algorithms. This paper introduces Epistatic Test case Segment (ETS) for multiobjective search-based regression Test Case Prioritization (MoTCP), based on epistasis theory that reflects the correlation between genes in evolution process. An ETS-based pheromone update strategy for ant colony optimization (ACO) algorithm is proposed. The experiments with three benchmarks and a real industrial program V8 illustrate that proposed pheromone update strategy guided by the epistasis theory can significant improve the performance of ACO in terms of effectiveness and efficiency for search-based MoTCP.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7935487,yes
695,Evaluation of AV systems against modern malware,"Countering the proliferation of malware has been for recent years one of the top priorities for governments, businesses, critical infrastructure, and end users. Despite the apparent evolvement of anti-virus (AV) systems, malicious authors have managed to create a sense of insecurity amongst computer users. Security controls do not appear to be sufficiently strong to stop malware proliferating. There seems to be a disconnect between public reports on AV tests and what people are experiencing on the daily basis. In this research, we are testing the efficiency of AV products and their ability to detect malicious files commonly known as malware. We manually generated payloads from five malware frameworks freely available to download and use. We use two modes of tests during our experiments. We manually installed a selection of AV systems in one first instance. We also use an online framework for testing malicious files. The findings in this study show that many antivirus systems were not able to achieve a higher score than 80% detection rate. Certain attack frameworks were much more successful in generating payloads that were not detectable by AV systems. We conclude that AV systems have their roles to play as they are the most common first line of defense, but more work is needed to successfully detect most malware the first day of their release.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356397,no
696,Exniffer: Learning to Prioritize Crashes by Assessing the Exploitability from Memory Dump,"An important component of software reliability is the assurance of certain security guarantees, such as absence of low-level bugs that may result in code exploitation, for example. A program crash is an early indicator of possible errors in the program like memory corruption, access violation or division by zero. In particular, a crash may indicate the presence of safety or security critical errors. A safety-error crash does not result in any exploitable condition, whereas a security-error crash allows an attacker to exploit a vulnerability. However, distinguishing one from the other is a non-trivial task. This exacerbates the problem in cases where we get hundreds of crashes and programmers have to make choices which crash to patch first! In this work, we present a technique to identify security critical crashes by applying machine learning on a set of features derived from core-dump files and runtime information obtained from hardware assisted monitoring such as the last branch record (LBR) register. We implement the proposed technique in a prototype called Exniffer. Our empirical results, obtained by experimenting Exniffer on several crashes on real-world applications show that proposed technique is able to classify a given crash as exploitable or not-exploitable with high accuracy.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305946,yes
697,Extravehicular activity operations concepts under communication latency and bandwidth constraints,"The Biologic Analog Science Associated with Lava Terrains (BASALT) project is a multi-year program dedicated to iteratively develop, implement, and evaluate concepts of operations (ConOps) and supporting capabilities intended to enable and enhance human scientific exploration of Mars. This paper describes the planning, execution, and initial results from the first field deployment, referred to as BASALT-1, which consisted of a series of ten simulated extravehicular activities on volcanic flows in Idaho's Craters of the Moon National Monument and Preserve. The ConOps and capabilities deployed and tested during BASALT-1 were based on previous NASA trade studies and analog testing. Our primary research question was whether those ConOps and capabilities work acceptably when performing real (non-simulated) biological and geological scientific exploration under four different Mars-to-Earth communication conditions: 5 and 15 min one-way light time communication latencies and low (0.512 Mb/s uplink, 1.54 Mb/s downlink) and high (5.0 Mb/s uplink, 10.0 Mb/s downlink) bandwidth conditions, which represent two alternative technical communication capabilities currently proposed for future human exploration missions. The synthesized results, based on objective and subjective measures, from BASALT-1 established preliminary findings that the baseline ConOp, software systems, and communication protocols were scientifically and operationally acceptable with minor improvements desired by the ƒ??Marsƒ? extravehicular and intravehicular crewmembers. However, unacceptable components of the ConOps and required improvements were identified by the ƒ??Earthƒ? Mission Support Center. These data provide a basis for guiding and prioritizing capability development for future BASALT deployments and, ultimately, future human exploration missions.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7943570,no
698,Flow Reconnaissance via Timing Attacks on SDN Switches,"When encountering a packet for which it has no matching forwarding rule, a software-defined networking (SDN) switch requests an appropriate rule from its controller; this request delays the routing of the flow until the controller responds. We show that this delay gives rise to a timing side channel in which an attacker can test for the recent occurrence of a target flow by judiciously probing the switch with forged flows and using the delays they encounter to discern whether covering rules were previously installed in the switch. We develop a Markov model of an SDN switch to permit the attacker to select the best probe (or probes) to infer whether a target flow has recently occurred. Our model captures practical challenges related to rule evictions to make room for other rules; rule timeouts due to inactivity; the presence of multiple rules that apply to overlapping sets of flows; and rule priorities. We show that our model enables detection of target flows with considerable accuracy in many cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979967,no
699,Formal Methods for Validation and Test Point Prioritization in Railway Signaling Logic,"The EN50128 Railway Safety Standard recommends the use of formal methods for proving the correctness of the yard-specific logic, which was developed for electronic signaling and interlocking systems. We present a tool flow, which consists of three components. The core component uses a novel method for automatically generating the relevant safety properties for a yard from its control table. The second component proves the validity of the properties on the application logic by using a new theory of invariant checking. The third component leverages the suite of formal properties to prioritize site acceptance test points. Experimental results are presented on real application data for the yards in India that are demonstrating the performance of the proposed methods.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529152,yes
700,Game Theoretic Study on Channel-Based Authentication in MIMO Systems,"In this paper, we investigate the authentication based on radio channel information in multiple-input multiple-output (MIMO) systems and formulate the interactions between a receiver with multiple antennas and a spoofing node as a zero-sum physical (PHY)-layer authentication game. In this game, the receiver chooses the test threshold of the hypothesis test to maximize its Bayesian risk-based utility in the spoofing detection, while the adversary chooses its attack rate, i.e., how often a spoofing signal is sent. We derive the Nash equilibrium (NE) of the static PHY-layer authentication game and present the condition that the NE exists, showing that both the spoofing detection error rates and the spoofing rate decrease with the number of transmit and receive antennas. We propose a PHY-layer spoofing detection algorithm for MIMO systems based on Q-learning, in which the receiver applies the reinforcement learning technique to achieve the optimal test threshold via trials in a dynamic game without knowing the system parameters, such as the channel time variation and spoofing cost. We also use Dyna architecture and prioritized sweeping (Dyna-PS) to improve the spoofing detection in time-variant radio environments. The proposed authentication algorithms are implemented over universal software radio peripherals and evaluated via experiments in an indoor environment. Experimental results show that the Dyna-PS-based spoofing detection algorithm further reduces the spoofing detection error rates and increases the utility of the receiver compared with the Q-learning-based algorithm, and both performances improve with more number of transmit or receive antennas.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815442,no
701,Gamifying Collaborative Prioritization: Does Pointsification Work?,"Gamification has been applied in software engineering contexts, and more recently in requirements engineering with the purpose of improving the motivation and engagement of people performing specific engineering tasks. But often an objective evaluation that the resulting gamified tasks successfully meet the intended goal is missing. On the other hand, current practices in designing gamified processes seem to rest on a try, test and learn approach, rather than on first principles design methods. Thus empirical evaluation should play an even more important role.We combined gamification and automated reasoning techniques to support collaborative requirements prioritization in software evolution. A first prototype has been evaluated in the context of three industrial use cases. To further investigate the impact of specific game elements, namely point-based elements, we performed a quasi-experiment comparing two versions of the tool, with and without pointsification. We present the results from these two empirical evaluations, and discuss lessons learned.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8049138,no
702,Impact of Static and Dynamic Coverage on Test-Case Prioritization: An Empirical Study,"Most of existing research in Test-Case Prioritization uses coverage information as the input during the process of prioritization and these coverage can be classified into two categories: static coverage and dynamic coverage. As these coverage information are collected in different ways, they have different influence on test-case prioritization. In this work, we present the first empirical study comparing the impact of static coverage and dynamic coverage with five typical techniques at different test-case granularities (e.g., test-method and test-class level) and different coverage criteria (e.g., method and statement coverage). This study is performed on 15 real-world Java projects (using 163 versions) and we find that the dynamic coverage performs better than static coverage in terms of the results of test-case prioritization.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899091,yes
703,Improving test case prioritization based on practical priority factors,"Test case prioritization involves prioritized the test cases for regression testing which improve the effectiveness of the testing process. By improving test case scheduling we can optimize time and cost as well as can produce better tested products. There are a number of methods to do prioritized test cases but not that effective or practical for the real-life large commercial systems. Most of the technique deals with finding defects or covering more test cases. In this paper, we will extend the previous work to incorporate real life practical aspects to schedule test cases. This will cover most of the businesses functionally based on the practical aspects. This approach covers more business area and ensure more defects. By prioritized test cases with this technique we will cover most important business functionally with less number of test cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8343055,yes
704,Improving the Cooperation of Fuzzing and Symbolic Execution by Test-cases Prioritizing,"Nowadays much attention is paid to the threat of vulnerabilities on the software security. Fuzzing and symbolic execution, complementary to each other, are two effective techniques in software testing. In this paper, we develop the prototype called FAS(Fuzzing and Symbolic) for software testing under both fuzzing and symbolic execution. In our method, the test cases are prioritized through deep-oriented strategy and large-distance-first strategy, in order to get a higher path-coverage with the condition of limited resource.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288548,yes
705,Knowledge Transfer for Global Roles in GSE,"This practice paper presents how a software engineering organization spread across three countries successfully transferred the knowledge of a few identified roles for a large mission-critical software system that had to conform to regulatory requirements. Multiple releases of the system have been delivered to customers over the 15 years it has been in the market. Each release of the product had a focus area. The competence availability for these focus areas was distributed. As a natural evolution of the globally distributed team, greater responsibility is devolved to a particular location, based on the availability of the competence at that location. Moving the increased responsibility to a location, created a global role, which did not exist earlier. Building the new role required a new skill, what is unique about a global role. Equipping the team members in the new skill was necessary to take up the roles effectively and quickly. The first step was the identification of the competence for a function/role, training for which may be imparted to another person, who will take over the function/role. This is followed by a process of knowledge transfer, which ensured that a person can take up a new global role from another location. Prioritization based on ease of knowledge transfer for different areas of work, that was found to be effective is described. This helped reduce possible problems that could occur due to incorrect or incomplete transfer of knowledge. The advantages by such knowledge transfer that resulted in new persons taking up global roles have outweighed its disadvantages. The practices described are generic and can be applied to any organization of similar size and complexity.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976692,yes
706,Learning to Predict Severity of Software Vulnerability Using Only Vulnerability Description,"Software vulnerabilities pose significant security risks to the host computing system. Faced with continuous disclosure of software vulnerabilities, system administrators must prioritize their efforts, triaging the most critical vulnerabilities to address first. Many vulnerability scoring systems have been proposed, but they all require expert knowledge to determine intricate vulnerability metrics. In this paper, we propose a deep learning approach to predict multi-class severity level of software vulnerability using only vulnerability description. Compared with intricate vulnerability metrics, vulnerability description is the ""surface level"" information about how a vulnerability works. To exploit vulnerability description for predicting vulnerability severity, discriminative features of vulnerability description have to be defined. This is a challenging task due to the diversity of software vulnerabilities and the richness of vulnerability descriptions. Instead of relying on manual feature engineering, our approach uses word embeddings and a one-layer shallow Convolutional Neural Network (CNN) to automatically capture discriminative word and sentence features of vulnerability descriptions for predicting vulnerability severity. We exploit large amounts of vulnerability data from the Common Vulnerabilities and Exposures (CVE) database to train and test our approach.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094415,no
707,Learning to Prioritize Test Programs for Compiler Testing,"Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985706,yes
708,On a Pursuit for Perfecting an Undergraduate Requirements Engineering Course,"Requirements Engineering (RE) is an essential component of any software development cycle. Understanding and satisfying stakeholder needs and wants is the difference between the success and failure of a product. However, RE is often perceived as a ""soft"" skill by our students and is often ignored by students who prioritize the learning of coding, testing, and algorithmic thinking. This view contrasts with the industry, where ""soft"" skills are instead valued equal to any other engineering ability. A key challenge in teaching RE is that students who are accustomed to technical work have a hard time relating to something that is non-technical. Furthermore, students are rarely afforded the opportunity to practice requirements elicitation and management skills in a meaningful way while learning the RE concepts as an adjunct to other content. At Rose-Hulman, we have experimented with several project-based approaches to teaching RE, which have evolved over time. In this paper, we document the progress of our teaching methodologies, capture the pros and cons of these varied approaches, and reflect on what worked and what did not in teaching RE to undergraduate engineering students.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8166688,no
709,Performance analysis of OSPF and hybrid networks,"Software Defined Network (SDN) for large-scale IP provider network is an open issue and different solutions were proposed. However, the hybrid IP networks in which both distributed and centralized approach provide centralization of SDN and reliability of distributed networks. The common approach in which SDN controls the prioritized traffic and OSPF (Open Shortest Path First) guarantees the operation of traffic. In this research, we propose the SDN segregation, which maintain central management over dispersed routing control. A given topology is split in some fields with OpenFlow enabled switches as in between nodes. OSPF enabled router triggered updates to other routers in other field via SDN switches. The centralized controller defines how two OSPF routers observe each other. There will be a tradeoff between central control of SDN and fault tolerance capability of OSPF. As we increase SDN nodes control will increase and fault tolerance capacity of overall network decreases. The novelty of research work for balanced topology segregation also offers the models for network management. To show the enhancement provided by hybrid network over routing protocol deployment we have deployed separate test beds for routing protocol and proposed hybrid network.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8250022,no
710,Petri net based software testing scheduling and selecting,Computer software system has a profound impact on human society. It increasingly highlights the importance of software testing. Reducing the cost and improving the efficiency of software testing has an important practical significance and economic value. This paper investigates on software testing workflow from the perspective of discrete event dynamic systems and presents a method to improve the efficiency of software testing by optimizing task scheduling and execution priorities. We developed a simulation program of task scheduling based on Petri net to compare the performance of each scheduling option in different situations and made the analysis of their differences.,2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000086,no
711,Predicting Fault-Prone Classes in Object-Oriented Software: An Adaptation of an Unsupervised Hybrid SOM Algorithm,"Many fault-proneness prediction models have been proposed in literature to identify fault-prone code in software systems. Most of the approaches use fault data history and supervised learning algorithms to build these models. However, since fault data history is not always available, some approaches also suggest using semi-supervised or unsupervised fault-proneness prediction models. The HySOM model, proposed in literature, uses function-level source code metrics to predict fault-prone functions in software systems, without using any fault data. In this paper, we adapt the HySOM approach for object-oriented software systems to predict fault-prone code at class-level granularity using object-oriented source code metrics. This adaptation makes it easier to prioritize the efforts of the testing team as unit tests are often written for classes in object-oriented software systems, and not for methods. Our adaptation also generalizes one main element of the HySOM model, which is the calculation of the source code metrics threshold values. We conducted an empirical study using 12 public datasets. Results show that the adaptation of the HySOM model for class-level fault-proneness prediction improves the consistency and the performance of the model. We additionally compared the performance of the adapted model to supervised approaches based on the Naive Bayes Network, ANN and Random Forest algorithms.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009935,no
712,PV-OWL ƒ?? Pharmacovigilance surveillance through semantic web-based platform for continuous and integrated monitoring of drug-related adverse effects in open data sources and social media,"The recent EU regulation on Pharmacovigilance [Regulation (EU) 1235/2010, Directive 2010/84/EU] imposes both to Pharmaceutical companies and Public health agencies to maintain updated safety information of drugs, monitoring all available data sources. Here, we present our project aiming to develop a web platform for continuous monitoring of adverse effects of medicines (pharmacovigilance), by integrating information from public databases, scientific literature and social media. The project will start by scanning all available data sources concerning drug adverse events, both open (e.g., FAERS - FDA Adverse Event Reporting Systems, medical literature, social media, etc.) and proprietary data (e.g., discharge hospital records, drug prescription archives, electronic health records), that require agreement with respective data owners. Subsequent, pharmacovigilance experts will perform a semi-automatic mapping of codes identifying drugs and adverse events, to build the thesaurus of the web based platform. After these preliminary activities, signal generation and prioritization will be the core of the project. This task will result in risk confidence scores for each included data source and a comprehensive global score, indicating the possible association between a specific drug and an adverse event. The software framework MOMIS, an open source data integration system, will allow semi-automatic virtual integration of heterogeneous and distributed data sources. A web platform, based on MOMIS, able to merge many heterogeneous data sets concerning adverse events will be developed. The platform will be tested by external specialized subjects (clinical researchers, public or private employees in pharmacovigilance field). The project will provide a) an innovative way to link, for the first time in Italy, different databases to obtain novel safety indicators; b) a web platform for a fast and easy integration of all available data, useful to verify and validate hypothesis generated in signal detection. Finally, the development of the unified safety indicator (global risk score) will result in a compelling, easy-to-understand, visual format for a broad range of professional and not professional users like patients, regulatory authorities, clinicians, lawyers, human scientists.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8065931,no
713,QoS-based routing over software defined networks,"Quality of Service (QoS) relies on the shaping of preferential delivery services for applications in favour of ensuring sufficient bandwidth, controlling latency and reducing packet loss. QoS can be achieved by prioritizing important broadband data traffic over the less important one. Thus, depending on the users' needs, video, voice or data traffic take different priority based on the prevalent importance within a particular context. This prioritization might require changes in the configuration of each network entity which can be difficult in traditional network architecture. To this extent, this paper investigates the use of a QoS-based routing scheme over a Software Defined Network (SDN). A real SDN test-bed is constructed using Raspberry Pi computers as virtual SDN switches managed by a centralized controller. It is shown that a QoS-based routing approach over SDN generates enormous control possibilities and enables automation.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986239,no
714,Ranking Modules for Integrate Testing Based on PageRank Algorithm,"The testing industry need to prioritize the limited resources and focus on testing modules whose failure is mostly likely to cause faults. This paper discusses a method that can rank modules in a software package for integrate testing using the PageRank algorithm. In this algorithm, a sequences of random walks iteratively can find a high likelihood of encountering a node, which is interpreted as it being an important performance resource. An experiment result prove that the proposed method actually can be used to prioritize testing of specific modules when testing resource are scarce.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8055343,no
715,Regression Testing Goals - View of Practitioners and Researchers,"Context: Regression testing is a well-researched area. However, the majority regression testing techniques proposed by the researchers are not getting the attention of the practitioners. Communication gaps between industry and academia, and disparity in the regression testing goals are the main reasons. Close collaboration can help in bridging the communication gaps and resolving the disparities. Objective: The study aims at exploring the views of academics and practitioners about the goals of regression testing. The purpose is to investigate the commonalities and differences in their viewpoints and defining some common goals for the success of regression testing. Method: We conducted a focus group study, with 7 testing experts from industry and academia. 4 testing practitioners from 2 companies and 3 researchers from 2 universities participated in the study. We followed GQM approach, to elicit the regression testing goals, information needs, and measures. Results: 43 regression testing goals were identified by the participants, which were reduced to 10 on the basis of similarity among the identified goals. Later during the priority assignment process, 5 goals were discarded, because the priority assigned to these goals was very low. Participants identified 47 information needs/questions required to evaluate the success of regression testing with reference to goal G5 (confidence). Which were then reduced to 10 on the basis of similarity. Finally, we identified measures to gauge those information needs/questions, which were corresponding to the goal (G5). Conclusions: We observed that participation level of practitioners and researchers during the elicitation of goals and questions was same. We found a certain level of agreement between the participants regarding the regression testing definitions and goals. But there was some level of disagreement regarding the priorities of the goals. We also identified the need to implement a regression testing evaluation framework in the participating companies.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8312521,no
716,Regression Testing of Database Applications Under an Incremental Software Development Setting,"Software regression testing verifies previous features on a software product when it is modified or new features are added to it. Because of the nature of regression testing it is a costly process. Different approaches have been proposed to reduce the costs of this activity, among which are: minimization, prioritization, and selection of test cases. Recently, soft computing techniques, such as data mining, machine learning, and others have been used to make regression testing more efficient and effective. Currently, in different contexts, to a greater or lesser extent, software products have access to databases (DBs). Given this situation, it is necessary to consider regression testing also for software products such as information systems that are usually integrated with or connected to DBs. In this paper, we present a selection regression testing approach that utilizes a combination of unsupervised clustering with random values, unit tests, and the DB schema to determine the test cases related to modifications or new features added to software products connected to DBs. Our proposed approach is empirically evaluated with two database software applications in a production context. Effectiveness metrics, such as test suite reduction, fault detection capability, recall, precision, and the F-measure are examined. Our results suggest that the proposed approach is enough effective with the resulting clusters of test cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8027014,no
717,Requirement dependencies-based formal approach for test case prioritization in regression testing,"Regression testing is the testing activity performed after changes occurred on software. Its aim is to increase confidence that achieved software adjustments have no negative impact on the already functional parts of the software. Test case prioritization is one technique that could be applied in regression testing with the aim to find faults early, resulting in reduced cost and shorten time of testing activities. Thus, prioritizing in the context of regression testing means to re-order test cases such that high priority ones are run first. The current paper addresses the test case prioritization as a consistent part of a larger approach on regression testing, which combines both test case prioritization and test case selection in order to overcome the limitations of each of them. A comprehensive formalization of test case prioritization is provided, incorporating beside the well known ingredients (test case, test requirement, fault, cost) also elements relating to the functional requirements and dependencies between requirements. An evolutionary algorithm is used to construct the re-ordering of test cases, considering as optimization objectives fault detection and cost. A synthetic case study was used to empirically prove our perspective for test case prioritization approach.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117002,yes
718,Requirement paramerisation of flap actuation system: Product life-cycle management processes &amp; tools,"Flap actuation systems (FAS) have numerous operational states, modes and environmental constraints, which translate to thousands of requirements associated with them. FAS must be analyzed and tested to determine compliance with requirements in any operating conditions. In this paper, examples of parametrized requirements for centrally driven FAS are presented and the advantages brought by a requirement parametrization process in the systems engineering life- cycle discussed, from requirements elicitation to requirements validation and verification. The challenges of robust re-use and customization of components within legacy systems to drive cost reduction is significant as often legacy products were not developed within a model base framework. The requirements prioritization process provides a low cost, high impact example of alternative to ensure integrity and full traceability of requirements without the need to implement complex simulation platforms.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8088317,no
719,Risk-based attack surface approximation: how much data is enough?,"Proactive security reviews and test efforts are a necessary component of the software development lifecycle. Resource limitations often preclude reviewing the entire code base. Making informed decisions on what code to review can improve a team's ability to find and remove vulnerabilities. Risk-based attack surface approximation (RASA) is a technique that uses crash dump stack traces to predict what code may contain exploitable vulnerabilities. The goal of this research is to help software development teams prioritize security efforts by the efficient development of a risk-based attack surface approximation. We explore the use of RASA using Mozilla Firefox and Microsoft Windows stack traces from crash dumps. We create RASA at the file level for Firefox, in which the 15.8% of the files that were part of the approximation contained 73.6% of the vulnerabilities seen for the product. We also explore the effect of random sampling of crashes on the approximation, as it may be impractical for organizations to store and process every crash received. We find that 10-fold random sampling of crashes at a rate of 10% resulted in 3% less vulnerabilities identified than using the entire set of stack traces for Mozilla Firefox. Sampling crashes in Windows 8.1 at a rate of 40% resulted in insignificant differences in vulnerability and file coverage as compared to a rate of 100%.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965451,no
720,Studies on open source real time operating systems: For vehicle suspension control,"Studying the TIMELINESS of PERIODIC tasks of real time operating systems available in open source, to determine the reliability and efficiency of the systems for implementing them in suspension control of ground vehicles. We take several operating systems which are open source and are available to the public through GNU license (ex: Linux). The Kernels, which are the building blocks of the operating systems which connect hardware and software, are patched using selective real time patches which makes the operating system real time. This is done in order to unlock real time capabilities such as unbounded latencies and real time priorities. These operating systems are tested for their efficiency and timeliness. The results are compared with the test logs of non-real time operating system. These kernels are then cross compiled and built for ARM architecture. This kernel is then applied to an embedded system which is then tested using the same afore mentioned tests. The results are logged and analyzed.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070787,no
721,Table of contents,The following topics are dealt with: software design; mutation testing; collective online testing; collective offline testing; usability tests; Web services; delta-oriented programming; delta-oriented product prioritization and feature models.,2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968031,no
722,Test Case Generation and Prioritization: A Process-Mining Approach,"Test cases are an essential tool in software quality assurance: they ensure that code behaves as specified in the requirement. However, writing test cases does not have only benefits, it comes with a cost: the programmer has to formulate the test cases and maintain them when the tested source code changes. Particularly for start-ups or small enterprises such costs become prohibitive, which often prefer to invest their time into the development of new functionalities instead of testing. This paper explores the use of process-mining as an approach to create a model of how users interact with a system to a) generate test cases and b) prioritize them. Using process-mining, it is possible to mine from the user behaviour which parts of the system are the most used, in which order they are executed, generate test cases repeating user input, and prioritizing test cases.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899028,yes
723,Test Optimization from Release Insights: An Analytical Hierarchy Approach,"Software Testing is an essential aspect to ensure software quality, reliability and consistent user experience. Digital applications such as mobile app usually follow rapid software delivery which consists of various releases. It typically uses insights from the development data such as defects, test logs for test execution optimization. Once the application is released and deployed, there is rich availability of untapped heterogeneous data which can also be effectively utilized for the next release test execution optimization. The data from the release includes direct customer feedback, application monitoring data such as user behavioral traces, device usages, release logs. In this position paper, we discuss about the various data sources and the multiple insights which can be derived from them. We also propose a framework which uses Analytical Hierarchy Process to prioritize the tests based on these insights available from the release data. The framework also recommends the prioritized and missed device configurations for next release test planning.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967947,yes
724,Test Prioritization with Optimally Balanced Configuration Coverage,"Testing configurable software for high assurancesystems developed in continuous integration requires effectivetechniques for selecting failure-inducing test cases, thoroughlycovering entire configuration space, while providing rapid feedbackon failures. This involves satisfying multiple objectives:maximizing test fault detection, maximizing test coverage ofthe configuration space, and minimizing test execution time, which often leads to compromises in practice. In this paper, weaddress this problem with a practical test optimization approachthat uses historical test data to determine an optimal order oftests ensuring high progressively uniform configuration coverage, early fault detection, and rapid test feedback. We extensivelyvalidate the approach in a set of experiments using industry testsuites, and report experimental results showing the improvementin efficiency compared to industry practice. In particular, theapproach showed to increase the uniformity of configurationcoverage by 39% on average, which increases fault detectionup to 15%, while just slightly delaying test feedback.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911878,yes
725,Test Suite Prioritization for Efficient Regression Testing of Model-Based Automotive Software,"Up to 80% of the automotive software can be generated from models. MATLAB Simulink is a common tool for creation of complex combinations of block diagrams and state machines, automated generation of executable code, and its deployment on a target ECU. The automotive safety standards require extensive testing of the developed models. Regression testing should be undertaken every time a model is updated to ensure that the modifications do not introduce new faults into the previously validated model. A common, time-consuming way is to rerun an entire test suite after even minor changes. This paper introduces a new method for automatic prioritization of test cases. The method is based on two principles: (i) A test case should stimulate an error in an updated block and (ii) the stimulated error should propagate to the place where it can be detected. The proposed method includes the evaluation of input vectors that are provided to updated blocks by each test case and a Markov-based stochastic error propagation analysis of the model. The application of the method is demonstrated with a Simulink model of a gearbox and a test-suite, automatically generated with the Reactis Tester.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8118517,yes
726,The organization of arrangements set to ensure enterprise IPV6 network secure work by modern switching equipment tools (using the example of a network attack on a default gateway),"The article issue is the enterprise information protection within the internet of things concept. The aim of research is to develop arrangements set to ensure secure enterprise IPv6 network operating. The object of research is the enterprise IPv6 network. The subject of research is modern switching equipment as a tool to ensure network protection. The research task is to prioritize functioning of switches in production and corporation enterprise networks, to develop a network host protection algorithm, to test the developed algorithm on the Cisco Packet Tracer 7 software emulator. The result of research is the proposed approach to IPv6-network security based on analysis of modern switches functionality, developed and tested enterprise network host protection algorithm under IPv6-protocol with an automated network SLAAC-configuration control, a set of arrangements for resisting default enterprise gateway attacks, using ACL, VLAN, SEND, RA Guard security technology, which allows creating sufficiently high level of networks security.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8239505,no
727,The Significant Effects of Data Sampling Approaches on Software Defect Prioritization and Classification,"Context: Recent studies have shown that performance of defect prediction models can be affected when data sampling approaches are applied to imbalanced training data for building defect prediction models. However, the magnitude (degree and power) of the effect of these sampling methods on the classification and prioritization performances of defect prediction models is still unknown. Goal: To investigate the statistical and practical significance of using resampled data for constructing defect prediction models. Method: We examine the practical effects of six data sampling methods on performances of five defect prediction models. The prediction performances of the models trained on default datasets (no sampling method) are compared with that of the models trained on resampled datasets (application of sampling methods). To decide whether the performance changes are significant or not, robust statistical tests are performed and effect sizes computed. Twenty releases of ten open source projects extracted from the PROMISE repository are considered and evaluated using the AUC, pd, pf and G-mean performance measures. Results: There are statistical significant differences and practical effects on the classification performance (pd, pf and G-mean) between models trained on resampled datasets and those trained on the default datasets. However, sampling methods have no statistical and practical effects on defect prioritization performance (AUC) with small or no effect values obtained from the models trained on the resampled datasets. Conclusions: Existing sampling methods can properly set the threshold between buggy and clean samples, while they cannot improve the prediction of defect-proneness itself. Sampling methods are highly recommended for defect classification purposes when all faulty modules are to be considered for testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170123,yes
728,Thermo-mechanical reliability analysis of flip-chip bonded silicon carbide Schottky diodes,"This paper presents the thermo-mechanical reliability analysis of a novel chip-scale wire bondless packaging technique for a SiC Schottky diode that leads to lower parasitics, higher reliability, lower costs, and lower losses. The proposed approach uses a flip-chip solder ball array to make connections to the anode. A copper connector was used to make contact with the bottom cathode, thus reconfiguring the bare die into a chip-scale, flip-chip capable device. Thermo-mechanical analysis in a finite element software showed that the proposed approach could better manage Coefficient of Thermal Expansion (CTE) mismatch stresses arising at the critical module interfaces as compared with a conventional wire bonded module. A detailed analysis of the flip-chip structure is presented and contrasted with a state-of-the-art wire bonded module. Different design parameters were explored for the drain connector to be able to make an optimized decision. However, keeping production costs low was prioritized without compromising significant performance. The fabrication process for manufacturing a flip-chip schottky diode module was also demonstrated along with preliminary test results to demonstrate functionality.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7936756,no
729,TITAN: Test Suite Optimization for Highly Configurable Software,"Exhaustive testing of highly configurable software developed in continuous integration is rarely feasible in practice due to the configuration space of exponential size on the one hand, and strict time constraints on the other. This entails using selective testing techniques to determine the most failure-inducing test cases, conforming to highly-constrained time budget. These challenges have been well recognized by researchers, such that many different techniques have been proposed. In practice, however, there is a lack of efficient tools able to reduce high testing effort, without compromising software quality. In this paper we propose a test suite optimization technology TITAN, which increases the time-and cost-efficiency of testing highly configurable software developed in continuous integration. The technology implements practical test prioritization and minimization techniques, and provides test traceability and visualization for improving the quality of testing. We present the TITAN tool and discuss a set of methodological and technological challenges we have faced during TITAN development. We evaluate TITAN in testing of Cisco's highly configurable software with frequent high quality releases, and demonstrate the benefit of the approach in such a complex industry domain.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7928010,no
730,Towards Execution Time Prediction for Manual Test Cases from Test Specification,"Knowing the execution time of test cases is important to perform test scheduling, prioritization and progress monitoring. This work in progress paper presents a novel approach for predicting the execution time of test cases based on test specifications and available historical data on previously executed test cases. Our approach works by extracting timing information (measured and maximum execution time)for various steps in manual test cases. This information is then used to estimate the maximum time for test steps that have not previously been executed, but for which textual specifications exist. As part of our approach, natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test activities is already available or not. Finally, linear regression is used to predict the actual execution time for test cases. A proof-of-concept use case at Bombardier Transportation serves to evaluate the proposed approach.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8051381,no
731,Towards the design of a secure and compliant framework for OpenEMR,"The purpose of this research is to explore and identify the vulnerabilities in OpenEMR 5.0.0, which is a free and open source medical practice management application. We are to provide recommendations/suggestions to OpenEMR developers on identifying the vulnerabilities. We chose to use vulnerabilities scanning tools to manually explore the demo site of OpenEMR 5.0.0. The targeted vulnerabilities belong to the following three types, namely, SQL Injection, Cross-Site Scripting (XSS) including persistent XSS and reflected XSS and Arbitrary File Upload. We have inducted a qualitative based risk assessment to determine the risk levels for the vulnerabilities identified. The results of risk assessment include two kinds of risk levels, which are high risk and medium risk, and two kinds of priorities, which are priority 1 (high) and priority 2 (medium). In addition, we provided recommendations and best practices about how to prevent the identified vulnerabilities. Furthermore, the research also presents an exploit automation program written in Python to test and exploit the vulnerabilities including SQL Injection and reflected XSS on the demo server of OpenEMR.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8217792,no
732,Trends on empty exception handlers for Java open source libraries,"Exception-handling structures provide a means to recover from unexpected or undesired flows that occur during software execution, allowing the developer to put the program in a valid state. Still, the application of proper exception-handling strategies is at the bottom of priorities for a great number of developers. Studies have already discussed this subject pinpointing that, frequently, the implementation of exception-handling mechanisms is enforced by compilers. As a consequence, several anti-patterns about Exception-handling are already identified in literature. In this study, we have picked several releases from different Java programs and we investigated one of the most well-known anti-patterns: the empty catch handlers. We have analysed how the empty handlers evolved through several releases of a software product. We have observed some common approaches in terms of empty catches' evolution. For instance, often an empty catch is transformed into a empty catch with a comment. Moreover, for the majority of the programs, the percentage of empty handlers has decreased when comparing the first and last releases. Future work includes the automation of the analysis allowing the inclusion of data collected from other software artefacts: test suites and data from issue tracking systems.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884644,no
733,Value-Based Decision-Making Using a Web-Based Tool: A Multiple Case Study,"[Context]: To remain competitive, innovative and to grow, companies should use a value-based decision-making where decisions are the best for that company's overall value creation. However, without tool support, the use of explicit value propositions and aggregation of different key stakeholders' decisions during decision-making may be a challenge for many companies. [Goal]: The goal of this paper is to investigate the extent to which a Web-based tool for value-based decision-making can successfully support stakeholders' decision-making process. [Method]: We conducted three case studies across four software projects, during six weeks, in the contexts of feature selection, test cases execution prioritization and user interfaces design selection. Prior to using the tool, stakeholders' value propositions were elicited via focus-group meetings; later, during a post-mortem phase, data was gathered via observation, semi-structured interviews and structured questionnaires. [Results]: Participants reported an improvement of their decision-making process and quality of decisions; further, they also felt confident about using the tool, and that it can be useful to their work. [Conclusions]: Results suggested that the use of tool support by the stakeholders in the investigated company for value-based decision-making improved their decision-making process and the quality of decisions.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305950,no
734,Weighting for Combinatorial Testing by Bayesian Inference,"Combinatorial testing (CT) is a widely-used technique to detect system interaction failures. To improve the test effectiveness of CT, prioritized combinatorial testing inputs priority weights of parameter-values, and generates combinatorial test suites based on the weights. This paper proposes a method to automatically determine the weights of parameter-values by Bayesian inference using previous testing results. Using two open source projects, we evaluate the fault detection effectiveness of the proposed weighting based prioritized combinatorial testing.",2017,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899090,no
735,A Model-Based Test Case Management Approach for Integrated Sets of Domain-Specific Models,"Due to rapid improvements in the area of embedded processing hardware, the complexity of developed systems constantly increases. In order to ensure a high quality level of such systems, related quality assurance concepts have to evolve. The introduction of Model-Based Testing (MBT) approaches has shown promising results by automating and abstracting multiple activities of the software testing life cycle. Nevertheless, there is a strong need for approaches supporting scoped test models, i.e. subsets of test cases, reflecting specific test purposes driven by risk-oriented development strategies. Therefore, we developed an integrated and model-based approach supporting test case management, which incorporates the beneficial aspects of abstract development methodologies with predominant research for test case management in non-model-based scenarios. Based on a new model artifact, the integration model, tasks like cross-domain information mapping and the integration of domain-specific KPIs derived by analyses favor the subsequently applied constraint-based mechanism for test case management. Further, a prototypical implementation of these concepts within the Architecture And Analysis Framework (A3F) is elaborated and further evaluated based on representative application scenarios. A comparative view on related work leads to a conclusive statement regarding our future work.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411751,no
736,Aggregation process for implementation of application security management based on risk assessment,This article is devoted to the review and analysis of existing methods of ensuring information security based on risk models. The strengths and weaknesses of the model are investigated on the basis of reliability theory. The article discusses potential obstacle to managing application security effectively and describes five steps for managing security. Create inventory of application and their attributes and evaluating their role in business impact (Create a profile for each application and conduction analysis of date processed in the application). Software vulnerability search (Static Analysis (ƒ??white-boxƒ?); Dynamic Analysis (ƒ??black-boxƒ?); Interactive Analysis (ƒ??glass-boxƒ?); Mobile Application Analysis); Risk assessment and prioritization of vulnerabilities (Setting priorities for applications; Setting priorities for types of vulnerabilities; Setting priorities for the development team; Changing vulnerability priorities and reassessing risks). Elimination of vulnerabilities and minimization of risks (security manager sets priorities and firmed tasks for the development team.,2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8317039,yes
737,An Empirical Comparison of Fixed-Strength and Mixed-Strength for Interaction Coverage Based Prioritization,"Test case prioritization (TCP) plays an important role in identifying, characterizing, diagnosing, and correcting faults quickly. The TCP has been widely used to order test cases of different types, including model inputs (also called<italic>abstract test cases</italic>). Model inputs are constructed by modeling the program according to its input parameters, values, and constraints, and has been used in different testing methods, such as combinatorial interaction testing and software product line testing. The<italic>Interaction coverage-based TCP</italic>(ICTCP) uses interaction coverage information derived from the model input to order inputs. Previous studies have focused generally on the<italic>fixed-strength</italic>ICTCP, which adopts a fixed strength (<italic>i.e.</italic>, the level of parameter interactions) to support the ICTCP process. It is generally accepted that using more strengths for ICTCP,<italic>i.e.</italic>,<italic>mixed-strength</italic>ICTCP, may give better ordering than fixed-strength. To confirm whether mixed-strength is better than fixed-strength, in this paper, we report on an extensive empirical study using five real-world programs (written in C), each of which has six versions. The results of the empirical studies show that mixed-strength has better rates of interaction coverage overall than fixed-strength, but they have very similar rates of fault detection. Our results also show that fixed-strength should be used instead of the mixed-strength at the later stage of software testing. Finally, we offer some practical guidelines for testers when using interaction coverage information to prioritize model inputs, under different testing scenarios and resources.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8523673,yes
738,Assessing Technical Debt in Automated Tests with CodeScene,"Test automation promises several advantages such as shorter lead times, higher code quality, and an executable documentation of the system's behavior. However, test automation won't deliver on those promises unless the quality of the automated test code itself is maintained, and to manually inspect the evolution of thousands of tests that change on a daily basis is impractical at best. This paper investigates how CodeScene - a tool for predictive analyses and visualizations - could be used to identify technical debt in automated test code. CodeScene combines repository mining, static code analysis, and machine learning to prioritize potential code improvements based on the most likely return on investment.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411742,no
739,Assessing Test Case Prioritization on Real Faults and Mutants,"Test Case Prioritization (TCP) is an important component of regression testing, allowing for earlier detection of faults or helping to reduce testing time and cost. While several TCP approaches exist in the research literature, a growing number of studies have evaluated them against synthetic software defects, called mutants. Hence, it is currently unclear to what extent TCP performance on mutants would be representative of the performance achieved on real faults. To answer this fundamental question, we conduct the first empirical study comparing the performance of TCP techniques applied to both real-world and mutation faults. The context of our study includes eight well-studied TCP approaches, 35k+ mutation faults, and 357 real-world faults from five Java systems in the Defects4J dataset. Our results indicate that the relative performance of the studied TCP techniques on mutants may not strongly correlate with performance on real faults, depending upon attributes of the subject programs. This suggests that, in certain contexts, the best performing technique on a set of mutants may not be the best technique in practice when applied to real faults. We also illustrate that these correlations vary for mutants generated by different operators depending on whether chosen operators reflect typical faults of a subject program. This highlights the importance, particularly for TCP, of developing mutation operators tailored for specific program domains.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530033,yes
740,BP: Profiling Vulnerabilities on the Attack Surface,"Security practitioners use the attack surface of software systems to prioritize areas of systems to test and analyze. To date, approaches for predicting which code artifacts are vulnerable have utilized a binary classification of code as vulnerable or not vulnerable. To better understand the strengths and weaknesses of vulnerability prediction approaches, vulnerability datasets with classification and severity data are needed. The goal of this paper is to help researchers and practitioners make security effort prioritization decisions by evaluating which classifications and severities of vulnerabilities are on an attack surface approximated using crash dump stack traces. In this work, we use crash dump stack traces to approximate the attack surface of Mozilla Firefox. We then generate a dataset of 271 vulnerable files in Firefox, classified using the Common Weakness Enumeration (CWE) system. We use these files as an oracle for the evaluation of the attack surface generated using crash data. In the Firefox vulnerability dataset, 14 different classifications of vulnerabilities appeared at least once. In our study, 85.3% of vulnerable files were on the attack surface generated using crash data. We found no difference between the severity of vulnerabilities found on the attack surface generated using crash data and vulnerabilities not occurring on the attack surface. Additionally, we discuss lessons learned during the development of this vulnerability dataset.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543394,yes
741,Characterizing Defective Configuration Scripts Used for Continuous Deployment,"In software engineering, validation and verification (V&V) resources are limited and characterization of defective software source files can help in efficiently allocating V&V resources. Similar to software source files, defects occur in the scripts used to automatically manage configurations and software deployment infrastructure, often known as infrastructure as code (IaC) scripts. Defects in IaC scripts can have dire consequences, for example, creating large-scale system outages. Identifying the characteristics of defective IaC scripts can help in mitigating these defects by allocating V&V efforts efficiently based upon these characteristics. The objective of this paper is to help software practitioners to prioritize validation and verification efforts for infrastructure as code (IaC) scripts by identifying the characteristics of defective IaC scripts. Researchers have previously extracted text features to characterize defective software source files written in general purpose programming languages. We investigate if text features can be used to identify properties that characterize defective IaC scripts. We use two text mining techniques to extract text features from IaC scripts: the bag-of-words technique, and the term frequency-inverse document frequency (TF-IDF) technique. Using the extracted features and applying grounded theory, we characterize defective IaC scripts. We also use the text features to build defect prediction models with tuned statistical learners. We mine open source repositories from Mozilla, Openstack, and Wikimedia Commons, to construct three case studies and evaluate our methodology. We identify three properties that characterize defective IaC scripts: filesystem operations, infrastructure provisioning, and managing user accounts. Using the bag-of-word technique, we observe a median F-Measure of 0.74, 0.71, and 0.73, respectively, for Mozilla, Openstack, and Wikimedia Commons. Using the TF-IDF technique, we observe a median F-Measure of 0.72, 0.74, and 0.70, respectively, for Mozilla, Openstack, and Wikimedia Commons.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367034,no
742,Clustering Based Prioritization of Test Cases,"Regression testing is the procedure of retesting the product and checking whether additional faults or errors have been created in the existing one. It is vital for keeping up programming quality. But it is a costly process. By., utilizing prioritization technique cost can be diminished. Prioritization increases productiveness of regression testing and its main criteria is to build the rate of error detection. Merging requirements information into current testing practice helps the engineers to recognize the source of faults easily. In this paper a research is done on whether the requirements-based grouping methodology can enhance the viability of prioritization techniques. So., here a grouping approach is performed on given requirements and prioritization techniques based on code scope metric.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8473253,yes
743,Component Selection in Software Engineering - Which Attributes are the Most Important in the Decision Process?,"Component-based software engineering is a common approach to develop and evolve contemporary software systems where different component sourcing options are available: 1)Software developed internally (in-house), 2)Software developed outsourced, 3)Commercial of the shelf software, and 4) Open Source Software. However, there is little available research on what attributes of a component are the most important ones when selecting new components. The object of the present study is to investigate what matters the most to industry practitioners during component selection. We conducted a cross-domain anonymous survey with industry practitioners involved in component selection. First, the practitioners selected the most important attributes from a list. Next, they prioritized their selection using the Hundred-Dollar ($100) test. We analyzed the results using Compositional Data Analysis. The descriptive results showed that Cost was clearly considered the most important attribute during the component selection. Other important attributes for the practitioners were: Support of the component, Longevity prediction, and Level of off-the-shelf fit to product. Next, an exploratory analysis was conducted based on the practitioners' inherent characteristics. Nonparametric tests and biplots were used. It seems that smaller organizations and more immature products focus on different attributes than bigger organizations and mature products which focus more on Cost.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8498206,no
744,Context-Aware Patch Generation for Better Automated Program Repair,"The effectiveness of search-based automated program repair is limited in the number of correct patches that can be successfully generated. There are two causes of such limitation. First, the search space does not contain the correct patch. Second, the search space is huge and therefore the correct patch cannot be generated (ie correct patches are either generated after incorrect plausible ones or not generated within the time budget). To increase the likelihood of including the correct patches in the search space, we propose to work at a fine granularity in terms of AST nodes. This, however, will further enlarge the search space, increasing the challenge to find the correct patches. We address the challenge by devising a strategy to prioritize the candidate patches based on their likelihood of being correct. Specifically, we study the use of AST nodes' context information to estimate the likelihood. In this paper, we propose CapGen, a context-aware patch generation technique. The novelty which allows CapGen to produce more correct patches lies in three aspects: (1) The fine-granularity design enables it to find more correct fixing ingredients; (2) The context-aware prioritization of mutation operators enables it to constrain the search space; (3) Three context-aware models enable it to rank correct patches at high positions before incorrect plausible ones. We evaluate CapGen on Defects4J and compare it with the state-of-the-art program repair techniques. Our evaluation shows that CapGen outperforms and complements existing techniques. CapGen achieves a high precision of 84.00% and can prioritize the correct patches before 98.78% of the incorrect plausible ones.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453055,yes
745,DevOps Improvements for Reduced Cycle Times with Integrated Test Optimizations for Continuous Integration,"DevOps, as a growing development practice that aims to enable faster development and efficient deployment of applications without compromising on quality, is often hampered by long cycle times. One contributing factor to long cycle times in DevOps is long build time. Automated testing in continuous integration is one of the build stages that is highly prone to long run-time due to software complexity and evolution, and inefficient due to unoptimized testing approaches. To be cost-effective, testing in continuous integration needs to use only a fast-running set of comprehensive tests that are able to ensure the level of quality needed for deployment to production. Known approaches use time-aware test selection methods to improve time-efficiency of continuous integration testing by providing optimized combinations and order of tests with respect to decreased run-time. However, focusing on time-efficiency as the sole criterion in DevOps often jeopardizes the quality of software deliveries. This paper proposes a technique that integrates fault-based and risk-based test selection and prioritization optimized for low run-time, to improve time-effectiveness of continuous integration testing, and thus reduce long cycle times in DevOps, without compromising on quality. The technique has been evaluated in testing of a large-scale configurable software in continuous integration, and has shown considerable improvement over industry practice with respect to time-efficiency.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377636,no
746,Dynamic programming optimization algorithm applied in test case selection,"In this paper we propose a quadratic dynamic programming algorithm applied in software testing domain, more specific in the test case selection decision making. We addressed a specific problem in software testing: running a subset of test cases from the whole set of available test cases in a limited time frame with the goal of maximizing the chances of finding potential defects. We employed both objective methods as the dynamic programming algorithm and subjective and empiric human decision as defining the selection and prioritization criteria. The proposed solution is suited for medium to large projects where in the worst-case scenarios the memory space complexity of the proposed algorithm does not exceed the order of GBytes. The proposed optimization algorithm is presented in pseudocode along with the dynamic programming recurrence formula and potential selection criteria as currently used in the industry.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8583984,no
747,Dynamic Random Testing Strategy for Test Case Optimization in Cloud Environment,"Dynamic Random Testing (DRT) strategy employs feedback mechanism to guide the selection of test cases, which has shown to be effective in fault detection process. Cloud testing is the combination of cloud computing and software testing, in which the parallel mechanism is introduced to handle multiple test tasks simultaneously. The efficiency of DRT can be improved by combining it into cloud environment. However, it faces challenges in cloud testing as its test cases are selected sequentially, which does not consist with the characteristic of parallelism underlying cloud testing. In this paper, we present a cloud-based DRT strategy to adapt DRT in cloud testing, in which both the test case prioritization and resource allocation are considered. The results of the experiments show that the cloud-based DRT can improve the efficiency of original DRT and provide stable fault detection performance enhancement over other testing strategies.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539185,yes
748,Embedded Platform for Gas Applications Using Hardware/Software Co-Design and RFID,"This paper presents the development of a wireless low power reconfigurable self-calibrated multi-sensing platform for gas sensing applications. The proposed electronic nose (EN) system monitors gas temperatures, concentrations, and mixtures wirelessly using the radio-frequency identification (RFID) technology. The EN takes the form of a set of gas and temperature sensors and multiple pattern recognition algorithms implemented on the Zynq system on chip (SoC) platform. The gas and temperature sensors are integrated on a semi-passive RFID tag to reduce the consumed power. Various gas sensors are tested, including an in-house fabricated 4??4 SnO<sub>2</sub>based sensor and seven commercial Figaro sensors. The data is transmitted to the Zynq based processing unit using a RFID reader, where it is processed using multiple pattern recognition algorithms for dimensionality reduction and classification. Multiple algorithms are explored for optimum performance, including principal component analysis (PCA) and linear discriminant analysis (LDA) for dimensionality reduction while decision tree (DT) and k-nearest neighbors (KNN) are assessed for classification purpose. Different gases are targeted at diverse concentration, including carbon monoxide (CO), ethanol (C<sub>2</sub>H<sub>6</sub>O), carbon dioxide (CO<sub>2</sub>), propane (C<sub>3</sub>H<sub>8</sub>), ammonia (NH<sub>3</sub>), and hydrogen (H<sub>2</sub>). An accuracy of 100% is achieved in many cases with an overall accuracy above 90% in most scenarios. Finally, the hardware/software heterogeneous solution to implementation PCA, LDA, DT, and KNN on the Zynq SoC shows promising results in terms of resources usage, power consumption, and processing time.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330010,no
749,FAST Approaches to Scalable Similarity-Based Test Case Prioritization,"Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in efficiency with no significant impact on effectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453081,yes
750,"Feature-Based Testing by Using Model Synthesis, Test Generation and Parameterizable Test Prioritization","An approach for feature-based testing in efficient test processes, especially for use in agile development, is presented. Methods of model synthesis, model-based test generation, as well as coverage-based and requirement-based test prioritization are linked together in order to systematically and efficiently obtain prioritized test cases. The result is a reordered test suite promising quick feedback for the test engineer during test execution. The process is highly parameterizable in regard to the selection of features to be tested and the optimization criteria for the test prioritization. Using an example from industrial automation, the results of the work are demonstrated.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411744,yes
751,Functional Dependency Detection for Integration Test Cases,"This paper presents a natural language processing (NLP) based approach that, given software requirements specification, allows the functional dependency detection between integration test cases. We analyze a set of internal signals to the implemented modules for detecting dependencies between requirements and thereby identifying dependencies between test cases such that: module 2 depends on module 1 if an output internal signal from module 1 enters as an input internal signal to the module 2. Consequently, all requirements (and thereby test cases) for module 2 are dependent on all the designed requirements (and test cases) for module 1. The dependency information between requirements (and thus corresponding test cases) can be utilized for test case prioritization and scheduling. We have implemented our approach as a tool and the feasibility is evaluated through an industrial use case in the railway domain at Bombardier Transportation (BT), Sweden.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8431975,no
752,Graphite: A Greedy Graph-Based Technique for Regression Test Case Prioritization,"To date, various test prioritization techniques have been developed, but the majority of these techniques consider a single objective that could limit the applicability of prioritization techniques by ignoring practical constraints imposed on regression testing. Multi-objective prioritization techniques try to reorder test cases so that they can optimize multiple goals that testers want to achieve. In this paper, we introduced a novel graph-based framework that maps the prioritization task to a graph traversal algorithm. To evaluate our approach, we performed an empirical study using 20 versions of four open source applications. Our results indicate that the use of the graph-based technique can improve the effectiveness and efficiency of test case prioritization technique.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539203,yes
753,How Do Software Metrics Affect Test Case Prioritization?,"In this paper we consider a statistical method to prioritize software test cases with operational profile, where the system behavior is described by a Markov reward model. Especially, we introduce software code metrics as reward parameters and apply the resulting Markov reward model to the test case prioritization problem, where our research question is set as how software code metrics affect the test case prioritization. In a numerical example with a real application software, we embed some seeding faults in advance and carry out 1,000 random test experiments. It is shown that our metrics-based test case prioritization can reduce a large amount of testing effort efficiently.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377662,yes
754,Identifying Technical Debt in Database Normalization Using Association Rule Mining,"In previous work, we explored a new context of technical debt that relates to database normalization design decisions. We claimed that database normalization debts are likely to be incurred for tables below the fourth normal form. We proposed a method to prioritize the tables that should be normalized based on their impact on data quality and performance. In this study, we propose a framework to identify normalization debt items (i.e. tables below the fourth normal form) by mining the data stored in each table. Our framework makes use of association rule mining to discover functional dependencies between attributes in a table, which will help determine the current normal form of that table and reveal debt tables. To illustrate our method, we use a case study from Microsoft, AdventureWorks database. The results revealed the applicability of our framework to identify debt tables.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8498244,yes
755,Industrially Applicable System Regression Test Prioritization in Production Automation,"When changes are performed on an automated production system (aPS), new faults can be accidentally introduced into the system, which are called regressions. A common method for finding these faults is regression testing. In most cases, this regression testing process is performed under high time pressure and onsite in a very uncomfortable environment. Until now, there has been no automated support for finding and prioritizing system test cases regarding the fully integrated aPS that are suitable for finding regressions. Thus, the testing technician has to rely on personal intuition and experience, possibly choosing an inappropriate order of test cases, finding regressions at a very late stage of the test run. Using a suitable prioritization, this iterative process of finding and fixing regressions can be streamlined and a lot of time can be saved by executing test cases likely to identify new regressions earlier. Thus, an approach is presented in this paper that uses previously acquired runtime data from past test executions and performs a change identification and impact analysis to prioritize test cases that have a high probability to unveil regressions caused by side effects of a system change. The approach was developed in cooperation with reputable industrial partners active in the field of aPS engineering, ensuring a development in line with industrial requirements. An industrial case study and an expert evaluation were performed, showing promising results.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320514,yes
756,Influencers of Quality Assurance in an Open Source Community,"ROS (Robot Operating System) is an open source community in robotics that is developing standard robotics operating system facilities such as hardware abstraction, low-level device control, communication middleware, and a wide range of software components for robotics functionality. This paper studies the quality assurance practices of the ROS community. We use qualitative methods to understand how ideology, priorities of the community, culture, sustainability, complexity, and adaptability of the community affect the implementation of quality assurance practices. Our analysis suggests that software engineering practices require social and cultural alignment and adaptation to the community particularities to achieve seamless implementation in open source environments. This alignment should be incorporated into the design and implementation of quality assurance practices in open source communities.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445538,no
757,Integrating Weight Assignment Strategies With NSGA-II for Supporting User Preference Multiobjective Optimization,"Driven by the needs of several industrial projects on the applications of multiobjective search algorithms, we observed that user preferences must be properly incorporated into optimization objectives. However, existing algorithms usually treat all the objectives with equal priorities and do not provide a mechanism to reflect user preferences. To address this, we propose an extension-user-preference multiobjective optimization algorithm (UPMOA), to the most commonly applied, nondominated sorting genetic algorithm II by introducing a user preference indicator ??, based on existing weight assignment strategies [e.g., uniformly distributed weights (UDW)]. We empirically evaluated UPMOA using four industrial problems from three diverse domains (i.e., communication, maritime, and subsea oil and gas). We also performed a sensitivity analysis for UPMOA with 625 algorithm parameter settings. To further assess the performance and scalability, 103 500 artificial problems were created and evaluated representing 207 sets of user preferences. Results show that the UDW strategy with UPMOA achieves the best performance and UPMOA significantly outperformed other three multiobjective search algorithms, and has the ability to solve problems with a wide range of complexity. We also observed that different parameter settings led to the varied performance of UPMOA, thus suggesting that configuring proper parameters is highly problem-specific.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8123878,no
758,Investigating NLP-Based Approaches for Predicting Manual Test Case Failure,"System-level manual acceptance testing is one of the most expensive testing activities. In manual testing, typically, a human tester is given an instruction to follow on the software. The results as ""passed"" or ""failed"" will be recorded by the tester, according to the instructions. Since this is a labourintensive task, any attempt in reducing the amount of this type of expensive testing is essential, in practice. Unfortunately, most of the existing heuristics for reducing test executions (e.g., test selection, prioritization, and reduction) are either based on source code or specification of the software under test, which are typically not being accessed during manual acceptance testing. In this paper, we propose a test case failure prediction approach for manual testing that can be used as a noncode/ specifcation-based heuristic for test selection, prioritization, and reduction. The approach uses basic Information Retrieval (IR) methods on the test case descriptions, written in natural language. The IR-based measure is based on the frequency of terms in the manual test scripts. We show that a simple linear regression model using the extracted natural language/IR-based feature together with a typical history-based feature (previous test execution results) can accurately predict the test cases' failure in new releases. We have conducted an extensive empirical study on manual test suites of 41 releases of Mozilla Firefox over three projects (Mobile, Tablet, Desktop). Our comparison of several proposed approaches for predicting failure shows that a) we can accurately predict the test case failure and b) the NLP-based feature can improve the prediction models.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367058,no
759,Learning to Accelerate Compiler Testing,"Compilers are one of the most important software infrastructures. Compiler testing is an effective and widely-used way to assure the quality of compilers. While many compiler testing techniques have been proposed to detect compiler bugs, these techniques still suffer from the serious efficiency problem. This is because these techniques need to run a large number of randomly generated test programs on the fly through automated test-generation tools (e.g., Csmith). To accelerate compiler testing, it is desirable to schedule the execution order of the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. Since different test programs tend to trigger the same compiler bug, the ideal goal of accelerating compiler testing is to execute the test programs triggering different compiler bugs in the beginning. However, such perfect goal is hard to achieve, and thus in this work, we design four steps to approach the ideal goal through learning, in order to largely accelerate compiler testing.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449630,yes
760,MDroid+: A Mutation Testing Framework for Android,"Mutation testing has shown great promise in assessing the effectiveness of test suites while exhibiting additional applications to test-case generation, selection, and prioritization. Traditional mutation testing typically utilizes a set of simple language specific source code transformations, called operators, to introduce faults. However, empirical studies have shown that for mutation testing to be most effective, these simple operators must be augmented with operators specific to the domain of the software under test. One challenging software domain for the application of mutation testing is that of mobile apps. While mobile devices and accompanying apps have become a mainstay of modern computing, the frameworks and patterns utilized in their development make testing and verification particularly difficult. As a step toward helping to measure and ensure the effectiveness of mobile testing practices, we introduce MDroid+, an automated framework for mutation testing of Android apps. MDroid+ includes 38 mutation operators from ten empirically derived types of Android faults and has been applied to generate over 8,000 mutants for more than 50 apps.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449438,no
761,Methods and Tools for Focusing and Prioritizing the Testing Effort,"Software testing is essential for any software development process, representing an extremely expensive activity. Despite its importance recent studies showed that developers rarely test their application and most programming sessions end without any test execution. Indeed, new methods and tools able to better allocating the developers effort are needed to increment the system reliability and to reduce the testing costs. In this work we focus on three activities able to optimize testing activities, specifically, bug prediction, test case prioritization, and energy leaks detection. Indeed, despite the effort devoted in the last decades by the research community led to interesting results, we highlight some aspects that might be improved and propose empirical investigations and novel approaches. Finally, we provide a set of open issues that should be addressed by the research community in the future.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530092,yes
762,MS-guided many-objective evolutionary optimisation for test suite minimisation,"Test suite minimisation is a process that seeks to identify and then eliminate the obsolete or redundant test cases from the test suite. It is a trade-off between cost and other value criteria and is appropriate to be described as a many-objective optimisation problem. This study introduces a mutation score (MS)-guided many-objective optimisation approach, which prioritises the fault detection ability of test cases and takes MS, cost and three standard code coverage criteria as objectives for the test suite minimisation process. They use six classical evolutionary many-objective optimisation algorithms to identify efficient test suite, and select three small programs from the Software-Artefact Infrastructure Repository (SIR) and two larger program space and gzip for experimental evaluation as well as statistical analysis. The experiment results of the three small programs show non-dominated sorting genetic algorithm II (NSGA-II) with tuning was the most effective approach. However, MOEA/D-PBI and MOEA/D-WS outperform NSGA-II in the cases of two large programs. On the other hand, the test cost of the optimal test suite obtained by their proposed MS-guided many-objective optimisation approach is much lower than the one without it in most situation for both small programs and large programs.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8572623,no
763,On the Selection of Strength for Fixed-Strength Interaction Coverage Based Prioritization,"Abstract test cases are derived by modeling the system under test, and have been widely applied in practice, such as for software product line testing and combinatorial testing. Abstract test case prioritization (ATCP) is used to prioritize abstract test cases and aims at achieving higher rates of fault detection. Many ATCP algorithms have been proposed, using different prioritization criteria and information. One ATCP approach makes use of fixed-strength level-combinations information covered by abstract test cases, and is called fixed-strength interaction coverage based prioritization (FICBP). Before using FICBP, the prioritization strength ?¯ needs to be decided. Previous studies have generally focused on ?¯ values ranging between 1 and 6. However, no study has investigated the appropriateness of such a range, nor how to assign the prioritization strength for FICBP. To answer these questions, this paper reports on an empirical study involving four real-life programs (each of which with six versions). The experimental results indicate that ?¯ should be set approximately equal to a value corresponding to half of the number of parameters, when testing resources are sufficient. Our results also show that when testing resources are limited or insufficient, either small or large ?¯ values are suggested for FICBP.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377673,yes
764,On the Suitability of a Portfolio-Based Design Improvement Approach,"The design debt metaphor tries to illustrate quality deficits in the design of a software and the impact thereof to the business value of the system. To pay off the debt, the literature offers various approaches for identifying and prioritizing these design flaws, but without proper support in aligning strategic improvement actions to the identified issues. This work addresses this challenge and examines the suitability of our proposed portfolio-based design assessment approach. Therefore, this investigation is conducted based on three case studies where the product source code was analyzed and assessed using our portfolio-based approach. As a result, the approach has proven to be able to recommend concrete and valuable design improvement actions that can be adapted to project constraints.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424976,no
765,Poster: An Experimental Analysis of Fault Detection Capabilities of Covering Array Constructors,"Combinatorial Interaction Testing (CIT) aims at constructing an effective test suite, such as a Covering Array (CA), that can detect faults that are caused by the interaction of parameters. In this paper, we report on some empirical studies conducted to examine the fault detection capabilities of five popular CA constructors: ACTS, Jenny, PICT, CASA, and TCA. The experimental results indicate that Jenny has the best performance, because it achieves better fault detection than the other four constructors in many cases. Our results also indicate that CAs generated using ACTS, PICT, or CASA should be prioritized before testing.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449522,no
766,Poster: Identification of Methods with Low Fault Risk,"Test resources are usually limited and therefore it is often not possible to completely test an application before a release. Therefore, testers need to focus their activities on the relevant code regions. In this paper, we introduce an inverse defect prediction approach to identify methods that contain hardly any faults. We applied our approach to six Java open-source projects and show that on average 31.6% of the methods of a project have a low fault risk; they contain in total, on average, only 5.8% of all faults. Furthermore, the results suggest that, unlike defect prediction, our approach can also be applied in cross-project prediction scenarios. Therefore, inverse defect prediction can help prioritize untested code areas and guide testers to increase the fault detection probability.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449591,no
767,Practical Test Dependency Detection,"Regression tests should consistently produce the same outcome when executed against the same version of the system under test. Recent studies, however, show a different picture: in many cases simply changing the order in which tests execute is enough to produce different test outcomes. These studies also identify the presence of dependencies between tests as one likely cause of this behavior. Test dependencies affect the quality of tests and of the correlated development activities, like regression test selection, prioritization, and parallelization, which assume that tests are independent. Therefore, developers must promptly identify and resolve problematic test dependencies. This paper presents PRADET, a novel approach for detecting problematic dependencies that is both effective and efficient. PRADET uses a systematic, data-driven process to detect problematic test dependencies significantly faster and more precisely than prior work. PRADET scales to analyze large projects with thousands of tests that existing tools cannot analyze in reasonable amount of time, and found 27 previously unknown dependencies.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367031,no
768,Prioritization of Metamorphic Relations Based on Test Case Execution Properties,"A test oracle is essential for software testing. In certain complex systems, it is hard to distinguish between correct and incorrect behavior. Metamorphic testing is one of the solution to solve the test oracle problem. In metamorphic testing, metamorphic relations (MRs) are derived based on the properties exhibited by the program under test (PUT). These MRs play a major role in the generation of test data for conducting MT. The effectiveness of MRs can be determined based on the ability to detect considerable faults for the given PUT. Many metamorphic relations with different fault finding capability can be used to test the PUT and it is important to identify and prioritize the MRs based on its fault finding effectiveness. In order to answer this challenge, we propose to prioritize the MRs based on the diversity in the execution path of the source and follow-up test cases of the MRs. We propose four metrics to capture different levels of diversity in the execution behavior of the test cases for each of the derived MRs. The total weight calculated for each of the MRs using the metrics is used to prioritize the MRs.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539189,yes
769,"Prioritizing Alerts from Multiple Static Analysis Tools, Using Classification Models","Static analysis (SA) tools examine code for flaws without executing the code, and produce warnings (""alerts"") about possible flaws. A human auditor then evaluates the validity of the purported code flaws. The effort required to manually audit all alerts and repair all confirmed code flaws is often too much for a project's budget and schedule. An alert triaging tool enables strategically prioritizing alerts for examination, and could use classifier confidence. We developed and tested classification models that predict if static analysis alerts are true or false positives, using a novel combination of multiple static analysis tools, features from the alerts, alert fusion, code base metrics, and archived audit determinations. We developed classifiers using a partition of the data, then evaluated the performance of the classifier using standard measurements, including specificity, sensitivity, and accuracy. Test results and overall data analysis show accurate classifiers were developed, and specifically using multiple SA tools increased classifier accuracy, but labeled data for many types of flaws were inadequately represented (if at all) in the archive data, resulting in poor predictive accuracy for many of those flaws.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445036,no
770,Prioritizing Browser Environments for Web Application Test Execution,"When testing client-side web applications, it is important to consider different web-browser environments. Different properties of these environments such as web-browser types and underlying platforms may cause a web application to exhibit different types of failures. As web applications evolve, they must be regression tested across these different environments. Because there are many environments to consider this process can be expensive, resulting in delayed feedback about failures in applications. In this work, we propose six techniques for providing a developer with faster feedback on failures when regression testing web applications across different web-browser environments. Our techniques draw on methods used in test case prioritization; however, in our case we prioritize web-browser environments, based on information on recent and frequent failures. We evaluated our approach using four non-trivial and popular open-source web applications. Our results show that our techniques outperform two baseline methods, namely, no ordering and random ordering, in terms of the cost-effectiveness. The improvement rates ranged from -12.24% to 39.05% for no ordering, and from -0.04% to 45.85% for random ordering.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453107,yes
771,Redefining Prioritization: Continuous Prioritization for Continuous Integration,"Continuous integration (CI) development environments allow soft-ware engineers to frequently integrate and test their code. While CI environments provide advantages, they also utilize non-trivial amounts of time and resources. To address this issue, researchers have adapted techniques for test case prioritization (TCP) to CI environments. To date, however, the techniques considered have operated on test suites, and have not achieved substantial improvements. Moreover, they can be inappropriate to apply when system build costs are high. In this work we explore an alternative: prioritization of commits. We use a lightweight approach based on test suite failure and execution history that is highly efficient; our approach ""continuously"" prioritizes commits that are waiting for execution in response to the arrival of each new commit and the completion of each previously scheduled commit. We have evaluated our approach on three non-trivial CI data sets. Our results show that our approach can be more effective than prior techniques.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453137,yes
772,Region Priority Based Adaptive 360-Degree Video Streaming Using DASH,"With the continuous improvement of Virtual Reality (VR) hardware and software facilities and development of VR streaming platform, the VR industry ushered in a period of rapid development. VR makes use of 360-degree panoramic or omnidirectional video with high resolution and high frame rate in order to create the immersive experience to the user. However those characteristics cause the big volume of 360-degree video and bandwidth intensive during transmission. Due to the limitation of the human eye vision, the user can only watch the part of the 360-degree video in a head-mounted display (HMD) at one time. Hence, streaming the VR video by the traditional video transmission method causes bandwidth waste. In view of this, this paper proposed a 360-degree video adaptive transmission method based on user viewport. Firstly, a 360-degree video region prioritization scheme is proposed based on user vision characteristics and head motion features. Different priority regions transmit 360-degree video content with different quality levels as the format of tiles. Then, based on the bandwidth estimation, buffer status and user viewport prediction, a 360-degree video adaptive transmission decision strategy is given. According to the predicted available bandwidth and viewport, the quality combination of the 360-degree video tiles to be transmitted in the near future is determined to implement adaptive transmission. The test results of the 360-degree video transmission experiment system based on the DASH standard show that the proposed 360-degree video adaptive transmission strategy can effectively reduce the bandwidth consumption on the basis of guaranteeing the user Quality of Experience(QoE).",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455396,no
773,REMAP: Using Rule Mining and Multi-objective Search for Dynamic Test Case Prioritization,"Test case prioritization (TP) prioritizes test cases into an optimal order for achieving specific criteria (e.g., higher fault detection capability) as early as possible. However, the existing TP techniques usually only produce a static test case order before the execution without taking runtime test case execution results into account. In this paper, we propose an approach for black-box dynamic TP using rule mining and multi-objective search (named as REMAP). REMAP has three key components: 1) Rule Miner, which mines execution relations among test cases from historical execution data; 2) Static Prioritizer, which defines two objectives (i.e., fault detection capability (FDC) and test case reliance score (TRS)) and applies multi-objective search to prioritize test cases statically; and 3) Dynamic Executor and Prioritizer, which executes statically-prioritized test cases and dynamically updates the test case order based on the runtime test case execution results. We empirically evaluated REMAP with random search, greedy based on FDC, greedy based on FDC and TRS, static search-based prioritization, and rule-based prioritization using two industrial and three open source case studies. Results showed that REMAP significantly outperformed the other approaches for 96% of the case studies and managed to achieve on average 18% higher Average Percentage of Faults Detected (APFD).",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367035,yes
774,Requirements Analysis Skills: How to Train Practitioners?,"One of the goals of any software development organization (SDO) is the assurance of a high quality software. To achieve this, it is important to perform all the software related activities, especially those of the requirements engineering (RE) phase, in the right way and ideally by experts. However, the current practice reveals that this crucial phase is commonly performed by people with limited experience in RE, indeed, some of them ignoring the basic activities. We present a training plan in order to improve practitioners' RE analysis skills. The training plan was applied to 44 practitioners working at a Mexican SDO. We developed such a plan based on the idea of considering six main dimensions that include theory, tests and mentoring sessions. The so called dimensions are: understanding the organization's domain, basic concepts of RE, requirements elicitation, requirements expression, requirements prioritization and requirements analysis. In this paper, we present what are the topics discussed in each dimension, the feedback received by the practitioners after the training and how we envision the evolution of the training plan.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8501281,no
775,SDN-Based Architecture for Providing QoS to High Performance Distributed Applications,"The specification of quality of service (QoS) requirements in traditional networks is limited by the high administrative cost of these environments. Nevertheless, newer network paradigms, as software-defined networks (SDNs), simplify and relaxes the management of networks. In this sense, SDN can provide a simple/effective way to develop QoS provisioning. In this paper, we propose a QoS provision architecture exploiting the capabilities of SDN. Our approach allows the specification of classes of service and also negotiates the QoS requirements between applications and the SDN network controller. The SDN controller, in turn, monitors the network and adjusts its performance through resource reservation and traffic prioritization. We developed a proof-of-concept of our proposal and, our experimental results show that the additional routines present low overhead, whereas -for a given test application- we observe a reduction of up to 47% in transfer times.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538694,no
776,Search-Based Optimization for the Testing Resource Allocation Problem: Research Trends and Opportunities,"This paper explores the usage of search-based techniques for the Testing Resource Allocation Problem (TRAP). We focus on the analysis of the literature, surveying the research proposals where search-based techniques are exploited for different formulations of the TRAP. Three dimensions are considered: the model formulation, solution, and validation. The analysis allows to derive several observations, and finally outline some new research directions towards better (namely, closer to real-world settings) modelling and solutions, highlighting the most promising areas of investigation.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452803,no
777,Test Case Prioritization Based on Method Call Sequences,"Test case prioritization is widely used in testing with the purpose of detecting faults as early as possible. Most existing techniques exploit coverage to prioritize test cases based on the hypothesis that a test case with higher coverage is more likely to catch bugs. Statement coverage and function coverage are the two widely used coverage granularity. The former typically achieves better test case prioritization in terms of fault detection capability, while the latter is more efficient because it incurs less overhead. In this paper we argue that static information such as statement and function coverage may not be the best criteria for guiding dynamic executions. Executions that cover the same set of statements /functions can may exhibit very different behavior. Therefore, the abstraction that reduces program behavior to statement/function coverage can be too simplistic to predicate fault detection capability. We propose a new approach that exploits function call sequences to prioritize test cases. This is based on the observation that the function call sequences rather than the set of executed functions is a better indicator of program behavior. Test cases that reveal unique function call sequences may have better chance to encounter faults. We choose function instead of statement sequences due to the consideration of efficiency. We have developed and implemented a new prioritization strategy AGC (Additional Greedy method Call sequence), that exploit function call sequences. We compare AGC against existing test case prioritization techniques on eight real-world open source Java projects. Our experiments show that our approach outperforms existing techniques on large programs (but not on small programs) in terms of bug detection capability. The performance shows a growth trend when the size of program increases.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377663,yes
778,Test Case Prioritization for GUI Regression Testing Based on Centrality Measures,"Regression testing has been widely used in GUI software testing. For the reason of economy, the prioritization of test cases is particularly important. However, few studies discussed test case prioritization (TCP) for GUI software. Based on GUI software features, a two-layer model is proposed to assist the test case prioritization in this paper, in which, the outer layer is an event handler tree (EHT), and the inner layer is a function call graph (FCG). Compared with the conventional methods, more source code information is used based on the two-layer model for prioritization. What is more, from a global perspective, centrality measure, a complex network viewpoint is used to highlight the importance of modified functions for specific version TCP. The experiment proved the effectiveness of this model and this method.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377903,yes
779,Test Re-Prioritization in Continuous Testing Environments,"New changes are constantly and concurrently being made to large software systems. In modern continuous integration and deployment environments, each change requires a set of tests to be run. This volume of tests leads to multiple test requests being made simultaneously, which warrant prioritization of such requests. Previous work on test prioritization schedules queued tests at set time intervals. However, after a test has been scheduled it will never be reprioritized even if new higher risk tests arrive. Furthermore, as each test finishes, new information is available which could be used to reprioritize tests. In this work, we use the conditional failure probability among tests to reprioritize tests after each test run. This means that tests can be reprioritized hundreds of times as they wait to be run. Our approach is scalable because we do not depend on static analysis or coverage measures and simply prioritize tests based on their co-failure probability distributions. We named this approach CODYNAQ and in particular, we propose three prioritization variants called CODYNAQSINGLE, CODYNAQDOUBLE and CODYNAQFLEXI. We evaluate our approach on two data sets, CHROME and Google testing data. We find that our co-failure dynamic re-prioritization approach, CODYNAQ, outperforms the default order, FIFOBASELINE, finding the first failure and all failures for a change request by 31% and 62% faster, respectively. CODYNAQ also outperforms GOOGLETCP by finding the first failure 27% faster and all failures 62% faster.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530018,yes
780,Using Controlled Numbers of Real Faults and Mutants to Empirically Evaluate Coverage-Based Test Case Prioritization,"Used to establish confidence in the correctness of evolving software, regression testing is an important, yet costly, task. Test case prioritization enables the rapid detection of faults during regression testing by reordering the test suite so that effective tests are run as early as is possible. However, a distinct lack of information about the regression faults found in complex real-world software forced prior experimental studies of these methods to use artificial faults called mutants. Using the Defects4J database of real faults, this paper presents the results of experiments evaluating the effectiveness of four representative test prioritization techniques. Since this paper's results show that prioritization is susceptible to high amounts of variance when only one fault is present, our experiments also control the number of real faults and mutants in the program subject to regression testing. Our overall findings are that, in comparison to mutants, real faults are harder for reordered test suites to quickly detect, suggesting that mutants are not a surrogate for real faults.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8536351,yes
781,Using Mutant Stubbornness to Create Minimal and Prioritized Test Sets,"In testing, engineers want to run the most useful tests early (prioritization). When tests are run hundreds or thousands of times, minimizing a test set can result in significant savings (minimization). This paper proposes a new analysis technique to address both the minimal test set and the test case prioritization problems. This paper precisely defines the concept of mutant stubbornness, which is the basis for our analysis technique. We empirically compare our technique with other test case minimization and prioritization techniques in terms of the size of the minimized test sets and how quickly mutants are killed. We used seven C language subjects from the Siemens Repository, specifically the test sets and the killing matrices from a previous study. We used 30 different orders for each set and ran every technique 100 times over each set. Results show that our analysis technique performed significantly better than prior techniques for creating minimal test sets and was able to establish new bounds for all cases. Also, our analysis technique killed mutants as fast or faster than prior techniques. These results indicate that our mutant stubbornness technique constructs test sets that are both minimal in size, and prioritized effectively, as well or better than other techniques.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424996,yes
782,Varying defect prediction approaches during project evolution: A preliminary investigation,"Defect prediction approaches use various features of software product or process to prioritize testing, analysis and general quality assurance activities. Such approaches require the availability of project's historical data, making them inapplicable in early phase. To cope with this problem, researchers have proposed cross-project and even cross-company prediction models, which use training material from other projects to build the model. Despite such advances, there is limited knowledge of how, as the project evolves, it would be convenient to still keep using data from other projects, and when, instead, it might become convenient to switch towards a local prediction model. This paper empirically investigates, using historical data from four open source projects, on how the performance of various kinds of defect prediction approaches - within-project prediction, local and global cross-project prediction, and mixed (injected local cross) prediction - varies over time. Results of the study are part of a long-term investigation towards supporting the customization of defect prediction models over projects' history.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368451,no
783,Virtual Test Method for Complex and Variant-Rich Automotive Systems,"The fast development of embedded automotive systems in form of connected Electronic Control Units (ECUs) has led to complex development processes. Especially for safetycritical functions, the testing activities are essential to check if the designed system complies with the requirements. Nowadays, the continuous development of mobile electronic devices through software updates is performed almost on a daily basis. This trend is now starting to be observed in cyber-physical systems with higher safety priorities. In the automotive field, the rising software portion in the vehicles and the shortening technology life-cycles are accentuating the need for Software Over The Air (SOTA) updates. Despite the opportunities offered by SOTA updates, the current test processes and methods must be adapted to manage the resulting complexity throughout the life-cycle of the vehicles. Especially the typical variants abundance in automotive product lines is considered as an important challenge, which cannot be solved only by ƒ?classicalƒ? testing methods such as Hardware-In-the-Loop. In this paper, we present a testing method for variantrich systems, which can be applied for automotive software updates. It uses virtual platforms for automated delta testing to handle the abundance of system configurations. Virtual testing is introduced as a powerful tool to reduce the amount of real tests and allow efficient variants verification. As a proof of concept, an Adaptive Cruise Control (ACC) composed of two ECUs has been implemented both in real hardware and using a virtual platform. With this approach, virtual delta tests, i. e. specific test-benches targeting the differences to a basic variant, can be rapidly executed for various system configurations. To prove the feasibility of the presented test method in more complex systems, a scalability study has been conducted.",2018,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519599,no