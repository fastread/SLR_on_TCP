ID,Document Title,Abstract,Year,PDF Link,code
301,A Workflow Scheduling Algorithm for Optimizing Energy-Efficient Grid Resources Usage,"Grid computing represents the main solution to integrate distributed and heterogeneous resources in global scale. However, the infrastructure necessary for maintaining a global grid in production is huge. Such fact has led to excessive power consumption. On the other hand, most green strategies for data centers are DVS (Dynamic Voltage Scaling)-based and become difficult to implement them in global grids. This paper proposes the HGreen heuristic (Heavier Tasks on Maximum Green Resource) and defines a workflow scheduling algorithm in order to implement it on global grids. HGreen algorithm aims to prioritize energy-efficient resources and explores workflow application profiles. Simulation results have shown that the proposed algorithm can significantly reduce the power consumption in global grids.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6119053,no
302,Adaptive Regression Testing Strategy: An Empirical Study,"When software systems evolve, different amounts and types of code modifications can be involved in different versions. These factors can affect the costs and benefits of regression testing techniques in different ways, and thus, there may be no single regression testing technique that is the most cost-effective technique to use on every version. To date, many regression testing techniques have been proposed, but no research has been done on the problem of helping practitioners systematically choose appropriate techniques on new versions as systems evolve. To address this problem, we propose adaptive regression testing (ART) strategies that attempt to identify the regression testing techniques that will be the most cost-effective for each regression testing session considering organization's situations and testing environment. To assess our approach, we conducted an experiment focusing on test case prioritization techniques. Our results show that prioritization techniques selected by our approach can be more cost-effective than those used by the control approaches.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132961,yes
303,Agent-Based Test Case Prioritization,In this paper an Adaptive Test Management System (ATMS) based on software agents is presented which prioritizes test cases considering available information from test teams and from developments teams about the software system and the test cases. The goal of the ATMS is to increase the number of found faults in the available test time with the determined prioritization order.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954404,yes
304,An Empirical Study on the Relation between Dependency Neighborhoods and Failures,"Changing source code in large software systems is complex and requires a good understanding of dependencies between software components. Modification to components with little regard to dependencies may have an adverse impact on the quality of the latter, i.e., increase their risk to fail. We conduct an empirical study to understand the relationship between the quality of components and the characteristics of their dependencies such as their frequency of change, their complexity, number of past failures and the like. Our study has been conducted on two large software systems: Microsoft VISTA and ECLIPSE. Our results show that components that have outgoing dependencies to components with higher object-oriented complexity tend to have fewer field failures for VISTA, but the opposite relation holds for ECLIPSE. Likewise, other notable observations have been made through our study that (a) confirm that certain characteristics of components increase the risk of their dependencies to fail and (b) some of the characteristics are project specific while some were also found to be common. We expect that such results can be leveraged for use to provide new directions for research in defect prediction, test prioritization and related research fields that utilize code dependencies in their empirical analysis. Additionally, these results provide insights to engineers on the potential reliability impacts of new component dependencies based upon the characteristics of the component.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770624,yes
305,An empirical validation of FindBugs issues related to defects,"Background: Effective use of bug finding tools promise to speed up the process of source code verification and to move a portion of discovered defects from testing to coding phase. However, many problems related to their usage, especially the large number of false positives, could easily hinder the potential benefits of such tools. Aims: Assess the percentage and type of issues of a popular bugfinding tool (FindBugs) that are actual defects. Method: We analyzed 301 Java Projects developed at a university with FindBugs, collecting the issues signalled on the source code. Afterwards, we checked the precision of issues with information on changes, we ranked and validated them using both manual inspection and validation with tests failures. Results: We observed that a limited set of issues have high precision and conversely we identified those issues characterized by low precision. We compared findings first with our previous experiment and then to related work: results are consistent with both of them. Conclusions: Since our and other empirical studies demonstrated that few issues are related to real defects with high precision, developers could enable only them (or prioritize), reducing the information overload of FindBugs and having the possibility to discover defects earlier. Furthermore, the technique presented in the paper can be adopted to other tools on a code base with tests to find issues with high precision that can be checked on code in production to find defects earlier.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083173,yes
306,An Improved Metric for Test Case Prioritization,"Test case prioritization is an effective and practical technique of regression testing. To illustrate its effectiveness, many test metrics were proposed. In this paper, the physical meanings of these metrics were explained and their limitations were pointed out. Then, an improved metric and its extension for test case prioritization were proposed. The case study indicates that, compared with existing metrics, our new metric can provide much more precise illustration of the effectiveness of test case prioritization techniques.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093578,yes
307,Black box test case prioritization techniques for semantic based composite web services using OWL-S,"Web services are the basic building blocks for the business which is different from web applications. Testing of web services is difficult and increases the cost due to the unavailability of source code. Researchers have, web services are tested based on the syntactic structure using Web Service Description Language (WSDL) for atomic web services. This paper proposes an automated testing framework for composite web services based on semantics where the domain knowledge of the web services is described using prot??g?? tool and the behaviour of the entire business operation flow for the composite web service is described by Ontology Web Language for services (OWL-S). Prioritization of test cases is performed based on various coverage criteria for composite web services. Series of experiments were conducted to assess the effectiveness of prioritization and empirical results shown that prioritization techniques perform well in detecting faults compared to traditional techniques.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972354,yes
308,Calculating Prioritized Interaction Test Sets with Constraints Using Binary Decision Diagrams,"Combinatorial interaction testing has become an established technique to systematically determine test sets for highly-configurable software systems. The generation of minimal test sets that fullfill the demanded coverage criteria is an NP-complete problem. Constraint handling and integrated test case prioritization, features necessary for practical use, further complicate the problem. We present a novel algorithm that exploits our observation that the combinatorial interaction testing problem with constraints can be modelled as a single propositional logic formula. Our test set calculation algorithm uses binary decision diagrams as efficient data structure for this formula. The algorithm supports constraints and prioritization. Our evaluation results prove its cost effectiveness. For many benchmark problems the algorithm calculates the best results compared to other greedy approaches.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954420,yes
309,Challenges in Audit Testing of Web Services,"Audit testing of Web services is a form of regression testing that applies to the integration of external services in a service composition. When such services change, the application that integrates them needs to be regressiontested, to ensure that the existing functionalities are not broken by the service changes. This means the external services must be audited for preserved compatibility with the integration. In this paper, we analyze the main research challenges behind audit testing of services. Such challenges descend from the limited testability of external services and encompass traditional regression testing problems, such as test case selection and prioritization, for which existing solutions are nonetheless inadequate. Moreover, in the paper we discuss in detail one such challenge, i.e., the problem of defining an oracle by making the integrator's assumptions explicit.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954397,no
310,"Challenges, benefits and opportunities in operating cabled ocean observatories: Perspectives from NEPTUNE Canada","The advent of the first cabled ocean observatories, with several others being planned, demonstrates the challenges, benefits and opportunities for ocean science and commercial applications. Examples are drawn primarily from NEPTUNE Canada (NC), which completed installation of the subsea infrastructure and 60 diverse instruments in 2009, with 40 more in 2010, thereby establishing the world's first regional cabled ocean observatory, northeast Pacific Ocean, off British Columbia's coast. Initial data flow started in December 2009. Another 30 instruments will be deployed in 2011-12. Introducing abundant power and high bandwidth communications into a range of ocean environments allows discrimination between short and long-term events, interactive experiments, real time data and imagery, and complex multidisciplinary teams interrogating a vast database over the observatory's 25-year design life. Scientific priorities and observatory node sites were identified through workshops. Alcatel-Lucent Submarine Networks designed, manufactured and installed the 800km backbone cable and five nodes (stepping 10kV DC to 400V DC). Node sites are located at the coast (Folger Passage), continental slope (ODP 889; Barkley Canyon), abyssal plain (ODP 1027), and ocean-spreading ridge (Endeavour), in water depths of 100-2660m. Principal scientific themes are: plate tectonic processes and earthquake dynamics; dynamic processes of seabed fluid fluxes and gas hydrates; regional ocean/climate dynamics and effects on marine biota; deep-sea ecosystem dynamics; and engineering and computational research. The Data Management and Archive System (DMAS) provides controls for the observatory network and transparent access to other data providers using interoperability techniques within a Web 2.0 environment. Users can perform data visualization and analysis on-line with either default or custom processing code, as well as simultaneously interacting with each other. Oceans 2.0 is adding tools to perform software-aided feature detection and classification of sounds in acoustic data streams. New knowledge and scientific interpretations are addressing important science applications of the observatory: ocean/climate change, ocean acidification, recognizing and mitigating natural hazards, non-renewable and renewable natural resources. Challenges are considerable: technical innovations, enlarging the user base, management, funding, maximizing educational/outreach activities. Socio-economic benefits are substantial: not only the transformation of ocean sciences but with many applications in sectors such as sovereignty, security, transportation, data services, and public policy. Opportunities for commercialization of technologies and data services/products are being facilitated by the Centre of Enterprise and Engagement (www.onccee.ca) within Ocean Networks Canada (www.networkscanada.ca) that manages the NC and VENUS observatories (www.neptunecanada.ca; www.uvic.venus.ca). Cabled ocean observatories are transforming the ocean sciences and will result in a progressive wiring of the oceans. They are designed to be expandable in footprint, nodes and instruments, and the range of scientific questions, and to provide facilities for testing technology prototypes. They will provide a wealth of new research opportunities and socio-economic benefits.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5774134,yes
311,Change Sensitivity Based Prioritization for Audit Testing of Webservice Compositions,"Modern software systems have often the form of Web service compositions. They take advantage of the availability of a variety of external Web services to provide rich and complex functionalities, obtained as the integration of external services. However, Web services change at a fast pace and while syntactic changes are easily detected as interface incompatibilities, other more subtle changes are harder to detect and may give raise to faults. They occur when the interface is compatible with the composition, but the semantics of the service response has changed. This typically involves undocumented or implicit aspects of the service interface. Audit testing of services is the process by which the service integrator makes sure that the service composition continues to work properly with the new versions of the integrated services. Audit testing of services is conducted under strict (sometimes extreme) time and budget constraints. Hence, prioritizing the audit test cases so as to execute the most important ones first becomes of fundamental importance. We propose a test case prioritization method specifically tailored for audit testing of services. Our method is based on the idea that the most important test cases are those that have the highest sensitivity to changes injected into the service responses (mutations). In particular, we consider only changes that do not violate the explicit contract with the service (i.e., the WSDL), but may violate the implicit assumptions made by the service integrator.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954434,yes
312,Code Hot Spot: A tool for extraction and analysis of code change history,"Commercial software development teams have limited time available to focus on improvements to their software. These teams need a way to quickly identify areas of the source code that would benefit from improvement, as well as quantifiable data to defend the selected improvements to management. Past research has shown that mining configuration management systems for change information can be useful in determining faulty areas of the code. We present a tool named Code Hot Spot, which mines change records out of Microsoft's TFS configuration management system and creates a report of hot spots. Hot spots are contiguous areas of the code that have higher values of metrics that are indicators of faulty code. We present a study where we use this tool to study projects at ABB to determine areas that need improvement. The resulting data have been used to prioritize areas for additional code reviews and unit testing, as well as identifying change prone areas in need of refactoring.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080806,yes
313,Compiling SyncCharts to Synchronous C,"SyncCharts are a synchronous Statechart variant to model reactive systems with a precise and deterministic semantics. The simulation and software synthesis for SyncCharts usually involve the compilation into Esterel, which is then further compiled into C code. This can produce efficient code, but has two principal drawbacks: 1) the arbitrary control flow that can be expressed with SyncChart transitions cannot be mapped directly to Esterel, and 2) it is very difficult to map the resulting C code back to the original SyncChart, which hampers traceability. This paper presents an alternative software synthesis approach for SyncCharts that compiles SyncCharts directly into Synchronous C (SC). The compilation preserves the structure of the original SyncChart, which is advantageous for validation and possibly certification. We present a static thread-scheduling scheme that reflects data dependencies and optimizes both the number of used threads as well as the maximal used priorities. This results in SC code with competitive speed and little memory requirements.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763284,no
314,Cost Optimizations in Runtime Testing and Diagnosis of Systems of Systems,"In practically all development processes tests are used to detect the presence of faults. This is not an exception for critical and high-availability systems. However, these systems cannot be taken offline or duplicated for testing in some cases. This makes runtime testing necessary. This paper presents work aimed at optimizing the three main sources of testing cost: preparation, execution and diagnosis. First, preparation cost is optimized by defining a metric of the runtime testability of the system, used to elaborate an implementation plan of preparative work for runtime testing. Second, the interrelated nature of test execution cost and diagnostic cost is highlighted and a new diagnostic test prioritization is introduced.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770638,yes
315,"CRANE: Failure Prediction, Change Analysis and Test Prioritization in Practice -- Experiences from Windows","Building large software systems is difficult. Maintaining large systems is equally hard. Making post-release changes requires not only thorough understanding of the architecture of a software component about to be changed but also its dependencies and interactions with other components in the system. Testing such changes in reasonable time and at a reasonable cost is a difficult problem as infinitely many test cases can be executed for any modification. It is important to obtain a risk assessment of impact of such post-release change fixes. Further, testing of such changes is complicated by the fact that they are applicable to hundreds of millions of users, even the smallest mistakes can translate to a very costly failure and re-work. There has been significant amount of research in the software engineering community on failure prediction, change analysis and test prioritization. Unfortunately, there is little evidence on the use of these techniques in day-to-day software development in industry. In this paper, we present our experiences with CRANE: a failure prediction, change risk analysis and test prioritization system at Microsoft Corporation that leverages existing research for the development and maintenance of Windows Vista. We describe the design of CRANE, validation of its useful-ness and effectiveness in practice and our learnings to help enable other organizations to implement similar tools and practices in their environment.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770625,yes
316,Critical component analyzer ƒ?? A novel test prioritization framework for component based real time systems,"Component based software development system is composed of many components and it uses the reusable components as the building blocks for constructing the complex software system. The major challenges in CBS are testing component dependency that is; it is a tricky task to test each and every component for each possible input data which will lead to exhaustive testing. To reduce the cost, the industries are following some stopping criteria and release the product to the customer side. These stopping criteria will at times lead to skipping up of some of the components from rigorous testing. This will lead to hazardous side effects such as loss in terms of revenue, human life and resources. This insight leads to the need to identify critical components which have the higher dependability measure in terms of functionality and receives higher priority in testing with rigorous test procedures. Hence, this paper proposes a novel method for identifying the critical components from the Software under Test (SUT) and prioritizes them for testing with at most care based on various dependency metrics and measures among the components with the help of Component Execution Sequence Graph (CESG).",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140684,yes
317,Critical components identification and verification for effective software test prioritization,"Nowadays, software complexity increases as the number of components in a component based system (CBS) increases. As the complexity level increases, the testing and verification of components also increases. This in turn rose up the testing time and cost which thus made industries to skip off some of the components due to the hard timeline and resource limitations especially during maintenance. This leads to hazardous effects if some of these missed components are critical in term of their core functionality and dependability with other components. Hence, a regression testing which is usually performed during maintenance phase should be developed meticulously to identify and test these critical components rigorously before releasing the software on to the customer side. This paper proposed a novel regression testing method based on the criticality measure calculated by means of dependability metrics and internal complexity metrics. Also, this paper compares the performance of the proposed approach with existing approaches and concluded that the proposed framework outperforms them.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165171,no
318,Dealing with Test Automation Debt at Microsoft,"At Microsoft, substantial time and resources are expended in test case development, execution and verification. Thousands of new tests are added to existing test suites without any kind of review regarding their unique contribution to test suite effectiveness or impact on test suite efficiency. This talk describes how we leverage existing code coverage data, together with reduction and prioritization techniques, to help each test team analyze its test suite and guide them in improving their suite's effectiveness and efficiency. The analysis focuses on identifying and deprecating/prioritizing groups of tests cases, given specific tactical goals for example, increasing current test suite stability and reliability, better structuring of test suite migration efforts, reducing test suite execution time and testing with limited hardware resources.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032226,yes
319,Design of mice maze based on PLC control unit,"In recent years, there are a number of mouse maze devices developed. In this paper, we introduce the Y-maze which is based on the traditional programmable logic controller as a control center to the new Y-maze. In the subject, the maze of automatic control system will be in addition to the sensor controller sub- section also part and actuator part. This paper mainly discusses the new Maze automatic control system, describes the software design process and priorities, hardware selection and layout. Among them, the interface circuit of the programmable controller, I/O port assignments, infrared sensor module and the maze rotation module are described in detail The maze of automatic control system to achieve the experimental data in mice can reduce errors, improve test efficiency and reduce human labor, side by side, in addition to the traditional Y-maze caused by the interference of some unavoidable factors, such as mice, leaving the smell of mice in the experimental results, the reliability of the experiment can be further improved.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6067621,no
320,Design Principles for Integration of Model-Driven Quality Assurance Tools,"The engineering of software systems is supported by tools in different phases of the software development. The integration of these tools is crucial to assure the trace ability of existing models and artifacts, and to support the automation of critical software development phases such as software testing and validation. In particular, the integration of novel software quality assurance tools into existing environments must be performed in a way that minimizes its impact on existing software process, while the benefits of the tool are leveraged. This guarantees the adoption of new methodologies with minimal interference in existing production workflow. In this paper we discuss our experience in integrating a model-driven software testing tool developed within SIEMENS with a widely-adopted model-driven design tool. In particular, we establish a set of design principles from the lessons learned in this integration. We conclude showing a design that prioritizes data integration over control and presentation that achieves a high degree of tool integration while minimizing the integration development effort.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114546,no
321,Designing VM schedulers for embedded real-time applications,"Virtual Machines (VMs) allow for platform-independent software development and their use in embedded systems is increasing. In particular, VMs are rewarding in the context of mixed-criticality applications to provide isolation between critical and non-critical tasks running on the same processor. In this paper, we study the design of a real-time system based on a VM monitor/hypervisor that supports multiple VMs/domains. Since each VM in the system runs several real-time tasks, scheduling the VMs leads to a hierarchical scheduling problem. So far, most published techniques for analyzing hierarchical scheduling deal with the schedulabil-ity problem, i.e., for a given hierarchical scheduler, testing whether a set of real-time tasks meet their deadlines. In this paper, we are rather concerned with the synthesis of hier-archical/VM schedulers; that is, how to design a scheduler such that all real-time tasks running on the different VMs meet their deadlines. We consider a setup where the tasks are scheduled on multiple VMs under fixed priorities according to the Deadline Monotonic (DM) policy. The VMs are scheduled under fixed priorities on a Rate Monotonic (RM) basis using one or more processors. A partitioned scheduling of VMs is considered, i.e., VMs are not allowed to migrate from one processor to the other. In this context, we propose a method for selecting optimum time slices and periods for each VM in the system. Our goal is to configure the VM scheduler such that not only all tasks are schedulable but also the minimum possible resources are used. Finally, to illustrate the proposed design technique, we present a case study based on automotive control applications.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062302,no
322,Developing a Single Model and Test Prioritization Strategies for Event-Driven Software,"Event-Driven Software (EDS) can change state based on incoming events; common examples are GUI and Web applications. These EDSs pose a challenge to testing because there are a large number of possible event sequences that users can invoke through a user interface. While valuable contributions have been made for testing these two subclasses of EDS, such efforts have been disjoint. This work provides the first single model that is generic enough to study GUI and Web applications together. In this paper, we use the model to define generic prioritization criteria that are applicable to both GUI and Web applications. Our ultimate goal is to evolve the model and use it to develop a unified theory of how all EDS should be tested. An empirical study reveals that the GUI and Web-based applications, when recast using the new model, show similar behavior. For example, a criterion that gives priority to all pairs of event interactions did well for GUI and Web applications; another criterion that gives priority to the smallest number of parameter value settings did poorly for both. These results reinforce our belief that these two subclasses of applications should be modeled and studied together.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401169,no
323,Dynamic performance stubs to simulate the main memory behavior of applications,"Dynamic performance stubs provide a framework to simulate the performance behavior of software modules and functions. Hence, they can be used as an extension to software performance engineering methodologies. The methodology of dynamic performance stubs targets to gain oriented performance improvement. Other applications include the identification of ""hidden"" bottlenecks and the prioritization of optimization alternatives. Main memory stubs have been developed to extend the simulation possibilities of the dynamic performance stubs framework. They are able to simulate the heap and stack behavior of software modules or functions. This paper evaluates an algorithm to generate the simulation data file, which serves as input for the main memory stubs simulation algorithm. Moreover, it presents an automatic error correction algorithm to consider the results from the calibration functions to improve the simulation results. Additionally, a proof of concept is given to depict the results of the simulation data file generation and the automatic error correction algorithm. This paper shows that, it is possible to generate the simulation data file as well as to optimize the simulation data to compensate inaccuracies in order to create main memory stubs.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984857,no
324,Dynamic Prioritization in Regression Testing,"Although used extensively in industry, regression testing is challenging from both a process management as well as a resource management perspective. In literature, proposed test case prioritization techniques assume a constant pool of test cases with non-changing coverage during the regression testing process, and therefore they work with a fixed, prioritized test suite. However, in practice, test cases and their coverage metrics may change during regression testing due to modifications of software artefacts (e.g. due to bug fixing). For example, modifying obsolete test cases or source code may change the coverage metrics during the process. This may lead to some changes in test case priorities. Dealing with manual tests cases, scheduling test case execution in shared environments and other constraints in practice may cause the same effect. In this paper, we highlight these challenges in industrial regression testing and propose a paradigm called Dynamic Prioritization, which uses in-process events and the most up-to-date test suite to re-order test cases.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954402,yes
325,"Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities","Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560680,no
326,Fast Start-up for Spartan-6 FPGAs using Dynamic Partial Reconfiguration,"This paper introduces the first available tool flow for Dynamic Partial Reconfiguration on the Spartan-6 family. In addition, the paper proposes a new configuration method called Fast Start-up targeting modern FPGA architectures, where the FPGA is configured in two-steps, instead of using a single (monolithic) full device configuration. In this novel approach, only the timing-critical modules are loaded at power-up using the first high-priority bitstream, while the non-timing critical modules are loaded afterwards. This two-step or prioritized FPGA start-up is used in order to meet the extremely tight startup timing specifications found in many modern applications, like PCI-express or automotive applications. Finally, the developed tool flow and methods for Fast Start-up have been used and tested to implement a CAN-based automotive ECU on a Spartan-6 evaluation board (i.e., SP605). By using this novel approach, it was possible to decrease the initial bitstream size and hence, achieve a configuration time speed-up of up to 4.5??, when compared to a standard configuration solution.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763244,no
327,GeTeX: A Tool for Testing Real-Time Embedded Systems Using CAN Applications,"Real-Time Embedded Systems (RTES) have an increasing role in controlling the IT that we use on a day-to-day basis. The behaviour of an RTES is not based solely on the interactions it might have with its surrounding environment, but also on the timing requirements it induces. As a result, ensuring that an RTES behaves correctly is non-trivial, especially after adding time as a new dimension to the complexity of the testing process. In previous research, we introduced a 'priority-based' approach which tested the logical and timing behaviour of an RTES modeled formally as UPPAAL Timed Automata (UTA). The 'priority-based' approach was based on producing sets of timed test traces by achieving timing constraints coverage according to three sets of priorities, namely boundary, out-boundary and in-boundary. In this paper, we introduce a new testing tool 'GeTeX' that deploys the ""priority-based"" testing approach. GeTeX is a complete testing tool which generates timed test-cases from UTA models and executes them on the System Under Test (SUT) to identify faults. In its current version, GeTeX supports Control Area Network (CAN) applications.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5934805,no
328,Goal-Oriented Test Case Selection and Prioritization for Product Line Feature Models,"The software product line engineering paradigm is amongst the widely used means for capturing and handling the commonalities and variabilities of the many applications of a target domain. The large number of possible products and complex interactions between software product line features makes the effective testing of them a challenge. To conquer the time and space complexity involved with testing a product line, an intuitive approach is the reduction of the test space. In this paper, we propose an approach to reduce the product line test space. We introduce a goal-oriented approach for the selection of the most desirable features from the product line. Such an approach allows us to identify the features that are more important and need to be tested more comprehensively from the perspective of the domain stakeholders. The more important features and the configurations that contain them will be given priority over the less important configurations, hence providing a hybrid test case reduction and prioritization strategy for testing software product lines.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945249,yes
329,How to Shop for Free Online -- Security Analysis of Cashier-as-a-Service Based Web Stores,"Web applications increasingly integrate third-party services. The integration introduces new security challenges due to the complexity for an application to coordinate its internal states with those of the component services and the web client across the Internet. In this paper, we study the security implications of this problem to merchant websites that accept payments through third-party cashiers (e.g., PayPal, Amazon Payments and Google Checkout), which we refer to as Cashier-as-a-Service or CaaS. We found that leading merchant applications (e.g., NopCommerce and Interspire), popular online stores (e.g., Buy.com and JR.com) and a prestigious CaaS provider (Amazon Payments) all contain serious logic flaws that can be exploited to cause inconsistencies between the states of the CaaS and the merchant. As a result, a malicious shopper can purchase an item at an arbitrarily low price, shop for free after paying for one item, or even avoid payment. We reported our findings to the affected parties. They either updated their vulnerable software or continued to work on the fixes with high priorities. We further studied the complexity in finding this type of logic flaws in typical CaaS-based checkout systems, and gained a preliminary understanding of the effort that needs to be made to improve the security assurance of such systems during their development and testing processes.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958046,no
330,Hybrid regression testing technique: A multi layered approach,Software needs to be delivered well in time and within budgets. One way of doing this is performing incremental delivery of the software with each increment being adding new features along with changes requests. This incremental delivery is supported with requirement prioritization and needs to be tested for checking the reliability and quality. Testing of this increment calls for testing of not only of old newly added functionality but also of existing features so as to make sure that old parts that works perfectly well do not malfunctions after new code is added. Thus a new hybrid technique is proposed in this paper that clusters the test cases and prioritizes the clusters on basis of priorities of requirements represented by the clusters and series of selections and prioritizations at levels of test cases reduces the number of test cases to manageable level and execution of these test cases guarantees the testing of highest priority requirements associated with statements that are often associated with failures or has highest number of parents or sibling's statements dependent on it and is likely to be influenced by changes or failures in this statement.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139363,yes
331,Impact Analysis of Configuration Changes for Test Case Selection,"Testing configurable systems, which are becoming prevalent, is expensive due to the large number of configurations and test cases. Existing approaches reduce this expense by selecting or prioritizing configurations. However, these approaches redundantly run the full test suite for the selected configurations. To address this redundancy, we propose a test case selection approach by analyzing the impact of configuration changes with static program slicing. Given an existing test suite T used for testing a system S under a configuration C, our approach decides for each t in T if t has to be used for testing S under a different configuration C'. We have evaluated our approach on a large industrial system within ABB with promising results.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132962,no
332,Improving independence in the community for stroke survivors: The role of biomechanics visualisation in ankle-foot orthosis tuning,"One of the key priorities for stroke survivors in their rehabilitation process is regaining their ability to walk. Evidence has shown that provision of ankle-foot orthoses (AFOs) can have a positive impact on walking. This paper discusses the role of gait analysis in the provision of AFOs for stroke survivors. A discussion of the shortcomings of gait analysis techniques is included, with a description of how these might be overcome during the AFO tuning process through the ongoing development of data visualisation software. The design of a randomised controlled trial in conjunction with a series of qualitative measures is described, which will be used to test the efficacy of the visualisation software.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038838,no
333,Improving Regression Testing Transparency and Efficiency with History-Based Prioritization -- An Industrial Case Study,"Background: History based regression testing was proposed as a basis for automating regression test selection, for the purpose of improving transparency and test efficiency, at the function test level in a large scale software development organization. Aim: The study aims at investigating the current manual regression testing process as well as adopting, implementing and evaluating the effect of the proposed method. Method: A case study was launched including: identification of important factors for prioritization and selection of test cases, implementation of the method, and a quantitative and qualitative evaluation. Results: 10 different factors, of which two are history-based, are identified as important for selection. Most of the information needed is available in the test management and error reporting systems while some is embedded in the process. Transparency is increased through a semi-automated method. Our quantitative evaluation indicates a possibility to improve efficiency, while the qualitative evaluation supports the general principles of history-based testing but suggests changes in implementation details.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770626,yes
334,IMS Threat and Attack Surface Analysis Using Common Vulnerability Scoring System,"For the purposes of this study, IMS specifications and public sources were analyzed using the general attack surface analysis methodology. These findings were verified and augmented by active scanning and passive analysis of the available real-world IMS test setups that were investigated during the project. As various tests and security probes were performed against the test setups, the system behaviour was analyzed for previously undetermined interactions and transient attack surfaces. After the IMS attack vectors had been identified, the Common Vulnerability Scoring System version 2 (CVSSv2) Base Scores were used to prioritize the IMS attack surface interfaces. CVSS is an industry standard for classifying vulnerabilities. It must be noted however that the idea of applying CVSS scoring to an a priori comparison of vulnerability categories and potential attack surfaces is original research by the authors of this study.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032214,no
335,Increasing test coverage using human-based approach of fault injection testing,"Fault injection testing (FIT) approach validates system's fault tolerance mechanism by actively injecting software faults into the targeted areas in the system in order to accelerate its failure rate. This highly complements other testing approaches such as requirements and regression testing implemented during the same testing phase. During testing, it is impossible to run all possible test scenarios. It is especially difficult to predict how the user might use the system functionality correctly as per design. The human interaction through the system may be varies and will leads to the functionality loophole. It is therefore important to have strategic testing approach for evaluating the dependability of computer systems especially in human errors. This paper proposed on applying Knowledge-Based, Fault Prediction Model and Test Case Prioritization approaches that can be combined to increase the test coverage. The goal of this paper is to highlight the needs and advantages of the selected approaches in performing FIT as one of effective testing techniques in the ongoing quest for increased software quality.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140685,yes
336,Industrial experiences with automated regression testing of a legacy database application,"This paper presents a practical approach and tool (DART) for functional black-box regression testing of complex legacy database applications. Such applications are important to many organizations, but are often difficult to change and consequently prone to regression faults during maintenance. They also tend to be built without particular considerations for testability and can be hard to control and observe. We have therefore devised a practical solution for functional regression testing that captures the changes in database state (due to data manipulations) during the execution of a system under test. The differences in changed database states between consecutive executions of the system under test, on different system versions, can help identify potential regression faults. In order to make the regression test approach scalable for large, complex database applications, classification tree models are used to prioritize test cases. The test case prioritization can be applied to reduce test execution costs and analysis effort. We report on how DART was applied and evaluated on business critical batch jobs in a legacy database application in an industrial setting, namely the Norwegian Tax Accounting System (SOFIE) at the Norwegian Tax Department (NTD). DART has shown promising fault detection capabilities and cost-effectiveness and has contributed to identify many critical regression faults for the past eight releases of SOFIE.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080803,no
337,Lower bounds for single machine subproblems occurring in weighted tardiness oriented shifting bottleneck procedures,"In this paper, we propose lower bounds for single machine scheduling problems which occur during a run of a shifting bottleneck procedure for total weighted tardiness job shops. The specific structure of this kind of problem and its objective function in particular prevent an immediate transfer or an adaption of existing lower bounds from ƒ??conventionalƒ? single machine problems with tardiness related objectives. Hence it has been necessary to develop bounding approaches which are to some extent conceptually new. Potential application scenarios range from exact subproblem solution methods or machine prioritization criteria in a shifting bottleneck procedure to branch-and-bound algorithms for job shops with total weighted tardiness objective. In order to provide a significant evaluation of the proposed lower bounds regarding their effectiveness and efficiency, we tested them based on problem instances which actually have been generated in a shifting bottleneck procedure applied to benchmark job shop problems.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976548,no
338,Making the Case for MORTO: Multi Objective Regression Test Optimization,This paper argues that regression test optimization problems such as selection and prioritization require multi objective optimization in order to adequately cater for real world regression testing scenarios. The paper presents several examples of costs and values that could be incorporated into such a Multi Objective Regression Test Optimization (MORTO) approach.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954399,yes
339,Modeling the Diagnostic Efficiency of Regression Test Suites,"Diagnostic performance, measured in terms of the manual effort developers have to spend after faults are detected, is not the only important quality of a diagnosis. Efficiency, i.e., the number of tests and the rate of convergence to the final diagnosis is a very important quality of a diagnosis as well. In this paper we present an analytical model and a simulation model to predict the diagnostic efficiency of test suites when prioritized with the information gain algorithm. We show that, besides the size of the system itself, an optimal coverage density and uniform coverage distribution are needed to achieve an efficient diagnosis. Our models allow us to decide whether using IG with our current test suite will provide a good diagnostic efficiency, and enable us to define criteria for the generation or improvement of test suites.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954476,no
340,On Practical Adequate Test Suites for Integrated Test Case Prioritization and Fault Localization,"An effective integration between testing and debugging should address how well testing and fault localization can work together productively. In this paper, we report an empirical study on the effectiveness of using adequate test suites for fault localization. We also investigate the integration of test case prioritization and statistical fault localization with a postmortem analysis approach. Our results on 16 test case prioritization techniques and four statistical fault localization techniques show that, although much advancement has been made in the last decade, test adequacy criteria are still insufficient in supporting effective fault localization. We also find that the use of branch-adequate test suites is more likely than statement-adequate test suites in the effective support of statistical fault localization.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004308,no
341,Pragmatic prioritization of software quality assurance efforts,"A plethora of recent work leverages historical data to help practitioners better prioritize their software quality assurance efforts. However, the adoption of this prior work in practice remains low. In our work, we identify a set of challenges that need to be addressed to make previous work on quality assurance prioritization more pragmatic. We outline four guidelines that address these challenges to make prior work on software quality assurance more pragmatic: 1) Focused Granularity (i.e., small prioritization units), 2) Timely Feedback (i.e., results can be acted on in a timely fashion), 3) Estimate Effort (i.e., estimate the time it will take to complete tasks), and 4) Evaluate Generality (i.e., evaluate findings across multiple projects and multiple domains). We present two approaches, at the code and change level, that demonstrate how prior approaches can be more pragmatic.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032601,no
342,Prioritising Refactoring Using Code Bad Smells,"We investigated the relationship between six of Fowler et al.'s Code Bad Smells (Duplicated Code, Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man) and software faults. In this paper we discuss how our results can be used by software developers to prioritise refactoring. In particular we suggest that source code containing Duplicated Code is likely to be associated with more faults than source code containing the other five Code Bad Smells. As a consequence, Duplicated Code should be prioritised for refactoring. Source code containing Message Chains seems to be associated with a high number of faults in some situations. Consequently it is another Code Bad Smell which should be prioritised for refactoring. Source code containing only one of the Data Clumps, Switch Statements, Speculative Generality, or Middle Man Bad Smell is not likely to be fault-prone. As a result these Code Bad Smells could be put into a lower refactoring priority.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954447,no
343,Prioritizing interaction test suite for t-way testing,"In recent years, many new t-way interaction based strategies (where t indicates the interaction strength), particularly based on covering arrays, have been developed in the literature. In search of an optimal strategy that generates the most minimum number of tests, many of existing t-way strategies have not sufficiently dealt with test prioritization (i.e. in terms of maximizing new interaction coverage per test). Addressing this issue, this paper highlights a useful prioritization algorithm to reorganize the test cases in order to improve the rate of interaction coverage. This algorithm takes a pre-generated t-way test suite as input and automatically generates a priority ordered test suite as output. In order to demonstrate its applicability, this paper demonstrates the use of the algorithm to help prioritize the test suite generated by existing t-way strategy, MC-MIPOG.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140686,yes
344,Prioritizing Requirements-Based Regression Test Cases: A Goal-Driven Practice,"Any changes for maintenance or evolution purposes may break existing working features, or may violate the requirements established in the previous software releases. Regression testing is essential to avoid these problems, but it may be ended up with executing many time-consuming test cases. This paper tries to address prioritizing requirements-based regression test cases. To this end, system-level testing is focused on two practical issues in industrial environments: i) addressing multiple goals regarding quality, cost and effort in a project, and ii) using non-code metrics due to the lack of detailed code metrics in some situations. This paper reports a goal-driven practice at Research In Motion (RIM) towards prioritizing requirements-based test cases regarding these issues. Goal-Question-Metric (GQM) is adopted in identifying metrics for prioritization. Two sample goals are discussed to demonstrate the approach: detecting bugs earlier and maintaining testing effort. We use two releases of a prototype Web-based email client to conduct a set of experiments based on the two mentioned goals. Finally, we discuss lessons learned from applying the goal-driven approach and experiments, and we propose few directions for future research.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741354,yes
345,Prioritizing tests for fault localization through ambiguity group reduction,"In practically all development processes, regression tests are used to detect the presence of faults after a modification. If faults are detected, a fault localization algorithm can be used to reduce the manual inspection cost. However, while using test case prioritization to enhance the rate of fault detection of the test suite (e.g., statement coverage), the diagnostic information gain per test is not optimal, which results in needless inspection cost during diagnosis. We present RAPTOR, a test prioritization algorithm for fault localization, based on reducing the similarity between statement execution patterns as the testing progresses. Unlike previous diagnostic prioritization algorithms, RAPTOR does not require false negative information, and is much less complex. Experimental results from the Software Infrastructure Repository's benchmarks show that RAPTOR is the best technique under realistic conditions, with average cost reductions of 40% with respect to the next best technique, with negligible impact on fault detection capability.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100153,no
346,Ranking of technology transfer barriers in developing countries; case study of Iran's biotechnology industry,"In this paper, first we shall define a list of technology transfer barriers in the biotechnology field through interviews and literature reviews. Next, we shall categorize them into four major categories of technology, and at last introduce all of the hypotheses on the existence of relationship between each part of technology and failure in the technology transfer process. Questionnaires consisting of two parts were created and handed out to all of the specialists and biotechnology idea-holders in the Iranian National Research center of Biotechnology, Lidco Co, Iran Pastor Institute, Biotechnology Department of University of Tehran, etc. Going on, we've evaluated the factors' validity through statistical tests and the recognized barriers' priorities in each category, which were generally, identified by Analytical Hierarchy Process (AHP) via the software Expert Choice. These priorities describe the high importance of organization-ware (34.1%), information-ware (27%), technique-ware (20.7%) and eventually human-ware (18.1%) respectively.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118187,no
347,Regression testing in Software as a Service: An industrial case study,"Many organizations are moving towards a business model of Software as a Service (SaaS), where customers select and pay for services dynamically via the web. In SaaS, service providers face the challenge of delivering and maintaining high quality software solutions which must continue to work under an enormous number of scenarios; customers can easily subscribe and unsubscribe from services at any point. To date, there has been little research on unique approaches for regression test methodologies for testing in a SaaS environment. In this paper, we present an industrial case study of a regression testing approach to improve test effectiveness and efficiency in SaaS. We model service level use cases from field failures as abstract events and then generate sequences of these for testing to provide a broad coverage of the possible use cases. In subsequent releases of the system we prioritize the tests to improve time to detection of faults in the modified system. We have applied our technique to two releases of a large industrial enterprise level SaaS application and demonstrate that using our approach (1) we could have uncovered escaped faults prior to the system release in both versions of the system; (2) using a priority order we could have improved the efficiency of testing in the first version; and (3) prioritization based on failure history from the first version increases the fault detection rate in the new version, suggesting a correlation between the important sequences in versions that can be leveraged for regression testing.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080804,no
348,Research and implementation of resource management system based on Xen virtual machine,"With the development of computer technology, there is gradually abundant resource available for computer systems. Virtualization technology provides a viable solution for effective management and rational allocation of system resources. Xen virtual machine is an excellent open source virtual machine, so attracting widespread attention, with broad application prospects. However, the traditional resource management of Xen virtual machine focuses on sharing processor resources fairly, while ignoring the effect of the virtual machines with the different priorities. This would cause the practicality and the performance issues in using virtual machine. This paper proposes a resource management system model based on Xen virtual machine. The model monitors guest domain and analyzes runtime information for automating resource allocation. It mainly take into account the virtual machines with different priorities. Through a series of comparative tests, the results verify that this model can enhance practicality.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6182220,no
349,Risk-Based Testing of Safety-Critical Embedded Systems Driven by Fault Tree Analysis,"One important aspect of the quality assurance process of safety-critical embedded systems is verifying the appropriateness, correctness of the implementation and effectiveness of safety functions. Due to the rapid growth in complexity, manual verification activities are no longer feasible. This holds especially for testing. A popular method for testing such complex systems is model-based testing. Recent techniques for model-based testing do not sufficiently take into consideration the information derived from the safety analyses like Failure Mode and Effect Analysis and Fault Tree Analyses (FTA). In this paper, we describe an approach to use the results of FTA during the construction of test models, such that test cases can be derived, selected and prioritized according to the severity of the identified risks and the number of basic events that cause it. This approach is demonstrated on an example from the automation domain, namely a modular production system. We find that the method provides a significant increase in coverage of safety functions, compared to regular model based testing.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954386,no
350,"Robotic test bed for autonomous surface exploration of Titan, Mars, and other planetary bodies","Tier-scalable robotic reconnaissance missions are called for in extreme space environments, including planetary atmospheres, surfaces (both solid and liquid), and subsurfaces (e.g., oceans), as well as in potentially hazardous or inaccessible operational areas on Earth. Such future missions will require increasing degrees of operational autonomy: (1) Automatic mapping of an operational area from different vantages (i.e., spaceborne, airborne, surface, subsurface); (2) automatic sensor deployment and sensor data gathering; (3) automatic feature extraction and target/region-of-interest/anomaly identification within the mapped operational area; (4) automatic target prioritization for follow-up or close-up (in-situ) examination; and (5) subsequent automatic, targeted deployment and navigation/relocation of agents/sensors (e.g., to follow up on transient events). We report on recent progress in developing an Earth-based (outdoors) robotic test bed for Tier-scalable Reconnaissance at the University of Arizona and Caltech for distributed, science-driven, and significantly less constrained (compared to state-of-the-art) reconnaissance of prime locations on a variety of planetary bodies, with particular focus on Saturn's moon Titan with its methane/hydrocarbon lakes and Mars. The test bed currently comprises several computer-controlled robotic surface vehicles, i.e., rovers and lake landers/boats equipped with a variety of sensors. To achieve a fully operational Tier-scalable Reconnaissance test bed, aerial platforms will be integrated as a next step. The robotic surface vehicles can be interactively or automatically controlled from anywhere in the world in near real-time via the Internet. The test bed enables the implementation, field-testing, and validation of algorithms and strategies for navigation, exploration, sensor deployment, sensor data gathering, feature extraction, anomaly detection, and science goal prioritization for autonomous planetary exploration. Furthermore, it permits field-testing of novel instruments and sensor technologies, as well as testing of cooperative multi-agent scenarios and distributed scientific exploration of operational areas. As such the robotic test bed enables the development, implementation, field-testing, and validation of software packages for inter-agent communication and coordination to navigate and explore operational areas with greatly reduced reliance on (ultimately without assistance from) ground operators, thus affording the degree of mission autonomy/flexibility necessary to support future missions to Titan, Mars, and other planetary bodies, including asteroids.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747267,no
351,Scenario Driven Testing,"Software testing has traditionally focused on evaluating the functionality of implemented modules against feature specifications. This approach assumes that customer requirements and usage scenarios are accurately translated into specifications and that individual modules implemented using the feature specifications would work seamlessly and coherently to solve business problems meant to be addressed by the software under test. To ensure software built would help customers solve their business problems as intended, test teams have to go beyond traditional feature driven testing approach and test software for quality and completeness with respect to targeted customer scenarios. For this, test teams have to adopt scenario driven test methodology which involves understanding the targeted customer scenarios and use them along with feature specifications for the intended software solution to translate them into test specifications, prioritization of test work items and use them throughout project for shared understanding of tradeoffs and making decisions. In this short paper, we describe scenario driven testing and share how it was applied to test a feature-set developed for a successful product line at Microsoft??.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945250,no
352,"Security Requirements Analysis, Specification, Prioritization and Policy Development in Cyber-Physical Systems","In recent past, the security of cyber-physical systems (CPSs) has been the subject of major concern. One of the reasons is that, CPSs are often applied to mission-critical processes. Also, the automation CPSs bring in managing physical processes, and the detail of information available to them for carrying out their tasks, make securing them a prime importance. Securing CPSs is a difficult task as systems are interconnected. In order to achieve a continuous secured CPS environment, there is the need for an integrated methodology to analyze, specify and prioritize security requirements and also to develop policies to meet them. First, CPS assets are represented using high-order object models. Second, swim lane diagrams are extended to include malactivities and prevention or mitigation options to decompose use cases. We analyze security threats pertaining to the hardware components, software components and the hardware-software interaction. Security requirements are then specified, and an analytical prioritization approach, based on relative priority analysis is employed to prioritize them. Finally, security policies are then developed to meet the requirements. To demonstrate its effectiveness and evaluate its application, the proposed methodology is applied in a structured approach to a test bed - Ayushman, a Pervasive Health Monitoring System (PHMS).",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004511,no
353,Test Case Generation and Prioritization from UML Models,"This paper proposes a novel approach to generating test cases from UML 2.0 activity diagrams and prioritizing those test cases using model information encapsulated in the activity diagrams. The test cases generated according to our approach are suitable for system level testing of the application. For prioritization of test cases, we propose a method based on coverage of all transitions in the activity diagram and usage probability of a particular flow in the activity model. We also propose an approach for selecting test data based on analysis of the branch conditions of the decision nodes in the activity diagrams.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734956,yes
354,Test case prioritization for regression testing based on fault dependency,"Test case prioritization techniques involve scheduling test cases for regression testing in an order that increases their effectiveness at meeting some performance goal. This is inefficient to re execute all the test cases in regression testing following the software modifications. Using information obtained from previous test case execution, prioritization techniques order the test cases for regression testing so that most beneficial are executed first thus allows an improved effectiveness of testing. One performance goal, rate of dependency detected among faults, measures how quickly dependency among faults are detected within the regression testing process. An improved rate of fault dependency can provide faster feedback on software and let developers start debugging on the severe faults that cause other faults to appear later. This paper presents the new metric for assessing rate of fault dependency detection and an algorithm to prioritize test cases. Using the new metric the effectiveness of this prioritization is shown comparing it with non-prioritized test case. Analysis proves that prioritized test cases are more effective in detecting dependency among faults.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941954,yes
355,Test Case Prioritization Technique Based on Genetic Algorithm,"With the rapid development of information technology, software testing, as a software quality assurance, is becoming more and more important. In the software life cycle, each time the code has changed need to be regression testing. The huge test case library makes running a full test case library being challenged. To this end, we designed a genetic algorithm-based test case prioritization algorithm and improved the genetic algorithm proposed software test case prioritization algorithm.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063222,yes
356,Test Generation for X-machines with Non-terminal States and Priorities of Operations,"Testing methods aiming to demonstrate that an implementation behaves the same as a specification X-machine (extended finite-state machine) usually assume that (1) all states are terminal states and (2) there are no priorities associated with operations on transitions. The considered model for the machine is such that outputs for transitions leading to non-terminal states will be buffered and contents of buffers will only be made observable when terminal states are entered. The X-machine testing method has been extended in this work to handle such an extension of X-machines (EFSM).Priorities of operations determine the order in which guards of transitions are evaluated. This makes it possible to reduce the size of a test suite. For instance, if testing has shown that a transition with a specific guard g has been implemented from some state, then no lower-priority transition with a guard implied by g may ever be executed from that state. It is hence not necessary to test for the presence of such a lower-priority transition.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770602,no
357,Towards Impact Analysis of Test Goal Prioritization on the Efficient Execution of Automatically Generated Test Suites Based on State Machines,"Test prioritization aims at reducing test execution costs. There are several approaches to prioritize test cases based on collected data of previous test runs, e.g., in regression testing. In this paper, we present a new approach to test prioritization for efficient test execution that is focused on the artifacts used in model-based test generation from state machines. We propose heuristics for test goal prioritizations and evaluate them using two different test models. Our finding is that the prioritizations can have a positive impacton the test execution efficiency. This impact, however, is hard to predict for a concrete situation. Thus, the question for the general gain of test goal prioritizations is still open.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004322,yes
358,Usability Testing Methodology: Effectiveness of Heuristic Evaluation in E-Government Website Development,"Software development organizations consist of marketing, project management, development, design, and quality assurance team. It is important for the various teams within the organization to understand the benefits and limitation of incorporating various usability testing methods within the software development life cycle. Some of the reasons for poor usability include effort prioritization conflicts from project management, development and design team. The role of the usability engineer is to get involved as the heuristic evaluator and facilitate the development and design efforts are based on usability principles and at the same time adhering to the project time line. Two of the common usability inspection methods consist of user experience testing and expert review or more commonly known as Heuristic Evaluation (HE). This paper focuses on understanding the effectiveness of HE as a methodology for defect detection. The results show the effectiveness of the HE as a usability testing methodology in capturing defects and prioritizing development and design efforts. The results also reinforce the need for integrating traditional heuristics with modified heuristics customized to the domain or field of the project being tested such as E-Government.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5961243,no
359,Using SQL Hotspots in a Prioritization Heuristic for Detecting All Types of Web Application Vulnerabilities,"Development organizations often do not have time to perform security fortification on every file in a product before release. One way of prioritizing security efforts is to use metrics to identify core business logic that could contain vulnerabilities, such as database interaction code. Database code is a source of SQL injection vulnerabilities, but importantly may be home to unrelated vulnerabilities. The goal of this research is to improve the prioritization of security fortification efforts by investigating the ability of SQL hotspots to be used as the basis for a heuristic for prediction of all vulnerability types. We performed empirical case studies of 15 releases of two open source PHP web applications: Word Press, a blogging application, and WikkaWiki, a wiki management engine. Using statistical analysis, we show that the more SQL hotspots a file contains per line of code, the higher the probability that file will contain any type of vulnerability.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770611,no
360,Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts,"Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these ƒ??top crashesƒ? thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711013,no
361,A Framework to Support Research in and Encourage Industrial Adoption of Regression Testing Techniques,"When software developers make changes to a program, it is possible that they will introduce faults into previously working parts of the code. As software grows, a regression test suite is run to ensure that the old functionality still works as expected. Yet, as the number of test cases increases, it becomes more expensive to execute the test suite. Reduction and prioritization techniques enable developers to manage large and unwieldy test suites. However, practitioners and researchers do not always use and study these methods due, in part, to a lack of availability. In response to this issue, this paper describes an already released open-source framework that supports both research and practice in regression testing. The sharing of this framework will enable the replication of empirical studies in regression testing and encourage faster industrial adoption of these useful, yet rarely used, techniques.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200107,no
362,A Heuristic Model-Based Test Prioritization Method for Regression Testing,"Due to the resource and time constraints for re-executing large test suites in regression testing, developers are interested in detecting faults in the system as early as possible. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. In this paper, we present a model-based heuristic method to prioritize test cases for regression testing, which takes into account two types of information collected during execution of the modified model on the test suite. The experiment shows that our algorithm has better effectiveness of early fault detection.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228450,yes
363,A Multi-Objective Technique to Prioritize Test Cases Based on Latent Semantic Indexing,"To early discover faults in source code, test case ordering has to be properly chosen. To this aim test prioritization techniques can be used. Several of these techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test prioritization technique that determines sequences of test cases that maximize the number of discovered faults that are both technical and business critical. The technique uses the information related to the code and requirements coverage, as well as the execution cost of each test case. The approach also uses recovered trace ability links among source code and system requirements via the Latent Semantic Indexing technique. We evaluated our proposal against both a random prioritization technique and two single-objective prioritization techniques on two Java applications. The results indicate that our proposal outperforms the baseline techniques and that additional improvements are still possible.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178873,yes
364,A Remote Sensing Approach for Landslide Hazard Assessment on Engineered Slopes,"Earthworks such as embankments and cuttings are integral to road and rail networks but can be prone to instability, necessitating rigorous and continual monitoring. To date, the potential of remote sensing for earthwork hazard assessment has been largely overlooked. However, techniques such as airborne laser scanning (ALS) are now ripe for addressing these challenges. This research presents the development of a novel hazard assessment strategy, combining high-resolution remote sensing with a numerical modeling approach. The research was implemented at a railway test site located in northern England, U.K.; ALS data and multispectral aerial imagery facilitated the determination of key slope stability variables, which were then used to parameterize a coupled hydrological-geotechnical model, in order to simulate slope behavior under current and future climates. A software toolset was developed to integrate the core elements of the methodology and determine resultant slope failure hazard which could then be mapped and queried within a geographical information system environment. Results indicate that the earthworks are largely stable, which is in broad agreement with the management company's slope hazard grading data, and in terms of morphological analysis, the remote methodology was able to correctly identify 99% of earthworks classed as embankments and 100% of cuttings. The developed approach provides an effective and practicable method for remotely quantifying slope failure hazard at fine spatial scales (0.5 m) and for prioritizing and reducing on-site inspection.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032742,no
365,"A Scalable, Lightweight WebOS Application Framework","Frequently, as applications scale, they are considered in the context of a web OS-based architecture. In support of this goal, we present a lightweight framework designed as a middleware application. This architecture is highly influenced by hypermedia-based techniques, leveraging metadata in the context of HTML5. Our framework relies on a novel incorporation of a number of open source technologies including node.js and couchDB to support priorities of fast-prototyping, scalability and maintainability. Initial experiments have demonstrated that our approach performs effectively among the dynamics of our environment.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424523,no
366,A Static Approach to Prioritizing JUnit Test Cases,"Test case prioritization is used in regression testing to schedule the execution order of test cases so as to expose faults earlier in testing. Over the past few years, many test case prioritization techniques have been proposed in the literature. Most of these techniques require data on dynamic execution in the form of code coverage information for test cases. However, the collection of dynamic code coverage information on test cases has several associated drawbacks including cost increases and reduction in prioritization precision. In this paper, we propose an approach to prioritizing test cases in the absence of coverage information that operates on Java programs tested under the JUnit framework-an increasingly popular class of systems. Our approach, JUnit test case Prioritization Techniques operating in the Absence of coverage information (JUPTA), analyzes the static call graphs of JUnit test cases and the program under test to estimate the ability of each test case to achieve code coverage, and then schedules the order of these test cases based on those estimates. To evaluate the effectiveness of JUPTA, we conducted an empirical study on 19 versions of four Java programs ranging from 2K-80K lines of code, and compared several variants of JUPTA with three control techniques, and several other existing dynamic coverage-based test case prioritization techniques, assessing the abilities of the techniques to increase the rate of fault detection of test suites. Our results show that the test suites constructed by JUPTA are more effective than those in random and untreated test orders in terms of fault-detection effectiveness. Although the test suites constructed by dynamic coverage-based techniques retain fault-detection effectiveness advantages, the fault-detection effectiveness of the test suites constructed by JUPTA is close to that of the test suites constructed by those techniques, and the fault-detection effectiveness of the test suites constructed by some of JUPTA's variants is better than that of the test suites constructed by several of those techniques.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363461,yes
367,A Two-Level Prioritization Approach for Regression Testing of Web Applications,A test case prioritization technique reschedules test cases for regression testing in an order to achieve specific goals like early fault detection. We propose a new two level prioritization approach to prioritize test cases for web applications as a whole. Our approach automatically selects modified functionalities in a web application and executes test cases on the basis of the impact of modified functionalities. We suggest several new prioritization strategies for web applications and examine whether these prioritization strategies improve the rate of fault detection for web applications. We propose a new automated test suite prioritization model for web applications that selects test cases related to modified functionalities and reschedules them using our new prioritization strategies to detect faults early in test suite execution.,2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462796,yes
368,Appropriate placement of series compensators to improve transient stability of power system,"Trajectory sensitivity analysis is used to find the best places for installation of thyristor controlled series capacitors (TCSC) to improve transient stability of the power system. Based on the rotor angles of generators, an equivalent angle (??<sub>eq</sub>) is defined by determining accelerating and decelerating machines, and then using trajectory sensitivities of this angle with respect to the impedances of the transmission lines in the post-fault system, appropriate locations for placing TCSC will be found. Severity of the faults is also considered in this calculation. This method is applied to the IEEE 3-machine 9-bus test system to find the priorities of the transmission lines for installation of TCSC. Simulation with industrial software verifies the obtained results.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6303243,no
369,Automated prediction of defect severity based on codifying design knowledge using ontologies,"Assessing severity of software defects is essential for prioritizing fixing activities as well as for assessing whether the quality level of a software system is good enough for release. In filling out defect reports, developers routinely fill out default values for the severity levels. The purpose of this research is to automate the prediction of defect severity. Our aim is to research how this severity prediction can be achieved through reasoning about the requirements and the design of a system using ontologies. In this paper we outline our approach based on an industrial case study.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227962,no
370,Case based reasoning approach for adaptive test suite optimization,"Case-based reasoning is an approach to problem solving and learning that has got a lot of attention over the last few years. This paper provides an overview of the foundational issues related to case-based reasoning, describing some of the leading methodological approaches within the field, and exemplifying the current state through pointers to some systems. The framework influences the recent methodologies for knowledge level descriptions of intelligent systems. The methods for case retrieval reuse, solution testing, and learning are summarized, and realization is discussed with few example systems that represent different CBR approaches. Regression testing occurs during the maintenance stage of the software life cycle, however, it requires large amounts of test cases to assure the attainment of a certain degree of quality. So, test suite sizes may grow significantly. This paper focuses primarily on application of CBR to test suite optimization.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395870,no
371,Code coverage-based regression test selection and prioritization in WebKit,"Automated regression testing is often crucial in order to maintain the quality of a continuously evolving software system. However, in many cases regression test suites tend to grow too large to be suitable for full re-execution at each change of the software. In this case selective retesting can be applied to reduce the testing cost while maintaining similar defect detection capability. One of the basic test selection methods is the one based on code coverage information, where only those tests are included that cover some parts of the changes. We experimentally applied this method to the open source web browser engine project WebKit to find out the technical difficulties and the expected benefits if this method is to be introduced into the actual build process. Although the principle is simple, we had to solve a number of technical issues, so we report how this method was adapted to be used in the official build environment. Second, we present results about the selection capabilities for a selected set of revisions of WebKit, which are promising. We also applied different test case prioritization strategies to further reduce the number of tests to execute. We explain these strategies and compare their usefulness in terms of defect detection and test suite reduction.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405252,yes
372,Dependency-Based Test Case Selection and Prioritization in Embedded Systems,"Embedded systems in automotive engineering are getting more and more complex due to a higher rate of integration and shared usage of sensor signals. A common solution to testing these systems is deriving test cases from models, so called model-based testing. In practice, generated test suites are typically very huge and have to be reduced by methods of regression-test selection and prioritization. In our field of application, we additionally suffer from the lack of knowledge on system internals like the source code. Therefore, our approach is based on dependences between the components of embedded systems. The model we use is derived from the system architecture and system requirements. We are using graph algorithms for selecting and prioritizing test cases to run on a certain system version. First statistical evaluations and the current implementation already have shown promising reduction of test-cases for regression testing.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200176,no
373,Diffusion of Software Features: An Exploratory Study,"New features are frequently proposed in many software libraries. These features include new methods, classes, packages, etc. These features are utilized in many open source and commercial software systems. Some of these features are adopted very quickly, while others take a long time to be adopted. Each feature takes much resource to develop, test, and document. Library developers and managers need to decide what feature to prioritize and what to develop next. As a first step to aid these stakeholders, we perform an exploratory study on the diffusion or rate of adoption of features in Java Development Kit (JDK) library. Our empirical study proposes such questions as how many new features are adopted by client applications, how long it takes for a new feature to spread to various software products, what features are diffused quickly, and what features are diffused widely. We perform an exploratory study with new features in Java Development Kit (JDK, from version 1.3 to 1.6) and provide empirical findings to answer the above research questions.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462682,no
374,Diversity maximization speedup for fault localization,"Fault localization is useful for reducing debugging effort. However, many fault localization techniques require non-trivial number of test cases with oracles, which can determine whether a program behaves correctly for every test input. Test oracle creation is expensive because it can take much manual labeling effort. Given a number of test cases to be executed, it is challenging to minimize the number of test cases requiring manual labeling and in the meantime achieve good fault localization accuracy. To address this challenge, this paper presents a novel test case selection strategy based on Diversity Maximization Speedup (DMS). DMS orders a set of unlabeled test cases in a way that maximizes the effectiveness of a fault localization technique. Developers are only expected to label a much smaller number of test cases along this ordering to achieve good fault localization results. Our experiments with more than 250 bugs from the Software-artifact Infrastructure Repository show (1) that DMS can help existing fault localization techniques to achieve comparable accuracy with on average 67% fewer labeled test cases than previously best test case prioritization techniques, and (2) that given a labeling budget (i.e., a fixed number of labeled test cases), DMS can help existing fault localization techniques reduce their debugging cost (in terms of the amount of code needed to be inspected to locate faults). We conduct hypothesis test and show that the saving of the debugging cost we achieve for the real C programs are statistically significant.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494903,no
375,Dynamic Fault Visualization Tool for Fault-based Testing and Prioritization,"Fault-based testing has been proven to be a cost effective testing technique for software logics and rules expressed in Boolean expressions. It can guarantee the elimination of common faults without exhaustive testing. However, average software testing practitioners may not have in-depth knowledge on Boolean algebra and complex logic derivations required to apply existing fault-based testing techniques. In this paper, a dynamic fault visualization tool has been proposed. This tool allows its user to visualize fault-based testing and prioritize test inputs with a simple greedy method. The performance evaluation of this tool has been done on Boolean expressions extracted from a real life aviation tool. The results show that it can achieve significant performance improvements compared to ordinary sequential order test execution and existing static technique. The proposed visualization tool could also identify possible faults to guide the debugging process.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516370,yes
376,G-RankTest: Regression testing of controller applications,"Since controller applications must typically satisfy real-time constraints while manipulating real-world variables, their implementation often results in programs that run extremely fast and manipulate numerical inputs and outputs. These characteristics make them particularly suitable for test case generation. In fact a number of test cases can be easily created, due to the simplicity of numerical inputs, and executed, due to the speed of computations. In this paper we present G-RankTest, a technique for test case generation and prioritization. The key idea is that test case generation can run for long sessions (e.g., days) to accurately sample the behavior of a controller application and then the generated test cases can be prioritized according to different strategies, and used for regression testing every time the application is modified. In this work we investigate the feasibility of using the gradient of the output as a criterion for selecting the test cases that activate the most tricky behaviors, which we expect easier to break when a change occurs, and thus deserve priority in regression testing.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228981,no
377,Graph-Based Optimization Algorithm and Software on Kidney Exchanges,"Kidney transplantation is typically the most effective treatment for patients with end-stage renal disease. However, the supply of kidneys is far short of the fast-growing demand. Kidney paired donation (KPD) programs provide an innovative approach for increasing the number of available kidneys. In a KPD program, willing but incompatible donor-candidate pairs may exchange donor organs to achieve mutual benefit. Recently, research on exchanges initiated by altruistic donors (ADs) has attracted great attention because the resultant organ exchange mechanisms offer advantages that increase the effectiveness of KPD programs. Currently, most KPD programs focus on rule-based strategies of prioritizing kidney donation. In this paper, we consider and compare two graph-based organ allocation algorithms to optimize an outcome-based strategy defined by the overall expected utility of kidney exchanges in a KPD program with both incompatible pairs and ADs. We develop an interactive software-based decision support system to model, monitor, and visualize a conceptual KPD program, which aims to assist clinicians in the evaluation of different kidney allocation strategies. Using this system, we demonstrate empirically that an outcome-based strategy for kidney exchanges leads to improvement in both the quantity and quality of kidney transplantation through comprehensive simulation experiments.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188515,no
378,Guiding Testing Activities by Predicting Defect-Prone Parts Using Product and Inspection Metrics,"Product metrics, such as size or complexity, are often used to identify defect-prone parts or to focus quality assurance activities. In contrast, quality information that is available early, such as information provided by inspections, is usually not used. Currently, only little experience is documented in the literature on whether data from early defect detection activities can support the identification of defect prone parts later in the development process. This article compares selected product and inspection metrics commonly used to predict defect-prone parts. Based on initial experience from two case studies performed in different environments, the suitability of different metrics for predicting defect-prone parts is illustrated. These studies revealed that inspection defect data seems to be a suitable predictor, and a combination of certain inspection and product metrics led to the best prioritizations in our contexts.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328182,no
379,High Performance Memory Requests Scheduling Technique for Multicore Processors,"In modern computer systems, long memory latency is one of the main bottlenecks micro-architects are facing for leveraging the system performance especially for memory-intensive applications. This emphasises the importance of the memory access scheduling to efficiently utilize memory bandwidth. Moreover, in recent micro-processors, multithread and multicore is turned to be the default choice for their design. This resulted in more contention on memory. Hence, the effect of memory access scheduling schemes is more critical to the overall performance boost. Although memory access scheduling techniques have been recently proposed for performance improvement, most of them have overlooked the fairness among the running applications. Achieving both high-throughput and fairness simultaneously is challenging. In this paper, we focus on the basic idea of memory requests scheduling, which includes how to assign priorities to threads, what request should be served first, and how to achieve fairness among the running applications for multicore microprocessors. We propose two new memory access scheduling techniques FLRMR, and FIQMR. Compared to recently proposed techniques, on average, FLRMR achieves 8.64% speedup relative to LREQ algorithm, and FIQMR achieves 11.34% speedup relative to IQ-based algorithm. FLRMR outperforms the best of the other techniques by 8.1% in 8-cores workloads. Moreover, FLRMR improves fairness over LREQ by 77.2% on average.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332168,no
380,Impact Analysis in the Presence of Dependence Clusters Using Static Execute after in WebKit,"Impact analysis based on code dependence can be an integral part of software quality assurance by providing opportunities to identify those parts of the software system that are affected by a change. Because changes usually have far reaching effects in programs, effective and efficient impact analysis is vital, which has different applications including change propagation and regression testing. Static Execute After (SEA) is a relation on program elements (procedures) that is efficiently computable and accurate enough to be a candidate for use in impact analysis in practice. To assess the applicability of SEA in terms of capturing real defects, we present results on integrating it into the build system of Web Kit, a large, open source software system, and on related experiments. We show that a large number of real defects can be captured by impact sets computed by SEA, albeit many of them are large. We demonstrate that this is not an issue in applying it to regression test prioritization, but generally it can be an obstacle in the path to efficient use of impact analysis. We believe that the main reason for large impact sets is the formation of dependence clusters in code. As apparently dependence clusters cannot be easily avoided in the majority of cases, we focus on determining the effects these clusters have on impact analysis.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392099,no
381,Investment optimization methodology applied to investments on non-technical losses reduction actions,"This electronic document presents the research developed on a R&D Project for the Energy Recovery Department of Rio de Janeiro's distribution company, Light S.E.S.A. The main purpose is prioritizing the grid investments on non-technical losses reduction actions. The work was developed along eighteen months, and resulted in experimental software. It analyses the historical results of the actions, allocating optimally the resources to a pre-defined period of time. The methodology consists of a statistic model based on historical results processed through a decision three optimization algorithm in order to maximize the objective function. It was tested on decision making process regarding grid investments to reduce non-technical losses, and the Return-on-Investment results were quite satisfactory.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249321,no
382,Log-based approach for performance requirements elicitation and prioritization,"Requirements engineering activities are a critical part of a project's lifecycle. Success of subsequent project phases is highly dependent on good requirements definition. However, eliciting and achieving consensus on priority between all stakeholders is a complex task. Considering software development of large scale global applications, the challenges increase by the need of managing discussions between groups of stakeholders with different roles and background. This paper presents a practical approach for requirements elicitation and prioritization based on realistic user behaviors observation. It uses basic statistic analysis and application usage information to automatically identify the most relevant requirements for majority of stakeholders. An industry case illustrates the feasibility and efficiency of our approach.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345818,no
383,Managing Software Quality Requirements,"This research study explores current quality requirements (QR) management practices in Australian organisations focusing on the elicitation, handling processes, challenges faced, quantification methods used and interdependency management. This research was conducted through six mini case studies, examining organizations that varied in size, structure, industry and function. A mixed methodology was utilised through an online survey for gathering quantitative data and semi-structured interviews for gathering explanatory qualitative data. The results found that five out of the six organisations studied did not have a formal and defined process for the handling of QRs. Large organisations treated QRs are part of their overall project specifications, while smaller organisations saw the management of QRs as more ad hoc. When prioritising QRs, Accuracy was considered the most important priority followed by Security and Reliability. The main challenges that organisations face in their management of QRs is defining and quantifying these requirements.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328174,no
384,Modular based multiple test case prioritization,"Cost and time effective reliable test case prioritization technique is the need for present software industries. The test case prioritization for the entire program consumes more time and the selection of test case for entire software is also affecting the test performance. In order to alleviate the above problem a new methodology using modular based test case prioritization is proposed for regression testing. In this method the program is divided into multiple modules. The test cases corresponding to each module is prioritized first. In the second stage, the individual modular based prioritized test suites are combined together and further prioritized for the whole program. This method is verified for fault coverage and compared with overall program test case prioritization method. The proposed method is assessed using three standard applications namely University Students Monitoring System, Hospital Management System, and Industrial Process Operation System. The empirical studies show that the proposed algorithm is significantly performed well. The superiority of the proposed method is also highlighted.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510205,yes
385,MOTCP: A tool for the prioritization of test cases based on a sorting genetic algorithm and Latent Semantic Indexing,"Test prioritization techniques can be used to determine test case ordering and early discover faults in source code. Several of these techniques exploit a single objective function, e.g., code or requirements coverage. In this tool demo paper, we present MOTCP, a software tool that implements a multi-objective test prioritization technique based on the information related to the code and requirements coverage, as well as the execution cost of each test case. To establish users' and system requirements coverage, the MOTCP uses Latent Semantic Indexing to recover traceability links among application source code and requirements specifications. The test case ordering is then obtained by applying a non-dominated sorting genetic algorithm.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405346,yes
386,On Capturing Effects of Modifications as Data Dependencies,"Dependence analysis on an Extended Finite State Machine (EFSM) representation of the requirements of a system under test has been used in requirements-based regression testing for regression test suite (RTS) reduction (reducing the size of a given test suite by eliminating redundancies), for RTS prioritization (ordering test cases in a given test suite for early fault detection) or for RTS selection (selecting a subset of a test suite covering the identified dependencies). These particular uses of dependence analysis are based on definitions of various types of control and data dependencies (between transitions in an EFSM) caused by a given set of modifications on the requirements. This abstract considers the definitions of data dependencies, gives examples of incompleteness of existing definitions, and presents insights on completing these definitions.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340172,no
387,On the Fault-Detection Capabilities of Adaptive Random Test Case Prioritization: Case Studies with Large Test Suites,"An adaptive random (AR) testing strategy has recently been developed and examined by a growing body of research. More recently, this strategy has been applied to prioritizing regression test cases based on code coverage using the concepts of Jaccard Distance (JD) and Coverage Manhattan Distance (CMD). Code coverage, however, does not consider frequency, furthermore, comparison between JD and CMD has not yet been made. This research fills the gap by first investigating the fault-detection capabilities of using frequency information for AR test case prioritization, and then comparing JD and CMD. Experimental results show that ""coverage"" was more useful than ""frequency"" although the latter can sometimes complement the former, and that CMD was superior to JD. It is also found that, for certain faults, the conventional ""additional"" algorithm (widely accepted as one of the best algorithms for test case prioritization) could perform much worse than random testing on large test suites.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149572,yes
388,Oracle-Centric Test Case Prioritization,"Recent work in testing has demonstrated the benefits of considering test oracles in the testing process. Unfortunately, this work has focused primarily on developing techniques for generating test oracles, in particular techniques based on mutation testing. While effective for test case generation, existing research has not considered the impact of test oracles in the context of regression testing tasks. Of interest here is the problem of test case prioritization, in which a set of test cases are ordered to attempt to detect faults earlier and to improve the effectiveness of testing when the entire set cannot be executed. In this work, we propose a technique for prioritizing test cases that explicitly takes into account the impact of test oracles on the effectiveness of testing. Our technique operates by first capturing the flow of information from variable assignments to test oracles for each test case, and then prioritizing to ``cover'' variables using the shortest paths possible to a test oracle. As a result, we favor test orderings in which many variables impact the test oracle's result early in test execution. Our results demonstrate improvements in rate of fault detection relative to both random and structural coverage based prioritization techniques when applied to faulty versions of three synchronous reactive systems.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405379,yes
389,Predicting the priority of a reported bug using machine learning techniques and cross project validation,"In bug repositories, we receive a large number of bug reports on daily basis. Managing such a large repository is a challenging job. Priority of a bug tells that how important and urgent it is for us to fix. Priority of a bug can be classified into 5 levels from PI to P5 where PI is the highest and P5 is the lowest priority. Correct prioritization of bugs helps in bug fix scheduling/assignment and resource allocation. Failure of this will result in delay of resolving important bugs. This requires a bug prediction system which can predict the priority of a newly reported bug. Cross project validation is also an important concern in empirical software engineering where we train classifier on one project and test it for prediction on other projects. In the available literature, we found very few papers for bug priority prediction and none of them dealt with cross project validation. In this paper, we have evaluated the performance of different machine learning techniques namely Support Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbors (KNN) and Neural Network (NNet) in predicting the priority of the newly coming reports on the basis of different performance measures. We performed cross project validation for 76 cases of five data sets of open office and eclipse projects. The accuracy of different machine learning techniques in predicting the priority of a reported bug within and across project is found above 70% except Naive Bayes technique.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416595,no
390,Preemptive Regression Test Scheduling Strategies: A New Testing Approach to Thriving on the Volatile Service Environments,"A workflow-based web service may use ultra-late binding to invoke external web services to concretize its implementation at run time. Nonetheless, such external services or the availability of recently used external services may evolve without prior notification, dynamically triggering the workflow-based service to bind to new replacement external services to continue the current execution. Any integration mismatch may cause a failure. In this paper, we propose Preemptive Regression Testing (PRT), a novel testing approach that addresses this adaptive issue. Whenever such a late-change on the service under regression test is detected, PRT preempts the currently executed regression test suite, searches for additional test cases as fixes, runs these fixes, and then resumes the execution of the regression test suite from the preemption point.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340125,no
391,Prioritization of Test Cases Using Software Agents and Fuzzy Logic,"Limited test time and restricted number of test resources confront test managers with big challenges, especially in the system test. Consequently, the test manager has to prioritize test cases before each test cycle. There is much information available for determining a reasonable prioritization order in software projects. However, due to the complexity of current software systems and the high number of existing test cases, the abundance of information relevant for prioritization is not manageable for the test manager, even with high effort. In this paper we present a concept for an automated prioritization of test cases using software agents and fuzzy logic. Our prioritization system determines the prioritization order which increases the test effectiveness and the fault detection rate.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200143,yes
392,Prioritizing demand response programs from reliability aspect,"In this paper, the impact of demand response programs (DRPs) on reliability improvement of the restructured power systems is quantified. In this regard, the demand response (DR) model which treats consistently the main characteristics of the demand curve is developed for modeling. In proposed model, some penalties for customers in case of no responding to load reduction and incentives for customers who respond to reducing their loads are considered. In order to make analytical evaluation of the reliability, a mixed integer DCOPF is proposed by which load curtailments and generation re-dispatches for each contingency state are determined. Both transmission and generation failures are considered in contingency enumeration. The proposed technique is modeled in the GAMS software and solved using CPLEX. Reliability indices for generation-side, transmission network and whole system are calculated using this technique. Different DRPs based on the DR model are implemented over the IEEE RTS 24-bus test system, and reliability indices for different parties are calculated. Afterward, using proposed performance index, the priority of the considered programs is determined from view point of different market participants.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221578,no
393,Quality Playbook: Ensuring Release to Release Improvement,"Summary form only given. Before a major feature release is made available to customers, it is important to be able to anticipate if the release will be of lesser quality than its predecessor release. Our research group has developed models that use development and test times, resource levels, code added, and bugs found and fixed (or not fixed) to predict whether or not a new feature release will achieve a key quality goal - to be of better quality than its predecessor release. If the release quality prediction models, developed early in the development branches integration phase, indicate a likely upcoming quality problem in the field, another set of predictive models ('playbook' models) are then developed and used by our team to identify development or test practices that are in need of improvement. These playbook models are key components of what we call 'quality playbooks,' that are designed to address several objectives: . Identify 'levers' that positively influence feature release quality. Levers are in-process engineering metrics that are associated with specific development or test processes/practices and measure their adoption and effectiveness. . If possible, identify levers that can be invoked early in the lifecycle, to enable the development and test teams to improve deficient practices and remediate the current release under development. If it is not possible to identify early levers but possible to identify levers later in the lifecycle, we can only change deficient practices to improve the quality of future successor releases. . Determine the potential quality impact of changes suggested by the profile of significant levers. Low impact levers are likely not to be addressed by development teams. . Determine the resource and schedule investments needed to change and implement practices: Training, disruption, additional engineering time, etc. . Using impact and investment calculations identify which practices to change, either for the current release or just for subsequent releases. Develop a prioritization/ROI scheme to provide planning guidance to development and test teams. . Identify specific practice changes needed, or new practices to adopt. . Design and plan pilot programs to test the models, including the impact and investment components. Using this 'playbook' approach, our team has developed models for 31 major feature releases that are resident on 11 different hardware platforms. These models have identified six narrowly-defined classes of metrics that include both actionable levers and 'indicator' metrics that correlate well with release quality. (Indicator metrics do also correlate well, but are less specifically actionable.) The models for these six classes of metrics (and their associated practices) include strong levers and strong indicators for all releases and platforms thus far examined. Impact and investment results are also described in this paper, as are pilot programs that have tested the validity of the modeling and business calculation results. Two additional large-scale pilots of the 'playbook' approach are underway, and these are also described.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405417,no
394,Resilient system design for Prognosis and Health Monitoring of an ocean power generator,"In this paper we introduce a new methodology that integrates system resilience engineering and hazard analysis into complex system design. We then demonstrate its performance by applying it to the design of a Prognosis and Health Monitoring (PHM) system for an ocean current power generator. Three common methodologies for system hazard analysis were tested by applying them to the PHM system's network topology architecture; STAMP-based Process Analysis (STPA), Hazard and Operability Analysis (HAZOP), and a Resilience Engineering, Heuristic-based approach. While all three approaches adequately revealed most PHM system hazards, which assisted in identifying the means with which to mitigate them, none of the approaches fully addressed the multi-state dimensionality of the sub-components of the system, missing risky and hazardous scenarios. We developed the System Hazard Indication and Extraction Learning Diagnosis (SHIELD) methodology for system hazard analysis and resilient design. SHIELD integrates state space analysis into the hazard analysis process in order to facilitate the location of undiscovered hazard scenarios. Our approach uses recursive, top-down system decomposition with subsystem, interface, and process cycle identification. Then, a bottom-up recursive evaluation is completed where we analyze the subsystem state space and state transitions with regard to hazards/failures in process cycles. This yields a comprehensive list of failure states and scenarios. Finally, a top-down prioritized application of resilient engineering heuristics which address hazard scenarios is prescribed. This final phase results in a comprehensive, complete analysis of complex system architectures forcing resilience into the final system design.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189490,no
395,School that has a vision,"The article is devoted to presentation of the concept and results of the Private secondary professional school in Poprad that is actually realization of the vision of two high school teachers on how education might look like at high school. Based on the inclusion of Private secondary professional school into the school system and the subsequent approval of the experimental verification of branches of study the founder began to build a school, which is based on two main priorities - high quality educational process and modern hardware and software. The school where students have their own laptops, use electronic books along with an educational portal, do test and homework via the internet and where parents can check the results by means of the electronic register of students' grades. The school which offers their students new perspective branches of study tailor-made according to the current and future requirements of the market, i.e. without the school subjects that are not relevant anymore, and with an ongoing innovative curriculum; the school with enthusiastic highly qualified teachers using new methods and forms of teaching including modern technique. The school that partners with the university sector to ensure the follow-up study of the graduates provides video conferencing for specialized subjects and conversational courses with schools abroad. Private secondary professional school, Ul.29.augusta 4812 in Poprad, officially began on 01.09.2008, the founder of which is Tatransk?­ Akad??mia.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418322,no
396,Selecting an appropriate framework for value-based requirements prioritization,"There are usually more requirements than feasible in a given schedule. Thus, it's imperative to be able to choose the most valuable ones for implementation to ensure the delivery of a high value software system. There are myriad requirements prioritization frameworks and selecting the most appropriate one is a decision problem in its own right. In this paper we present our approach in selecting the most appropriate value based requirements prioritization framework as per the requirements of our stakeholders. Based on our analysis a single framework was selected, validated by requirements engineers and project managers and deployed for company-wide use by a major IT player in India.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345819,no
397,Size-Constrained Regression Test Case Selection Using Multicriteria Optimization,"To ensure that a modified software system has not regressed, one approach is to rerun existing test cases. However, this is a potentially costly task. To mitigate the costs, the testing effort can be optimized by executing only a selected subset of the test cases that are believed to have a better chance of revealing faults. This paper proposes a novel approach for selecting and ordering a predetermined number of test cases from an existing test suite. Our approach forms an Integer Linear Programming problem using two different coverage-based criteria, and uses constraint relaxation to find many close-to-optimal solution points. These points are then combined to obtain a final solution using a voting mechanism. The selected subset of test cases is then prioritized using a greedy algorithm that maximizes minimum coverage in an iterative manner. The proposed approach has been empirically evaluated and the results show significant improvements over existing approaches for some cases and comparable results for the rest. Moreover, our approach provides more consistency compared to existing approaches.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928351,yes
398,Software testing suite prioritization using multi-criteria fitness function,"Regression testing is the process of validating modifications introduced in a system during software maintenance. It is an expensive, yet an important process. As the test suite size is very large, system retesting consumes large amount of time and computing resources. Unfortunately, there may be insufficient resources to allow for the re-execution of all test cases during regression testing. Testcase prioritization techniques aim to improve the effectiveness of regression testing, by ordering the testcases so that the most beneficial are executed first with higher priority. The objective of test case prioritization is to detect faults as early as possible. An approach for automating the test case prioritization process using genetic algorithm with Multi-Criteria Fitness function is presented. It uses multiple control flow coverage metrics. These metrics measure the degree of coverage of conditions, multiple conditions and statements that the test case covers. Theses metrics are weighted by the number of faults revealed and their severity. The proposed Multi-criteria technique showed superior results compared to similar work.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523563,no
399,Test Case Prioritization Due to Database Changes in Web Applications,"A regression test case prioritization (TCP) technique reorders test cases for regression testing to achieve early fault detection. Most TCP techniques have been developed for regression testing of source code in an application. Most web applications rely on a database server for serving client requests. Any changes in the database result in erroneous client interactions and may bring down the entire web application. However, most prioritization techniques are unsuitable for prioritizing test suites for early detection of changes in databases. There are very few proposals in the literature for prioritization of test cases that can detect faults in the database early. We propose a new automated TCP technique for web applications that automatically identifies the database changes, prioritizes test cases related to database changes and executes them in priority order to detect faults early.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200175,yes
400,Test Case Prioritization for Regression Testing Based on Function Call Path,"Test case prioritization is an effective and practical technique of regression testing. It is helpful to increase the efficiency of regression testing by sorting and executing test cases according to their importance. Static paths on function call obtained by analyzing the source code, combined with the dynamic path after executing test cases, the correspondence is built between test cases and the static paths, identifying the changes which software developers modify program to correct defects, giving different priority to test case based on path coverage , test cases are selected in accordance with their priorities in regression testing. Firstly, the background and related concept of test case prioritization are introduced. And then, the relevant research work is outlined, a set of new prioritization algorithms are proposed; implementation and analysis of the algorithm are given finally.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6301421,yes
401,Test case prioritization incorporating ordered sequence of program elements,"Test suites often grow very large over many releases, such that it is impractical to re-execute all test cases within limited resources. Test case prioritization, which rearranges test cases, is a key technique to improve regression testing. Code coverage information has been widely used in test case prioritization. However, other important information, such as the ordered sequence of program elements measured by execution frequencies, was ignored by previous studies. It raises a risk to lose detections of difficult-to-find bugs. Therefore, this paper improves the similarity-based test case prioritization using the ordered sequence of program elements measured by execution counts. The empirical results show that our new technique can increase the rate of fault detection more significantly than the coverage-based ART technique. Moreover, our technique can detect bugs in loops more quickly and be more cost-benefits than the traditional ones.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228980,yes
402,Towards reliable web applications: ISO 19761,"This research adopts a new scenario-based black box testing methodology for testing web applications. It combines a black box testing strategy with the functions (scenarios) measured by the COSMIC-FFP measurement procedure (ISO/IEC 19761 standard) to produce an optimal set of test cases. This testing approach shows its applicability during all the development phases. Moreover, it can be applied during the early development phase once the specifications have been documented as well as after the development phase where we don't have the access to the code. This paper also considers the use of a functional complexity measure for assigning priorities to the generated test cases. Finally, those concepts have been applied on part of Online Banking System as a case study.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6389396,no
403,Use of embedded intelligence in tactical grids for energy surety and fuel conservation,"This paper describes a system for creating an energy-sharing infrastructure, effectively creating redundant sources of energy supply and significantly reducing the logistical burdens associated with providing power. An intelligent power management and power grid system has been developed and tested. This system optimizes performance and efficiency through local and system-level autonomous controls. The grid system was based on existing military, trailer-mounted, mobile power equipment. A reduction in fuel consumption of 36 percent was observed. In addition, prioritized load shedding was demonstrated as a means to prevent the generators from being overloaded.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345122,no
404,Using Non-redundant Mutation Operators and Test Suite Prioritization to Achieve Efficient and Scalable Mutation Analysis,"Mutation analysis is a powerful and unbiased technique to assess the quality of input values and test oracles. However, its application domain is still limited due to the fact that it is a time consuming and computationally expensive method, especially when used with large and complex software systems. Addressing these challenges, this paper makes several contributions to significantly improve the efficiency of mutation analysis. First, it investigates the decrease in generated mutants by applying a reduced, yet sufficient, set of mutants for replacing conditional (COR) and relational (ROR) operators. The analysis of ten real-world applications, with 400,000 lines of code and more than 550,000 generated mutants in total, reveals a reduction in the number of mutants created of up to 37% and more than 25% on average. Yet, since the isolated use of non-redundant mutation operators does not ensure that mutation analysis is efficient and scalable, this paper also presents and experimentally evaluates an optimized workflow that exploits the redundancies and runtime differences of test cases to reorder and split the corresponding test suite. Using the same ten open-source applications, an empirical study convincingly demonstrates that the combination of non-redundant operators and prioritization leveraging information about the runtime and mutation coverage of tests reduces the total cost of mutation analysis further by as much as 65%.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405400,no
405,Using Prioritized Disk Service to Expedite Program Execution,"Computer systems often host multiple programs in execution simultaneously. Among those running programs, some may be important and time-critical, which users would expect them to finish their execution as soon as possible. Generally speaking, the course of program execution includes CPU operation and hard disk operation (disk I/O). For the CPU operation, modern computer systems have the ability to adjust the CPU scheduling sequence according to program priority. However, most computer systems do not have effective ways to conduct disk I/O based on program priority. The Linux operating system has been widely used in many areas. It supports several disk schedulers. The Complete Fair Queuing (CFQ) and the Anticipatory Scheduling (AS) are among those most well-known. Currently, CFQ is the default disk scheduler in the Linux operating system. AS is the predecessor of CFQ. Unfortunately, CFQ only offers prioritized disk I/O to some extent through the tool ""ionice"", while AS does not provide any prioritized service at all. We propose and implement a new disk scheduler, namely Prioritized Anticipatory Scheduling (PAS), by adding schemes of supporting prioritized disk I/O into AS in the Linux kernel. Our experimental results show that PAS surpasses CFQ with ionice for the vast majority of all test cases. Compared with AS, PAS can improve the performance of programs with high disk I/O priority by up to 71.88%.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332286,no
406,Value-Based Coverage Measurement in Requirements-Based Testing: Lessons Learned from an Approach Implemented in the TOSCA Testsuite,"Testing is one of the most widely practiced quality assurance measures and also one of the most resource-intensive activities in software development. Still, however, most of the available methods, techniques and tools for software testing are value-neutral and do not realize the potential value contribution of testing. In this paper we present an approach for value-based coverage measurement that can be used to align the testing effort with the achievable value associated with requirements and functional units. It has been implemented as part of a commercial test tool and was successfully applied in real-world projects. The results demonstrated its ability to adequately capture the distribution of the business value and risks involved in different requirements. The paper concludes with sharing important lessons learned from developing value-based coverage measurement in the practical setting of commercial tool development and real-world test projects.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328176,no
407,A Comparison of Different Defect Measures to Identify Defect-Prone Components,"(Background) Defect distribution in software systems has been shown to follow the Pareto rule of 20-80. This motivates the prioritization of components with the majority of defects for testing activities. (Research goal) Are there significant variations between defective components and architectural hotspots identified by other defect measures? (Approach) We have performed a study using post-release data of an industrial Smart Grid application with a well-maintained defect tracking system. Using the Pareto principle, we identify and compare defect-prone and hotspots components based on four defect metrics. Furthermore, we validated the quantitative results against qualitative data from the developers. (Results) Our results show that at the top 25% of the measures 1) significant variations exist between the defective components identified by the different defect metrics and that some of the components persist as defective across releases 2) the top defective components based on number of defects could only identify about 40% of critical components in this system 3) other defect metrics identify about 30% additional critical components 4) additional quality challenges of a component could be identified by considering the pair wise intersection of the defect metrics. (Discussion and Conclusion) Since a set of critical components in the system is missed by using largest-first or smallest-first prioritization approaches, this study, therefore, makes a case for an all-inclusive metrics during defect model construction such as number of defects, defect density, defect severity and defect correction effort to make us better understand what comprises defect-prone components and architectural hotspots, especially in critical applications.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693238,no
408,A Fuzzy Expert System for Cost-Effective Regression Testing Strategies,"Different testing environments and software change characteristics can affect the choice of regression testing techniques. In our prior work, we developed adaptive regression testing (ART) strategies to investigate this problem. While the ART strategies showed promising results, we also found that the multiple criteria decision making processes required for the ART strategies are time-consuming, often inaccurate and inconsistent, and limited in their scalability. To address these issues, in this research, we develop and empirically study a fuzzy expert system (FESART) to aid decision makers in choosing the most cost-effective technique for a particular software version. The results of our study show that FESART is consistently more cost-effective than the previously proposed ART strategies. One of the biggest contributors to FESART being more cost-effective is the reduced time required to apply the strategy. This contribution has significant impact because a strategy that is less time-consuming will be easier for researchers and practitioners to adopt, and will provide even greater cost-savings for regression testing sessions.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676871,no
409,A novel approach for test case prioritization,"The process of verifying the modified software in the maintenance phase is called Regression Testing. The size of the regression test suite and its selection process is a complex task for regression testers because of time and budget constraints. In this research paper, new Prioritization technique based on hamming distance has been proposed. It is illustrated using an example and found that it produces good results. Average Percentage of Fault Detection (APFD) metrics and charts has been used to show the effectiveness of proposed algorithm.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724209,yes
410,A refactoring-based approach for test case selection and prioritization,"Refactoring edits, commonly applied during software development, may introduce faults in a previously-stable code. Therefore, regression testing is usually applied to check whether the code maintains its previous behavior. In order to avoid rerunning the whole regression suite, test case prioritization techniques have been developed to order test cases for earlier achievement of a given goal, for instance, improving the rate of fault detection during regression testing execution. However, as current techniques are usually general purpose, they may not be effective for early detection of refactoring faults. In this paper, we propose a refactoring-based approach for selecting and prioritizing regression test cases, which specializes selection/prioritization tasks according to the type of edit made. The approach has been evaluated through a case study that compares it to well-known prioritization techniques by using a real open-source Java system. This case study indicates that the approach can be more suitable for early detection of refactoring faults when comparing to the other prioritization techniques.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595798,yes
411,A Study in Prioritization for Higher Strength Combinatorial Testing,"Recent studies have shown that combinatorial interaction testing (CIT) is an effective fault detection technique and that early fault detection can be improved by ordering test suites by interaction based prioritization approaches. Despite research that has shown that higher strength CIT improves fault detection, there have been fewer studies that aim to understand the impact of prioritization based on higher strength criteria. In this paper, we aim to understand how interaction based prioritization techniques perform, in terms of early fault detection when we prioritize based on 3-way interactions. We generalize prior work on prioritizing using 2-way interactions to t-way prioritization, and empirically evaluate this on three open source subjects, across multiple versions of each. We examine techniques that prioritize both existing CIT suites as well as generate new ones in prioritized order. We find that early fault detection can be improved when prioritizing 3-way CIT test suites by interactions that cover more code, and to a lesser degree when generating tests in prioritized order. Our techniques that work only from the specification, appear to work best with 2-way generation.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571645,yes
412,A Uniform Representation of Hybrid Criteria for Regression Testing,"Regression testing tasks of test case prioritization, test suite reduction/minimization, and regression test selection are typically centered around criteria that are based on code coverage, test execution costs, and code modifications. Researchers have developed and evaluated new individual criteria; others have combined existing criteria in different ways to form what we--and some others--call hybrid criteria. In this paper, we formalize the notion of combining multiple criteria into a hybrid. Our goal is to create a uniform representation of such combinations so that they can be described unambiguously and shared among researchers. We envision that such sharing will allow researchers to implement, study, extend, and evaluate the hybrids using a common set of techniques and tools. We precisely formulate three hybrid combinations, Rank, Merge, and Choice, and demonstrate their usefulness in two ways. First, we recast, in terms of our formulations, others' previously reported work on hybrid criteria. Second, we use our previous results on test case prioritization to create and evaluate new hybrid criteria. Our findings suggest that hybrid criteria of others can be described using our Merge and Rank formulations, and that the hybrid criteria we developed most often outperformed their constituent individual criteria.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6484067,no
413,Adaptive Test-Case Prioritization Guided by Output Inspection,"Test-case prioritization is to schedule the execution order of test cases so as to maximize some objective (e.g., revealing faults early). The existing test-case prioritization approaches separate the process of test-case prioritization and the process of test-case execution by presenting the execution order of all test cases before programmers start running test cases. As the execution information of the modified program is not available for the existing test-case prioritization approaches, these approaches mainly rely on only the execution information of the previous program before modification. To address this problem, we present an adaptive test-case prioritization approach, which determines the execution order of test cases simultaneously during the execution of test cases. In particular, the adaptive approach selects test cases based on their fault-detection capability, which is calculated based on the output of selected test cases. As soon as a test case is selected and runs, the fault-detection capability of each unselected test case is modified according to the output of the latest selected test case. To evaluate the effectiveness of the proposed adaptive approach, we conducted an experimental study on eight C programs and four Java programs. The experimental results show that the adaptive approach is usually significantly better than the total test-case prioritization approach and competitive to the additional test-case prioritization approach. Moreover, the adaptive approach is better than the additional approach on some subjects (e.g, replace and schedule).",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649818,yes
414,Bridging the gap between the total and additional test-case prioritization strategies,"In recent years, researchers have intensively investigated various topics in test-case prioritization, which aims to re-order test cases to increase the rate of fault detection during regression testing. The total and additional prioritization strategies, which prioritize based on total numbers of elements covered per test, and numbers of additional (not-yet-covered) elements covered per test, are two widely-adopted generic strategies used for such prioritization. This paper proposes a basic model and an extended model that unify the total strategy and the additional strategy. Our models yield a spectrum of generic strategies ranging between the total and additional strategies, depending on a parameter referred to as the p value. We also propose four heuristics to obtain differentiated p values for different methods under test. We performed an empirical study on 19 versions of four Java programs to explore our results. Our results demonstrate that wide ranges of strategies in our basic and extended models with uniform p values can significantly outperform both the total and additional strategies. In addition, our results also demonstrate that using differentiated p values for both the basic and extended models with method coverage can even outperform the additional strategy using statement coverage.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606565,yes
415,Bypassing Code Coverage Approximation Limitations via Effective Input-Based Randomized Test Case Prioritization,"Test case prioritization assigns the execution priorities of the test cases in a given test suite with the aim of achieving certain goals. Many existing test case prioritization techniques however assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in many software development projects. This paper proposes a novel family of LBS techniques. They make adaptive tree-based randomized explorations with an adaptive randomized candidate test set strategy to diversify the explorations among the branches of the exploration trees constructed by the test inputs in the test suite. They get rid of the assumption on the historical correlation of code coverage between program versions. Our techniques can be applied to programs with or without any previous versions, and hence are more general than many existing test case prioritization techniques. The empirical study on four popular UNIX utility benchmarks shows that, in terms of APFD, our LBS techniques can be as effective as some of the best code coverage-based greedy prioritization techniques ever proposed. We also show that they are significantly more efficient and scalable than the latter techniques.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649820,yes
416,CDM-Suite: An Attributed Test Selection Tool,"Many reasons lead to embedded systems getting more complex than ever. The higher integration and interconnection and the additional effort spend for safety-critical functions, require new techniques to support the testing process. Beneath using model-driven test techniques, test engineers need support to take the right decisions on test case selection and prioritization during the whole development process. Available approaches on test selection and prioritization lack in adaptability to different testing techniques. We previously presented our approach of modeling components and dependences of a system, such as black-box systems, using Component-Dependency-Models (CDMs) as shown in earlier publications. In this paper we present our tool CDM-Suite in detail and show the way it can be used by test engineers. We therefore start with the presentation of our approach and discussing related tools. The next section describes the graphical user interface (GUI) and the different ways of interaction for data generation. This point includes the introduction of metrics build upon fuzzy logic and graph analysis. At last we show how to interpret analysis results and derive knowledge for test case selection and prioritization. We sum the paper up by giving a conclusion and a presentation outline.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569753,no
417,Closed-loop analysis of soft decisions for serial links,"We describe the benefit of using closed-loop measurements for a radio receiver paired with a counterpart transmitter. We show that real-time analysis of the soft decision output of a receiver can provide rich and relevant insight far beyond the traditional hard-decision bit error rate (BER) test statistic. We describe a Soft Decision Analyzer (SDA) implementation for closed-loop measurements on single- or dual-(orthogonal) channel serial data communication links. The analyzer has been used to identify, quantify, and prioritize contributors to implementation loss in live-time during the development of software defined radios. This test technique gains importance as modern receivers are providing soft decision symbol synchronization as radio links are challenged to push more data and more protocol overhead through noisier channels, and software-defined radios (SDRs) use error-correction codes that approach Shannon's theoretical limit of performance.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737541,no
418,Compositional analysis of switched Ethernet topologies,"In this paper we study distributed automotive control applications whose tasks are mapped onto different ECUs communicating via a switched Ethernet network. As traditional automotive communication buses like CAN, FlexRay, LIN and MOST are gradually reaching their performance limits because of the increasing complexity of automotive architectures and applications, Ethernet-based in-vehicle communication systems have attracted a lot of attention in recent times. However, currently there is very little work on systematic timing analysis for Ethernet which is important for its deployment in safety-critical scenarios like in an automotive architecture. In this work, we propose a compositional timing analysis technique that takes various features of switched Ethernet into account like network topology, frame priorities, communication delay, memory requirement on switches, performance, etc. Such an analysis technique is particularly suitable during early design phases of automotive architectures and control software deployment. We demonstrate its use in analyzing mixed-criticality traffic patterns consisting of messages from performance-oriented control loops and timing-sensitive real-time tasks. We further evaluate the tightness of the obtained analytical bounds with an OMNeT++ based network simulation environment, which involves long simulation time and does not provide formal guarantees.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513677,no
419,Coverage-Based Test Case Prioritisation: An Industrial Case Study,This paper presents an industrial case study of coverage-based prioritisation techniques on a real world system with real regression faults. The study evaluates four common and different test case prioritisation techniques and examines the effects of using various coverage criteria on the fault detection rates of the prioritised test suites. The results show that prioritisation techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information does not significantly enhance fault detection rates.,2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569742,yes
420,Data Centers as Software Defined Networks: Traffic Redundancy Elimination with Wireless Cards at Routers,"We propose a novel architecture of data center networks (DCN), which adds wireless network card to both servers and routers. Existing traffic redundancy elimination (TRE) mechanisms reduce link loads and increase network capacity in several environments by removing strings that have appeared in earlier packets through encoding and decoding them several hops downstream. This article is the first to explore TRE mechanisms in large-scale DCNs and the first to exploit cooperative TRE among servers. Moreover, it also achieves the `logically centralized' control over the physically distributed states in emerging software defined networks (SDN) paradigm, by sharing information among servers and routers in data centers with wireless cards. We first formulate the TREDaCeN (TRE in Data Center Networks) problem and reduce the cycle cover problem to prove that finding an optimal caching task assignment for TREDaCeN problem is NP-hard. We further describe an offline TREDaCeN algorithm which is proved to have good approximation ratio. We then discuss efficient online zero-delay and semi-distributed implementations of TREDaCeN supported by physical proximity of servers and routers, enabling status updates in a single wireless transmission, using an efficient prioritized schedule. We also address online cache replacement and consistency of information in servers and routers with and without delay. Our framework is tested on different parameters and shows superior performance in comparison to other mechanisms (imported directly to this setting). Our results show the robustness and the trade-off between the `logically centralized' implementation and the overhead on handling inconsistency of distributed information in DCN.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678112,no
421,Defect Prioritization in the Software Industry: Challenges and Opportunities,"Defect prioritization is a decision making process wherein stakeholders determine the temporal order of open defects to be fixed. It is critical to the software development lifecycle as the decisions made during this process directly affect release planning, resource management, and maintenance costs. In fact, defect prioritization is complex as many factors need to be taken into consideration and the decisions made can be subjective or incorporate inherent knowledge and intuition of decision makers. We believe that managing the complexities of the decision making process can provide valuable support and help in uncovering any inconsistencies in the interpretation of criteria to prioritize defects. In this paper, we explore the defect triaging process in Research In Motion to gain a better understanding of the shortcomings and challenges of the current practices. Based on our findings, we sketch some research directions to improve industrial software defect prioritization.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569718,no
422,Developing new Automatic Test Equipments (ATE) using systematic design approaches,"Keeping Automatic Test Equipments (ATE) current with technology is one of the major challenges in automatic testing world. Needs and priorities can quickly evolve throughout the life cycle of ATEs and handling obsolescence via performing upgrades on hardware and software can be impossible after several years. While developing Test Program Sets (TPS), if existing ATE systems cannot meet the necessary requirements without inserting extra test devices or decreasing test coverage, then designing a new ATE can be seen inevitable. If a new ATE system is to be designed, it is very crucial that the requirements for the new ATE system should be identified before design process begins. Determining the requirements is a very critical stage in designing ATE because if enough effort is not focused and extended analysis is not carried out on determining the requirements, then the newly formed ATE system will likely fail to cover the test requirements of the DUTs. Setting up the hardware and software architecture is the next stage after the process of determining the requirements of the ATE system. Practical and cost effective solutions should be considered without compromising performance and capabilities of the test devices. The architectures should be suitable for future enhancements to the system. Throughout the design process, the design requirements, critical design descriptions, verification and validation procedures should be clearly documented and reviewed with relevant engineers. In this paper, the design process of a new ATE system by using systematic design approach is discussed. This process is followed during the design of the ATE system which is under use from the beginning of the year 2013. The challenges in the design process, determining the requirements and the formation of hardware and software architecture are explained in detail benefiting from real experiences.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645035,no
423,Efficient Automated Program Repair through Fault-Recorded Testing Prioritization,"Most techniques for automated program repair use test cases to validate the effectiveness of the produced patches. The validation process can be time-consuming especially when the object programs ship with either lots of test cases or some long-running test cases. To alleviate the cost for testing, we first introduce regression test prioritization insight into the area of automated program repair, and present a novel prioritization technique called FRTP with the goal of reducing the number of test case executions in the repair process. Unlike most existing prioritization techniques frequently requiring additional cost for gathering previous test executions information, FRTP iteratively extracts that information just from the repair process, and thus incurs trivial performance lose. We also built a tool called TrpAutoRepair, which implements our FRTP technique and has the ability of automatically repairing C programs. To evaluate TrpAutoRepair, we compared it with GenProg, a state-of-the-art tool for automated C program repair. The experiment on the 5 subject programs with 16 real-life bugs provides evidence that TrpAutoRepair performs at least as good as GenProg in term of success rate, in most cases (15/16), TrpAutoRepair can significantly improve the repair efficiency by reducing efficiently the test case executions when searching a valid patch in the repair process.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676889,yes
424,Evolutionary Search Algorithms for Test Case Prioritization,"To improve the effectiveness of certain performance goals, test case prioritization techniques are used. These technique schedule the test cases in particular order for execution so as to increase the efficacy in meeting the performance goals. For every change in the program it is considered inefficient to re-execute each and every test case. Test case prioritization techniques arrange the test cases within a test suite in such a way that the most important test case is executed first. This process enhances the effectiveness of testing. This algorithm during time constraint execution has been shown to have detected maximum number fault while including the sever test cases.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918806,yes
425,Getting more from requirements traceability: Requirements testing progress,"Requirements Engineering (RE) and Testing are important steps in many software development processes. It is critical to monitor the progress of the testing phase to allocate resources (person-power, time, computational resources) properly, and to make sure the prioritization of requirements are reflected during testing, i.e. more critical requirements are given higher priority and tested well. In this paper, we propose a new metric to help stakeholders monitor the progress of the testing phase from a requirements perspective, i.e. which requirements are tested adequately, and which ones insufficiently. Unlike existing progress related metrics, such as code coverage and MC/DC (modified condition/decision) coverage, this metric is on the requirements level, not source code level. We propose to automatically reverse engineer this metric from the existing test cases of a system. We also propose a method to evaluate this metric, and report the results of three case studies. On these case studies, our technique obtains results within 75.23% - 91.11% of the baseline on average.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620148,no
426,History-Based Test Case Prioritization with Software Version Awareness,"Test case prioritization techniques schedule the test cases in an order based on some specific criteria so that the tests with better fault detection capability are executed at an early position in the regression test suite. Many existing test case prioritization approaches are code-based, in which the testing of each software version is considered as an independent process. Actually, the test results of the preceding software versions may be useful for scheduling the test cases of the later software versions. Some researchers have proposed history-based approaches to address this issue, but they assumed that the immediately preceding test result provides the same reference value for prioritizing the test cases of the successive software version across the entire lifetime of the software development process. Thus, this paper describes ongoing research that studies whether the reference value of the immediately preceding test results is version-aware and proposes a test case prioritization approach based on our observations. The experimental results indicate that, in comparison to existing approaches, the presented one can schedule test cases more effectively.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601820,yes
427,"Leveraging the Crowd: How 48,000 Users Helped Improve Lync Performance","Performance is a critical component of customer satisfaction with network-based applications. Unfortunately, accurately evaluating the performance of collaborative software that operates in extremely heterogeneous environments is difficult with traditional techniques such as modeling workloads or testing in controlled environments. To evaluate performance of an application in the wild during development, the authors deployed early versions of the software, collecting performance data from application users for key usage scenarios. The analysis package they used produces visualizations to help development teams identify and prioritize performance issues by focusing on performance early in the development cycle, evaluating progress, identifying defects, and estimating timelines.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509371,no
428,Managing technical debt: An industrial case study,"Technical debt is the consequence of trade-offs made during software development to ensure speedy releases. The research community lacks rigorously evaluated guidelines to help practitioners characterize, manage and prioritize debt. This paper describes a study conducted with an industrial partner during their implementation of Agile development practices for a large software development division within the company. The report contains our initial findings based on ethnographic observations and semi-structured interviews. The goal is to identify the best practices regarding managing technical debt so that the researchers and the practitioners can further evaluate these practices to extend their knowledge of the technical debt metaphor. We determined that the developers considered their own taxonomy of technical debt based on the type of work they were assigned and their personal understanding of the term. Despite management's high-level categories, the developers mostly considered design debt, testing debt and defect debt. In addition to developers having their own taxonomy, assigning dedicated teams for technical debt reduction and allowing other teams about 20% of time per sprint for debt reduction are good initiatives towards lowering technical debt. While technical debt has become a well-regarded concept in the Agile community, further empirical evaluation is needed to assess how to properly apply the concept for various development organizations.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608672,no
429,On Combining Model-Based Analysis and Testing,"Testing a computer system is a challenging task, both due to the large number of possible test cases and the limited resources allocated for testing activities. This means that only a subset of all possible test cases can be chosen to test a system, and therefore the decision on the selection of test cases becomes important. The result of static analysis of a system can be used to help with this decision, in the context of model-based development of systems, this means that the analysis performed on a system model can be used to prioritize and guide the testing efforts. Furthermore, since models allow expression of non-functional requirements (such as performance, timing and security), model-guided testing can be used to direct testing towards specific parts of the system which have large impact on such requirements. In this paper, we focus on modeling and trade-off analysis of non-functional requirements and how static analysis helps to identify problematic parts of a system and thus guide the selection of test cases to target such parts.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614319,no
430,On the Correlation between the Effectiveness of Metamorphic Relations and Dissimilarities of Test Case Executions,"Metamorphic testing (MT) is a property-based automated software testing method. It alleviates the oracle problem by testing programs against metamorphic relations (MRs), which are necessary properties among multiple executions of the target program. For a given problem, usually more than one MR can be identified. It is therefore of practical importance for testers to know the nature of good MRs, that is, which MRs are likely to have higher chances of revealing failures. To address this issue we investigate the correlation between the fault-detection effectiveness of MRs and the dissimilarity (distance) of test case execution profiles. Empirical study results reveal that there is a strong and statistically significant positive correlation between the fault-detection effectiveness and the distance. The findings of this research can help to develop automated means of selecting/prioritizing MRs for cost-effective metamorphic testing.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605921,no
431,On the Gain of Measuring Test Case Prioritization,"Test case prioritization (TCP) techniques aim to schedule the order of regression test suite to maximize some properties, such as early fault detection. In order to measure the abilities of different TCP techniques for early fault detection, a metric named average percentage of faults detected (APFD) is widely adopted. In this paper, we analyze the metric APFD and explore the gain of measuring TCP techniques from a control theory viewpoint. Based on that, we propose a generalized metric for TCP. This new metric focuses on the gain of defining early fault detection and measuring TCP techniques for various needs in different evaluation scenarios. By adopting this new metric, not only flexibility can be guaranteed, but also explicit physical significance for the metric will be provided before evaluation.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649891,yes
432,On the Influence of Model Structure and Test Case Profile on the Prioritization of Test Cases in the Context of Model-Based Testing,"Test case prioritization techniques aim at defining an ordering of test cases that favor the achievement of a goal during test execution, such as revealing faults as earlier as possible. A number of techniques have already been proposed and investigated in the literature and experimental results have discussed whether a technique is more successful than others. However, in the context of model-based testing, only a few attempts have been made towards either proposing or experimenting test case prioritization techniques. Moreover, a number of factors that may influence on the results obtained still need to be investigated before more general conclusions can be reached. In this paper, we present empirical studies that focus on observing the effects of two factors: the structure of the model and the profile of the test case that fails. Results show that the profile of the test case that fails may have a definite influence on the performance of the techniques investigated.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800188,yes
433,Optimization of test suite-test case in regression test,"Exhaustive product evolution and testing is required to ensure the quality of product. Regression testing is crucial to ensure software excellence. Regression test cases are applied to assure that new or adapted features do not relapse the existing features. As innovative features are included, new test cases are generated to assess the new functionality, and then included in the existing pool of test cases, thus escalating the cost and the time required in performing regression test and this unswervingly impacts the release, laid plan and the quality of the product. Hence there is a need to select minimal test cases that will test all the functionalities of the engineered product and it must rigorously test the functionalities that have high risk exposure. Test Suite-Test Case Refinement Technique will reduce regression test case pool size, reduce regression testing time, cost &amp; effort and also ensure the quality of the engineered product. This technique is a regression test case optimization technique that is a hybrid of Test Case Minimization based on specifications and Test Case Prioritization based on risk exposure. This approach will facilitate achievement of quality product with decreased regression testing time and cost yet uncover same amount of errors as the original test cases.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724206,no
434,Prioritizing Variable-Strength Covering Array,"Combinatorial interaction testing is a well-studied testing strategy, and has been widely applied in practice. Combinatorial interaction test suite, such as fixed-strength and variable-strength interaction test suite, is widely used for combinatorial interaction testing. Due to constrained testing resources in some applications, for example in combinatorial interaction regression testing, prioritization of combinatorial interaction test suite has been proposed to improve the efficiency of testing. However, nearly all prioritization techniques may only support fixed-strength interaction test suite rather than variable-strength interaction test suite. In this paper, we propose two heuristic methods in order to prioritize variable-strength interaction test suite by taking advantage of its special characteristics. The experimental results show that our methods are more effective for variable-strength interaction test suite by comparing with the technique of prioritizing combinatorial interaction test suites according to test case generation order, the random test prioritization technique, and the fixed-strength interaction test suite prioritization technique. Besides, our methods have additional advantages compared with the prioritization techniques for fixed-strength interaction test suite.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649874,no
435,Randomizing regression tests using game theory,"As software evolves, the number of test-cases in the regression test suites continues to increase, requiring testers to prioritize their execution. Usually only a subset of the test cases is executed due to limited testing resources. This subset is often known to the developers who may try to ƒ??gameƒ? the system by committing insufficiently tested code for parts of the software that will not be tested. In this new ideas paper, we propose a novel approach for randomizing regression test scheduling, based on Stackelberg games for deployment of scarce resources. We apply this approach to randomizing test cases in such a way as to maximize the testers' expected payoff when executing the test cases. Our approach accounts for resource limitations (e.g., number of testers) and provides a probabilistic distribution for scheduling test cases. We provide an example application of our approach showcasing the idea of using Stackelberg games for randomized regression test scheduling.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693122,yes
436,Regression Testing Prioritization Based on Model Checking for Safety-Crucial Embedded Systems,"The order in which test-cases are executed has an influence on the rate at which faults can be detected. In this paper we demonstrate how test-case prioritization can be performed with the use of model-checkers. For this, different well known prioritization techniques are adapted for model-based use. New property based prioritization techniques are introduced. In addition it is shown that prioritization can be done at test-case generation time, thus removing the need for test-suite post-processing. Several experiments for safety-crucial embedded systems are used to show the validity of these ideas.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598153,no
437,Research on optimization scheme of regression testing,"Regression testing is an important process during software development. In order to reduce costs of regression testing, research on optimization of scheme of regression testing have been done in this paper. For the purpose of reducing the number of test cases and detecting faults of programs early, this paper proposed to combine test case selection with test case prioritization. Regression testing process has been designed and optimization of testing scheme has been implemented. The criterion of test case selection is modify impact of programs, finding programs which are impacted by program modification according to modify information of programs and dependencies between programs. Test cases would be selected during test case selection. The criterion of test case prioritization is coverage ability and troubleshooting capabilities of test case. Test cases which have been selected during test case selection would be ordering in test case prioritization. Finally, the effectiveness of the new method is discussed.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818242,no
438,Reusing black box test paths for white box testing of websites,"As the numbers of web users are increasing exponentially, the software complexity is increasing exponentially and the malwares are increasing exponentially, so exhaustive and extensive testing of websites has become a necessity today. But testing of a website is not 100% exhaustive as the page explosion problem is also very usual. In this paper, we propose to reuse the basis test paths as obtained from the Page-Test-Trees (PTTs) for white box testing of websites. We traverse the same set of paths (obtained above) and test for the source code at these nodes. This saves significant amount of time required to generate test paths and hence test cases as compared to the existing approaches of white box testing. The cost and efforts are also minimized. The proposed technique ensures better website testing coverage as white box testing provides better results than black box testing. Then we validate the proposed reusability testing with two web navigational structures. The results show that doing regression testing can save several billion dollars. These test cases can be further minimized by using prioritization techniques of regression testing.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514424,yes
439,RisCal -- A Risk Estimation Tool for Software Engineering Purposes,"Decision making in software engineering requires the consideration of risk information. The reliability of risk information is strongly influenced by the underlying risk estimation process which consists of the steps risk identification, risk analysis and risk prioritization. In this paper we present a novel risk estimation tool for software engineering pruposes called RisCal. RisCal is based on a generic risk model and supports the integration of manually and automatically determined metrics into the risk estimation. This makes the tool applicable for arbitrary software engineering activities like risk-based testing or release planning. We show how RisCal supports risk identification, analysis and prioritizations, provide an estimation example, and discuss its application to risk-based testing and release planning.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619524,yes
440,Selection and Prioritization of Test Cases by Combining White-Box and Black-Box Testing Methods,"In this paper, we present a methodology that combines both white-box and black-box testing, in order to improve testing quality for a given class of embedded systems. The goal of this methodology is generation of test cases for the new functional testing campaign based on the test coverage information from the previous testing campaign, in order to maximize the test coverage. Test coverage information is used for selection of proper test cases in order to improve the quality of testing and save available resources for testing. As an output, a set of test cases is produced. Generated test cases are processed by the test Executor application that decides whether results have passed or failed, based on the results of image grabbing, OCR text extraction, and comparison with expected text. The presented methodology is finally validated by means of a case-study targeting an Android device. The results of the case study are affirmative and they indicate that the proposed methodology is applicable for testing embedded systems of this kind.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664523,yes
441,SMT Malleability in IBM POWER5 and POWER6 Processors,"While several hardware mechanisms have been proposed to control the interaction between hardware threads in an SMT processor, few have addressed the issue of software-controllable SMT performance. The IBM POWER5 and POWER6 are the first high-performance processors implementing a software-controllable hardware-thread prioritization mechanism that controls the rate at which each hardware-thread decodes instructions. This paper shows the potential of this basic mechanism to improve several target metrics for various applications on POWER5 and POWER6 processors. Our results show that although the software interface is exactly the same, the software-controlled priority mechanism has a different effect on POWER5 and POWER6. For instance, hardware threads in POWER6 are less sensitive to priorities than in POWER5 due to the in order design. We study the SMT thread malleability to enable user-level optimizations that leverage software-controlled thread priorities. We also show how to achieve various system objectives such as parallel application load balancing, in order to reduce execution time. Finally, we characterize user-level transparent execution on POWER5 and POWER6, and identify the workload mix that best benefits from it.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138854,no
442,Software components prioritization using OCL formal specification for effective testing,"In soft real time system development, testing effort minimization is a challenging task. Earlier research has shown that often a small percentage of components are responsible for most of the faults reported at the later stages of software development. Due to the time and other resource constraints, fault-prone components are ignored during testing activity which leads to compromises on software quality. Thus there is a need to identify fault-prone components of the system based on the data collected at the early stages of software development. The major focus of the proposed methodology is to identify and prioritize fault-prone components of the system using its OCL formal specifications. This approach enables testers to distribute more effort on fault-prone components than non fault-prone components of the system. The proposed methodology is illustrated based on three case study applications.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844288,no
443,Software defect prediction using software metrics - A survey,"Traditionally software metrics have been used to define the complexity of the program, to estimate programming time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the metric values are to be used in mathematical equations designed to represent a model of the software process, metrics associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully apply. Work on the mechanics of implementing metrics programs. The goal of this research is to help developers identify defects based on existing software metrics using data mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in the development and maintenance phase. This research focuses in identifying defective modules and hence the scope of software that needs to be examined for defects can be prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology helps in identifying modules that require immediate attention and hence the reliability of the software can be improved faster as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the Data mining algorithm. To initiate this process we initially propose to evaluate the existing classification algorithms and based on its weakness we propose a novel Neural network algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508369,no
444,Suitable placements of multiple FACTS devices to improve the transient stability using trajectory sensitivity analysis,"Trajectory sensitivity analysis (TSA) is used as a tool for suitable placement of multiple series compensators in the power system. The goal is to maximize the benefit of these devices in order to enhance the transient stability of the system. For this purpose, the trajectory sensitivities of the rotor angles of the most critical generators with respect to the reactances of transmission lines are calculated in the presence of the most severe faults. Based on the obtained trajectory sensitivities, a method is proposed to determine how effective the series compensation of each transmission line is for improving the transient stability. This method is applied to the Nordic-32 test system to find the priorities of the transmission lines for installation of several series compensators. Simulation with industrial software shows the validity and efficiency of the proposed method.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666828,no
445,Test Case Prioritization for Continuous Regression Testing: An Industrial Case Study,"Regression testing in continuous integration environment is bounded by tight time constraints. To satisfy time constraints and achieve testing goals, test cases must be efficiently ordered in execution. Prioritization techniques are commonly used to order test cases to reflect their importance according to one or more criteria. Reduced time to test or high fault detection rate are such important criteria. In this paper, we present a case study of a test prioritization approach ROCKET (Prioritization for Continuous Regression Testing) to improve the efficiency of continuous regression testing of industrial video conferencing software. ROCKET orders test cases based on historical failure data, test execution time and domain-specific heuristics. It uses a weighted function to compute test priority. The weights are higher if tests uncover regression faults in recent iterations of software testing and reduce time to detection of faults. The results of the study show that the test cases prioritized using ROCKET (1) provide faster fault detection, and (2) increase regression fault detection rate, revealing 30% more faults for 20% of the test suite executed, comparing to manually prioritized test cases.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676952,yes
446,Test Case Prioritization Using Requirements-Based Clustering,"The importance of using requirements information in the testing phase has been well recognized by the requirements engineering community, but to date, a vast majority of regression testing techniques have primarily relied on software code information. Incorporating requirements information into the current testing practice could help software engineers identify the source of defects more easily, validate the product against requirements, and maintain software products in a holistic way. In this paper, we investigate whether the requirements-based clustering approach that incorporates traditional code analysis information can improve the effectiveness of test case prioritization techniques. To investigate the effectiveness of our approach, we performed an empirical study using two Java programs with multiple versions and requirements documents. Our results indicate that the use of requirements information during the test case prioritization process can be beneficial.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569743,yes
447,Test suite prioritisation using trace events technique,"The size of the test suite and the duration of time determines the time taken by the regression testing. Conversely, the testers can prioritise the test cases by the use of a competent prioritisation technique to obtain an increased rate of fault detection in the system, allowing for earlier corrections, and getting higher overall confidence that the software has been tested suitably. A prioritised test suite is more likely to be more effective during that time period than would have been achieved via a random ordering if execution needs to be suspended after some time. An enhanced test case ordering may be probable if the desired implementation time to run the test cases is proven earlier. This research work's main intention is to prioritise the regressiontesting test cases. In order to prioritise the test cases some factors are considered here. These factors are employed in the prioritisation algorithm. The trace events are one of the important factors, used to find the most significant test cases in the projects. The requirement factor value is calculated and subsequently a weightage is calculated and assigned to each test case in the software based on these factors by using a thresholding technique. Later, the test cases are prioritised according to the weightage allocated to them. Executing the test cases based on the prioritisation will greatly decreases the computation cost and time. The proposed technique is efficient in prioritising the regression test cases. The new prioritised subsequences of the given unit test suites are executed on Java programs after the completion of prioritisation. Average of the percentage of faults detected is an evaluation metric used for evaluating the 'superiority' of these orderings.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519507,yes
448,Using Dependency Structures for Prioritization of Functional Test Suites,"Test case prioritization is the process of ordering the execution of test cases to achieve a certain goal, such as increasing the rate of fault detection. Increasing the rate of fault detection can provide earlier feedback to system developers, improving fault fixing activity and, ultimately, software delivery. Many existing test case prioritization techniques consider that tests can be run in any order. However, due to functional dependencies that may exist between some test cases-that is, one test case must be executed before another-this is often not the case. In this paper, we present a family of test case prioritization techniques that use the dependency information from a test suite to prioritize that test suite. The nature of the techniques preserves the dependencies in the test ordering. The hypothesis of this work is that dependencies between tests are representative of interactions in the system under test, and executing complex interactions earlier is likely to increase the fault detection rate, compared to arbitrary test orderings. Empirical evaluations on six systems built toward industry use demonstrate that these techniques increase the rate of fault detection compared to the rates achieved by the untreated order, random orders, and test suites ordered using existing ""coarse-grainedƒ? techniques based on function coverage.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189361,yes
449,A code coverage-based test suite reduction and prioritization framework,"Software testing is extensively used to ensure the development of a quality software system. The test suite size tends to increase by including new test cases due to software evolution. Consequently, the entire test suite cannot be executed considering budget and time limitations. Researchers have examined test suite reduction and prioritization techniques to address the test suite size problem. However, combination of these techniques can be useful for various regression testing situations. In this paper, we present a new code coverage-based test suite reduction and prioritization framework called TestOptimizer. The framework performs a suitable combination of TestFilter and St-Total techniques to determine optimal test cases, keeping in view of time restrictions. The performance of the proposed framework has been assessed using a case study. Results show that TestOptimizer can be beneficial to solve the test suite size problem within time constraints and has a profound impact on the required cost and effort of regression testing.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076910,yes
450,A Comparison of Test Case Prioritization Criteria for Software Product Lines,"Software Product Line (SPL) testing is challenging due to the potentially huge number of derivable products. To alleviate this problem, numerous contributions have been proposed to reduce the number of products to be tested while still having a good coverage. However, not much attention has been paid to the order in which the products are tested. Test case prioritization techniques reorder test cases to meet a certain performance goal. For instance, testers may wish to order their test cases in order to detect faults as soon as possible, which would translate in faster feedback and earlier fault correction. In this paper, we explore the applicability of test case prioritization techniques to SPL testing. We propose five different prioritization criteria based on common metrics of feature models and we compare their effectiveness in increasing the rate of early fault detection, i.e. a measure of how quickly faults are detected. The results show that different orderings of the same SPL suite may lead to significant differences in the rate of early fault detection. They also show that our approach may contribute to accelerate the detection of faults of SPL test suites based on combinatorial testing.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823864,yes
451,A distributed algorithm for virtual traffic lights with IEEE 802.11p,"A virtual traffic light (VTL) is a mechanism that allows vehicles to autonomously solve priorities at road junctions in the absence of fixed infrastructures (i.e., conventional traffic lights). To develop an effective VTL system, communication between vehicles is a crucial factor and can be handled either using cellular infrastructure or adopting a vehicle-to-vehicle (V2V) communication paradigm. In this paper we present the design, the implementation, and the field trial of a VTL which exploits V2V communications based on IEEE 802.11p. Specif-ically, we propose a decentralized algorithm, that adopts both broadcast signaling and unicast messages to assign priorities to the vehicles approaching intersections, thus preventing accidents and reducing traffic congestions. The algorithm has been tested both in a controlled laboratory environment and in a field trial with equipped vehicles.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6882621,no
452,A hierarchical test case prioritization technique for object oriented software,"Software reuse is the use of existing artifacts to create new software. Inheritance is the foremost technique of reuse. But the inherent complexity due to inheritance hierarchy found in object - oriented paradigm also affect testing. Every time any change occurs in the software, new test cases are added in addition to the existing test suite. So there is need to conduct effective regression testing having less number of test cases to reduce cost and time. In this paper a hierarchical test case prioritization technique is proposed wherein various factors have been considered that affect error propagation in the inheritance. In this paper prioritization of test cases take place at two levels. In the first level the classes are prioritized and in the second level the test cases of prioritized classes are ordered. To show the effectiveness of proposed technique it was applied and analyze on a C++ program.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019794,yes
453,A Method to Test the Information Quality of Technical Documentation on Websites,"In software engineering, testing is one of the corner-stones of quality assurance. The idea of software testing can be applied to information quality as well. Technical documentation has a set of intended uses that correspond to use cases in a software system. Documentation is, in many cases, presented via software systems, e.g., web servers and browsers, and contains software, e.g., Javascript for user interaction, animation, and customization, etc. This makes it difficult to find a clear-cut distinction between a software system and technical documentation. However, we can assume that each use case of a technical documentation involves retrieval of some sort of information that helps a user answer a specific questions. To assess information testing as a method, we implemented QAnalytics, a tool to assess the information quality of documentation that is provided by a website. The tool is web-based and allows test managers and site owners to define test cases and success criteria, disseminate the test cases to testers, and to analyze the test results. This way, information testing is easily manageable even for non-technical stakeholders. We applied our testing method and tool in a case study. According to common perception, the website of Linnaeus University needs to be re-engineered. Our method and tool helped the stakeholders identify what information is presented well and which parts of the website that need to change. The test results allowed the design and development effort to prioritize actual quality issues and potentially save time and resources.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958417,no
454,A study of applying severity-weighted greedy algorithm to software test case prioritization during testing,"Regression testing is a very useful technique for software testing. Traditionally, there are several techniques for test case prioritization; two of the most used techniques are Greedy and Additional Greedy Algorithm (GA and AGA). However, it can be found that they may not consider the severity while prioritizing test cases. In this paper, an Enhanced Additional Greedy Algorithm (EAGA) is proposed for test case prioritization. Experiments with eight subject programs are performed to investigate the effects of different techniques under different criteria and fault severity. Experimental results show that proposed EAGA perform well than other techniques.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058806,no
455,An improved genetic approach for test path generation,"Quality of a software system depends on testing approaches adopted to analyze the software product. Testing process itself depends on two main vectors called test sequence generation and test data generation. Test sequence generation is about to identify the order in which the particular test cases will be executed and the test data defines the various checks performed on each test case. In this present work, a fuzzy improved genetic approach is suggested for test case generation. The sequence on these test cases is here dependent on module interaction analysis. Based on this analysis, the test case prioritization will be defined. Once the test cases will be prioritized, the next work is to apply fuzzy improved genetic approach for test path generation. The work is analyzed under different prioritization vectors. Analysis of work is defined in terms of test cost estimation under different prioritization scenarios.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012823,yes
456,Applying Parameter Value Weighting to a Practical Application,"This paper reports a case study where pair-wise testing was applied to a real-world program. In particular we focus on weighting, an added feature which allows the tester to prioritize particular parameter values. In our previous work we proposed a weighting method that can reflect given weights in the resulting test suite more directly than can existing methods. To asses the effects of weighting in a practical testing process, we compare the number of execution times of the program's methods among three pair-wise test suites, including the test suite generated by our weighting method and those generated by an existing test case generation tool with and without the weighting option. The results show that the effects of weighting were most clearly observed when our weighting method was used.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983821,no
457,Athena: A Visual Tool to Support the Development of Computational Intelligence Systems,"Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984580,no
458,Automated Prioritization of Metrics-Based Design Flaws in UML Class Diagrams,"The importance of software architecture in software development prolongs throughout the entire software life cycle. This is because quality of the architectural design defines the structural aspects of the system that are difficult to change, and hence will affect most of the subsequent development and maintenance activities. This paper considers software design flaws (related to the system structure) and not flaws identified at run time (by testing). These design flaws are akin to what is described in the literature as anti-patterns, bad smells or rotting design. Recently, two tools that have been developed for quality assurance of software designs represented in the UML notation: SDMetrics and Metric View. However these tools are not considered practical because they report many design flaws which are not considered by developers (false positives). This paper explores an approach that tries to identify which design flaws should be considered important and which are not. To this end, we propose an approach for automated prioritization of software design flaws (BX approach), to facilitate developers to focus on important design flaws more effectively. We designed and implemented a tool (PoSDef) that implements this approach. The BX approach and the PoSDef tool have been validated using two open source projects and one large industrial system. Our validation consists of comparing our approach and tool with the existing design flaw tools. The evaluation has shown that the proposed approach could facilitate developers to identify and prioritize important design flaws effectively.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928837,no
459,Bench level automotive EMC validation test laboratory challenges and preferences,"Automotive original equipment manufacturers (OEMs) and suppliers of electrical and electronic subsystems are required to perform bench level electrical and electromagnetic compatibility (EMC) validation testing. This is an important process that requires a significant investment in time and money. In an effort to improve the efficiency of testing, a survey was developed to gain an understanding of the challenges faced by test laboratories and also their preferences. This paper summarizes the results of the survey. Given the amount of time and money spent annually for the type of testing considered, the results suggest that pursuing improvements will result in a long term savings for the original equipment manufacturers (OEMs), suppliers, and labs involved. In order to increase test efficiency, the OEMs, suppliers, and laboratories will need to work together to better prepare test plans and test setups. It is suggested that the results of this survey are used to prioritize the improvement activities.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898989,no
460,Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines,"Large Software Product Lines (SPLs) are common in industry, thus introducing the need of practical solutions to test them. To this end, t-wise can help to drastically reduce the number of product configurations to test. Current t-wise approaches for SPLs are restricted to small values of t. In addition, these techniques fail at providing means to finely control the configuration process. In view of this, means for automatically generating and prioritizing product configurations for large SPLs are required. This paper proposes (a) a search-based approach capable of generating product configurations for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of product configurations. Both these techniques employ a similarity heuristic. The ability of the proposed techniques is assessed in an empirical study through a comparison with state of the art tools. The comparison focuses on both the product configuration generation and the prioritization aspects. The results demonstrate that existing t-wise tools and prioritization techniques fail to handle large SPLs. On the contrary, the proposed techniques are both effective and scalable. Additionally, the experiments show that the similarity heuristic can be used as a viable alternative to t-wise.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823132,no
461,Competence transfer through enterprise mobile application development,"Large world corporations need corresponding information technology (IT) support as well as constant improvement of software tools, which should enable further business development and more efficient work to operational organizations. The main interest of IT support organizations is currently more and more connected with mobile IT equipment of the employees. Specific business mobile applications improve the efficiency and result in new operating possibilities. The cooperation between Ericsson Nikola Tesla (ENT) and Faculty of Electrical Engineering and Computing (FER) of the Zagreb University, has enabled a fast development of competencies in ENT, necessary for mobile application development and quality realization of innovative solution for platforms iOS and Android. In this cooperation at the project realization the methodology of application development was defined, the corresponding competencies were developed, the system architecture was designed together with the communication of mobile application and back-end IT systems. In the project the iterative development approach, tools for software code versioning and project control have been used thus enabling continuous insight in project progressing. At any moment it was possible to determine priorities of functional development and solution elements, as well as to realize the necessary additional transfer of knowledge. The project has resulted in an enterprise mobile application for iOS and Android platforms, which had been implemented and tested in several countries. The complete solution enables data management by means of the portal, thus decreasing the frequent changes of users' mobile application and significantly accelerating the process of new functionalities introduction.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859609,no
462,Development and design of a platform for arbitration and sharing control applications,"In this paper, a description of the ADAS development platform in DESERVE project framework is presented. This work is framed within the Sub Project 2 (Development platform) of DESERVE project, and it is divided in 6 different and complementary lines of work. Most of the functions described in the tools and development systems, perception layer and the platform system architecture show the modularity and scalability of our proposal. Moreover, based on vehicle modelling, driver behaviour and intention, a first approach for arbitration and control strategies, which can anticipate the priorities on the control in emergency situations, is described. Furthermore, some simulations will allow the virtual testing for the future implementation in demonstrators. The presented work is the core of DESERVE project, and it is developed in parallel with Driver behaviour and HMI activities (SP3). This work presents some of the achievements in SP2, mainly the application platform integration in one of the demonstrators, along with the arbitration and sharing control, based on intelligent techniques (Fuzzy logic). Simulation shows the feasibility of proposal. This approach will be tested, integrated and validated in a real vehicle in the next stages of the project.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6893228,no
463,Development test case prioritization technique in regression testing based on hybrid criteria,"Test case prioritization techniques improve the performance of regression testing, and arrange test cases in order to obtain maximum available fault that is going to be detected in a shorter time. In this research the priority is given to test cases that are performed based on multiple criteria and hybrid criteria to enhance the effectiveness of time and cost for proposed technique. This paper shows that our prioritization technique is appropriate for regression testing environment and show that our prioritization approach frequently produces a higher average percentage of fault detection rate value, for web application. The experiments also reveal fundamental tradeoffs in the performance of time aware prioritization. In this technique some fault will be seeded in subject application, then applying the prioritization criteria on test cases to obtain the effective time of average percentage fault detection rate.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986033,yes
464,DITEC (DoD-Centric and Independent Technology Evaluation Capability): A Process for Testing Security,"Information Assurance (IA) is one of the Department of Defense's (DoD) top priorities today. IA technologies are constantly evolving to protect critical information from the growing number of cyber threats. Furthermore, DoD spends millions of dollars each year procuring, maintaining, and discontinuing various IA and Cyber technologies. Today, there is no process and/or standardized method for making informed decisions about which IA technologies are better/best. Due to this, efforts for selecting technologies go through very disparate evaluations that are often times non-repeatable and very subjective. DITEC (DoD-centric and Independent Technology Evaluation Capability) is a new capability that streamlines IA technology evaluation. DITEC defines a Process for evaluating whether or not a product meets DoD needs, Security Metrics for measuring how well needs are met, and a Framework for comparing various products that address the same IA technology area. DITEC seeks to reduce the time and cost of creating a test plan and expedite the test and evaluation effort for considering new IA technologies, consequently streamlining the deployment of IA products across DoD and increasing the potential to meet its needs.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825634,no
465,Dynamic test case prioritization based on multi-objective,"Test case prioritization technology is to sort the test cases before the software testing designed to improve test efficiency. This paper presents a dynamic test case prioritization technique based on multi-objective. It integrates several traditional single-objective technologies so that makes it more flexible. This technology, from five dimensions, calculates prioritization values of test cases separately. Then a weighted sum is made to the values and it sorts the test cases according to the values. The results return to the storage in order to dynamically adjust the sort of test cases. This technology not only meets the high demands of regression testing, but also ensures the high efficiency of the test results.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888744,yes
466,Effective Regression Testing Using Requirements and Risks,"The use of system requirements and their risks enables software testers to identify more important test cases that can reveal faults associated with risky components. Having identified those test cases, software testers can manage the testing schedule more effectively by running such test cases earlier so that they can fix faults sooner. Some work in this area has been done, but the previous approaches and studies have some limitations, such as an improper use of requirements risks in prioritization and an inadequate evaluation method. To address the limitations, we implemented a new requirements risk-based prioritization technique and evaluated it considering whether the proposed approach can detect faults earlier overall. It can also detect faults associated with risky components earlier. Our results indicate that the proposed approach is effective for detecting faults early and even better for finding faults associated with risky components of the system earlier than the existing techniques.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6895426,no
467,Enabling Prioritized Cloud I/O Service in Hadoop Distributed File System,"Cloud computing has become more and more popular nowadays. Both governments and enterprises provide service through the construction of public and private clouds accordingly. Among the platforms used in cloud computing, Hadoop is considered one of the most practical and stable systems. Nevertheless, as with other regular software, Hadoop still needs to rely on the underlying operating system to communicate with hardware to function appropriately. For modern computer systems, CPUs excessively outrun hard drives (hard disks). The computer hard disk has become a major bottleneck to the overall system performance. Consequently, computer programs can execute faster if their corresponding I/O operation can be completed sooner. This is important in particular when we want to expedite the execution of urgent programs in a busy system. Unfortunately, under the current Hadoop environment, users cannot prioritize operation of disk and memory for programs which they would like them to run faster. With the help of prioritized I/O service we developed earlier, we proposed and implemented a Hadoop environment with the ability of providing prioritized I/O service. Our Hadoop environment could accelerate the execution of programs with high priority assigned by users. We evaluated our design by executing prioritized programs in environments with different busy levels. Experimental results show that programs can improve their performance by up to 33.79% if executed with high priority.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056748,no
468,Exploring Model-Based Repositories for a Broad Range of Industrial Applications and Challenges,"Nowadays, systems are becoming increasingly complex and large and the process of developing such large-scale systems is becoming complicated with high cost and enormous effort required. Such a complicated process has a prominent challenge to ensure the quality of delivered artifacts. Therefore there is clearly a need to facilitate reuse of developed artifacts (e.g., requirements, architecture, tests) and enable automated analyses such as risk analyses, prioritizing test cases, change impact analysis, with the objective to reduce cost, effort and improve quality. Model-based engineering provides a promising mechanism to facilitate reuse and enable automation. The key idea is to use models as the backbone of structuring repositories that contain reusable artifacts (e.g., test cases, requirements). Such a backbone model is subse-quently used to enable various types of automation such as model-based testing and automated rule verification. In this paper, we report 12 industrial projects from five different industry domains that all require the construction of model-based repositories to enable various types of automation. We believe using models as the backbone to structure repositories for the purpose of enabling different types of automation in different contexts is a new and non-conventional model-based development research approach. This exploratory paper will serve the basis for future research to derive a generic model-based repository.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958385,no
469,Fair and delay adaptive scheduler for UC and NGN networks,"Fair bandwidth allocation while conforming to stringent end-to-end delay constraints is a major requirement for the successful delivery of next generation QoS demanding traffic. Research has been carried out in the area of processor and scheduler sharing for decades to try to achieve the QoS requirements of network traffic. Fairness and traffic prioritization are two main objectives that many schedulers were originally designed to meet. Another important issue is how schedulers treat the delay sensitive traffic. Although the combination of fairness and prioritization is implemented in several schedulers but, to our knowledge, incorporating adaptive traffic delay treatment in fair and prioritized schedulers has not yet been successfully implemented. In this paper, we introduce a new scheduler that balances between the fairness of bandwidth allocation between flows while implementing prioritization and minimizes the number of end-to-end delay bound breaches. The scheduler combines the virtual clock concept used in well-known fair schedulers together with schedulability testing and evaluation implemented in delay sensitive schedulers. The scheduler is designed to achieve the fairness of bandwidth allocation, such as in fair schedulers, while minimizing the number of possible violation of end-to-end QoS delay of individual flows' packets.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900970,no
470,Genetic algorithm secure procedures algorithm to manage data integrity of test case prioritization methodology,"The focus of present research paper is to manage data integrity and trustworthiness of issues involved in software testing phase of software development life cycle. Many times, it seems that, data integration left behind the software testing phase and it is directly focused at software deployment or delivery time. To avoid issues due to lack of data integration, we developed algorithm which can track integration of test case prioritization along with trustworthiness of test case execution. This paper demonstrates how software testing can be efficient with management of data integrity factor to avoid major security issues. One of the main advantages of our approach is that domain specific semantics can be integrated with the data quality test cases prioritization, thus being able to discover test feed data quality problems beyond conventional quality measures.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030880,yes
471,How Does the UML Testing Profile Support Risk-Based Testing,"The increasing complexity of software-intensive systems raises a lot of challenges demanding new techniques for ensuring their overall quality. The risk of not meeting the expected level of quality has negative impact on business, customers, environment and people, especially in the context of safety/security-critical systems. The importance of risk assessment, analysis and management has been well understood both in the literature and practice, which has led to the definition of a number of well-known standards. In the recent years, Risk-Based Testing (RBT) is gaining more attention, especially focusing on test prioritization and selection based on risks. On the other hand, model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software-intensive systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in literature and practice in the last decade. In this paper, we study the feasibility of combining RBT with MBT by using the upcoming version of UML Testing Profile (UTP 2) as the mechanism. We present potential traceability between RBT and UTP 2 concepts.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983859,no
472,Identifying usability risk: A survey study,"As defined in various quality models, usability is recognized as an important attribute of software quality. Failing to address usability requirements in a software product could lead to poor quality and high usability problems in software product. Research is still in progress to introduce the best methods for reducing usability problems and increase the rate of successful usable software products. Studies have shown that problems in software products can also be controlled using Software Risk Management methods, even though these problems cannot be eliminated totally. Using Software Risk Management, problems in software products are dealt before it occurs. This paper presents usability problems as a risk factor and by managing usability risk at earlier phases of the development process, successful and high usability software products can be developed. Unfortunately, currently there is little effort in identifying, analyzing and prioritizing potential usability risks at earlier phases of the development process. This paper focuses on usability risk identification as it is the first stage in usability risk management. This paper presents the results of an industry survey based on the opinion of Malaysian Public Sector involving sample size of 330 software developers and software projects managers regarding potential usability risk that could occur during Software Development Life Cycle (SDLC). Our finding has identified 42 potential usability risks, defined as a list for further risk analysis in future.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986005,no
473,Implementing test case selection and reduction techniques using meta-heuristics,"Regression Testing is an inevitable and very costly maintenance activity that is implemented to make sure the validity of modified software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization to select and prioritize a minimum set of test cases, fulfilling some chosen criteria, that is, covering all possible faults in minimum time and other. In this paper a test case reduction hybrid Particle Swarm Optimization (PSO) algorithm has been proposed. This PSO algorithm uses GA mutation operator while processing. PSO is a swarm intelligence algorithm based on particles behavior. GA is an evolutionary algorithm (EA). The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949377,yes
474,Parallelized ACO algorithm for regression testing prioritization in hadoop framework,"Regression testing is an important strategy in the software maintenance phase to produce a high quality software product. This testing ensures that the modified system code does not have an effect on the original software system. Initially, the test suite is generated for the existing software system. After the system undergoes changes the test suite contains both the original test cases and the modified test cases. Regression test prioritization method helps to separate the optimal test cases from the modified test suite. In the existing work, the multi-criteria optimization was applied for generating optimal regression test cases and it was carried out in a non-parallelized environment. The proposed solution is to extend the existing work by generating an optimized test suite using Ant Colony Optimization (ACO) technique on Hadoop Map reduce framework in a parallelized environment.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019371,yes
475,"Planning of Prioritized Test Procedures in Large Integrated Systems: Best Strategy of Defect Discovery and Early Stop of Testing Session, The Selex-ES Experience","Large integrated systems verification can be made more efficient if driven by specific strategy, classification and prioritization of test procedures is the Selex ES way to speed up important defects discovery and cost of testing activities.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983814,yes
476,Practical Software Quality Prediction,"Software systems continue to play an increasingly important role in our daily lives, making the quality of software systems an extremely important issue. Therefore, a significant amount of recent research focused on the prioritization of software quality assurance efforts. One line of work that has been receiving an increasing amount of attention is Software Defect Prediction (SDP), where predictions are made to determine where future defects might appear. Our survey showed that in the past decade, more than 100 papers were published on SDP. Nevertheless, the practical adoption of SDP to date is limited. In this paper, we highlight the findings of our thesis, which identifies the challenges that hinder the adoption of SDP in practice. These challenges include the fact that the majority of SDP research rarely considers the impact of defects when performing their predictions, seldom provides guidance on how to use the SDP results, and is too reactive and defect-centric in nature. Therefore, we propose approaches that tackle these challenges. First, we present approaches that predict high-impact defects. Our approaches illustrate how SDP research can be tailored to consider the impact of defects when making their predictions. Second, we present approaches that simplify SDP models so they can be easily understood and illustrates how these simple models can be used to assist practitioners in prioritizing the creation of unit tests in large software systems. These approaches illustrate how SDP research can provide guidance to practitioners using SDP. Then, we argue that organizations are interested in proactive risk management, which covers more than just defects. For example, risky changes may not introduce defects but they could delay the release of projects. Therefore, we present an approach that predicts risky changes, illustrating how SDP can be more encompassing (i.e., by predicting risk, not only defects) and proactive (i.e., by predicting changes before they are incorporated into the code base).",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976158,no
477,Prioritization of Unit Testing on non-object oriented using a top-down based approach,"The issue which makes Unit Testing so tough is the ambiguous ways that the software world keeps moving forward. Although sometimes by implementing simple unit testing methods, this task become easy to handle. However to achieve a comprehensive unit testing it is supposed to test all corners of software such as database, devices, communication etc. This paper proposes an orthogonal software testing approach based on Top-Down technique which treats the input parameters of a software unit in an orthogonal partitioning to dynamic level of testing. Describes how test cases are statistically for each trial of software testing steps and makes a dynamic partitioning approach on non-object oriented experiments. The adequacy of the generated test cases can be validated by examining testing coverage metrics. The authors have considered using of different partitioning and mock objects help to make an isolated testing, improve code's structure and automated testing possibility. The results of the test case executions can be analyzed in order to find the ƒ??IFƒ? metrics for partitioning the traceable ways and detecting defects, to generate more effective test cases in future testing, and to help locate and correct defects in the early stage of testing.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6985991,yes
478,Quality of Service (QoS)-Guaranteed Network Resource Allocation via Software Defined Networking (SDN),"Quality of Service (QoS) -- based bandwidth allocation plays a key role in real-time computing systems and applications such as voice IP, teleconferencing, and gaming. Likewise, customer services often need to be distinguished according to their service priorities and requirements. In this paper, we consider bandwidth allocation in the networks of a cloud carrier in which cloud users' requests are processed and transferred by a cloud provider subject to QoS requirements. We present a QoS-guaranteed approach for bandwidth allocation that satisfies QoS requirements for all priority cloud users by using Open vSwitch, based on software defined networking (SDN). We implement and test the proposed approach on the Global Environment for Networking Innovations (GENI). Experimental results show the effectiveness of the proposed approach.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945296,no
479,"RDCC: An effective test case prioritization framework using software requirements, design and source code collaboration","Test case prioritization is a technique for selecting those test cases, which are expected to outperform for determining faulty modules earlier. Different phases of software development lifecycle represent the total software from different point of views, where priority module may vary from phase to phase. However, information from different phases of software development lifecycle is rarely introduced and no one integrates that information to prioritize test cases. This paper presents an effective test case prioritization framework, which takes software requirements specification, design diagrams, source codes and test cases as input and provides a prioritized order of test cases using their collaborative information as output. Requirement IDs are split into words or terms excluding stop words to calculate requirements relativity. Design diagrams are extracted as readable XML format to calculate the degree of interconnectivity among the activities. Source codes are parsed as call graphs where vertices and edges represent classes, and calls between two classes respectively. Requirements relativity, design interconnectivity and class dependencies are multiplied by their assigned weight to calculate final weight and select test cases by mapping the customers' requirements and test cases using that weight. The proposed framework is validated with an academic project and the results show that use of collaborative information during prioritization process can be beneficial.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7073072,yes
480,Regression Testing Approach for Large-Scale Systems,"Regression testing is an important and expensive activity that is undertaken every time a program is modified to ensure that the changes do not introduce new bugs into previously validated code. Instead of re-running all test cases, different approaches were studied to solve regression testing problems. Data mining techniques are introduced to solve regression testing problems with large-scale systems containing huge sets of test cases, as different data mining techniques were studied to group test cases with similar features. Dealing with groups of test cases instead of each test case separately helped to solve regression testing scalability issues. In this paper, we propose a new methodology for regression testing of large-scale systems using data mining techniques to prioritize and select test cases based on their coverage criteria and fault history.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983822,yes
481,Reverse Engineering Complex Feature Correlations for Product Line Configuration Improvement,"As a Software Product Line (SPL) evolves with increasing number of variant features and feature values, the feature correlations become extremely intricate. However, these correlations are often incompletely documented (e.g., In feature models) so that most features can only be configured manually. In order to make product configuration processes more efficient, we present an approach to extracting complex feature correlations from existing product configurations using association mining techniques. Then these correlations are pruned and prioritized in order to minimize the effort of correlation validation. Our approach is conducted on an industrial SPL with 100 product configurations across 480 features. While 80 out of the 100 configurations are used as training data to automatically extract 4834 complex feature correlations, the rest 20 configurations are used as test data to evaluate the improvement potential of configuration efficiency. In the end, avg. 25% features in each of the 20 products can be configured automatically.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928830,no
482,RSUs placement using overlap based greedy method for urban and rural roads,"The deployment of efficient roadside networks is a necessity for ITS deployment. The main challenge for the roadside deployment is to find a satisfying or best distribution of RSUs on the roads network according to the given conditions in order to meet the requested requirements of the roads operator. Additionally, various factors affect this process such as traffic, infrastructure and topological characteristics of the roads. This paper introduces an Overlap based Greedy Method (OGM) as a basis for RSUs placement and can be applied on urban and rural roads. This method in its current development mainly considers the RSU coverage radius and the overlap rate into the RSUs distribution process. Moreover, a wide range of influencing factors is considered through the prioritization of Sites of Interest (SoIs). The OGM method is developed within the recently started PRONET project and is integrated into its software platform. The tests conducted on selected roads environment in the city of Rostock (Germany) ensure: (1) decrease in number of chosen SoIs for placement compared to the original scanned number of SoIs, and (2) full (continuous) coverage cannot be guaranteed for a roads network using only the available junctions.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000905,no
483,SimLatte: A Framework to Support Testing for Worst-Case Interrupt Latencies in Embedded Software,"Embedded systems tend to be interrupt-driven, yet the presence of interrupts can affect system dependability because there can be delays in servicing interrupts. Such delays can occur when multiple interrupt service routines and interrupts of different priorities compete for resources on a given CPU. For this reason, researchers have sought approaches by which to estimate worst-case interrupt latencies (WCILs) for systems. Most existing approaches, however, are based on static analysis. In this paper, we present SIMLATTE, a testing-based approach for finding WCILs. SIMLATTE uses a genetic algorithm for test case generation that converges on a set of inputs and interrupt arrival points that are likely to expose WCILs. It also uses an opportunistic interrupt invocation approach to invoke interrupts at a variety of feasible locations. Our evaluation of SIMLATTE on several non-trivial embedded systems reveals that it is considerably more effective and efficient than random testing. We also determine that the combination of the genetic algorithm and opportunistic interrupt invocation allows SIMLATTE to perform better than it can when using either one in isolation.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823893,no
484,Test Case Prioritization Based on Information Retrieval Concepts,"In regression testing, running all a system's test cases can require a great deal of time and resources. Test case prioritization (TCP) attempts to schedule test cases to achieve goals such as higher coverage or faster fault detection. While code coverage-based approaches are typical in TCP, recent work has explored the use of additional information to improve effectiveness. In this work, we explore the use of Information Retrieval (IR) techniques to improve the effectiveness of TCP, particularly for testing infrequently tested code. Our approach considers the frequency at which elements have been tested, in additional to traditional coverage information, balancing these factors using linear regression modeling. Our empirical study demonstrates that our approach is generally more effective than both random and traditional code coverage-based approaches, with improvements in rate of fault detection of up to 4.7%.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091286,yes
485,Test case prioritization techniques ƒ??an empirical studyƒ?,"Regression testing is an expensive process. A number of methodologies of regression testing are used to improve its effectiveness. These are retest all, test case selection, test case reduction and test case prioritization. Retest all technique involves re-execution of all available test suites, which are critical moreover cost effective. In order to increase efficiency, test case prioritization is being utilized for rearranging the test cases. A number of algorithms has been stated in the literature survey such as Greedy Algorithms and Metaheuristic search algorithms. A simple greedy algorithm focuses on test case prioritization but results in less efficient manner, due to which researches moved towards the additional greedy and 2-Optimal algorithms. Forthcoming metaheuristic search technique (Hill climbing and Genetic Algorithm) produces a much better solution to the test case prioritization problem. It implements stochastic optimization while dealing with problem concern. The genetic algorithm is an evolutionary algorithm which gives an exact mathematical fitness value for the test cases on which prioritization is done. This paper focuses on the comparison of metaheuristic genetic algorithm with other algorithms and proves the efficiency of genetic algorithm over the remaining ones.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045344,yes
486,Test case prioritization using multi objective particle swarm optimizer,"The goal of regression testing is to validate the modified software. Due to the resource and time constraints, it becomes necessary to develop techniques to minimize existing test suites by eliminating redundant test cases and prioritizing them. This paper proposes a 3-phase approach to solve test case prioritization. In the first phase, we are removing redundant test cases by simple matrix operation. In the second phase, test cases are selected from the test suite such that selected test cases represent the minimal set which covers all faults and also at the minimum execution time. For this phase, we are using multi objective particle swarm optimization (MOPSO) which optimizes fault coverage and execution time. In the third phase, we allocate priority to test cases obtained from the second phase. Priority is obtained by calculating the ratio of fault coverage to the execution time of test cases, higher the value of the ratio higher will be the priority and the test cases which are not selected in phase 2 are added to the test suite in sequential order. We have also performed experimental analysis based on maximum fault coverage and minimum execution time. The proposed MOPSO approach is compared with other prioritization techniques such as No Ordering, Reverse Ordering and Random Ordering by calculating Average Percentage of fault detected (APFD) for each technique and it can be concluded that the proposed approach outperformed all techniques mentioned above.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6884931,yes
487,Test case prioritization with improved genetic algorithm,"In software development, the most time consuming phase is maintenance. Regression testing, which is a part of maintenance, deals with test case prioritization that aims to increase rate of fault detection with less number of tests. In our study, we used 100 tests and 1000 faults; however, faults are detected by tests using genetic algorithm and improved genetic algorithm. After test case prioritization, we may detect all faults with less number of tests so there'll no need to apply all 100 tests (re-test).",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6830456,yes
488,Test Suite Prioritization by Switching Cost,"Test suite generation and prioritization are two main research fields to improve testing efficiency. Combinatorial testing has been proven as an effective method to generate test suite for highly configurable software systems, while test suites are often prioritized by interaction coverage to detect faults as early as possible. However, for some cases, there exists reasonable cost of reconfiguring parameter settings when switching test cases in different orders. Surprisingly, only few studies paid attention to it. In this paper, by proposing greedy algorithms and graph-based algorithms, we aim to prioritize a given test suite to minimize its total switching cost. We also compare two different prioritization strategies by a series of experiments, and discuss the advantages of our prioritization strategy and the selection of prioritization techniques. The results show that prioritization by switching cost can improve testing efficiency and our prioritization strategy can produce a small test suite with a reasonably low switching cost. This prioritization can be used widely and help locate fault causing interactions. The results also suggest that when testing highly configurable software systems and no knowledge of fault detection can be used, prioritization by switching cost is a good choice to detect faults earlier.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825648,yes
489,Test Suite Reduction by Combinatorial-Based Coverage of Event Sequences,"Combinatorial-based criteria are useful in several studies for test suite generation, prioritization, and minimization. In this paper, we extend previous work by using combinatorial-based criteria for test suite reduction. We use criteria that are based on combinatorial coverage of events and consider the order in which events occur. A simple combinatorial-based criterion covers t-way events and does not differentiate between the order of events. The event pair (e<sub>1</sub>, e<sub>2</sub>) is counted the same as if it occurs in the order (e<sub>2</sub>, e<sub>1</sub>). We also use two criteria that consider the order in which events occur since different orderings of events may be valuable during testing. First, the consecutive sequence-based criterion counts all event sequences in different orders, but they must occur adjacent to each other. The sequence-based criterion counts pairs in all orders without the requirement that events must be adjacent. We evaluate the new criteria on three GUI applications. We use 2way inter-window coverage in our studies. All of the 2way combinatorial-based criteria are effective in reducing the test suites and maintaining close to 100% fault finding effectiveness. Our future work examines larger test suites, higher strength coverage, techniques to partition event data, and further empirical studies.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825647,yes
490,Testability of object-oriented systems: An AHP-based approach for prioritization of metrics,This paper investigates testability from the perspective of metrics used in an object-oriented system. The idea is to give an overview of object oriented design metrics with the prioritization of same keeping testability as the overall goal. We have used Analytic Hierarchy Process (AHP) method to attain which metric is mostly used and is best for testability.,2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019595,no
491,The research of the test case prioritization algorithm for black box testing,"In order to improve the efficiency of software test case execution, this paper analyzed the impact of some factors to test cases prioritization and presented two adjustment algorithms. These factors included software requirement prioritization, software failure severity and software failure probability level. Firstly, gave the definition of software requirement prioritization, the ranking methods of software failure severity and software failure probability level, the description of the relationship between test cases and test requirements. Then, presented an initial test case prioritization method based on the analysis. And then, proposed a dynamic adjustment algorithm using of software requirement prioritization and software failure probability level when software failure occurred. Experimental data show that the two test case prioritization algorithms can improve the efficiency of software testing and are helpful to find more software defects in a short period.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933509,yes
492,Toolset and Program Repository for Code Coverage-Based Test Suite Analysis and Manipulation,"Code coverage is often used in academic and industrial practice of white-box software testing. Various test optimization methods, e.g. Test selection and prioritization, rely on code coverage information, but other related fields benefit from it as well, such as fault localization. These methods require access to the fine details of coverage information and efficient ways of processing this data. The purpose of the (free) SoDA library and toolset is to provide an efficient set of data structures and algorithms which can be used to prepare, store and analyze in various ways data related to code coverage. The focus of SoDA is not on the calculation of coverage data (such as instrumentation and test execution) but on the analysis and manipulation of test suites based on such information. An important design goal of the library was to be usable on industrial-size programs and test suites. Furthermore, there is no limitation on programming language, analysis granularity and coverage criteria. In this paper, we demonstrate the purpose and benefits of the library, the associated toolset, which also includes a graphical user interface, as well as possible usage scenarios. SoDA also includes a repository of prepared programs, which are from small to large sizes and can be used for experimentation and as a benchmark for code coverage related research.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975635,no
493,Using Dual Priority scheduling to improve the resource utilization in the nMPRA microcontrollers,"The current practice in most of the safety-critical areas, including automotive, avionics systems and factory automation, encouraging the use of real-time time-trigger schedulers that does not allow interference to take place between safety-critical components and non-critical. Furthermore, in these systems the lack of interference between safety-critical components and non-critical components is achieved by a strict isolation between components with different degrees of severity. This approach can assure, easily, the certification of the safety-critical functionality, but leads to very low resource utilization. For this purpose it will be presented a solution that when the system enters into a state that is different from the normal running state (test service), allowing relaxation and a change in the activation time of tasks (release) violating the fixed priorities scheduling, but avoiding starvation of the system tasks. The proposed solution modifies a static scheduler in a dynamic scheduler depending on the system status using Dual Priority scheduling. The algorithm has been proposed to be implemented on a nMPRA processor, by multiplying hardware resources (PC, pipeline registers and file registers) and other facilities (events, mutexes, interrupts, IPC communication, timer's, the static scheduler and support for dynamic scheduler) provides a switching and response time for events within 1 to 3 machine cycles.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842431,no
494,Using TTCN-3 to Test SPDY Protocol Interaction Property,"SPDY protocol is a new application-layer communication protocol which was proposed by Google in order to overcome the defects of HTTP. On the basis of HTTP protocol, SPDY offered four improvements to shorten the page loading time such as multiplexed requests, prioritized requests, server pushed streams and compressed headers. However, there is no much test work of the SPDY especially treating it as a black box, focusing on its interaction property, or testing it by using TTCN-3. In this paper, the interaction property of SPDY protocol is analyzed according to the SPDY protocol draft specification, and a novel test work designed in view of SPDY interaction property is implemented. During the test work, the interaction granularity of the SPDY peers is divided into three kinds of granularity from different levels, meanwhile the test cases were generated in accordance with the draft specification and encoded by utilizing TTCN-3 language for executing on the TT work bench Professional software. Above all we must ensure the SPDY server-side can support the SPDY protocol and the test host installs TT work bench Professional software to complete TTCN-3 testing. Finally, the interaction property of SPDY protocol was tested and the test results were reported. Moreover, some failed test cases were analyzed in detail.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903186,no
495,8th International Workshop on Search-Based Software Testing (SBST 2015),"This paper is a report on the 8th International Workshop on Search-Based Software Testing at the 37th International Conference on Sofrware Engineering (ICSE). Search-Based Software Testing (SBST) is a form of Search-Based Software Engineering (SBSE) that optimizes testing through the use of computational search. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service-orientated architectures, construct test suites for interaction testing, and validate real time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.Three full research papers, three short papers, and threeposition papers will be presented in the two-day workshop. Additionally, six development groups have pitted their test generation tools against a common set of programs and benchmarks, and will present their techniques and results. This report will give the background of the workshop and detail the provisional program.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203148,no
496,A Clustering-Bayesian Network Based Approach for Test Case Prioritization,"Test case prioritization can effectively reduce the cost of regression testing by executing test cases with respect to their contributions to testing goals. Previous research has proved that the Bayesian Networks based technique which uses source code change information, software quality metrics and test coverage data has better performance than those methods merely depending on only one of the items above. Although the former Bayesian Networks based Test Case Prioritization (BNTCP) focusing on assessing the fault detection capability of each test case can utilize all three items above, it still has a deficiency that ignores the similarity between test cases. For mitigating this problem, this paper proposes a hybrid regression test case prioritization technique which aims to achieve better prioritization by incorporating code coverage based clustering approach with BNTCP to depress the impact of those similar test cases having common code coverage. Experiments on two Java projects with mutation faults and one Java project with hand-seeded faults have been conducted to evaluate the fault detection performance of the proposed approach against Additional Greedy approach, Bayesian Networks based approach (BNTCP), Bayesian Networks based approach with feedback (BNA) and code coverage based clustering approach. The experimental results showed that the proposed approach is promising.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273420,yes
497,A coupling effect based test case prioritization technique,"Regression testing is a process that executes subset of tests that have already been conducted to ensure that changes have not propagated unintended side effects. Test case prioritization aims at reordering the regression test suit based on certain criteria, so that the test cases with higher priority can be executed first rather than those with lower priority. In this paper, a new approach for test case prioritization has been proposed which is based on a module-coupling effect that considers the module-coupling value for the purpose of prioritizing the modules in the software so that critical modules can be identified which in turn will find the prioritized set of test cases. In this way there will be high percentage of detecting critical errors that have been propagated to other modules due to any change in a module. The proposed approach has been evaluated with the case study of software consisting of ten modules.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100468,yes
498,A defect dependency based approach to improve software quality in integrated software products,"Integrated software products are complex in design. They are prone to defects caused by integrated and non-integrated modules of the entire integrated software suite. In such software products, a small proportion of defects are fixed as soon as they are reported. Rest of the defects are targeted for fixes in future product release cycles. Among such targeted defects, most of them seem to be insignificant and innocuous in the current version but have the potential to become acute in future versions. In this paper, we propose an approach to study defect dependency of the reported defect using a dependency metric. Identifying the dependency of a defect in an integrated product suite can help the product stake-owners to prioritize them and help improve software quality.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320343,no
499,A methodology for regression testing reduction and prioritization of agile releases,"Regression testing is the type of software testing that seeks to uncover new software bugs in existing areas of a system after changes have been made to them. The significance of regression testing have grown in the past decade with the amplified adoption of agile development methodologies, which requires the execution of regression testing at the end of each release. In this paper, we present an automated agile regression testing approach that reduces the number of test cases to be used at regression phase depending on the similarity of issues exposed from the different test cases, taking into consideration the user story coverage. It then prioritizes the reduced test cases using user-provided weighted agile parameters. The proposed approach achieves enhancement for both the reduction and prioritization of test cases for agile regression testing.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426903,yes
500,A novel dynamic analysis of test cases to improve testing efficiency in object-oriented systems,"In this paper, we present a series of methods to improve testing efficiency especially for regression testing from a novel view, namely dynamic analysis of test cases suitable for class testing in object-oriented systems. We mine static call graphs and dynamic call trees to represent the static features and dynamic tests of the program. By graph analysis, we present a series of methods and testing criteria to evaluate test cases from the view of code coverage. These methods improve testing efficiency for class testing from the following aspects: automation; multi-angle evaluations of test cases; improvement and management of test cases; providing different prioritization criteria and optimization criteria for regression testing to meet different testing requirements etc. What's more, they can be used in large-scale OO systems, and the test results are quantifiable.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490789,yes
501,A Schema Support for Selection of Test Case Prioritization Techniques,"Regression testing is a vast field of research. It is very costly and time consuming process but on the other hand very important process in software testing. Retest all, Test case Selection, Hybrid and Test Case Prioritization are its various techniques which are used to reduce the efforts in maintenance phase. In technical literature several techniques are present with their different and vast number of goals which can be applied in software projects despite of that they have not proven their true efficiency in the testing process. The major problem in regression testing area is to select the test case prioritization technique/s that is effective in such a way that maximum project characteristics should be cover in a minimum time span. However, consideration of this decision be carefully done so that loss of resources can be avoided in a software project. Based on the above scenario, author proposes a selection schema to support the selection of TCP techniques for a given software project aiming at maximizing the coverage of software project characteristics considering aspect of prioritization of software project characteristics. At the end, preliminary results of an experimental evaluation are presented. The purpose of this research is decision should be based on the objective knowledge of the techniques rather than considering some perception and assumptions.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079143,yes
502,A similarity-based approach for test case prioritization using historical failure data,"Test case prioritization is a crucial element in software quality assurance in practice, specially, in the context of regression testing. Typically, test cases are prioritized in a way that they detect the potential faults earlier. The effectiveness of test cases, in terms of fault detection, is estimated using quality metrics, such as code coverage, size, and historical fault detection. Prior studies have shown that previously failing test cases are highly likely to fail again in the next releases, therefore, they are highly ranked, while prioritizing. However, in practice, a failing test case may not be exactly the same as a previously failed test case, but quite similar, e.g., when the new failing test is a slightly modified version of an old failing one to catch an undetected fault. In this paper, we define a class of metrics that estimate the test cases quality using their similarity to the previously failing test cases. We have conducted several experiments with five real world open source software systems, with real faults, to evaluate the effectiveness of these quality metrics. The results of our study show that our proposed similarity-based quality measure is significantly more effective for prioritizing test cases compared to existing test case quality measures.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381799,yes
503,A simple statically reconfigurable processor architecture,"A reconfigurable Processor architecture is presented that has been designed prioritizing modularity, scalability and simplicity. Modular design enables swapping of functional units within the main processing core while maintaining the same programming model. This ensures that the associated software tools chain such as Assembler and Compiler need not be redesigned. Scalable design enables reconfiguring the datapath width to suite application requirements without redesigning the processor architecture or making changes to the software program already written. Applications for such design range from academia where real world performance of many proposed Adder/Multiplier structures may be tested; to data centers where the nature of operation to be performed on massive chunks of data changes regularly requiring ASIC like performance.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226112,no
504,A Subsumption Hierarchy of Test Case Prioritization for Composite Services,"Many composite workflow services utilize non-imperative XML technologies such as WSDL, XPath, XML schema, and XML messages. Regression testing should assure the services against regression faults that appear in both the workflows and these artifacts. In this paper, we propose a refinement-oriented level-exploration strategy and a multilevel coverage model that captures progressively the coverage of different types of artifacts by the test cases. We show that by using them, the test case prioritization techniques initialized on top of existing greedy-based test case prioritization strategy form a subsumption hierarchy such that a technique can produce more test suite permutations than a technique that subsumes it. Our experimental study of a model instance shows that a technique generally achieves a higher fault detection rate than a subsumed technique, which validates that the proposed hierarchy and model have the potential to improve the cost-effectiveness of test case prioritization techniques.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839018,yes
505,A Test Framework for Communications-Critical Large-Scale Systems,"Today's large-scale systems couldn't function without the reliable availability of a range of network communications capabilities. Software, hardware, and communications technologies have been advancing throughout the past two decades. However, the methods that industry commonly uses to test large-scale systems that incorporate critical communications interfaces haven't kept pace. The need exists for a specifically tailored framework to achieve effective, precise testing of communications-critical large-scale systems. A proposed test framework offers an alternative to the current generic approaches that lead to inefficient, costly testing in industry. A case study illustrates its benefits, which can also be realized with other comparable systems.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6785925,no
506,AEGIS autonomous targeting for the Curiosity rover's ChemCam instrument,"AEGIS (Autonomous Exploration for Gathering Increased Science) is a software suite that will imminently be operational aboard NASA's Curiosity Mars rover, allowing the rover to autonomously detect and prioritize targets in its surroundings, and acquire geochemical spectra using its ChemCam instrument. ChemCam, a Laser-Induced Breakdown Spectrometer (LIBS), is normally used to study targets selected by scientists using images taken by the rover on a previous sol and relayed by Mars orbiters to Earth. During certain mission phases, ground-based target selection entails significant delays and the use of limited communication bandwidth to send the images. AEGIS will allow the science team to define the properties of preferred targets, and obtain geochemical data more quickly, at lower data penalty, without the extra ground-inthe-loop step. The system uses advanced image analysis techniques to find targets in images taken by the rover's stereo navigation cameras (NavCam), and can rank, filter, and select targets based on properties selected by the science team. AEGIS can also be used to analyze images from ChemCam's Remote Micro Imager (RMI) context camera, allowing it to autonomously target very fine-scale features - such as veins in a rock outcrop - which are too small to detect with the range and resolution of NavCam. AEGIS allows science activities to be conducted in a greater range of mission conditions, and saves precious time and command cycles during the rover's surface mission. The system is currently undergoing initial tests and checkouts aboard the rover, and is expected to be operational by late 2015. Other current activities are focused on science team training and the development of target profiles for the environments in which AEGIS is expected to be used on Mars.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444544,no
507,An effective test case prioritization method based on fault severity,"In regression testing area, test case prioritization is one of the main techniques to improve the test validity and test effectiveness. However, when the test cases have the same maximum coverage rate, the random selection of the additional statement will influence the effect of sorting. For dealing with this problem, a new method is proposed to optimize test case prioritization based on fault severity, referred to as additional-statement-on-fault-severity. Facing those same maximum coverage rate, the new technique main consider a factor, fault severity, to sort test cases, it figures out the value of test case based on the algorithm of the new technique and order the sequence from high to low. Experiment results show that the improved technique of test case prioritizaftion can improve the efficiency of regression testing.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339162,yes
508,An Information Retrieval Approach for Regression Test Prioritization Based on Program Changes,"Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight open-source Java projects shows that REPiR is computationally efficient and performs better than existing (dynamic or static) techniques for the majority of subject systems.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194580,yes
509,Applying Ant Colony Optimization in software testing to generate prioritized optimal path and test data,"Software testing is one of the most important parts of software development lifecycle. Among various types of software testing approaches structural testing is widely used. Structural testing can be improved largely by traversing all possible code paths of the software. Genetic algorithm is the most used search technique to automate path testing and test case generation. Recently, different novel search based optimization techniques such as Ant Colony Optimization (ACO), Artificial Bee Colony (ABC), Artificial Immune System (AIS), Particle Swarm Optimization (PSO) have been applied to generate optimal path to complete software coverage. In this paper, ant colony optimization (ACO) based algorithm has been proposed which will generate set of optimal paths and prioritize the paths. Additionally, the approach generates test data sequence within the domain to use as inputs of the generated paths. Proposed approach guarantees full software coverage with minimum redundancy. This paper also demonstrates the proposed approach applying it in a program module.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307500,yes
510,Approximating Attack Surfaces with Stack Traces,"Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202964,no
511,Architecting to Ensure Requirement Relevance: Keynote TwinPeaks Workshop,"Research has shown that up to two thirds of features in software systems are hardly ever used or not even used at all. This represents a colossal waste of R&amp;D resources and occurs across the industry. On the other hand, product management and many others work hard at interacting with customers, building business cases and prioritizing requirements. A fundamentally different approach to deciding what to build is required: requirements should be treated as hypothesis throughout the development process and constant feedback from users and systems in the field should be collected to dynamically reprioritize and change requirements. This requires architectural support beyond the current state of practice as continuous deployment, split testing and data collection need to be an integral part of the architecture. In this paper, we present a brief overview of our research and industry collaboration to address this challenge.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184704,no
512,Challenges and Issues of Mining Crash Reports,"Automatic crash reporting tools built in many software systems allow software practitioners to understand the origin of field crashes and help them prioritise field crashes or bugs, locate erroneous files, and/or predict bugs and crash occurrences in subsequent versions of the software systems. In this paper, after illustrating the structure of crash reports in Mozilla, we discuss some techniques for mining information from crash reports, and highlight the challenges and issues of these techniques. Our aim is to raise the awareness of the research community about issues that may bias research results obtained from crash reports and provide some guidelines to address certain challenges related to mining crash reports.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070480,no
513,Clustering based novel test case prioritization technique,"Regression testing is an activity during the maintenance phase to validate the changes made to the software and to ensure that these changes would not affect the previously verified code or functionality. Often, regression testing is performed with limited computing resources and time budget. So in this phase, it is infeasible to run the complete test suite Thus, test-case prioritization approaches are applied to ensure the execution of test cases in some prioritized order and to achieve some specific goals like, increasing the rate of bug detection, identifying the most critical bugs as early as possible etc. In this research work, we are going to propose a new and more effective clustering based prioritization technique that uses various metrics and execution time of test cases to reorder them. The results of implementation will prove that the suggested approach is more productive than the existing coverage and clustering based prioritization techniques.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506447,yes
514,Communication and collaboration of heterogeneous unmanned systems using the joint architecture for Unmanned Systems (JAUS) standards,"The Naval Undersea Warfare Center Division Newport (NUWCDIVNPT) and Georgia Tech Research Institute (GTRI) completed a successful at-sea exercise with autonomous UAS and UUV systems demonstrating cross-domain unmanned system communication and collaboration. The exercise was held at the NUWC Narragansett Bay Shallow Water Test Facility (NBSWTF) range, and it represented for the first time the use of standard protocols and formats that effectively support cross-domain unmanned system operations. Four man-portable Iver2 UUVs operating in coordinated missions autonomously collected environmental data, which was compressed in-stride, re-formatted, and exfiltrated via UAS relay for display and tactical decision making. Two UAS with autonomous flight take-off and mission execution were sequenced to serve as ISR platforms and to support communications as RF relays for the UUVs performing Intelligence Preparation of the Environment missions. Two Command and Control nodes ashore provided unmanned system tasking and re-tasking, and they served to host and display both geo-positional data and status for UAS and UUV vehicles during the operational scenarios run during the exercise. The SAE Joint Architecture for Unmanned Systems (JAUS) standards were used for all message traffic between shore-based C2 nodes, UAS, and UUVs active in the NBSWTF exercise area. Exercise goals focused on CNO priorities expressed in the Undersea Domain Operating Concept of AUG 2013 which emphasized protocols essential to effective command and control of networked unmanned systems with decentralization and flexibility of command structures. Development for this project highlighted both the strengths and shortfalls of JAUS and captured the requirements for moving forward in effective cross-domain communications that support distributed, agile C2 nodes to meet evolving CONOPS for growing unmanned system presence and mission roles. The scenario employed operating parameters for UAS and UUV that have been established in real-world operations and ongoing unmanned system programs. The tactical information from unmanned systems was displayed in real-time on shore-based C2 displays: the tactical FalconView display and the developmental TOPSIDE command and control station. This work represents a critical step in communications for networking of heterogeneous unmanned systems and establishes a solid platform for alignment of development and ongoing programs. The evaluation of JAUS suitability for near-term operational applications provides significant value as Concepts of Operation that rely on netted heterogeneous systems are being targeted. The focus on affordable commercial unmanned systems for this experimentation establishes the value of highly capable, portable systems to provide economical development and test opportunities with low-cost and low-risk alternatives to many planned and fielded systems. The JAUS architecture was introduced to the NUWC and GTRI unmanned systems though an instantiation of the Mission Oriented Operating Suite (MOOS) autonomy framework on secondary CPUs integrated into the Iver2 UUVs and the GTRI UAS. Since the GTRI UASs already had ROS installed, a MOOS-ROS bridge was employed to support use of the developed JAUS messaging capability. Established JAUS services were employed where the required functions could be met. New JAUS services were developed to meet functionality required for the operational scenarios in this exercise but not yet supported in the existing releases of SAE JAUS. Independent C++ header libraries that could be compiled at run time for specific autonomy frameworks, such as MOOS, were employed to support a software-agnostic approach. Immediate targets for broadening the influence of this work to coalition partners include the NATO Recognized Environmental Picture (REP) 2015 and The Technical Cooperation Program (TTCP) 2015 exercises. This project and demonstration was funded under a NUWC Strategic Initiative and GTRI program support.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271613,yes
515,Component based reliability assessment from UML models,"Model based development and testing techniques have ventured diverse research directions to assure quality of the software product. Models developed during architecture and design phases are efficient tools to assess quality at an early development stage. However, testing the extra-functional or non-functional properties of software systems is not frequently practised eg. reliability. The motivation to our work is to model the context of execution which is significant in system reliability analysis. In this paper we visualize the components of complex software systems and their interactions in the form of Functional Flow Diagram (FFD). This notation specifies the dynamic aspect of system behavior as the context of execution. To further asses reliability, the FFD is translated into Reliability Block Diagram (RBD). The relative importance of the components in terms of reliability is evaluated and is associated with prioritization of the component. The model is simple but significant for system maintenance, improvisation and modification. This model supports analysis and testing through better understanding of the interacting components and their reliabilities.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275704,no
516,Component-Based Software System Test Case Prioritization with Genetic Algorithm Decoding Technique Using Java Platform,"Test case prioritization includes testing experiments in a request that builds the viability in accomplishing some execution objectives. The importance amongst the most imperative testing objectives is the fast rate of fault recognition. Test case ought to run in a request that extends the likelihood of fault discovery furthermore that detects the most serious issues at the early stage of testing life cycle. In this paper, we develop and prove the necessity of Component-Based Software testing prioritization framework which plans to uncover more extreme bugs at an early stage and to enhance software product deliverable quality utilizing Genetic Algorithm (GA) with java decoding technique. For this, we propose a set of prioritization keys to plan the proposed Component-Based Software java framework. In our proposed method, we allude to these keys as Prioritization Keys (PK). These keys may be project size, scope of the code, information stream, and bug inclination and impact of fault or bug on overall system, which prioritizes the Component-Based Software framework testing. The integrity of these keys was measured with implementation of key assessment metric called KAM that will likewise be ascertained. This paper demonstrates how software testing can be efficient with management of data integrity factor to avoid major security issues. One of the main advantages of our approach is that domain specific semantics can be integrated with the data quality test cases prioritization, thus being able to discover test feed data quality problems beyond conventional quality measures.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155967,yes
517,Considerations on application of selective hardening based on software fault tolerance techniques,"This paper analyses the nature of fault tolerance software-based techniques and the influence of their overheads to determine an efficient strategy for applying those techniques in a selective way. Several considerations that have to be taken into account are presented in this work. These include an analysis of fault coverage and overheads when selective hardening is adopted; side effects of selective protection based on software; and the need of new criticality metrics, apart from those used for hardware-based techniques (e.g., AVF), to facilitate and prioritize the selection of resources to be protected.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102509,no
518,Decision support system prototype on obstetrics ultrasonography for primary service physicians,"Introduction. The National Social Security System (SJSN) prioritizes primary service as a spearhead to assist Primary Care Physicians to make medical decisions. The purpose of this research is to develop computer software that will assist primary care physicians in the fields of Obstetrics Ultrasonography, related to referral decision-making abilities. Methods. A quasi-experimental post-test only design without a control group. The stages of the research process: Systems Analysis and Design, Prototyping and Testing by Lecture, Students, Programmers and Doctors. Results. From Analysis and Systems design document, has been produced prototype of software, and a test run has been proven successful Decision Support System software helping doctors develop diagnosis and specialty referrals. Conclusion: The Decision Support System software can be used in Obstetrics Ultrasonography by Primary Care Physicians to provide aid in their diagnosis and referrals. Before it is used, it is recommended for trainings and application tests.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401367,no
519,Detecting Display Energy Hotspots in Android Apps,"Energy consumption of mobile apps has become an important consideration as the underlying devices are constrained by battery capacity. Display represents a significant portion of an app's energy consumption. However, developers lack techniques to identify the user interfaces in their apps for which energy needs to be improved. In this paper, we present a technique for detecting display energy hotspots - user interfaces of a mobile app whose energy consumption is greater than optimal. Our technique leverages display power modeling and automated display transformation techniques to detect these hotspots and prioritize them for developers. In an evaluation on a set of popular Android apps, our technique was very accurate in both predicting energy consumption and ranking the display energy hotspots. Our approach was also able to detect display energy hotspots in 398 Android market apps, showing its effectiveness and the pervasiveness of the problem. These results indicate that our approach represents a potentially useful technique for helping developers to detect energy related problems and reduce the energy consumption of their mobile apps.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102585,no
520,Detecting hardware Trojans in unspecified functionality using mutation testing,"Existing functional Trojan detection methodologies assume Trojans violate the design specification under carefully crafted rare triggering conditions. We present a new type of Trojan that leaks secret information from the design by only modifying unspecified functionality, meaning the Trojan is no longer restricted to being active only under rare conditions. We provide a method based on mutation testing for detecting this new Trojan type along with mutant ranking heuristics to prioritize analysis of the most dangerous functionality. Applying our method to a UART controller design, we discover unspecified and untested bus functionality with the potential to leak 32 bits of information during hundreds of cycles without being detected! Our method also reveals poorly tested interrupt functionality with information leakage potential. After modifying the specification and test bench to remove the discovered vulnerabilities, we close the verification loop by re-analyzing the design using our methodology and observe the functionality is no longer flagged as dangerous.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372619,no
521,Effective test strategy for testing automotive software,"Electronic content is increasing in automobiles day by day. Functionalities like Air Bags, Anti-lock Braking, Driver Assistance Systems, Body Controllers, Passive entry Passive Start, Electronic Power Steering etc. are realized electronically with complex software. These functionalities are related to automobile system safety. Hence, safety is one of the key issues of future automobile development. Risk of system failure is high due to increasing technological complexity and software content. The software shall be tested well to arrest almost all the defects. This paper explains a test case development and execution strategy based on practical implementation. It explains how test case reduction using Taguchi method, prioritization of test execution and automation help to make testing effective. It also demonstrates how maximum defects are discovered in short time.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150821,no
522,Effective verification of low-level software with nested interrupts,"Interrupt-driven software is difficult to test and debug, especially when interrupts can be nested and subject to priorities. Interrupts can arrive at arbitrary times, leading to an explosion in the number of cases to be considered. We present a new formal approach to verifying interrupt-driven software based on symbolic execution. The approach leverages recent advances in the encoding of the execution traces of interacting, concurrent threads. We assess the performance of our method on benchmarks drawn from embedded systems code and device drivers, and experimentally compare it to conventional formal approaches that use source-to-source transformations. Our experimental results show that our method significantly outperforms conventional techniques. To the best of our knowledge, our technique is the first to demonstrate effective formal verification of low-level embedded software with nested interrupts.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092387,no
523,Feasibility Analysis of Engine Control Tasks under EDF Scheduling,"Engine control applications include software tasks that are triggered at predetermined angular values of the crankshaft, thus generating a computational workload that varies with the engine speed. To avoid overloads at high rotation speeds, these tasks are implemented to self adapt and reduce their computational demand by switching mode at given rotation speeds. For this reason, they are referred to as adaptive variable rate (AVR) tasks. Although a few works have been proposed in the literature to model and analyze the schedulability of such a peculiar type of tasks, an exact analysis of engine control applications has been derived only for fixed priority systems, under a set of simplifying assumptions. The major problem of scheduling AVR tasks with fixed priorities, however, is that, due to engine accelerations, the interarrival period of an AVR task is subject to large variations, therefore there will be several speeds at which any fixed priority assignment is far from being optimal, significantly penalizing the schedulability of the system. This paper proposes for the first time an exact feasibility test under the Earliest Deadline First scheduling algorithm for tasks sets including regular periodic tasks and AVR tasks triggered by a common rotation source. In addition, a set of simulation results are reported to evaluate the schedulability gain achieved in this context by EDF over fixed priority scheduling.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176033,no
524,Fusion of LIDAR and video cameras to augment medical training and assessment,"The Mobile Medical Lane Trainer (MMLT) is a multi-sensor rapidly deployed After-Action Review (AAR) system for Army medical lane training. Current AAR systems have two main drawbacks: 1) video does not provide a complete view of the medical and tactical situation, and 2) the video is not readily available for effective evaluation. The MMLT program is developing a ƒ??smarterƒ? AAR system by using 3D LIDAR (LIght Detection And Ranging), a camera array, People Tracking software and Medical Training Evaluation and Review (MeTER) software. This system can be brought to the field and deployed in less than an hour to provide hands-off data collection for the exercise. MMLT supplements existing evaluation systems deployed at the Medical Simulation Training Centers (MSTCs) by providing a 3-D perspective of the training event for tactical evaluation with synchronized video technology to capture both tactical and clinical skills and instructor scoring. This capability is used in conjunction with the MeTER system's skill assessment checklists for automated performance review. An immediate synchronized playback capability has been developed, ultimately resulting in a rapid AAR for debriefing. This paper will discuss the technical components of the system, including hardware components, data fusion technique, tracking algorithms, and camera prioritization approaches, and will conclude with operational test results and lessons learned.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7295832,no
525,GUI Test Case Prioritization by State-Coverage Criterion,"Graphical User Interface (GUI) application is a kind of typical event-driven software (EDS) that transforms state according to input events invoked through a user interface. It is time consuming to test a GUI application since there are a large number of possible event sequences generated by the permutations and combinations of user operations. Although some GUI test case prioritization techniques have been proposed to determine ""which test case to execute next"" for early fault detection, most of them use random ordering to break tie cases, which has been proved to be ineffective. Recent research presents the opinion that using hybrid criteria can be an effective way for tie-breaking, but few studies focus on seeking a new criterion cooperating well with other criteria when breaking tie cases. In this paper, we propose a state-distance-based method using state coverage as a new criterion to prioritize GUI test cases. An empirical study on three GUI programs reveals that the state-distance-based method is really suitable for GUI test case prioritization and can cooperate well with the (additional) event length criterion.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166260,yes
526,History-Based Test Case Prioritization for Black Box Testing Using Ant Colony Optimization,"Test case prioritization is a technique to improve software testing. Although a lot of work has investigated test case prioritization, they focus on white box testing or regression testing. However, software testing is often outsourced to a software testing company, in which testers are rarely able to access to source code due to a contract. Herein a framework is proposed to prioritize test cases for black box testing on a new product using the test execution history collected from a similar prior product and the Ant Colony Optimization. A simulation using two actual products shows the effectiveness and practicality of our proposed framework.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102622,yes
527,Improving Multi-Objective Test Case Selection by Injecting Diversity in Genetic Algorithms,"A way to reduce the cost of regression testing consists of selecting or prioritizing subsets of test cases from a test suite according to some criteria. Besides greedy algorithms, cost cognizant additional greedy algorithms, multi-objective optimization algorithms, and multi-objective genetic algorithms (MOGAs), have also been proposed to tackle this problem. However, previous studies have shown that there is no clear winner between greedy and MOGAs, and that their combination does not necessarily produce better results. In this paper we show that the optimality of MOGAs can be significantly improved by diversifying the solutions (sub-sets of the test suite) generated during the search process. Specifically, we introduce a new MOGA, coined as DIversity based Genetic Algorithm (DIV-GA), based on the mechanisms of orthogonal design and orthogonal evolution that increase diversity by injecting new orthogonal individuals during the search process. Results of an empirical study conducted on eleven programs show that DIV-GA outperforms both greedy algorithms and the traditional MOGAs from the optimality point of view. Moreover, the solutions (sub-sets of the test suite) provided by DIV-GA are able to detect more faults than the other algorithms, while keeping the same test execution cost.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936894,yes
528,Improving prioritization of software weaknesses using security models with AVUS,"Testing tools for application security have become an integral part of secure development life-cycles. Despite their ability to spot important software weaknesses, the high number of findings require rigorous prioritization. Most testing tools provide generic ratings to support prioritization. Unfortunately, ratings from established tools lack context information especially with regard to the security requirements of respective components or source code. Thus experts often spend a great deal of time re-assessing the prioritization provided by these tools. This paper introduces our lightweight tool AVUS that adjusts context-free ratings of software weaknesses according to a user-defined security model. We also present a first evaluation applying AVUS to a well-known open source project and the findings of a popular, commercially available application security testing tool.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335423,no
529,Improving reliability using software operational profile and testing profile,"Software testing has ever remained a challenge particularly when testing is done with intention in enhancing the reliability. Conventional testing is increasing the testing in an unpredictable way by reducing the number of faults. There is a need to enhance the reliability by assigning probabilistic priorities to testing mechanism, which is done through software operational profile. This study adopts a case study to generate test cases and test suites with perspective of probabilistic reliability using the proposed framework based on software operational profile and testing profile.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219603,no
530,Introducing Continuous Delivery of Mobile Apps in a Corporate Environment: A Case Study,"Software development is conducted in increasingly dynamic business environments. Organizations need the capability to develop, release and learn from software in rapid parallel cycles. The abilities to continuously deliver software, to involve users, and to collect and prioritize their feedback are necessary for software evolution. In 2014, we introduced Rugby, an agile process model with workflows for continuous delivery and feedback management, and evaluated it in university projects together with industrial clients. Based on Rugby's release management workflow we identified the specific needs for project-based organizations developing mobile applications. Varying characteristics and restrictions in projects teams in corporate environments impact both process and infrastructure. We found that applicability and acceptance of continuous delivery in industry depend on its adaptability. To address issues in industrial projects with respect to delivery process, infrastructure, neglected testing and continuity, we extended Rugby's workflow and made it tailor able. Eight projects at Capgemini, a global provider of consulting, technology and outsourcing services, applied a tailored version of the workflow. The evaluation of these projects shows anecdotal evidence that the application of the workflow significantly reduces the time required to build and deliver mobile applications in industrial projects, while at the same time increasing the number of builds and internal deliveries for feedback.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167166,no
531,Investigating the Correspondence between Mutations and Static Warnings,"This paper provides evidences on the correspondence between mutations and static warnings. We used mutation operators as a fault model to evaluate the direct correspondence between mutations and static warnings. The main advantage of using mutation operators is that they generate a large number of programs containing faults of different types, which can be used to decide the ones most probable to be detected by static analyzers. Since static analyzers, in general, report a substantial number of false positive warnings, the intention of this study is to define a prioritization approach of static warnings based on the probability they correspond to a true positive and lead to detect software faults. The results obtained for a set of open-source programs indicate that a correspondence exist when considering specific mutation operators such that static warnings may be prioritized based on their correspondence level with mutations.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328004,no
532,Kvazaar HEVC encoder for efficient intra coding,"This paper presents an open-source Kvazaar encoder for HEVC intra coding. This academic software encoder has been developed from the scratch using C as an implementation language by prioritizing modularity, portability, and readability of the source code. Kvazaar implements almost the same intra coding functionality as HEVC reference encoder (HM) but its rewritten source code makes it significantly faster. In all-intra (AI) coding, a single-threaded C implementation of Kvazaar is 2.3 times faster than HM at a cost of 1.7% bit rate increase. The respective values with a high speed preset of Kvazaar are 10.6 and 8.8%. Compared to a single-threaded C++ implementation of x265, Kvazaar improves rate-distortion performance and increases encoding speed in both high-quality and high-speed test cases. Kvazaar has a particular edge in the high-speed test case where it almost halves the BD-rate loss and more than doubles the performance.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168970,no
533,Lexical Parsing Expression Recognition Schemata,"Parsing expression grammars (PEGs) have emerged as a promising substitute for context-free grammars (CFGs) and regular expressions (REs) in programming language specification. The benefits of PEGs are twofold. First, parsing expression grammars replace unordered choice between alternatives by prioritized choice, which naturally solves the ubiquitous ""dangling else"" problem in grammar definitions. Second, PEGs employ ""character-level syntax"" specifications that eliminate the need to separate the lexical and hierarchical components of a language specification. However, there is ""no free lunch"" in PEGs. PEGs capture only syntactic relationships, but many language constructs cannot be parsed without additional semantic information. Moreover, character-level specifications can become unwieldy, as every aspect of the language, including spacing, has to be accounted for. To overcome these issues, we extend the original PEG formalism to incorporate semantic predicates that yield a programmatic means for state-based token recognition control. Furthermore, rather than requiring a single complete specification, we capture lexical components as PEG closures that provide a self-contained token recognition mechanism to reduce the clutter associated with purely character-level PEGs. To test the effectiveness of our approach, we use it for the construction of a Delphi language front-end and practically confirm that Ford's theoretical linear-time result also holds for PEG closures.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365805,no
534,METEOR: An Enterprise Health Informatics Environment to Support Evidence-Based Medicine,"Goal: The aim of this paper is to propose the design and implementation of next-generation enterprise analytics platform developed at the Houston Methodist Hospital (HMH) system to meet the market and regulatory needs of the healthcare industry. Methods: For this goal, we developed an integrated clinical informatics environment, i.e., Methodist environment for translational enhancement and outcomes research (METEOR). The framework of METEOR consists of two components: the enterprise data warehouse (EDW) and a software intelligence and analytics (SIA) layer for enabling a wide range of clinical decision support systems that can be used directly by outcomes researchers and clinical investigators to facilitate data access for the purposes of hypothesis testing, cohort identification, data mining, risk prediction, and clinical research training. Results: Data and usability analysis were performed on METEOR components as a preliminary evaluation, which successfully demonstrated that METEOR addresses significant niches in the clinical informatics area, and provides a powerful means for data integration and efficient access in supporting clinical and translational research. Conclusion: METEOR EDW and informatics applications improved outcomes, enabled coordinated care, and support health analytics and clinical research at HMH. Significance: The twin pressures of cost containment in the healthcare market and new federal regulations and policies have led to the prioritization of the meaningful use of electronic health records in the United States. EDW and SIA layers on top of EDW are becoming an essential strategic tool to healthcare institutions and integrated delivery networks in order to support evidence-based medicine at the enterprise level.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7137654,no
535,Modification Impact Analysis Based Test Case Prioritization for Regression Testing of Service-Oriented Workflow Applications,"Test case prioritization for regression testing is an approach that schedules test cases to improve the efficiency of service-oriented workflow application testing. Most of existing prioritization approaches range test cases according to various metrics (e.g., Statement coverage, path coverage) in different application context. Service-oriented workflow applications orchestrate web services to provide value-added service and typically are long-running and time-consuming processes. Therefore, these applications need more precise prioritization to execute earlier those test cases that may detect failures. Surprisingly, most of current regression test case prioritization researches neglect to use internal structure information of software, which is a significant factor influencing the prioritization of test cases. Considering the internal structure information and fault propagation behavior of modifications respect to modified version for service-oriented workflow applications, we present in this paper a new regression test case prioritization approach. Our prioritization approach schedules test cases based on dependence analysis of internal activities in service-oriented workflow applications. Experimental results show that test case prioritization using our approach is more effective than conventional coverage-based techniques.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273631,yes
536,Multi-perspective Regression Test Prioritization for Time-Constrained Environments,"Test case prioritization techniques are widely used to enable reaching certain performance goals during regression testing faster. A commonly used goal is high fault detection rate, where test cases are ordered in a way that enables detecting faults faster. However, for optimal regression testing, there is a need to take into account multiple performance indicators, as considered by different project stakeholders. In this paper, we introduce a new optimal multi-perspective approach for regression test case prioritization. The approach is designed to optimize regression testing for faster fault detection integrating three different perspectives: business perspective, performance perspective, and technical perspective. The approach has been validated in regression testing of industrial mobile device systems developed in continuous integration. The results show that our proposed framework efficiently prioritizes test cases for faster and more efficient regression fault detection, maximizing the number of executed test cases with high failure frequency, high failure impact, and cross-functional coverage, compared to manual practice.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272927,yes
537,Mutation-based test-case prioritization in software evolution,"During software evolution, to assure the software quality, test cases for an early version tend to be reused by its latter versions. As a large number of test cases may aggregate during software evolution, it becomes necessary to schedule the execution order of test cases so that the faults in the latter version may be detected as early as possible, which is test-case prioritization in software evolution. In this paper, we proposed a novel test-case prioritization approach for software evolution, which first uses mutation faults on the difference between the early version and the latter version to simulate real faults occurred in software evolution, and then schedules the execution order of test cases based on their fault-detection capability, which is defined based on mutation faults. In particular, we present two models on calculating fault-detection capability, which are statistics-based model and probability-based model. Moreover, we conducted an experimental study and found that our approach with the statistics-based model outperforms our approach with the probability-based model and the total statement coverage-based approach, and slightly outperforms the additional statement-coverage based approach in many cases. Furthermore, compared with the total or additional statement coverage-based approach, our approach with either the statistics-based model or the probability-based model tends to be stably effective when the difference on the source code between the early version and the latter version is non-trivial.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381798,yes
538,PORA: Proportion-Oriented Randomized Algorithm for Test Case Prioritization,"Effective testing is essential for assuring software quality. While regression testing is time-consuming, the fault detection capability may be compromised if some test cases are discarded. Test case prioritization is a viable solution. To the best of our knowledge, the most effective test case prioritization approach is still the additional greedy algorithm, and existing search-based algorithms have been shown to be visually less effective than the former algorithms in previous empirical studies. This paper proposes a novel Proportion-Oriented Randomized Algorithm (PORA) for test case prioritization. PORA guides test case prioritization by optimizing the distance between the prioritized test suite and a hierarchy of distributions of test input data. Our experiment shows that PORA test case prioritization techniques are as effective as, if not more effective than, the total greedy, additional greedy, and ART techniques, which use code coverage information. Moreover, the experiment shows that PORA techniques are more stable in effectiveness than the others.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272924,yes
539,Practical Combinatorial Interaction Testing: Empirical Findings on Efficiency and Early Fault Detection,"Combinatorial interaction testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. Simulated Annealing (SA) and Greedy Algorithms have been widely used to find CIT test suites. From the literature, there is a widely-held belief that SA is slower, but produces more effective tests suites than Greedy and that SA cannot scale to higher strength coverage. We evaluated both algorithms on seven real-world subjects for the well-studied two-way up to the rarely-studied six-way interaction strengths. Our findings present evidence to challenge this current orthodoxy: real-world constraints allow SA to achieve higher strengths. Furthermore, there was no evidence that Greedy was less effective (in terms of time to fault revelation) compared to SA; the results for the greedy algorithm are actually slightly superior. However, the results are critically dependent on the approach adopted to constraint handling. Moreover, we have also evaluated a genetic algorithm for constrained CIT test suite generation. This is the first time strengths higher than 3 and constraint handling have been used to evaluate GA. Our results show that GA is competitive only for pairwise testing for subjects with a small number of constraints.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081752,yes
540,Preemptive Regression Testingof Workflow-Based Web Services,"An external web service may evolve without prior notification. In the course of the regression testing of a workflow-based web service, existing test case prioritization techniques may only verify the latest service composition using the not-yet-executed test cases, overlooking high-priority test cases that have already been applied to the service composition before the evolution. In this paper, we propose Preemptive Regression Testing (PRT), an adaptive testing approach to addressing this challenge. Whenever a change in the coverage of any service artifact is detected, PRT recursively preempts the current session of regression test and creates a sub-session of the current test session to assure such lately identified changes in coverage by adjusting the execution priority of the test cases in the test suite. Then, the sub-session will resume the execution from the suspended position. PRT terminates only when each test case in the test suite has been executed at least once without any preemption activated in between any test case executions. The experimental result confirms that testing workflow-based web service in the face of such changes is very challenging; and one of the PRT-enriched techniques shows its potential to overcome the challenge.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6812226,no
541,Prioritization and ranking of ERP testing components,"Software Testing is one of the most important activities but more often than not attracts less attention than it deserves in software development and implementation. It often takes twenty to sometimes even more than fifty percent of the total software development time. Enterprise Resource Planning (ERP) Systems provide synergy by integrating all operations of an enterprise. So implementation of ERP systems need even more rigorous testing than that employed in stand-alone software development. Software testing is a well-researched area but software testing as employed on ERP systems albeit is droughted with respect to research. This research paper is an extension of the patent by Kapur et al.[8] that identified the ERP Testing Components to measure ERP testing efficiency. Here, the ERP Testing Components have been accumulated and categorized under five heads. Thereafter, these testing components have been prioritized and ranked with the help of Analytic Hierarchy Process (AHP), as given by Saaty [17].",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359245,yes
542,Prioritization of test scenarios using hybrid genetic algorithm based on UML activity diagram,"Software testing is an essential part of the SDLC(Software Development Life Cycle). Test scenarios are used to derive test cases for model based testing. However, with the software rapidly growing in size and complexity, the cost of software will be too high if we want to test all the test cases. So this paper presents an approach using Hybrid Genetic Algorithm(HGA) to prioritize test scenarios, which improves efficiency and reduces cost as well. The algorithm combines Genetic Algorithm(GA) with Particle Swarm Optimization(PSO) algorithm and uses Local Search Strategy to update the local and global best information of the PSO. The proposed algorithm can prioritize test scenarios so as to find a critical scenario. Finally, the proposed method is applied to several typical UML activity diagrams, and compared with the Simple Genetic Algorithm(SGA). The experimental results show that the proposed method not only prioritizes test scenarios, but also improves the efficiency, and further saves effort, time as well as cost.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339189,yes
543,Prioritized test-driven reverse engineering process: A case study,"In this study we empirically investigate the adaptation of Test-Driven Development (TDD) practice into software Reverse Engineering (RE) process. We call this adaptation as Test-Driven Reverse Engineering (TDRE) process. We propose a two-layer prioritization process, which firstly prioritizes the already-implemented functionalities using the Cumulative Voting (CV) method and three prioritization criteria (importance, complexity and dependency), and secondly prioritizes test-cases for each prioritized functionality, using the same criteria. We conducted a case study in academia with students to empirically evaluate the usability and effectiveness of the prioritization process and the TDD adaptation into RE process. The results have shown that students with a good performance in testing had also good performance in designing UML class-diagrams. Moreover, the implementation of hierarchical test-cases for the already prioritized functionalities, improves code comprehension and redesigning in the RE process in terms of better total grades obtained.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7388099,yes
544,Prioritizing Manual Test Cases in Traditional and Rapid Release Environments,"Test case prioritization is one of the most practically useful activities in testing, specially for large scale systems. The goal is ranking the existing test cases in a way that they detect faults as soon as possible, so that any partial execution of the test suite detects maximum number of defects for the given budget. Test prioritization becomes even more important when the test execution is time consuming, e.g., manual system tests vs. automated unit tests. Most existing test case prioritization techniques are based on code coverage, which requires access to source code. However, manual testing is mainly done in a black- box manner (manual testers do not have access to the source code). Therefore, in this paper, we first examine the existing test case prioritization techniques and modify them to be applicable on manual black-box system testing. We specifically study a coverage- based, a diversity-based, and a risk driven approach for test case prioritization. Our empirical study on four older releases of Mozilla Firefox shows that none of the techniques are strongly dominating the others in all releases. However, when we study nine more recent releases of Firefox, where the development has been moved from a traditional to a more agile and rapid release environment, we see a very signifiant difference (on average 65% effectiveness improvement) between the risk-driven approach and its alternatives. Our conclusion, based on one case study of 13 releases of an industrial system, is that test suites in rapid release environments, potentially, can be very effectively prioritized for execution, based on their historical riskiness; whereas the same conclusions do not hold in the traditional software development environments.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102602,yes
545,Priority Integration for Weighted Combinatorial Testing,"Priorities (weights) for parameter values can improve the effectiveness of combinatorial testing. Previous approaches have employed weights to derive high-priority test cases either earlier or more frequently. Our approach integrates these order-focused and frequency-focused prioritizations. We show that our priority integration realizes a small test suite providing high-priority test cases early and frequently in a good balance. We also propose two algorithms that apply our priority integration to existing combinatorial test generation algorithms. Experimental results using numerous test models show that our approach improves the existing approaches w.r.t. Order-focused and frequency-focused metrics, while overheads in the size and generation time of test suites are small.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273624,no
546,Pushing to the top,"IC3 is undoubtedly one of the most successful and important recent techniques for unbounded model checking. Understanding and improving IC3 has been a subject of a lot of recent research. In this regard, the most fundamental questions are how to choose Counterexamples to Induction (CTIs) and how to generalize them into (blocking) lemmas. Answers to both questions influence performance of the algorithm by directly affecting the quality of the lemmas learned. In this paper, we present a new IC3-based algorithm, called QUIP1, that is designed to more aggressively propagate (or push) learned lemmas to obtain a safe inductive invariant faster. QUIP modifies the recursive blocking procedure of IC3 to prioritize pushing already discovered lemmas over learning of new ones. However, a naive implementation of this strategy floods the algorithm with too many useless lemmas. In QUIP, we solve this by extending IC3 with may-proof-obligations (corresponding to the negations of learned lemmas), and by using an under-approximation of reachable states (i.e., states that witness why a may-proof-obligation is satisfiable) to prune non-inductive lemmas. We have implemented QUIP on top of an industrial-strength implementation of IC3. The experimental evaluation on HWMCC benchmarks shows that the QUIP is a significant improvement (at least 2x in runtime and more properties solved) over IC3. Furthermore, the new reasoning capabilities of QUIP naturally lead to additional optimizations and new techniques that can lead to further improvements in the future.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542254,no
547,Quantifying security risk by measuring network risk conditions,"Software vulnerabilities are the weaknesses in the software that inadvertently allow dangerous operations. If the vulnerability is in a network service, it poses serious security threats because a cyber-attacker can exploit it to gain unauthorized access to the system. Hence, rapid discovery and remediation of network vulnerabilities is critical issues in network security. In today's dynamic IT environment, it is common practice that an organization prioritizes the mitigation of discovered vulnerabilities according to their risk levels. Currently available technologies, however, associate each vulnerability to the static risk level which does not take the unique characteristics of the target network into account. This often leads to inaccurate risk prioritization and less-than-optimal resource allocation. In this research, we introduce a novel way of quantifying the risk of network vulnerability by augmenting the static risk level with conditions specific to the target network. The method calculates the risk value of each vulnerability by measuring the proximity to the untrusted network and risk of the neighboring hosts. The resulting risk value, RCR is a composite index of the individual risk, network location and neighborhood risk conditions. Thus, it can be effectively used for prioritization, comparison and trending. We tested the methodology through the network intrusion simulation. The results shows average 88.9% the correlation between RCR and number of successful attacks on each vulnerability.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166562,no
548,Replicating and Re-Evaluating the Theory of Relative Defect-Proneness,"A good understanding of the factors impacting defects in software systems is essential for software practitioners, because it helps them prioritize quality improvement efforts (e.g., testing and code reviews). Defect prediction models are typically built using classification or regression analysis on product and/or process metrics collected at a single point in time (e.g., a release date). However, current defect prediction models only predict if a defect will occur, but not when, which makes the prioritization of software quality improvements efforts difficult. To address this problem, Koru et al. applied survival analysis techniques to a large number of software systems to study how size (i.e., lines of code) influences the probability that a source code module (e.g., class or file) will experience a defect at any given time. Given that 1) the work of Koru et al. has been instrumental to our understanding of the size-defect relationship, 2) the use of survival analysis in the context of defect modelling has not been well studied and 3) replication studies are an important component of balanced scholarly debate, we present a replication study of the work by Koru et al. In particular, we present the details necessary to use survival analysis in the context of defect modelling (such details were missing from the original paper by Koru et al.). We also explore how differences between the traditional domains of survival analysis (i.e., medicine and epidemiology) and defect modelling impact our understanding of the size-defect relationship. Practitioners and researchers considering the use of survival analysis should be aware of the implications of our findings.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914599,no
549,Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling,"Mixed-criticality real-time scheduling has been developed to improve resource utilization while guaranteeing safe execution of critical applications. These studies use optimistic resource reservation for all the applications to improve utilization, but prioritize critical applications when the reservations become insufficient at runtime. Many of them however share an impractical assumption that all the critical applications will simultaneously demand additional resources. As a consequence, they under-utilize resources by penalizing all the low-criticality applications. In this paper we overcome this shortcoming using a novel mechanism that comprises a parameter to model the expected number of critical applications simultaneously demanding more resources, and an execution strategy based on the parameter to improve resource utilization. Since most mixed criticality systems in practice are component-based, we design our mechanism such that the component boundaries provide the isolation necessary to support the execution of low-criticality applications, and at the same time protect the critical ones. We also develop schedulability tests for the proposed mechanism under both a flat as well as a hierarchical scheduling framework. Finally, through simulations, we compare the performance of the proposed approach with existing studies in terms of schedulability and the capability to support low-criticality applications.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176022,no
550,Strategies for Prioritizing Test Cases Generated Through Model-Based Testing Approaches,"Software testing is expensive and time consuming,especially for complex software. In order to deal with the costof testing, researchers develop Model-Based Testing (MBT). InMBT, test cases are generated automatically and a drawback isa huge generated test suite. Our research aims at studying the Test Case Prioritization problem in MBT context. So far, we already evaluated the influence of the model structure and the characteristics of the test cases that fail. Results suggest that the former does not affect significantly the performance of techniques, however, the latter indeed represents a major impact. Therefore, a worthy information in this context might be an expert who knows the crucial parts of the software, thus we propose the first version of a prioritization technique that considers hints from the expert and the distance notion in order to prioritize test cases. Evaluation and tuning of the technique are ongoing, but preliminary evaluation reveals promising results.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203104,yes
551,Supporting Continuous Integration by Code-Churn Based Test Selection,"Continuous integration promises advantages in large-scale software development by enabling software development organizations to deliver new functions faster. However, implementing continuous integration in large software development organizations is challenging because of organizational, social and technical reasons. One of the technical challenges is the ability to rapidly prioritize the test cases which can be executed quickly and trigger the most failures as early as possible. In our research we propose and evaluate a method for selecting a suitable set of functional regression tests on system level. The method is based on analysis of correlations between test-case failures and source code changes and is evaluated by combining semi-structured interviews and workshops with practitioners at Ericsson and Axis Communications in Sweden. The results show that using measures of precision and recall, the test cases can be prioritized. The prioritization leads to finding an optimal test suite to execute before the integration.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167168,no
552,Test case analytics: Mining test case traces to improve risk-driven testing,"In risk-driven testing, test cases are generated and/or prioritized based on different risk measures. For example, the most basic risk measure would analyze the history of the software and assigns higher risk to the test cases that used to detect bugs in the past. However, in practice, a test case may not be exactly the same as a previously failed test, but quite similar. In this study, we define a new risk measure that assigns a risk factor to a test case, if it is similar to a failing test case from history. The similarity is defined based on the execution traces of the test cases, where we define each test case as a sequence of method calls. We have evaluated our new risk measure by comparing it to a traditional risk measure (where the risk measure would be increased only if the very same test case, not a similar one, failed in the past). The results of our study, in the context of test case prioritization, on two open source projects show that our new risk measure is by far more effective in identifying failing test cases compared to the traditional risk measure.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070482,yes
553,Test case prioritization for regression testing based on ant colony optimization,"Test case prioritization technique is an efficient method to improve regression testing activities. It orders a regression test suite to execute the test cases with higher priority earlier than those with lower priority, and the problem is how to optimize the test case ordering according to some criterion. In this paper, we have proposed an algorithm which prioritizes the test cases based on ant colony optimization (ACO), considering three factors: number of faults detected, execution time and fault severity, and these three factors are used in ant colony optimization algorithm to help to reveal more severe faults at earlier stage of the regression testing process. The effectiveness of the algorithm is demonstrated using the metric named APFD, and the results of experiment show the algorithm optimizes the test case orderings effectively.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339054,yes
554,Test case prioritization with textual comparison metrics,"Regression testing of a large test pool consistently needs a prioritization technique that caters requirements changes. Conventional prioritization techniques cover only the methods to find the ideal ordering of test cases neglecting requirement changes. In this paper, we propose string dissimilarity-based priority assignment to test cases through the combination of classical and non-classical textual comparison metrics and elaborate a prioritization algorithm considering requirement changes. The proposed technique is suitable to be used as a preliminary testing when the information of the entire program is not in possession. We performed evaluation on random permutations and three textual comparison metrics and concluded the findings of the experiment.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475187,yes
555,Test case prioritization: An approach based on modified ant colony optimization (m-ACO),"Intense and widespread usage of software in every field of life has attracted the researchers to focus their attention on developing the methods to improve the efficiency of software testing; which is the most crucial and cost intensive phase of software development. Software testing aims to uncover the potential faults in Application Under Test by running the test cases on software code. Software code keeps on changing as the uncovered faults during testing are fixed by the developers. Regression testing is concerned with verifying the modified software code to ensure that changes in software code does not induce any undesired effect on rest of the code. Test Case Prioritization is a regression testing technique which re-schedule the execution sequence of test cases to improve the fault detection rate and enhance the performance of regression test suite. This paper focuses on proposing a novel method ""m-ACO"" for test case prioritization and the performance evaluation of the proposed method using Average Percentage of faults Detected.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375627,yes
556,Test case selection and prioritization using cuckoos search algorithm,"Regression Testing is an inevitable and very costly activity that is implemented to ensure the validity of new version of software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization for proper selection and schedule of test cases in a specific sequence, fulfilling some chosen criteria. Cuckoo search (CS) algorithm is an optimization algorithm proposed by Yang and Deb [13]. It is inspired by the obligate brood parasitism of some cuckoo species by laying their eggs in the nests of other host birds. Cuckoo Search is very easy to implement as it depends on single parameter only unlike other optimization algorithms. In this paper a test case selection and prioritization algorithm has been proposed using Cuckoo Search. This algorithm selects and prioritizes the test cases based on the number of faults covered in minimum time. The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155012,yes
557,Test case selection for networked production systems,"This paper provides a discussion on the coming technological changes in process automation of networked production systems, which will change the testing procedure. In the smart factory of the future there will be no possibility to reach a test coverage of 100%, assuming a flexible automation with continuous reconfiguration and dynamic changes during runtime. Consequently, large amounts of test cases and powerful algorithms for their prioritization are needed in order to certify the correct functionality of the production systems in the network. A concept is presented on how to analyze and prioritize the enormous amount of test cases resulting from the changes during runtime. The proposed approach for test case selection utilizes information of the product, the process and the status of the for the prioritization and selection.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301604,no
558,Test cases prioritization for software regression testing using analytic hierarchy process,"Test cases are considered an important asset in the software testing process since they are used to detect defects in the software. In order to produce quality software covering all of the requirements, the test case designer requires much time and effort in designing test cases to cover all requirements and conditions according to the test case structure. This research proposes a method for storing and retrieving of test cases affected by software requirements changes, as well as ranking the retrieved test cases using the AHP method to improve the quality of the ranking. There are to assist system testers in identifying test cases for complete regression testing. An example application of the proposed method will also be presented.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219790,yes
559,Testing analytics on software variability,"Software testing is a tool-driven process. However, there are many situations in which different hardware/software components are tightly integrated. Thus system integration testing has to be manually executed to evaluate the system's compliance with its specified requirements and performance. There could be many combinations of changes as different versions of hardware and software components could be upgraded and/or substituted. Occasionally, some software components could even be replaced by clones. The whole system after each component change demands to be re-tested to ensure proper system behavior. For better utilization of resources, there is a need to prioritize the past test cases to test the newly integrated systems. We propose a way to facilitate the use of historical testing records of the previous systems so that a testcase portfolio can be developed, which intends to maximize testing resources for the same integrated product family. As the proposed framework does not consider much of internal software complexity, the implementation costs are relatively low.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070483,no
560,The Effect of GoF Design Patterns on Stability: A Case Study,"Stability refers to a software system's resistance to the ƒ??ripple effectƒ?, i.e., propagation of changes. In this paper, we investigate the stability of classes that participate in instances/occurrences of GoF design patterns. We examine whether the stability of such classes is affected by (a) the pattern type, (b) the role that the class plays in the pattern, (c) the number of pattern occurrences in which the class participates, and (d) the application domain. To this end, we conducted a case study on about 65.000 Java open-source classes, where we performed change impact analysis on classes that participate in zero, one (single pattern), or more than one (coupled) pattern occurrences. The results suggest that, the application of design patterns can provide the expected ƒ??shieldingƒ? of certain pattern-participating classes against changes, depending on their role in the pattern. Moreover, classes that participate in coupled pattern occurrences appear to be the least stable. The results can be used for assessing the benefits and liabilities of the use of patterns and for testing and refactoring prioritization, because less stable classes are expected to require more effort while testing, and urge for refactoring activities that would make them more resistant to change propagation.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7066925,no
561,The evaluation of the results of an eye tracking based usability tests of the so called Instructor's Portal framework (http://tanitlap.ektf.hu/csernaiz),"The research discussed in this paper can be positioned at the cross-section of Applied Computer Science, Didactics and Human-Computer Interaction. Accordingly, an Instructor's Portal (IPo) framework system was developed at the Department of Human Informatics of the Eszterh?­zy K?­roly College (EKC) in 2015. The aim of the framework system is to enable instructors working in higher education institutions to establish, customize, and update their own webpages independently without any help from informatics professionals. Said system not only fulfills a gap filling function in the higher education sphere, but performs complex tasks while serving a wide range of users. In order to establish a logically arranged content structure and user interface the respective development process observed several developmental principles, methods, and web-ergonomic rules. This paper introduces the results of usability tests and examinations pertaining to the system. The examination utilized, an eye-tracking hardware device and a mouse movement recording software along with a special software facilitating the evaluation and presentation of the respective results. This essay introduces and highlight how the respective apparatus complements the traditional human observation-based usability tests while identifying the cognitive skills to be ascertained with such devices, one of the main priorities of Cognitive Infocommunications (CogInfoCom).",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390637,no
562,Total coverage based regression test case prioritization using genetic algorithm,"Regression Testing is a test to ensure that a program that was changed is still working. Changes introduced to a software product often come with defects. Additional test cases are, this could reduce the main challenges of regression testing is test case prioritization. Time, effort and budget needed to retest the software. Former studies in test case prioritization confirm the benefits of prioritization techniques. Most prioritization techniques concern with choosing test cases based on their ability to cover more faults. Other techniques aim to maximize code coverage. Thus, the test cases selected should secure the total coverage to assure the adequacy of software testing. In this paper, we present an algorithm to prioritize test cases based on total coverage using a modified genetic algorithm. Its performance on the average percentage of condition covered and execution time are compared with five other approaches.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207103,yes
563,Towards a framework for automatic correction of anti-patterns,"One of the biggest concerns in software maintenance is design quality; poor design hinders software maintenance and evolution. One way to improve design quality is to detect and correct anti-patterns (i.e., poor solutions to design and implementation problems), for example through refactorings. There are several approaches to detect anti-patterns, that rely on metrics and structural properties. However, finding a specific solution to remove anti-patterns is a challenging task as candidate refactorings can be conflicting and their number very large, making it costly. Hence, development teams often have to prioritize the refactorings to be applied on a system. In addition to this, refactoring is risky, since non-experienced developers can change the behaviour of a system, without a comprehensive test suite. Therefore, there is a need for tools that can automatically remove anti-patterns. We will apply meta-heuristics to propose a technique for automated refactoring that improves design quality.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081891,no
564,Towards testing variability intensive systems using user reviews,"Variability intensive systems are software systems that describe a large set of diverse and different configurations that share some characteristics. This high number of configurations makes testing such systems an expensive and error-prone task. For example, in the Android ecosystem we can find up to 24 different valid configurations, thus, making it impossible to test an application on all of them. To alleviate this problem, previous research suggest the selection of a subset of test cases that maximize the changes of finding errors while maximizing the diversity of configurations. Concretely, the proposals focus on the prioritization and selection of tests, so only relevant configurations are tested according to some criterion. In this paper, we envision the use of user reports to prioritize and select meaningful tests. To do this, we explore the use of recommender systems as a possible improvement to the selection of test cases in intensive variability systems.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333410,yes
565,Tracking down high coverage configuration using clustering and fault detection,"Mostly all software systems are highly configured, it has many benefits but there is a difficulty of software testing because there will be unique errors could be hidden in any of the configurations and undergoing testing for each of the configurations will lead to expensive testing and it is also impractical. The dependable systems will have some mechanism for fault tolerance in software testing. If the rate of the fault detection is calculated then the coverage of the configuration can be easily generated. First load the application for which it is going to be tested by using our test case prioritization approach and loading the dataset for the test case for the given application. After this process, need to assign the individual ids for all the test cases in the test case dataset. Also it is able to add the test cases in the dynamic nature. Then to compute the test case prioritization, first built the dependency structure for the test cases. Through the approach get the height and weight matrix for the test cases after this computation the test cases. The cosine similarity values between the test cases. In the similarity values it will show how it is highly related with the other test cases. Thus the clustering approach is introduced for grouping the test cases. These test cases are analyzed for measuring their relevancy and relationship between the test cases using their constrains and the clustering of the test cases is done for the better result in the rate of fault detect. With the Average percentage fault detection the graph is drawn and it shown the high coverage configurations.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453839,no
566,UPMOA: An improved search algorithm to support user-preference multi-objective optimization,"Multi-objective search algorithms (e.g., non-dominated sorting genetic algorithm II (NSGA-II)) have been applied extensively to solve various multi-objective optimization problems in software engineering such as problems in testing. However, existing multi-objective algorithms usually treat all the objectives with equivalent priorities and do not provide a mechanism to reflect various user preferences when guiding search. The need to have such a mechanism was observed in one of our industrial projects on applying search algorithms for test optimization of a product line of Videoconferencing Systems (VCSs) called Saturn, where user preferences must be incorporated into optimization objectives, based on domain knowledge of test engineers for VCS testing. To address this, we propose an extension to the most commonly-used multi-objective search algorithm NSGA-II, which has shown promising results with user preferences. We name the extension as User-Preference Multi-Objective Optimization Algorithm (UPMOA), which includes a user preference indicator p and is based on existing weight assignment strategies. We empirically evaluated UPMOA with two industrial problems focusing on optimizing the test execution system for Saturn in Cisco. To assess the performance and scalability of UPMOA, inspired by the two industrial problems, in total we created 64000 artificial problems with 128 different sets of user preferences. The evaluation includes two aspects: 1) Three weight assignment strategies together with UPMOA were empirically evaluated to identify a best weight assignment strategy for p. Results show that the Uniformly Distributed Weights (UDW) strategy can assist UPMOA in achieving the best performance; 2) UPMOA was compared with three representative multi-objective search algorithms (including NSGA-II) and results show that UPMOA significantly outperformed the others and has the ability to solve problems with a wide range of complexity.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381833,no
567,Using Artificial Bee Colony for Code Coverage Based Test Suite Prioritization,"The goal of test suite prioritization is maximizing fault detection and code coverage rate. Several nature inspired optimization algorithms such as Swarm Intelligence (SI) have been studied for the optimization of such problems. The studies revealed the benefits of Artificial Bee Colony (ABC) over other algorithms. ABC and its variations were implemented in software testing areas, test suite prioritization in particular. However, most SI based approaches focus on fault detection ability which is difficult to predict. In this paper, the standard ABC algorithm is used to prioritize test suites based on code coverage. The results reveal that ABC shows promising results and, hence, is a great candidate for prioritizing test suites. It also suggests that a modification to the standard ABC algorithm or combination of ABC and another SI algorithm should yield an even better result.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371038,yes
568,Using Defect Taxonomies for Testing Requirements,"Systematic defect management based on bug-tracking systems such as Bugzilla is well established and has been successfully used in many software organizations. Defect management weights the failures observed during test execution according to their severity and forms the basis for effective defect taxonomies. In practice, most defect taxonomies are used only for the a posteriori allocation of testing resources to prioritize failures for debugging. Thus, these taxonomies' full potential to control and improve all the steps of testing has remained unexploited. This is especially the case for testing a system's user requirements. System-level defect taxonomies can improve the design of requirements-based tests, the tracing of defects to requirements, the quality assessment of requirements, and the control of the relevant defect management. So, we developed requirements-based testing with defect taxonomies (RTDT). This approach is aligned with the standard test process and uses defect taxonomies to support all phases of testing requirements. To illustrate this approach and its benefits, we use an example project (which we call Project A) from a public health insurance institution.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799150,no
569,Using Fuzzy Logic and Symbolic Execution to Prioritize UML-RT Test Cases,"The relative ease of test case generation associated with model-based testing can lead to an increased number of test cases being identified for any given system; this is problematic as it is becoming near impossible to run (or even generate) all of the possible tests in available time frames. Test case prioritization is a method of ranking the tests in order of importance, or priority based on criteria specific to a domain or implementation, and selecting some subset of tests to generate and run. Some approaches require the generation of all tests, and simply prioritize the ones to be run, however we propose an approach that would prevent unnecessary generation of tests through the use of symbolic execution trees to determine which tests provide the most benefit to coverage of execution. Our approach makes use of fuzzy logic, specifically fuzzy control systems, to prioritize test cases generated from these execution; the prioritization is based on natural language rules about testing priority. Within this paper we present our motivation, some background research, our methodology and implementation, results, and conclusions.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102610,yes
570,Using Partition Information to Prioritize Test Cases for Fault Localization,"Fault Localization Prioritization (FLP) aims at reordering existing test cases so that the location of detected faulty components can be identified earlier, using certain fault localization techniques. Although some researchers have proposed adaptive prioritization strategies with white-box code coverage information, such information may not always be available. In this paper, we address the FLP problem using black-box information derived from partitioning the input domain. Based on the well-known technique of Spectra-Based Fault Localization (SBFL), three test case prioritization strategies are designed following some basic SBFL heuristics. The implementation of these proposed strategies relies only on the partition information, and does not require any test case execution history. Experiments show that our strategies, when compared with pure random selection, result in a faster localization of faulty statements, reducing the number of test case executions required. Here, we analyze the characteristics and merits of the three proposed strategies.",2015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273609,yes
571,A hybrid approach for test case prioritization and optimization using meta-heuristics techniques,"Software testing is a very crucial and important phase for (SDLC) software development life cycle. Software is being tested on its effectiveness for generating good quality software. Regression testing is done by considering the constraints of resources and in this phase optimization of test suite is very important and crucial. This paper mainly aims to make use of hybrid approach of meta-heuristics, It comprises of two algorithms first is genetic algorithm and second is particle swarm optimization. In addition to algorithm the comparison of proposed algorithm hybrid GA_PSO with other optimization algorithms are been done. To validate the research Average Percentage Fault Detection (APFD) metric is used for comparison and fitness evaluation of the proposed algorithm.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975319,yes
572,A hybrid approach for test case prioritization and selection,"Software testing consists in the dynamic verification of the behavior of a program on a set of test cases. When a program is modified, it must be tested to verify if the changes did not imply undesirable effects on its functionality. The rerunning of all test cases can be impossible, due to cost, time and resource constraints. So, it is required the creation of a test cases subset before the test execution. This is a hard problem and the use of standard Software Engineering techniques could not be suitable. This work presents an approach for test case prioritization and selection, based in relevant inputs obtained from a software development environment. The approach uses Software Quality Function Deployment (SQFD) to deploy the features relevance among the system components, Mamdani fuzzy inference systems to infer the criticality of each class and Ant Colony Optimization to select test cases. An evaluation of the approach is presented, using data from simulations with different number of tests.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744363,yes
573,A Multi-Objective Technique to Prioritize Test Cases,"While performing regression testing, an appropriate choice for test case ordering allows the tester to early discover faults in source code. To this end, test case prioritization techniques can be used. Several existing test case prioritization techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test case prioritization technique that determines the ordering of test cases that maximize the number of discovered faults that are both technical and business critical. In other words, our new technique aims at both early discovering faults and reducing the execution cost of test cases. To this end, we automatically recover links among software artifacts (i.e., requirements specifications, test cases, and source code) and apply a metric-based approach to automatically identify critical and fault-prone portions of software artifacts, thus becoming able to give them more importance during test case prioritization. We experimentally evaluated our technique on 21 Java applications. The obtained results support our hypotheses on efficiency and effectiveness of our new technique and on the use of automatic artifacts analysis and weighting in test case prioritization.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362042,yes
574,A novel approach for selecting an effective regression testing technique,"All software systems need modifications with time, these modifications involve different types or amounts of code modifications in different versions. To validate these modifications many regression testing sessions are needed. But researchers do not have a single regression testing technique that can be used on every version. The objective of this scrutiny is to evolve a methodology that attempts to determine the re-testing technique that would be effective for every re-testing period accounting testing domain and conditions. This methodology is based on Revised Analytical Hierarchy Process (Revised AHP). There are numerous regression testing techniques. But this investigation is limited to test case prioritization techniques only. The result showed that prioritization techniques selected by proposed technique are more efficacious than those used by the forgoing techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724438,yes
575,A random and coverage-based approach for fault localization prioritization,"Fault Localization Prioritization (FLP) aims to order the execution sequence of test cases so that faulty statements in a faulty program can be localized faster. FLP is an important part of the automation of testing and fault localization in software engineering. The key issue is to identify which test cases can provide most useful information to help locate the faulty statement. Assuming the well-known technique of Spectra-Based Fault Localization (SBFL) is applied, this paper evaluates the quality of a test case based on the characteristics of its statement coverage information. We propose the COverage-based Random (COR) approach to address the FLP problem. Two statement coverage characteristics, the diversity characteristic and the failure-like characteristic, are analyzed and identified as having significant impacts on the effectiveness of fault localization. When using the COR approach, each test case is examined and the degree of each characteristic is measured, with test cases showing high degrees of the characteristics being assigned higher priority for execution. Because of the power of random strategies to improve the robustness of the approach, some random factors in the selection of test cases are included. Empirical studies show that, compared with existing approaches, the COR approach results in a faster localization of faulty statements, reducing the number of necessary test case executions.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7531562,yes
576,A study of critical success factors on software quality assurance of cloud networking devices,"In recent years, many more enterprises and organizations are embracing cloud computing for its commercial benefits and competitive advantages. However, these companies have prioritized hardware specifications when evaluating cloud networking devices, often neglecting the importance of software quality as a result. Most cloud networking devices currently offered on the market include embedded software systems, which mean that software and hardware compatibility can be one of the most essential characteristics to look for. In order to investigate the critical success factors (CSFs) for the software quality assurance (QA) of cloud networking devices, this study employed a Plan-Do-Check-Act (PDCA) research framework. Important Software QA factors from relevant IEEE standards and hardware factors related to software quality were then employed in the 4 facets of the PDCA process to conduct a 2-stage analysis. For the first stage of this study, 5 experts were interviewed and asked to complete a survey form and select CSFs that were then used to generate a questionnaire for the second stage where a total of 15 experts and QA engineers were invited to fill out questionnaires. Completed questionnaires were then subject to an analytic hierarchy process (AHP) to calculate the weight and priority orders for each of the CSFs. The overall results of Stage 2 indicate that Cloud devices security testing, Security of cloud networking devices, and Resources and their allocation are the top three orders of all CSFs when experts take a real concern in software QA of cloud networking devices.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811054,no
577,A tool for constrained pairwise test case generation using statistical user profile based prioritization,"Pairwise testing is a wildly used approach in order to reduce the size of test suite and take steps to combinatorial testing problems because of an extensively large number of possible combinations between input parameters and values. In some cases, there will be invalid combinations between input parameters and values if constraints have not been handled. In this paper, we present a pairwise test generation tool called CPTG, a tool to generate test cases for pairwise testing by applying user profile for guiding and prioritizing in order to select optimal input parameters and values which do not depend on individual tester skills and also providing constraint handling solution between input parameters and values. We performed experiments and comparison with other tools. The experimental results of our tool demonstrated that our tool becomes particularly valuable in guiding testing with a maximized reliability by testing the most frequently used of the system and can generate comparable results of the size of the test case set.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748881,no
578,ACO based embedded system testing using UML Activity Diagram,"This paper proposed a model-based technique for test scenario generation using Activity Diagram (AD). We transform an AD specification into an intermediate graph called Activity Interaction Graph (AIG) using the proposed parser. After that, we apply combination of BFS and DFS algorithms for generating test scenarios. Then, we apply an algorithm called ACOToTSP (Ant Colony Optimization for Test Scenarios Prioritization) algorithm on the generated test scenarios with respect to some decision and concurrent criteria, for prioritizing the test scenarios. This approach generates test scenarios according to forks, Joins, and merge point's strength in the activity diagram.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847997,no
579,An Adaptive Sequence Approach for OOS Test Case Prioritization,"Test case prioritization (TCP) attempts to improve fault detection effectiveness by scheduling important test cases earlier, where important is determined by some criteria and strategy. Adaptive random sequences (ARSs) may be applied to improve the effectiveness of TCP in black-box testing. In this paper, to improve the effectiveness of TCP for object-oriented software, we present an ARS approach based on clustering techniques. In the proposed approach, test cases are clustered according to the number of objects and methods, using two clustering algorithms - K-means and K-medoids. Our proposed sampling strategy can construct ARSs within the clustering framework, constructing two ARS sequences based on the two clustering algorithms, which results in generated test cases with different execution sequences. We also report on experimental studies to verify the proposed approach, with the results showing that our approach can enhance the probability of earlier fault detection, and deliver higher effectiveness than random prioritization.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789402,yes
580,An exploration of various quality of service mechanisms in an OpenFlow and software defined networking environment,"Technology prevalent in networks today logically detaches the control from the data plane yet physically deploys them on the same device on all network devices. This has negatively resulted in expensive management of disparate and closed network devices and the inability to implement coherent, synchronized, and wide-ranged network policies. Software Defined Networking (SDN) solves these issues by providing an open and virtually centralized network system through cost-effective and gradual network introduction that minimizes investment in infrastructure. The OpenFlow protocol defines communications from the control plane where the logical center of the network, the controller, resides to the packet-routing devices in the data plane and vice-versa. The Mininet SDN emulator mimics real-world SDN network environments for rapid system testing and proof-of-concept experimentation. Quality Of Service (QoS) represents a ripe and proven technology that allows for the enforcement of priorities and assured performance levels on network links and objects. This study examined the application of QoS-based methods to an SDN OpenFlow environment. QoS parameters were utilized to implement rudimentary Class-Based Queuing (CBQ) scheduling algorithms on a custom OpenFlow controller within a Mininet-emulated SDN network topology. Two CBQ algorithms were used in the experiments contrasted by the classes on which they were based. One algorithm is Basic CBQ which modeled scheduling based on the ƒ??Streamingƒ?, ƒ??Burstyƒ?, and ƒ??Catch-Allƒ? traffic types. The other algorithm, Source CBQ, was based on predefined Source-IP address groupings of client hosts. Each Class Profile was broken down in terms of QoS enforcement points at the Leaf Switches and at the Core Switch. Tests were conducted on the topology with these CBQ-based Class Profiles to gather performance data and observe any issues encountered. Experiment results showed that CBQs at the Leaves yielded better overall average bandwidth than CBQs at the Core with Basic CBQ at the Leaves at 2% higher than Basic CBQ at the Core and Source CBQ at the Leaves at 30% higher than Source CBQ at the Core. This indicated that QoS implemented at the Client Leaf Switches rather than at the Core Switch yielded better performance as they were not prone to the same bottlenecks as the Core Switch. Additionally, experiment results seemingly showed that Basic CBQ, whose classes are based on traffic types, yielded optimum performance at less variability as it managed bandwidth better by segmenting traffic and limiting errors.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811055,no
581,Application of Mahalanobis-Taguchi Method and 0-1 Programming Method to Cost-Effective Regression Testing,"To enhance the cost effectiveness of regression testing, this paper proposes a method for prioritizing test cases. In general, a test case can be evaluated from various different points of view, therefore whether it is worth it to re-run should be discussed using multi criteria. This paper shows that the Mahalanobis-Taguchi (MT) method is a useful way to successfully integrate different evaluations of a test case. Moreover, this paper proposes to use the 0-1 programming method together with the MT method in order to take into account not only the priority of a test case but also its cost to run. The empirical study with 300 test cases for an industrial software system shows that the combination of the MT method and the 0-1 programming method is more cost-effective than other conventional methods.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592803,no
582,Applying Assemble Clustering Algorithm and Fault Prediction to Test Case Prioritization,"Cluster application is proposed as an efficient approach to improve test case prioritization, Test case in a same cluster are considered to have similar behaviors. In the process of cluster test case, the selection of test case feature and the clusters number have great influence on the clustering results. but to date almost clustering algorithm to improve test case prioritization are selected random clusters number and clustering result are based on one or a few of the code features, the paper propose a new prioritization techniques that not only consider the best clusters number but also produce the best clustering result based on test case multidimensional feature. After clustering, considering the inter-cluster prioritization and intra-cluster prioritization,in order to improve the effectiveness of our approach, the fault prediction value of code corresponding to the test case is used as one of a prioritization weight. Finally,we implemented an empirical studies using an industrial software to illustrate the effectiveness of the test case prioritization techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780203,yes
583,Automated comparison of X-ray images for cargo scanning,"Customs administrations are responsible for the enforcement of fiscal integrity and security of movements of goods across land and sea borders. In order to verify whether the transported goods match the transport declaration, X-ray imaging of containers is used at many customs site worldwide. The main objective of the research and development project ƒ??Automated Comparison of X-ray Images for Cargo Scanning (ACXIS)ƒ?, which is funded by the European 7<sup>th</sup>Framework Program, is to improve the efficiency and effectiveness of the inspection procedures of cargo at customs using X-ray technology. The current inspection procedures are reviewed to identify risks, catalogue illegal cargo, and prioritize detection scenarios. Based on these results, we propose an integrated solution that provides automation, information exchange between customs administrations, and computer-based training modules for customs officers. Automated target recognition (ATR) functions analyze the X-ray image after a scan is made to detect certain types of goods such as cigarettes, weapons and drugs in the freight or container. Other helpful information can also be provided, such as the load homogeneity, total or partial weight, or the number of similar items. The ATR functions are provided as an option to the user. The X-ray image is transformed into a manufacturer-independent format through geometrical and spectral corrections and stored into a database along with the user feedback and other related data. This information can be exchanged with similar systems at other sites, thus facilitating information exchange between customs administrations. The database is seeded with over 30'000 examples of legitimate and illegal goods. These examples are used by the ATR functions through machine learning techniques, which are further strengthened by the information exchange. In order to improve X-ray image interpretation competency of human operators (customs officers), a computer-based training software is developed that simulates these new inspection procedures. A study is carried out to validate the effectiveness and efficiency of the computer-based training as well as the implemented procedures. Officers from the Dutch and Swiss Customs administrations partake in the study, covering both land and sea borders.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815714,no
584,Automated testcase generation for software quality assurance,"The overall venture of the software engineering is to guarantee the delivery of high quality software to the client. To certify high quality software, it is required to test software. Testing is a decisive constituent of software engineering. In software testing there are number of underlying issues like effective generation of test cases, prioritisation of test cases which need to be tackled. This automated test case generation mainly depends on these four aspects: test strategy, test case generation, test execution and test evaluation. Test strategy is a collection of events that determines the testing approach to be followed by the testing team. The test case generation refers to the generation of testcases based on the certain application. The test execution briefs about the execution of those tests then comparing the expected result with actual result. The test evaluation investigates the test cases and helps us to generate test report and software quality assurance report automatically. The intention of producing this tool is to generate test cases automatically and to decrease the cost of testing in addition to accumulate the time of deriving test cases physically. Hence this system helps to improve overall quality of the software.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727003,no
585,Automatic Reproducible Crash Detection,"Crash reproduction, which spends much time of developers in reading and understanding source code, is a crucial yet time-consuming task in program debugging. To reduce the time and resource cost, automatic techniques of test generation have been proposed. These techniques aim to automatically generate test cases to reproduce the scenario of a crashed project. Unfortunately, due to the lack of a detailed comprehension of the source code, a generated test case may fail in reproducing an expected crash. In this paper, we propose an automatic approach to reproducible bug detection. This approach predicts whether a crash is difficult to reproduce or not via training a classifier based on historical reproducible crash data. If a crash is difficult to reproduce, it is better to assign the crash to a developer, instead of using an automatic technique of test generation. Our work can help to prioritize crashes and to save the cost of developers. Preliminary experiments show that our approach effectively detects reproducible crashes via evaluating 45 crashes.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780193,no
586,Automatically Learning Semantic Features for Defect Prediction,"Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886912,no
587,Automation Architecture for Bayesian Network Based Test Case Prioritization and Execution,"An automation architecture for Bayesian Network based test case prioritization is designed for software written in Java programming language following the approach proposed by Mirarab and Tahvildari [2]. The architecture is implemented as an integration of a series of tools and called Bayesian Network based test case prioritization and execution platform. The platform is triggered by a change in the source code, then it collects necessary information to be supplied to Bayesian Network and uses Bayesian Network evaluation results to run high priority unit tests.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552177,yes
588,AVISAR - a three tier architectural framework for the testing of Object Oriented Programs,"In this research paper, we have proposed a three tier conceptual framework AVISAR for the testing of Object Oriented Programs. We have devised the proposed framework into three levels: Requirements Modeling, Test Case Generation and Effort Estimation. At the first level, we have used the Extend relationship of the Use Case to capture the events generated by the Classes during message passing between them through their objects. These events were captured by an Object Instantiator using Event Templates and further given to a Test case scenario generator, which generates the test cases. At the second level, the test cases will be generated based on the event templates that are used for capturing the events generated by the Extend relationship of the Use cases. Further, we have proposed a Genetic Algorithm (GA) for the effective test case prioritization to ensure maximum code coverage. At the third level, effort estimation, we have used the Cyclometric Complexity and Token count to perform the effort estimation for the Software under Test (SUT).",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918730,no
589,Comparing White-Box and Black-Box Test Prioritization,"Although white-box regression test prioritization has been well-studied, the more recently introduced black-box prioritization approaches have neither been compared against each other nor against more well-established white-box techniques. We present a comprehensive experimental comparison of several test prioritization techniques, including well-established white-box strategies and more recently introduced black-box approaches. We found that Combinatorial Interaction Testing and diversity-based techniques (Input Model Diversity and Input Test Set Diameter) perform best among the black-box approaches. Perhaps surprisingly, we found little difference between black-box and white-box performance (at most 4% fault detection rate difference). We also found the overlap between black- and white-box faults to be high: the first 10% of the prioritized test suites already agree on at least 60% of the faults found. These are positive findings for practicing regression testers who may not have source code available, thereby making white-box techniques inapplicable. We also found evidence that both black-box and white-box prioritization remain robust over multiple system releases.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886931,yes
590,Conc-iSE: Incremental symbolic execution of concurrent software,"Software updates often introduce new bugs to existing code bases. Prior regression testing tools focus mainly on test case selection and prioritization whereas symbolic execution tools only handle code changes in sequential software. In this paper, we propose the first incremental symbolic execution method for concurrent software to generate new tests by exploring only the executions affected by code changes between two program versions. Specifically, we develop an inter-thread and inter-procedural change-impact analysis to check if a statement is affected by the changes and then leverage the information to choose executions that need to be re-explored. We also check if execution summaries computed in the previous program can be used to avoid redundant explorations in the new program. We have implemented our method in an incremental symbolic execution tool called Conc-iSE and evaluated it on a large set of multithreaded C programs. Our experiments show that the new method can significantly reduce the overall symbolic execution time when compared with state-of-the-art symbolic execution tools such as KLEE.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582788,no
591,Concolic test generation for PLC programs using coverage metrics,"This paper presents a technique for fully automated generation of test cases for PLC programs adhering to the IEC 61131-3 standard. While previous methods strive for completeness and therefore struggle with the state explosion we pursue a symbolic execution based approach, dropping completeness but nevertheless achieving similar or even better results in practice. The core component is a symbolic execution engine which chooses the next state to execute, handles constraints emerging during the execution and derives respective test vectors leading to a state. To make for a high coverage of the generated tests, we adopt techniques from concolic testing, allow for use of heuristics to prioritise promising states but also merge states to alleviate the path explosion. We exploit peculiarities of PLC semantics to determine reasonable merge-points and unlike similar approaches even handle unreachable code. To examine the feasibility of our technique we evaluate it on function blocks used in industry.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497884,no
592,Customized Regression Testing Using Telemetry Usage Patterns,"Pervasive telemetry in modern applications is providing new possibilities in the application of regression testing techniques. Similar to how research in bioinformatics is leading to personalized medicine, tailored to individuals, usage telemetry in modern software allows for custom regression testing, tailored to the usage patterns of an installation. By customizing regression testing based on software usage, the effectiveness of regression testing techniques can be greatly improved, leading to reduced testing costs and enhanced detection of defects that are most important to that customer. In this research, we introduce the concept of fingerprinting software usage patterns through telemetry. We provide various algorithms tocompute fingerprints and conduct an empirical study that shows that fingerprints are effective in identifying distinct usage patterns. Further, we discuss how usage fingerprints can be used to improve regression test prioritization run time by over 30 percent compared to traditional prioritization techniques.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816511,no
593,Data flow based quality testing approach using ACO for component based software development,"Component-based Software Engineering (CBSE) has been centered around advancements identified with configuration and implementation of software components and frameworks assembled from software components. Quality Assurance (QA) for CBSE is a new subject in the software development research area. In this paper an enhanced data flow based QA model is presented for CBSE by employing the Ant Colony Optimization (ACO)algorithm to optimize the given code for automatic generation and prioritization of optimal path in decision to decision Control Flow Graph (CFG), which results an enhanced testing phase for QA model with reduced complexity. After that, proposed ACO based approach is also utilized for the generation of test data to satisfy the generated set of paths. This paper additionally exhibits the proposed approach applying it in a program module. Results show that a better testing is achieved by applying proposed ACO based scheme on component based software. Proposed approach ensures full software coverage with least redundancy.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813850,no
594,Database design for error searching system based on keyword priority,"Because there are too many types of errors occurred in a multi-software integrated platform, such as Integrated Decision Information System (IDIS). It is urgent to design an error searching system to solve all different problems. However, those errors belong to different stages like setup, configuration, and operation, or those errors may occurred in different services, applications, or IP ports, or may be happened in different system software, different version of software, and those errors are also can be classified into different types. The new requirement on the design of database has been announced, that it has to locate the error, find out the reason of this error, as well as the corresponding solution. Also, it requires to provide the location, phase, etc. of an error. For a multi-software integrated platform, this paper proposed a database design for error searching system based on keyword priority. This DB made the correspondence among error, the reason of the error, and corresponding solution, and put them to different categories in terms of their characteristics, such that it is easy to manage, search, and use. This method treats those characteristics as keywords with higher priority, which are correspondent with errors, reasons, and solutions, respectively. While the keywords extracted from errors, reasons, and solutions are treated as keywords with lower priority. Keywords with different priorities can all be used as index to search errors, reasons, and solutions, while keywords with higher priority can be used to filter the searching results finer, and make the searching results more accurate. On one hand, the database design method based on keyword priority has simplified the logical structure of the database. On the other hand, users do not only search the reason and solution of an error, but also can find out the accurate information of the error, such as in which stage, layer, software, or categories, etc. the error has happened. The data of the database is complete, which has been provided by 500 technicians who had found thousands of errors. Those errors haven been added to the DB after tests to make the DB more complete.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811011,no
595,Diversity-Aware Mutation Adequacy Criterion for Improving Fault Detection Capability,"Many existing testing techniques adopt diversity as an important criterion for the selection and prioritization of tests. However, mutation adequacy has been content with simply maximizing the number of mutants that have been killed. We propose a novel mutation adequacy criterion that considers the diversity in the relationship between tests and mutants, as well as whether mutants are killed. Intuitively, the proposed criterion is based on the notion that mutants can be distinguished by the sets of tests that kill them. A test suite is deemed adequate by our criterion if the test suite distinguishes all mutants in terms of their kill patterns. Our hypothesis is that, simply by using a stronger adequacy criterion, it is possible to improve fault detection capabilities of mutation-adequate test suites. The empirical evaluation selects tests for real world applications using the proposed mutation adequacy criterion to test our hypothesis. The results show that, for real world faults, test suites adequate to our criterion can increase the fault detection success rate by up to 76.8 percentage points compared to test suites adequate to the traditional criterion.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528954,yes
596,Do We Have a Chance to Fix Bugs When Refactoring Code Smells?,"Code smells are used to describe code structures that may cause detrimental effects on software and should be refactored. Previous studies show that some code smells have significant effect on faults. However, how to refactor code smells to reduce bugs still needs more concern. We investigate the possibility of prioritizing code smell refactoring with the help of fault prediction results. We also investigate the possibility of improving the performance of fault prediction by using code smell detection results. We use Cohen's Kappa statistic to report agreements between results of code smell detections and fault predictions. We use fault prediction result as an indicator to guide code smell refactoring. Our results show that refactoring Blob, Long Parameter List, and Refused Parent Be Request may have a good chance to detect and fix bugs, and some code smells are particularly useful for improving the recall of fault prediction.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780189,no
597,Dynamic Integration Test Selection Based on Test Case Dependencies,"Prioritization, selection and minimization of test cases are well-known problems in software testing. Test case prioritization deals with the problem of ordering an existing set of test cases, typically with respect to the estimated likelihood of detecting faults. Test case selection addresses the problem of selecting a subset of an existing set of test cases, typically by discarding test cases that do not add any value in improving the quality of the software under test. Most existing approaches for test case prioritization and selection suffer from one or several drawbacks. For example, they to a large extent utilize static analysis of code for that purpose, making them unfit for higher levels of testing such as integration testing. Moreover, they do not exploit the possibility of dynamically changing the prioritization or selection of test cases based on the execution results of prior test cases. Such dynamic analysis allows for discarding test cases that do not need to be executed and are thus redundant. This paper proposes a generic method for prioritization and selection of test cases in integration testing that addresses the above issues. We also present the results of an industrial case study where initial evidence suggests the potential usefulness of our approach in testing a safety-critical train control management subsystem.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528974,yes
598,Effect of Time Window on the Performance of Continuous Regression Testing,"Test prioritization is an effective technique used to reduce the amount of work required to support regression testing in continuous integration development. It aims at finding an optimal order of tests that can detect regressions faster, potentially increasing the frequency of software releases. Prioritization techniques based on test execution history use the results of preceding executions to determine an optimal order of regression tests in the succeeding test executions. In this paper, we investigate how can execution history be optimally used to increase the effectiveness of regression test prioritization. We analyze the effect of history time window on the fault detection effectiveness of prioritized regression tests. We report an experimental study using a data set from Cisco. The results suggest that varying the size of the window can considerably change the performance of regression testing. Our findings will potentially help developers and test teams in adjusting test prioritization techniques for achieving higher cost-effectiveness in continuous regression testing.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816510,no
599,Effectiveness of prioritization of test cases based on Faults,"Regression testing (RT) is an expensive activity. It is applied on a modified program to enhance confidence and reliability by ensuring that the changes are accurately true and have not affected the unmodified portions of the SUT. Due to limited resources, it is not practical to re-run each test cases (TC). To improve the regression testing's effectiveness, the TCs should be arranged according to some objective function or criteria. Test case prioritization (TCP) arranges TCs in an order for execution that enhances their effectiveness by satisfying some testing goals. The highest priority assigned to TCs must execute before the TCs with low priority by virtue of some performance goal. Numerous goals are possible to achieve of which one such goal is rate of fault detection (RFT) in which the faults are surfaced as quickly as possible within the testing process. In this paper, a novel technique is suggested to prioritize the TCs that increase its effectiveness in detecting faults. The effectiveness of the proposed method is compared and matched with other prioritization approaches with the help of Average Percentage of Fault Detection (APFD) metric from which charts have been prepared.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507977,yes
600,Empirical Evaluation of Cross-Release Effort-Aware Defect Prediction Models,"To prioritize quality assurance efforts, various fault prediction models have been proposed. However, the best performing fault prediction model is unknown due to three major drawbacks: (1) comparison of few fault prediction models considering small number of data sets, (2) use of evaluation measures that ignore testing efforts and (3) use of n-fold cross-validation instead of the more practical cross-release validation. To address these concerns, we conducted cross-release evaluation of 11 fault density prediction models using data sets collected from 2 releases of 25 open source software projects with an effort-aware performance measure known as Norm(P<sub>opt</sub>). Our result shows that, whilst M5 and K* had the best performances, they were greatly influenced by the percentage of faulty modules present and size of data set. Using Norm(P<sub>opt</sub>) produced an overall average performance of more than 50% across all the selected models clearly indicating the importance of considering testing efforts in building fault-prone prediction models.",2016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589801,no