ID,Document Title,Abstract,Year,PDF Link,code
151,Techniques for building excellent Operator Machine Interfaces (OMI),"Establishing a process to continually improve understanding of operator requirements -the why as well as the how-is key to developing an optimal Operator-Machine Interface (OMI) for a large program. The P-8A program is one example of a complex software development, with over 2 million lines of code and a unique and complex OMI. The OMI itself is used as a tool for achieving, and is designed in parallel with understanding end goals, operator decision making chains, and thought processes. The OMI can be designed to support and anticipate information needs, and alert operators to unusual occurrences. Operator actions and decision making techniques can be discovered by using expert operators and capturing their knowledge through the use of multiple tools to identify the problem set: storytelling, decision mapping, is/is not matrices, etc. When a solution set has been identified and agreed to by the customer, resources allocated, and schedule set, a standard process captures and prioritizes further changes, identifies their impact, and decides on implementation. Documents describing design and processes and a Design Description Document describing the current version of the OMI are made accessible to stakeholders at all times. ""What's important is not that we can conceive the idea, but that when we actually test it on people you discover it doesn't work... your intuition is wrong."" -Daniel M. Russell (IBM Almaden / Xerox PARC).",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4391947,no
152,Test Case Prioritization Based on Varying Testing Requirement Priorities and Test Case Costs,"Test case prioritization is an effective and practical technique in regression testing. It schedules test cases in order of precedence that increases their ability to meet some performance goals, such as code coverage, rate of fault detection. In previous work, the test case prioritization techniques and metrics usually assumed that testing requirement priorities and test case costs are uniform. In this paper, basing on varying testing requirement priorities and test case costs, we present a new, general test case prioritization technique and an associated metric. The case study illustrates that the rate of ""units-oftesting-requirement-priority-satisfied- per-unit-test-case-cost"" can be increased, and then the testing quality and customer satisfaction can be improved.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385476,yes
153,Test Case Prioritization for Black Box Testing,"Test case prioritization is an effective and practical technique that helps to increase the rate of regression fault detection when software evolves. Numerous techniques have been reported in the literature on prioritizing test cases for regression testing. However, existing prioritization techniques implicitly assume that source or binary code is available when regression testing is performed, and therefore cannot be implemented when there is no program source or binary code to be analyzed. In this paper, we presented a new technique for black box regression testing, and we performed an experiment to measure our technique. Our results show that the new technique is helpful to improve the effectiveness of fault detection when performing regression test in black box environment.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291039,yes
154,The Effect of Organization Process Focus and Organizational Learning on Project Performance: An Examination of Taiwan's Companies,"The impact of organizational learning on project performance has received a great deal of attention in recent years. Process focus is recognized as one of five factors which help to promote organizational learning through out the process. A theoretical model is derived based upon prior researches in literature to examine the effects of organizational learning and process focus on project performance. The structural equation modeling was adopted to test the proposed hypotheses, and Taiwanese corporate IS companies served as examples. The results revealed that organization process focus has a positive impact on organizational learning, which in turn has a positive influence on project performance. Both organization process focus and organizational learning play the influence on project performance. These findings should give valuable information for managers to revisit their priorities in terms of the relative efforts in organization process focus and organization learning.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349429,no
155,The Impact of Organizational Learning on Lack of Team's Expertise Risk in Information Systems Projects,"During the past decade, information systems investment has grown rapidly worldwide and information systems project development has become one of the most important targets in e-business. Yet, the failure of information systems projects is a common occurrence in many organizations around the world. A theoretical model is derived based upon organizational learning theory and prior research in order to examine the effects of organizational learning on lack of team's expertise risk. A survey method is applied to test the hypotheses proposed by the research model, and Taiwanese corporate companies serve as examples. After survey by questionnaire and analyze the data by structure equation modeling, the result reveals that organizational learning has significantly negative impacts on all of the lack of development expertise risk, lack of domain expertise risk, and lack of general expertise risk. These findings support information systems managers with valuable information to revisit their priorities in terms of the relative efforts in organization learning.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402174,no
156,The need for self-managed access nodes in grid environments.,"The Grid is constantly growing and it is being used by more and more applications. In this scenario the entry node is an important component in the whole architecture and will become a contention point. In this paper we will demonstrate that the use of a self-managed layer on the entry node of a grid is necessary. A self-managed system can allow more jobs to be accepted and finished correctly. Since it's not acceptable for a grid middleware layer to lose jobs, we would normally need to prioritize the finishing/acceptance of jobs over the response time or the throughput. A prototype of what could be considered an autonomous system, is presented and tested over an installation of Globus Toolkit (GT4) and shows that we can greatly improve the performance of the original middleware by a factor of 30%. In this paper GT is used as an example but it could be added to any grid middleware",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148878,no
157,The TeraPaths Testbed: Exploring End-to-End Network QoS,"The TeraPaths project at Brookhaven National Laboratory (BNL) investigates the combination of DiffServ-based LAN QoS with WAN MPLS tunnels in creating end-to-end (host-to-host) virtual paths with bandwidth guarantees. These virtual paths prioritize, protect, and throttle network flows in accordance with site agreements and user requests, and prevent the disruptive effects that conventional network flows can cause in one another. This paper focuses on the TeraPaths testbed, a collection of end-site subnets connected through high-performance WANs, serving the research and software development needs of the TeraPaths project. The testbed is rapidly evolving towards a multiple end-site infrastructure, dedicated to QoS networking research, and it offers unique opportunities for experimentation with minimal or no impact on regular, production networking operations.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4444698,no
158,Toward the Use of Automated Static Analysis Alerts for Early Identification of Vulnerability- and Attack-prone Components,"Extensive research has shown that software metrics can be used to identify fault- and failure-prone components. These metrics can also give early indications of overall software quality. We seek to parallel the identification and prediction of fault- and failure-prone components in the reliability context with vulnerability- and attack-prone components in the security context. Our research will correlate the quantity and severity of alerts generated by source code static analyzers to vulnerabilities discovered by manual analyses and testing. A strong correlation may indicate that automated static analyzers (ASA), a potentially early technique for vulnerability identification in the development phase, can identify high risk areas in the software system. Based on the alerts, we may be able to predict the presence of more complex and abstract vulnerabilities involved with the design and operation of the software system. An early knowledge of vulnerability can allow software engineers to make informed risk management decisions and prioritize redesign, inspection, and testing efforts. This paper presents our research objective and methodology.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271764,no
159,Towards an empirical method of efficiency testing of system parts: A methodological study,"Current usability evaluation methods are essentially holistic in nature. However, engineers that apply a component-based software engineering approach might also be interested in understanding the usability of individual parts of an interactive system. This paper examines the efficiency dimension of usability by describing a method, which engineers can use to test, empirically and objectively, the physical interaction effort to operate components in a single device. The method looks at low-level events, such as button clicks, and attributes the physical effort associated with these interaction events to individual components in the system. This forms the basis for engineers to prioritise their improvement effort. The paper discusses face validity, content validity, criterion validity, and construct validity of the method. The discussion is set within the context of four usability tests, in which 40 users participated to evaluate the efficiency of four different versions of a mobile phone. The results of the study show that the method can provide a valid estimation of the physical interaction event effort users made when interacting with a specific part of a device.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8147077,no
160,Value-Oriented Requirements Prioritization in a Small Development Organization,"Requirements engineering, especially requirements prioritization and selection, plays a critical role in overall project development. In small companies, this often difficult process can affect not only project success but also overall company survivability. A value-oriented prioritization (VOP) framework can help this process by clarifying and quantifying the selection and prioritization issues. A case study of a small development company shows a successful VOP deployment that improved communications and saved time by focusing requirements decisions for new product releases on core company values",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052549,no
161,Verification and Validation of (Real Time) COTS Products using Fault Injection Techniques,"With the goal of reducing time to market and project costs, the current trend of real time business and mission critical systems is evolving from the development of custom made applications to the use of commercial off the shelf (COTS) products. Obviously, the same confidence and quality of the custom made software components is expected from the commercial applications. In most cases, such products (COTS) are not designed with stringent timing and/or safety requirements as priorities. Thus, to decrease the gap between the use of custom made components and COTS components, this paper presents a methodology for evaluating COTS products in the scope of dependable, real time systems, through the application of fault injection techniques at key points of the software engineering process. By combining the use of robustness testing (fault injection at interface level) with software fault injection (using educated fault injection operators), a COTS component can be assessed in the context of the system it will belong to, with special emphasis given to timing and safety constraints that are usually imposed by the target real time dependable environment. In the course of this work, three case studies have been performed to assess the methodology using realistic scenarios that used common COTS products. Results for one case study are presented",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127318,no
162,"""It's Not the Pants, it's the People in the Pants"" Learnings from the Gap Agile Transformation What Worked, How We Did it, and What Still Puzzles Us","After 7 years of traditional IT delivery, Gap Inc.Direct decided to adopt Agile. This experience report discusses three key factors that contributed to our successful (and ongoing) Agile transformation: 1. Ambitious Pilot Project, 2. Massive Investment in Continuous Integration, 3. Rethinking our Assets. The choices we made might seem risky and even counter-intuitive, but understanding them could help other organizations consider different points of view and priorities as they embark on the transition to Agile. Additionally, we will identify ongoing challenges and what is left in our transformation backlog.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599459,no
163,A dynamic scheduler for balancing HPC applications,"Load imbalance cause significant performance degradation in High Performance Computing applications. In our previous work we showed that load imbalance can be alleviated by modern MT processors that provide mechanisms for controlling the allocation of processors internal resources. In that work, we applied static, hand-tuned resource allocations to balance HPC applications, providing improvements for benchmarks and real applications. In this paper we propose a dynamic process scheduler for the Linux kernel that automatically and transparently balances HPC applications according to their behavior. We tested our new scheduler on an IBM POWER5 machine, which provides a software-controlled prioritization mechanism that allows us to bias the processor resource allocation. Our experiments show that the scheduler reduces the imbalance of HPC applications, achieving results similar to the ones obtained by hand-tuning the applications (up to 16%). Moreover, our solution reduces the application's execution time combining effect of load balance and high responsive scheduling.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5217785,no
164,Adaptive Test Question Selection for Web-Based Educational System,"In this paper we present a method proposed to select test questions adapting to individual needs of students in the context of Web-based educational system. It functions as a combination of three particular methods. First one is based on course structure and focuses on the selection of the most appropriate topic for learning, second uses the Item Response Theory to select k-best questions with adequate difficulty for particular learner and the last is based on usage history and prioritizes questions according to specific strategies, e.g. to filter out the questions that was recently asked. We describe how these methods evaluate user answers to gather information concerning their characteristics for more precise selection of further questions. We evaluated proposed method within our Web-based system called Flip on domain of functional programming.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724867,no
165,Addressing Low Base Rates in Intrusion Detection via Uncertainty-Bounding Multi-Step Analysis,"Existing approaches to characterizing intrusion detection systems focus on performance under test conditions. While it is well-understood that operational conditions may differ from test conditions, little attention has been paid to the question of assessing the effect on IDS results of parameter estimation errors resulting from these differences. In this paper we consider this question in the context of multi-step attacks. We derive simulated distributions of the posterior probability of exploit given the observation of a series of alerts and bounds on the posterior uncertainty given a particular distribution of the model parameters. Knowledge of such bounds introduces the novel prospect of a confidence versus agility tradeoff in IDS administration. Such a tradeoff could give administrators flexibility in IDS configuration, allowing them to choose detection confidence at the price of detection latency, according to organizational priorities.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721564,no
166,Agent multiplication: An economical large-scale testing environment for system management solutions,"System management solutions are designed to scale to thousands or more machines and networked devices. However, it is challenging to test and verify the proper operation and scalability of management software given the limited resources of a testing lab. We have developed a method called agent multiplication, in which one physical testing machine is used to represent hundreds of client machines. This provides the necessary client load to test the performance and scalability of the management software and server within limited resources. In addition, our approach guarantees that the test environment remains consistent between test runs, ensuring that test results can be meaningfully compared. We used agent multiplication to test and verify the operation of a server managing 4,000 systems. We exercised the server functions with only 8 test machines. Applying this test environment to an early version of a real enterprise system management solution we were able to uncover critical bugs, resolve race conditions, and examine and adjust thread prioritization levels for improved performance.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536552,no
167,An Empirical Study on Bayesian Network-based Approach for Test Case Prioritization,"A cost effective approach to regression testing is to prioritize test cases from a previous version of a software system for the current release. We have previously introduced a new approach for test case prioritization using Bayesian Networks (BN) which integrates different types of information to estimate the probability of each test case finding bugs. In this paper, we enhance our BN-based approach in two ways. First, we introduce a feedback mechanism and a new change information gathering strategy. Second, a comprehensive empirical study is performed to evaluate the performance of the approach and to identify the effects of using different parameters included in the technique. The study is performed on five open source Java objects. The obtained results show relative advantage of using feedback mechanism for some objects in terms of early fault detection. They also provide insight into costs and benefits of the various parameters used in the approach.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539555,yes
168,Application of system models in regression test suite prioritization,"During regression testing, a modified system needs to be retested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Most of the existing test prioritization methods are based on the code of the system, but model-based test prioritization has been recently proposed. System modeling is a widely used technique to model state-based systems. The existing model based test prioritization methods can only be used when models are modified during system maintenance. In this paper, we present model-based prioritization for a class of modifications for which models are not modified (only the source code is modified). After identification of elements of the model related to source-code modifications, information collected during execution of a model is used to prioritize tests for execution. In this paper, we discuss several model-based test prioritization heuristics. The major motivation to develop these heuristics was simplicity and effectiveness in early fault detection. We have conducted an experimental study in which we compared model-based test prioritization heuristics. The results of the study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658073,yes
169,Application-Level QoS: Improving Video Conferencing Quality through Sending the Best Packet Next,"In a traditional network stack, data from an application is transmitted in the order that it is received. An algorithm is proposed where information about the priority of packets and expiry times is used by the transport layer to reorder or discard packets at the time of transmission to optimise the use of available bandwidth. This can be used for video conferencing to prioritise important data. This scheme is implemented and compared to unmodified datagram congestion control protocol (DCCP). This algorithm is implemented as an interface to DCCP and tested using traffic modelled on video conferencing software. The results show improvement can be made to video conferencing during periods of congestion - substantially more audio packets arrive on time with the algorithm, which leads to higher quality video conferencing. In many cases video packet arrival rate also increases and adopting the algorithm gives improvements to video conferencing that are better than using unmodified queuing for DCCP. The algorithm proposed is implemented on the server only, so benefits can be obtained on the client without changes being required to the client.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756479,no
170,Applying Particle Swarm Optimization to Prioritizing Test Cases for Embedded Real Time Software Retesting,"In recent years, complex embedded systems are used in every device that is infiltrating our daily lives. Since most of the embedded systems are multi-tasking real time systems, the task interleaving issues, dead lines and other factors needs software units retesting to follow the subsequence changes. Regression testing is used for the software maintenance that revalidates the old functionality of the software unit. Testing is one of the most complex and time-consuming activities, in which running of all combination of test cases in test suite may require a large amount of efforts. Test case prioritization techniques can take advantage that orders test cases, which attempts to increase effectiveness in regression testing. This paper proposes to use particle swarm optimization (PSO) algorithm to prioritize the test cases automatically based on the modified software units. Regarding to the recent investigations, PSO is a multi-object optimization technique that can find out the best positions of the objects. The goal is to prioritize the test cases to the new best order, based on modified software components, so that test cases, which have new higher priority, can be selected in the regression testing process. The empirical results show that by using the PSO algorithm, the test cases can be prioritized in the test suites with their new best positions effectively and efficiently.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4568558,yes
171,Automated Generation and Assessment of Autonomous Systems Test Cases,"Verification and validation testing of autonomous spacecraft routinely culminates in the exploration of anomalous or faulted mission-like scenarios. Prioritizing which scenarios to develop usually comes down to focusing on the most vulnerable areas and ensuring the best return on investment of test time. Rules-of-thumb strategies often come into play, such as injecting applicable anomalies prior to, during, and after system state changes; or, creating cases that ensure good safety-net algorithm coverage. Although experience and judgment in test selection can lead to high levels of confidence about the majority of a system's autonomy, it's likely that important test cases are overlooked. One method to fill in potential test coverage gaps is to automatically generate and execute test cases using algorithms that ensure desirable properties about the coverage. For example, generate cases for all possible fault monitors, and across all state change boundaries. Of course, the scope of coverage is determined by the test environment capabilities, where a faster-than-real-time, high-fidelity, software-only simulation would allow the broadest coverage. Even real-time systems that can be replicated and run in parallel, and that have reliable set-up and operations features provide an excellent resource for automated testing. Making detailed predictions for the outcome of such tests can be difficult, and when algorithmic means are employed to produce hundreds or even thousands of cases, generating predicts individually is impractical, and generating predicts with tools requires executable models of the design and environment that themselves require a complete test program. Therefore, evaluating the results of large number of mission scenario tests poses special challenges. A good approach to address this problem is to automatically score the results based on a range of metrics. Although the specific means of scoring depends highly on the application, the use of formal scoring metrics has high value in identifying and prioritizing anomalies, and in presenting an overall picture of the state of the test program. In this paper we present a case study based on automatic generation and assessment of faulted test runs for the Dawn mission, and discuss its role in optimizing the allocation of resources for completing the test program.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526484,no
172,Change Priority Determination in IT Service Management Based on Risk Exposure,"In the Change Management process within IT Service Management, some activities need to evaluate the risk exposure associated with changes to be made to the infrastructure and services. The paper presents a method to evaluate risk exposure associated with a change. Further, we show how to use the risk exposure metric to automatically assign priorities to changes. The formal model developed for this purpose captures the business perspective by using financial metrics in the evaluation of risk. Thus the method is an example of Business-Driven IT Management. A case study, performed in conjunction with a large IT service provider, is reported and provides good results when compared to decisions made by human managers.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4805134,no
173,Choosing the Right Prioritisation Method,"There are many methods available for prioritising software requirements. Choosing the most suitable one can often be quite difficult. A number of factors need to be considered such as the project development methodology being used, the amount of time available, the amount of information known about requirements, the stage of the project and the degree of information about priority required. This paper examines the type of information available at different stages in a project and matches it to the properties of prioritisation methods. It then recommends the usage of specific prioritisation methods at certain stages of a project.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483241,no
174,Collaborative group membership and access control for JXTA,"This paper presents a proposal for group membership and access control services for JXTA, both based on the principle of self-organization and collaboration of peer group members. The need for collaboration strengthens the resistance against free riding and eases management of revocation data. The proposal prioritizes group autonomy and makes use of the concepts of web of trust and small world phenomenon in order to achieve its ends, distancing itself from approaches based on centralized PKI models or trusted third parties external to the group. It also offers an alternative to the basic group membership services distributed with the JXTA platform implementations.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554399,no
175,Creating Agile Streams for Business & Technical Value,"Have you ever played the role of business owner and found yourself between ""a rock and a hard place"" of organizational politics when prioritizing backlog features? The Agile Stream approach negates those politics by dedicating development teams to organizational units and allowing those teams to continue working, iteration after iteration, as long as they continue delivering business value.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599466,no
176,Effective RTL Method to Develop On-Line Self-Test Routine for the Processors Using the Wavelet Transform,"In this paper, we introduce a new efficient register transfer level (RTL) method to develop on-line self- test routines. We consider some prioritizations to select the components and instructions of the processor. In addition, we choose test patterns based on spectral RTL test pattern generation (TPG) strategy. For the purpose of spectral analysis, we use the wavelet transform. Also, we use a few extra instructions for the purpose of the signature monitoring to detect control flow errors. We demonstrate that the combination of these three strategies is effective for developing small test programs with high fault coverage in a small test development time. In this case, we only need the instruction set architecture (ISA) and RTL information. Our method not only provides a simple and fast algorithm for on-line self-test applications, also gains the advantages of utilizing lower memory and reducing the test generation time complexities in comparison with proposed methods so far. We focus on the application of this approach for Parwan processor. We develop a self-test routine using our proposed method for Parwan processor and demonstrate the effectiveness of our proposed methodology for on-line testing by presenting experimental results for Parwan processor.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529795,no
177,Evaluating ALPHAN: A Communication Protocol for Haptic Interaction,"In our previous work we introduced a novel application layer protocol, named ALPHAN, for haptic data communication. The protocol is characterized by three distinguished features: first, it is designed at the application layer to enable the application to define and control the networking parameters. Second, it is made highly customizable using XML-based descriptions. Finally, the protocol supports multi-buffering mechanisms to prioritize the communicated information. In this paper, we present a thorough evaluation of the protocol using a collaborative haptic game; the balance ball game that we developed is for this purpose. The performance metrics and the test bed of the protocol evaluation are also discussed. Finally, we comment on our findings and provide directions for prospective research.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479972,no
178,Historical Value-Based Approach for Cost-Cognizant Test Case Prioritization to Improve the Effectiveness of Regression Testing,"Regression testing has been used to support software testing activities and assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive because it requires many test case executions, and the number of test cases increases sharply as the software evolves. In this paper, we propose the Historical Value-Based Approach, which is based on the use of historical information, to estimate the current cost and fault severity for cost-cognizant test case prioritization. We also conducted a controlled experiment to validate the proposed approach, the results of which proved the proposed approachpsilas usefulness. As a result of the proposed approach, software testers who perform regression testing are able to prioritize their test cases so that their effectiveness can be improved in terms of average percentage of fault detected per cost.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579792,yes
179,Incorporating varying requirement priorities and costs in test case prioritization for new and regression testing,"Test case prioritization schedules the test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects the most severe faults at the earliest in its testing life cycle. Test case prioritization techniques have proved to be beneficial for improving regression testing activities. While code coverage based prioritization techniques are found to be taken by most scholars, test case prioritization based on requirements in a cost effective manner has not been taken for study so far. Hence, in this paper, we propose to put forth a model for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software that can also be cost effective and to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the six factors: customer priority, changes in requirement, implementation complexity, usability, application flow and fault impact. The proposed prioritization technique is experimented in three phases with student projects and two sets of industrial projects and the results show convincingly that the proposed prioritization technique improves the rate of severe fault detection.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787662,yes
180,mod kaPoW: Protecting the web with transparent proof-of-work,"Attacks from automated Web clients are a significant problem on the Internet. Web sites often employ Turing tests known as CAPTCHAs to combat automated agents. Unfortunately, such defenses require frequent human user input, are becoming less effective as computer vision techniques improve, and can be subverted by adversaries willing to hire humans to solve challenges. Several alternative defenses based upon cryptographic methods rather than human input have been proposed to achieve the same goals. Such ""proof-of-work"" techniques prioritize clients based on their willingness to solve computational challenges of client-specific difficulty set by the server. Unfortunately, few proof-of-work schemes have been deployed since they require wide-scale adoption of special client software to operate properly. To address these problems we present mocLkaPoW, a novel system that has the efficiency and human-transparency of proof-of-work schemes as well as the software backwards-compatibility of CAPTCHA schemes. The system leverages common Web technologies to deliver a challenge, solve it, and submit the client response, while providing accessibility for legacy clients. This paper describes and evaluates a prototype of this system.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544602,no
181,Model for optimizing software testing period using non homogenous poisson process based on cumulative test case prioritization,"Most of the software organizations has trouble when deciding the release dates their product. This difficulty is due to the fact that an under tested software could lead to many bugs propping up at the client side which would in turn lead to expensive bug-fixes and more importantly loss of customer goodwill. On the other hand, testing beyond certain time would lead to loss of revenue to the organization due to the dilution of the early bird advantage. The aim of our paper is optimizes the time and cost of entire software. In this paper we used non homogeneous Poisson process model based on cumulative priority. Our paper also tries to answer when to release any software.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766422,no
182,"Perspective on Embedded Systems: Challenges, Solutions and Research Priorities","This paper introduces THALES vision and research priorities for embedded systems and illustrates them through presentations of solutions and on-going research projects and initiatives. Thales effort related to mission-critical systems is focused on advanced high-performance embedded computing platforms, on middleware technologies, on software systems design and verification tools for safety and security and on the emergence of open standards in these domains. THALES is also actively contributing to the development of innovation eco-systems: the Joint Undertaking ARTEMIS in Europe; the Pole de Competitivite SYSTEM@TIC PARIS REGION in France.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484650,no
183,Prioritizing User-Session-Based Test Cases for Web Applications Testing,"Web applications have rapidly become a critical part of business for many organizations. However, increased usage of Web applications has not been reciprocated with corresponding increases in reliability. Unique characteristics, such as quick turnaround time, coupled with growing popularity motivate the need for efficient and effective Web application testing strategies. In this paper, we propose several new test suite prioritization strategies for Web applications and examine whether these strategies can improve the rate of fault detection for three Web applications and their preexisting test suites. We prioritize test suites by test lengths, frequency of appearance of request sequences, and systematic coverage of parameter-values and their interactions. Experimental results show that the proposed prioritization criteria often improve the rate of fault detection of the test suites when compared to random ordering of test cases. In general, the best prioritization metrics either (1) consider frequency of appearance of sequences of requests or (2) systematically cover combinations of parameter-values as early as possible.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539541,yes
184,Providing the Guideline of Determining Quality Checklists Priorities Based on Evaluation Records of Software Products,"COTS (commercial-off-the-shelf) software products are usually provided in a packaged style without the source code but with many ready-to-use functions. Generally, their vendors are reluctant to disclose the source code. Thus, the major way of quality evaluation and certification requires dynamic behavior testing, essentially black-box testing. Since observing every aspect of external software behavior is almost impossible, it is crucial to designate an adequate range for quality evaluation such as an adequate number of quality checklists or product quality metrics for external behavior testing. Hence, to establish rules of selecting quality evaluation criteria in systematic ways, there have been attempts to analyze and utilize the past records of software evaluation. In this paper, multiple characteristics of software are mapped as nodes to affect and determine the priority ranks of external software quality metrics on Bayesian belief network. The nodes are set to be under the influence of multiple inheritances so that every external characteristic of COTS software is considered thoroughly.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724545,no
185,Quota-constrained test-case prioritization for regression testing of service-centric systems,"Test-case prioritization is a typical scenario of regression testing, which plays an important role in software maintenance. With the popularity of Web services, integrating Web services to build service-centric systems (SCSs) has attracted attention of many researchers and practitioners. During regression testing, as SCSs may use up constituent Web servicespsila request quotas (e.g., the upper limit of the number of requests that a user can send to a Web service during a certain time range), the quota constraint may delay fault exposure and the subsequent debugging. In this paper, we investigate quota-constrained test-case prioritization for SCSs, and propose quota-constrained strategies to maximize testing requirement coverage. We divide the testing time into time slots, and iteratively select and prioritize test cases for each time slot using integer linear programming (ILP). We performed an experimental study on our strategies together with three other strategies, and the results show that with the constraint of request quotas, our strategies can schedule test cases for execution in an order with higher effectiveness in exposing faults and achieving total and additional branch coverage.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658074,yes
186,Ranking Attack-Prone Components with a Predictive Model,"Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. An early security risk analysis that ranks software components by probability of being attacked can provide an affordable means to prioritizing fortification efforts to the highest risk components. We created a predictive model using classification and regression trees and the following internal metrics: quantity of Klocwork static analysis warnings, file coupling, and quantity of changed and added lines of code. We validated the model against pre-release security testing failures on a large commercial telecommunications system. The model assigned a probability of attack to each file where upon ranking the probabilities in descending order we found that 72% of the attack-prone files are in the top 10% of the ranked files and 90% in the top 20% of the files.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700353,no
187,Scheduling Product Line Features for Effective Roadmapping,"Large industrial product lines may produce tens of thousands of variants each year. Each variant typically contains both reusable assets as well as product specific code created by different organizational units. To produce this vast number of variants the organizational resources must be used efficiently. For roadmapping this means an ability to schedule production of reusable assets so that all variants can be completed according to their requirements. When aiming for centralized variability management, roadmapping requires effective management of product line feature dependences and priorities. In this paper, we first introduce the problems haunting feature roadmapping in industrial product lines. Then we investigate how these problems can be solved using a novel approach for organizing product lines based on our practical experiences. Finally, we discuss our experiences and compare our approach with results by other researchers.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724548,no
188,Scheduling Timed Modules for Correct Resource Sharing,"Real-time embedded systems typically include concurrent tasks of different priorities with time-dependent operations accessing common resources. In this context, unsynchronized parallel executions may lead to hazard situations caused by e.g., race conditions. To be able to detect such faulty system behaviors before implementation, we introduce a unified model of resource constrained, scheduled real-time system descriptions, in Alur's and Henzinger's rigorous framework of timed reactive modules. We take a component-based design perspective and construct the realtime system model, by refinement, as a composition of realtime periodic preemptible tasks with encoded functionality, and a fixed-priority scheduler, all modeled as timed modules. For the model, we express the notions of race condition and redundant locking, formally, as invariance properties that can be verified by model-checking.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539537,no
189,Slack-based global multiprocessor scheduling of aperiodic tasks in parallel embedded real-time systems,"We provide a constant time schedulability test and priority assignment algorithm for an on-line multiprocessor server handling aperiodic tasks. Dhall's effect is avoided by dividing tasks in two priority classes based on their utilization: heavy and light. The improvement in this paper is due to assigning priority of light tasks based on slack - not on deadlines. We prove that if the load on the multiprocessor stays below (3 - radic5)/2 ap 38.197%, the server can accept an incoming aperiodic task and guarantee that the deadlines of all accepted tasks will be met. This is better than the current state-of- the-art algorithm where the priorities of light tasks are based on deadlines (the corresponding bound is in that case 35.425%).",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493574,no
190,Software development methods and usability: Perspectives from a survey in the software industry in Norway,"This paper investigates the relationship between software development methodologies and usability. The point of departure is the assumption that two important disciplines in software development, one of software development methods (SDMs) and one of usability work, are not integrated in industrial software projects.Building on previous research we investigate two questions; (1) Will software companies generally acknowledge the importance of usability, but not prioritise it in industrial projects? and (2) To what degree are software development methods and usability perceived by practitioners as being integrated? To this end a survey in the Norwegian IT industry was conducted. From a sample of 259 companies we received responses from 78 companies.In response to our first research question, our findings show that although there is a positive bias towards usability, the importance of usability testing is perceived to be much less than that of usability requirements. Given the strong time and cost pressures associated with the software industry, we believe that these results highlight that there is a gap between intention and reality. Regarding our second research question our survey revealed that companies perceive usability and software development methods to be integrated. This is in contrast to earlier research, which, somewhat pessimistically, has argued for the existence of two different cultures, one of software development and one of usability. The findings give hope for the future, in particular because the general use of system development methods are pragmatic and adaptable.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8149869,no
191,Software Quality model Analysis Program,"It is vital that data is obtained so that actions can be taken to improve the performance. Such improvement can be measured in terms of improved quality, increased customer satisfaction and decreased cost of quality. Different researchers have proposed software quality models to help measure the quality of software products. These models often include metrics for this purpose. Some of the classical and recent models are discussed and analyzed in this paper showing the points of strength and weakness of each model type. A new comprehensive model is proposed and analyzed. A complete solution is discussed through the paper to enable an effective and efficient use of the proposed model to help the development team in prioritizing the important metrics while developing the software products according to some inputs from the user and the objectives of the software being developed. The solution developed is called the quality model analysis program (QAP) and is a fuzzy system that weights the proposed model attributes according to certain rules. The solution enables software project managers to better utilize their resources and take specific actions to better improve the quality of the software produced.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4773015,no
192,Software Quality Requirements: How to Balance Competing Priorities,"The elicitation, analysis, and specification of quality requirements involve careful balancing of a broad spectrum of competing priorities. Developers must therefore focus on identifying qualities and designing solutions that optimize the product's value to its stakeholders.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4455627,no
193,Test Case Prioritization Based on Analysis of Program Structure,"Test case prioritization techniques have been empirically proved to be effective in improving the rate of fault detection in regression testing. However, most of previous techniques assume that all the faults have equal severity, which dose not meet the practice. In addition, because most of the existing techniques rely on the information gained from previous execution of test cases or source code changes, few of them can be directly applied to non-regression testing. In this paper, aiming to improve the rate of severe faults detection for both regression testing and non-regression testing, we propose a novel test case prioritization approach based on the analysis of program structure. The key idea of our approach is the evaluation of testing-importance for each module (e.g., method) covered by test cases. As a proof of concept, we implement $Apros$, a test case prioritization tool, and perform an empirical study on two real, non-trivial Java programs. The experimental result represents that our approach could be a promising solution to improve the rate of severe faults detection.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724580,yes
194,Test Case Prioritization for Multiple Processing Queues,"Test case prioritization is an effective technique that helps to increase the rate of fault detection or code coverage in regression testing. However, all existing methods can only prioritize test cases to a single queue. Once there are two or more machines that participate in testing, all exiting techniques are not applicable any more. To extend the prioritization methods to parallel scenario, this paper defines the prioritization problem in such scenario and applies the task scheduling method to prioritization algorithms to help partitioning a test suite into multiple prioritized subsets. Besides, this paper also discusses the limitation of previous metrics and proposes a new measure of effectiveness of prioritization methods in a parallel scenario. Finally, a case study is performed to illustrate the algorithms and metrics presented in this article.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732476,yes
195,"Testing Optimization for Mission-Critical, Complex, Distributed Systems","The goal of the research was to optimize the regression testing of the software application to address the identified problem of a missing, unclear or even contradictory requirement. The approach was mainly aimed at regression test prioritization and selection of regression test cases per test campaigns. A combination of subjective data based on expert knowledge and objective historical data were the inputs to the model where the output was determining the quality of test selection. Proposed model is aimed at finding newly introduced defects, and it could be extremely useful when the system is in the state of cleaning up or ordering requirements.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591679,yes
196,The use of explicit congestion notification to shape traffic of an intelligent satellite system,"Since the official standardization of explicit congestion notification (ECN) in 2001, the differentiated service (DS) bits 6 and 7 in packets are now classified for purposes of shaping and prioritization. These classifications are used to mark packet streams for controlling traffic flow in an intelligent satellite system (ISS). Using a forward link connection between a transmitting ground terminal to a geostationary satellite (GEO) acting as a relay, to a receiving hub back on the earth; a traffic shaping software which evaluates the ECN type or classification is used to control traffic flow and the results are observed.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463618,no
197,Two Case Studies of User Experience Design and Agile Development,"How can user experience design (UED) practices be leveraged in agile development to improve product usability? UED practices identify the needs and goals of the user through user research and testing. By incorporating UED in agile development, user research and testing can be utilized to prioritize features in the product backlog and to iteratively refine designs to achieve better usability. Furthermore, integrating UED and Agile processes can be accomplished with little or no impact on release schedules. The cases studies presented in this paper describe two examples of UED and agile integration at VeriSign.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599534,no
198,Using Statistical Models to Predict Software Regressions,"Incorrect changes made to the stable parts of a software system can cause failures - software regressions. Early detection of faulty code changes can be beneficial for the quality of a software system when these errors can be fixed before the system is released. In this paper, a statistical model for predicting software regressions is proposed. The model predicts risk of regression for a code change by using software metrics: type and size of the change, number of affected components, dependency metrics, developerpsilas experience and code metrics of the affected components. Prediction results could be used to prioritize testing of changes: the higher is the risk of regression for the change, the more thorough testing it should receive.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700331,no
199,A Hybrid Approach to Build Prioritized Pairwise Interaction Test Suites,"Traditional interaction testing aims to build test suites that cover all t-way interactions of inputs. But in many test scenarios, the entire test suites cannot be fully run due to the limited budget. Therefore it is necessary to take the importance of interactions into account and prioritize these tests of the test suite. In the paper, we use the hybrid approach to build prioritized pairwise interaction test suites (PITS). It adopts a one-test-at-a-time strategy to construct final test suites. But to generate a single test it firstly generates a candidate test and then applies a specific metaheuristic search strategy to enhance this test. Here we experiment four different metaheuristic search strategies. In the experiments, we compare our approach to weighted density algorithm (WDA). Meanwhile, we also analyze the effectiveness of four different search strategies and the effectiveness of the increasing iterations. Empirical results demonstrate the effectiveness of our proposed approach.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365886,yes
200,A new method of test data generation for branch coverage in software testing based on EPDG and Genetic Algorithm,"A new method called EPDG-GA which utilizes the edge partitions dominator graph (EPDG) and genetic algorithm (GA) for branch coverage testing is presented in this paper. First, a set of critical branches (CBs) are obtained by analyzing the EPDG of the tested program, while covering all the CBs implies covering all the branches of the control flow graph (CFG). Then, the fitness functions are instrumented in the right position by analyzing the pre-dominator tree (PreDT), and two metrics are developed to prioritize the CBs. Coverage-Table is established to record the CBs information and keeps track of whether a branch is executed or not. GA is used to generate test data to cover CBs so as to cover all the branches. The comparison results show that this approach is more efficient than random testing approach.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276897,no
201,A Survey of Coverage-Based Testing Tools,"Test coverage is sometimes used to measure how thoroughly software is tested and developers and vendors sometimes use it to indicate their confidence in the readiness of their software. This survey studies and compares 17 coverage-based testing tools primarily focusing on, but not restricted to, coverage measurement. We also survey features such as program prioritization for testing, assistance in debugging, automatic generation of test cases and customization of test reports. Such features make tools more useful and practical, especially for large-scale, commercial software applications. Our initial motivations were both to understand the available test coverage tools and to compare them to a tool that we have developed, called eXVantage (a tool suite that includes code coverage testing, debugging, performance profiling and reporting). Our study shows that each tool has some unique features tailored to its application domains. The readers may use this study to help pick the right coverage testing tools for their needs and environment. This paper is also valuable to those who are new to the practice and the art of software coverage testing, as well as those who want to understand the gap between industry and academia.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8130777,yes
202,Adaptive Random Test Case Prioritization,"Regression testing assures changed programs against unintended amendments. Rearranging the execution order of test cases is a key idea to improve their effectiveness. Paradoxically, many test case prioritization techniques resolve tie cases using the random selection approach, and yet random ordering of test cases has been considered as ineffective. Existing unit testing research unveils that adaptive random testing (ART) is a promising candidate that may replace random testing (RT). In this paper, we not only propose a new family of coverage-based ART techniques, but also show empirically that they are statistically superior to the RT-based technique in detecting faults. Furthermore, one of the ART prioritization techniques is consistently comparable to some of the best coverage-based prioritization techniques (namely, the ""additional"" techniques) and yet involves much less time cost.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431769,yes
203,Agent Based Replica Placement in a Data Grid Environement,"In a data grid, large quantities of data files are produced and data replication is applied to reduce data access time. Determining when and where to replicate data in order to meet performance goals in grid systems with many users and files, dynamic network and resource characteristics and changing user behavior is difficult. Therefore efficiency and fast access to replicated data are influenced by the location of the resource holding the replica. In this paper, we present an agent based replica placement algorithm to determine the candidate site for the placement of replica. An agent is deployed at each site holding the master copies of the shared data files. To create a replica, each agent prioritizes the resources in the grid based on the resource configuration, bandwidth in the network and the demand for the replica at their sites and then creates a replica at suitable resource locations. We have carried out the simulation using GridSim Toolkit-4.0 for EU Data Grid Testbed1. The simulation results show that the aggregated data transfer time and the execution time for jobs at various resources is less for agent based replica placement.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231873,no
204,BugFix: A learning-based tool to assist developers in fixing bugs,"We present a tool called BugFix that can assist developers in fixing program bugs. Our tool automatically analyzes the debugging situation at a statement and reports a prioritized list of relevant bug-fix suggestions that are likely to guide the developer to an appropriate fix at that statement. BugFix incorporates ideas from machine learning to automatically learn from new debugging situations and bug fixes over time. This enables more effective prediction of the most relevant bug-fix suggestions for newly-encountered debugging situations. The tool takes into account the static structure of a statement, the dynamic values used at that statement by both passing and failing runs, and the interesting value mapping pairs associated with that statement. We present a case study illustrating the efficacy of BugFix in helping developers to fix bugs.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090029,no
205,Building Prioritized Pairwise Interaction Test Suites with Ant Colony Optimization,"Interaction testing offers a stable cost-benefit ratio in identifying faults. But in many testing scenarios, the entire test suite cannot be fully executed due to limited time or cost. In these situations, it is essential to take the importance of interactions into account and prioritize these tests. To tackle this issue, the biased covering array is proposed and the Weighted Density Algorithm (WDA) is developed. To find a better solution, in this paper we adopt ant colony optimization (ACO) to build this prioritized pairwise interaction test suite (PITS). In our research, we propose four concrete test generation algorithms based on Ant System, Ant System with Elitist, Ant Colony System and Max-Min Ant System respectively. We also implement these algorithms and apply them to two typical inputs and report experimental results. The results show the effectiveness of these algorithms.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381411,yes
206,Configuration aware prioritization techniques in regression testing,"Configurable software lets users customize applications in many ways, and is becoming increasingly prevalent. Regression testing is an important but expensive way to build confidence that software changes introduce no new faults as software evolves, resulting in many attempts to improve its performance given limited resources. Whereas problems such as test selection and prioritization at the test case level have been extensively researched in the regression testing literature, they have rarely been considered for configurations, though there is evidence that we should not ignore the effects of configurations on regression testing. This research intends to provide a framework for configuration aware prioritization techniques, evaluated through empirical studies.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071025,yes
207,Dynamic admission control and path allocation for SLAs in DiffServ networks,"Today's converged networks are mainly characterized by their support of real-time and high priority traffic requiring a certain level of quality of service (QoS). In this context, traffic classification and prioritization are key features in providing preferential treatments of the traffic in the core of the network. In this paper, we address the joint problem of path allocation and admission control (JPAC) of new Service Level Agreements (SLA) in a DiffServ domain. In order to maximize the resources utilization and the number of admitted SLAs in the network, we consider a statistical bandwidth constraints allowing for a certain overbooking over the network's links. SLAs' admissibility decisions are based on solving to optimality an integer linear programming (ILP) model. When tested by simulations, numerical results confirm that the proposed model can be solved to optimality for real-sized instances within acceptable computation times and substantially reduces the SLAs blocking probability, compared to a the Greedy mechanism proposed in the literature.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090106,no
208,Experimental Comparison of Code-Based and Model-Based Test Prioritization,"During regression testing, a modified system needs to beretested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Code-based test prioritization methods are based on the source code of the system, whereas model-based test prioritization methods are based on system models. System modeling is a widely used technique to model state-based systems. Models can be used not only during software development but also during testing. In this paper, we briefly overview codebased and model-based test prioritization. In addition, we present an experimental study in which the code based test prioritization and the model-based test prioritization are compared.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976373,yes
209,Formal Study of Prioritized Service Compositions,"This paper presents an enhanced derivation procedure to obtain a system of services, from a given choreography. In addition to the basic framework, we introduce several situations where nondeterminism appears and it is resolved by using a dynamic prioritized system. The priority policy is based on several parameters such as the request dispatching, the response time, the quality of the response, etc. These parameters are identified as resources used by a utility function, which determines the priority of each possible option in a nondeterministic choice.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5633969,no
210,From Cradle to Sprint: Creating a Full-Lifecycle Request Pipeline at Nationwide Insurance,"After a successful transition from a prescriptive waterfall process to Scrum and XP, the Corporate Internet Solutions group at Nationwide Insurance found velocity and efficiency stumbling due to the competing and vague priorities of corporate silos. This presentation discusses how the team evolved the traditional Scrum process to better manage 17 dependent projects, and reluctant internal business partners, through a combination of activities including clear Pre-Discovery activities, scenario planning, RITE usability testing, and kanban-style visual management systems.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5261080,no
211,Fundamentals of risk based inspection ?? a practical approach,"New API structural integrity management recommended practice and post-hurricane inspection bulletin, MMS proposed changes to CFR's on decommissioning, and increased interest in developing risk based inspection plans for offshore assets require better understanding of reliability principles. This paper will discuss the fundamentals of RBI from a practical viewpoint and gives details of recently published applicable DNV offshore standards. Offshore structures must be inspected to maintain an acceptable safety level throughout their lifetime. Inspections have traditionally been based upon experience, and judgment of likelihood and consequence of failure. Risk based inspection planning (RBI) for structures, as developed by DNV, represents a systematic, qualitative and quantitative approach which combines theoretical models, test results and in-service experiences. The method is specially developed for application to all types of offshore structures including jackets, jack-ups, TLPs, FPSOs, spars, semi-submersibles, GBSS, and subsea templates. The basis of RBI is to prioritize individual items and systems by considering the associated risks. Attention is given to high risk items, while low risk items receive a more appropriately lesser level of inspection. RBI focuses on cost optimization in all associated activities, to ensure a cost optimal inspection program. The RBI analysis is performed in two steps, comprising a risk screening and a subsequent inspection scheduling. Dedicated software models are utilized to establish the Risk Matrix and to calculate the ?time to next inspection?. The final deliverable of an RBI analysis is an inspection plan, in which inspection efforts are prioritized from an overall risk perspective.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422330,no
212,Host based intrusion detection using RBF neural networks,A novel approach of host based intrusion detection is suggested in this paper that uses Radial basis Functions Neural Networks as profile containers. The system works by using system calls made by privileged UNIX processes and trains the neural network on its basis. An algorithm is proposed that prioritize the speed and efficiency of the training phase and also limits the false alarm rate. In the detection phase the algorithm provides implementation of window size to detect intrusions that are temporally located. Also a threshold is implemented that is altered on basis of the process behavior. The system is tested with attacks that target different intrusion scenarios. The result shows that the radial Basis Functions Neural Networks provide better detection rate and very low training time as compared to other soft computing methods. The robustness of the training phase is evident by low false alarm rate and high detection capability depicted by the application.,2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353204,no
213,How Well Do Test Case Prioritization Techniques Support Statistical Fault Localization,"In continuous integration, a tight integration of test case prioritization techniques and fault-localization techniques may both expose failures faster and locate faults more effectively. Statistical fault-localization techniques use the execution information collected during testing to locate faults. Executing a small fraction of a prioritized test suite reduces the cost of testing, and yet the subsequent fault localization may suffer. This paper presents the first empirical study to examine the impact of test case prioritization on the effectiveness of fault localization. Among many interesting empirical results, we find that coverage-based and random techniques can be more effective than distribution-based techniques in supporting statistical fault localization.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254274,yes
214,Implementation of the Software Quality Ranks method in the legacy product development environment,"Software quality ranks (SQR) is an important method to manage and improve software quality. Component software quality has a major influence in development project lead time and cost. SQR enables better management and visibility of the quality effort associated with the component implementation. It also provides a roadmap for continuous improvement leading to value add quality attributes like low maintenance, self optimizing software and short development lifecycles. SQR method focuses attention to prioritizing the quality investment on design component level through different quality assurance mechanisms (basic test, code review, desk checks, documentation and other actions). The resulting design delivery to verification phase will be more predictable quality software with shorter lead-time and time-to-market (TTM).",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206363,no
215,Jtop: Managing JUnit Test Cases in Absence of Coverage Information,"Test case management may make the testing process more efficient and thus accelerate software delivery. With the popularity of using JUnit for testing Java software, researchers have paid attention to techniques to manage JUnit test cases in regression testing of Java software. Typically, most existing test case management tools are based on the coverage information. However, coverage information may need extra efforts to obtain. In this paper, we present an Eclipse IDE plug-in (named Jtop) for managing JUnit test cases in absence of coverage information. Jtop statically analyzes the program under test and its corresponding JUnit test cases to perform the following management tasks: regression test case selection, test suite reduction and test case prioritization. Furthermore, Jtop also enables the programmer to manually manipulate test cases through a graphical user interface.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431709,no
216,Keynote: Security Engineering: Developments and Directions,"Security Engineering is a critical component of systems engineering. When complex and large systems are put together, one needs to ensure that the systems are secure. Security engineering methodologies include gathering the security requirements, specifying the security policies, designing the security model, identifying the security critical components of the system design, security verification and validation and security testing. Before installation, one needs to develop a concept of operation (CONOPS) as well as carry out certification and accreditation. Much of the previous work in security engineering has focused on end to end security. That is, the organization needs to ensure that the applications, database systems, operating systems and networks have to be secure. In addition, one needs to ensure security when the subsystems are composed to form a larger system. More recently with open systems and the Web, secure system development is taking a whole new direction. The Office of the Deputy Assistant Secretary of Defense in the United States (Information and Identity Assurance) has stated that ""the Department of Defense's (DoD) policy, planning, and war fighting capabilities are heavily dependent on the information technology foundation provided by the Global Information Grid (GIG). However, the GIG was built for business efficiency instead of mission assurance against sophisticated adversaries who have demonstrated intent and proven their ability to use cyberspace as a tool for espionage and criminal theft of data. GIG mission assurance works to ensure the DoD is able to accomplish its critical missions when networks, services, or information are unavailable, degraded, or distrusted."" To meet the needs of mission assurance challenges, President's (George W. Bush) cyber plan (CNCI) has listed the area of developing multipronged approaches to supply chain risk management as one of the priorities. CNCI states that the reality of global supply chains presents significant challenges in thwarting counterfeit, or maliciously designed hardware and software products. To overcome such challenges and support successful mission assurance we need to design flexible and secure systems whose components may be untrusted or faulty. We need to achieve the secure operation of mission critical systems constructed from untrusted, semitrusted and fully trusted components for successful mission assurance. This keynote address will discuss the developments in security engineering from requirements, to policy to model to design to verification to testing as well as developing CONOPS and conducting certification and accreditation. System evaluation, usability and metrics related issues will also be discussed. Finally we will discuss the changes that have to be made to security engineering to support the next generation of secure systems for mission critical applications.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325402,no
217,Language Identification from an Indian Multilingual Document Using Profile Features,"In order to reach a larger cross section of people, it is necessary that a document should be composed of text contents in different languages. But on the other hand, this causes practical difficulty in OCRing such a document, because the language type of the text should be pre-determined, before employing a particular OCR. In this research work, this problem of recognizing the language of the text content is addressed, however it is perhaps impossible to design a single recognizer which can identify a large number of scripts/languages. As a via media, in this research we have proposed to work on the prioritized requirements of a particular region, for instance in Karnataka state in India,generally any document including official ones, would contain the text in three languages-English-the language of general importance, Hindi-the language of National importance and Kannada -the language of State/Regional importance. We have proposed to learn identifying the language of the text by thoroughly understanding the nature of top and bottom profiles of the printed text lines in these three languages.Experimentation conducted involved 800 text lines for learning and 600 text lines for testing. The performance has turned out to be 95.4%.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804543,no
218,Lightweight Elicitation and Analysis of Software Product Quality Goals: A Multiple Industrial Case Study,"We developed and used a method that gathers relevant stakeholders to elicit, prioritize, and elaborate the quality goals of a software product. It is designed to be lightweight and easy to learn compared to methods for a more comprehensive analysis of non-functional requirements. The method and the resulting quality goals are meant especially for improving the software product management process. We used it in four software product companies, and report lessons learned and evaluation of the method based on practitioners' comments. We found it better to set the goals first for the product in general before discussing a specific release project. In addition to identifying goals that needed improvement, the practitioners considered identifying already achieved goals relevant, but they were neg- lected unless explicitly considered. Using ISO 9126 as a checklist after brainstorming did not add many goals. Prioritization was challenging due to numerous relevant perspectives. Conceiving measures for impor- tant goals seemed to concretize them.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457326,no
219,Measurement and control for risk-based test cases and activities,"Risk-based testing is an approach that consists of a set of activities regarding risk factors identification related to software requirements. Once identified, the risks are prioritized according to its likelihood and impact, and the test cases are projected based on the strategies for treatment of the identified risk factors. Then, test efforts are continuously adjusted according the risk monitoring. Most risk-based testing approaches focuses on activities related to risk identification, analysis and prioritizing. However, metrics are fundamental as they quantify characteristics of a process or product and support software project management activities. In this light, this paper proposes and discusses risk-based testing metrics to measure and control test cases and test activities progress, efforts and costs.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813802,no
220,Mesh your Senses: Multimedia Applications over WiFi-based Wireless Mesh Networks,"This demo aims at (i) validating the design choices we have made in conceiving and deploying the WING testbed, and (ii) showing the capability of out software toolkit to properly support heterogeneous multimedia applications. Additionally, the mesh networking toolkit's fault management features is demonstrated. We hope that our wireless mesh networking toolkit is considered by both researchers and practitioners as platform of choice to test innovative solutions and to provide end-users with wireless connectivity. WING is an experimental multi-radio WMN testbed designed and built exploiting commodity hardware and open-source software components. WING implements a flexible and scalable WMN architecture capable of supporting next-generation Internet services with a particular focus on multimedia applications. The WING project aims at providing an open-platform on top of which innovative solution can be implemented and tested in a realistic environment. Currently, the testbed consist of 10 nodes deployed at CREATE-NET premises and implementing a two-tiers architecture. Other well-known IEEE 802.11-based WMNs include Roofnet, Hyacinth, Microsoft's MCL, and Meraki. We establish the uniqueness of our mesh solution in that it is capable of achieving both service differentiation and performance isolation in IEEE 802.11-based WMNs. While not providing strict QoS performance bounds, the proposed scheme aims at enhancing the perceived quality of experience by combining opportunistic scheduling and packet aggregation and by implementing a DiffServ-like architecture in order to provide traffic prioritization.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172945,no
221,Performance impact analysis with KPP using application response measurement in E-government systems,"In this paper, the performance impact analysis of e-government systems with key performance parameters is being considered. Meaningful impact analysis in sustained government systems is required for considering non-functional requirements and functional requirements. Performance requirements are a critical component of non-functional areas. For example, if a new system change is set to the system, the impact in terms of the response time must be implemented in each sub-system. In this paper, an XML-based framework can be used to analyze performance impacts on sub-systems and can provide a scheme to enhance impact analysis by performance monitoring using application response measurement. Through a health system example as a case study, a performance requirement model to describe extended trees and adapting analysis result of performance monitoring using application response measurement and XML tree representation are addressed. This paper also proposes a scheme for prioritized processing and an algorithm for effectively enhancing impact analysis in a timely fashion.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306282,no
222,Policy-Based Network Management in Home Area Networks: Interim Test Results,"This paper argues that Home Area Networks (HANs) are a good candidate for advanced network management automation techniques, such as Policy-Based Network Management (PBNM). What is proposed is a simple use of policy based network management to introduce some level of Quality of Service (QoS) and Security management in the HAN, whilst hiding this complexity from the home user. In this paper we have presented the interim test results of our research experiments (based on a scenario) using the HAN testbed. After using policies to prioritize different traffic, packet loss decreased to 30% and VoIP quality improved dramatically without employing any intelligent bandwidth allocation technique.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384722,no
223,Predicting Attack-prone Components,"Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. This limitation necessitates security risk management where security efforts are prioritized to the highest risk vulnerabilities that cause the most damage to the end user. We created a predictive model that identifies the software components that pose the highest security risk in order to prioritize security fortification efforts. The input variables to our model are available early in the software life cycle and include security-related static analysis tool warnings, code churn and size, and faults identified by manual inspections. These metrics are validated against vulnerabilities reported by testing and those found in the field. We evaluated our model on a large Cisco software system and found that 75.6% of the system's vulnerable components are in the top 18.6% of the components predicted to be vulnerable. The model's false positive rate is 47.4% of this top 18.6% or 9.1% of the total system components. We quantified the goodness of fit of our model to the Cisco data set using a receiver operating characteristic curve that shows 94.4% of the area is under the curve.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815350,no
224,Prioritization of Scenarios Based on UML Activity Diagrams,"Increased size and complexity of software requires better methods for different activities in the software development lifecycle. Quality assurance of software is primarily done by means of testing, an activity that faces constraints of both time and resources. Hence, there is need to test effectively within the constraints in order to maximize throughput i.e. rate of fault detection, coverage, etc. Test case prioritization involves techniques aimed at finding the best prioritized test suite. In this paper, we propose a prioritization technique based on UML activity diagrams. The constructs of an activity diagram are used to prioritize scenarios. Preliminary results obtained on a case-study indicate that the technique is effective in extracting the critical scenarios from the activity diagram.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231936,yes
225,Prioritized Test Generation Strategy for Pair-Wise Testing,"Pair-wise testing is widely used to detect faults in software systems. In many applications where pair-wise testing is needed, the whole test set can not be run completely due to time or budget constraints. In these situations, it is essential to prioritize the tests. In this paper, we drive weight for each value of each parameter, and adapt UWA algorithm to generate an ordered pair-wise coverage test suite. UWA algorithm is to accord weights set for each value of each parameter of the system, then produce ordered pair-wise coverage test set for having generated but unordered one. Finally, a greedy algorithm is adopted to prioritize generated pair-wise coverage test set with driven weights, so that whenever the testing is interrupted, interactions deemed, most important are tested.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368244,yes
226,Prioritizing component compatibility tests via user preferences,"Many software systems rely on third-party components during their build process. Because the components are constantly evolving, quality assurance demands that developers perform compatibility testing to ensure that their software systems build correctly over all deployable combinations of component versions, also called configurations. However, large software systems can have many configurations, and compatibility testing is often time and resource constrained. We present a prioritization mechanism that enhances compatibility testing by examining the ldquomost importantrdquo configurations first, while distributing the work over a cluster of computers. We evaluate our new approach on two large scientific middleware systems and examine tradeoffs between the new prioritization approach and a previously developed lowest-cost-configuration-first approach.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306357,yes
227,Prioritizing JUnit test cases in absence of coverage information,"Better orderings of test cases can detect faults in less time with fewer resources, and thus make the debugging process earlier and accelerate software delivery. As a result, test case prioritization has become a hot topic in the research of regression testing. With the popularity of using the JUnit testing framework for developing Java software, researchers also paid attention to techniques for prioritizing JUnit test cases in regression testing of Java software. Typically, most of them are based on coverage information of test cases. However, coverage information may need extra costs to acquire. In this paper, we propose an approach (named Jupta) for prioritizing JUnit test cases in absence of coverage information. Jupta statically analyzes call graphs of JUnit test cases and the software under test to estimate the test ability (TA) of each test case. Furthermore, Jupta provides two prioritization techniques: the total TA based technique (denoted as JuptaT) and the additional TA based technique (denoted as JuptaA). To evaluate Jupta, we performed an experimental study on two open source Java programs, containing 11 versions in total. The experimental results indicate that Jupta is more effective and stable than the untreated orderings and Jupta is approximately as effective and stable as prioritization techniques using coverage information at the method level.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306350,yes
228,Prioritizing test cases for resource constraint environments using historical test case performance data,"Regression testing has been widely used to assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive in that, it requires many test case executions and a large number of test cases. To provide the missing flexibility, researchers introduced prioritization techniques. The aim in this paper has been to prioritize test cases during software regression test. To achieve this, a new equation is presented. The proposed equation considers historical effectiveness of the test cases in fault detection, each test case's execution history in regression test and finally the last priority assigned to the test case. The results of applying the proposed equation to compute the priority of regression test cases for two benchmarks, known as Siemens suite and Space program, demonstrate the relatively faster fault detection in resource and time constrained environments.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234968,yes
229,Prioritizing Use Cases to Aid Ordering of Scenarios,"Models are used as the basis for design and testing of software. The unified modeling language (UML) is used to capture and model the requirements of a software system. One of the major requirements of a development process is to detect defects as early as possible. Effective prioritization of scenarios helps in early detection of defects as well maximize effort and utilization of resources. Use case diagrams are used to represent the requirements of a software system. In this paper, we propose using data captured from the primitives of the use case diagrams to aid in prioritization of scenarios generated from activity diagrams. Interactions among the primitives in the diagrams are used to guide prioritization. Customer prioritization of use cases is taken as one of the factors. Preliminary results on a case study indicate that the technique is effective in prioritization of test scenarios.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358815,yes
230,Quasi-Renewal Time-Delay Fault-Removal Consideration in Software Reliability Modeling,"Software reliability growth models based on a nonhomogeneous Poisson process (NHPP) have been considered as one of the most effective among various models since they integrate the information regarding testing and debugging activities observed in the testing phase into the software reliability model. Although most of the existing NHPP models have progressed successfully in their estimation/prediction accuracies by modifying the assumptions with regard to the testing process, these models were developed based on the instantaneous fault-removal assumption. In this paper, we develop a generalized NHPP software reliability model considering quasi-renewal time-delay fault removal. The quasi-renewal process is employed to estimate the time delay due to identifying and prioritizing the detected faults before actual code change in the software reliability assessment. Model formulation based on the quasi-renewal time-delay assumption is provided, and the generalized mean value function (MVF) for the proposed model is derived by using the method of steps. The general solution of the MVFs for the proposed model is also obtained for some specific existing models. The numerical examples, based on a software failure data set, show that the consideration of quasi-renewal time-delay fault-removal assumption improves the descriptive properties of the model, which means that the length of time delay is getting decreased since testers and programmers adapt themselves to the working environment as testing and debugging activities are in progress.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694128,no
231,Reducing Field Failures in System Configurable Software: Cost-Based Prioritization,"System testing of configurable software is an expensive and resource constrained process. Insufficient testing often leads to escaped faults in the field where failures impact customers and are costly to repair. Prior work has shown that it is possible to efficiently sample configurations for testing using combinatorial interaction testing, and to prioritize these configurations to increase the rate of early fault detection. The underlying assumption to date has been that there is no added complexity to configuring a system level environment over a user configurable one; i.e. the time required to setup and test each individual configuration is nominal. In this paper we examine prioritization of system configurable software driven not only by fault detection but also by the cost of configuration and setup time that moving between different configurations incurs. We present a case study on two releases of an enterprise software system using failures reported in the field. We examine the most effective prioritization technique and conclude that (1) using failure history of configurations can improve the early fault detection rate, but that (2) we must consider fault detection rate over time, not by the number of configurations tested. It is better to test related configurations which incur minimal setup time than to test fewer, more diverse configurations.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362084,no
232,Simple Time-to-Failure Estimation Techniques for Reliability and Maintenance of Equipment,"Proper reliability and maintenance best practice processes have a direct impact on equipment availability, throughput capacity, and spare inventories. The purpose of the time-to-failure estimation (TTFE) technique is to provide a tool for engineers and technicians for risk-based reporting of condition- based maintenance tests and inspections. Through the proper application of this technique, corrective action may be prioritized improving the effectiveness of the maintenance program. Instead of stakeholders being required to make decisions based upon experience only, equipment failure, and repair history can be used to enhance the process, improving the availability of critical equipment.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5191412,no
233,Surviving Insecure IT: Effective Patch Management,"The amount of time to protect enterprise systems against potential vulnerability continues to shrink. Enterprises need an effective patch management mechanism to survive the insecure IT environment. Effective patch management is a systematic and repeatable patch distribution process which includes establishing timely and practical alerts, receiving notification of patches or discovering them, downloading patches and documentation, assessing and prioritizing vulnerabilities, performing testing, deploying patches, and auditing.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804050,no
234,Tag-Based Techniques for Black-Box Test Case Prioritization for Service Testing,"A web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable. The rich interface specifications of a web service, however, provide peer services with a means to formulate black-box testing strategies. In this paper, we formulate new test case prioritization strategies using tags embedded in XML messages to reorder regression test cases, and reveal how the test cases use the interface specifications of services. We evaluate experimentally their effectiveness on revealing regression faults in modified WS-BPEL programs. The results show that the new techniques can have a high probability of outperforming random ordering.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381531,yes
235,Techniques for building excellent operator machine interfaces (OMI),"Establishing a process to continually improve understanding of operator requirements - the why as well as the how - is key to developing an optimal Operator-Machine Interface (OMI) for a large program. The P-8A program is one example of a complex software development, with over 2 million lines of code and a unique and complex OMI. The OMI itself is used as a tool for achieving and is designed in parallel with understanding end goals, operator decision making chains, and thought processes. The OMI can be designed to support and anticipate information needs and alert operators to unusual occurrences. Operator actions and decision-making techniques can be discovered by using expert operators and capturing their knowledge through the use of multiple tools to identify the problem set: storytelling, decision mapping, and is/is not matrices. When a solution set has been identified and agreed to by the customer, resources allocated, and schedule set, a standard process captures and prioritizes further changes, identities their impact, and decides on implementation. Documents describing design and processes and a design description document describing the current version of OMI are made accessible to stakeholders at all times.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5317781,no
236,Test case prioritization based on data reuse an experimental study,"The order in which tests are executed can significantly impact the total test execution time. In this paper, we evaluate two test prioritization techniques (manual and automatic) in the context of mobile phone testing. The manual technique produces test sequences created by test experts, while the automatic one generates sequences mechanically based on the permutation of the tests. Both techniques take into account a data reuse: the more the data is reused among tests, the faster the sequence is executed. In order to evaluate the benefits of these two techniques, we carried out an experiment with 8 testers and 2 test suites arranged in a 2times2 Latin square design replicated four times. The automatic technique reduced approximately 25% of the data generation time and 13.5% of the execution time. The automatic technique is clearly better than the manual one with respect to the generation of sequences. Our experiment showed that the automatic technique also generates sequences whose execution is faster than those created manually by test experts.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5315980,yes
237,Test Selection Prioritization Strategy,"A wide divergence is observed in projects between test activities planned in the test plan and the actual tests that can be executed. Estimates for test execution computed during the planning are inaccurate without test design. The actual time and resources available are usually less than planned. Assuming that time and resources cannot be changed, a dynamic selection of tests for execution that maximizes quality is required.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254084,yes
238,Testing Processes in Business-Critical Chain Software Lifecycle,"The business-critical chain lifecycle is an agile software development lifecycle that aims at aligning the software project deliverables to attain the business objectives based on business priorities. The traditional software development projects work on the assumption that all `equal effort consumers' would be treated equally and worked upon. The agile methodology ensures that the delivery cycles are reduced thus introducing agility in the way business is supported by underlying technologies. The proposed lifecycle model introduces new variables pertaining to the business value generation each finished piece of code would produce. Hence the software project processes have to be modified to cater to it. Even the usual agile lifecycle testing strategies need to be modified to suit the proposed model. The test plans, test estimations, test resource management, quality control, regression plans and automation road map and plans have to be customized to cater to the new life cycle model. The secondary project management activities such as risk management, procurement management, etc also may to be modified with respect to the testing processes. My paper aims at using the critical chain principles and proposes a software lifecycle model that can cater to business priorities and aligning the testing processes not only to development cycles but also to the actual business value created. In this paper I would take a case study and compare and contrast when project uses the regular models and this new model. I would also provide guidelines to use this lifecycle model and modify regular project management activities to cater to the new model with emphasis on the testing processes. The paper would try to provide the ideal scenarios; in project teams, in consulting firms and in new customers and expectations; where such a model could provide high impact on the way consulting companies can do successful projects and creating more value to the customers.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319542,yes
239,The impact of test case reduction and prioritization on software testing effectiveness,"Software testing is critical but most expensive phase of Software Development Life Cycle (SDLC). Development organizations desire to thoroughly test the software. But this exhaustive testing is impractical due to resource constraints. A large number of test suites are generated using automated tools. But the real challenge is the selection of subset of test cases and/or high order test cases crucial to validate the System Under Test (SUT). Test case reduction and prioritization techniques help test manager to solve this problem at a little cost. In this paper, we investigate their impact on testing process effectiveness using previous empirical studies. The results indicate that these techniques improve the testing effectiveness significantly. At the end, a case study is presented that suggests different useful combinations of these techniques, which are helpful for different testing scenarios.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353136,yes
240,Type Inference for Soft-Error Fault-Tolerance Prediction,"Software systems are becoming increasingly vulnerable to a new class of soft errors, originating from voltage spikes produced by cosmic radiation. The standard technique for assessing the source-level impact of these soft errors, fault injection - essentially a black-box testing technique - provides limited high-level information. Since soft errors can occur anywhere, even control-structured white-box techniques offer little insight. We propose a type-based approach, founded on data-flow structure, to classify the usage pattern of registers and memory cells. To capture all soft errors, the type system is defined at the assembly level, close to the hardware, and allows inferring types in the untyped assembly representation. In a case study, we apply our type inference scheme to a prototype brake-by-wire controller, developed by Volvo Technology, and identify a high correlation between types and fault-injection results. The case study confirms that the inferred types are good predictors for soft-error impact.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431783,no
241,"Uncertainty management in software engineering: Past, present, and future","Software development has significantly matured in the last decade. However, one of the critical challenges today is uncertainty inherent to every aspect of software development including requirement specifications, design, coding, and testing. In this paper, we propose a framework for uncertainty management in software engineering. The framework is used to model uncertainty inherent to software development activities and manage their consequences. The framework consists of four main phases: identification and prioritization, modeling and analysis, management and planning, and monitoring and evaluation. Commercial off-the-shelf (COTS)-based development is selected as an example to illustrate how the proposed framework is used in a simple but intuitive case study to represent uncertainty and manage its consequences.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090081,no
242,Using String Distances for Test Case Prioritisation,"Test case prioritisation aims at finding an ordering which enhances a certain property of an ordered test suite. Traditional techniques rely on the availability of code or a specification of the program under test. In this paper, we propose to use string distances on the text of test cases for their comparison and elaborate a prioritisation algorithm. Such a prioritisation does not require code and can be useful for initial testing and in cases when code is difficult to instrument. We also briefly report on preliminary results of an experiment where the proposed prioritisation technique was compared with random permutations and four classical string distance metrics were evaluated.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431745,yes
243,Visualizing the structure of field testing problems,Field testing of a software application prior to general release is an important and essential quality assurance step. Field testing helps identify unforeseen problems. Extensive field testing leads to the reporting of a large number of problems which often overwhelm the allocated resources. Prior efforts focus primarily on studying the reported problems in isolation. We believe that a global view of the interdependencies between these problems will help in rapid understanding and resolution of reported problems. We present a visualization that highlights the commonalities between reported problems. The visualization helps developers identify two patterns that they can use to prioritize and focus their efforts. We demonstrate the applicability of our visualization through a case study on problems reported during field testing efforts for two releases of a large scale enterprise application.,2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306297,yes
244,A Regression Testing Approach for Software Product Lines Architectures,"In the Software Product Lines (SPL) context, where products are derived from a common platform, the reference architecture can be considered the main asset. In order to maintain its correctness and reliability after modifications, a regression testing approach based on architecture specification and code was developed. It aims to reduce the testing effort, by reusing test cases, execution results, as well as, selecting and prioritizing an effective set of test cases. Taking advantage of SPL architectures similarities, this approach can be applied among product architectures and between the reference and product architecture. This study also presents an evaluation performed in order to calibrate and improve the proposed approach.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631684,no
245,A Simulation Study on Some Search Algorithms for Regression Test Case Prioritization,"Test case prioritization is an approach aiming at increasing the rate of faults detection during the testing phase, by reordering test case execution. Many techniques for regression test case prioritization have been proposed. In this paper, we perform a simulation experiment to study five search algorithms for test case prioritization and compare the performance of these algorithms. The target of the study is to have an in-depth investigation and improve the generality of the comparison results. The simulation study provides two useful guidelines: (1) Two search algorithms, Additional Greedy Algorithm and 2-Optimal Greedy Algorithm, outperform the other three search algorithms in most cases. (2) The performance of the five search algorithms will be affected by the overlap of test cases with regard to test requirements.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562946,yes
246,A source-based risk analysis approach for software test optimization,"In this paper we introduce our proposed technique for software component test prioritization and optimization which is based on a source-code based risk analysis. Software test is one of the most critical steps in the software development. Considering that the time and human resources of a software project are limited, software test should be scheduled and planned very carefully. In this paper we introduce a classification approach that provides the developers with a risk model of the application which is specifically designed to assist the testing process by identifying the most important components and their corresponding test effort estimation. We designed an analyser tool to apply our technique to a test software project and we presented the results in this paper.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485639,yes
247,An Approach for Classifying Program Failures,"In this work, we leverage hardware performance counters-collected data to automatically group program failures that stem from closely related causes into clusters, which can in turn help developers prioritize failures as well as diagnose their causes. Hardware counters have been used for performance analysis of software systems in the past. By contrast, in this paper they are used as abstraction mechanisms for program executions. The results of our feasibility studies conducted on two widely-used applications suggest that hardware counters-collected data can be used to reliably classify failures.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5617204,no
248,An Automatic Configuration Approach to Improve Real-Time Application Throughput While Attaining Determinism,"Determinism and throughput are two important performance measures for Java-based real-time applications, but they often conflict. Therefore, it is significant to improve throughput for Java-based real-time applications while guaranteeing its execution time determinism. In this paper, we propose an automatic configuration approach to assign real-time thread priorities to solve the above-mentioned problem. In this approach, we propose an innovative representation of determinism related with real-time thread priorities using stochastic process. Java-based real-time application's throughput is quantified with thread priorities as parameters. The algorithm of integer programming is used to optimize throughput with boundary conditions of the level of determinism. Finally, the Sweet Factory application is tested to evaluate the effect of our approach. Experiment results show that throughput for Java-based real-time applications could be efficiently improved while keeping the execution time determinism with our approach.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676292,no
249,An Intelligent Approach of Obtaining Feasible Machining Processes and Their Selection Priorities for Features Based on Neural Network,"To obtain all feasible machining processes and their quantitative selection priorities, an intelligent making decision approach combining back-propagation neural network and backward planning is proposed. Uniform design method, which is adapted for the problem of multiple factors and multiple levels, is adopted to build representative sample sets for the neural network. The neural network is trained by an improved back-propagation algorithm which can adjust momentum factor and learning rate simultaneously, and tested by linear regression analysis. A case study has been conducted to demonstrate the effectiveness of the proposed approach.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677004,no
250,Analysis and optimization of software requirements prioritization techniques,"Prioritizing requirements helps the project team to understand which requirements are most important and most urgent. Based on this finding a software engineer can decide what to develop/implement in the first release and what on the coming releases. Prioritization is also a useful activity for decision making in other phases of software engineering like development, testing, and implementation. There are a number of techniques available to prioritize the requirements with their associated strengths and limitations. In this paper we will examine state of the art techniques and analyze their applicability on software requirements domain. At the end we present a framework that will help the software engineer of how to perform prioritization process by combining existing techniques and approaches.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625687,no
251,Analytical survey on automated software test data evaluation,Automated software test data optimization has become a major aspect in quality of any software. For quality different test cases has to be performed for testing. In order to evaluate every aspect of the software program the number of test cases has increased tremendously. In this paper author have tried to evaluate different proposed techniques for automated software test data optimization and emphasize is made to extract the critical factors which need to be present in any technique to make the technique optimized one. These factors are then evaluated on the basis of different papers and concluded some results which are beneficial to work for the creation of an optimized technique.,2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488549,no
252,Arranging software test cases through an optimization method,"During the software testing process, the customers would be invited to review or inspect an ongoing software product. This phase is called the ??in-plant? test, often known as an ??alpha? test. Typically, this test phase lasts for a very short period of time in which the software test engineers or software quality engineers rush to execute a list of software test cases in the test suite with customers. Because of the time constraint, the test cases have to be arranged in terms of test case severities, estimated test time, and customers' demands. As important as the test case arrangement is, this process is mostly performed manually by the project managers and software test engineers together. As the software systems are getting more sophisticated and complex, a greater volume of test cases have to be generated, and the manual arrangement approach may not be the most efficient way to handle this. In this paper, we propose a framework for automating the process of test case arrangement and management through an optimization method. We believe that this framework will help software test engineers facing with the challenges of prioritizing test cases.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602131,yes
253,Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows,"We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062117,no
254,Constructing Prioritized Interaction Test Suite with Interaction Relationship,"Interaction testing has addressed some issues on how to select a small subset of test cases. In many systems where interaction testing is needed, the entire test suite is not executed because of time or budget constraints. It is important to prioritize the test cases in these situations. On the other hand, there are not always interactions among any factors in real systems. Moreover, some factors may need N-way (N&gt;2) testing since there is a closer relationship among them. We present a model for prioritized interaction testing with interaction relationship and propose a greedy algorithm for generating variable strength covering arrays with bias.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459713,yes
255,Design and Analysis of Cost-Cognizant Test Case Prioritization Using Genetic Algorithm with Test History,"During software development, regression testing is usually used to assure the quality of modified software. The techniques of test case prioritization schedule the test cases for regression testing in an order that attempts to increase the effectiveness in accordance with some performance goal. The most general goal is the rate of fault detection. It assumes all test case costs and fault severities are uniform. However, those factors usually vary. In order to produce a more satisfactory order, the cost-cognizant metric that incorporates varying test case costs and fault severities is proposed. In this paper, we propose a cost-cognizant test case prioritization technique based on the use of historical records and a genetic algorithm. We run a controlled experiment to evaluate the proposed technique's effectiveness. Experimental results indicate that our proposed technique frequently yields a higher Average Percentage of Faults Detected per Cost (APFDc). The results also show that our proposed technique is also useful in terms of APFDc when all test case costs and fault severities are uniform.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676289,yes
256,Efficient Reduction of Model-Based Generated Test Suites through Test Case Pair Prioritization,"During the development and maintenance of software, test suites often reach a size that exceeds the costs allocated for test suite execution. In such a case, the test suite needs to be reduced. Many papers are dedicated to the problem of test suite reduction. Most of them consider the removal or merging of test cases. However, less attention has been paid to the identification of test case pairs, which are eminently suitable for merging. In this paper, we fill this gap by presenting a novel approach that helps identifying those test case pairs within a given set of systematically generated test cases which, when merged, have potential for high test suite reduction. As a result, test suites reduced by our approach are considerably smaller in size than those, whose pairs are selected randomly.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772249,yes
257,Evaluating and Enhancing Xen-Based Virtual Routers to Support Real-Time Applications,"Router virtualization seems as the obvious next step to system virtualization and the key to easily deploy and manage next generation overlay virtual networks. In this paper, we investigate the viability of virtual routers on a Xen-based system. We first evaluate the system throughput when achieving forwarding in the virtual routers. Then, we consider the context where virtual routers are dedicated to flows of different types and propose a mechanism to guarantee the required throughput and latency to real time applications while maintaining an optimal aggregated system throughput. We achieved this through both configuring the Xen Credit scheduler and establishing priorities between packets in the driver domain before switching them to the target virtual router.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421620,no
258,Making defect-finding tools work for you,"Given the high costs of software testing and fixing bugs after release, early detection of bugs using static analysis can result in significant savings. However, despite their many benefits, recent availability of many such tools, and evidence of a positive return-on-investment, static-analysis tools are not used widely because of various usability and usefulness problems. The usability inhibitors include the lack of features, such as capabilities to merge reports from multiple tools and view warning deltas between two builds of a system. The usefulness problems are related primarily to the accuracy of the tools: identification of false positives (or, spurious bugs) and uninteresting bugs among the true positives. In this paper, we present the details of an online portal, developed at IBM Research, to address these problems and promote the adoption of static-analysis tools. We report our experience with the deployment of the portal within the IBM developer community. We also highlight the problems that we have learned are important to address, and present our approach toward solving some of those problems.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062143,no
259,Managing Testing Complexity in Dynamically Adaptive Systems: A Model-Driven Approach,"Autonomous systems are increasingly conceived as a means to allow operation in changeable or poorly understood environments. However, granting a system autonomy over its operation removes the ability of the developer to be completely sure of the system's behaviour under all operating contexts. This combination of environmental and behavioural uncertainty makes the achievement of assurance through testing very problematic. This paper focuses on a class of system, called an m-DAS, that uses run-time models to drive run-time adaptations in changing environmental conditions. We propose a testing approach which is itself model-driven, using model analysis to significantly reduce the set of test cases needed to test for emergent behaviour. Limited testing resources may therefore be prioritised for the most likely scenarios in which emergent behaviour may be observed.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463659,no
260,Measuring unmeasurable attributes of software quality using Pragmatic Quality Factor,"Software quality is evolving beyond static measurement to a wider scope of quality definition. Previous studies have indicated the importance of human aspect in software quality. But the quality models have not included comprehensively this aspect together with the behavioural aspect of software quality. This research has proposed a Pragmatic Quality Factors (or PQF) as a software quality measurement and metrics that includes both aspects of quality. These aspects of quality are essential as to balance between technical and non-technical (human) facet. In addition, this model provides flexibility by giving priorities and weights to the quality attributes. The priority and weight are necessary to reflect business requirement in the real business environment. Therefore, it is more practical that suits with different users and purposes. It is implemented through collaborative perspective approach between users, developers and independent assessor. This model shows how the unmeasurable characteristics can be measured indirectly using measures and metrics approach. It has been tested involving assessment and certification exercises in real case studies in Malaysia.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564077,no
261,Notice of Retraction<br>Designing and formulating organization performance evaluation model in AHP method based on EFQM criteria (case study),"Designing and formulating a comprehensive organization performance evaluation model based on European Foundation for Quality Management (EFQM) in AHP (Analytic Hierarchy Process) method is the main aim of the present research study. Evaluation is considered as one of the most important activities in each organization in a way that reformation of processes and procedures of doing activity without evaluation of results will be impossible. At the present research activity, AHP (Analytic Hierarchy Process) has been used as one of MADM (Multi-Attribute Decision Making) methods for the evaluation of performance of organizations through the application of EFQM (European Foundation for Quality Management) excellence model criteria. Also, Municipality of City of Tabriz has been selected as subjects for testing the presented model. In the same direction, seven districts of this municipality were selected as sample model. Necessary and required information were accumulated through questionnaire, interview and also taking advantage of data and library resources, details of which were analyzed and studied through the application of advanced Excel and Expert Choice 11.5 software package system. Eventually, various districts of this organization were evaluated and prioritized in terms of performance.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553098,no
262,"On the Integration of Test Adequacy, Test Case Prioritization, and Statistical Fault Localization","Testing and debugging account for at least 30% of the project effort. Scientific advancements in individual activities or their integration may bring significant impacts to the practice of software development. Fault localization is the foremost debugging sub-activity. Any effective integration between testing and debugging should address how well testing and fault localization can be worked together productively. How likely does a testing technique provide test suites for effective fault localization? To what extent may such a test suite be prioritized so that the test cases having higher priority can be effectively used in a standalone manner to support fault localization? In this paper, we empirically study these two research questions in the context of test data adequacy, test case prioritization and statistical fault localization. Our preliminary postmortem analysis results on 16 test case prioritization techniques and four statistical fault localizations show that branch-adequate test suites on the Siemens suite are unlikely to support effective fault localization. On the other hand, if such a test suite is effective, around 60% of the test cases can be further prioritized to support effective fault localization, which indicates that the potential savings in terms of effort can be significant.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562990,yes
263,Optimizing the Software Architecture for Extensibility in Hard Real-Time Distributed Systems,"We consider a set of control tasks that must be executed on distributed platforms so that end-to-end latencies are within deadlines. We investigate how to allocate tasks to nodes, pack signals to messages, allocate messages to buses, and assign priorities to tasks and messages, so that the design is extensible and robust with respect to changes in task requirements. We adopt a notion of extensibility metric that measures how much the execution times of tasks can be increased without violating end-to-end deadlines. We optimize the task and message design with respect to this metric by adopting a mathematical programming front-end followed by postprocessing heuristics. The proposed algorithm as applied to industrial strength test cases shows its effectiveness in optimizing extensibility and a marked improvement in running time with respect to an approach based on randomized optimization.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535223,no
264,Point-of-Interest Aware Test Case Prioritization: Methods and Experiments,"Location based services personalize their behaviors based on location data. When data kept by a service have evolved or the code has been modified, regression testing can be employed to assure the quality of services. Frequent data update however may lead to frequent regression testing and any faulty implementation of a service may affect many service consumers. Proper test case prioritization helps reveal service problems efficiently. In this paper, we review a set of point-of-interest (POI) aware test case prioritization techniques and report an experiment on such techniques. The empirical results show that these POI-aware techniques are more effective than random ordering and input-guided test case prioritization in terms of APFD. Furthermore, their effectiveness is observed to be quite stable over different sizes of the test suite.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563000,yes
265,Prioritization of Issues and Requirements by Cumulative Voting: A Compositional Data Analysis Framework,"Cumulative Voting (CV), also known as Hundred-Point Method, is a simple and straightforward technique, used in various prioritization studies in software engineering. Multiple stakeholders (users, developers, consultants, marketing representatives or customers) are asked to prioritize issues concerning requirements, process improvements or change management in a ratio scale. The data obtained from such studies contain useful information regarding correlations of issues and trends of the respondents towards them. However, the multivariate and constrained nature of data requires particular statistical analysis. In this paper we propose a statistical framework; the multivariate Compositional Data Analysis (CoDA) for analyzing data obtained from CV prioritization studies. Certain methodologies for studying the correlation structure of variables are applied to a dataset concerning impact analysis issues prioritized by software professionals under different perspectives. These involve filling of zeros, transformation using the geometric mean, principle component analysis on the transformed variables and graphical representation by biplots and ternary plots.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598119,no
266,Prioritization of test case scenarios derived from activity diagram using genetic algorithm,"Software testing involves identifying the test cases which discovers the errors in the program. However, the exhaustive testing is rarely impossible and very time consuming. In this paper, the software testing efficiency is optimized by identifying the critical path clusters. The test case scenarios are derived from the activity diagram and the testing efficiency is optimized by applying the genetic algorithm on the test data. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of activity diagram.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640479,yes
267,Prioritizing Mutation Operators Based on Importance Sampling,"Mutation testing is a fault-based testing technique for measuring the adequacy of a test suite. Test suites are assigned scores based on their ability to expose synthetic faults (i.e., mutants) generated by a range of well-defined mathematical operators. The test suites can then be augmented to expose the mutants that remain undetected and are not semantically equivalent to the original code. However, the mutation score can be increased superfluously by mutants that are easy to expose. In addition, it is infeasible to examine all the mutants generated by a large set of mutation operators. Existing approaches have therefore focused on determining the sufficient set of mutation operators and the set of equivalent mutants. Instead, this paper proposes a novel Bayesian approach that prioritizes operators whose mutants are likely to remain unexposed by the existing test suites. Probabilistic sampling methods are adapted to iteratively examine a subset of the available mutants and direct focus towards the more informative operators. Experimental results show that the proposed approach identifies more than 90% of the important operators by examining ? 20% of the available mutants, and causes a 6% increase in the importance measure of the selected mutants.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635074,no
268,Prioritizing State-Based Aspect Tests,"In aspect-oriented programming, aspects are essentially incremental modifications to their base classes. Therefore aspect-oriented programs can be tested in an incremental fashion - we can first test the base classes and then test the base classes and aspects as a whole. This paper demonstrates that, in this incremental testing paradigm, we can prioritize aspect tests so as to report failure earlier. We explore test prioritization for testing aspect-oriented programs against their state models with transition coverage and round-trip coverage. Aspect tests are generated from woven state models obtained by composing aspect models into their base class models. We prioritize aspect tests by identifying the extent to which an aspect modifies its base classes. The modification is measured by the number of new and changed components in state transitions (start state, event, precondition, postcondition, end state). Transitions with more changes have higher priorities for test generation. We evaluate the impact of aspect test prioritization through mutation analysis of two AspectJ programs, where all aspects and their base classes can be modeled by finite state machines. We create aspect mutants of each AspectJ program according to a comprehensive AspectJ fault model. Then we test each mutant with the test suites generated without prioritization and with prioritization, respectively. Our experiment results show that prioritization of aspect tests has accelerated failure report.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477076,yes
269,Prioritizing Tests for Software Fault Localization,"Test prioritization techniques select test cases that maximize the confidence on the correctness of the system when the resources for quality assurance (QA) are limited. In the event of a test failing, the fault at the root of the failure has to be localized, adding an extra debugging cost that has to be taken into account as well. However, test suites that are prioritized for failure detection can reduce the amount of useful information for fault localization. This deteriorates the quality of the diagnosis provided, making the subsequent debugging phase more expensive, and defeating the purpose of the test cost minimization. In this paper we introduce a new test case prioritization approach that maximizes the improvement of the diagnostic information per test. Our approach minimizes the loss of diagnostic quality in the prioritized test suite. When considering QA cost as the combination of testing cost and debugging cost, on the Siemens set, the results of our test case prioritization approach show up to a 53% reduction of the overall QA cost, compared with the next best technique.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562943,yes
270,Prioritizing Unit Test Creation for Test-Driven Maintenance of Legacy Systems,"Test-Driven Development (TDD) is a software development practice that prescribes writing unit tests before writing implementation code. Recent studies have shown that TDD practices can significantly reduce the number of pre-release defects. However, most TDD research thus far has focused on new development. We investigate the adaptation of TDD-like practices for already implemented code, in particular legacy systems. We call this adaptation of TDD-like practices for already implemented code ``Test-Driven Maintenance'' (TDM). In this paper, we present an approach that assists software development and testing managers, who employ TDM, utilize the limited resources they have for testing legacy systems efficiently. The approach leverages the development history of the project to generate a prioritized list of functions that managers should focus their unit test writing resources on. The list is updated dynamically as the development of the legacy system progresses. To evaluate our approach, we conduct a case study on a large commercial legacy software system. Our findings suggest that heuristics based on the function size, modification frequency and bug fixing frequency should be used to prioritize the unit test writing of legacy systems.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562952,yes
271,Ranking Attacks Based on Vulnerability Analysis,"Now that multiple-known attacks can affect one software product at the same time, it is necessary to rank and prioritize those attacks in order to establish a better defense. The purpose of this paper is to provide a set of security metrics to rank attacks based on vulnerability analysis. The vulnerability information is retrieved from a vulnerability management ontology, which integrates commonly used standards like CVE, CWE, CVSS, and CAPEC. Among the benefits of ranking attacks through the method proposed here are: a more effective mitigation or prevention of attack patterns against systems, a better foundation to test software products, and a better understanding of vulnerabilities and attacks.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428663,no
272,Regression test cases prioritization using Failure Pursuit Sampling,"The necessity of lowering the execution of system tests' cost is a consensual point in the software development community. The present study presents an optimization of the regression tests' activity, by adapting a test cases prioritization technique called Failure Pursuit Sampling-previously used and validated for the prioritization of tests in general-improving its efficiency for the exclusive execution of regression test. For this purpose, the clustering and sampling phases of the original technique were modified, so that it becomes capable of receive information from tests made on the previous version of a program, and can use this information to drive de efficiency of the new developed technique, for tests made on a present version. The adapted technique was implemented and executed using the Schedule program, of the Siemens suit. By using Average of the Percentage of Faults Detected charts, the modified Failure Pursuit Sampling technique presented a high level of efficiency improvement.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687069,yes
273,Regression Test Generation Approach Based on Tree-Structured Analysis,"Regression test generation is an important process to make sure that changes of program have no unintended side-effects. To achieve full confidence, many projects have to re-run all the test cases for entire program, which makes it a time consuming and expensive activity. In this paper, a code based regression testing approach is proposed to generate selected test suites for unit testing. The framework contains five phases: program change detection phase, logical verification phase, branch pruning phase, test case prioritization phase and test suite generation phase. These five phases can achieve detection of program's modification, coding standard, test case pruning, test case prioritizing and inputs generation for regression test cases respectively. A prototype based on this framework is implemented using logical tree-structured analysis, and the preliminary experiment shows that proposed approach can provide efficient regression test suites.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476640,no
274,Requirement based test case prioritization,"Test case prioritization involves scheduling test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects faults at the earliest in its testing life cycle. In this paper, an algorithm is proposed for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software and also to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the three factors: customer priority, changes in requirement, implementation complexity. The proposed prioritization technique is validated with two different sets of industrial projects and the results show that the proposed prioritization technique improves the rate of severe fault detection.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670728,yes
275,Requirement-based test case generation and prioritization,"Software release testing is a critical phase in the software development life cycle, as it validates the software against its requirements. Designing comprehensive release test cases that are driven by the software requirements remain the major success factor of the testing phase as far as the software customers are concerned. Further, availing sufficient traceability information to ensure complete coverage of requirements validation in the designed test case suite is significant to software quality assurance. In this paper, we propose a systematic mechanism to derive a set of release test cases from a set of requirements modeled with the Genetic Software Engineering (GSE) method. GSE models functional requirements with a semi-formal visual notation called Behavior Trees (BT). Our algorithm prioritizes the requirements modeled with BTs and derives a set of prioritized release test cases systematically. Additionally, our algorithm provides sufficient traceability information relating test cases to the requirements being tested. This allows for ensuring completeness of test case coverage. We also demonstrate our test case derivation mechanism through a case study.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720443,yes
276,Risk-Based Testing: A Case Study,"This paper describes the application of risk-based testing for a software product evaluation in a real case study. Risk-based testing consists of a set of activities regarding risk factors identification related to software requirements. Once identified, the risks are prioritized according to their likelihood and impact and test cases are designed based on the strategies for treatment of the identified risk factors. Thus, test efforts are continuously adjusted according to risk monitoring. The paper also briefly reviews available risk-based approaches, describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of problems, challenges and future work.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501497,no
277,Risks of unrecognized commonalities in information technology supply chains,"In this paper we examine the interdependencies and common points of failure (and attack) that plague commonly-used system and network hardware and software. The proposed approach requires not only generating inventories of acquiring organizations' equipment and software products, and clear and detailed descriptions of every link in the supply chain, but also the identification of common components and their sources. This information is required not only for manufacturer and OEM supply chains, but also for the services supply chains of maintenance and repair organizations. When such critical components and services have been identified, one must prioritize their importance and apply appropriate security and testing. Such an identification and tracking system is only as good as its ability to incorporate up-to-the-minute changes and additions. This requires extensive real-time reporting and information sharing. The author presents a general description of a proprietary tool that facilitates the collaboration needed for such an approach to be effective.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5654970,no
278,Sequence-based techniques for black-box test case prioritization for composite service testing,"Web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable to web services. In this paper, we formulate new test case prioritization strategies using sequences in XML messages to reorder regression test cases for composite web services, against the tag based techniques given in and reveal how the test cases use the interface specifications of the composite services. The results were evaluated experimentally and the results show that the new techniques can have a high probability of outperforming random ordering and the techniques given in.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705784,yes
279,Stress Testing an AI Based Web Service: A Case Study,"The stress testing of AI-based systems differs from the approach taken for more traditional Web services, both in terms of the design of test cases and the metrics used to measure quality. The expected variability in responses of an AI-based system to the same request adds a level of complexity to stress testing, when compared to more standard systems where the system response is deterministic and any deviations may easily be characterized as product defects. Generating test cases for AI-based systems requires balancing breadth of test cases with depth of response quality: most AI-systems may not return a perfect answer. An example of a machine learning translation system is considered, and the approach used for stress testing it is presented, alongside comparisons with a more traditional approach. The challenges of shipping such a system to support a growing set of features and language pairs necessitate a mature prioritization of test cases. This approach has been successful in shipping a Web service that currently serves millions of users per day.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501500,no
280,Supporting Concern-Based Regression Testing and Prioritization in a Model-Driven Environment,"Traditional regression testing and prioritization approaches are bottom-up (or white-box). They rely on the analysis of the impact of changes in source code artifacts, identifying corresponding parts of software to retest. While effective in minimizing the amount of testing required to validate code changes, they do not leverage on specification-level design and requirements concerns that motivated these changes. Model-based testing approaches support a top-down (or black box) testing approach, where design and requirements models are used in support of test generation. They augment code-based approaches with the ability to test from a higher-level design and requirements perspective. In this paper, we present a model-based regression testing and prioritization approach that efficiently selects test cases for regression testing based on different concerns. It relies on traceability links between models, test cases and code artifacts, together with user-defined properties associated to model elements. In particular we describe how to support concern-based regression testing and prioritization using TDE/UML, an extensible model-based testing environment.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615818,yes
281,Taking Advantage of Service Selection: A Study on the Testing of Location-Based Web Services Through Test Case Prioritization,"Dynamic service compositions pose new verification and validation challenges such as uncertainty in service membership. Moreover, applying an entire test suite to loosely coupled services one after another in the same composition can be too rigid and restrictive. In this paper, we investigate the impact of service selection on service-centric testing techniques. Specifically, we propose to incorporate service selection in executing a test suite and develop a suite of metrics and test case prioritization techniques for the testing of location-aware services. A case study shows that a test case prioritization technique that incorporates service selection can outperform their traditional counterpart - the impact of service selection is noticeable on software engineering techniques in general and on test case prioritization techniques in particular. Further-more, we find that points-of-interest-aware techniques can be significantly more effective than input-guided techniques in terms of the number of invocations required to expose the first failure of a service composition.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552784,no
282,Test Case Prioritization for Web Service Regression Testing,"Regression testing is necessary to assure the quality of service-oriented business applications in their evolutions. However, because of the constraint of testing resource, entire test suite may not run as a result. Therefore, test case prioritization technique is required to increase the efficiency of Web service application regression testing. In this paper, we propose a dependence analysis based test case prioritization technique. First, we analyze the dependence relationship using control and data flow information in an orchestration language: WS-BPEL. Then we construct a weighted graph and do impact analysis to identify modification-affected elements. After that, we prioritize test cases according to covering more modification-affected elements with the highest weight. Finally we conduct a case study to illustrate the applicability of our method.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569910,yes
283,The Effectiveness of Regression Testing Techniques in Reducing the Occurrence of Residual Defects,"Regression testing is a necessary maintenance activity that can ensure high quality of the modified software system, and a great deal of research on regression testing has been performed. Most of the studies performed to date, however, have evaluated regression testing techniques under the limited context, such as a short-term assessment, which do not fully account for system evolution or industrial circumstances. One important issue associated with a system lifetime view that we have overlooked in past years is the effects of residual defects - defects that persist undetected - across several releases of a system. Depending on an organization's business goals and the type of system being built, residual defects might affect the level of success of the software products. In this paper, we conducted an empirical study to investigate whether regression testing techniques are effective in reducing the occurrence and persistence of residual defects across a system's lifetime, in particular, considering test case prioritization techniques. Our results show that heuristics can be effective in reducing both the occurrence of residual defects and their age. Our results also indicate that residual defects and their age have a strong impact on the cost-benefits of test case prioritization techniques.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477096,no
284,The Effects of Time Constraints on Test Case Prioritization: A Series of Controlled Experiments,"Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. If this is correct, a better understanding of the effects of time constraints could lead to improved prioritization techniques and improved maintenance and testing processes. We therefore conducted a series of experiments to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our first experiment manipulates time constraint levels and shows that time constraints do play a significant role in determining both the cost-effectiveness of prioritization and the relative cost-benefit trade-offs among techniques. Our second experiment replicates the first experiment, controlling for several threats to validity including numbers of faults present, and shows that the results generalize to this wider context. Our third experiment manipulates the number of faults present in programs to examine the effects of faultiness levels on prioritization and shows that faultiness level affects the relative cost-effectiveness of prioritization techniques. Taken together, these results have several implications for test engineers wishing to cost-effectively regression test their software systems. These include suggestions about when and when not to prioritize, what techniques to employ, and how differences in testing processes may relate to prioritization cost--effectiveness.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482587,yes
285,Time Windows Based Dynamic Routing in Multi-AGV Systems,"This paper presents a dynamic routing method for supervisory control of multiple automated guided vehicles (AGVs) that are traveling within a layout of a given warehouse. In dynamic routing a calculated path particularly depends on the number of currently active AGVs' missions and their priorities. In order to solve the shortest path problem dynamically, the proposed routing method uses time windows in a vector form. For each mission requested by the supervisor, predefined candidate paths are checked if they are feasible. The feasibility of a particular path is evaluated by insertion of appropriate time windows and by performing the windows overlapping tests. The use of time windows makes the algorithm apt for other scheduling and routing problems. Presented simulation results demonstrate efficiency of the proposed dynamic routing. The proposed method has been successfully implemented in the industrial environment in a form of a multiple AGV control system.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907246,no
286,"To Strengthen Security, Change Developers' Incentives","Many of the most common software vulnerabilities, such as buffer overflows, cross-site scripting, and misapplications of cryptography, are wholly avoidable if software makers apply an appropriate level of training, testing, and care.Yet developers today have the ""wrong"" incentives, often leading them to underinvest in security or even to directly harm it. If we can understand these incentives and their causes, we might be able to reshape them and radically improve security.Software makers have shown a dramatic ability to strengthen their products' security given sufficient motivation.The most famous example is Microsoft's transformation over the past decade from a security laughingstock to a leader. In 2002, stung by several widely publicized vulnerabilities across its product line, the company began a major security initiative that produced lasting changes in its priorities, processes, and culture. Gone were the days of ""creating designs and code that emphasize features over security."" Yet changes like these are exceptional. Microsoft's shift was motivated by an intense level of scrutiny and withering global publicity that few firms experience, and it had the unusual luxury of responding with vast engineering resources paid for by monopoly rents. Most developers face far weaker security incentives.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439535,no
287,Using Coverage Information to Guide Test Case Selection in Adaptive Random Testing,"Random Testing (RT) is a fundamental software testing technique. Adaptive Random Testing (ART) improves the fault-detection capability of RT by employing the location information of previously executed test cases. Compared with RT, test cases generated in ART are more evenly spread across the input domain. ART has conventionally been applied to programs that have only numerical input types, because the distance between numerical inputs is readily measurable. The vast majority of computer programs, however, involve non-numerical inputs. To apply ART to these programs requires the development of effective new distance measures. Different from those measures that focus on the concrete values of program inputs, in this paper we propose a method to measure the distance using coverage information. The proposed method enables ART to be applied to all kinds of programs regardless of their input types. Empirical studies are further conducted for the branch coverage Manhattan distance measure using the replace and space programs. Experimental results show that, compared with RT, the proposed method significantly reduces the number of test cases required to detect the first failure. This method can be directly applied to prioritize regression test cases, and can also be incorporated into code-based and model-based test case generation tools.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615787,no
288,Using Methods & Measures from Network Analysis for GUI Testing,"Graphical user interfaces (GUIs) for today's applications are extremely large. Moreover, they provide many degrees of freedom to the end-user, thus allowing the user to perform a very large number of event sequences on the GUI. The large sizes and degrees of freedom create severe problems for GUI quality assurance, including GUI testing. In this paper, we leverage methods and measures from network analysis to analyze and study GUIs, with the goal of aiding GUI testing activities. We apply these methods and measures on the event-flow graph model of GUIs. Results of a case study show that ""network centrality measures"" are able to identify the most important events in the GUI as well as the most important sequences of events. These events and sequences are good candidates for test prioritization. In addition, the ""betweenness clustering"" method is able to partition the GUI into regions that can be tested separately.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463654,no
289,Web services regression test case prioritization,"Web services and their underlying system grow over time and need to be retested whenever there is a change. This is essential for ensuring uncompromised quality. If we have modified only a small part of the system, it should be possible to reuse the existing test suite. Anyhow, for large modifications or for large systems, retesting the entire test suite will consume large amounts of time and computing resources. In this paper we propose a new method to prioritize test cases in web applications. Our test prioritization technique orders test cases in such a way that the most beneficial is executed first. Most of the existing test prioritization methods are based on the code of the system, but we propose a model-based test prioritization using activity diagram. Our technique identifies difference between original model and modified model. Using this information we plot activity paths for each test case and identify the most promising paths. The test case which covers these paths is considered as the most beneficial test cases. Our approach is effective in revealing the most promising regression test cases. We have applied our method on an online air ticket reservation system in which we could identify the most beneficial test cases from the existing ones.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5643499,yes
290,A clustering approach to improving test case prioritization: An industrial case study,"Regression testing is an important activity for controlling the quality of a software product, but it accounts for a large proportion of the costs of software. We believe that an understanding of the underlying relationships in data about software systems, including data correlations and patterns, could provide information that would help improve regression testing techniques. We conjecture that if test cases have common properties, then test cases within the same group may have similar fault detection ability. As an initial approach to investigating the relationships in massive data in software repositories, in this paper, we consider a clustering approach to help improve test case prioritization. We implemented new prioritization techniques that incorporate a clustering approach and utilize code coverage, code complexity, and history data on real faults. To assess our approach, we have designed and conducted empirical studies using an industrial software product, Microsoft Dynamics Ax, which contains real faults. Our results show that test case prioritization that utilizes a clustering approach can improve the effectiveness of test case prioritization techniques.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080805,yes
291,A Diagnostic Point of View for the Optimization of Preparation Costs in Runtime Testing,"Runtime testing is emerging as the solution for the validation and acceptance testing of service-oriented systems, where many services are external to the organization, and duplicating the system's components and their context is too complex, if possible at all. In order to perform runtime tests, an additional expense in the test preparation phase is required, both in software development and in hardware. Preparation cost prioritization methods have been based on runtime testability (i.e, coverage) and do not consider whether a good runtime testability is sufficient for a good runtime diagnosis quality in case faults are detected, and whether this diagnosis will be obtained efficiently (i.e., with a low number of test cases). In this paper we show (1) the direct relationship between testability and diagnosis quality, that (2) these two properties do not guarantee an efficient diagnosis, and (3) a measurement that ensures better prediction of efficiency.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954478,no
292,A Fuzzy Logic Approach for Scheduling Preventive Maintenance in ERP System,"Many firms have proceeded to the adoption of Enterprise Resources Planning ERP solutions to maintain competitiveness. ERP is a packaged software system that enables enterprises to integrate operations, business processes and functions through common database. However, the majority of ERP systems do not support Preventive Maintenance (PM) scheduling process. The objective of PM is to minimize equipment downtime using the limited resources of an organization. Therefore, prioritizing PM activities for equipment is essential. In this paper, a fuzzy logic-based system for PM scheduling is proposed to interpret the linguistic variables extracted from expert's knowledge for determining equipment priorities, which could be incorporated as a custom module in ERP systems. The system was tested and proved to be reliable in solving PM scheduling problem.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999330,no
293,A general purpose Ethernet based readout data acquisition system,"A flexible dedicated readout system is one of the most important part of any kind of dedicated detection system, especially for its testing phase as well as when the final system is ready for implementation. An obvious choice is to use a FPGA (apart from dedicated front-end electronics) as a first stage of data storage and processing element. Furthermore the FPGA has to prepare and transfer the incoming/processed data to the host PC. The implementation of the data exchange can be a problem, especially for small groups of developers, who have an option to buy a general solution with its limitations and a price, or to do time-consuming development of their own system practically from scratch. This paper presents a FPGA based general purpose readout solution which lies in between the two opposite approaches. Presented system uses a FPGA mezzanine board equipped with Ethernet Gigabit connection to PC. The FPGA FIFO based readout of a digital data stream is packed directly into the Ethernet frames and send to the destination PC using point-to point connection. The standard Ethernet frames are used in this design, additionally equipped with one byte carrying information on data type. When a high throughput is needed the data type is employed to prioritize them. This moderately simple but very powerful interface is relatively easy to be implemented in many applications [1]. The custom approach chosen for FPGA implementation causes a need to prepare dedicated software suite to process all incoming data in the PC side. The developed software package is called EPPRO (Ethernet Packet PROxy) since it exploits special Ether net frames for data exchange. The core part of EPPRO is a Linux kernel module, responsible for data reception/transmission and dispatching, taking into account their types to filter and prioritize the incoming packets. Overall performance of the whole system has been evaluated in respect to its throughput and reliability, presented test results confirm that all of the design goals have been fulfilled.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154542,no
294,A genetic algorithm based approach for prioritization of test case scenarios in static testing,"White box testing is a test technique that takes into account program code, code structure and internal design flow. White box testing is primarily of two kinds-static and structural. Whereas static testing requires only the source code of the product, not the binaries or executables, in structural testing tests are actually run by the computer on built products. In this paper, we propose a technique for optimizing static testing efficiency by identifying the critical path clusters using genetic algorithm. The testing efficiency is optimized by applying the genetic algorithm on the test data. The test case scenarios are derived from the source code. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of the control flow graph generated from the source code. This research paper is an extension of our previous research paper [18].",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075160,yes
295,"A low-cost distributed instrumentation system for monitoring, identifying and diagnosing irregular patterns of behavior in critical ITS components","ITS telecommunication infrastructure and information gathering/distribution equipment such as fiber-optic routing devices, communication huts, uninterrupted power supplies (UPSs), cameras and dynamic message signs (DMSs) are critical to the effective management of resources in emergency situations. Frequently encountered scenarios include evaluating the severity of vehicle collisions to dispatch appropriate law enforcement and ambulatory services, or issuing time-sensitive AMBER alerts to assist in the effort to find missing or kidnapped children. Due to the unacceptably high cost of ITS equipment failures, preventative maintenance and dense operational testing are high priorities. In this paper we present a low-cost distributed instrumentation system (DIS) for continuous monitoring of critical ITS components. Over the last three years, we have developed and deployed the DIS in the Oklahoma Department of Transportation (ODOT) private fiberoptic network that spans several major metropolitan areas in and around Oklahoma City, Tulsa and Lawton. The Oklahoma DIS is responsible for monitoring the health of the entire ODOT ITS communications network as well as the integrity of each camera video signal and the operational status of each DMS. All of the information acquired by the DIS is integrated into an operational summary that is available on a private website for the design and execution of ITS equipment maintenance plans. In Oklahoma, information acquired by the DIS has been successfully integrated into a wide range of operation and maintenance (O&M) planning, which has led to a significant improvement in terms of overall ITS quality of service (QoS) and a quantifiable reduction in wasted costs associated with the premature discarding of energy storage devices.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082868,no
296,A model based prioritization technique for component based software retesting using uml state chart diagram,"Regression testing is the process of testing a modified system using the old test suite. As the test suite size is large, system retesting consumes large amount of time and computing resources. This issue of retesting of software systems can be handled using a good test case prioritization technique. A prioritization technique schedules the test cases for execution so that the test cases with higher priority executed before lower priority. The objective of test case prioritization is to detect fault as early as possible so that the debuggers can begin their work earlier. In this paper we propose a new prioritization technique to prioritize the test cases to perform regression testing for Component Based Software System (CBSS). The components and the state changes for a component based software systems are being represented by UML state chart diagrams which are then converted into Component Interaction Graph (CIG) to describe the interrelation among components. Our prioritization algorithm takes this CIG as input along with the old test cases and generates a prioritized test suit taking into account total number of state changes and total number of database access, both direct and indirect, encountered due to each test case. Our algorithm is found to be very effective in maximizing the objective function and minimizing the cost of system retesting when applied to few JAVA projects.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941719,yes
297,A software safety test approach based on FTA and Bayesian networks,"As an important way to verify software safety, software safety test has caught more attentions in practice. However, it is still an open question that how engineers could make software safety test more efficient. Currently, FTA based method is one of the approaches in software safety test, but it can not utilize the finished software test results, and can not be determined the priorities of all the use cases. In order to solve these problems, this paper gives a quantitative approach of software safety test based on FTA and Bayesian networks. In the approach, top-level events of fault trees are identified from system hazards firstly. Then, fault trees are built using FTA and transferred into Bayesian networks. Finally, test cases of software safety test are determined by the Bayesian networks. Besides, the paper also shows an example using the approach, which could guide software engineers to make software safety test more efficient. The example shows that the approach could take advantage of Bayesian Theorem and FTA methodology together, and give reasonable priorities of use cases in software safety test.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939497,no
298,A statistical approach to TPS transport optimization,"This paper discusses the statistical challenges of TPS Transport. A TPS is not considered fieldable until all tests are passing. Based on the number of tests in a TPS and the inherent complexity of the transport process, the probability exists that less than 100% of all tests will pass the first time they are tested after undergoing the transport process. Optimizing what types of tests are transported significantly improves the probability that the next TPS will be successfully transported. This paper illustrates that prioritizing which functions to focus on can greatly improve the probability of success while reducing the overall characterization effort.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058752,no
299,A Test Case Design Algorithm Based on Priority Techniques,"Testing is an important step of building e-commerce system. In regression testing, it is the key issue that how to reuse the test suite efficiently. This paper presents a dynamic adjustment prioritization based on the design information of test suite which is a new exploration of regression test prioritization. It improves the shortcoming of the existing technologies which failed to use the design information of test cases effectively. It adjusts the priority of test case by collecting running information, gradually optimizes the test suite and makes the test suite adapted to the current test environment to obtain a better error detection results. Experiments show the error-detected efficiency of it has certain advantages than the existing algorithms.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092632,no
300,A tool for combination-based prioritization and reduction of user-session-based test suites,"Test suite prioritization and reduction are two approaches to managing large test suites. They play an important role in regression testing, where a large number of tests accumulate over time from previous versions of the system. Accumulation of tests is exacerbated in user-session-based testing of web applications, where field usage data is continually logged and converted into test cases. This paper presents a tool that allows testers to easily collect, prioritize, and reduce user-session-based test cases. Our tool provides four contributions: (1) guidance to users on how to configure their web server to log important usage information, (2) automated parsing of web logs into XML formatted test cases that can be used by test replay tools, (3) automated prioritization of test cases by length-based and combinatorial-based criteria, and (4) automated reduction of test cases by combinatorial coverage.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080833,yes
301,A Workflow Scheduling Algorithm for Optimizing Energy-Efficient Grid Resources Usage,"Grid computing represents the main solution to integrate distributed and heterogeneous resources in global scale. However, the infrastructure necessary for maintaining a global grid in production is huge. Such fact has led to excessive power consumption. On the other hand, most green strategies for data centers are DVS (Dynamic Voltage Scaling)-based and become difficult to implement them in global grids. This paper proposes the HGreen heuristic (Heavier Tasks on Maximum Green Resource) and defines a workflow scheduling algorithm in order to implement it on global grids. HGreen algorithm aims to prioritize energy-efficient resources and explores workflow application profiles. Simulation results have shown that the proposed algorithm can significantly reduce the power consumption in global grids.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6119053,no
302,Adaptive Regression Testing Strategy: An Empirical Study,"When software systems evolve, different amounts and types of code modifications can be involved in different versions. These factors can affect the costs and benefits of regression testing techniques in different ways, and thus, there may be no single regression testing technique that is the most cost-effective technique to use on every version. To date, many regression testing techniques have been proposed, but no research has been done on the problem of helping practitioners systematically choose appropriate techniques on new versions as systems evolve. To address this problem, we propose adaptive regression testing (ART) strategies that attempt to identify the regression testing techniques that will be the most cost-effective for each regression testing session considering organization's situations and testing environment. To assess our approach, we conducted an experiment focusing on test case prioritization techniques. Our results show that prioritization techniques selected by our approach can be more cost-effective than those used by the control approaches.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132961,no
303,Agent-Based Test Case Prioritization,In this paper an Adaptive Test Management System (ATMS) based on software agents is presented which prioritizes test cases considering available information from test teams and from developments teams about the software system and the test cases. The goal of the ATMS is to increase the number of found faults in the available test time with the determined prioritization order.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954404,yes
304,An Empirical Study on the Relation between Dependency Neighborhoods and Failures,"Changing source code in large software systems is complex and requires a good understanding of dependencies between software components. Modification to components with little regard to dependencies may have an adverse impact on the quality of the latter, i.e., increase their risk to fail. We conduct an empirical study to understand the relationship between the quality of components and the characteristics of their dependencies such as their frequency of change, their complexity, number of past failures and the like. Our study has been conducted on two large software systems: Microsoft VISTA and ECLIPSE. Our results show that components that have outgoing dependencies to components with higher object-oriented complexity tend to have fewer field failures for VISTA, but the opposite relation holds for ECLIPSE. Likewise, other notable observations have been made through our study that (a) confirm that certain characteristics of components increase the risk of their dependencies to fail and (b) some of the characteristics are project specific while some were also found to be common. We expect that such results can be leveraged for use to provide new directions for research in defect prediction, test prioritization and related research fields that utilize code dependencies in their empirical analysis. Additionally, these results provide insights to engineers on the potential reliability impacts of new component dependencies based upon the characteristics of the component.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770624,no
305,An empirical validation of FindBugs issues related to defects,"Background: Effective use of bug finding tools promise to speed up the process of source code verification and to move a portion of discovered defects from testing to coding phase. However, many problems related to their usage, especially the large number of false positives, could easily hinder the potential benefits of such tools. Aims: Assess the percentage and type of issues of a popular bugfinding tool (FindBugs) that are actual defects. Method: We analyzed 301 Java Projects developed at a university with FindBugs, collecting the issues signalled on the source code. Afterwards, we checked the precision of issues with information on changes, we ranked and validated them using both manual inspection and validation with tests failures. Results: We observed that a limited set of issues have high precision and conversely we identified those issues characterized by low precision. We compared findings first with our previous experiment and then to related work: results are consistent with both of them. Conclusions: Since our and other empirical studies demonstrated that few issues are related to real defects with high precision, developers could enable only them (or prioritize), reducing the information overload of FindBugs and having the possibility to discover defects earlier. Furthermore, the technique presented in the paper can be adopted to other tools on a code base with tests to find issues with high precision that can be checked on code in production to find defects earlier.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083173,no
306,An Improved Metric for Test Case Prioritization,"Test case prioritization is an effective and practical technique of regression testing. To illustrate its effectiveness, many test metrics were proposed. In this paper, the physical meanings of these metrics were explained and their limitations were pointed out. Then, an improved metric and its extension for test case prioritization were proposed. The case study indicates that, compared with existing metrics, our new metric can provide much more precise illustration of the effectiveness of test case prioritization techniques.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093578,yes
307,Black box test case prioritization techniques for semantic based composite web services using OWL-S,"Web services are the basic building blocks for the business which is different from web applications. Testing of web services is difficult and increases the cost due to the unavailability of source code. Researchers have, web services are tested based on the syntactic structure using Web Service Description Language (WSDL) for atomic web services. This paper proposes an automated testing framework for composite web services based on semantics where the domain knowledge of the web services is described using prot??g?? tool and the behaviour of the entire business operation flow for the composite web service is described by Ontology Web Language for services (OWL-S). Prioritization of test cases is performed based on various coverage criteria for composite web services. Series of experiments were conducted to assess the effectiveness of prioritization and empirical results shown that prioritization techniques perform well in detecting faults compared to traditional techniques.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972354,yes
308,Calculating Prioritized Interaction Test Sets with Constraints Using Binary Decision Diagrams,"Combinatorial interaction testing has become an established technique to systematically determine test sets for highly-configurable software systems. The generation of minimal test sets that fullfill the demanded coverage criteria is an NP-complete problem. Constraint handling and integrated test case prioritization, features necessary for practical use, further complicate the problem. We present a novel algorithm that exploits our observation that the combinatorial interaction testing problem with constraints can be modelled as a single propositional logic formula. Our test set calculation algorithm uses binary decision diagrams as efficient data structure for this formula. The algorithm supports constraints and prioritization. Our evaluation results prove its cost effectiveness. For many benchmark problems the algorithm calculates the best results compared to other greedy approaches.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954420,yes
309,Challenges in Audit Testing of Web Services,"Audit testing of Web services is a form of regression testing that applies to the integration of external services in a service composition. When such services change, the application that integrates them needs to be regressiontested, to ensure that the existing functionalities are not broken by the service changes. This means the external services must be audited for preserved compatibility with the integration. In this paper, we analyze the main research challenges behind audit testing of services. Such challenges descend from the limited testability of external services and encompass traditional regression testing problems, such as test case selection and prioritization, for which existing solutions are nonetheless inadequate. Moreover, in the paper we discuss in detail one such challenge, i.e., the problem of defining an oracle by making the integrator's assumptions explicit.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954397,no
310,"Challenges, benefits and opportunities in operating cabled ocean observatories: Perspectives from NEPTUNE Canada","The advent of the first cabled ocean observatories, with several others being planned, demonstrates the challenges, benefits and opportunities for ocean science and commercial applications. Examples are drawn primarily from NEPTUNE Canada (NC), which completed installation of the subsea infrastructure and 60 diverse instruments in 2009, with 40 more in 2010, thereby establishing the world's first regional cabled ocean observatory, northeast Pacific Ocean, off British Columbia's coast. Initial data flow started in December 2009. Another 30 instruments will be deployed in 2011-12. Introducing abundant power and high bandwidth communications into a range of ocean environments allows discrimination between short and long-term events, interactive experiments, real time data and imagery, and complex multidisciplinary teams interrogating a vast database over the observatory's 25-year design life. Scientific priorities and observatory node sites were identified through workshops. Alcatel-Lucent Submarine Networks designed, manufactured and installed the 800km backbone cable and five nodes (stepping 10kV DC to 400V DC). Node sites are located at the coast (Folger Passage), continental slope (ODP 889; Barkley Canyon), abyssal plain (ODP 1027), and ocean-spreading ridge (Endeavour), in water depths of 100-2660m. Principal scientific themes are: plate tectonic processes and earthquake dynamics; dynamic processes of seabed fluid fluxes and gas hydrates; regional ocean/climate dynamics and effects on marine biota; deep-sea ecosystem dynamics; and engineering and computational research. The Data Management and Archive System (DMAS) provides controls for the observatory network and transparent access to other data providers using interoperability techniques within a Web 2.0 environment. Users can perform data visualization and analysis on-line with either default or custom processing code, as well as simultaneously interacting with each other. Oceans 2.0 is adding tools to perform software-aided feature detection and classification of sounds in acoustic data streams. New knowledge and scientific interpretations are addressing important science applications of the observatory: ocean/climate change, ocean acidification, recognizing and mitigating natural hazards, non-renewable and renewable natural resources. Challenges are considerable: technical innovations, enlarging the user base, management, funding, maximizing educational/outreach activities. Socio-economic benefits are substantial: not only the transformation of ocean sciences but with many applications in sectors such as sovereignty, security, transportation, data services, and public policy. Opportunities for commercialization of technologies and data services/products are being facilitated by the Centre of Enterprise and Engagement (www.onccee.ca) within Ocean Networks Canada (www.networkscanada.ca) that manages the NC and VENUS observatories (www.neptunecanada.ca; www.uvic.venus.ca). Cabled ocean observatories are transforming the ocean sciences and will result in a progressive wiring of the oceans. They are designed to be expandable in footprint, nodes and instruments, and the range of scientific questions, and to provide facilities for testing technology prototypes. They will provide a wealth of new research opportunities and socio-economic benefits.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5774134,no
311,Change Sensitivity Based Prioritization for Audit Testing of Webservice Compositions,"Modern software systems have often the form of Web service compositions. They take advantage of the availability of a variety of external Web services to provide rich and complex functionalities, obtained as the integration of external services. However, Web services change at a fast pace and while syntactic changes are easily detected as interface incompatibilities, other more subtle changes are harder to detect and may give raise to faults. They occur when the interface is compatible with the composition, but the semantics of the service response has changed. This typically involves undocumented or implicit aspects of the service interface. Audit testing of services is the process by which the service integrator makes sure that the service composition continues to work properly with the new versions of the integrated services. Audit testing of services is conducted under strict (sometimes extreme) time and budget constraints. Hence, prioritizing the audit test cases so as to execute the most important ones first becomes of fundamental importance. We propose a test case prioritization method specifically tailored for audit testing of services. Our method is based on the idea that the most important test cases are those that have the highest sensitivity to changes injected into the service responses (mutations). In particular, we consider only changes that do not violate the explicit contract with the service (i.e., the WSDL), but may violate the implicit assumptions made by the service integrator.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954434,no
312,Code Hot Spot: A tool for extraction and analysis of code change history,"Commercial software development teams have limited time available to focus on improvements to their software. These teams need a way to quickly identify areas of the source code that would benefit from improvement, as well as quantifiable data to defend the selected improvements to management. Past research has shown that mining configuration management systems for change information can be useful in determining faulty areas of the code. We present a tool named Code Hot Spot, which mines change records out of Microsoft's TFS configuration management system and creates a report of hot spots. Hot spots are contiguous areas of the code that have higher values of metrics that are indicators of faulty code. We present a study where we use this tool to study projects at ABB to determine areas that need improvement. The resulting data have been used to prioritize areas for additional code reviews and unit testing, as well as identifying change prone areas in need of refactoring.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080806,no
313,Compiling SyncCharts to Synchronous C,"SyncCharts are a synchronous Statechart variant to model reactive systems with a precise and deterministic semantics. The simulation and software synthesis for SyncCharts usually involve the compilation into Esterel, which is then further compiled into C code. This can produce efficient code, but has two principal drawbacks: 1) the arbitrary control flow that can be expressed with SyncChart transitions cannot be mapped directly to Esterel, and 2) it is very difficult to map the resulting C code back to the original SyncChart, which hampers traceability. This paper presents an alternative software synthesis approach for SyncCharts that compiles SyncCharts directly into Synchronous C (SC). The compilation preserves the structure of the original SyncChart, which is advantageous for validation and possibly certification. We present a static thread-scheduling scheme that reflects data dependencies and optimizes both the number of used threads as well as the maximal used priorities. This results in SC code with competitive speed and little memory requirements.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763284,no
314,Cost Optimizations in Runtime Testing and Diagnosis of Systems of Systems,"In practically all development processes tests are used to detect the presence of faults. This is not an exception for critical and high-availability systems. However, these systems cannot be taken offline or duplicated for testing in some cases. This makes runtime testing necessary. This paper presents work aimed at optimizing the three main sources of testing cost: preparation, execution and diagnosis. First, preparation cost is optimized by defining a metric of the runtime testability of the system, used to elaborate an implementation plan of preparative work for runtime testing. Second, the interrelated nature of test execution cost and diagnostic cost is highlighted and a new diagnostic test prioritization is introduced.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770638,no
315,"CRANE: Failure Prediction, Change Analysis and Test Prioritization in Practice -- Experiences from Windows","Building large software systems is difficult. Maintaining large systems is equally hard. Making post-release changes requires not only thorough understanding of the architecture of a software component about to be changed but also its dependencies and interactions with other components in the system. Testing such changes in reasonable time and at a reasonable cost is a difficult problem as infinitely many test cases can be executed for any modification. It is important to obtain a risk assessment of impact of such post-release change fixes. Further, testing of such changes is complicated by the fact that they are applicable to hundreds of millions of users, even the smallest mistakes can translate to a very costly failure and re-work. There has been significant amount of research in the software engineering community on failure prediction, change analysis and test prioritization. Unfortunately, there is little evidence on the use of these techniques in day-to-day software development in industry. In this paper, we present our experiences with CRANE: a failure prediction, change risk analysis and test prioritization system at Microsoft Corporation that leverages existing research for the development and maintenance of Windows Vista. We describe the design of CRANE, validation of its useful-ness and effectiveness in practice and our learnings to help enable other organizations to implement similar tools and practices in their environment.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770625,yes
316,Critical component analyzer ?? A novel test prioritization framework for component based real time systems,"Component based software development system is composed of many components and it uses the reusable components as the building blocks for constructing the complex software system. The major challenges in CBS are testing component dependency that is; it is a tricky task to test each and every component for each possible input data which will lead to exhaustive testing. To reduce the cost, the industries are following some stopping criteria and release the product to the customer side. These stopping criteria will at times lead to skipping up of some of the components from rigorous testing. This will lead to hazardous side effects such as loss in terms of revenue, human life and resources. This insight leads to the need to identify critical components which have the higher dependability measure in terms of functionality and receives higher priority in testing with rigorous test procedures. Hence, this paper proposes a novel method for identifying the critical components from the Software under Test (SUT) and prioritizes them for testing with at most care based on various dependency metrics and measures among the components with the help of Component Execution Sequence Graph (CESG).",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140684,yes
317,Critical components identification and verification for effective software test prioritization,"Nowadays, software complexity increases as the number of components in a component based system (CBS) increases. As the complexity level increases, the testing and verification of components also increases. This in turn rose up the testing time and cost which thus made industries to skip off some of the components due to the hard timeline and resource limitations especially during maintenance. This leads to hazardous effects if some of these missed components are critical in term of their core functionality and dependability with other components. Hence, a regression testing which is usually performed during maintenance phase should be developed meticulously to identify and test these critical components rigorously before releasing the software on to the customer side. This paper proposed a novel regression testing method based on the criticality measure calculated by means of dependability metrics and internal complexity metrics. Also, this paper compares the performance of the proposed approach with existing approaches and concluded that the proposed framework outperforms them.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165171,yes
318,Dealing with Test Automation Debt at Microsoft,"At Microsoft, substantial time and resources are expended in test case development, execution and verification. Thousands of new tests are added to existing test suites without any kind of review regarding their unique contribution to test suite effectiveness or impact on test suite efficiency. This talk describes how we leverage existing code coverage data, together with reduction and prioritization techniques, to help each test team analyze its test suite and guide them in improving their suite's effectiveness and efficiency. The analysis focuses on identifying and deprecating/prioritizing groups of tests cases, given specific tactical goals for example, increasing current test suite stability and reliability, better structuring of test suite migration efforts, reducing test suite execution time and testing with limited hardware resources.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032226,no
319,Design of mice maze based on PLC control unit,"In recent years, there are a number of mouse maze devices developed. In this paper, we introduce the Y-maze which is based on the traditional programmable logic controller as a control center to the new Y-maze. In the subject, the maze of automatic control system will be in addition to the sensor controller sub- section also part and actuator part. This paper mainly discusses the new Maze automatic control system, describes the software design process and priorities, hardware selection and layout. Among them, the interface circuit of the programmable controller, I/O port assignments, infrared sensor module and the maze rotation module are described in detail The maze of automatic control system to achieve the experimental data in mice can reduce errors, improve test efficiency and reduce human labor, side by side, in addition to the traditional Y-maze caused by the interference of some unavoidable factors, such as mice, leaving the smell of mice in the experimental results, the reliability of the experiment can be further improved.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6067621,no
320,Design Principles for Integration of Model-Driven Quality Assurance Tools,"The engineering of software systems is supported by tools in different phases of the software development. The integration of these tools is crucial to assure the trace ability of existing models and artifacts, and to support the automation of critical software development phases such as software testing and validation. In particular, the integration of novel software quality assurance tools into existing environments must be performed in a way that minimizes its impact on existing software process, while the benefits of the tool are leveraged. This guarantees the adoption of new methodologies with minimal interference in existing production workflow. In this paper we discuss our experience in integrating a model-driven software testing tool developed within SIEMENS with a widely-adopted model-driven design tool. In particular, we establish a set of design principles from the lessons learned in this integration. We conclude showing a design that prioritizes data integration over control and presentation that achieves a high degree of tool integration while minimizing the integration development effort.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114546,no
321,Designing VM schedulers for embedded real-time applications,"Virtual Machines (VMs) allow for platform-independent software development and their use in embedded systems is increasing. In particular, VMs are rewarding in the context of mixed-criticality applications to provide isolation between critical and non-critical tasks running on the same processor. In this paper, we study the design of a real-time system based on a VM monitor/hypervisor that supports multiple VMs/domains. Since each VM in the system runs several real-time tasks, scheduling the VMs leads to a hierarchical scheduling problem. So far, most published techniques for analyzing hierarchical scheduling deal with the schedulabil-ity problem, i.e., for a given hierarchical scheduler, testing whether a set of real-time tasks meet their deadlines. In this paper, we are rather concerned with the synthesis of hier-archical/VM schedulers; that is, how to design a scheduler such that all real-time tasks running on the different VMs meet their deadlines. We consider a setup where the tasks are scheduled on multiple VMs under fixed priorities according to the Deadline Monotonic (DM) policy. The VMs are scheduled under fixed priorities on a Rate Monotonic (RM) basis using one or more processors. A partitioned scheduling of VMs is considered, i.e., VMs are not allowed to migrate from one processor to the other. In this context, we propose a method for selecting optimum time slices and periods for each VM in the system. Our goal is to configure the VM scheduler such that not only all tasks are schedulable but also the minimum possible resources are used. Finally, to illustrate the proposed design technique, we present a case study based on automotive control applications.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062302,no
322,Developing a Single Model and Test Prioritization Strategies for Event-Driven Software,"Event-Driven Software (EDS) can change state based on incoming events; common examples are GUI and Web applications. These EDSs pose a challenge to testing because there are a large number of possible event sequences that users can invoke through a user interface. While valuable contributions have been made for testing these two subclasses of EDS, such efforts have been disjoint. This work provides the first single model that is generic enough to study GUI and Web applications together. In this paper, we use the model to define generic prioritization criteria that are applicable to both GUI and Web applications. Our ultimate goal is to evolve the model and use it to develop a unified theory of how all EDS should be tested. An empirical study reveals that the GUI and Web-based applications, when recast using the new model, show similar behavior. For example, a criterion that gives priority to all pairs of event interactions did well for GUI and Web applications; another criterion that gives priority to the smallest number of parameter value settings did poorly for both. These results reinforce our belief that these two subclasses of applications should be modeled and studied together.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401169,yes
323,Dynamic performance stubs to simulate the main memory behavior of applications,"Dynamic performance stubs provide a framework to simulate the performance behavior of software modules and functions. Hence, they can be used as an extension to software performance engineering methodologies. The methodology of dynamic performance stubs targets to gain oriented performance improvement. Other applications include the identification of ""hidden"" bottlenecks and the prioritization of optimization alternatives. Main memory stubs have been developed to extend the simulation possibilities of the dynamic performance stubs framework. They are able to simulate the heap and stack behavior of software modules or functions. This paper evaluates an algorithm to generate the simulation data file, which serves as input for the main memory stubs simulation algorithm. Moreover, it presents an automatic error correction algorithm to consider the results from the calibration functions to improve the simulation results. Additionally, a proof of concept is given to depict the results of the simulation data file generation and the automatic error correction algorithm. This paper shows that, it is possible to generate the simulation data file as well as to optimize the simulation data to compensate inaccuracies in order to create main memory stubs.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984857,no
324,Dynamic Prioritization in Regression Testing,"Although used extensively in industry, regression testing is challenging from both a process management as well as a resource management perspective. In literature, proposed test case prioritization techniques assume a constant pool of test cases with non-changing coverage during the regression testing process, and therefore they work with a fixed, prioritized test suite. However, in practice, test cases and their coverage metrics may change during regression testing due to modifications of software artefacts (e.g. due to bug fixing). For example, modifying obsolete test cases or source code may change the coverage metrics during the process. This may lead to some changes in test case priorities. Dealing with manual tests cases, scheduling test case execution in shared environments and other constraints in practice may cause the same effect. In this paper, we highlight these challenges in industrial regression testing and propose a paradigm called Dynamic Prioritization, which uses in-process events and the most up-to-date test suite to re-order test cases.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954402,yes
325,"Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities","Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560680,no
326,Fast Start-up for Spartan-6 FPGAs using Dynamic Partial Reconfiguration,"This paper introduces the first available tool flow for Dynamic Partial Reconfiguration on the Spartan-6 family. In addition, the paper proposes a new configuration method called Fast Start-up targeting modern FPGA architectures, where the FPGA is configured in two-steps, instead of using a single (monolithic) full device configuration. In this novel approach, only the timing-critical modules are loaded at power-up using the first high-priority bitstream, while the non-timing critical modules are loaded afterwards. This two-step or prioritized FPGA start-up is used in order to meet the extremely tight startup timing specifications found in many modern applications, like PCI-express or automotive applications. Finally, the developed tool flow and methods for Fast Start-up have been used and tested to implement a CAN-based automotive ECU on a Spartan-6 evaluation board (i.e., SP605). By using this novel approach, it was possible to decrease the initial bitstream size and hence, achieve a configuration time speed-up of up to 4.5??, when compared to a standard configuration solution.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763244,no
327,GeTeX: A Tool for Testing Real-Time Embedded Systems Using CAN Applications,"Real-Time Embedded Systems (RTES) have an increasing role in controlling the IT that we use on a day-to-day basis. The behaviour of an RTES is not based solely on the interactions it might have with its surrounding environment, but also on the timing requirements it induces. As a result, ensuring that an RTES behaves correctly is non-trivial, especially after adding time as a new dimension to the complexity of the testing process. In previous research, we introduced a 'priority-based' approach which tested the logical and timing behaviour of an RTES modeled formally as UPPAAL Timed Automata (UTA). The 'priority-based' approach was based on producing sets of timed test traces by achieving timing constraints coverage according to three sets of priorities, namely boundary, out-boundary and in-boundary. In this paper, we introduce a new testing tool 'GeTeX' that deploys the ""priority-based"" testing approach. GeTeX is a complete testing tool which generates timed test-cases from UTA models and executes them on the System Under Test (SUT) to identify faults. In its current version, GeTeX supports Control Area Network (CAN) applications.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5934805,yes
328,Goal-Oriented Test Case Selection and Prioritization for Product Line Feature Models,"The software product line engineering paradigm is amongst the widely used means for capturing and handling the commonalities and variabilities of the many applications of a target domain. The large number of possible products and complex interactions between software product line features makes the effective testing of them a challenge. To conquer the time and space complexity involved with testing a product line, an intuitive approach is the reduction of the test space. In this paper, we propose an approach to reduce the product line test space. We introduce a goal-oriented approach for the selection of the most desirable features from the product line. Such an approach allows us to identify the features that are more important and need to be tested more comprehensively from the perspective of the domain stakeholders. The more important features and the configurations that contain them will be given priority over the less important configurations, hence providing a hybrid test case reduction and prioritization strategy for testing software product lines.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945249,yes
329,How to Shop for Free Online -- Security Analysis of Cashier-as-a-Service Based Web Stores,"Web applications increasingly integrate third-party services. The integration introduces new security challenges due to the complexity for an application to coordinate its internal states with those of the component services and the web client across the Internet. In this paper, we study the security implications of this problem to merchant websites that accept payments through third-party cashiers (e.g., PayPal, Amazon Payments and Google Checkout), which we refer to as Cashier-as-a-Service or CaaS. We found that leading merchant applications (e.g., NopCommerce and Interspire), popular online stores (e.g., Buy.com and JR.com) and a prestigious CaaS provider (Amazon Payments) all contain serious logic flaws that can be exploited to cause inconsistencies between the states of the CaaS and the merchant. As a result, a malicious shopper can purchase an item at an arbitrarily low price, shop for free after paying for one item, or even avoid payment. We reported our findings to the affected parties. They either updated their vulnerable software or continued to work on the fixes with high priorities. We further studied the complexity in finding this type of logic flaws in typical CaaS-based checkout systems, and gained a preliminary understanding of the effort that needs to be made to improve the security assurance of such systems during their development and testing processes.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958046,no
330,Hybrid regression testing technique: A multi layered approach,Software needs to be delivered well in time and within budgets. One way of doing this is performing incremental delivery of the software with each increment being adding new features along with changes requests. This incremental delivery is supported with requirement prioritization and needs to be tested for checking the reliability and quality. Testing of this increment calls for testing of not only of old newly added functionality but also of existing features so as to make sure that old parts that works perfectly well do not malfunctions after new code is added. Thus a new hybrid technique is proposed in this paper that clusters the test cases and prioritizes the clusters on basis of priorities of requirements represented by the clusters and series of selections and prioritizations at levels of test cases reduces the number of test cases to manageable level and execution of these test cases guarantees the testing of highest priority requirements associated with statements that are often associated with failures or has highest number of parents or sibling's statements dependent on it and is likely to be influenced by changes or failures in this statement.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139363,no
331,Impact Analysis of Configuration Changes for Test Case Selection,"Testing configurable systems, which are becoming prevalent, is expensive due to the large number of configurations and test cases. Existing approaches reduce this expense by selecting or prioritizing configurations. However, these approaches redundantly run the full test suite for the selected configurations. To address this redundancy, we propose a test case selection approach by analyzing the impact of configuration changes with static program slicing. Given an existing test suite T used for testing a system S under a configuration C, our approach decides for each t in T if t has to be used for testing S under a different configuration C'. We have evaluated our approach on a large industrial system within ABB with promising results.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132962,no
332,Improving independence in the community for stroke survivors: The role of biomechanics visualisation in ankle-foot orthosis tuning,"One of the key priorities for stroke survivors in their rehabilitation process is regaining their ability to walk. Evidence has shown that provision of ankle-foot orthoses (AFOs) can have a positive impact on walking. This paper discusses the role of gait analysis in the provision of AFOs for stroke survivors. A discussion of the shortcomings of gait analysis techniques is included, with a description of how these might be overcome during the AFO tuning process through the ongoing development of data visualisation software. The design of a randomised controlled trial in conjunction with a series of qualitative measures is described, which will be used to test the efficacy of the visualisation software.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038838,no
333,Improving Regression Testing Transparency and Efficiency with History-Based Prioritization -- An Industrial Case Study,"Background: History based regression testing was proposed as a basis for automating regression test selection, for the purpose of improving transparency and test efficiency, at the function test level in a large scale software development organization. Aim: The study aims at investigating the current manual regression testing process as well as adopting, implementing and evaluating the effect of the proposed method. Method: A case study was launched including: identification of important factors for prioritization and selection of test cases, implementation of the method, and a quantitative and qualitative evaluation. Results: 10 different factors, of which two are history-based, are identified as important for selection. Most of the information needed is available in the test management and error reporting systems while some is embedded in the process. Transparency is increased through a semi-automated method. Our quantitative evaluation indicates a possibility to improve efficiency, while the qualitative evaluation supports the general principles of history-based testing but suggests changes in implementation details.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770626,
334,IMS Threat and Attack Surface Analysis Using Common Vulnerability Scoring System,"For the purposes of this study, IMS specifications and public sources were analyzed using the general attack surface analysis methodology. These findings were verified and augmented by active scanning and passive analysis of the available real-world IMS test setups that were investigated during the project. As various tests and security probes were performed against the test setups, the system behaviour was analyzed for previously undetermined interactions and transient attack surfaces. After the IMS attack vectors had been identified, the Common Vulnerability Scoring System version 2 (CVSSv2) Base Scores were used to prioritize the IMS attack surface interfaces. CVSS is an industry standard for classifying vulnerabilities. It must be noted however that the idea of applying CVSS scoring to an a priori comparison of vulnerability categories and potential attack surfaces is original research by the authors of this study.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032214,no
335,Increasing test coverage using human-based approach of fault injection testing,"Fault injection testing (FIT) approach validates system's fault tolerance mechanism by actively injecting software faults into the targeted areas in the system in order to accelerate its failure rate. This highly complements other testing approaches such as requirements and regression testing implemented during the same testing phase. During testing, it is impossible to run all possible test scenarios. It is especially difficult to predict how the user might use the system functionality correctly as per design. The human interaction through the system may be varies and will leads to the functionality loophole. It is therefore important to have strategic testing approach for evaluating the dependability of computer systems especially in human errors. This paper proposed on applying Knowledge-Based, Fault Prediction Model and Test Case Prioritization approaches that can be combined to increase the test coverage. The goal of this paper is to highlight the needs and advantages of the selected approaches in performing FIT as one of effective testing techniques in the ongoing quest for increased software quality.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140685,no
336,Industrial experiences with automated regression testing of a legacy database application,"This paper presents a practical approach and tool (DART) for functional black-box regression testing of complex legacy database applications. Such applications are important to many organizations, but are often difficult to change and consequently prone to regression faults during maintenance. They also tend to be built without particular considerations for testability and can be hard to control and observe. We have therefore devised a practical solution for functional regression testing that captures the changes in database state (due to data manipulations) during the execution of a system under test. The differences in changed database states between consecutive executions of the system under test, on different system versions, can help identify potential regression faults. In order to make the regression test approach scalable for large, complex database applications, classification tree models are used to prioritize test cases. The test case prioritization can be applied to reduce test execution costs and analysis effort. We report on how DART was applied and evaluated on business critical batch jobs in a legacy database application in an industrial setting, namely the Norwegian Tax Accounting System (SOFIE) at the Norwegian Tax Department (NTD). DART has shown promising fault detection capabilities and cost-effectiveness and has contributed to identify many critical regression faults for the past eight releases of SOFIE.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080803,no
337,Lower bounds for single machine subproblems occurring in weighted tardiness oriented shifting bottleneck procedures,"In this paper, we propose lower bounds for single machine scheduling problems which occur during a run of a shifting bottleneck procedure for total weighted tardiness job shops. The specific structure of this kind of problem and its objective function in particular prevent an immediate transfer or an adaption of existing lower bounds from ??conventional? single machine problems with tardiness related objectives. Hence it has been necessary to develop bounding approaches which are to some extent conceptually new. Potential application scenarios range from exact subproblem solution methods or machine prioritization criteria in a shifting bottleneck procedure to branch-and-bound algorithms for job shops with total weighted tardiness objective. In order to provide a significant evaluation of the proposed lower bounds regarding their effectiveness and efficiency, we tested them based on problem instances which actually have been generated in a shifting bottleneck procedure applied to benchmark job shop problems.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976548,no
338,Making the Case for MORTO: Multi Objective Regression Test Optimization,This paper argues that regression test optimization problems such as selection and prioritization require multi objective optimization in order to adequately cater for real world regression testing scenarios. The paper presents several examples of costs and values that could be incorporated into such a Multi Objective Regression Test Optimization (MORTO) approach.,2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954399,no
339,Modeling the Diagnostic Efficiency of Regression Test Suites,"Diagnostic performance, measured in terms of the manual effort developers have to spend after faults are detected, is not the only important quality of a diagnosis. Efficiency, i.e., the number of tests and the rate of convergence to the final diagnosis is a very important quality of a diagnosis as well. In this paper we present an analytical model and a simulation model to predict the diagnostic efficiency of test suites when prioritized with the information gain algorithm. We show that, besides the size of the system itself, an optimal coverage density and uniform coverage distribution are needed to achieve an efficient diagnosis. Our models allow us to decide whether using IG with our current test suite will provide a good diagnostic efficiency, and enable us to define criteria for the generation or improvement of test suites.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954476,yes
340,On Practical Adequate Test Suites for Integrated Test Case Prioritization and Fault Localization,"An effective integration between testing and debugging should address how well testing and fault localization can work together productively. In this paper, we report an empirical study on the effectiveness of using adequate test suites for fault localization. We also investigate the integration of test case prioritization and statistical fault localization with a postmortem analysis approach. Our results on 16 test case prioritization techniques and four statistical fault localization techniques show that, although much advancement has been made in the last decade, test adequacy criteria are still insufficient in supporting effective fault localization. We also find that the use of branch-adequate test suites is more likely than statement-adequate test suites in the effective support of statistical fault localization.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004308,yes
341,Pragmatic prioritization of software quality assurance efforts,"A plethora of recent work leverages historical data to help practitioners better prioritize their software quality assurance efforts. However, the adoption of this prior work in practice remains low. In our work, we identify a set of challenges that need to be addressed to make previous work on quality assurance prioritization more pragmatic. We outline four guidelines that address these challenges to make prior work on software quality assurance more pragmatic: 1) Focused Granularity (i.e., small prioritization units), 2) Timely Feedback (i.e., results can be acted on in a timely fashion), 3) Estimate Effort (i.e., estimate the time it will take to complete tasks), and 4) Evaluate Generality (i.e., evaluate findings across multiple projects and multiple domains). We present two approaches, at the code and change level, that demonstrate how prior approaches can be more pragmatic.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032601,no
342,Prioritising Refactoring Using Code Bad Smells,"We investigated the relationship between six of Fowler et al.'s Code Bad Smells (Duplicated Code, Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man) and software faults. In this paper we discuss how our results can be used by software developers to prioritise refactoring. In particular we suggest that source code containing Duplicated Code is likely to be associated with more faults than source code containing the other five Code Bad Smells. As a consequence, Duplicated Code should be prioritised for refactoring. Source code containing Message Chains seems to be associated with a high number of faults in some situations. Consequently it is another Code Bad Smell which should be prioritised for refactoring. Source code containing only one of the Data Clumps, Switch Statements, Speculative Generality, or Middle Man Bad Smell is not likely to be fault-prone. As a result these Code Bad Smells could be put into a lower refactoring priority.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954447,no
343,Prioritizing interaction test suite for t-way testing,"In recent years, many new t-way interaction based strategies (where t indicates the interaction strength), particularly based on covering arrays, have been developed in the literature. In search of an optimal strategy that generates the most minimum number of tests, many of existing t-way strategies have not sufficiently dealt with test prioritization (i.e. in terms of maximizing new interaction coverage per test). Addressing this issue, this paper highlights a useful prioritization algorithm to reorganize the test cases in order to improve the rate of interaction coverage. This algorithm takes a pre-generated t-way test suite as input and automatically generates a priority ordered test suite as output. In order to demonstrate its applicability, this paper demonstrates the use of the algorithm to help prioritize the test suite generated by existing t-way strategy, MC-MIPOG.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140686,yes
344,Prioritizing Requirements-Based Regression Test Cases: A Goal-Driven Practice,"Any changes for maintenance or evolution purposes may break existing working features, or may violate the requirements established in the previous software releases. Regression testing is essential to avoid these problems, but it may be ended up with executing many time-consuming test cases. This paper tries to address prioritizing requirements-based regression test cases. To this end, system-level testing is focused on two practical issues in industrial environments: i) addressing multiple goals regarding quality, cost and effort in a project, and ii) using non-code metrics due to the lack of detailed code metrics in some situations. This paper reports a goal-driven practice at Research In Motion (RIM) towards prioritizing requirements-based test cases regarding these issues. Goal-Question-Metric (GQM) is adopted in identifying metrics for prioritization. Two sample goals are discussed to demonstrate the approach: detecting bugs earlier and maintaining testing effort. We use two releases of a prototype Web-based email client to conduct a set of experiments based on the two mentioned goals. Finally, we discuss lessons learned from applying the goal-driven approach and experiments, and we propose few directions for future research.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741354,yes
345,Prioritizing tests for fault localization through ambiguity group reduction,"In practically all development processes, regression tests are used to detect the presence of faults after a modification. If faults are detected, a fault localization algorithm can be used to reduce the manual inspection cost. However, while using test case prioritization to enhance the rate of fault detection of the test suite (e.g., statement coverage), the diagnostic information gain per test is not optimal, which results in needless inspection cost during diagnosis. We present RAPTOR, a test prioritization algorithm for fault localization, based on reducing the similarity between statement execution patterns as the testing progresses. Unlike previous diagnostic prioritization algorithms, RAPTOR does not require false negative information, and is much less complex. Experimental results from the Software Infrastructure Repository's benchmarks show that RAPTOR is the best technique under realistic conditions, with average cost reductions of 40% with respect to the next best technique, with negligible impact on fault detection capability.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100153,no
346,Ranking of technology transfer barriers in developing countries; case study of Iran's biotechnology industry,"In this paper, first we shall define a list of technology transfer barriers in the biotechnology field through interviews and literature reviews. Next, we shall categorize them into four major categories of technology, and at last introduce all of the hypotheses on the existence of relationship between each part of technology and failure in the technology transfer process. Questionnaires consisting of two parts were created and handed out to all of the specialists and biotechnology idea-holders in the Iranian National Research center of Biotechnology, Lidco Co, Iran Pastor Institute, Biotechnology Department of University of Tehran, etc. Going on, we've evaluated the factors' validity through statistical tests and the recognized barriers' priorities in each category, which were generally, identified by Analytical Hierarchy Process (AHP) via the software Expert Choice. These priorities describe the high importance of organization-ware (34.1%), information-ware (27%), technique-ware (20.7%) and eventually human-ware (18.1%) respectively.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118187,no
347,Regression testing in Software as a Service: An industrial case study,"Many organizations are moving towards a business model of Software as a Service (SaaS), where customers select and pay for services dynamically via the web. In SaaS, service providers face the challenge of delivering and maintaining high quality software solutions which must continue to work under an enormous number of scenarios; customers can easily subscribe and unsubscribe from services at any point. To date, there has been little research on unique approaches for regression test methodologies for testing in a SaaS environment. In this paper, we present an industrial case study of a regression testing approach to improve test effectiveness and efficiency in SaaS. We model service level use cases from field failures as abstract events and then generate sequences of these for testing to provide a broad coverage of the possible use cases. In subsequent releases of the system we prioritize the tests to improve time to detection of faults in the modified system. We have applied our technique to two releases of a large industrial enterprise level SaaS application and demonstrate that using our approach (1) we could have uncovered escaped faults prior to the system release in both versions of the system; (2) using a priority order we could have improved the efficiency of testing in the first version; and (3) prioritization based on failure history from the first version increases the fault detection rate in the new version, suggesting a correlation between the important sequences in versions that can be leveraged for regression testing.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080804,no
348,Research and implementation of resource management system based on Xen virtual machine,"With the development of computer technology, there is gradually abundant resource available for computer systems. Virtualization technology provides a viable solution for effective management and rational allocation of system resources. Xen virtual machine is an excellent open source virtual machine, so attracting widespread attention, with broad application prospects. However, the traditional resource management of Xen virtual machine focuses on sharing processor resources fairly, while ignoring the effect of the virtual machines with the different priorities. This would cause the practicality and the performance issues in using virtual machine. This paper proposes a resource management system model based on Xen virtual machine. The model monitors guest domain and analyzes runtime information for automating resource allocation. It mainly take into account the virtual machines with different priorities. Through a series of comparative tests, the results verify that this model can enhance practicality.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6182220,no
349,Risk-Based Testing of Safety-Critical Embedded Systems Driven by Fault Tree Analysis,"One important aspect of the quality assurance process of safety-critical embedded systems is verifying the appropriateness, correctness of the implementation and effectiveness of safety functions. Due to the rapid growth in complexity, manual verification activities are no longer feasible. This holds especially for testing. A popular method for testing such complex systems is model-based testing. Recent techniques for model-based testing do not sufficiently take into consideration the information derived from the safety analyses like Failure Mode and Effect Analysis and Fault Tree Analyses (FTA). In this paper, we describe an approach to use the results of FTA during the construction of test models, such that test cases can be derived, selected and prioritized according to the severity of the identified risks and the number of basic events that cause it. This approach is demonstrated on an example from the automation domain, namely a modular production system. We find that the method provides a significant increase in coverage of safety functions, compared to regular model based testing.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954386,no
350,"Robotic test bed for autonomous surface exploration of Titan, Mars, and other planetary bodies","Tier-scalable robotic reconnaissance missions are called for in extreme space environments, including planetary atmospheres, surfaces (both solid and liquid), and subsurfaces (e.g., oceans), as well as in potentially hazardous or inaccessible operational areas on Earth. Such future missions will require increasing degrees of operational autonomy: (1) Automatic mapping of an operational area from different vantages (i.e., spaceborne, airborne, surface, subsurface); (2) automatic sensor deployment and sensor data gathering; (3) automatic feature extraction and target/region-of-interest/anomaly identification within the mapped operational area; (4) automatic target prioritization for follow-up or close-up (in-situ) examination; and (5) subsequent automatic, targeted deployment and navigation/relocation of agents/sensors (e.g., to follow up on transient events). We report on recent progress in developing an Earth-based (outdoors) robotic test bed for Tier-scalable Reconnaissance at the University of Arizona and Caltech for distributed, science-driven, and significantly less constrained (compared to state-of-the-art) reconnaissance of prime locations on a variety of planetary bodies, with particular focus on Saturn's moon Titan with its methane/hydrocarbon lakes and Mars. The test bed currently comprises several computer-controlled robotic surface vehicles, i.e., rovers and lake landers/boats equipped with a variety of sensors. To achieve a fully operational Tier-scalable Reconnaissance test bed, aerial platforms will be integrated as a next step. The robotic surface vehicles can be interactively or automatically controlled from anywhere in the world in near real-time via the Internet. The test bed enables the implementation, field-testing, and validation of algorithms and strategies for navigation, exploration, sensor deployment, sensor data gathering, feature extraction, anomaly detection, and science goal prioritization for autonomous planetary exploration. Furthermore, it permits field-testing of novel instruments and sensor technologies, as well as testing of cooperative multi-agent scenarios and distributed scientific exploration of operational areas. As such the robotic test bed enables the development, implementation, field-testing, and validation of software packages for inter-agent communication and coordination to navigate and explore operational areas with greatly reduced reliance on (ultimately without assistance from) ground operators, thus affording the degree of mission autonomy/flexibility necessary to support future missions to Titan, Mars, and other planetary bodies, including asteroids.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747267,no
351,Scenario Driven Testing,"Software testing has traditionally focused on evaluating the functionality of implemented modules against feature specifications. This approach assumes that customer requirements and usage scenarios are accurately translated into specifications and that individual modules implemented using the feature specifications would work seamlessly and coherently to solve business problems meant to be addressed by the software under test. To ensure software built would help customers solve their business problems as intended, test teams have to go beyond traditional feature driven testing approach and test software for quality and completeness with respect to targeted customer scenarios. For this, test teams have to adopt scenario driven test methodology which involves understanding the targeted customer scenarios and use them along with feature specifications for the intended software solution to translate them into test specifications, prioritization of test work items and use them throughout project for shared understanding of tradeoffs and making decisions. In this short paper, we describe scenario driven testing and share how it was applied to test a feature-set developed for a successful product line at Microsoft??.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945250,no
352,"Security Requirements Analysis, Specification, Prioritization and Policy Development in Cyber-Physical Systems","In recent past, the security of cyber-physical systems (CPSs) has been the subject of major concern. One of the reasons is that, CPSs are often applied to mission-critical processes. Also, the automation CPSs bring in managing physical processes, and the detail of information available to them for carrying out their tasks, make securing them a prime importance. Securing CPSs is a difficult task as systems are interconnected. In order to achieve a continuous secured CPS environment, there is the need for an integrated methodology to analyze, specify and prioritize security requirements and also to develop policies to meet them. First, CPS assets are represented using high-order object models. Second, swim lane diagrams are extended to include malactivities and prevention or mitigation options to decompose use cases. We analyze security threats pertaining to the hardware components, software components and the hardware-software interaction. Security requirements are then specified, and an analytical prioritization approach, based on relative priority analysis is employed to prioritize them. Finally, security policies are then developed to meet the requirements. To demonstrate its effectiveness and evaluate its application, the proposed methodology is applied in a structured approach to a test bed - Ayushman, a Pervasive Health Monitoring System (PHMS).",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004511,yes
353,Test Case Generation and Prioritization from UML Models,"This paper proposes a novel approach to generating test cases from UML 2.0 activity diagrams and prioritizing those test cases using model information encapsulated in the activity diagrams. The test cases generated according to our approach are suitable for system level testing of the application. For prioritization of test cases, we propose a method based on coverage of all transitions in the activity diagram and usage probability of a particular flow in the activity model. We also propose an approach for selecting test data based on analysis of the branch conditions of the decision nodes in the activity diagrams.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734956,no
354,Test case prioritization for regression testing based on fault dependency,"Test case prioritization techniques involve scheduling test cases for regression testing in an order that increases their effectiveness at meeting some performance goal. This is inefficient to re execute all the test cases in regression testing following the software modifications. Using information obtained from previous test case execution, prioritization techniques order the test cases for regression testing so that most beneficial are executed first thus allows an improved effectiveness of testing. One performance goal, rate of dependency detected among faults, measures how quickly dependency among faults are detected within the regression testing process. An improved rate of fault dependency can provide faster feedback on software and let developers start debugging on the severe faults that cause other faults to appear later. This paper presents the new metric for assessing rate of fault dependency detection and an algorithm to prioritize test cases. Using the new metric the effectiveness of this prioritization is shown comparing it with non-prioritized test case. Analysis proves that prioritized test cases are more effective in detecting dependency among faults.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941954,yes
355,Test Case Prioritization Technique Based on Genetic Algorithm,"With the rapid development of information technology, software testing, as a software quality assurance, is becoming more and more important. In the software life cycle, each time the code has changed need to be regression testing. The huge test case library makes running a full test case library being challenged. To this end, we designed a genetic algorithm-based test case prioritization algorithm and improved the genetic algorithm proposed software test case prioritization algorithm.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063222,yes
356,Test Generation for X-machines with Non-terminal States and Priorities of Operations,"Testing methods aiming to demonstrate that an implementation behaves the same as a specification X-machine (extended finite-state machine) usually assume that (1) all states are terminal states and (2) there are no priorities associated with operations on transitions. The considered model for the machine is such that outputs for transitions leading to non-terminal states will be buffered and contents of buffers will only be made observable when terminal states are entered. The X-machine testing method has been extended in this work to handle such an extension of X-machines (EFSM).Priorities of operations determine the order in which guards of transitions are evaluated. This makes it possible to reduce the size of a test suite. For instance, if testing has shown that a transition with a specific guard g has been implemented from some state, then no lower-priority transition with a guard implied by g may ever be executed from that state. It is hence not necessary to test for the presence of such a lower-priority transition.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770602,no
357,Towards Impact Analysis of Test Goal Prioritization on the Efficient Execution of Automatically Generated Test Suites Based on State Machines,"Test prioritization aims at reducing test execution costs. There are several approaches to prioritize test cases based on collected data of previous test runs, e.g., in regression testing. In this paper, we present a new approach to test prioritization for efficient test execution that is focused on the artifacts used in model-based test generation from state machines. We propose heuristics for test goal prioritizations and evaluate them using two different test models. Our finding is that the prioritizations can have a positive impacton the test execution efficiency. This impact, however, is hard to predict for a concrete situation. Thus, the question for the general gain of test goal prioritizations is still open.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004322,yes
358,Usability Testing Methodology: Effectiveness of Heuristic Evaluation in E-Government Website Development,"Software development organizations consist of marketing, project management, development, design, and quality assurance team. It is important for the various teams within the organization to understand the benefits and limitation of incorporating various usability testing methods within the software development life cycle. Some of the reasons for poor usability include effort prioritization conflicts from project management, development and design team. The role of the usability engineer is to get involved as the heuristic evaluator and facilitate the development and design efforts are based on usability principles and at the same time adhering to the project time line. Two of the common usability inspection methods consist of user experience testing and expert review or more commonly known as Heuristic Evaluation (HE). This paper focuses on understanding the effectiveness of HE as a methodology for defect detection. The results show the effectiveness of the HE as a usability testing methodology in capturing defects and prioritizing development and design efforts. The results also reinforce the need for integrating traditional heuristics with modified heuristics customized to the domain or field of the project being tested such as E-Government.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5961243,no
359,Using SQL Hotspots in a Prioritization Heuristic for Detecting All Types of Web Application Vulnerabilities,"Development organizations often do not have time to perform security fortification on every file in a product before release. One way of prioritizing security efforts is to use metrics to identify core business logic that could contain vulnerabilities, such as database interaction code. Database code is a source of SQL injection vulnerabilities, but importantly may be home to unrelated vulnerabilities. The goal of this research is to improve the prioritization of security fortification efforts by investigating the ability of SQL hotspots to be used as the basis for a heuristic for prediction of all vulnerability types. We performed empirical case studies of 15 releases of two open source PHP web applications: Word Press, a blogging application, and WikkaWiki, a wiki management engine. Using statistical analysis, we show that the more SQL hotspots a file contains per line of code, the higher the probability that file will contain any type of vulnerability.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770611,no
360,Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts,"Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these ??top crashes? thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711013,no
361,A Framework to Support Research in and Encourage Industrial Adoption of Regression Testing Techniques,"When software developers make changes to a program, it is possible that they will introduce faults into previously working parts of the code. As software grows, a regression test suite is run to ensure that the old functionality still works as expected. Yet, as the number of test cases increases, it becomes more expensive to execute the test suite. Reduction and prioritization techniques enable developers to manage large and unwieldy test suites. However, practitioners and researchers do not always use and study these methods due, in part, to a lack of availability. In response to this issue, this paper describes an already released open-source framework that supports both research and practice in regression testing. The sharing of this framework will enable the replication of empirical studies in regression testing and encourage faster industrial adoption of these useful, yet rarely used, techniques.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200107,no
362,A Heuristic Model-Based Test Prioritization Method for Regression Testing,"Due to the resource and time constraints for re-executing large test suites in regression testing, developers are interested in detecting faults in the system as early as possible. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. In this paper, we present a model-based heuristic method to prioritize test cases for regression testing, which takes into account two types of information collected during execution of the modified model on the test suite. The experiment shows that our algorithm has better effectiveness of early fault detection.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228450,yes
363,A Multi-Objective Technique to Prioritize Test Cases Based on Latent Semantic Indexing,"To early discover faults in source code, test case ordering has to be properly chosen. To this aim test prioritization techniques can be used. Several of these techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test prioritization technique that determines sequences of test cases that maximize the number of discovered faults that are both technical and business critical. The technique uses the information related to the code and requirements coverage, as well as the execution cost of each test case. The approach also uses recovered trace ability links among source code and system requirements via the Latent Semantic Indexing technique. We evaluated our proposal against both a random prioritization technique and two single-objective prioritization techniques on two Java applications. The results indicate that our proposal outperforms the baseline techniques and that additional improvements are still possible.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178873,yes
364,A Remote Sensing Approach for Landslide Hazard Assessment on Engineered Slopes,"Earthworks such as embankments and cuttings are integral to road and rail networks but can be prone to instability, necessitating rigorous and continual monitoring. To date, the potential of remote sensing for earthwork hazard assessment has been largely overlooked. However, techniques such as airborne laser scanning (ALS) are now ripe for addressing these challenges. This research presents the development of a novel hazard assessment strategy, combining high-resolution remote sensing with a numerical modeling approach. The research was implemented at a railway test site located in northern England, U.K.; ALS data and multispectral aerial imagery facilitated the determination of key slope stability variables, which were then used to parameterize a coupled hydrological-geotechnical model, in order to simulate slope behavior under current and future climates. A software toolset was developed to integrate the core elements of the methodology and determine resultant slope failure hazard which could then be mapped and queried within a geographical information system environment. Results indicate that the earthworks are largely stable, which is in broad agreement with the management company's slope hazard grading data, and in terms of morphological analysis, the remote methodology was able to correctly identify 99% of earthworks classed as embankments and 100% of cuttings. The developed approach provides an effective and practicable method for remotely quantifying slope failure hazard at fine spatial scales (0.5 m) and for prioritizing and reducing on-site inspection.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032742,no
365,"A Scalable, Lightweight WebOS Application Framework","Frequently, as applications scale, they are considered in the context of a web OS-based architecture. In support of this goal, we present a lightweight framework designed as a middleware application. This architecture is highly influenced by hypermedia-based techniques, leveraging metadata in the context of HTML5. Our framework relies on a novel incorporation of a number of open source technologies including node.js and couchDB to support priorities of fast-prototyping, scalability and maintainability. Initial experiments have demonstrated that our approach performs effectively among the dynamics of our environment.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424523,no
366,A Static Approach to Prioritizing JUnit Test Cases,"Test case prioritization is used in regression testing to schedule the execution order of test cases so as to expose faults earlier in testing. Over the past few years, many test case prioritization techniques have been proposed in the literature. Most of these techniques require data on dynamic execution in the form of code coverage information for test cases. However, the collection of dynamic code coverage information on test cases has several associated drawbacks including cost increases and reduction in prioritization precision. In this paper, we propose an approach to prioritizing test cases in the absence of coverage information that operates on Java programs tested under the JUnit framework-an increasingly popular class of systems. Our approach, JUnit test case Prioritization Techniques operating in the Absence of coverage information (JUPTA), analyzes the static call graphs of JUnit test cases and the program under test to estimate the ability of each test case to achieve code coverage, and then schedules the order of these test cases based on those estimates. To evaluate the effectiveness of JUPTA, we conducted an empirical study on 19 versions of four Java programs ranging from 2K-80K lines of code, and compared several variants of JUPTA with three control techniques, and several other existing dynamic coverage-based test case prioritization techniques, assessing the abilities of the techniques to increase the rate of fault detection of test suites. Our results show that the test suites constructed by JUPTA are more effective than those in random and untreated test orders in terms of fault-detection effectiveness. Although the test suites constructed by dynamic coverage-based techniques retain fault-detection effectiveness advantages, the fault-detection effectiveness of the test suites constructed by JUPTA is close to that of the test suites constructed by those techniques, and the fault-detection effectiveness of the test suites constructed by some of JUPTA's variants is better than that of the test suites constructed by several of those techniques.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363461,yes
367,A Two-Level Prioritization Approach for Regression Testing of Web Applications,A test case prioritization technique reschedules test cases for regression testing in an order to achieve specific goals like early fault detection. We propose a new two level prioritization approach to prioritize test cases for web applications as a whole. Our approach automatically selects modified functionalities in a web application and executes test cases on the basis of the impact of modified functionalities. We suggest several new prioritization strategies for web applications and examine whether these prioritization strategies improve the rate of fault detection for web applications. We propose a new automated test suite prioritization model for web applications that selects test cases related to modified functionalities and reschedules them using our new prioritization strategies to detect faults early in test suite execution.,2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462796,yes
368,Appropriate placement of series compensators to improve transient stability of power system,"Trajectory sensitivity analysis is used to find the best places for installation of thyristor controlled series capacitors (TCSC) to improve transient stability of the power system. Based on the rotor angles of generators, an equivalent angle (??<sub>eq</sub>) is defined by determining accelerating and decelerating machines, and then using trajectory sensitivities of this angle with respect to the impedances of the transmission lines in the post-fault system, appropriate locations for placing TCSC will be found. Severity of the faults is also considered in this calculation. This method is applied to the IEEE 3-machine 9-bus test system to find the priorities of the transmission lines for installation of TCSC. Simulation with industrial software verifies the obtained results.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6303243,no
369,Automated prediction of defect severity based on codifying design knowledge using ontologies,"Assessing severity of software defects is essential for prioritizing fixing activities as well as for assessing whether the quality level of a software system is good enough for release. In filling out defect reports, developers routinely fill out default values for the severity levels. The purpose of this research is to automate the prediction of defect severity. Our aim is to research how this severity prediction can be achieved through reasoning about the requirements and the design of a system using ontologies. In this paper we outline our approach based on an industrial case study.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227962,no
370,Case based reasoning approach for adaptive test suite optimization,"Case-based reasoning is an approach to problem solving and learning that has got a lot of attention over the last few years. This paper provides an overview of the foundational issues related to case-based reasoning, describing some of the leading methodological approaches within the field, and exemplifying the current state through pointers to some systems. The framework influences the recent methodologies for knowledge level descriptions of intelligent systems. The methods for case retrieval reuse, solution testing, and learning are summarized, and realization is discussed with few example systems that represent different CBR approaches. Regression testing occurs during the maintenance stage of the software life cycle, however, it requires large amounts of test cases to assure the attainment of a certain degree of quality. So, test suite sizes may grow significantly. This paper focuses primarily on application of CBR to test suite optimization.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395870,yes
371,Code coverage-based regression test selection and prioritization in WebKit,"Automated regression testing is often crucial in order to maintain the quality of a continuously evolving software system. However, in many cases regression test suites tend to grow too large to be suitable for full re-execution at each change of the software. In this case selective retesting can be applied to reduce the testing cost while maintaining similar defect detection capability. One of the basic test selection methods is the one based on code coverage information, where only those tests are included that cover some parts of the changes. We experimentally applied this method to the open source web browser engine project WebKit to find out the technical difficulties and the expected benefits if this method is to be introduced into the actual build process. Although the principle is simple, we had to solve a number of technical issues, so we report how this method was adapted to be used in the official build environment. Second, we present results about the selection capabilities for a selected set of revisions of WebKit, which are promising. We also applied different test case prioritization strategies to further reduce the number of tests to execute. We explain these strategies and compare their usefulness in terms of defect detection and test suite reduction.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405252,yes
372,Dependency-Based Test Case Selection and Prioritization in Embedded Systems,"Embedded systems in automotive engineering are getting more and more complex due to a higher rate of integration and shared usage of sensor signals. A common solution to testing these systems is deriving test cases from models, so called model-based testing. In practice, generated test suites are typically very huge and have to be reduced by methods of regression-test selection and prioritization. In our field of application, we additionally suffer from the lack of knowledge on system internals like the source code. Therefore, our approach is based on dependences between the components of embedded systems. The model we use is derived from the system architecture and system requirements. We are using graph algorithms for selecting and prioritizing test cases to run on a certain system version. First statistical evaluations and the current implementation already have shown promising reduction of test-cases for regression testing.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200176,yes
373,Diffusion of Software Features: An Exploratory Study,"New features are frequently proposed in many software libraries. These features include new methods, classes, packages, etc. These features are utilized in many open source and commercial software systems. Some of these features are adopted very quickly, while others take a long time to be adopted. Each feature takes much resource to develop, test, and document. Library developers and managers need to decide what feature to prioritize and what to develop next. As a first step to aid these stakeholders, we perform an exploratory study on the diffusion or rate of adoption of features in Java Development Kit (JDK) library. Our empirical study proposes such questions as how many new features are adopted by client applications, how long it takes for a new feature to spread to various software products, what features are diffused quickly, and what features are diffused widely. We perform an exploratory study with new features in Java Development Kit (JDK, from version 1.3 to 1.6) and provide empirical findings to answer the above research questions.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462682,no
374,Diversity maximization speedup for fault localization,"Fault localization is useful for reducing debugging effort. However, many fault localization techniques require non-trivial number of test cases with oracles, which can determine whether a program behaves correctly for every test input. Test oracle creation is expensive because it can take much manual labeling effort. Given a number of test cases to be executed, it is challenging to minimize the number of test cases requiring manual labeling and in the meantime achieve good fault localization accuracy. To address this challenge, this paper presents a novel test case selection strategy based on Diversity Maximization Speedup (DMS). DMS orders a set of unlabeled test cases in a way that maximizes the effectiveness of a fault localization technique. Developers are only expected to label a much smaller number of test cases along this ordering to achieve good fault localization results. Our experiments with more than 250 bugs from the Software-artifact Infrastructure Repository show (1) that DMS can help existing fault localization techniques to achieve comparable accuracy with on average 67% fewer labeled test cases than previously best test case prioritization techniques, and (2) that given a labeling budget (i.e., a fixed number of labeled test cases), DMS can help existing fault localization techniques reduce their debugging cost (in terms of the amount of code needed to be inspected to locate faults). We conduct hypothesis test and show that the saving of the debugging cost we achieve for the real C programs are statistically significant.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494903,no
375,Dynamic Fault Visualization Tool for Fault-based Testing and Prioritization,"Fault-based testing has been proven to be a cost effective testing technique for software logics and rules expressed in Boolean expressions. It can guarantee the elimination of common faults without exhaustive testing. However, average software testing practitioners may not have in-depth knowledge on Boolean algebra and complex logic derivations required to apply existing fault-based testing techniques. In this paper, a dynamic fault visualization tool has been proposed. This tool allows its user to visualize fault-based testing and prioritize test inputs with a simple greedy method. The performance evaluation of this tool has been done on Boolean expressions extracted from a real life aviation tool. The results show that it can achieve significant performance improvements compared to ordinary sequential order test execution and existing static technique. The proposed visualization tool could also identify possible faults to guide the debugging process.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516370,yes
376,G-RankTest: Regression testing of controller applications,"Since controller applications must typically satisfy real-time constraints while manipulating real-world variables, their implementation often results in programs that run extremely fast and manipulate numerical inputs and outputs. These characteristics make them particularly suitable for test case generation. In fact a number of test cases can be easily created, due to the simplicity of numerical inputs, and executed, due to the speed of computations. In this paper we present G-RankTest, a technique for test case generation and prioritization. The key idea is that test case generation can run for long sessions (e.g., days) to accurately sample the behavior of a controller application and then the generated test cases can be prioritized according to different strategies, and used for regression testing every time the application is modified. In this work we investigate the feasibility of using the gradient of the output as a criterion for selecting the test cases that activate the most tricky behaviors, which we expect easier to break when a change occurs, and thus deserve priority in regression testing.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228981,yes
377,Graph-Based Optimization Algorithm and Software on Kidney Exchanges,"Kidney transplantation is typically the most effective treatment for patients with end-stage renal disease. However, the supply of kidneys is far short of the fast-growing demand. Kidney paired donation (KPD) programs provide an innovative approach for increasing the number of available kidneys. In a KPD program, willing but incompatible donor-candidate pairs may exchange donor organs to achieve mutual benefit. Recently, research on exchanges initiated by altruistic donors (ADs) has attracted great attention because the resultant organ exchange mechanisms offer advantages that increase the effectiveness of KPD programs. Currently, most KPD programs focus on rule-based strategies of prioritizing kidney donation. In this paper, we consider and compare two graph-based organ allocation algorithms to optimize an outcome-based strategy defined by the overall expected utility of kidney exchanges in a KPD program with both incompatible pairs and ADs. We develop an interactive software-based decision support system to model, monitor, and visualize a conceptual KPD program, which aims to assist clinicians in the evaluation of different kidney allocation strategies. Using this system, we demonstrate empirically that an outcome-based strategy for kidney exchanges leads to improvement in both the quantity and quality of kidney transplantation through comprehensive simulation experiments.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188515,no
378,Guiding Testing Activities by Predicting Defect-Prone Parts Using Product and Inspection Metrics,"Product metrics, such as size or complexity, are often used to identify defect-prone parts or to focus quality assurance activities. In contrast, quality information that is available early, such as information provided by inspections, is usually not used. Currently, only little experience is documented in the literature on whether data from early defect detection activities can support the identification of defect prone parts later in the development process. This article compares selected product and inspection metrics commonly used to predict defect-prone parts. Based on initial experience from two case studies performed in different environments, the suitability of different metrics for predicting defect-prone parts is illustrated. These studies revealed that inspection defect data seems to be a suitable predictor, and a combination of certain inspection and product metrics led to the best prioritizations in our contexts.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328182,no
379,High Performance Memory Requests Scheduling Technique for Multicore Processors,"In modern computer systems, long memory latency is one of the main bottlenecks micro-architects are facing for leveraging the system performance especially for memory-intensive applications. This emphasises the importance of the memory access scheduling to efficiently utilize memory bandwidth. Moreover, in recent micro-processors, multithread and multicore is turned to be the default choice for their design. This resulted in more contention on memory. Hence, the effect of memory access scheduling schemes is more critical to the overall performance boost. Although memory access scheduling techniques have been recently proposed for performance improvement, most of them have overlooked the fairness among the running applications. Achieving both high-throughput and fairness simultaneously is challenging. In this paper, we focus on the basic idea of memory requests scheduling, which includes how to assign priorities to threads, what request should be served first, and how to achieve fairness among the running applications for multicore microprocessors. We propose two new memory access scheduling techniques FLRMR, and FIQMR. Compared to recently proposed techniques, on average, FLRMR achieves 8.64% speedup relative to LREQ algorithm, and FIQMR achieves 11.34% speedup relative to IQ-based algorithm. FLRMR outperforms the best of the other techniques by 8.1% in 8-cores workloads. Moreover, FLRMR improves fairness over LREQ by 77.2% on average.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332168,no
380,Impact Analysis in the Presence of Dependence Clusters Using Static Execute after in WebKit,"Impact analysis based on code dependence can be an integral part of software quality assurance by providing opportunities to identify those parts of the software system that are affected by a change. Because changes usually have far reaching effects in programs, effective and efficient impact analysis is vital, which has different applications including change propagation and regression testing. Static Execute After (SEA) is a relation on program elements (procedures) that is efficiently computable and accurate enough to be a candidate for use in impact analysis in practice. To assess the applicability of SEA in terms of capturing real defects, we present results on integrating it into the build system of Web Kit, a large, open source software system, and on related experiments. We show that a large number of real defects can be captured by impact sets computed by SEA, albeit many of them are large. We demonstrate that this is not an issue in applying it to regression test prioritization, but generally it can be an obstacle in the path to efficient use of impact analysis. We believe that the main reason for large impact sets is the formation of dependence clusters in code. As apparently dependence clusters cannot be easily avoided in the majority of cases, we focus on determining the effects these clusters have on impact analysis.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392099,no
381,Investment optimization methodology applied to investments on non-technical losses reduction actions,"This electronic document presents the research developed on a R&D Project for the Energy Recovery Department of Rio de Janeiro's distribution company, Light S.E.S.A. The main purpose is prioritizing the grid investments on non-technical losses reduction actions. The work was developed along eighteen months, and resulted in experimental software. It analyses the historical results of the actions, allocating optimally the resources to a pre-defined period of time. The methodology consists of a statistic model based on historical results processed through a decision three optimization algorithm in order to maximize the objective function. It was tested on decision making process regarding grid investments to reduce non-technical losses, and the Return-on-Investment results were quite satisfactory.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249321,no
382,Log-based approach for performance requirements elicitation and prioritization,"Requirements engineering activities are a critical part of a project's lifecycle. Success of subsequent project phases is highly dependent on good requirements definition. However, eliciting and achieving consensus on priority between all stakeholders is a complex task. Considering software development of large scale global applications, the challenges increase by the need of managing discussions between groups of stakeholders with different roles and background. This paper presents a practical approach for requirements elicitation and prioritization based on realistic user behaviors observation. It uses basic statistic analysis and application usage information to automatically identify the most relevant requirements for majority of stakeholders. An industry case illustrates the feasibility and efficiency of our approach.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345818,yes
383,Managing Software Quality Requirements,"This research study explores current quality requirements (QR) management practices in Australian organisations focusing on the elicitation, handling processes, challenges faced, quantification methods used and interdependency management. This research was conducted through six mini case studies, examining organizations that varied in size, structure, industry and function. A mixed methodology was utilised through an online survey for gathering quantitative data and semi-structured interviews for gathering explanatory qualitative data. The results found that five out of the six organisations studied did not have a formal and defined process for the handling of QRs. Large organisations treated QRs are part of their overall project specifications, while smaller organisations saw the management of QRs as more ad hoc. When prioritising QRs, Accuracy was considered the most important priority followed by Security and Reliability. The main challenges that organisations face in their management of QRs is defining and quantifying these requirements.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328174,no
384,Modular based multiple test case prioritization,"Cost and time effective reliable test case prioritization technique is the need for present software industries. The test case prioritization for the entire program consumes more time and the selection of test case for entire software is also affecting the test performance. In order to alleviate the above problem a new methodology using modular based test case prioritization is proposed for regression testing. In this method the program is divided into multiple modules. The test cases corresponding to each module is prioritized first. In the second stage, the individual modular based prioritized test suites are combined together and further prioritized for the whole program. This method is verified for fault coverage and compared with overall program test case prioritization method. The proposed method is assessed using three standard applications namely University Students Monitoring System, Hospital Management System, and Industrial Process Operation System. The empirical studies show that the proposed algorithm is significantly performed well. The superiority of the proposed method is also highlighted.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510205,yes
385,MOTCP: A tool for the prioritization of test cases based on a sorting genetic algorithm and Latent Semantic Indexing,"Test prioritization techniques can be used to determine test case ordering and early discover faults in source code. Several of these techniques exploit a single objective function, e.g., code or requirements coverage. In this tool demo paper, we present MOTCP, a software tool that implements a multi-objective test prioritization technique based on the information related to the code and requirements coverage, as well as the execution cost of each test case. To establish users' and system requirements coverage, the MOTCP uses Latent Semantic Indexing to recover traceability links among application source code and requirements specifications. The test case ordering is then obtained by applying a non-dominated sorting genetic algorithm.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405346,yes
386,On Capturing Effects of Modifications as Data Dependencies,"Dependence analysis on an Extended Finite State Machine (EFSM) representation of the requirements of a system under test has been used in requirements-based regression testing for regression test suite (RTS) reduction (reducing the size of a given test suite by eliminating redundancies), for RTS prioritization (ordering test cases in a given test suite for early fault detection) or for RTS selection (selecting a subset of a test suite covering the identified dependencies). These particular uses of dependence analysis are based on definitions of various types of control and data dependencies (between transitions in an EFSM) caused by a given set of modifications on the requirements. This abstract considers the definitions of data dependencies, gives examples of incompleteness of existing definitions, and presents insights on completing these definitions.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340172,no
387,On the Fault-Detection Capabilities of Adaptive Random Test Case Prioritization: Case Studies with Large Test Suites,"An adaptive random (AR) testing strategy has recently been developed and examined by a growing body of research. More recently, this strategy has been applied to prioritizing regression test cases based on code coverage using the concepts of Jaccard Distance (JD) and Coverage Manhattan Distance (CMD). Code coverage, however, does not consider frequency, furthermore, comparison between JD and CMD has not yet been made. This research fills the gap by first investigating the fault-detection capabilities of using frequency information for AR test case prioritization, and then comparing JD and CMD. Experimental results show that ""coverage"" was more useful than ""frequency"" although the latter can sometimes complement the former, and that CMD was superior to JD. It is also found that, for certain faults, the conventional ""additional"" algorithm (widely accepted as one of the best algorithms for test case prioritization) could perform much worse than random testing on large test suites.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149572,yes
388,Oracle-Centric Test Case Prioritization,"Recent work in testing has demonstrated the benefits of considering test oracles in the testing process. Unfortunately, this work has focused primarily on developing techniques for generating test oracles, in particular techniques based on mutation testing. While effective for test case generation, existing research has not considered the impact of test oracles in the context of regression testing tasks. Of interest here is the problem of test case prioritization, in which a set of test cases are ordered to attempt to detect faults earlier and to improve the effectiveness of testing when the entire set cannot be executed. In this work, we propose a technique for prioritizing test cases that explicitly takes into account the impact of test oracles on the effectiveness of testing. Our technique operates by first capturing the flow of information from variable assignments to test oracles for each test case, and then prioritizing to ``cover'' variables using the shortest paths possible to a test oracle. As a result, we favor test orderings in which many variables impact the test oracle's result early in test execution. Our results demonstrate improvements in rate of fault detection relative to both random and structural coverage based prioritization techniques when applied to faulty versions of three synchronous reactive systems.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405379,yes
389,Predicting the priority of a reported bug using machine learning techniques and cross project validation,"In bug repositories, we receive a large number of bug reports on daily basis. Managing such a large repository is a challenging job. Priority of a bug tells that how important and urgent it is for us to fix. Priority of a bug can be classified into 5 levels from PI to P5 where PI is the highest and P5 is the lowest priority. Correct prioritization of bugs helps in bug fix scheduling/assignment and resource allocation. Failure of this will result in delay of resolving important bugs. This requires a bug prediction system which can predict the priority of a newly reported bug. Cross project validation is also an important concern in empirical software engineering where we train classifier on one project and test it for prediction on other projects. In the available literature, we found very few papers for bug priority prediction and none of them dealt with cross project validation. In this paper, we have evaluated the performance of different machine learning techniques namely Support Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbors (KNN) and Neural Network (NNet) in predicting the priority of the newly coming reports on the basis of different performance measures. We performed cross project validation for 76 cases of five data sets of open office and eclipse projects. The accuracy of different machine learning techniques in predicting the priority of a reported bug within and across project is found above 70% except Naive Bayes technique.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416595,yes
390,Preemptive Regression Test Scheduling Strategies: A New Testing Approach to Thriving on the Volatile Service Environments,"A workflow-based web service may use ultra-late binding to invoke external web services to concretize its implementation at run time. Nonetheless, such external services or the availability of recently used external services may evolve without prior notification, dynamically triggering the workflow-based service to bind to new replacement external services to continue the current execution. Any integration mismatch may cause a failure. In this paper, we propose Preemptive Regression Testing (PRT), a novel testing approach that addresses this adaptive issue. Whenever such a late-change on the service under regression test is detected, PRT preempts the currently executed regression test suite, searches for additional test cases as fixes, runs these fixes, and then resumes the execution of the regression test suite from the preemption point.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340125,yes
391,Prioritization of Test Cases Using Software Agents and Fuzzy Logic,"Limited test time and restricted number of test resources confront test managers with big challenges, especially in the system test. Consequently, the test manager has to prioritize test cases before each test cycle. There is much information available for determining a reasonable prioritization order in software projects. However, due to the complexity of current software systems and the high number of existing test cases, the abundance of information relevant for prioritization is not manageable for the test manager, even with high effort. In this paper we present a concept for an automated prioritization of test cases using software agents and fuzzy logic. Our prioritization system determines the prioritization order which increases the test effectiveness and the fault detection rate.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200143,yes
392,Prioritizing demand response programs from reliability aspect,"In this paper, the impact of demand response programs (DRPs) on reliability improvement of the restructured power systems is quantified. In this regard, the demand response (DR) model which treats consistently the main characteristics of the demand curve is developed for modeling. In proposed model, some penalties for customers in case of no responding to load reduction and incentives for customers who respond to reducing their loads are considered. In order to make analytical evaluation of the reliability, a mixed integer DCOPF is proposed by which load curtailments and generation re-dispatches for each contingency state are determined. Both transmission and generation failures are considered in contingency enumeration. The proposed technique is modeled in the GAMS software and solved using CPLEX. Reliability indices for generation-side, transmission network and whole system are calculated using this technique. Different DRPs based on the DR model are implemented over the IEEE RTS 24-bus test system, and reliability indices for different parties are calculated. Afterward, using proposed performance index, the priority of the considered programs is determined from view point of different market participants.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221578,no
393,Quality Playbook: Ensuring Release to Release Improvement,"Summary form only given. Before a major feature release is made available to customers, it is important to be able to anticipate if the release will be of lesser quality than its predecessor release. Our research group has developed models that use development and test times, resource levels, code added, and bugs found and fixed (or not fixed) to predict whether or not a new feature release will achieve a key quality goal - to be of better quality than its predecessor release. If the release quality prediction models, developed early in the development branches integration phase, indicate a likely upcoming quality problem in the field, another set of predictive models ('playbook' models) are then developed and used by our team to identify development or test practices that are in need of improvement. These playbook models are key components of what we call 'quality playbooks,' that are designed to address several objectives: . Identify 'levers' that positively influence feature release quality. Levers are in-process engineering metrics that are associated with specific development or test processes/practices and measure their adoption and effectiveness. . If possible, identify levers that can be invoked early in the lifecycle, to enable the development and test teams to improve deficient practices and remediate the current release under development. If it is not possible to identify early levers but possible to identify levers later in the lifecycle, we can only change deficient practices to improve the quality of future successor releases. . Determine the potential quality impact of changes suggested by the profile of significant levers. Low impact levers are likely not to be addressed by development teams. . Determine the resource and schedule investments needed to change and implement practices: Training, disruption, additional engineering time, etc. . Using impact and investment calculations identify which practices to change, either for the current release or just for subsequent releases. Develop a prioritization/ROI scheme to provide planning guidance to development and test teams. . Identify specific practice changes needed, or new practices to adopt. . Design and plan pilot programs to test the models, including the impact and investment components. Using this 'playbook' approach, our team has developed models for 31 major feature releases that are resident on 11 different hardware platforms. These models have identified six narrowly-defined classes of metrics that include both actionable levers and 'indicator' metrics that correlate well with release quality. (Indicator metrics do also correlate well, but are less specifically actionable.) The models for these six classes of metrics (and their associated practices) include strong levers and strong indicators for all releases and platforms thus far examined. Impact and investment results are also described in this paper, as are pilot programs that have tested the validity of the modeling and business calculation results. Two additional large-scale pilots of the 'playbook' approach are underway, and these are also described.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405417,no
394,Resilient system design for Prognosis and Health Monitoring of an ocean power generator,"In this paper we introduce a new methodology that integrates system resilience engineering and hazard analysis into complex system design. We then demonstrate its performance by applying it to the design of a Prognosis and Health Monitoring (PHM) system for an ocean current power generator. Three common methodologies for system hazard analysis were tested by applying them to the PHM system's network topology architecture; STAMP-based Process Analysis (STPA), Hazard and Operability Analysis (HAZOP), and a Resilience Engineering, Heuristic-based approach. While all three approaches adequately revealed most PHM system hazards, which assisted in identifying the means with which to mitigate them, none of the approaches fully addressed the multi-state dimensionality of the sub-components of the system, missing risky and hazardous scenarios. We developed the System Hazard Indication and Extraction Learning Diagnosis (SHIELD) methodology for system hazard analysis and resilient design. SHIELD integrates state space analysis into the hazard analysis process in order to facilitate the location of undiscovered hazard scenarios. Our approach uses recursive, top-down system decomposition with subsystem, interface, and process cycle identification. Then, a bottom-up recursive evaluation is completed where we analyze the subsystem state space and state transitions with regard to hazards/failures in process cycles. This yields a comprehensive list of failure states and scenarios. Finally, a top-down prioritized application of resilient engineering heuristics which address hazard scenarios is prescribed. This final phase results in a comprehensive, complete analysis of complex system architectures forcing resilience into the final system design.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189490,no
395,School that has a vision,"The article is devoted to presentation of the concept and results of the Private secondary professional school in Poprad that is actually realization of the vision of two high school teachers on how education might look like at high school. Based on the inclusion of Private secondary professional school into the school system and the subsequent approval of the experimental verification of branches of study the founder began to build a school, which is based on two main priorities - high quality educational process and modern hardware and software. The school where students have their own laptops, use electronic books along with an educational portal, do test and homework via the internet and where parents can check the results by means of the electronic register of students' grades. The school which offers their students new perspective branches of study tailor-made according to the current and future requirements of the market, i.e. without the school subjects that are not relevant anymore, and with an ongoing innovative curriculum; the school with enthusiastic highly qualified teachers using new methods and forms of teaching including modern technique. The school that partners with the university sector to ensure the follow-up study of the graduates provides video conferencing for specialized subjects and conversational courses with schools abroad. Private secondary professional school, Ul.29.augusta 4812 in Poprad, officially began on 01.09.2008, the founder of which is Tatransk? Akad??mia.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418322,no
396,Selecting an appropriate framework for value-based requirements prioritization,"There are usually more requirements than feasible in a given schedule. Thus, it's imperative to be able to choose the most valuable ones for implementation to ensure the delivery of a high value software system. There are myriad requirements prioritization frameworks and selecting the most appropriate one is a decision problem in its own right. In this paper we present our approach in selecting the most appropriate value based requirements prioritization framework as per the requirements of our stakeholders. Based on our analysis a single framework was selected, validated by requirements engineers and project managers and deployed for company-wide use by a major IT player in India.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345819,no
397,Size-Constrained Regression Test Case Selection Using Multicriteria Optimization,"To ensure that a modified software system has not regressed, one approach is to rerun existing test cases. However, this is a potentially costly task. To mitigate the costs, the testing effort can be optimized by executing only a selected subset of the test cases that are believed to have a better chance of revealing faults. This paper proposes a novel approach for selecting and ordering a predetermined number of test cases from an existing test suite. Our approach forms an Integer Linear Programming problem using two different coverage-based criteria, and uses constraint relaxation to find many close-to-optimal solution points. These points are then combined to obtain a final solution using a voting mechanism. The selected subset of test cases is then prioritized using a greedy algorithm that maximizes minimum coverage in an iterative manner. The proposed approach has been empirically evaluated and the results show significant improvements over existing approaches for some cases and comparable results for the rest. Moreover, our approach provides more consistency compared to existing approaches.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928351,no
398,Software testing suite prioritization using multi-criteria fitness function,"Regression testing is the process of validating modifications introduced in a system during software maintenance. It is an expensive, yet an important process. As the test suite size is very large, system retesting consumes large amount of time and computing resources. Unfortunately, there may be insufficient resources to allow for the re-execution of all test cases during regression testing. Testcase prioritization techniques aim to improve the effectiveness of regression testing, by ordering the testcases so that the most beneficial are executed first with higher priority. The objective of test case prioritization is to detect faults as early as possible. An approach for automating the test case prioritization process using genetic algorithm with Multi-Criteria Fitness function is presented. It uses multiple control flow coverage metrics. These metrics measure the degree of coverage of conditions, multiple conditions and statements that the test case covers. Theses metrics are weighted by the number of faults revealed and their severity. The proposed Multi-criteria technique showed superior results compared to similar work.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523563,yes
399,Test Case Prioritization Due to Database Changes in Web Applications,"A regression test case prioritization (TCP) technique reorders test cases for regression testing to achieve early fault detection. Most TCP techniques have been developed for regression testing of source code in an application. Most web applications rely on a database server for serving client requests. Any changes in the database result in erroneous client interactions and may bring down the entire web application. However, most prioritization techniques are unsuitable for prioritizing test suites for early detection of changes in databases. There are very few proposals in the literature for prioritization of test cases that can detect faults in the database early. We propose a new automated TCP technique for web applications that automatically identifies the database changes, prioritizes test cases related to database changes and executes them in priority order to detect faults early.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200175,yes
400,Test Case Prioritization for Regression Testing Based on Function Call Path,"Test case prioritization is an effective and practical technique of regression testing. It is helpful to increase the efficiency of regression testing by sorting and executing test cases according to their importance. Static paths on function call obtained by analyzing the source code, combined with the dynamic path after executing test cases, the correspondence is built between test cases and the static paths, identifying the changes which software developers modify program to correct defects, giving different priority to test case based on path coverage , test cases are selected in accordance with their priorities in regression testing. Firstly, the background and related concept of test case prioritization are introduced. And then, the relevant research work is outlined, a set of new prioritization algorithms are proposed; implementation and analysis of the algorithm are given finally.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6301421,yes
401,Test case prioritization incorporating ordered sequence of program elements,"Test suites often grow very large over many releases, such that it is impractical to re-execute all test cases within limited resources. Test case prioritization, which rearranges test cases, is a key technique to improve regression testing. Code coverage information has been widely used in test case prioritization. However, other important information, such as the ordered sequence of program elements measured by execution frequencies, was ignored by previous studies. It raises a risk to lose detections of difficult-to-find bugs. Therefore, this paper improves the similarity-based test case prioritization using the ordered sequence of program elements measured by execution counts. The empirical results show that our new technique can increase the rate of fault detection more significantly than the coverage-based ART technique. Moreover, our technique can detect bugs in loops more quickly and be more cost-benefits than the traditional ones.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228980,yes
402,Towards reliable web applications: ISO 19761,"This research adopts a new scenario-based black box testing methodology for testing web applications. It combines a black box testing strategy with the functions (scenarios) measured by the COSMIC-FFP measurement procedure (ISO/IEC 19761 standard) to produce an optimal set of test cases. This testing approach shows its applicability during all the development phases. Moreover, it can be applied during the early development phase once the specifications have been documented as well as after the development phase where we don't have the access to the code. This paper also considers the use of a functional complexity measure for assigning priorities to the generated test cases. Finally, those concepts have been applied on part of Online Banking System as a case study.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6389396,no
403,Use of embedded intelligence in tactical grids for energy surety and fuel conservation,"This paper describes a system for creating an energy-sharing infrastructure, effectively creating redundant sources of energy supply and significantly reducing the logistical burdens associated with providing power. An intelligent power management and power grid system has been developed and tested. This system optimizes performance and efficiency through local and system-level autonomous controls. The grid system was based on existing military, trailer-mounted, mobile power equipment. A reduction in fuel consumption of 36 percent was observed. In addition, prioritized load shedding was demonstrated as a means to prevent the generators from being overloaded.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345122,no
404,Using Non-redundant Mutation Operators and Test Suite Prioritization to Achieve Efficient and Scalable Mutation Analysis,"Mutation analysis is a powerful and unbiased technique to assess the quality of input values and test oracles. However, its application domain is still limited due to the fact that it is a time consuming and computationally expensive method, especially when used with large and complex software systems. Addressing these challenges, this paper makes several contributions to significantly improve the efficiency of mutation analysis. First, it investigates the decrease in generated mutants by applying a reduced, yet sufficient, set of mutants for replacing conditional (COR) and relational (ROR) operators. The analysis of ten real-world applications, with 400,000 lines of code and more than 550,000 generated mutants in total, reveals a reduction in the number of mutants created of up to 37% and more than 25% on average. Yet, since the isolated use of non-redundant mutation operators does not ensure that mutation analysis is efficient and scalable, this paper also presents and experimentally evaluates an optimized workflow that exploits the redundancies and runtime differences of test cases to reorder and split the corresponding test suite. Using the same ten open-source applications, an empirical study convincingly demonstrates that the combination of non-redundant operators and prioritization leveraging information about the runtime and mutation coverage of tests reduces the total cost of mutation analysis further by as much as 65%.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405400,no
405,Using Prioritized Disk Service to Expedite Program Execution,"Computer systems often host multiple programs in execution simultaneously. Among those running programs, some may be important and time-critical, which users would expect them to finish their execution as soon as possible. Generally speaking, the course of program execution includes CPU operation and hard disk operation (disk I/O). For the CPU operation, modern computer systems have the ability to adjust the CPU scheduling sequence according to program priority. However, most computer systems do not have effective ways to conduct disk I/O based on program priority. The Linux operating system has been widely used in many areas. It supports several disk schedulers. The Complete Fair Queuing (CFQ) and the Anticipatory Scheduling (AS) are among those most well-known. Currently, CFQ is the default disk scheduler in the Linux operating system. AS is the predecessor of CFQ. Unfortunately, CFQ only offers prioritized disk I/O to some extent through the tool ""ionice"", while AS does not provide any prioritized service at all. We propose and implement a new disk scheduler, namely Prioritized Anticipatory Scheduling (PAS), by adding schemes of supporting prioritized disk I/O into AS in the Linux kernel. Our experimental results show that PAS surpasses CFQ with ionice for the vast majority of all test cases. Compared with AS, PAS can improve the performance of programs with high disk I/O priority by up to 71.88%.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332286,no
406,Value-Based Coverage Measurement in Requirements-Based Testing: Lessons Learned from an Approach Implemented in the TOSCA Testsuite,"Testing is one of the most widely practiced quality assurance measures and also one of the most resource-intensive activities in software development. Still, however, most of the available methods, techniques and tools for software testing are value-neutral and do not realize the potential value contribution of testing. In this paper we present an approach for value-based coverage measurement that can be used to align the testing effort with the achievable value associated with requirements and functional units. It has been implemented as part of a commercial test tool and was successfully applied in real-world projects. The results demonstrated its ability to adequately capture the distribution of the business value and risks involved in different requirements. The paper concludes with sharing important lessons learned from developing value-based coverage measurement in the practical setting of commercial tool development and real-world test projects.",2012,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328176,yes
407,A Comparison of Different Defect Measures to Identify Defect-Prone Components,"(Background) Defect distribution in software systems has been shown to follow the Pareto rule of 20-80. This motivates the prioritization of components with the majority of defects for testing activities. (Research goal) Are there significant variations between defective components and architectural hotspots identified by other defect measures? (Approach) We have performed a study using post-release data of an industrial Smart Grid application with a well-maintained defect tracking system. Using the Pareto principle, we identify and compare defect-prone and hotspots components based on four defect metrics. Furthermore, we validated the quantitative results against qualitative data from the developers. (Results) Our results show that at the top 25% of the measures 1) significant variations exist between the defective components identified by the different defect metrics and that some of the components persist as defective across releases 2) the top defective components based on number of defects could only identify about 40% of critical components in this system 3) other defect metrics identify about 30% additional critical components 4) additional quality challenges of a component could be identified by considering the pair wise intersection of the defect metrics. (Discussion and Conclusion) Since a set of critical components in the system is missed by using largest-first or smallest-first prioritization approaches, this study, therefore, makes a case for an all-inclusive metrics during defect model construction such as number of defects, defect density, defect severity and defect correction effort to make us better understand what comprises defect-prone components and architectural hotspots, especially in critical applications.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693238,no
408,A Fuzzy Expert System for Cost-Effective Regression Testing Strategies,"Different testing environments and software change characteristics can affect the choice of regression testing techniques. In our prior work, we developed adaptive regression testing (ART) strategies to investigate this problem. While the ART strategies showed promising results, we also found that the multiple criteria decision making processes required for the ART strategies are time-consuming, often inaccurate and inconsistent, and limited in their scalability. To address these issues, in this research, we develop and empirically study a fuzzy expert system (FESART) to aid decision makers in choosing the most cost-effective technique for a particular software version. The results of our study show that FESART is consistently more cost-effective than the previously proposed ART strategies. One of the biggest contributors to FESART being more cost-effective is the reduced time required to apply the strategy. This contribution has significant impact because a strategy that is less time-consuming will be easier for researchers and practitioners to adopt, and will provide even greater cost-savings for regression testing sessions.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676871,no
409,A novel approach for test case prioritization,"The process of verifying the modified software in the maintenance phase is called Regression Testing. The size of the regression test suite and its selection process is a complex task for regression testers because of time and budget constraints. In this research paper, new Prioritization technique based on hamming distance has been proposed. It is illustrated using an example and found that it produces good results. Average Percentage of Fault Detection (APFD) metrics and charts has been used to show the effectiveness of proposed algorithm.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724209,yes
410,A refactoring-based approach for test case selection and prioritization,"Refactoring edits, commonly applied during software development, may introduce faults in a previously-stable code. Therefore, regression testing is usually applied to check whether the code maintains its previous behavior. In order to avoid rerunning the whole regression suite, test case prioritization techniques have been developed to order test cases for earlier achievement of a given goal, for instance, improving the rate of fault detection during regression testing execution. However, as current techniques are usually general purpose, they may not be effective for early detection of refactoring faults. In this paper, we propose a refactoring-based approach for selecting and prioritizing regression test cases, which specializes selection/prioritization tasks according to the type of edit made. The approach has been evaluated through a case study that compares it to well-known prioritization techniques by using a real open-source Java system. This case study indicates that the approach can be more suitable for early detection of refactoring faults when comparing to the other prioritization techniques.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595798,yes
411,A Study in Prioritization for Higher Strength Combinatorial Testing,"Recent studies have shown that combinatorial interaction testing (CIT) is an effective fault detection technique and that early fault detection can be improved by ordering test suites by interaction based prioritization approaches. Despite research that has shown that higher strength CIT improves fault detection, there have been fewer studies that aim to understand the impact of prioritization based on higher strength criteria. In this paper, we aim to understand how interaction based prioritization techniques perform, in terms of early fault detection when we prioritize based on 3-way interactions. We generalize prior work on prioritizing using 2-way interactions to t-way prioritization, and empirically evaluate this on three open source subjects, across multiple versions of each. We examine techniques that prioritize both existing CIT suites as well as generate new ones in prioritized order. We find that early fault detection can be improved when prioritizing 3-way CIT test suites by interactions that cover more code, and to a lesser degree when generating tests in prioritized order. Our techniques that work only from the specification, appear to work best with 2-way generation.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571645,yes
412,A Uniform Representation of Hybrid Criteria for Regression Testing,"Regression testing tasks of test case prioritization, test suite reduction/minimization, and regression test selection are typically centered around criteria that are based on code coverage, test execution costs, and code modifications. Researchers have developed and evaluated new individual criteria; others have combined existing criteria in different ways to form what we--and some others--call hybrid criteria. In this paper, we formalize the notion of combining multiple criteria into a hybrid. Our goal is to create a uniform representation of such combinations so that they can be described unambiguously and shared among researchers. We envision that such sharing will allow researchers to implement, study, extend, and evaluate the hybrids using a common set of techniques and tools. We precisely formulate three hybrid combinations, Rank, Merge, and Choice, and demonstrate their usefulness in two ways. First, we recast, in terms of our formulations, others' previously reported work on hybrid criteria. Second, we use our previous results on test case prioritization to create and evaluate new hybrid criteria. Our findings suggest that hybrid criteria of others can be described using our Merge and Rank formulations, and that the hybrid criteria we developed most often outperformed their constituent individual criteria.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6484067,no
413,Adaptive Test-Case Prioritization Guided by Output Inspection,"Test-case prioritization is to schedule the execution order of test cases so as to maximize some objective (e.g., revealing faults early). The existing test-case prioritization approaches separate the process of test-case prioritization and the process of test-case execution by presenting the execution order of all test cases before programmers start running test cases. As the execution information of the modified program is not available for the existing test-case prioritization approaches, these approaches mainly rely on only the execution information of the previous program before modification. To address this problem, we present an adaptive test-case prioritization approach, which determines the execution order of test cases simultaneously during the execution of test cases. In particular, the adaptive approach selects test cases based on their fault-detection capability, which is calculated based on the output of selected test cases. As soon as a test case is selected and runs, the fault-detection capability of each unselected test case is modified according to the output of the latest selected test case. To evaluate the effectiveness of the proposed adaptive approach, we conducted an experimental study on eight C programs and four Java programs. The experimental results show that the adaptive approach is usually significantly better than the total test-case prioritization approach and competitive to the additional test-case prioritization approach. Moreover, the adaptive approach is better than the additional approach on some subjects (e.g, replace and schedule).",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649818,yes
414,Bridging the gap between the total and additional test-case prioritization strategies,"In recent years, researchers have intensively investigated various topics in test-case prioritization, which aims to re-order test cases to increase the rate of fault detection during regression testing. The total and additional prioritization strategies, which prioritize based on total numbers of elements covered per test, and numbers of additional (not-yet-covered) elements covered per test, are two widely-adopted generic strategies used for such prioritization. This paper proposes a basic model and an extended model that unify the total strategy and the additional strategy. Our models yield a spectrum of generic strategies ranging between the total and additional strategies, depending on a parameter referred to as the p value. We also propose four heuristics to obtain differentiated p values for different methods under test. We performed an empirical study on 19 versions of four Java programs to explore our results. Our results demonstrate that wide ranges of strategies in our basic and extended models with uniform p values can significantly outperform both the total and additional strategies. In addition, our results also demonstrate that using differentiated p values for both the basic and extended models with method coverage can even outperform the additional strategy using statement coverage.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606565,yes
415,Bypassing Code Coverage Approximation Limitations via Effective Input-Based Randomized Test Case Prioritization,"Test case prioritization assigns the execution priorities of the test cases in a given test suite with the aim of achieving certain goals. Many existing test case prioritization techniques however assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in many software development projects. This paper proposes a novel family of LBS techniques. They make adaptive tree-based randomized explorations with an adaptive randomized candidate test set strategy to diversify the explorations among the branches of the exploration trees constructed by the test inputs in the test suite. They get rid of the assumption on the historical correlation of code coverage between program versions. Our techniques can be applied to programs with or without any previous versions, and hence are more general than many existing test case prioritization techniques. The empirical study on four popular UNIX utility benchmarks shows that, in terms of APFD, our LBS techniques can be as effective as some of the best code coverage-based greedy prioritization techniques ever proposed. We also show that they are significantly more efficient and scalable than the latter techniques.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649820,yes
416,CDM-Suite: An Attributed Test Selection Tool,"Many reasons lead to embedded systems getting more complex than ever. The higher integration and interconnection and the additional effort spend for safety-critical functions, require new techniques to support the testing process. Beneath using model-driven test techniques, test engineers need support to take the right decisions on test case selection and prioritization during the whole development process. Available approaches on test selection and prioritization lack in adaptability to different testing techniques. We previously presented our approach of modeling components and dependences of a system, such as black-box systems, using Component-Dependency-Models (CDMs) as shown in earlier publications. In this paper we present our tool CDM-Suite in detail and show the way it can be used by test engineers. We therefore start with the presentation of our approach and discussing related tools. The next section describes the graphical user interface (GUI) and the different ways of interaction for data generation. This point includes the introduction of metrics build upon fuzzy logic and graph analysis. At last we show how to interpret analysis results and derive knowledge for test case selection and prioritization. We sum the paper up by giving a conclusion and a presentation outline.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569753,no
417,Closed-loop analysis of soft decisions for serial links,"We describe the benefit of using closed-loop measurements for a radio receiver paired with a counterpart transmitter. We show that real-time analysis of the soft decision output of a receiver can provide rich and relevant insight far beyond the traditional hard-decision bit error rate (BER) test statistic. We describe a Soft Decision Analyzer (SDA) implementation for closed-loop measurements on single- or dual-(orthogonal) channel serial data communication links. The analyzer has been used to identify, quantify, and prioritize contributors to implementation loss in live-time during the development of software defined radios. This test technique gains importance as modern receivers are providing soft decision symbol synchronization as radio links are challenged to push more data and more protocol overhead through noisier channels, and software-defined radios (SDRs) use error-correction codes that approach Shannon's theoretical limit of performance.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737541,no
418,Compositional analysis of switched Ethernet topologies,"In this paper we study distributed automotive control applications whose tasks are mapped onto different ECUs communicating via a switched Ethernet network. As traditional automotive communication buses like CAN, FlexRay, LIN and MOST are gradually reaching their performance limits because of the increasing complexity of automotive architectures and applications, Ethernet-based in-vehicle communication systems have attracted a lot of attention in recent times. However, currently there is very little work on systematic timing analysis for Ethernet which is important for its deployment in safety-critical scenarios like in an automotive architecture. In this work, we propose a compositional timing analysis technique that takes various features of switched Ethernet into account like network topology, frame priorities, communication delay, memory requirement on switches, performance, etc. Such an analysis technique is particularly suitable during early design phases of automotive architectures and control software deployment. We demonstrate its use in analyzing mixed-criticality traffic patterns consisting of messages from performance-oriented control loops and timing-sensitive real-time tasks. We further evaluate the tightness of the obtained analytical bounds with an OMNeT++ based network simulation environment, which involves long simulation time and does not provide formal guarantees.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513677,no
419,Coverage-Based Test Case Prioritisation: An Industrial Case Study,This paper presents an industrial case study of coverage-based prioritisation techniques on a real world system with real regression faults. The study evaluates four common and different test case prioritisation techniques and examines the effects of using various coverage criteria on the fault detection rates of the prioritised test suites. The results show that prioritisation techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information does not significantly enhance fault detection rates.,2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569742,yes
420,Data Centers as Software Defined Networks: Traffic Redundancy Elimination with Wireless Cards at Routers,"We propose a novel architecture of data center networks (DCN), which adds wireless network card to both servers and routers. Existing traffic redundancy elimination (TRE) mechanisms reduce link loads and increase network capacity in several environments by removing strings that have appeared in earlier packets through encoding and decoding them several hops downstream. This article is the first to explore TRE mechanisms in large-scale DCNs and the first to exploit cooperative TRE among servers. Moreover, it also achieves the `logically centralized' control over the physically distributed states in emerging software defined networks (SDN) paradigm, by sharing information among servers and routers in data centers with wireless cards. We first formulate the TREDaCeN (TRE in Data Center Networks) problem and reduce the cycle cover problem to prove that finding an optimal caching task assignment for TREDaCeN problem is NP-hard. We further describe an offline TREDaCeN algorithm which is proved to have good approximation ratio. We then discuss efficient online zero-delay and semi-distributed implementations of TREDaCeN supported by physical proximity of servers and routers, enabling status updates in a single wireless transmission, using an efficient prioritized schedule. We also address online cache replacement and consistency of information in servers and routers with and without delay. Our framework is tested on different parameters and shows superior performance in comparison to other mechanisms (imported directly to this setting). Our results show the robustness and the trade-off between the `logically centralized' implementation and the overhead on handling inconsistency of distributed information in DCN.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678112,no
421,Defect Prioritization in the Software Industry: Challenges and Opportunities,"Defect prioritization is a decision making process wherein stakeholders determine the temporal order of open defects to be fixed. It is critical to the software development lifecycle as the decisions made during this process directly affect release planning, resource management, and maintenance costs. In fact, defect prioritization is complex as many factors need to be taken into consideration and the decisions made can be subjective or incorporate inherent knowledge and intuition of decision makers. We believe that managing the complexities of the decision making process can provide valuable support and help in uncovering any inconsistencies in the interpretation of criteria to prioritize defects. In this paper, we explore the defect triaging process in Research In Motion to gain a better understanding of the shortcomings and challenges of the current practices. Based on our findings, we sketch some research directions to improve industrial software defect prioritization.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569718,no
422,Developing new Automatic Test Equipments (ATE) using systematic design approaches,"Keeping Automatic Test Equipments (ATE) current with technology is one of the major challenges in automatic testing world. Needs and priorities can quickly evolve throughout the life cycle of ATEs and handling obsolescence via performing upgrades on hardware and software can be impossible after several years. While developing Test Program Sets (TPS), if existing ATE systems cannot meet the necessary requirements without inserting extra test devices or decreasing test coverage, then designing a new ATE can be seen inevitable. If a new ATE system is to be designed, it is very crucial that the requirements for the new ATE system should be identified before design process begins. Determining the requirements is a very critical stage in designing ATE because if enough effort is not focused and extended analysis is not carried out on determining the requirements, then the newly formed ATE system will likely fail to cover the test requirements of the DUTs. Setting up the hardware and software architecture is the next stage after the process of determining the requirements of the ATE system. Practical and cost effective solutions should be considered without compromising performance and capabilities of the test devices. The architectures should be suitable for future enhancements to the system. Throughout the design process, the design requirements, critical design descriptions, verification and validation procedures should be clearly documented and reviewed with relevant engineers. In this paper, the design process of a new ATE system by using systematic design approach is discussed. This process is followed during the design of the ATE system which is under use from the beginning of the year 2013. The challenges in the design process, determining the requirements and the formation of hardware and software architecture are explained in detail benefiting from real experiences.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645035,yes
423,Efficient Automated Program Repair through Fault-Recorded Testing Prioritization,"Most techniques for automated program repair use test cases to validate the effectiveness of the produced patches. The validation process can be time-consuming especially when the object programs ship with either lots of test cases or some long-running test cases. To alleviate the cost for testing, we first introduce regression test prioritization insight into the area of automated program repair, and present a novel prioritization technique called FRTP with the goal of reducing the number of test case executions in the repair process. Unlike most existing prioritization techniques frequently requiring additional cost for gathering previous test executions information, FRTP iteratively extracts that information just from the repair process, and thus incurs trivial performance lose. We also built a tool called TrpAutoRepair, which implements our FRTP technique and has the ability of automatically repairing C programs. To evaluate TrpAutoRepair, we compared it with GenProg, a state-of-the-art tool for automated C program repair. The experiment on the 5 subject programs with 16 real-life bugs provides evidence that TrpAutoRepair performs at least as good as GenProg in term of success rate, in most cases (15/16), TrpAutoRepair can significantly improve the repair efficiency by reducing efficiently the test case executions when searching a valid patch in the repair process.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676889,no
424,Evolutionary Search Algorithms for Test Case Prioritization,"To improve the effectiveness of certain performance goals, test case prioritization techniques are used. These technique schedule the test cases in particular order for execution so as to increase the efficacy in meeting the performance goals. For every change in the program it is considered inefficient to re-execute each and every test case. Test case prioritization techniques arrange the test cases within a test suite in such a way that the most important test case is executed first. This process enhances the effectiveness of testing. This algorithm during time constraint execution has been shown to have detected maximum number fault while including the sever test cases.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918806,yes
425,Getting more from requirements traceability: Requirements testing progress,"Requirements Engineering (RE) and Testing are important steps in many software development processes. It is critical to monitor the progress of the testing phase to allocate resources (person-power, time, computational resources) properly, and to make sure the prioritization of requirements are reflected during testing, i.e. more critical requirements are given higher priority and tested well. In this paper, we propose a new metric to help stakeholders monitor the progress of the testing phase from a requirements perspective, i.e. which requirements are tested adequately, and which ones insufficiently. Unlike existing progress related metrics, such as code coverage and MC/DC (modified condition/decision) coverage, this metric is on the requirements level, not source code level. We propose to automatically reverse engineer this metric from the existing test cases of a system. We also propose a method to evaluate this metric, and report the results of three case studies. On these case studies, our technique obtains results within 75.23% - 91.11% of the baseline on average.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620148,no
426,History-Based Test Case Prioritization with Software Version Awareness,"Test case prioritization techniques schedule the test cases in an order based on some specific criteria so that the tests with better fault detection capability are executed at an early position in the regression test suite. Many existing test case prioritization approaches are code-based, in which the testing of each software version is considered as an independent process. Actually, the test results of the preceding software versions may be useful for scheduling the test cases of the later software versions. Some researchers have proposed history-based approaches to address this issue, but they assumed that the immediately preceding test result provides the same reference value for prioritizing the test cases of the successive software version across the entire lifetime of the software development process. Thus, this paper describes ongoing research that studies whether the reference value of the immediately preceding test results is version-aware and proposes a test case prioritization approach based on our observations. The experimental results indicate that, in comparison to existing approaches, the presented one can schedule test cases more effectively.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601820,yes
427,"Leveraging the Crowd: How 48,000 Users Helped Improve Lync Performance","Performance is a critical component of customer satisfaction with network-based applications. Unfortunately, accurately evaluating the performance of collaborative software that operates in extremely heterogeneous environments is difficult with traditional techniques such as modeling workloads or testing in controlled environments. To evaluate performance of an application in the wild during development, the authors deployed early versions of the software, collecting performance data from application users for key usage scenarios. The analysis package they used produces visualizations to help development teams identify and prioritize performance issues by focusing on performance early in the development cycle, evaluating progress, identifying defects, and estimating timelines.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509371,no
428,Managing technical debt: An industrial case study,"Technical debt is the consequence of trade-offs made during software development to ensure speedy releases. The research community lacks rigorously evaluated guidelines to help practitioners characterize, manage and prioritize debt. This paper describes a study conducted with an industrial partner during their implementation of Agile development practices for a large software development division within the company. The report contains our initial findings based on ethnographic observations and semi-structured interviews. The goal is to identify the best practices regarding managing technical debt so that the researchers and the practitioners can further evaluate these practices to extend their knowledge of the technical debt metaphor. We determined that the developers considered their own taxonomy of technical debt based on the type of work they were assigned and their personal understanding of the term. Despite management's high-level categories, the developers mostly considered design debt, testing debt and defect debt. In addition to developers having their own taxonomy, assigning dedicated teams for technical debt reduction and allowing other teams about 20% of time per sprint for debt reduction are good initiatives towards lowering technical debt. While technical debt has become a well-regarded concept in the Agile community, further empirical evaluation is needed to assess how to properly apply the concept for various development organizations.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608672,no
429,On Combining Model-Based Analysis and Testing,"Testing a computer system is a challenging task, both due to the large number of possible test cases and the limited resources allocated for testing activities. This means that only a subset of all possible test cases can be chosen to test a system, and therefore the decision on the selection of test cases becomes important. The result of static analysis of a system can be used to help with this decision, in the context of model-based development of systems, this means that the analysis performed on a system model can be used to prioritize and guide the testing efforts. Furthermore, since models allow expression of non-functional requirements (such as performance, timing and security), model-guided testing can be used to direct testing towards specific parts of the system which have large impact on such requirements. In this paper, we focus on modeling and trade-off analysis of non-functional requirements and how static analysis helps to identify problematic parts of a system and thus guide the selection of test cases to target such parts.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614319,yes
430,On the Correlation between the Effectiveness of Metamorphic Relations and Dissimilarities of Test Case Executions,"Metamorphic testing (MT) is a property-based automated software testing method. It alleviates the oracle problem by testing programs against metamorphic relations (MRs), which are necessary properties among multiple executions of the target program. For a given problem, usually more than one MR can be identified. It is therefore of practical importance for testers to know the nature of good MRs, that is, which MRs are likely to have higher chances of revealing failures. To address this issue we investigate the correlation between the fault-detection effectiveness of MRs and the dissimilarity (distance) of test case execution profiles. Empirical study results reveal that there is a strong and statistically significant positive correlation between the fault-detection effectiveness and the distance. The findings of this research can help to develop automated means of selecting/prioritizing MRs for cost-effective metamorphic testing.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605921,no
431,On the Gain of Measuring Test Case Prioritization,"Test case prioritization (TCP) techniques aim to schedule the order of regression test suite to maximize some properties, such as early fault detection. In order to measure the abilities of different TCP techniques for early fault detection, a metric named average percentage of faults detected (APFD) is widely adopted. In this paper, we analyze the metric APFD and explore the gain of measuring TCP techniques from a control theory viewpoint. Based on that, we propose a generalized metric for TCP. This new metric focuses on the gain of defining early fault detection and measuring TCP techniques for various needs in different evaluation scenarios. By adopting this new metric, not only flexibility can be guaranteed, but also explicit physical significance for the metric will be provided before evaluation.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649891,yes
432,On the Influence of Model Structure and Test Case Profile on the Prioritization of Test Cases in the Context of Model-Based Testing,"Test case prioritization techniques aim at defining an ordering of test cases that favor the achievement of a goal during test execution, such as revealing faults as earlier as possible. A number of techniques have already been proposed and investigated in the literature and experimental results have discussed whether a technique is more successful than others. However, in the context of model-based testing, only a few attempts have been made towards either proposing or experimenting test case prioritization techniques. Moreover, a number of factors that may influence on the results obtained still need to be investigated before more general conclusions can be reached. In this paper, we present empirical studies that focus on observing the effects of two factors: the structure of the model and the profile of the test case that fails. Results show that the profile of the test case that fails may have a definite influence on the performance of the techniques investigated.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800188,yes
433,Optimization of test suite-test case in regression test,"Exhaustive product evolution and testing is required to ensure the quality of product. Regression testing is crucial to ensure software excellence. Regression test cases are applied to assure that new or adapted features do not relapse the existing features. As innovative features are included, new test cases are generated to assess the new functionality, and then included in the existing pool of test cases, thus escalating the cost and the time required in performing regression test and this unswervingly impacts the release, laid plan and the quality of the product. Hence there is a need to select minimal test cases that will test all the functionalities of the engineered product and it must rigorously test the functionalities that have high risk exposure. Test Suite-Test Case Refinement Technique will reduce regression test case pool size, reduce regression testing time, cost &amp; effort and also ensure the quality of the engineered product. This technique is a regression test case optimization technique that is a hybrid of Test Case Minimization based on specifications and Test Case Prioritization based on risk exposure. This approach will facilitate achievement of quality product with decreased regression testing time and cost yet uncover same amount of errors as the original test cases.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724206,yes
434,Prioritizing Variable-Strength Covering Array,"Combinatorial interaction testing is a well-studied testing strategy, and has been widely applied in practice. Combinatorial interaction test suite, such as fixed-strength and variable-strength interaction test suite, is widely used for combinatorial interaction testing. Due to constrained testing resources in some applications, for example in combinatorial interaction regression testing, prioritization of combinatorial interaction test suite has been proposed to improve the efficiency of testing. However, nearly all prioritization techniques may only support fixed-strength interaction test suite rather than variable-strength interaction test suite. In this paper, we propose two heuristic methods in order to prioritize variable-strength interaction test suite by taking advantage of its special characteristics. The experimental results show that our methods are more effective for variable-strength interaction test suite by comparing with the technique of prioritizing combinatorial interaction test suites according to test case generation order, the random test prioritization technique, and the fixed-strength interaction test suite prioritization technique. Besides, our methods have additional advantages compared with the prioritization techniques for fixed-strength interaction test suite.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649874,yes
435,Randomizing regression tests using game theory,"As software evolves, the number of test-cases in the regression test suites continues to increase, requiring testers to prioritize their execution. Usually only a subset of the test cases is executed due to limited testing resources. This subset is often known to the developers who may try to ??game? the system by committing insufficiently tested code for parts of the software that will not be tested. In this new ideas paper, we propose a novel approach for randomizing regression test scheduling, based on Stackelberg games for deployment of scarce resources. We apply this approach to randomizing test cases in such a way as to maximize the testers' expected payoff when executing the test cases. Our approach accounts for resource limitations (e.g., number of testers) and provides a probabilistic distribution for scheduling test cases. We provide an example application of our approach showcasing the idea of using Stackelberg games for randomized regression test scheduling.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693122,yes
436,Regression Testing Prioritization Based on Model Checking for Safety-Crucial Embedded Systems,"The order in which test-cases are executed has an influence on the rate at which faults can be detected. In this paper we demonstrate how test-case prioritization can be performed with the use of model-checkers. For this, different well known prioritization techniques are adapted for model-based use. New property based prioritization techniques are introduced. In addition it is shown that prioritization can be done at test-case generation time, thus removing the need for test-suite post-processing. Several experiments for safety-crucial embedded systems are used to show the validity of these ideas.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598153,yes
437,Research on optimization scheme of regression testing,"Regression testing is an important process during software development. In order to reduce costs of regression testing, research on optimization of scheme of regression testing have been done in this paper. For the purpose of reducing the number of test cases and detecting faults of programs early, this paper proposed to combine test case selection with test case prioritization. Regression testing process has been designed and optimization of testing scheme has been implemented. The criterion of test case selection is modify impact of programs, finding programs which are impacted by program modification according to modify information of programs and dependencies between programs. Test cases would be selected during test case selection. The criterion of test case prioritization is coverage ability and troubleshooting capabilities of test case. Test cases which have been selected during test case selection would be ordering in test case prioritization. Finally, the effectiveness of the new method is discussed.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818242,yes
438,Reusing black box test paths for white box testing of websites,"As the numbers of web users are increasing exponentially, the software complexity is increasing exponentially and the malwares are increasing exponentially, so exhaustive and extensive testing of websites has become a necessity today. But testing of a website is not 100% exhaustive as the page explosion problem is also very usual. In this paper, we propose to reuse the basis test paths as obtained from the Page-Test-Trees (PTTs) for white box testing of websites. We traverse the same set of paths (obtained above) and test for the source code at these nodes. This saves significant amount of time required to generate test paths and hence test cases as compared to the existing approaches of white box testing. The cost and efforts are also minimized. The proposed technique ensures better website testing coverage as white box testing provides better results than black box testing. Then we validate the proposed reusability testing with two web navigational structures. The results show that doing regression testing can save several billion dollars. These test cases can be further minimized by using prioritization techniques of regression testing.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514424,no
439,RisCal -- A Risk Estimation Tool for Software Engineering Purposes,"Decision making in software engineering requires the consideration of risk information. The reliability of risk information is strongly influenced by the underlying risk estimation process which consists of the steps risk identification, risk analysis and risk prioritization. In this paper we present a novel risk estimation tool for software engineering pruposes called RisCal. RisCal is based on a generic risk model and supports the integration of manually and automatically determined metrics into the risk estimation. This makes the tool applicable for arbitrary software engineering activities like risk-based testing or release planning. We show how RisCal supports risk identification, analysis and prioritizations, provide an estimation example, and discuss its application to risk-based testing and release planning.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619524,no
440,Selection and Prioritization of Test Cases by Combining White-Box and Black-Box Testing Methods,"In this paper, we present a methodology that combines both white-box and black-box testing, in order to improve testing quality for a given class of embedded systems. The goal of this methodology is generation of test cases for the new functional testing campaign based on the test coverage information from the previous testing campaign, in order to maximize the test coverage. Test coverage information is used for selection of proper test cases in order to improve the quality of testing and save available resources for testing. As an output, a set of test cases is produced. Generated test cases are processed by the test Executor application that decides whether results have passed or failed, based on the results of image grabbing, OCR text extraction, and comparison with expected text. The presented methodology is finally validated by means of a case-study targeting an Android device. The results of the case study are affirmative and they indicate that the proposed methodology is applicable for testing embedded systems of this kind.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664523,yes
441,SMT Malleability in IBM POWER5 and POWER6 Processors,"While several hardware mechanisms have been proposed to control the interaction between hardware threads in an SMT processor, few have addressed the issue of software-controllable SMT performance. The IBM POWER5 and POWER6 are the first high-performance processors implementing a software-controllable hardware-thread prioritization mechanism that controls the rate at which each hardware-thread decodes instructions. This paper shows the potential of this basic mechanism to improve several target metrics for various applications on POWER5 and POWER6 processors. Our results show that although the software interface is exactly the same, the software-controlled priority mechanism has a different effect on POWER5 and POWER6. For instance, hardware threads in POWER6 are less sensitive to priorities than in POWER5 due to the in order design. We study the SMT thread malleability to enable user-level optimizations that leverage software-controlled thread priorities. We also show how to achieve various system objectives such as parallel application load balancing, in order to reduce execution time. Finally, we characterize user-level transparent execution on POWER5 and POWER6, and identify the workload mix that best benefits from it.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138854,no
442,Software components prioritization using OCL formal specification for effective testing,"In soft real time system development, testing effort minimization is a challenging task. Earlier research has shown that often a small percentage of components are responsible for most of the faults reported at the later stages of software development. Due to the time and other resource constraints, fault-prone components are ignored during testing activity which leads to compromises on software quality. Thus there is a need to identify fault-prone components of the system based on the data collected at the early stages of software development. The major focus of the proposed methodology is to identify and prioritize fault-prone components of the system using its OCL formal specifications. This approach enables testers to distribute more effort on fault-prone components than non fault-prone components of the system. The proposed methodology is illustrated based on three case study applications.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844288,no
443,Software defect prediction using software metrics - A survey,"Traditionally software metrics have been used to define the complexity of the program, to estimate programming time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the metric values are to be used in mathematical equations designed to represent a model of the software process, metrics associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully apply. Work on the mechanics of implementing metrics programs. The goal of this research is to help developers identify defects based on existing software metrics using data mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in the development and maintenance phase. This research focuses in identifying defective modules and hence the scope of software that needs to be examined for defects can be prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology helps in identifying modules that require immediate attention and hence the reliability of the software can be improved faster as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the Data mining algorithm. To initiate this process we initially propose to evaluate the existing classification algorithms and based on its weakness we propose a novel Neural network algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508369,no
444,Suitable placements of multiple FACTS devices to improve the transient stability using trajectory sensitivity analysis,"Trajectory sensitivity analysis (TSA) is used as a tool for suitable placement of multiple series compensators in the power system. The goal is to maximize the benefit of these devices in order to enhance the transient stability of the system. For this purpose, the trajectory sensitivities of the rotor angles of the most critical generators with respect to the reactances of transmission lines are calculated in the presence of the most severe faults. Based on the obtained trajectory sensitivities, a method is proposed to determine how effective the series compensation of each transmission line is for improving the transient stability. This method is applied to the Nordic-32 test system to find the priorities of the transmission lines for installation of several series compensators. Simulation with industrial software shows the validity and efficiency of the proposed method.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666828,no
445,Test Case Prioritization for Continuous Regression Testing: An Industrial Case Study,"Regression testing in continuous integration environment is bounded by tight time constraints. To satisfy time constraints and achieve testing goals, test cases must be efficiently ordered in execution. Prioritization techniques are commonly used to order test cases to reflect their importance according to one or more criteria. Reduced time to test or high fault detection rate are such important criteria. In this paper, we present a case study of a test prioritization approach ROCKET (Prioritization for Continuous Regression Testing) to improve the efficiency of continuous regression testing of industrial video conferencing software. ROCKET orders test cases based on historical failure data, test execution time and domain-specific heuristics. It uses a weighted function to compute test priority. The weights are higher if tests uncover regression faults in recent iterations of software testing and reduce time to detection of faults. The results of the study show that the test cases prioritized using ROCKET (1) provide faster fault detection, and (2) increase regression fault detection rate, revealing 30% more faults for 20% of the test suite executed, comparing to manually prioritized test cases.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676952,yes
446,Test Case Prioritization Using Requirements-Based Clustering,"The importance of using requirements information in the testing phase has been well recognized by the requirements engineering community, but to date, a vast majority of regression testing techniques have primarily relied on software code information. Incorporating requirements information into the current testing practice could help software engineers identify the source of defects more easily, validate the product against requirements, and maintain software products in a holistic way. In this paper, we investigate whether the requirements-based clustering approach that incorporates traditional code analysis information can improve the effectiveness of test case prioritization techniques. To investigate the effectiveness of our approach, we performed an empirical study using two Java programs with multiple versions and requirements documents. Our results indicate that the use of requirements information during the test case prioritization process can be beneficial.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569743,yes
447,Test suite prioritisation using trace events technique,"The size of the test suite and the duration of time determines the time taken by the regression testing. Conversely, the testers can prioritise the test cases by the use of a competent prioritisation technique to obtain an increased rate of fault detection in the system, allowing for earlier corrections, and getting higher overall confidence that the software has been tested suitably. A prioritised test suite is more likely to be more effective during that time period than would have been achieved via a random ordering if execution needs to be suspended after some time. An enhanced test case ordering may be probable if the desired implementation time to run the test cases is proven earlier. This research work's main intention is to prioritise the regressiontesting test cases. In order to prioritise the test cases some factors are considered here. These factors are employed in the prioritisation algorithm. The trace events are one of the important factors, used to find the most significant test cases in the projects. The requirement factor value is calculated and subsequently a weightage is calculated and assigned to each test case in the software based on these factors by using a thresholding technique. Later, the test cases are prioritised according to the weightage allocated to them. Executing the test cases based on the prioritisation will greatly decreases the computation cost and time. The proposed technique is efficient in prioritising the regression test cases. The new prioritised subsequences of the given unit test suites are executed on Java programs after the completion of prioritisation. Average of the percentage of faults detected is an evaluation metric used for evaluating the 'superiority' of these orderings.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519507,yes
448,Using Dependency Structures for Prioritization of Functional Test Suites,"Test case prioritization is the process of ordering the execution of test cases to achieve a certain goal, such as increasing the rate of fault detection. Increasing the rate of fault detection can provide earlier feedback to system developers, improving fault fixing activity and, ultimately, software delivery. Many existing test case prioritization techniques consider that tests can be run in any order. However, due to functional dependencies that may exist between some test cases-that is, one test case must be executed before another-this is often not the case. In this paper, we present a family of test case prioritization techniques that use the dependency information from a test suite to prioritize that test suite. The nature of the techniques preserves the dependencies in the test ordering. The hypothesis of this work is that dependencies between tests are representative of interactions in the system under test, and executing complex interactions earlier is likely to increase the fault detection rate, compared to arbitrary test orderings. Empirical evaluations on six systems built toward industry use demonstrate that these techniques increase the rate of fault detection compared to the rates achieved by the untreated order, random orders, and test suites ordered using existing ""coarse-grained? techniques based on function coverage.",2013,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189361,yes
449,A code coverage-based test suite reduction and prioritization framework,"Software testing is extensively used to ensure the development of a quality software system. The test suite size tends to increase by including new test cases due to software evolution. Consequently, the entire test suite cannot be executed considering budget and time limitations. Researchers have examined test suite reduction and prioritization techniques to address the test suite size problem. However, combination of these techniques can be useful for various regression testing situations. In this paper, we present a new code coverage-based test suite reduction and prioritization framework called TestOptimizer. The framework performs a suitable combination of TestFilter and St-Total techniques to determine optimal test cases, keeping in view of time restrictions. The performance of the proposed framework has been assessed using a case study. Results show that TestOptimizer can be beneficial to solve the test suite size problem within time constraints and has a profound impact on the required cost and effort of regression testing.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076910,yes
450,A Comparison of Test Case Prioritization Criteria for Software Product Lines,"Software Product Line (SPL) testing is challenging due to the potentially huge number of derivable products. To alleviate this problem, numerous contributions have been proposed to reduce the number of products to be tested while still having a good coverage. However, not much attention has been paid to the order in which the products are tested. Test case prioritization techniques reorder test cases to meet a certain performance goal. For instance, testers may wish to order their test cases in order to detect faults as soon as possible, which would translate in faster feedback and earlier fault correction. In this paper, we explore the applicability of test case prioritization techniques to SPL testing. We propose five different prioritization criteria based on common metrics of feature models and we compare their effectiveness in increasing the rate of early fault detection, i.e. a measure of how quickly faults are detected. The results show that different orderings of the same SPL suite may lead to significant differences in the rate of early fault detection. They also show that our approach may contribute to accelerate the detection of faults of SPL test suites based on combinatorial testing.",2014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823864,yes