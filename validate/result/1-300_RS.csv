ID,Document Title,Abstract,Year,PDF Link,code
1,Application of Decision Theory to the Testing of Large Systems,"A methodology for determining priorities in allocating test resources among the various subsystems within a large system is described. The methodology is based on concepts from applied decision theory. Two versions of the methodology are presented: a complete version, called the extensive form, and an approximate version, called the diminutive form.",1971,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4103708,yes
2,Multiaccess in a Nonqueueing Mailbox Environment,"A new and flexible solution to the problem of multiple users accessing a single resource, such as communication bandwidth or composite object in memory, is derived. The means of communication consists of sending and receiving messages in known locations (or equivalently, mailboxes without queueing). Any particular user is able to deposit, and hence destroy, previous messages in a mailbox. It is assumed that exclusive access to a mailbox is supplied by an underlying system. The major results of this paper are: 1) a simple tree-based algorithm that guarantees ƒ?› no user or group of users can conspire to prevent access by some other user to the resource; ƒ?› only one user accesses the resource at a time; ƒ?› if there are N users, an individual user is guaranteed access, when requested, to the resource in no more than N-1 turns; Knuth's solution [6] can delay a user up to 2** (N-1)-1 turns; 2) an extension of Dekker's algorithm (2 users) [2] that allows the relative rates of reservations for access to the resource to be proportional to a set of N integers. When a reservation is not being used by its ``owner,'' it will be assigned to another contending request. The assignment is optimal for periodic requests.",1984,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5010232,no
3,Practical Priorities in System Testing,"During the system test phase, ""thorough testing"" can pass the limits of practicality. Test case selection, based on simple priority rules, is one solution to the problem of practicality vs. thoroughness.",1985,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1695401,yes
4,Testing and Evaluation of the Defense Data Network,"The purpose of this document is to establish a concept for test and evaluation (T&E) for the Defense Data Network (DDN) as a whole. This concept is based on the requirements of the DDN in its evolving configurations (e.g., MILNET, SACDIN, DODIIS, ..., DDN); the goals and priorities of the DDN DCS DSD; the needs of the various DDN user communities; knowledge of the ARPANET technology; and experience in T&E of systems and independent verification, validation, and test (IVV&T) of software and hardware. Since 1983 Intermetrics has been a contractor for Independent Validation and Verification (IV&V) support to the Testing branch of the Defense Data Network Defense Communications System Data Systems Directorate (DDN DCS DSD).",1987,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4795240,no
5,Formal methods and iterative design,"Formal methods allow a system designer to describe precisely how the system will be. In the area of human-computer interaction this means describing the structure and detail of the user interface. Giving the designer tools to think clearly about the decisions made must be helpful. However, the problem of deciding how the user interface is to be designed remains. In particular, to use such tools effectively a designer must have a good understanding of how the users think about the task, what their expectations and priorities are. This kind of information is best obtained by user testing with prototypes. An initial design is put forward and then refined using feedback from typical users doing typical tasks with a prototype. This refinement should happen in parallel with the refinement of a formal model of the user interface by the application of domain independent principles. In the early stages the cost of this procedure can be minimised by using simulations and mock ups rather than full prototypes. This is known as iterative design. This paper describes, through an example, some techniques which can be used to get insights about how a user approaches a task and the difficulties they have with a particular prototype.<<ETX>>",1988,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=209309,no
6,Real-time adaptive control of knowledge based avionics tasks,"Advanced decision-making capabilities are being developed to aid the pilots of the next generation of tactical fighters. Due to the limited processing resources available in an avionics suite, efforts have focused on developing a distributed fault-tolerant software architecture that permits the real-time prioritization and scheduling of these tasks. The authors outline the design details of an architecture under development to meet these performance requirements. The system has been tested with a threat-avoidance system, implemented on a testbed of five internetted LISP workstations, to evaluate overall system capabilities including scheduling, task operations, and database accesses. It has a simulation cycle of 50 ms and synchronization between distributed nodes can be achieved within 2 ms. This test case has nine knowledge tasks, one of which is defined as a simulation cycle that drives the test case. This system has been evaluated with the current trace capabilities and runs with a peak of 16 task instances active at any time.<<ETX>>",1989,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=40357,no
7,Group decision support and multiple criteria optimization,The paper proposes a structured group decision aid based on multiple criteria optimization. The procedure is designed to solve optimization problems which involve conflicting objectives and multiple decision-makers with different priorities. Most of the empirical findings regarding the performance of multi criteria techniques involve a single decision-maker. The focus in these algorithms is on determining a compromise solution to a multicriteria problem which best coincides with the preference structure of a decision-maker. The approach taken is to imbed the task of finding a compromise solution in a more general and flexible framework. The underlying concept in this framework is the analytic hierarchy process and the Tchebycheff algorithm is used to solve the multiple criteria problem. The objectives and the alternative solutions to the multiple criteria problem are evaluated through the analytic hierarchy process.<<ETX>>,1991,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=184201,no
8,Setting maintenance quality objectives and prioritizing maintenance work by using quality metrics,"Metrics that are collected and validated during development can be used during maintenance to control quality and prioritize maintenance work. Validity criteria are defined mathematically. The approach is based on validating selected metrics against related quality factors during development and using the validated metrics during maintenance to: establish initial quality objectives and quality control criteria and prioritize software components (e.g., module) and allocate resources to maintain them. The author illustrates both a case of passing a validation test (discriminative power) and failing a validation test (tracking).<<ETX>>",1991,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=160337,no
9,Specification criticism via goal-directed envisionment,"Validating a complex system specification is a difficult problem. Generating behaviors and using them to critique a specification is one effective approach. Up until now, symbolic evaluation has been the key technique of behavior generation. Unfortunately, it has drawbacks both in the amount of time it takes to complete a symbolic run, and in the large amount of uninteresting data it produces. The authors propose goal-directed envisionment as an alternative to symbolic evaluation, supplementing the basic envisioning techniques of qualitative physics with behavioral goals. This approach overcomes the problems of symbolic evaluation by generating interpretations in a reasonable amount of time and by exploiting goals to prioritize and analyze the interpretations. The authors describe and evaluate SC, an implemented system which employs goal-directed envisionment to critique specifications.<<ETX>>",1991,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=213080,no
10,Quality Function Deployment (QFD) in testing,"QFD is a quality-oriented process that can play an important role in the market-driven, total quality control environment. It can be deployed in almost all areas of product development, test and manufacturing processes. QFD is one way to ensure the reliability of the software products. The author is concerned with the role that software testing can play in increasing the reliability of software. He also examines how Quality Function Deployment (QFD) could be used to achieve this objective. QFD provides integration of the various functions by tying design and process activities together. It priorities product and manufacturing-process characteristics and highlights areas which would require further analysis.<<ETX>>",1992,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=168144,no
11,A method for automatic evaluation of fault effects in the advanced intelligent network,"The method described here provides network operators with criteria for deciding the priorities with which services degraded by network faults should be restored. This method consists of four processes: the first lists the unavailable services of customers affected by the fault, the second predicts the mean time needed to repair the fault, the third predicts the traffic trends for the affected services, and the fourth calculates the criteria used to decide the service restoration priorities. This method enables network operators to choose suitable means restoring services and to avoid future congestion due to faults. It also lets them respond more quickly and precisely to customer claims. We are introducing this method into our advanced IN operations systems.<<ETX>>",1993,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318129,no
12,New probabilistic measures for accelerating the automatic test pattern generation algorithm,"In order to approximate the signal controllabilities, the authors introduce new probabilistic measures called signal priorities, whose computation relies on the minimum-value distributions of fanout input variables of a digital circuit. The signal priorities serve the same purpose as do the signal controllabilities. That is, they are used to accelerate the automatic test pattern generation algorithm; however, their computation requires much less effort. This new method is formally defined and tested with several practical example circuits.<<ETX>>",1993,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396314,no
13,Cost-benefit analysis of electric power system reliability,"The purpose of this investigation is to determine an appropriate cost-benefit formula that will help the power system planners in prioritizing transmission system projects. This paper deals with describing the value of increased reliability and security in bulk power systems. Three different approaches used for prioritization of transmission system projects by the electric utilities are discussed and analyzed for two different transmission system alternatives. Utilizing the best approach, transmission alternatives are prioritized and the best alternative is placed on top of the prioritized table. An analysis of the three approaches and a relative comparison is performed on the IEEE 25 bus Reliability Test System. TRELSS (Transmission Reliability Evaluation of Large Scale Systems), a software package developed by EPRI, is utilized in determining the probabilistic indices that are used in the proposed approach.<<ETX>>",1994,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=287831,no
14,Priority based data flow testing,"Software testing is an expensive component of software development and maintenance. For data flow testing, test cases must be found to test the def-use pairs in a program. Since some of the def-use pairs identified through static analysis may be infeasible, no amount of testing effort may result in exhaustive testing of a program. Therefore in practice a fixed amount of effort is spent in testing a program. We develop an approach for assigning priorities to def-use pairs, such that the def-use pairs with higher priorities can be expected to require less effort for test case generation and therefore testing. Thus, by using the priorities as a guide for ordering the def-use pairs for testing, we can maximize the number of def-use pairs tested using a fixed amount of testing effort. We apply the technique to regression testing during the software maintenance phase, in which case the priorities are assigned to capture not only the difficulty in test case generation but also the likelihood that an error introduced by a program change is uncovered by the test case.",1995,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526556,yes
15,The peer tasking design method,"This paper is a preliminary report of an ARPA sponsored study. It focuses on designing real-time command and control or battle management systems for parallel and distributed architectures. Due to delays in other ARPA programs, the targeted architectures were not available during the time frame of the study. The results of the study were, however, tested on more conventional sequential and parallel platforms. The design method discussed here is fundamentally different from those assumed by current real-time scheduling theories, e.g., rate-monotonic, earliest-deadline-first, least-laxity or best-effort. These theories assume that the fundamental unit of prioritization is the task. In this new method, the fundamental unit of prioritization is called a work item. Work items are functions the system performs that have timing requirements (deadlines) associated with them in the requirements specification. Current scheduling theories are applied using artifact deadlines introduced by designers whereas this new method schedules work items to meet specification deadlines (sometimes called end-to-end deadlines) required by the user. With this method, tasks have no priorities. A collection of tasks with no priorities will be called a collection of peer tasks. The study showed that it is possible to schedule work items based on importance rather than urgency while still meeting as many work item deadlines as can be met by scheduling tasks with respect to urgency. Also, it showed that the minimum on-line deadline that can be guaranteed for a work item of highest importance, scheduled at run-time, is approximately the inverse of the throughput, measured in work items per second, for a work load consisting only of work items of that type. Further, it was shown that it provides optimal utilization of a single processor machine, and that its timing behavior is predictable (provable) for both single and multiprocessor machines. Finally, it was shown that throughput is not degraded during overload.<<ETX>>",1995,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470511,no
16,High-level synthesis of recoverable microarchitectures,Two algorithms that combine the operations of scheduling and recovery point insertion for high-level synthesis of recoverable microarchitectures are presented. The first uses a prioritized cost function in which functional unit cost is minimized first and register cost second. The second algorithm minimizes a weighted sum of functional unit and register costs. Both algorithms are optimal according to their respective cost functions and require less than 10 minutes of CPU time on widely-used high-level synthesis benchmarks. The best previous result reported several hours of CPU time for some of the same benchmarks.,1996,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494128,no
17,Selecting engineering techniques using fuzzy logic based decision support,"The task of selecting software engineering methods, techniques, metrics, and tools is usually performed manually, based on the expertise of individuals. This paper presents a systematic tool supported approach, that bases its suggestions an the technical situation, the existing goals, and constraints of a specific organization or a particular project. A prototype of the decision support system supports the elaboration of test strategies. The approach uses information about the technical situation that is provided by answering predefined questions with fuzzy data. The objective is to assign ""adequacy values"" to combinations of test methods, techniques, metrics, tools, and quantified test situations. The priorities of goals and constraints have assessed by applying a technique that is based on comparing goals in pairs. This permits to check certain consistency criteria by static analysis. A hierarchy of the importance of goals and constraints is calculated, which provides the basis for the determination of the suitability of test methods; techniques, metrics, and tools with respect to goals and constraints.",1996,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494570,no
18,A high-level synthesis approach to design of fault-tolerant systems,"Fault-tolerance in embedded systems is a requirement of increasing importance; solutions must achieve a balance between performances and costs that was not usually requested in design of more classical fault-tolerant applications and that involves as a consequence new approaches. A design technique is here proposed supporting fault-tolerance of hardware modules in complex hardware-software systems, fault-tolerance requirements for each hardware-mapped process are specified in terms of time constraints and of relative priorities, and a high-level synthesis methodology allowing to design - for each process - a processor capable of supporting both the nominal execution of the process itself in a fault-free environment and simultaneous execution of a reconfigured pair of processes in a fault-affected environment is defined Performances of the scheduling algorithm, allowing to achieve reconfiguration with minimum resource increase and within the required limits of speed degradation, are evaluated on some relevant instances of algorithms discussed in current literature on high-level synthesis.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=600305,no
19,A multi-platform support environment,"Legacy weapon systems, such as attack aircraft, have taken advantage of embedded computers and software to provide enormous capabilities for flexibility and expandability. The provision of these capabilities has been at a cost, and that is in the dedicated software development facilities which have sprung up to support these legacy systems. Unfortunately, the costs of these dedicated facilities is becoming prohibitive. The Advanced Avionics Multi-Radar Software Support Study (AAMRSSS) project offers experience in handling the above problem. AAMRSSS studied the feasibility of using a dedicated Software Development Facility (SDF) to support multiple software system platforms. Issues of the study were: commonality; unique requirements of the new system to be added; platform priorities; and future expansion. In particular, this study has addressed supporting the AC-130U Gunship's Radar Operational Flight Program (OFF) in the F-15's Radar Software Development Facility.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=622734,no
20,A study of effective regression testing in practice,"The purpose of regression testing is to ensure that changes made to software, such as adding new features or modifying existing features, have not adversely affected features of the software that should not change. Regression testing is usually performed by running some, or all, of the test cases created to test modifications in previous versions of the software. Many techniques have been reported on how to select regression tests so that the number of test cases does not grow too large as the software evolves. Our proposed hybrid technique combines modification, minimization and prioritization-based selection using a list of source code changes and the execution traces from test cases run on previous versions. This technique seeks to identify a representative subset of all test cases that may result in different output behavior on the new software version. We report our experience with a tool called ATAC (Automatic Testing Analysis tool in C) which implements this technique.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630875,yes
21,A systematic tradeoff analysis for conflicting imprecise requirements,"The need to deal with conflicting system requirements has become increasingly important over the past several years. Often, these requirements are elastic in that they can be satisfied to a degree. The overall goal of this research is to develop a formal framework that facilitates the identification and the tradeoff analysis of conflicting requirements by explicitly capturing their elasticity. Based on a fuzzy set theoretic foundation for representing imprecise requirements, we describe a systematic approach for analyzing the tradeoffs between conflicting requirements using the techniques in decision science. The systematic tradeoff analyses are used for three important tasks in the requirement engineering process: (1) for validating the structure used in aggregating prioritized requirements, (2) for identifying the structures and the parameters of the underlying representation of imprecise requirements and (3) for assessing the priorities of conflicting requirements. We illustrate these techniques using the requirements of a conference room scheduling system.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566845,no
22,An optimal algorithm for scheduling soft aperiodic tasks in dynamic-priority preemptive systems,"The paper addresses the problem of jointly scheduling tasks with both hard and soft real time constraints. We present a new analysis applicable to systems scheduled using a priority preemptive dispatcher, with priorities assigned dynamically according to the EDF policy. Further, we present a new efficient online algorithm (the acceptor algorithm) for servicing aperiodic work load. The acceptor transforms a soft aperiodic task into a hard one by assigning a deadline. Once transformed, aperiodic tasks are handled in exactly the same way as periodic tasks with hard deadlines. The proposed algorithm is shown to be optimal in terms of providing the shortest aperiodic response time among fixed and dynamic priority schedulers. It always guarantees the proper execution of periodic hard tasks. The approach is composed of two parts: an offline analysis and a run time scheduler. The offline algorithm runs in pseudopolynomial time O(mn), where n is the number of hard periodic tasks and m is the hyperperiod/min deadline.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601081,no
23,Design and implementation of a real-time switch for segmented Ethernets,"Providing network bandwidth guarantees over an Ethernet requires coordination of the network nodes for traffic prioritization such that real-time data can have deterministic access to the network. We have shown previously how RETHER, a software based token passing protocol can efficiently provide real-time support over a single shared Ethernet segment. This work extends the token passing mechanism into a switched, multi-segment Ethernet environment. This paper describes the detailed design issues, their solutions, and a fully operational switch implementation built into the FreeBSD kernel. By testing the protocol independently and as the underlying network protocol of the Stony Brook Video Server, we have verified that the bandwidth guarantees are successfully provided, with relatively low run-time overhead, for real-time connections crossing multiple Ethernet segments. This paper also provides a comprehensive performance evaluation of the prototype.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643709,no
24,Failproof team projects in software engineering courses,"The computer science department of the University of Pittsburgh offers two undergraduate and two graduate courses in software engineering in which we emphasize the importance of general engineering principles for software development. For the last ten years the undergraduate courses have been based on team projects. This structure has advantages: students see immediately the relevance of what they learn, and the team setting leads to a better understanding of what they learn. The projects in the two courses are of different types. In one course the result is the formal specification and design of a software system. In the other, the teams implement such a system, but emphasis is on testing rather than on the implementation itself. The success of each project is guaranteed by making it open-ended. A team establishes a list of priorities that is to ensure that a useful product will have been built by the time the term ends. We discuss the nature of team projects, and our evaluation scheme.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=636027,no
25,Prioritizing Software Requirements In An Industrial Setting,,1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=610408,no
26,READS: a prototyping environment for real-time active applications,"We present our Real-time Active Database System, READS, which is a prototyping environment for real-time active database applications on a conventional Unix environment, e.g., Solaris 2.4. In READS, transactions are associated with deadlines and priorities. Priority scheduling is supported by the real-time extensions provided in the underlying operating system. READS can be served as a testbed for evaluating different issues in the design of real-time active database systems (RTADBS). Different approaches for assigning deadlines and priorities to triggered transactions have been suggested and discussed. An application, the programmed trading database system, is implemented with READS and experiments have been performed to study the impact of the deadline constraints on the performance of the deferred and immediate coupling modes for triggering.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617285,no
27,Stochastic Petri nets applied to the performance evaluation of static task allocations in heterogeneous computing environments,"A stochastic Petri net (SPN) is systematically constructed from a task graph whose component subtasks are statically allocated onto the processor suite of a heterogeneous computing system (HCS). Given that subtask execution times are exponentially distributed an exponential distribution can be generated for the overall completion time. In particular the enabling functions and rate functions used to specify the SPN model provide needed versatility to integrate processor heterogeneity, task priorities, allocation schemes, communication costs, and other factors characteristic of a HCS into a comprehensive performance analysis. The manner in which these parameters are incorporated into the SPN allows the model to be transformed into a testbed for optimization schemes and heuristics. The proposed approach can be applied to arbitrary task graphs including non-series-parallel.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=581420,no
28,Testing for millennium risk management,"The Year 2000 conversion is a challenge to the economics of both testing and maintenance, but as a whole we are not responding with balanced priorities. Those looking for Year 2000 solutions typically allocate their first energies and budgets to acquiring automated analysis and conversion deals and services. This disturbing tendency ignores two facts that surface in virtually every analysis of the Year 2000 challenge: fifty percent or more of the effort will be in testing; and despite consuming a wealth of resources each year, current testing practices cannot satisfy the demands of current maintenance unrelated to the Y2K conversion. For the moment, most organizations continue to delay action on the Y2K Test problem while wading through the dozens of available analysis and conversion solutions. As a result, many Y2K projects have started on master plans that will need major revision once the true needs and benefits of testing automation become apparent. A growing number of those projects have already corrected course, revising strategy and reallocating budgets once they appreciated the nature of the Y2K testing challenge. Embarrassment will probably be the least of many worries for those that ignore the challenge much longer.",1997,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=589256,no
29,Derivation of an integrated operational profile and use case model,"Requirements engineering and software reliability engineering both involve model building related to the usage of the intended system; requirements models and test case models respectively are built. Use case modelling for requirements engineering and operational profile testing for software reliability engineering are techniques which are evolving into software engineering practice. Approaches towards integration of the use case model and the operational profile model are proposed. By integrating the derivation of the models, effort may be saved in both development and maintenance of software artifacts. Two integration approaches are presented, transformation and extension. It is concluded that the use case model structure can be transformed into an operational profile model adding the profile information. As a next step, the use case model can be extended to include the information necessary for the operational profile. Through both approaches, modelling and maintenance effort as well as risks for inconsistencies can be reduced. A positive spin-off effect is that quantitative information on usage frequencies is available in the requirements, enabling planning and prioritizing based on that information.",1998,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730843,no
30,Prioritized token-based mutual exclusion for distributed systems,"A number of solutions have been proposed for the problem of mutual exclusion in distributed systems. Some of these approaches have since been extended to a prioritized environment suitable for real-time applications but impose a higher message passing overhead than our approach. We present a new protocol for prioritized mutual exclusion in a distributed environment. Our approach uses a token-based model working on a logical tree structure, which is dynamically modified. In addition, we utilize a set of local queues whose union would resemble a single global queue. Furthermore, our algorithm is designed for out-of-order message delivery, handles messages asynchronously and supports multiple requests from one node for multi-threaded nodes. The prioritized algorithm has an average overhead of O(log(n)) messages per request for mutual exclusion with a worst-case overhead of O(n), where n represents the number of nodes in the system. Thus, our prioritized algorithm matches the message complexity of the best non-prioritized algorithms while previous prioritized algorithms have a higher message complexity, to our knowledge. Our concept of local queues can be incorporated into arbitrary token-based protocols with or without priority support to reduce the amount of messages. Performance results indicate that the additional functionality of our algorithm comes at the cost of 30% longer response times within our test environment for distributed execution when compared with an unprioritized algorithm. This result suggests that the algorithm should be used when strict priority ordering is required.",1998,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670018,no
31,Software testability measurements derived from data flow analysis,"The purpose of the research is to develop formulations to measure the testability of a program. Testability is a program's property which is introduced with the intention of predicting efforts required for testing the program. A program with a high degree of testability indicates that a selected testing criterion could be achieved with less effort and the existing faults can be revealed more easily during testing. We propose a new program normalization strategy that makes the measurement of testability more precise and reasonable. If the program testability metric derived from data flow analysis could be applied at the beginning of a software testing phase, much more effective testing of resource allocation and prioritizing is possible.",1998,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=665760,no
32,Scenario based integration testing for object-oriented software development,"The adaptive use case methodology for software development proposed by Carlson and Hurlbutt [1997, 1998] forms the backdrop for this paper. Their methodology integrates the software design, development and testing processes through a series of design preserving, algorithmic transformations. This paper focuses on the software testing metrics used in the generation of object oriented test plans as part of that methodology. During the design phase, interaction diagrams are developed from which use case action matrices are then generated. A use case action matrix contains a collection of related scenarios each describing a specific variant of an executable sequence of use case actions. The proposed software testing metrics are employed to improve the productivity of the testing process through scenario prioritization.",1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=810764,yes
33,"Static properties of commercial embedded real-time programs, and their implication for worst-case execution time analysis","We have used a modified C compiler to analyze a large number of commercial real time and embedded applications written in C for 8- and 18-bit processors. Only static aspects of the programs have been studied i.e., such information that can be obtained from the source code without running the programs. The purpose of the study is to provide guidance for the development of worst-case execution time (WCET) analysis tools, and to increase the knowledge about embedded programs in general. Knowing how real programs are written makes it easier to focus research in relevant areas and set priorities. The conclusion is that real time and embedded programs are not necessarily simple just because they are written for small machines. This indicates that real life WCET analysis tools need to handle advanced programming constructions, including function pointer calls and recursion.",1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777660,no
34,Static-priority scheduling of multiframe tasks,The multiframe model of hard-real-time tasks is a generalization of the well-known periodic task model of C. Liu and J. Layland (1973). The feasibility analysis of systems of multiframe tasks which are assigned priorities according to the rate-monotonic priority assignment scheme is studied. An efficient sufficient feasibility test for such systems of multiframe tasks is presented and proved correct-this generalizes a result of A.K. Mok and D. Chen (1997).,1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777448,no
35,Teenagers identify causes of violence in schools and develop strategies to eliminate violence using GroupSystems electronic meeting system (EMS),"Is it possible to engage teenagers in a serious effort to identify the root causes of school violence and to develop strategies to deal with it? If so, will computer-aided group decision support tools add value to the process? Those are the questions we addressed with the 1998 Teen Think Tank on School Violence. While this was neither a formal nor a scientific treatment of the subject, the results of the initial experiment were overwhelmingly impressive. Using the GroupSystems Electronic Meeting System (EMS), sixteen teenagers grappled with the issue of school violence and generated more than 800 different ways to predict, prevent, avoid, protect, react, eliminate, or cope with youth violence. After brainstorming for ideas, they also used EMS to categorize, prioritize, and to reach consensus about their best ideas. Then they developed teen recommendations for students, parents, teachers, school administrators, and law enforcement officers. This was all accomplished in two EMS sessions; and none of the students had any prior knowledge or experience with EMS. This report presents a synopsis of their findings and a brief description of the EMS process.",1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=772733,no
36,Test case prioritization: an empirical study,Test case prioritization techniques schedule test cases for execution in an order that attempts to maximize some objective function. A variety of objective functions are applicable; one such function involves rate of fault detection-a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during regression testing can provide faster feedback on a system under regression test and let debuggers begin their work earlier than might otherwise be possible. In this paper we describe several techniques for prioritizing test cases and report our empirical results measuring the effectiveness of these techniques for improving rate of fault detection. The results provide insights into the tradeoffs among various techniques for test case prioritization.,1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792604,yes
37,The far ultraviolet spectroscopic explorer (FUSE) instrument data system,"This paper describes the architecture for the IDS flight hardware and its real-time embedded flight software. The design uses commercial off-the-shelf (COTS) software components as much as possible, to reduce cost and software development time. The features of the IDS design that provide radiation hardness and fault tolerance are described. Implementation of software to meet the functional requirements is accomplished using a relatively small number of prioritized real-time tasks. A commercial real-time operating system kernel manages and supports these tasks. Inter-task communication is described, as are the software test and validation methods. The paper shows how custom ground support equipment was developed to facilitate software development and testing. Reliable communications between the IDS and the FUSE spacecraft bus are accomplished using a MIL-STD-1553B bus that has an imposed, deterministic real-time protocol. Similarly, communication between the IDS and the other instrument subsystems uses a second MIL-STD-1553B bus that has its own time division multiplex real-time protocol. The design of these real-time protocols is described, with particular attention to reliability and testability.",1999,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=821995,no
38,A physics/engineering of failure based analysis and tool for quantifying residual risks in hardware,"NASA Code Q is supporting efforts to improve the verification and validation and the risk management processes for spaceflight projects. A physics-of-failure based Defect Detection and Prevention (DDP) methodology previously developed has been integrated into a software tool and is currently being implemented on various NASA projects and as part of NASA's new model-based spacecraft development environment. The DDP methodology begins with prioritizing the risks (or failure modes, FMs) relevant to a mission which need to be addressed. These risks can be reduced through the implementation of a set of detection and prevention activities referred to herein as PACTs (preventative measures, analyses, process controls and tests). Each of these PACTs has some effectiveness against one or more FMs but also has an associated resource cost. The FMs can be weighted according to their likelihood of occurrence and their mission impact should they occur. The net effectiveness of various combinations of PACTs can then be evaluated against these weighted FMs to obtain the residual risk for each of these FMs and the associated resource costs to achieve these risk levels. The process thus identifies the project-relevant ""tall pole"" FMs and design drivers and allows real time tailoring with the evolution of the design and technology content. The DDP methodology allows risk management in its truest sense: it identifies and assesses risk, provides options and tools for risk decision making and mitigation and allows for real-time tracking of current risk status.",2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816338,no
39,Architecture of the simplified Chinese embedded system STARTH,"There is a trend for the information products that are integrated by computer, communication, and consumer electronics. The OS is required more compact and practical. An embedded system STARTH is developed based on the core of the Motorola PPSM that is a real time 32-bit kernel with prioritized interrupt scheduling. All tasks are interrupt-driven. The PPSM kernel does not access hardware device directly. The kernel controls all peripherals indirectly, through software device drivers. The PPSM tools consist of pen input, graphics, database, text, character input, system and communication. The PPSM toolsets, together with its device drivers, provides the basic contro of the LCD, the drawing functions, the real time clock and the UART. The architecture of the embedded system STARTH on the Dragon Ball EZ platform is discussed in parts in details. The development environments of both software and hardware are described. The system is analyzed from its initialization, registration to system management, even the applications programming. The STARTH is tested and run on the hardware system of Dragon Ball platform. It is found that STARTH is practical and reliable for personal information devices.",2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=843622,no
40,Building an IP network quality-of-service testbed,The Drexel Network Toolkit is a software package for testing various approaches to QoS on IP-based networks. It uses Linux and DiffServ packet-marking primitives to classify and prioritize packets. DNT was used in a project to evaluate satellite based IP delivery for multimedia applications in telemedicine and telemaintenance.,2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=865089,no
41,GBS IP multicast tunneling,"Internet protocol (IP) multicast over the DoD's Global Broadcast Service (GBS) provides a one-way communications path to the warfighter as part of an interactive communications system. GBS provides the capability to deliver information products of varying size, timeliness requirements, and security levels. The products share satellite resources based on CINC/JTF commanders priorities, operational locations, availability of the satellite resources, and platform capabilities of the deployed users. Currently, IP multicast (IPMC) is not supported by the Defense Information Infrastructure (DII). This document defines a real-time IP solution for tunneling multicast data across the Defense Information Services Network (DISN), through GBS components, and to the end-user LANs. This paper describes a cost-effective, operationally sound, incremental approach for rapid prototyping and integration of these technologies. It outlines each of the implementation approaches researched to date, the designs selected for systematic testing, and an overview of the plan for testing selected designs. The initial architecture is based on commercial-off-the-shelf IP routers, ATM switches, multicast software, and VPN techniques.",2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=904019,no
42,Generating test cases for GUI responsibilities using complete interaction sequences,"Testing graphical user interfaces (GUI) is a difficult problem due to the fact that the GUI possesses a large number of states to be tested, the input space is extremely large due to different permutations of inputs and events which affect the GUI, and complex GUI dependencies which may exist. There has been little systematic study of this problem yielding a resulting strategy which is effective and scalable. The proposed method concentrates upon user sequences of GUI objects and selections which collaborate, called complete interaction sequences (CIS), that produce a desired response for the user. A systematic method to test these CIS utilizes a finite-state model to generate tests. The required tests can be substantially reduced by identifying components of the CIS that can be tested separately. Since consideration is given to defects totally within each CIS, and the components reduce required testing further, this approach is scalable. An empirical investigation of this method shows that substantial reduction in tests can still detect the defects in the GUI. Future research will prioritize testing related to the CIS testing for maximum benefit if testing time is limited.",2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885865,no
43,The Digital Library for Earth System Education: implementing the DLESE Community Plan,"Over the past year, geoscience educators, librarians and information technologists have made substantial progress in initiating the construction ofa Digital Library for Earth System Education (DLESE). Two major efforts, the Portal to the Future Workshop and the Geoscience Digital Library (GDL) project, have established a vision for the library, a governance process to enable community ownership, management, and construction, and have begun development of a testbed collection, discovery system, and user interface. The DLESE Community Plan lays out in detail the need for this facility, a community vision for its goals and priorities, and a strategy for initial construction of the library. From this initial work, two conclusions emerge as paramount in moving forward with the library. First, it is essential that development of the library community and the building of the technological infrastructure for the library go hand in hand. Second, the library will be most effectively built as a highly coordinated, but distributed community effort. In this way, the full range of talents in the community can be leveraged and rapid development of the library is possible. This paper briefly reviews our technical accomplishments to date and outlines their plans for further development. Progress and plans can be charted in the following three areas: 1. Community-centered design and use case development 2. Discovery system, metadata, and collection testbeds 3. System architecture and interoperability.",2000,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860416,no
44,A hybrid genetic algorithm for generating optimal synthetic aperture radar target servicing strategies,"The purpose of this research was to develop a software tool for generating optimal target servicing strategies for imaging fixed ground targets with a spaceborne SAR. Given a list of targets and their corresponding geographic locations and relative priorities, this tool generates a target servicing strategy that maximizes the overall collection utility based on the number of targets successfully imaged weighted by their relative priorities. This tool is specifically designed to maximize sensor utility in the case of a target-rich environment. For small numbers of targets, a target servicing strategy is unnecessary, and the targets may be imaged in any order without paying any particular attention to geographic proximity or target priority. However, for large, geographically diverse target decks, the order in which targets are serviced is of great importance. The target servicing problem is shown to be of the class NP-hard, and thus cannot be solved to optimality in polynomial time. Therefore, global search techniques such as genetic algorithms are called for. A unique hybrid algorithm that combines genetic algorithms with simulated annealing has been developed to generate optimized target servicing strategies. The performance of this hybrid algorithm was compared against that of three different greedy algorithms in a series of 20 test cases. Preliminary results indicate consistent performance improvements over greedy algorithms for target-rich environments. Over the course of 20 trials, the hybrid optimizing algorithm produced weighted collection scores that were on average 10% higher than the best greedy algorithm.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931250,no
45,A selective software testing method based on priorities assigned to functional modules,"As software systems have been introduced to many advanced applications, the size of software systems increases so much. Simultaneously, the lifetime of software systems becomes very small and thus their development is required within a relatively short period. We propose a novel selective software testing method that aims to attain the requirement of short period development. The proposed method consists of 3 steps: assign priorities to functional modules (Step 1), derive a test specification (Step 2), and construct a test plan (Step 3) according to the priorities. In Step 1, for development of functional modules, we select both product and process properties to calculate priorities. Then, in Step 2, we generate detailed test items for each module according to its priority. Finally, in Step 3, we manage test resources including time and developer's skill to attain the requirement. As a result of experimental application, we can show the superiority of the proposed testing method compared to the conventional testing method.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990028,yes
46,Air traffic control improvement using prioritized CSMA,"Version 7 simulations of the industry-standard network simulation software ""OPNET"" are presented of two applications of the Aeronautical Telecommunications Network (ATN), Controller Pilot Data Link Communications (CPDLC) and Automatic Dependent Surveillance-Broadcast mode (ADS-B), over VHF Data Link mode 2 (VDL-2). Communication is modeled for air traffic between just three cities. All aircraft are assumed to have the same equipage. The simulation involves Air Traffic Control (ATC) ground stations and 105 aircraft taking off, flying realistic free-flight trajectories, and landing in a 24-hr period. All communication is modeled as unreliable. Collision-less, prioritized carrier sense multiple access (CSMA) is successfully tested. The statistics presented include latency, queue length, and packet loss. This research may show that a communications system simpler than the currently accepted standard envisioned may not only suffice, but also surpass performance of the standard at a lower cost of deployment.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931366,no
47,Applying Moore's technology adoption life cycle model to quality of EDA software,"This paper describes a methodology for allocating priority levels and resources to quality activities during the development of EDA software projects. Geoffrey Moore's technology adoption life cycle model is used to provide a baseline understanding of what the market and the target users require at any point in time during the product life cycle. Applying this model, EDA software development teams can make choices and prioritize quality objectives which are based on the customer segment that they are targeting at any point in time during the product life cycle.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915209,no
48,DDP-a tool for life-cycle risk management,"At JPL we have developed, and implemented, a process for achieving life-cycle risk management. This process has been embodied in a software tool and is called Defect Detection and Prevention (DDP). The DDP process can be succinctly stated as: determine where we want to be, what could get in the way and how we will get there. The 'determine where we want to be' is captured as trees of requirements and the 'what could get in the way' is captured as trees of potential failure modes. Scoring the impacts of these failure modes on the requirements results in a prioritized set of failure modes. The user then selects from a set of PACTs (Preventative measures, Analyses, process Controls and Tests) each of which has an effectiveness versus the various failure modes. It is the goal of the DDP process to optimally select the subset of the PACTs/spl Dagger/ which minimizes the residual risk subject to the project resource constraints.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931736,no
49,Failure modes and effects analysis for software reliability,"This paper presents a systematic problem solving approach, which is based on the failure modes and effects analysis (FMEA), to system software reliability. This approach will practically: (a) ensure that all of conceivable failure modes and their effects on operational success of the software system have been considered; (b) list potential failures, and identify the magnitude of their effects; (c) develop criteria for test planning, design of the tests, and checkout systems (e.g., logging mechanism); (d) provide a basis for quantitative reliability and availability analysis; and (e) provide a basis for establishing corrective action priorities. This approach was created for software reliability analysis and testing in the multimedia digital distribution system (MDDS) at Thomson-CSF Sextant In-Flight Systems. First it was used to improve the software reliability for the ISDN Communication Control Unit (CCU) subsystem of the MDDS, and then globally applied to the software reliability analysis MDDS and improvement for the whole MDDS. It has been proven to be an effective and efficient approach to system software reliability.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902470,no
50,Incorporating varying test costs and fault severities into test case prioritization,"Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919106,yes
51,"Integrated reliability analysis, diagnostics and prognostics for critical power systems","Critical power systems, such as data centers and communication switching facilities, have very high availability requirements (5 min./year downtime). A data center that consumes electricity at a rate of 3 MW can have a downtime cost of $300,000 an hour. Even a momentary interruption of two seconds may cause a loss of two hours of data processing. Consequently, power quality has emerged as an issue of significant importance in the operation of these systems. In this paper, we address three issues of power quality: real-time detection and diagnosis of power quality problems, reliability and availability evaluation, and capacity margin analysis. The objective of real-time detection and diagnosis is to provide a seamless on-line monitoring and off-line maintenance process. The techniques are being applied to monitor the power quality of a few facilities at the University of Connecticut. Reliability analysis, based on a computationally efficient sum of disjoint products, enables analysts to decide on the optimum levels of redundancy, aids operators in prioritizing the maintenance options within a given budget, and in monitoring the system for capacity margin. Capacity margin analysis helps operators to plan for additional loads and to schedule repair/replacement activities. The resulting analytical and software tool is demonstrated on a sample data center.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949035,no
52,"Managing the maintenance of ported, outsourced, and legacy software via orthogonal defect classification","From the perspective of maintenance, software systems that include COTS software, legacy, ported or outsourced code pose a major challenge. The dynamics of enhancing or adapting a product to address evolving customer usage and the inadequate documentation of these changes over a period of time (and several generations) are just two of the factors which may have a debilitating effect on the maintenance effort. While many approaches and solutions have been offered to address the underlying problems, few offer methods which directly affect a team's ability to quickly identify and prioritize actions targeting the product which is already in front of them. The paper describes a method to analyze the information contained in the form of defect data and arrive at technical actions to address explicit product and process weaknesses which can be feasibly addressed in the current effort. The defects are classified using Orthogonal Defect Classification (ODC) and actual case studies are used to illustrate the key points.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972791,no
53,Prioritizing test cases for regression testing,"Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detection, a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test and let software engineers begin correcting faults earlier than might otherwise be possible. One application of prioritization techniques involves regression testing, the retesting of software following modifications; in this context, prioritization techniques can take advantage of information gathered about the previous execution of test cases to obtain test case orderings. We describe several techniques for using test execution information to prioritize test cases for regression testing, including: 1) techniques that order test cases based on their total coverage of code components; 2) techniques that order test cases based on their coverage of code components not previously covered; and 3) techniques that order test cases based on their estimated ability to reveal faults in the code components that they cover. We report the results of several experiments in which we applied these techniques to various test suites for various programs and measured the rates of fault detection achieved by the prioritized test suites, comparing those rates to the rates achieved by untreated, randomly ordered, and optimally ordered suites.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962562,yes
54,Providing absolute differentiated services for real-time applications in static-priority scheduling networks,"We propose and analyze a methodology for providing absolute differentiated services for real-time applications in networks that use static-priority schedulers. We extend previous work on worst-case delay analysis and develop a method that can be used to derive delay bounds without specific information on flow population. With this new method, we are able to successfully employ a utilization-based admission control approach for flow admission. This approach does not require explicit delay computation at admission time and hence is scalable to large systems. We assume the underlying network to use static-priority schedulers. We design and analyze several priority assignment algorithms, and investigate their ability to achieve higher utilization bounds. Traditionally, schedulers in differentiated services networks assign priorities on a class-by-class basis, with the same priority for each class on each router. We show that relaxing this requirement, that is, allowing different routers to assign different priorities to classes, achieves significantly higher utilization bounds.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=916255,no
55,Test-suite reduction and prioritization for modified condition/decision coverage,"Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique, and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows, and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software, according to some criterion, as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. The paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of a case study of the test-suite reduction algorithm.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972715,yes
56,The importance of quality requirements in software platform development-a survey,"This paper presents a survey where some quality requirements that commonly affect software architecture have been prioritized with respect to cost and lead-time impact when developing software platforms and when using them. Software platforms are the basis for a product-line, i.e. a collection of functionality that a number of products is based on. The survey has been carried out in two large software development organizations using 34 senior participants. The prioritization was carried out using the Incomplete Pairwise Comparison method (IPC). The analysis shows that there are large differences between the importance of the quality requirements studied. The differences between the views of different stakeholders are also analysed and it is found to be less than the difference between the quality requirements. Yet this is identified as a potential source of negative impact on product development cost and lead-time, and rules of thumb for reducing the impact are given.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=927252,no
57,Understanding and measuring the sources of variation in the prioritization of regression test suites,"Test case prioritization techniques let testers order their test cases so that those with higher priority, according to some criterion, are executed earlier than those with lower priority. In previous work (1999, 2000), we examined a variety of prioritization techniques to determine their ability to improve the rate of fault detection of test suites. Our studies showed that the rate of fault detection of test suites could be significantly improved by using more powerful prioritization techniques. In addition, they indicated that rate of fault detection was closely associated with the target program. We also observed a large quantity of unexplained variance, indicating that other factors must be affecting prioritization effectiveness. These observations motivate the following research questions. (1) Are there factors other than the target program and the prioritization technique that consistently affect the rate of fault detection of test suites? (2) What metrics are most representative of each factor? (3) Can the consideration of additional factors lead to more efficient prioritization techniques? To address these questions, we performed a series of experiments exploring three factors: program structure, test suite composition and change characteristics. This paper reports the results and implications of those experiments.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915525,yes
58,Yemanja-a layered event correlation engine for multi-domain server farms,"Yemanja is a model-based event correlation engine for multi-layer fault diagnosis. It targets complex propagating fault scenarios, and can smoothly correlate low-level network events with high-level application performance alerts related to quality of service violations. Entity-models that represent devices or abstract components encapsulate entity behavior. Distantly associated entities are not explicitly aware of each other, and communicate through event propagation chains. Yemanja's state-based engine supports generic scenario definitions, prioritization of alternate solutions, integrated problem-state and device testing, and simultaneous analysis of overlapping problems. The system of correlation rules was developed based on device, layer, and dependency analysis, and reveals the layered structure of computer networks. The primary objectives of this research include the development of reusable, configuration independent, correlation scenarios; adaptability and the extensibility of the engine to match the constantly changing topology of a multi-domain server farm; and the development of a concise specification language that is relatively simple yet powerful.",2001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918051,no
59,A history-based test prioritization technique for regression testing in resource constrained environments,"Regression testing is an expensive and frequently executed maintenance process used to revalidate modified software. To improve it, regression test selection (RTS) techniques strive to lower costs without overly reducing effectiveness by carefully selecting a subset of the test suite. Under certain conditions, some can even guarantee that the selected test cases perform no worse than the original test suite. This ignores certain software development realities such as resource and time constraints that may prevent using RTS techniques as intended (e.g., regression testing must be done overnight, but RTS selection returns two days worth of tests). In practice, testers work around this by prioritizing the test cases and running only those that fit within existing constraints. Unfortunately this generally violates key RTS assumptions, voiding RTS technique guarantees and making regression testing performance unpredictable. Despite this, existing prioritization techniques are memoryless, implicitly assuming that local choices can ensure adequate long run performance. Instead, we propose a new technique that bases prioritization on historical execution data. We conducted an experiment to assess its effects on the long run performance of resource constrained regression testing. Our results expose essential tradeoffs that should be considered when using these techniques over a series of software releases.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007961,no
60,"Analysis of preemptive periodic real-time systems using the (max, plus) algebra with applications in robotics","We present the model of a system of periodic real-time tasks with fixed priorities, preemption and synchronization, performed by a robot controller, using marked graphs. Then, with the help of the (max, plus) algebra, we derive simple tests to check real-time constraints on those tasks such as response times and the respect of deadlines. This method takes into account the precedence and synchronization constraints and is not limited to a particular scheduling policy.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998024,no
61,Dual purpose simulation: new data link test and comparison with VDL-2,"While the results of this paper are similar to those of previous research, in this paper the technical difficulties present previously are eliminated, producing better results, enabling one to more readily see the benefits of Prioritized CSMA (PCSMA). A new analysis section also helps to generalize this research so that it is not limited to exploration of the new concept of PCSMA. Commercially available network simulation software, OPNET version 7.0, simulations are presented involving an important application of the Aeronautical Telecommunications Network (ATN), Controller Pilot Data Link Communications (CPDLC) over the Very High Frequency Data Link Mode 2 (VDL-2). Communication is modeled for essentially all incoming and outgoing nonstop air-traffic for just three United States cities: Cleveland, Cincinnati, and Detroit. Collision-less PCSMA is successfully tested and compared with the traditional CSMA typically associated with VDL-2. The performance measures include latency, throughput, and packet loss. As expected, PCSMA is much quicker and more efficient than traditional CSMA. These simulation results show the potency of PCSMA for implementing low latency, high throughput and efficient connectivity. We are also testing a new and better data link that could replace CSMA with relative ease.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1067949,no
62,Elimination of crucial faults by a new selective testing method,"Recent software systems contain a lot of functions to provide various services. According to this tendency, software testing becomes more difficult than before and cost of testing increases so much, since many test items are required. In this paper we propose and discuss such a new selective software testing method that is constructed from previous testing method by simplifying testing specification. We have presented, in the previous work, a selective testing method to perform highly efficient software testing. The selective testing method has introduced an idea of functional priority testing and generated test items according to their functional priorities. Important functions with high priorities are tested in detail, and functions with low priorities are tested less intensively. As a result, additional cost for generating testing instructions becomes relatively high. In this paper in order to reduce its cost, we change the way of giving information, with respect to priorities. The new method gives the priority only rather than generating testing instructions to each test item, which makes the testing method quite simple and results in cost reduction. Except for this change, the new method is essentially the same as the previous method. We applied this new method to actual development of software tool and evaluated its effectiveness. From the result of the application experiment, we confirmed that many crucial faults can be detected by using the proposed method.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166937,yes
63,Extreme programming modified: embrace requirements engineering practices,"Extreme programming (XP) is an agile (lightweight) software development methodology and it becomes more and more popular. XP proposes many interesting practices, but it also has some weaknesses. From the software engineering point of view the most important issues are: maintenance problems resulting from very limited documentation (XP relies on code and test cases only), and lack of wider perspective of a system to be built. Moreover, XP assumes that there is only one customer representative. In many cases there are several representatives (each one with his own view of the system and different priorities) and then some XP practices should be modified. In the paper we assess XP from two points of view: the capability maturity model and the Sommerville-Sawyer model (1997). We also propose how to introduce documented requirements to XP, how to modify the planning game to allow many customer representatives and how to get a wider perspective of a system to be built at the beginning of the project lifecycle.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048543,no
64,How much information is needed for usage-based reading? A series of experiments,"Software inspections are regarded as an important technique to detect faults throughout the software development process. The individual preparation phase of software inspections has enlarged its focus from only comprehension to also include fault searching. Hence, reading techniques to support the reviewers on fault detection are needed. Usage-based reading (UBR) is a reading technique, which focuses on the important parts of a software document by using prioritized use cases. This paper presents a series of three UBR experiments on design specifications, with focus on the third. The first experiment evaluates the prioritization of UBR and the second compares UBR against checklist-based reading. The third experiment investigates the amount of information needed in the use cases and whether a more active approach helps the reviewers to detect more faults. The third study was conducted at two different places with a total of 82 subjects. The general result from the experiments is that UBR works as intended and is efficient as well as effective in guiding reviewers during the preparation phase of software inspections. Furthermore, the results indicate that use cases developed in advance are preferable compared to developing them as part of the preparation phase of the inspection.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166932,no
65,ICT in Japanese university language education: a case study,This paper reports part of a study conducted at a university in Japan. The belief that ICT provides students with more opportunities to negotiate target forms has been used to justify its use in language education. This study could not substantiate that ICT will change the way language is used in Japan or that CALL is providing new ways for learning and acquiring a new language. Students were experiencing difficulties in prioritising their learning repertoire between the acquisition of computer skills and language proficiency. Individuals experienced positive and negative coding and de-coding filters when communicating in ICT and this was related to the validity and reliability applied to the text. The value attached to the ICT interaction may influence the degree to which the ICT event influences face-to-face communicative acts.,2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1186116,no
66,Maintaining software with a security perspective,"Testing for software security is a lengthy, complex and costly process. Currently, security testing is done using penetration analysis and formal verification of security kernels. These methods are not complete and are difficult to use. Hence it is essential to focus testing effort in areas that have a greater number of security vulnerabilities to develop secure software as well as meet budget and time constraints. We propose a testing strategy based on a classification of vulnerabilities to develop secure and stable systems. This taxonomy will enable a system testing and maintenance group to understand the distribution of security vulnerabilities and prioritize their testing effort according to the impact the vulnerabilities have on the system. This is based on Landwehr's (1994) classification scheme for security flaws and we evaluated it using a database of 1360 operating system vulnerabilities. This analysis indicates vulnerabilities tend to be focused in relatively few areas and associated with a small number of software engineering issues.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167766,no
67,Modeling the cost-benefits tradeoffs for regression testing techniques,"Regression testing is an expensive activity that can account for a large proportion of the software maintenance budget. Because engineers add tests into test suites as software evolves, over time, increased test suite size makes revalidation of the software more expensive. Regression test selection, test suite reduction, and test case prioritization techniques can help with this, by reducing the number of regression tests that must be run and by helping testers meet testing objectives more quickly. These techniques, however can be expensive to employ and may not reduce overall regression testing costs. Thus, practitioners and researchers could benefit from cost models that would help them assess the cost-benefits of techniques. Cost models have been proposed for this purpose, but some of these models omit important factors, and others cannot truly evaluate cost-effectiveness. In this paper, we present new cost-benefits models for regression test selection, test suite reduction, and test case prioritization, that capture previously omitted factors, and support cost-benefits analyses where they were not supported before. We present the results of an empirical study assessing these models.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167767,yes
68,Parameter optimization tool for enhancing on-chip network performance,"In this paper, we present a tool to be used in the optimization of interconnection parameters in order to achieve optimal performance and implementation with minimal costs. The optimization tool uses an iterative algorithm to optimize the interconnection parameters, such as data width, priorities, and the time an agent can reserve the interconnection, to fulfill the given constraints. In the used test case, the required area decreased 50% while 85% of the original bandwidth was obtained. This was due to an improved arbitration process.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010388,no
69,Simulation of restaurant operations using the Restaurant Modeling Studio,"The operation of quick service restaurants (QSR) is a highly engineered process, with many factors coming into play: physical layout, equipment availability, and worker staffing levels, positioning, and priorities. The Restaurant Modeling Studio (RMS) provides an analysis platform for investigating the impacts of these factors on critical performance metrics, especially speed of service and service capacity. The key components of the RMS are a simulation engine built in Arena, and two custom applications built on Microsoft Visio - the Kitchen and Process Designers. The simulation engine supports a large number of behaviors, including parallel operations, inventory replenishment, prioritized task selection and many more. The Kitchen Designer and Process Designer provide the user with powerful tools for specifying the physical layout and order fulfillment processes. This paper presents the components of the RMS and its use in an analysis kitchen design comparison and labor deployment standards.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166417,no
70,Test case prioritization: a family of empirical studies,"To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of such prioritization is to increase a test suite's rate of fault detection. Previous work reported results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: 1) Can prioritization techniques be effective when targeted at specific modified versions; 2) what trade-offs exist between fine granularity and coarse granularity prioritization techniques; 3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? To address these questions, we have performed several new studies in which we empirically compared prioritization techniques using both controlled experiments and case studies.",2002,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988497,yes
71,A comparison of coverage-based and distribution-based techniques for filtering and prioritizing test cases,"This paper presents an empirical comparison of four different techniques for filtering large test suites: test suite minimization, prioritization by additional coverage, cluster filtering with one-per-cluster sampling, and failure pursuit sampling. The first two techniques are based on selecting subsets that maximize code coverage as quickly as possible, while the latter two are based on analyzing the distribution of the tests' execution profiles. These techniques were compared with data sets obtained from three large subject programs: the GCC, Jikes, and javac compilers. The results indicate that distribution-based techniques can be as efficient or more efficient for revealing defects than coverage-based techniques, but that the two kinds of techniques are also complementary in the sense that they find different defects. Accordingly, some simple combinations of these techniques were evaluated for use in test case prioritization. The results indicate that these techniques can create more efficient prioritizations than those generated using prioritization by additional coverage.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251065,yes
72,A task space redundancy-based scheme for motion planning,"In many applications, the manipulations require only part of the degrees of freedom (DOFs) of the end-effector, or some DOFs are more important than the rest. We name these applications prioritized manipulations. The end-effectors DOFs are divided into those which are critical and must be controlled as precisely as possible, and those which have loose specifications, so their tracking performance can traded-off to achieve other needs. In this paper, we derive a formulation for partitioning the task space into major and secondary task directions and finding the velocity and static force mappings that precisely accomplish the major task and locally optimize some secondary goals. The techniques are tested on a 6-DOF parallel robot performing a 2-DOF tracking task.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244063,no
73,"Generating, selecting and prioritizing test cases from specifications with tool support","The classification-tree method provides a systematic way for software testers to derive test cases by considering important relevant aspects that are identified from the specification. The method has been used in many real-life applications and shown to be effective. This paper presents several enhancements to the method by annotating the classification tree with additional information to reduce manual effort in the generation, selection and prioritization of test cases. A tool for supporting this enhanced process is also described.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319089,no
74,Global implementation of ERP software - critical success factors on upgrading technical infrastructure,"Implementing an Enterprise Resource planning (ERP) software in a global environment, executive sponsors face two key challenges. While business processes are to be re-engineered to align with the ERP software best practices, technical architecture and infrastructure needs to be in place globally as per specifications of the packaged software. In the legacy environment, different countries or different business units use different systems, based on local standards supported by local resources. In the new ERP world, globally all the countries must conform to same technical infrastructure. Technical managers face multiple critical issues implementing a global solution. Most of the ERP software are developed in technically advanced countries, standards are often too high for under developed or developing countries. In an effort to bring the global organization to a common platform different countries needs different levels of upgrades. In this paper the authors review key technical issues faced is a global upgrade process to support a global ERP implementation and how to resolve those. We conclude although technical infrastructure and business process reengineering both are equally important and each implementation is unique, but following some simple steps it is easy to prioritize each ones during different phases of the project. Also time lines of two sub-projects must converge after initial phase and must follow a common plan for the project to be successful. Multiple scenarios are described to facilitate the process.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1252285,no
75,Global multiprocessor scheduling of aperiodic tasks using time-independent priorities,"We provide a constant time schedulability test for a multiprocessor server handling aperiodic tasks. Dhall's effect is avoided by dividing the tasks in two priority classes based on task utilization: heavy and light. We prove that if the load on the multiprocessor server stays below U/sub threshold/ = 3 - /spl radic/7 = 35.425%, the server can accept incoming aperiodic tasks and guarantee that the deadlines of all accepted tasks will be met. 35.425% utilization is also a threshold for a task to be characterized as heavy. The bound U/sub threshold/ = 3 - /spl radic/7 = 35.425% is easy-to-use, but not sharp if we know the number of processors in the multiprocessor. For a server with m processors, we calculate a formula for the sharp bound U/sub threshold/(m), which converges to Uthreshold from above as m - -. The results are based on a utilization function u/sub m/(x) = 2(1 - x)/(2 + /spl radic/(2 + 2x)) + x/m. By using this function, the performance of the multiprocessor can in some cases be improved beyond U/sub threshold/ (m) by paying the extra overhead of monitoring the individual utilization of the current tasks.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203049,no
76,Market-based task allocation for dynamic processing environments,"Flexible and large-scale information processing across enterprises entails dynamic and decentralized control of workflow through adaptive allocation of knowledge and processing resources. Markets comprise a well-understood class of mechanisms for decentralized resource allocation, where agents interacting through a price system direct resources toward their most valued uses as indicated by these prices. The information-processing domain presents several challenges for market-based approaches, including (1) representing knowledge-intensive tasks and capabilities, (2) propagating price signals across multiple levels of information processing, (3) handling dynamic task arrival and changing priorities, and (4) accommodating the increasing-returns and public-good characteristics of information products. A market gaming environment provides a methodology for testing alternative market structures and agent strategies, and evaluating proposed solutions in a realistic decentralized manner.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245031,no
77,Prioritized use cases as a vehicle for software inspections,"The usage-based reading technique combines traditional inspection principles, use cases, and operational profile testing to create an efficient user-oriented software inspection reading technique. UBR can find faults more effectively and efficiently than the traditional checklist-based method.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207451,no
78,Putting your best tests forward,"Test case prioritization orders tests so that they help you meet your testing goals earlier during regression testing. Prioritization techniques can, for example, order tests to achieve coverage at the fastest rate possible, exercise features in order of expected frequency of use, or reveal faults as early as possible. We focus on the last goal, which we describe as ""increasing a test suite's rate of fault detection"" or the speed with which the test suite reveals faults. A faster fault detection rate during regression testing provides earlier feedback on a system under test, supporting earlier strategic decisions about release schedules and letting engineers begin debugging sooner. Also, if testing time is limited or unexpectedly reduced, prioritization increases the chance that testing resources will have been spent as cost effectively as possible in the available time.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231157,yes
79,Smart debugging software architectural design in SDL,"Statistical data show that it is much less expensive to correct software bugs at the early design stage rather than the late stage of the development process when the final system has already been implemented and integrated together. The use of slicing and execution histories as an aid in software debugging is well established for programming languages like C and C++; however, it is rarely applied in the field of software design specification. We propose a solution by applying the source code level technologies to debugging software designs represented in a high-level specification and description language such as SDL. More specifically, we extend execution slice-based heuristics from source code-based debugging to the software design specification level. Suspicious locations in an SDL specification are prioritized by their likelihood of containing faults. Locations with a higher priority should be examined first rather than those with a lower priority as the former are more likely to contain the faults. A debugging tool, SmartD/sub DSL/, with user-friendly interfaces was developed to support our method. An illustration is provided to demonstrate the feasibility of using our method to effectively debug an architectural design.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245320,no
80,Test-suite reduction and prioritization for modified condition/decision coverage,"Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software according to some criterion as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. This paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of empirical studies of these algorithms.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183927,yes
81,Web-technology of university's evaluation rating,"At present there is no a unified standard system of both qualitative and quantitative criteria to estimate objectively the conformity of the University activities and its infrastructure with the requirements of the public state attestation and accreditation. So the use of heuristic and expert procedures is considered to be necessary. In the suggested model of the University (institute, faculty, department) rating, the fragments of the models used by the Ministry of Education of Russia, the Association for Technical Universities of Russia, the Independent Accreditation Center, the advanced technical universities of Russia, as well as long-term developments of the Tomsk Polytechnic University in quality control of the University activities and its departments, modern tendencies and priorities of the universities developments, are accepted as the basic ones. 55 indices have been determined. Numerical values of indices are normalized on the maximum value achieved in the objects under consideration that provides a commensurability of indices by their essential spread in various objects. The data convolution by sections and subsections is carried out linearly taking into account their importance. The value of the index is determined by an expert method taking into account its relative importance within the limits of each section (subsection). The numerical value of each rating index is determined by its relation to the ""base"". The ""base"" is determined as the on-budget salary fund of the University (institute, faculty, department) staff, providing educational process, and the on-budget salary fund of the staff carrying out state budgetary research. The software for the collective interactive remote input of the indices, as well as for the centralized calculation and operative analysis of the universities rating has been developed. The server Apache, the database server MySQL, and the scripting language PHP have been used for the above work to be carried out.",2003,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222647,no
82,Agile development: evaluation and experience,"Agile methods such as Extreme Programming, Crystal, Scrum, and others have attracted a lot of attention recently. Agile methods stress early and continuous delivery of software, welcome changing requirements, and value early feedback from customers. Agile methods seek to cut out inefficiency, bureaucracy, and anything that adds no value to a software product. Proponents of agile methods often see software specification and documentation as adding no value, which has led observers to conclude that agile development is nothing but unprincipled hacking, perhaps even an anarchic counter-reaction to bureaucratic, heavyweight software processes that demand ever more intermediate deliverables from developers. The purpose of this panel is to discuss under what circumstances agile methods work and don't work. Some of the key practices of agile methods are: scheduling according to feature priorities, incremental delivery of software, feedback from expert users, emphasis on face-to-face communication, pair development, minimalist design combined with refactoring, test-driven development, automated regression testing, daily integration, self-organizing teams, and periodic tuning of the methods. Working software is the primary measure of success. Find out what the latest practical experience with agile methods is and learn about the latest thinking in this area.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317492,no
83,An approach to generate the thin-threads from the UML diagrams,"Software testing plays a crucial role in assuring software quality. One of the most important issues in software testing research is the generation of the test cases. For scenario-based software testing, the thin-threads, which are the usage scenarios in a software system from the end user's point of view, are frequently used to generate test cases. However, the generation of the thin-threads is not an easy task. A scenario-based business model has to be manually derived or labor-intensive business analysis has to be manually carried out in order to extract the thin-threads from a software system. In this work, we propose an automated approach to directly generate thin-threads from the UML artifacts. The generated thin-threads can be used to generate and to prioritize the test cases for scenario-based software testing.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342893,yes
84,An execution slice and inter-block data dependency-based approach for fault localization,"Localizing a fault in a program is a complex and time-consuming process. In this paper we present a novel approach using execution slice and inter-block data dependency to effectively identify the locations of program faults. An execution slice with respect to a given test case is the set of code executed by this test, and two blocks are data dependent if one block contains a definition that is used by another block or vice versa. Not only can our approach reduce the search domain for program debugging, but also prioritize suspicious locations in the reduced domain based on their likelihood of containing faults. More specifically, the likelihood of a piece of code containing a specific fault is inversely proportional to the number of successful tests that execute it. In addition, the likelihood also depends on whether this piece of code is data dependent on other suspicious code. A debugging tool, DESiD, was developed to support our method. A case study that shows the effectiveness of our method in locating faults on an application developed for the European Space Agency is also reported.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371939,no
85,An improved AHP method in performance assessment,"In order to reduce subjective errors in the traditional analytic hierarchy process (AHP), an improved AHP method integrated with orthogonal experimental design was presented in this paper. The new method combined both AHP and orthogonal design principles to make decisions, objectives in assessment system and values computed through mapping and satisfying function respectively as the corresponding factors, levels and results in orthogonal experimental design. With software SAS analysis, priorities on performance were to be ranked out accordingly. Application has showed that the method can improve the accuracy of AHP in performance assessment.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1340551,no
86,Assessing staffing needs for a software maintenance project through queuing simulation,"We present an approach based on queuing theory and stochastic simulation to help planning, managing, and controlling the project staffing and the resulting service level in distributed multiphase maintenance processes. Data from a Y2K massive maintenance intervention on a large COBOL/JCL financial software system were used to simulate and study different service center configurations for a geographically distributed software maintenance project. In particular, a monolithic configuration corresponding to the customer's point-of-view and more fine-grained configurations, accounting for different process phases as well as for rework, were studied. The queuing theory and stochastic simulation provided a means to assess staffing, evaluate service level, and assess the likelihood to meet the project deadline while executing the project. It turned out to be an effective staffing tool for managers, provided that it is complemented with other project-management tools, in order to prioritize activities, avoid conflicts, and check the availability of resources.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265735,no
87,Aviation application over IPv6: performance issues,"Aviation industries in United States and in Europe are undergoing a major paradigm shift in the introduction of new network technologies. In the US, NASA is also actively investigating the feasibility of IPv6 based networks for the aviation needs of the United States. In Europe, the Eurocontrol lead, Internet protocol for aviation exchange (iPAX) Working Group is actively investigating the various ways of migrating the aviation authorities backbone infrastructure from X.25 based networks to an IPv6 based network. For the last 15 years, the global aviation community has pursued the development and implementation of an industry-specific set of communications standards known as the aeronautical telecommunications network (ATN). These standards are now beginning to affect the emerging military global air traffic management (GATM) community as well as the commercial air transport community. Efforts are continuing to gain a full understanding of the differences and similarities between ATN and Internet architectures as related to communications, navigation, and surveillance (CNS) infrastructure choices. This research paper describes the implementation of the IPv6 testbed at Computer Networks & Software, Inc. and it's interface connection mechanism to Eurocontrol and NASA's (Cleveland) testbed in the first phase of the project. In the second phase this research work investigates the performance issues of aviation applications such as controller to pilot data link communication (CPDLC), on an IPv6 based backbone network. Aviation applications are grouped into different priority levels. Desired quality of service (QoS) to each priority level is implemented via Diffserv implementation. This research work looks into the possibility of providing similar QoS performance for aviation application in an IPv6 network as is provided in an ATN based network. The testbed consists of three autonomous systems. The autonomous system represents CNS domain, NASA domain and a EUROCONTROL domain. The primary mode of connection between CNS IPv6 testbed and NASA and EUROCONTROL IPv6 testbed is initially a set of IPv6 over IPv4 tunnels. The aviation application under test (CPDLC) consists of two processes running on different IPv6 enabled machines. These processes communicate with each other over the IPv6 network. One machine resides on the CNS portion of the testbed and other may reside in NASA (Cleveland) and/or in Eurocontrol. The IPv6 packets between Eurocontrol, NASA and CNS testbeds would be carried on IPv6 over IPv4 tunnels. We present some results, which suggest that IPv6 QoS has matured enough, so as to provide the QoS service, which is similar in capability to die ATN architecture. We implemented three basic priorities of flow: (1) command & control; (2) surveillance; and (3) general traffic. Various parameters like throughput, packet loss and delay are investigated. The results are analyzed to get a conceptual view of the effect of IPv6 based network on the aviation applications.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1367941,no
88,Best practices for a FRACAS implementation,"Many companies use a FRACAS (failure reporting analysis and corrective action system) process, better known as a closed-loop analysis and correction action process, to track and report problems or failures. Very few companies, however, fully realize all of the possible benefits of a FRACAS process, such as improving quality and productivity while reducing costs. Although many issues may prevent an effective implementation, there are three areas of concern that may cause negative impacts irrespective of the best-intentioned technology. In particular, complex organization interaction, inefficient and ineffective data tracking, and a lack of prioritized goals prevent the dramatic results that can be achieved with a FRACAS. By following the suggested eight step methodology and best practices presented, the potential for implementing a high-performance FRACAS can be greatly increased.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285523,no
89,Code-coverage guided prioritized test generation,"With Internet applications spreading like wildfire, software testing is challenged with new topics related to the distributed nature of Web applications. We apply code based testing techniques to the testing of Web applications, specifically Java programs. Source code based automatic test generation is difficult because most previous methods use constraint satisfaction models as a solution, which is an NP complete problem [M. J. Gallagher et al. (1997)]. We present a method of guiding users through test case generations. Instead of automating the entire procedure, our method aims at generating a framework of test cases and providing instructions for users to instantiate the framework into actual executable test cases. An early experimental study of this method shows the effectiveness of this method.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342705,no
90,Empirical studies of test case prioritization in a JUnit testing environment,"Test case prioritization provides a way to run test cases with the highest priority earliest. Numerous empirical studies have shown that prioritization can improve a test suite's rate of fault detection, but the extent to which these results generalize is an open question because the studies have all focused on a single procedural language, C, and a few specific types of test suites, in particular, Java and the JUnit testing framework are being used extensively in practice, and the effectiveness of prioritization techniques on Java systems tested under JUnit has not been investigated. We have therefore designed and performed a controlled experiment examining whether test case prioritization can be effective on Java programs tested under JUnit, and comparing the results to those achieved in earlier studies. Our analyses show that test case prioritization can significantly improve the rate of fault detection of JUnit test suites, but also reveal differences with respect to previous studies that can be related to the language and testing paradigm.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383111,yes
91,New optical switches enable automated testing with true flexibility,"The proliferation of fiber optic systems in military and avionics platforms is driven by the ever increasing need for higher data rates to support multi-sensor data fusion. Traditionally, the test systems to support these optical deployments are manual and inefficient. Increasingly fast optical components require optical test equipment that is very expensive. To make cost effective test suites, it is essential that these high value resources be used efficiently. This is most effectively accomplished through test architectures that are remotely controlled and automatically scheduled. These test architectures also enable a diverse set of testing applications to be simultaneously executed within an optical test lab or manufacturing environment. The advent of optical matrix switching technology with sub 1dB insertion loss performance and repeatability measured in milli dB's opens up new doors for highly efficient, remotely controlled, automated test systems. The ultra low loss aspects of these switches enable distributed test architectures that were previously unrealizable. Distributed test architectures create a test environment where expensive test equipment can be leveraged over a greater number of test samples in a more timely and automated fashion. This allows the lab manager to prioritize and schedule tests across many users, DUTs, and test equipment bays in an operation that can run 24/7. This paper explores the enabling photonic switch technology and a couple generic test architectures that can be applied in a variety of automated applications to increase test equipment usage and efficiency, thus lowering end costs for deployable fiber optic components and systems.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436883,no
92,Requirements driven software evolution,"Software evolution is an integral part of the software life cycle. Furthermore in the recent years the issue of keeping legacy systems operational in new platforms has become critical and one of the top priorities in IT departments worldwide. The research community and the industry have responded to these challenges by investigating and proposing techniques for analyzing, transforming, integrating, and porting software systems to new platforms, languages, and operating environments. However, measuring and ensuring that compliance of the migrant system with specific target requirements have not been formally and thoroughly addressed. We believe that issues such as the identification, measurement, and evaluation of specific re-engineering and transformation strategies and their impact on the quality of the migrant system pose major challenges in the software re-engineering community. Other related problems include the verification, validation, and testing of migrant systems, and the design of techniques for keeping various models (architecture, design, source code) during evolution, synchronized. In this working session, we plan to assess the state of the art in these areas, discuss on-going work, and identify further research issues.",2004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311070,no
93,A controlled experiment assessing test case prioritization techniques via mutation faults,"Regression testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may prioritize their test cases so that those that are more important are run earlier in the regression testing process. Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited to hand-seeded faults, primarily due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults. We have therefore designed and performed a controlled experiment to assess the ability of prioritization techniques to improve the rate of fault detection techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. We also compare our results to those collected earlier with respect to the relationship between hand-seeded faults and mutation faults, and the implications this has for researchers performing empirical studies of prioritization.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510136,yes
94,Concealment of whole-frame losses for wireless low bit-rate video based on multiframe optical flow estimation,"In low bit-rate packet-based video communications, video frames may have very small size, so that each frame fills the payload of a single network packet; thus, packet losses correspond to whole-frame losses, to which the existing error concealment algorithms are badly suited and generally not applicable. In this paper, we deal with the problem of concealment of whole frame-losses, and propose a novel technique which is capable of handling this very critical case. The proposed technique presents other two major innovations with respect to the state-of-the-art: i) it is based on optical flow estimation applied to error concealment and ii) it performs multiframe estimation, thus optimally exploiting the multiple reference frame buffer featured by the most modern video coders such as H.263+ and H.264. If data partitioning is employed, by e.g., sending headers, motion vectors, and coding modes in prioritized packets as can be done in the DiffServ network model, the algorithm is capable of exploiting the motion vectors to improve the error concealment results. The algorithm has been embedded in the H.264 test model software, and tested under both independent and correlated packet loss models with parameters typical of the wireless environment. Results show that the proposed algorithm significantly outperforms other techniques by several dBs in peak signal-to-noise ratio (PSNR), provides good visual quality, and has a rather low complexity, which makes it possible to perform real-time operation with reasonable computational resources.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407904,no
95,Current results from a rover science data analysis system,"The Onboard Autonomous Science Investigation System (OASIS) evaluates geologic data gathered by a planetary rover. This analysis is used to prioritize the data for transmission, so that the data with the highest science value is transmitted to Earth. In addition, the onboard analysis results are used to identify science opportunities. A planning and scheduling component of the system enables the rover to take advantage of the identified science opportunity. OASIS is a NASA-funded research project that is currently being tested on the FIDO rover at JPL for use on future missions. In this paper, we provide a brief overview of the OASIS system, and then describe our recent successes in integrating with and using rover hardware. OASIS currently works in a closed loop fashion with onboard control software (e.g., navigation and vision) and has the ability to autonomously perform the following sequence of steps: analyze gray scale images to find rocks, extract the properties of the rocks, identify rocks of interest, retask the rover to take additional imagery of the identified target and then allow the rover to continue on its original mission. We also describe the early 2004 ground test validation of specific OASIS components on selected Mars exploration rover (MER) images. These components include the rock-finding algorithm, RockIT, and the rock size feature extraction code. Our team also developed the RockIT GUI, an interface that allows users to easily visualize and modify the rock-finder results. This interface has allowed us to conduct preliminary testing and validation of the rock-finder's performance.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559328,no
96,Early community building: a critical success factor for XP projects,"Extreme programming (XP) literature and discussions often view successful projects only as customer-driven product development: planning, coding and testing an unfolding series of prioritized units of vertical functionality. I claim, however, that a successful project also requires a prospering community, comprising an introspective group of committed professionals communicating effectively, and using a well-understood, stable process. Weakness on any of these fronts presents a high risk of failure; therefore, I advise every XP project's members to actively engage in building their community, such that it reaches its critical level of development already by the first internal release. To help in this endeavor, I provide a comprehensive list of activities and attitudes to practice and avoid during the first release.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609817,no
97,Empirical results from an experiment on value-based review (VBR) processes,"As part of our research on value-based software engineering, we conducted an experiment on the use of value-based review (VBR) processes. We developed a set of VBR checklists with issues ranked by success-criticality, and a set of VBR processes prioritized by issue criticality and stakeholder-negotiated product capability priorities. The experiment involved 28 independent verification and validation (IV&V) subjects (full-time working professionals taking a distance learning course) reviewing specifications produced by 18 real-client, full-time student e-services projects. The IV&V subjects were randomly assigned to use either the VBR approach or our previous value-neutral checklist-based reading (CBR) approach. The difference between groups was not statistically significant for number of issues reported, but was statistically significant for number of issues per review hour, total issue impact, and cost effectiveness in terms of total issue impact per review hour. For the latter, the VBRs were roughly twice as cost-effective as the CBRs.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541809,no
98,Identification of test process improvements by combining fault trigger classification and faults-slip-through measurement,"Successful software process improvement depends on the ability to analyze past projects and determine which parts of the process that could become more efficient. One source of such an analysis is the faults that are reported during development. This paper proposes how a combination of two existing techniques for fault analysis can be used to identify where in the test process improvements are needed, i.e. to pinpoint which activities in which phases that should be improved. This was achieved by classifying faults after which test activities that triggered them and which phase each fault should have been found in, i.e. through a combination of orthogonal defect classification (ODC) and faults-slip-through measurement. As a part of the method, the paper proposes a refined classification scheme due to identified problems when trying to apply ODC classification schemes in practice. The feasibility of the proposed method was demonstrated by applying it on an industrial software development project at Ericsson AB. The obtained measures resulted in a set of quantified and prioritized improvement areas to address in consecutive projects.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541824,no
99,Integrated Product Policy and distributed supplier structures: SME and sound LCA data in conflict,"The sustainable development of our societies is one of the priorities of the European Commission. Through its new Integrated Product Policy (IPP), the European Commission is developing a series of measures that influence the supply and demand of environmentally sound products. Some IPP tools are based on product and process self-declarations, while others require the performance of a Life-Cycle Assessment (LCA). Life-Cycle Inventory (LCI) data availability is the fundamental premise in order to be able to perform an LCA. In this paper, the work is to investigate the diffusion of required LCA data along the supplier chain with the aim of identifying strategies to increase the awareness of Small and Medium Enterprises (SMEs) in respect to LCA, to suggest methodologies to facilitate the collection of sound LCI data and to test available low-cost software tools to support LCA, with particular reference to the production phase is reported",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1619261,no
100,Plug and play testbed to enable responsive space missions,"There exists a growing need in the DOD for a tactical or responsive space asset to support real-time battlefield intelligence, surveillance, and reconnaissance. The desired attributes include being: 1. responsive /sub e/ployable in days; 2. affordable - expendable tactical resources at a cost comparable to other tactical systems; 3. employable - assets must support the joint force commander (JFC); 4. integrated space/air/terrestrial system-of-systems - full network connectivity, bandwidth on demand, and augment other assets. To the warfighter, a responsive space asset would provide the capability to respond to unanticipated military needs in days, providing flexibility of response to rapidly field tailored payloads and coverage. This capability could also provide rapid reconstitution after a loss from attack or failure and counteract enemy adaptation, through denial or deception, to existing space capabilities. Most importantly the short deployment times and low cost would provide the United States a means for efficiently using the versatility, and relative safety of space to provide real-time support to the war fighter. To be responsive the space element must possess a modular design supporting ""plug and play"" (PnP) architecture, leveraging commercial parts and standards. Lending itself to a lean production and integration environment again utilizing standard interfaces and taking advantage of pre-qualified inventoried subsystems. Rapid deployment of these elements will make use of ""canned"" mission planning tools, tailored orbits for a given theater, built-in health and status monitoring, and autonomous test and checkout software and operations. The two emerging responsive mission objectives include space control and tailored, tactical intelligence, surveillance, reconnaissance (ISR). Space control involves a situational awareness to sense threats against, and provide protection for, US space assets. The tailored, tactical missions offer high tempo ISR operations such as target characterization and emitter location in theater, perform real-time blue force tracking, and provide gap-filler, specialized communications support. The challenge is to develop and qualify the satellite technologies and rapid integration and test processes to support an operational responsive system in the next five years. Under an Air Force Research Laboratory SBIR program, MicroSat Systems is developing a PnP testbed to enable an operational responsive capability through development and ground validation of the various elements. Those elements include the mission definition and CONOPS specification processes, space segment, and the operational prioritization, tasking, processing, exploitation, and dissemination (PTPED) process/infrastructure. Specific to providing an end-to-end mission simulation, the testbed should be equipped with the modeling and simulation tools to develop tactical satellite CONOPS and provide the warfighter with a front end tool for training. To validate utility to the end-user the testbed must be equipped with the capability to simulate the data processing and dissemination infrastructure envisioned to provide the battlefield commander with real-time data. The space segment of the testbed should consist of the hardware and software elements required to simulate the operations of a fully functional satellite system. The approach to developing a responsive mission testbed includes defining requirements, hardware/software architecture, technology development roadmap, starting with a core capability, and incrementally integrating and validating the developing components and processes as they emerge.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559345,no
101,Prioritize code for testing to improve code coverage of complex software,"Code prioritization for testing promises to achieve the maximum testing coverage with the least cost. This paper presents an innovative method to provide hints on which part of code should be tested first to achieve best code coverage. This method claims two major contributions. First it takes into account a ""global view"" of the execution of a program being tested, by considering the impact of calling relationship among methods/functions of complex software. It then relaxes the ""guaranteed"" condition of traditional dominator analysis to be ""at least"" relationship among dominating nodes, which makes dominator calculation much simpler without losing its accuracy. It also then expands this modified dominator analysis to include global impact of code coverage, i.e. the coverage of the entire software other than just the current function. We implemented two versions of code prioritization methods, one based on original dominator analysis and the other on relaxed dominator analysis with global view. Our comparison study shows that the latter is consistently better in terms of identifying code for testing to increase code coverage",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544723,yes
102,System test case prioritization of new and regression test cases,"Test case prioritization techniques have been shown to be beneficial for improving regression-testing activities. With prioritization, the rate of fault detection is improved, thus allowing testers to detect faults earlier in the system-testing phase. Most of the prioritization techniques to date have been code coverage-based. These techniques may treat all faults equally. We build upon prior test case prioritization research with two main goals: (1) to improve user-perceived software quality in a cost effective way by considering potential defect severity and (2) to improve the rate of detection of severe faults during system-level testing of new code and regression testing of existing code. We present a value-driven approach to system-level test case prioritization called the prioritization of requirements for test (PORT). PORT prioritizes system test cases based upon four factors: requirements volatility, customer priority, implementation complexity, and fault proneness of the requirements. We conducted a PORT case study on four projects developed by students in advanced graduate software testing class. Our results show that PORT prioritization at the system level improves the rate of detection of severe faults. Additionally, customer priority was shown to be one of the most important prioritization factors contributing to the improved rate of fault detection.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541815,yes
103,"Systematic incremental development of agent systems, using Prometheus","This paper presents a mechanism for dividing an agent oriented application into the three IEEE defined scoping levels of essential, conditional and optional. This mechanism is applied after the initial system specification, and is then used to direct incremental development with three separate releases. The scoping described can be applied at any stage of a project, in order to guide consistent scoping back if such is needed. The three levels of scoping that are used are consistent with the approach used in many companies. The approach to scoping requires that scenarios are prioritised manually on a five point scale. All other aspects are then prioritised automatically, based on this information. The approach used allows a developer to indicate what size partitions - based on number of scenarios - are required for each scoping level. The mechanisms are applied to the Prometheus development methodology and are integrated into the Prometheus design tool (PDT).",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579165,no
104,Test factoring: focusing test suites for the task at hand,"Frequent execution of a test suite during software maintenance can catch regression errors early, indicate whether progress is being made, and improve productivity. However, if the test suite takes a long time to produce feedback, the developer is slowed down, and the benefit of frequent testing is reduced. After a program is edited, ideally, only changed code would be tested. Any time spent executing previously tested, unchanged parts of the code is wasted. For a large test suite containing many small unit tests, test selection and prioritization can be effective. Test selection runs only those tests that are possibly affected by the most recent change, and test prioritization can run first the tests that are most likely to reveal a recently-introduced error.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553636,yes
105,Test prioritization using system models,"During regression testing, a modified system is retested using the existing test suite. Because the size of the test suite may be very large, testers are interested in detecting faults in the system as early as possible during the retesting process. Test prioritization tries to order test cases for execution so the chances of early detection of faults during retesting are increased. The existing prioritization methods are based on the code of the system. System modeling is a widely used technique to model state-based systems. In this paper, we present methods of test prioritization based on state-based models after changes to the model and the system. The model is executed for the test suite and information about model execution is used to prioritize tests. Execution of the model is inexpensive as compared to execution of the system; therefore the overhead associated with test prioritization is relatively small. In addition, we present an analytical framework for evaluation of test prioritization methods. This framework may reduce the cost of evaluation as compared to the existing evaluation framework that is based on experimentation (observation). We have performed an experimental study in which we compared different test prioritization methods. The results of the experimental study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510150,yes
106,Using occurrence properties of defect report data to improve requirements,"Defect reports generated for faults found during testing provide a rich source of information regarding problematic phrases used in requirements documents. These reports indicate that faults often derive from instances of ambiguous, incorrect or otherwise deficient language. In this paper, we report on a method combining elements of linguistic theory and information retrieval to guide the discovery of problematic phrases throughout a requirements specification, using defect reports and correction requests generated during testing to seed our detection process. We found that phrases known from these materials to be problematic have occurrence properties in requirements documents that both allow the direction of resources to prioritize their correction, and generate insights characterizing more general locations of difficulty within the requirements. Our findings lead to some recommendations for more efficiently and effectively managing certain natural language issues in the creation and maintenance of requirements specifications.",2005,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531046,no
107,A Dynamic Partitioning Approach for GUI Testing,"Previous works on GUI testing are mainly concerned with how to define or generate GUI test cases. The issue of how to employ generated GUI test cases or primitive actions is seldom discussed. In this paper we propose a dynamic partitioning approach for GUI testing to address the issue. In this approach, the given GUI primitive actions are dynamically partitioned into two disjoint classes: one comprising prioritized primitive actions and the other comprising non-prioritized ones. The testing process is divided into two stages and contains two feedback loops. The first stage prioritizes primitive actions and the second stage selects and performs prioritized primitive actions. The first feedback loop is local and occurs in the second stage, which adjusts the memberships of primitive actions after they are performed. The second feedback loop is global and occurs between the first and second stages. It switches GUI testing from the second stage to the first stage upon no prioritized primitive actions are available. Two testing experiments with real GUI applications show that the proposed dynamic partitioning approach can really work in practice and may significantly outperform the random testing approach in the sense that the dynamic partitioning approach uses fewer primitive actions to achieve given testing goals and behaves more stable. The dynamic partitioning approach adopts explicit feedback mechanisms and contributes to the emerging area of software cybernetics that explores the interplay between software and control",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020171,no
108,A Feature-Oriented Requirements Tracing Method: A Study of Cost-benefit Analysis,"Establishing and maintaining traceability links places a big burden since complex systems have especially yield an enormous number of various artifacts although traceability links is useful for requirements change impact analysis, requirements conflict analysis, and requirements consistency checking. Hence, we propose a feature-oriented requirements tracing method including value consideration and intermediate catalysis. To achieve our goal in this paper, we present (1) a meta-model of feature-oriented requirements tracing, (2) a featureoriented requirement tracing process overview, and (3) cost-benefits analysis. The meta-model is a formalization of feature-oriented requirement tracing using UML notation. The feature-oriented requirement tracing process consists of requirements definition, feature modeling, feature prioritization, requirements linking, and traceability links evaluation. We also carry out cost-benefit analysis through a case study to demonstrate the feasibility of our approach.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021275,no
109,A hardware implementation of layer 2 MPLS,"This paper presents a hardware architecture for layer 2 Multi Protocol Label Switching (MPLS). MPLS is a protocol framework used primarily to prioritize internet traffic and improve bandwidth utilization. Furthermore it increases the performance of internet applications and overall efficiency. However, most existing MPLS solutions are entirely software based which decreases performance. MPLS performance can be enhanced by executing core tasks in hardware while allowing other tasks to be executed in software to guard against performance degradation. This paper proposes a hardware design of MPLS on an FPGA for increased performance and efficiency.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1581247,no
110,An Add-On for Managing Behaviours with Priority in JADE,"In this article, two new implementations for behaviours in JADE are presented. These new behaviours, while being able to reproduce the functioning of the old JADE's behaviours, allow the user to define priorities. This fact is of vital importance for several multiagent applications. Finally, a test was developed to show that the performance of the new behaviours is similar to the original ones.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053000,no
111,An Agent-Based E-Learning Assessing and Instructing System,"The research work on e-learning has become very important field in the education. Many demands for instructing strategies in the adaptive learning have turn out. Base on these instructing strategies, the learners could reduce their blindness in the learning process. This paper proposes a method to build a Bayesian networks model in order to assess the learner's knowledge level and instruct the learner. This e-learning assessing and instructing system is designed and implemented based on multi-agent systems (ELAIS). In this system, the knowledge cognitive level, the learning priorities and weakness of a learner could be analyzed and assessed through the tracking information. Then the corresponding instruction will give to the learner in order to improve the learner's learning efficiency. The parameters assessment is achieved using EM algorithm and the assessment results in this system reveal the pretty accuracy to the real testing situation",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019230,no
112,Applying CMMI and Strategy to ATE Development,"This paper provides a viewpoint of the capability maturity model integration (CMMI<sup>SM</sup> ) from the perspective of automated test equipment (ATE) development and test engineering. ATE development is a specialized segment of product development and shares many of the same issues. Requirements for the test equipment are very dependent on continually evolving product characteristics. Even with the best planning, lead times for ATE development are typically eroded by late changes to product requirements and designs, and eventually the critical path leads right through test! Without a solid process foundation, chaos ensues. The CMMI process models provide a framework for the integration of best practices in many disciplines. Portions of the systems engineering, software engineering, Integrated Product and Process Development and Supplier Sourcing models all offer important perspectives which affect ATE developers. This paper focuses on the CMMI processes and best practices which yield the greatest impact to test organizations and groups that provide ATE. The overall Test Strategy should help prioritize the process areas which deserve the most attention. Mature ATE organizations use a Balanced Scorecard approach to provide alignment with corporate and program level goals. Metrics monitor their progress against their corporate goals. At the program level, they apply a risk- driven approach to selectively apply resources that achieve the highest ROI for test dollars. From this business-oriented vantage point, organizations are likely to see increased efficiencies that will decrease overall system development costs by streamlining the testing component of their budgets.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062482,no
113,DDP: a tool for life-cycle risk management,"At JPL we have developed and implemented a process for achieving life-cycle risk management. This process has been embodied in a software tool and is called defect detection and prevention (DDP). The DDP process can be succinctly stated as: determine where we want to be, what could get in the way and how we will get there. The ""determine where we want to be"" is captured as trees of requirements and the ""what could get in the way"" is captured as trees of potential failure modes. Scoring the impacts of these failure modes on the requirements results in a prioritized set of failure modes. The user then selects from a set of preventative measures, analyses, process controls and tests (PACTs) each of which has an effectiveness versus the various failure modes. It is the goal of the DDP process to optimally select the subset of the PACTs which minimizes the residual risk subject to the project resource constraints. The DDP process is intended to facilitate risk management over the entire project life cycle beginning with architectural and advanced technology decisions all the way through operation. As the project design, technology content, and implementation approach matures, the requirements and failure mode trees are elaborated upon to accommodate the additional information. Thus, the DDP process is a systematic, continuous, top-down approach to managing risk. Implementation of the DDP process requires a critical mass of expertise (usually the project team and a few specialists) and captures both their engineering judgement as well as available quantitative data. This additional data may result from models, layouts, prototype testing, other focused risk evaluations and institutional experiences. The DDP process also identifies areas where additional information would be advantageous, thus allowing a project to target critical areas of risk or risk uncertainty. This also allows the project to identify those areas which would benefit the most from application of other quantitative tools and methods (e.g. Monte Carlo simulations, FMECAs, fault trees). The software tool supports the DDP process by providing guidance for implementing the process steps, graphical visualizations of the various trees, their interrelationships and the current risk landscape. The tool is capable of supporting on-the-fly knowledge elicitation as well as integrating off-line deliberations. There are a variety of available outputs including graphs, trees and reports as well as clear identification of the driving requirements, ""tall-pole"" residual risks and the PACTs which have been selected and agreed upon. The DDP process has been applied at various levels of assembly including the system and subsystem levels, as well as down to the component level. Recently, significant benefits have been realized from application to advanced technologies, where the focus has been on increasing the infusion rates of these technologies by identification and mitigation of risks prior to delivery to a project",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662004,no
114,"Expert System for Quality Cost Planning, Monitoring and Control","On the market, there are some commercial available software tools for quality cost management. However, these tools do not incorporate specialized agents for handling complex tasks related to the current needs in quality cost planning, like interpretation of the results over a horizon of time and automatic generation of reliable guidelines to prioritize resources in order to improve the quality of the business processes. This paper is going to introduce the results of some researches performed by the authors in designing and developing an expert system for comprehensive monitoring, controlling and planning of quality costs within business processes. Results are already successful implemented in a large enterprise from chemical industry",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022922,no
115,How International Standards Such as ATML and IEEE 1641 STD can Make the Realisation of an Open System Architecture on a Common Test Platform a Reality,"A perspective on how the DoD and MoD are integrating open standards into their ATS frameworks and policy in the search for a common test platform architecture solution for use on all test platforms. The paper examines the two approaches being taken and draws on their commonality to propose how open standards can help meet both their aims and circumstances. Benefits such as TPS interoperability, re-host and re-use are examined and contrasted for open systems versus common architecture to identify the practical implication for real systems. The life cycle cost of support for the system is identified and the trade-off in cost between fast optimal TPSs and fully interoperable TPSs is considered. The paper goes on to show the difference between using information models utilizing a development process versus the use of run time interfaces and how they can lead to different solutions to the same basic problem but with different peripheral benefits. In conclusion an approach to maximize benefit between the two framework groups is considered whilst maintaining individual priorities.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062469,no
116,Improving Testing Efficiency using Cumulative Test Analysis,"It can be impossible to thoroughly test complex software projects with a large library of tests to be run in many environments and configurations. The cumulative test analysis (CTA) technique described reduces the time to find defects by prioritising and minimising the testing. Tests are chosen to target the product areas having the highest risk of defects. Test effectiveness, test code coverage, product code changes and changes to dependencies are monitored and analysed to prioritise the testing. Test results from build to build are accumulated. Build reports clearly identify areas at risk, test results, and the tests that must be run. Experiences with a prototype tool are discussed and conclusions drawn from the use of CTA show that defects are found sooner, more time is available for writing new tests and the focus of test execution moves towards product quality instead of simply test results",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691682,no
117,On the Use of Mutation Faults in Empirical Assessments of Test Case Prioritization Techniques,"Regression testing is an important activity in the software life cycle, but it can also be very expensive. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of test case prioritization techniques is to increase a test suite's rate of fault detection (how quickly, in a run of its test cases, that test suite can detect faults). Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited primarily to hand-seeded faults, largely due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults and that the use of hand-seeded faults can be problematic for the validity of empirical results focusing on fault detection. We have therefore designed and performed two controlled experiments assessing the ability of prioritization techniques to improve the rate of fault detection of test case prioritization techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. More importantly, a comparison of our results with those collected using hand-seeded faults reveals several implications for researchers performing empirical studies of test case prioritization techniques in particular and testing techniques in general",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1707670,yes
118,Optimizing the selection of representative configurations in verification of evolving product lines of distributed embedded systems,"Electronics and computer science play a more and more prominent role in automotive technology. In the future the prevalence of those new technologies and the customers' demand for individuality leads to tremendously large configuration spaces of vehicle control systems. To cope with the resulting complexity in verification, new strategies need to be explored. One likely future challenge is to determine a set of vehicle configurations, such that the successful verification of this small set implies the correctness of the entire product family. This paper presents a method to address this task, based on exploiting communalities in architecture and requirements. We introduce efficient algorithms with provable quality guarantees for the optimization problems of choosing the minimum set of configurations necessary to verify all possible configurations and choosing the best k configurations to maximize the verification coverage of the entire product family. We discuss extensions of our method which allow requirement priorities and the consideration of configuration costs, and present a technique for automatically determining communalities in architecture and requirements which can be exploited by our optimization methods. We demonstrate the effectiveness of our method on an indicator light system product family. In this example a configuration reduction by 60% can be achieved",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691579,no
119,Prioritizing Software Inspection Results using Static Profiling,"Static software checking tools are useful as an additional automated software inspection step that can easily be integrated in the development cycle and assist in creating secure, reliable and high quality code. However, an often quoted disadvantage of these tools is that they generate an overly large number of warnings, including many false positives due to the approximate analysis techniques. This information overload effectively limits their usefulness. In this paper we present ELAN, a technique that helps the user prioritize the information generated by a software inspection tool, based on a demand-driven computation of the likelihood that execution reaches the locations for which warnings are reported. This analysis is orthogonal to other prioritization techniques known from literature, such as severity levels and statistical analysis to reduce false positives. We evaluate feasibility of our technique using a number of case studies and assess the quality of our predictions by comparing them to actual values obtained by dynamic profiling.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026864,no
120,Prognostics usefulness criteria,"Prognostics and health management (PHM) can provide remarkable insight for maintenance management of large systems, but must be implemented with a healthy respect for the end user and a practical view of the hardware and software capabilities. The F-35 Joint Strike Fighter (JSF) program is implementing a comprehensive PHM system to maximize the supportability of the air system. The prognostic algorithms must have a defined minimum capability to aid implementation and verification. However, the complexity of the air system precludes creating exact requirements for remaining useful life and confidence. Instead, ""usefulness criteria"" are created to link the user need with the minimum capability of the algorithm. The usefulness criteria are a list of goals related to aircraft supportability which can be used to define the minimum acceptable time to maintenance indication for the prognostic algorithm. The goals in the usefulness criteria were applied to each prognostic algorithm in the F-35 PHM system. When assigned, these usefulness criteria provide a means to measure the improved performance of the aircraft and fleet maintenance as well as prioritize the implementation of the prognostic algorithms. The development and implementation of the algorithms in relation to these usefulness criteria are still in process, but it is expected that most algorithms exceeds the criteria. Those which do not meet the criteria are re-evaluated with a trade study to determine if further efforts in hardware and software development are warranted. This process of usefulness criteria development and application can be rigorously applied to the development of any PHM system. This paper covers the development of the usefulness criteria for the F-35 program and the implementation results to date",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656123,no
121,"Recorders, Reasoners and Artificial Intelligence - Integrated Diagnostics on Military Transport Aircraft","A research group at Boeing has developed a prototype for an integrated diagnostic system to optimize maintenance on military transport aircraft by decreasing maintenance costs and increasing aircraft availability. The integrated diagnostic system comprises an on-board recorder, and a ground-based reasoner that analyzes the recorded data, to optimize maintenance. The functions of the ground-based reasoner (GBR) include identification of root-cause, filtering of false alarms, and prioritization of maintenance actions. The technologies used include expert systems/fast state recognition methods, data mining technologies and Bayesian analyses. The ground-based reasoner provides an open plug-n-play software framework for incorporating these technologies into a software tool, for field maintenance. The tool has a simple, intuitive graphic user interface that is designed to help the end-user, the maintenance technician, with everyday maintenance tasks. The integrated diagnostic system prototype is currently undergoing testing on pre-delivery test flights for the C-17 military transport, at a Boeing facility, and initial results in applying the system to the aerial delivery subsystem and the hydraulic subsystem are discussed",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656074,no
122,Search Based Approaches to Component Selection and Prioritization for the Next Release Problem,This paper addresses the problem of determining the next set of releases in the course of software evolution. It formulates both ranking and selection of candidate software components as a series of feature subset selection problems to which search based software engineering can be applied. The approach is automated using greedy and simulated annealing algorithms and evaluated using a set of software components from the component base of a large telecommunications organization. The results are compared to those obtained by a panel of (human) experts. The results show that the two automated approaches convincingly outperform the expert judgment approach,2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021335,no
123,Test Case Prioritization Using Relevant Slices,"Software testing and retesting occurs continuously during the software development lifecycle to detect errors as early as possible. The sizes of test suites grow as software evolves. Due to resource constraints, it is important to prioritize the execution of test cases so as to increase chances of early detection of faults. Prior techniques for test case prioritization are based on the total number of coverage requirements exercised by the test cases. In this paper, we present a new approach to prioritize test cases based on the coverage requirements present in the relevant slices of the outputs of test cases. We present experimental results comparing the effectiveness of our prioritization approach with that of existing techniques that only account for total requirement coverage, in terms of ability to achieve high rate of fault detection. Our results present interesting insights into the effectiveness of using relevant slices for test case prioritization",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020103,yes
124,The Incremental Evolution of Attack Agents in Xpilot,"In the research presented in this paper, we use incremental evolution to learn multifaceted neural network (NN) controllers for agents operating in the space game Xpilot. Behavioral components specific to the accomplishment of specific tasks, such as bullet-dodging, shooting, and closing on an enemy, are learned in the first increment. These behavioral components are used in the second increment to evolve a NN that prioritizes the output of a two-layer NN depending on that agent's current situation.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688415,no
125,Towards Interactive Fault Localization Using Test Information,"Finding the location of a fault is a central task of debugging. Typically, a developer employs an interactive process for fault localization. To accelerate this task, several approaches have been proposed to automate fault localization. In practice, testing-based fault localization (TBFL), which uses test information to locate faults, has become a research focus. However, experimental results reported in the literature showed that current automation of fault localization can only serve as a means to confirming the search space and prioritizing search sequences, not a substitute of the interactive fault localization process. In this paper, we propose an approach based on test information to support the entire interactive fault localization process. During this process, the information gathered from previous interaction steps can be used to provide the ranking of suspicious statements for the current interaction step. As a feasibility study of our approach, we performed an experiment on applying our approach together with some other TBFL approaches on the Siemens programs, which have been used in the literature. Our experimental results show the effectiveness of our approach.",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137428,no
126,Using the Case-Based Ranking Methodology for Test Case Prioritization,"The test case execution order affects the time at which the objectives of testing are met. If the objective is fault detection, an inappropriate execution order might reveal most faults late, thus delaying the bug fixing activity and eventually the delivery of the software. Prioritizing the test cases so as to optimize the achievement of the testing goal has potentially a positive impact on the testing costs, especially when the test execution time is long. Test engineers often possess relevant knowledge about the relative priority of the test cases. However, this knowledge can be hardly expressed in the form of a global ranking or scoring. In this paper, we propose a test case prioritization technique that takes advantage of user knowledge through a machine learning algorithm, case-based ranking (CBR). CBR elicits just relative priority information from the user, in the form of pairwise test case comparisons. User input is integrated with multiple prioritization indexes, in an iterative process that successively refines the test case ordering. Preliminary results on a case study indicate that CBR overcomes previous approaches and, for moderate suite size, gets very close to the optimal solution",2006,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021329,yes
127,A histogram-matching approach to the evolution of bin-packing strategies,"We present a novel algorithm for the one- dimension offline bin packing problem with discrete item sizes based on the notion of matching the item-size histogram with the bin-gap histogram. The approach is controlled by a constructive heuristic function which decides how to prioritise items in order to minimise the difference between histograms. We evolve such a function using a form of linear register-based genetic programming system. We test our evolved heuristics and compare them with hand-designed ones, including the well- known best fit decreasing heuristic. The evolved heuristics are human-competitive, generally being able to outperform high- performance human-designed heuristics.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424926,no
128,A model to predict anti-regressive effort in Open Source Software,"Accumulated changes on a software system are not uniformly distributed: some elements are changed more often than others. For optimal impact, the limited time and effort for complexity control, called anti-regressive work, should be applied to the elements of the system which are frequently changed and are complex. Based on this, we propose a maintenance guidance model (MGM) which is tested against real-world data. MGM takes into account several dimensions of complexity: size, structural complexity and coupling. Results show that maintainers of the eight open source systems studied tend, in general, to prioritize their anti-regressive work in line with the predictions given by our MGM, even though, divergences also exist. MGM offers a history-based alternative to existing approaches to the identification of elements for anti-regressive work, most of which use static code characteristics only.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362632,no
129,A Multi-Agent Solution to Distribution Systems Restoration,"The goal to provide faster and faster restoration after a fault is pushing the technical envelope related to new algorithms. While many approaches use centralized strategies, the concept of multi-agent systems (MAS) is creating a new option related to distributed analyses for restoration. This paper provides details on a MAS that restores a power system after a fault. The development of agents and behaviors of the agents are described, including communication of agents. The MAS is tested on two test systems and facilitates both full and partial restoration, including load prioritization and shedding.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4282055,no
130,A Multipurpose Code Coverage Tool for Java,"Most test coverage analyzers help in evaluating the effectiveness of testing by providing data on statement and branch coverage achieved during testing. If made available, the coverage information can be very useful for many other related activities, like, regression testing, test case prioritization, test-suite augmentation, test-suite minimization, etc. In this paper, we present a Java-based tool JavaCodeCoverage for test coverage reporting. It supports testing and related activities by recording the test coverage for various code-elements and updating the coverage information when the code being tested is modified. The tool maintains the test coverage information for a set of test cases on individual as well as test suite basis and provides effective visualization for the same. Open source database support of the tool makes it very useful for software testing research",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076910,no
131,A Novel Approach of Prioritizing Use Case Scenarios,"Modern softwares are very large and complex. As the size and complexity of software increases, software developers feel an urgent need for a better management of different activities during the course of software development. In this paper, we present an approach of use case scenario prioritization suitable for project planning at an early phase of the software development. We consider only use case model in our work. For prioritization, we focus on how critical a scenario path is, which essentially depends on density of overlapping of sub path of a scenario path with other scenario path(s) of a use case. Our proposed approach provides an analytical solution on use case scenario prioritization and is very much effective in project management related activities as substantiated by our experimental results.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425898,no
132,A process framework for customising software quality models,The quality objective of many software organisations is to deliver software products that meet and or exceed customer expectations. The key to achieving this is to capture these expectations at the beginning of the project by clearly defining all quality requirements. The characteristics particularly defined in ISO/IEC 9126-1 (2001) provide the framework for specifying quality requirements. The ISO/IEC 9126-1 quality model is intended to be applicable to any type of software product or intermediate product. Before application this model needs to be tailored to a specific software and specific need. Since these characteristics cannot be directly measured this makes it difficult to directly prioritise and choose the most relevant characteristics and sub-characteristics. Hence a process framework that will link these characteristics and sub- characteristics to user needs is required. This will in turn help customise software quality models like ISO/IEC 9126-1 (2001) and other general software quality models. A process framework for customising software quality models is proposed in the text and it is further shown how this framework was applied in a real working environment in an attempt to quantitatively validate it. The results collected in the study showed that the framework could be used reliably in customising a generic software quality model at characteristic level only. The deviations at sub-characteristic level were due to unclear questions in the generated Generic Quality Questionnaire that resulted in misunderstandings. And the metrics used to create these questions were not fully tested for validity and reliability due to time constraints. Enhancements are discussed in the study and it is further shown how reliability can also be achieved at sub-characteristic level.,2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401495,no
133,A Visualization Framework for Web Service Discovery and Selection Based on Quality of Service,"The visualization of Web service execution process is an emerging research in service-oriented computing (SOC) area. This paper presents a practical visualization framework in putting service discovery and selection process based on quality of service (QoS) attributes into a visual context. The proposed practical architecture serves as a foundation for designing the novel GUIs for different users. By considering the preferences and priorities for service consumers and service providers, we adopt different application UI design criteria and design patterns which tailored to service-based visualization design. Based on that, evaluation is carried out to test the usability, effectiveness and acceptability of the proposed visualization framework.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4414476,no
134,Applying Interface-Contract Mutation in Regression Testing of Component-Based Software,"Regression testing, which plays an important role in software maintenance, usually relies on test adequacy criteria to select and prioritize test cases. However, with the wide use and reuse of black-box components, such as reusable class libraries and COTS components, it is challenging to establish test adequacy criteria for testing software systems built on components whose source code is not available. Without source code or detailed documents, the misunderstanding between the system integrators and component providers has become a main factor of causing faults in component-based software. In this paper, we apply mutation on interface contracts, which can describe the rights and obligations between component users and providers, to simulate the faults that may occur in this way of software development. The mutation adequacy score for killing the mutants of interface contracts can serve as a test adequacy criterion. We performed an experimental study on three subject systems to evaluate the proposed approach together with four other existing criteria. The experimental results show that our adequacy criterion is helpful for both selecting good-quality test cases and scheduling test cases in an order of exposing faults quickly in regression testing of component-based software.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362630,no
135,Combinatorial Interaction Regression Testing: A Study of Test Case Generation and Prioritization,"Regression testing is an expensive part of the software maintenance process. Effective regression testing techniques select and order (or prioritize) test cases between successive releases of a program. However, selection and prioritization are dependent on the quality of the initial test suite. An effective and cost efficient test generation technique is combinatorial interaction testing, CIT, which systematically samples all t-way combinations of input parameters. Research on CIT, to date, has focused on single version software systems. There has been little work that empirically assesses the use of CIT test generation as the basis for selection or prioritization. In this paper we examine the effectiveness of CIT across multiple versions of two software subjects. Our results show that CIT performs well in finding seeded faults when compared with an exhaustive test set. We examine several CIT prioritization techniques and compare them with a re-generation/prioritization technique. We find that prioritized and re-generated/prioritized CIT test suites may find faults earlier than unordered CIT test suites, although the re-generated/prioritized test suites sometimes exhibit decreased fault detection.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362638,yes
136,Compatibility and Regression Testing of COTS-Component-Based Software,"Software engineers frequently update COTS components integrated in component-based systems, and can often chose among many candidates produced by different vendors. This paper tackles both the problem of quickly identifying components that are syntactically compatible with the interface specifications, but badly integrate in target systems, and the problem of automatically generating regression test suites. The technique proposed in this paper to automatically generate compatibility and prioritized test suites is based on behavioral models that represent component interactions, and are automatically generated while executing the original test suites on previous versions of target systems.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222571,yes
137,Control unit for a laboratory motor test bench for monitoring and controlling PMSM and induction motors,"The work presents a state-of-art control unit for pulse width modulated inverter-fed AC motors. The unit is suitable for a wide range of voltage source inverters with frequencies ranging from 1 KHz to up to 100 KHz. The hardware is characterized by a powerful floating point DSP, FPGA unit, asynchronous serial and IEEE 1394 communication interface, and 12 channels Analog/Digital interface with sample and conversion times together equaling 250 ns. The control software is managed by a specially designed real-time multitasking operating system. The operating system guarantees less than 300 ns time duration when jumping from one task to another upon internal or external event. The operating system can be adapted easily for arbitrary number of tasks with various prioritization levels and triggering events, and therefore suitable for interfacing hardware in the loop (HIL) simulation environments. The on-line interaction between the user and the running control software is implemented by a specially designed IEEE1394 driver for Windows XP and a graphical user interface (GUI). This allows graphical and numerical monitoring of software variables and their modification at will.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4417431,no
138,Data Mining Static Code Attributes to Learn Defect Predictors,"The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of ""McCabes versus Halstead versus lines of code counts"" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027145,no
139,Effective Fault Localization using Code Coverage,"Localizing a bug in a program can be a complex and time- consuming process. In this paper we propose a code coverage-based fault localization method to prioritize suspicious code in terms of its likelihood of containing program bugs. Code with a higher risk should be examined before that with a lower risk, as the former is more suspicious (i.e., more likely to contain program bugs) than the latter. We also answer a very important question: how can each additional test case that executes the program successfully help locate program bugs? We propose that with respect to a piece of code, the aid introduced by the first successful test that executes it in computing its likelihood of containing a bug is larger than or equal to that of the second successful test that executes it, which is larger than or equal to that of the third successful test that executes it, etc. A tool, chiDebug, was implemented to automate the computation of the risk of the code and the subsequent prioritization of suspicious code for locating program bugs. A case study using the Siemens suite was also conducted. Data collected from our study support the proposal described above. They also indicate that our method (in particular Heuristics III (c), (d), and (e)) can effectively reduce the search domain for locating program bugs.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291037,no
140,Enhancing the Efficiency of Regression Testing through Intelligent Agents,"Software testing is indispensable for all software development. As all mature engineering disciplines need to have systematic testing methodologies, software testing is a very important subject of software engineering. In software development practice, testing accounts for as much as 50% of total development efforts. Testing can be manual, automated, or a combination of both.Manual testing is the process of executing the application and manually interacting with the application, specifying inputs and observing outputs. Manually testing the software is inefficient and costly. It is imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process, which contains many testing related activities using various techniques and methods. In order to automate the process, we have to have some ways to generate oracles from the specification, and generate test cases to test the target software against the oracles to decide their correctness. Today we still don't have a full-scale system that has achieved this goal. In general, significant amount of human intervention is still needed in testing. The degree of automation remains at the automated test script level. This paper therefore provides a timely summary and enhancement of agent theory in software testing ,which motivates recent efforts in adapting concepts and methodologies for agent oriented software testing to complex system which has not previously done. Agent technologies facilitate the automated software testing by virtue of their high level decomposition, independency and parallel activation[4]. Usage of agent based regression testing reduces the complexity involved in prioritizing the testcases.With the ability of agents to act autonomously, monitoring code changes and generating test cases for the changed version of the code can be done dynamically. Agent-Oriented Software testing (AOST) is a nascent but active field of research . A comprehensive methodology that plays an essential role in Software testing must be robust but easy-to use. Moreover, it should provide a roadmap to guide engineers in creating Agent-based systems. The agent based regression testing(ABRT) proposed here is to offer a definition for encompassing to cover the regression testing phenomena based on agents, yet sufficiently tight that it can rule out complex systems that are clearly not agent based.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426561,no
141,"Increased Mars Rover Autonomy using AI Planning, Scheduling and Execution","This paper presents technology for performing autonomous commanding of a planetary rover. Through the use of AI planning, scheduling and execution techniques, the OASIS autonomous science system provides capabilities for the automated generation of a rover activity plan based on science priorities, the handling of opportunistic science, including new science targets identified by onboard data analysis software, other dynamic decision-making such as modifying the rover activity plan in response to problems or other state and resource changes. We first describe some of the particular challenges this work has begun to address and then describe our system approach. Finally, we report on our experience testing this software with a Mars rover prototype.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209854,no
142,Indexing Noncrashing Failures: A Dynamic Program Slicing-Based Approach,"Recent software systems usually feature an automated failure reporting component, with which a huge number of failures are collected from software end-users. With a proper support of failure indexing, which identifies failures due to the same fault, the collected failure data can help developers prioritize failure diagnosis, among other utilities of the failure data. Since crashing failures can be effectively indexed by program crashing venues, current practice has seen great success in prioritizing crashing failures. A recent study of bug characteristics indicates that as excellent memory checking tools are widely adopted, semantic bugs and the resulting noncrashing failures have become dominant. Unfortunately, the problem of how to index non-crashing failures has not been seriously studied before. In previous study, two techniques have been proposed to index noncrashing failures, and they are T-Proximity and R-Proximity. However, as T-Proximity indexes failures by the profile of the entire execution, it is generally not effective because most information in the profile is fault-irrelevant. On the other hand, although R-Proximity is more effective than T-Proximity, it relies on a sufficient number of correct executions that may not be available in practice. In this paper, we propose a dynamic slicing-based approach, which does not require any correct executions, and is comparably effective as R-Proximity. A detailed case study with gzip is reported, which clearly strates the advantages of the proposed approach.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362658,no
143,Model Checking Safety-Critical Systems Using Safecharts,"With rapid developments in science and technology, we now see the ubiquitous use of different types of safety-critical systems in our daily lives such as in avionics, consumer electronics, and medical systems. In such systems, unintentional design faults might result in injury or even death to human beings. To make sure that safety-critical systems are really safe, there is a need to verify them formally. However, the verification of such systems is getting more and more difficult because designs are becoming very complex. To cope with high design complexity, currently, model-driven architecture design is becoming a well-accepted trend. However, existing methods of testing and standards conformance are restricted to implementation code, so they do not fit very well with model-based approaches. To bridge this gap, we propose a model-based formal verification technique for safety-critical systems. In this work, the model-checking paradigm is applied to the Safecharts model, which was used for modeling but not yet used for verification. Our contributions listed are as follows: first, the safety constraints in Safecharts are mapped to semantic equivalents in timed automata for verification. Second, the theory for safety constraint verification is proven and implemented in a compositional model checker (that is, the state-graph manipulator (SGM)). Third, prioritized and urgent transitions are implemented in SGM to model the risk semantics in Safecharts. Finally, it is shown that the priority-based approach to mutual exclusion of resource usage in the original Safecharts is unsafe and corresponding solutions are proposed. Application examples show the feasibility and benefits of the proposed model-driven verification of safety-critical systems",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4141241,no
144,Multi - Layered Best Basis Image Compression,"Software testing is indispensable for all software development. As all mature engineering disciplines need to have systematic testing methodologies, software testing is a very important subject of software engineering. In software development practice, testing accounts for as much as 50% of total development efforts. Testing can be manual, automated, or a combination of both. Manual testing is the process of executing the application and manually interacting with the application, specifying inputs and observing outputs. Manually testing the software is inefficient and costly. It is imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process, which contains many testing related activities using various techniques and methods. In order to automate the process, we have to have some ways to generate oracles from the specification, and generate test cases to test the target software against the oracles to decide their correctness. Today we still don't have a full-scale system that has achieved this goal. In general, significant amount of human intervention is still needed in testing. The degree of automation remains at the automated test script level. This paper therefore provides a timely summary and enhancement of agent theory in software testing ,which motivates recent efforts in adapting concepts and methodologies for agent oriented software testing to complex system which has not previously done. Agent technologies facilitate the automated software testing by virtue of their high level decomposition, independency and parallel activation. Usage of agent based regression testing reduces the complexity involved in prioritizing the test cases.With the ability of agents to act autonomously, monitoring code changes and generating test cases for the changed version of the code can be done dynamically. Agent-oriented software testing (AOST) is a nascent but active field of research . A comprehensive methodology that plays an essential role in software testing must be robust but easy-to use. Moreover, it should provide a roadmap to guide engineers in creating agent-based systems. The agent based regression testing (ABRT) proposed here is to offer a definition for encompassing to cover the regression testing phenomena based on agents, yet sufficiently tight that it can rule out complex systems that are clearly not agent based.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426345,no
145,Prioritization of Regression Tests using Singular Value Decomposition with Empirical Change Records,"During development and testing, changes made to a system to repair a detected fault can often inject a new fault into the code base. These injected faults may not be in the same files that were just changed, since the effects of a change in the code base can have ramifications in other parts of the system. We propose a methodology for determining the effect of a change and then prioritizing regression test cases by gathering software change records and analyzing them through singular value decomposition. This methodology generates clusters of files that historically tend to change together. Combining these clusters with test case information yields a matrix that can be multiplied by a vector representing a new system modification to create a prioritized list of test cases. We performed a post hoc case study using this technique with three minor releases of a software product at IBM. We found that our methodology suggested additional regression tests in 50% of test runs and that the highest-priority suggested test found an additional fault 60% of the time.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402199,yes
146,Prioritized Constraints with Data Sampling Scores for Automatic Test Data Generation,"Many automatic test data generation approaches use constraint solvers to find data values. One problem with this method is that it cannot generate test data when the constraints are not solvable, either because there is no solution or the constraints are too complex. We propose a constraint prioritization method using data sampling scores to generate valid test data even when a set of constraints is not solvable. Our case study illustrates the effectiveness of this method.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288019,no
147,Prioritizing Coverage-Oriented Testing Process - An Adaptive-Learning-Based Approach and Case Study,"This paper proposes a graph-model-based approach to prioritizing the test process. Tests are ranked according to their preference degrees which are determined indirectly, i.e., through classifying the events. To construct the groups of events, unsupervised neural network is trained by adaptive competitive learning algorithm. A case study demonstrates and validates the approach.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291124,yes
148,Search Algorithms for Regression Test Case Prioritization,"Regression testing is an expensive, but important, process. Unfortunately, there may be insufficient resources to allow for the reexecution of all test cases during regression testing. In this situation, test case prioritization techniques aim to improve the effectiveness of regression testing by ordering the test cases so that the most beneficial are executed first. Previous work on regression test case prioritization has focused on greedy algorithms. However, it is known that these algorithms may produce suboptimal results because they may construct results that denote only local minima within the search space. By contrast, metaheuristic and evolutionary search algorithms aim to avoid such problems. This paper presents results from an empirical study of the application of several greedy, metaheuristic, and evolutionary search algorithms to six programs, ranging from 374 to 11,148 lines of code for three choices of fitness metric. The paper addresses the problems of choice of fitness metric, characterization of landscape modality, and determination of the most suitable search technique to apply. The empirical results replicate previous results concerning greedy algorithms. They shed light on the nature of the regression testing search space, indicating that it is multimodal. The results also show that genetic algorithms perform well, although greedy approaches are surprisingly effective, given the multimodal nature of the landscape",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123325,yes
149,Software Fault Localization Based on Testing Requirement and Program Slice,"A heuristic approach is proposed to locate a fault according to the priority. To a given test case wt, fault localization has to be proceeded when its output wrong. Firstly, four assistant test cases, one failed and three successful test cases, are selected out according to the biggest cardinality of Req(wt,t<sub>i</sub>), which stand for the common testing requirements both covered by wt and t<sub>i</sub>. Then, code prioritization methodology is put forward based on program slice technique. Dynamic slice technique is taken for wt and execution slice technique for four assistant test cases. Some dices are constructed with different priority which means the possibility of containing bug and is evaluated according to the occurrences in the selected slices. Thirdly, the key algorithm including two procedures, refining and augmenting, is followed here to fault localization based on priority. In the refining phase, the most suspicious codes am checked step by step; in the augmenting phase, more codes will be gradually considered on the basis of direct data dependency. At last, experimental studies are performed to illustrate the effectiveness of the technique.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286423,no
150,Speaking Truth to Power,"Whenever the author conducts an architectural assessment for software development projects, he endeavors to speak truth to power: those with true power never fear the truth. Sam Guckenheimer has observed that in software code there is truth. Code represents the stark reality of a software development organization's labor. There is also truth to be found in a system's architecture. Every system's architecture is molded by the forces that swirl around it, and the collective concerns of all the stakeholders represent the most dynamic forces shaping a system. The software development organization's unique task is to address all the essential concerns of all the important stakeholders and to ensure that they aren't blindsided by unexpected problems and stakeholders. This is why employing a process that incrementally and iteratively grows a system's architecture through the regular release of testable executables is so important. Such a process lets the software team engage the right stakeholders at the right time and to make the right decisions, neither too early nor too late. In creating a software-intensive system that's both relevant and beautiful, every stakeholder, no matter how close or how far from the code, deserves the truth",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118643,no
151,Techniques for building excellent Operator Machine Interfaces (OMI),"Establishing a process to continually improve understanding of operator requirements -the why as well as the how-is key to developing an optimal Operator-Machine Interface (OMI) for a large program. The P-8A program is one example of a complex software development, with over 2 million lines of code and a unique and complex OMI. The OMI itself is used as a tool for achieving, and is designed in parallel with understanding end goals, operator decision making chains, and thought processes. The OMI can be designed to support and anticipate information needs, and alert operators to unusual occurrences. Operator actions and decision making techniques can be discovered by using expert operators and capturing their knowledge through the use of multiple tools to identify the problem set: storytelling, decision mapping, is/is not matrices, etc. When a solution set has been identified and agreed to by the customer, resources allocated, and schedule set, a standard process captures and prioritizes further changes, identifies their impact, and decides on implementation. Documents describing design and processes and a Design Description Document describing the current version of the OMI are made accessible to stakeholders at all times. ""What's important is not that we can conceive the idea, but that when we actually test it on people you discover it doesn't work... your intuition is wrong."" -Daniel M. Russell (IBM Almaden / Xerox PARC).",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4391947,no
152,Test Case Prioritization Based on Varying Testing Requirement Priorities and Test Case Costs,"Test case prioritization is an effective and practical technique in regression testing. It schedules test cases in order of precedence that increases their ability to meet some performance goals, such as code coverage, rate of fault detection. In previous work, the test case prioritization techniques and metrics usually assumed that testing requirement priorities and test case costs are uniform. In this paper, basing on varying testing requirement priorities and test case costs, we present a new, general test case prioritization technique and an associated metric. The case study illustrates that the rate of ""units-oftesting-requirement-priority-satisfied- per-unit-test-case-cost"" can be increased, and then the testing quality and customer satisfaction can be improved.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385476,yes
153,Test Case Prioritization for Black Box Testing,"Test case prioritization is an effective and practical technique that helps to increase the rate of regression fault detection when software evolves. Numerous techniques have been reported in the literature on prioritizing test cases for regression testing. However, existing prioritization techniques implicitly assume that source or binary code is available when regression testing is performed, and therefore cannot be implemented when there is no program source or binary code to be analyzed. In this paper, we presented a new technique for black box regression testing, and we performed an experiment to measure our technique. Our results show that the new technique is helpful to improve the effectiveness of fault detection when performing regression test in black box environment.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291039,yes
154,The Effect of Organization Process Focus and Organizational Learning on Project Performance: An Examination of Taiwan's Companies,"The impact of organizational learning on project performance has received a great deal of attention in recent years. Process focus is recognized as one of five factors which help to promote organizational learning through out the process. A theoretical model is derived based upon prior researches in literature to examine the effects of organizational learning and process focus on project performance. The structural equation modeling was adopted to test the proposed hypotheses, and Taiwanese corporate IS companies served as examples. The results revealed that organization process focus has a positive impact on organizational learning, which in turn has a positive influence on project performance. Both organization process focus and organizational learning play the influence on project performance. These findings should give valuable information for managers to revisit their priorities in terms of the relative efforts in organization process focus and organization learning.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349429,no
155,The Impact of Organizational Learning on Lack of Team's Expertise Risk in Information Systems Projects,"During the past decade, information systems investment has grown rapidly worldwide and information systems project development has become one of the most important targets in e-business. Yet, the failure of information systems projects is a common occurrence in many organizations around the world. A theoretical model is derived based upon organizational learning theory and prior research in order to examine the effects of organizational learning on lack of team's expertise risk. A survey method is applied to test the hypotheses proposed by the research model, and Taiwanese corporate companies serve as examples. After survey by questionnaire and analyze the data by structure equation modeling, the result reveals that organizational learning has significantly negative impacts on all of the lack of development expertise risk, lack of domain expertise risk, and lack of general expertise risk. These findings support information systems managers with valuable information to revisit their priorities in terms of the relative efforts in organization learning.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402174,no
156,The need for self-managed access nodes in grid environments.,"The Grid is constantly growing and it is being used by more and more applications. In this scenario the entry node is an important component in the whole architecture and will become a contention point. In this paper we will demonstrate that the use of a self-managed layer on the entry node of a grid is necessary. A self-managed system can allow more jobs to be accepted and finished correctly. Since it's not acceptable for a grid middleware layer to lose jobs, we would normally need to prioritize the finishing/acceptance of jobs over the response time or the throughput. A prototype of what could be considered an autonomous system, is presented and tested over an installation of Globus Toolkit (GT4) and shows that we can greatly improve the performance of the original middleware by a factor of 30%. In this paper GT is used as an example but it could be added to any grid middleware",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148878,no
157,The TeraPaths Testbed: Exploring End-to-End Network QoS,"The TeraPaths project at Brookhaven National Laboratory (BNL) investigates the combination of DiffServ-based LAN QoS with WAN MPLS tunnels in creating end-to-end (host-to-host) virtual paths with bandwidth guarantees. These virtual paths prioritize, protect, and throttle network flows in accordance with site agreements and user requests, and prevent the disruptive effects that conventional network flows can cause in one another. This paper focuses on the TeraPaths testbed, a collection of end-site subnets connected through high-performance WANs, serving the research and software development needs of the TeraPaths project. The testbed is rapidly evolving towards a multiple end-site infrastructure, dedicated to QoS networking research, and it offers unique opportunities for experimentation with minimal or no impact on regular, production networking operations.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4444698,no
158,Toward the Use of Automated Static Analysis Alerts for Early Identification of Vulnerability- and Attack-prone Components,"Extensive research has shown that software metrics can be used to identify fault- and failure-prone components. These metrics can also give early indications of overall software quality. We seek to parallel the identification and prediction of fault- and failure-prone components in the reliability context with vulnerability- and attack-prone components in the security context. Our research will correlate the quantity and severity of alerts generated by source code static analyzers to vulnerabilities discovered by manual analyses and testing. A strong correlation may indicate that automated static analyzers (ASA), a potentially early technique for vulnerability identification in the development phase, can identify high risk areas in the software system. Based on the alerts, we may be able to predict the presence of more complex and abstract vulnerabilities involved with the design and operation of the software system. An early knowledge of vulnerability can allow software engineers to make informed risk management decisions and prioritize redesign, inspection, and testing efforts. This paper presents our research objective and methodology.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271764,no
159,Towards an empirical method of efficiency testing of system parts: A methodological study,"Current usability evaluation methods are essentially holistic in nature. However, engineers that apply a component-based software engineering approach might also be interested in understanding the usability of individual parts of an interactive system. This paper examines the efficiency dimension of usability by describing a method, which engineers can use to test, empirically and objectively, the physical interaction effort to operate components in a single device. The method looks at low-level events, such as button clicks, and attributes the physical effort associated with these interaction events to individual components in the system. This forms the basis for engineers to prioritise their improvement effort. The paper discusses face validity, content validity, criterion validity, and construct validity of the method. The discussion is set within the context of four usability tests, in which 40 users participated to evaluate the efficiency of four different versions of a mobile phone. The results of the study show that the method can provide a valid estimation of the physical interaction event effort users made when interacting with a specific part of a device.",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8147077,no
160,Value-Oriented Requirements Prioritization in a Small Development Organization,"Requirements engineering, especially requirements prioritization and selection, plays a critical role in overall project development. In small companies, this often difficult process can affect not only project success but also overall company survivability. A value-oriented prioritization (VOP) framework can help this process by clarifying and quantifying the selection and prioritization issues. A case study of a small development company shows a successful VOP deployment that improved communications and saved time by focusing requirements decisions for new product releases on core company values",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052549,no
161,Verification and Validation of (Real Time) COTS Products using Fault Injection Techniques,"With the goal of reducing time to market and project costs, the current trend of real time business and mission critical systems is evolving from the development of custom made applications to the use of commercial off the shelf (COTS) products. Obviously, the same confidence and quality of the custom made software components is expected from the commercial applications. In most cases, such products (COTS) are not designed with stringent timing and/or safety requirements as priorities. Thus, to decrease the gap between the use of custom made components and COTS components, this paper presents a methodology for evaluating COTS products in the scope of dependable, real time systems, through the application of fault injection techniques at key points of the software engineering process. By combining the use of robustness testing (fault injection at interface level) with software fault injection (using educated fault injection operators), a COTS component can be assessed in the context of the system it will belong to, with special emphasis given to timing and safety constraints that are usually imposed by the target real time dependable environment. In the course of this work, three case studies have been performed to assess the methodology using realistic scenarios that used common COTS products. Results for one case study are presented",2007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127318,no
162,"""It's Not the Pants, it's the People in the Pants"" Learnings from the Gap Agile Transformation What Worked, How We Did it, and What Still Puzzles Us","After 7 years of traditional IT delivery, Gap Inc.Direct decided to adopt Agile. This experience report discusses three key factors that contributed to our successful (and ongoing) Agile transformation: 1. Ambitious Pilot Project, 2. Massive Investment in Continuous Integration, 3. Rethinking our Assets. The choices we made might seem risky and even counter-intuitive, but understanding them could help other organizations consider different points of view and priorities as they embark on the transition to Agile. Additionally, we will identify ongoing challenges and what is left in our transformation backlog.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599459,no
163,A dynamic scheduler for balancing HPC applications,"Load imbalance cause significant performance degradation in High Performance Computing applications. In our previous work we showed that load imbalance can be alleviated by modern MT processors that provide mechanisms for controlling the allocation of processors internal resources. In that work, we applied static, hand-tuned resource allocations to balance HPC applications, providing improvements for benchmarks and real applications. In this paper we propose a dynamic process scheduler for the Linux kernel that automatically and transparently balances HPC applications according to their behavior. We tested our new scheduler on an IBM POWER5 machine, which provides a software-controlled prioritization mechanism that allows us to bias the processor resource allocation. Our experiments show that the scheduler reduces the imbalance of HPC applications, achieving results similar to the ones obtained by hand-tuning the applications (up to 16%). Moreover, our solution reduces the application's execution time combining effect of load balance and high responsive scheduling.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5217785,no
164,Adaptive Test Question Selection for Web-Based Educational System,"In this paper we present a method proposed to select test questions adapting to individual needs of students in the context of Web-based educational system. It functions as a combination of three particular methods. First one is based on course structure and focuses on the selection of the most appropriate topic for learning, second uses the Item Response Theory to select k-best questions with adequate difficulty for particular learner and the last is based on usage history and prioritizes questions according to specific strategies, e.g. to filter out the questions that was recently asked. We describe how these methods evaluate user answers to gather information concerning their characteristics for more precise selection of further questions. We evaluated proposed method within our Web-based system called Flip on domain of functional programming.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724867,no
165,Addressing Low Base Rates in Intrusion Detection via Uncertainty-Bounding Multi-Step Analysis,"Existing approaches to characterizing intrusion detection systems focus on performance under test conditions. While it is well-understood that operational conditions may differ from test conditions, little attention has been paid to the question of assessing the effect on IDS results of parameter estimation errors resulting from these differences. In this paper we consider this question in the context of multi-step attacks. We derive simulated distributions of the posterior probability of exploit given the observation of a series of alerts and bounds on the posterior uncertainty given a particular distribution of the model parameters. Knowledge of such bounds introduces the novel prospect of a confidence versus agility tradeoff in IDS administration. Such a tradeoff could give administrators flexibility in IDS configuration, allowing them to choose detection confidence at the price of detection latency, according to organizational priorities.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721564,no
166,Agent multiplication: An economical large-scale testing environment for system management solutions,"System management solutions are designed to scale to thousands or more machines and networked devices. However, it is challenging to test and verify the proper operation and scalability of management software given the limited resources of a testing lab. We have developed a method called agent multiplication, in which one physical testing machine is used to represent hundreds of client machines. This provides the necessary client load to test the performance and scalability of the management software and server within limited resources. In addition, our approach guarantees that the test environment remains consistent between test runs, ensuring that test results can be meaningfully compared. We used agent multiplication to test and verify the operation of a server managing 4,000 systems. We exercised the server functions with only 8 test machines. Applying this test environment to an early version of a real enterprise system management solution we were able to uncover critical bugs, resolve race conditions, and examine and adjust thread prioritization levels for improved performance.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536552,no
167,An Empirical Study on Bayesian Network-based Approach for Test Case Prioritization,"A cost effective approach to regression testing is to prioritize test cases from a previous version of a software system for the current release. We have previously introduced a new approach for test case prioritization using Bayesian Networks (BN) which integrates different types of information to estimate the probability of each test case finding bugs. In this paper, we enhance our BN-based approach in two ways. First, we introduce a feedback mechanism and a new change information gathering strategy. Second, a comprehensive empirical study is performed to evaluate the performance of the approach and to identify the effects of using different parameters included in the technique. The study is performed on five open source Java objects. The obtained results show relative advantage of using feedback mechanism for some objects in terms of early fault detection. They also provide insight into costs and benefits of the various parameters used in the approach.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539555,no
168,Application of system models in regression test suite prioritization,"During regression testing, a modified system needs to be retested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Most of the existing test prioritization methods are based on the code of the system, but model-based test prioritization has been recently proposed. System modeling is a widely used technique to model state-based systems. The existing model based test prioritization methods can only be used when models are modified during system maintenance. In this paper, we present model-based prioritization for a class of modifications for which models are not modified (only the source code is modified). After identification of elements of the model related to source-code modifications, information collected during execution of a model is used to prioritize tests for execution. In this paper, we discuss several model-based test prioritization heuristics. The major motivation to develop these heuristics was simplicity and effectiveness in early fault detection. We have conducted an experimental study in which we compared model-based test prioritization heuristics. The results of the study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658073,yes
169,Application-Level QoS: Improving Video Conferencing Quality through Sending the Best Packet Next,"In a traditional network stack, data from an application is transmitted in the order that it is received. An algorithm is proposed where information about the priority of packets and expiry times is used by the transport layer to reorder or discard packets at the time of transmission to optimise the use of available bandwidth. This can be used for video conferencing to prioritise important data. This scheme is implemented and compared to unmodified datagram congestion control protocol (DCCP). This algorithm is implemented as an interface to DCCP and tested using traffic modelled on video conferencing software. The results show improvement can be made to video conferencing during periods of congestion - substantially more audio packets arrive on time with the algorithm, which leads to higher quality video conferencing. In many cases video packet arrival rate also increases and adopting the algorithm gives improvements to video conferencing that are better than using unmodified queuing for DCCP. The algorithm proposed is implemented on the server only, so benefits can be obtained on the client without changes being required to the client.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756479,no
170,Applying Particle Swarm Optimization to Prioritizing Test Cases for Embedded Real Time Software Retesting,"In recent years, complex embedded systems are used in every device that is infiltrating our daily lives. Since most of the embedded systems are multi-tasking real time systems, the task interleaving issues, dead lines and other factors needs software units retesting to follow the subsequence changes. Regression testing is used for the software maintenance that revalidates the old functionality of the software unit. Testing is one of the most complex and time-consuming activities, in which running of all combination of test cases in test suite may require a large amount of efforts. Test case prioritization techniques can take advantage that orders test cases, which attempts to increase effectiveness in regression testing. This paper proposes to use particle swarm optimization (PSO) algorithm to prioritize the test cases automatically based on the modified software units. Regarding to the recent investigations, PSO is a multi-object optimization technique that can find out the best positions of the objects. The goal is to prioritize the test cases to the new best order, based on modified software components, so that test cases, which have new higher priority, can be selected in the regression testing process. The empirical results show that by using the PSO algorithm, the test cases can be prioritized in the test suites with their new best positions effectively and efficiently.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4568558,yes
171,Automated Generation and Assessment of Autonomous Systems Test Cases,"Verification and validation testing of autonomous spacecraft routinely culminates in the exploration of anomalous or faulted mission-like scenarios. Prioritizing which scenarios to develop usually comes down to focusing on the most vulnerable areas and ensuring the best return on investment of test time. Rules-of-thumb strategies often come into play, such as injecting applicable anomalies prior to, during, and after system state changes; or, creating cases that ensure good safety-net algorithm coverage. Although experience and judgment in test selection can lead to high levels of confidence about the majority of a system's autonomy, it's likely that important test cases are overlooked. One method to fill in potential test coverage gaps is to automatically generate and execute test cases using algorithms that ensure desirable properties about the coverage. For example, generate cases for all possible fault monitors, and across all state change boundaries. Of course, the scope of coverage is determined by the test environment capabilities, where a faster-than-real-time, high-fidelity, software-only simulation would allow the broadest coverage. Even real-time systems that can be replicated and run in parallel, and that have reliable set-up and operations features provide an excellent resource for automated testing. Making detailed predictions for the outcome of such tests can be difficult, and when algorithmic means are employed to produce hundreds or even thousands of cases, generating predicts individually is impractical, and generating predicts with tools requires executable models of the design and environment that themselves require a complete test program. Therefore, evaluating the results of large number of mission scenario tests poses special challenges. A good approach to address this problem is to automatically score the results based on a range of metrics. Although the specific means of scoring depends highly on the application, the use of formal scoring metrics has high value in identifying and prioritizing anomalies, and in presenting an overall picture of the state of the test program. In this paper we present a case study based on automatic generation and assessment of faulted test runs for the Dawn mission, and discuss its role in optimizing the allocation of resources for completing the test program.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526484,no
172,Change Priority Determination in IT Service Management Based on Risk Exposure,"In the Change Management process within IT Service Management, some activities need to evaluate the risk exposure associated with changes to be made to the infrastructure and services. The paper presents a method to evaluate risk exposure associated with a change. Further, we show how to use the risk exposure metric to automatically assign priorities to changes. The formal model developed for this purpose captures the business perspective by using financial metrics in the evaluation of risk. Thus the method is an example of Business-Driven IT Management. A case study, performed in conjunction with a large IT service provider, is reported and provides good results when compared to decisions made by human managers.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4805134,no
173,Choosing the Right Prioritisation Method,"There are many methods available for prioritising software requirements. Choosing the most suitable one can often be quite difficult. A number of factors need to be considered such as the project development methodology being used, the amount of time available, the amount of information known about requirements, the stage of the project and the degree of information about priority required. This paper examines the type of information available at different stages in a project and matches it to the properties of prioritisation methods. It then recommends the usage of specific prioritisation methods at certain stages of a project.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483241,no
174,Collaborative group membership and access control for JXTA,"This paper presents a proposal for group membership and access control services for JXTA, both based on the principle of self-organization and collaboration of peer group members. The need for collaboration strengthens the resistance against free riding and eases management of revocation data. The proposal prioritizes group autonomy and makes use of the concepts of web of trust and small world phenomenon in order to achieve its ends, distancing itself from approaches based on centralized PKI models or trusted third parties external to the group. It also offers an alternative to the basic group membership services distributed with the JXTA platform implementations.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554399,no
175,Creating Agile Streams for Business & Technical Value,"Have you ever played the role of business owner and found yourself between ""a rock and a hard place"" of organizational politics when prioritizing backlog features? The Agile Stream approach negates those politics by dedicating development teams to organizational units and allowing those teams to continue working, iteration after iteration, as long as they continue delivering business value.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599466,no
176,Effective RTL Method to Develop On-Line Self-Test Routine for the Processors Using the Wavelet Transform,"In this paper, we introduce a new efficient register transfer level (RTL) method to develop on-line self- test routines. We consider some prioritizations to select the components and instructions of the processor. In addition, we choose test patterns based on spectral RTL test pattern generation (TPG) strategy. For the purpose of spectral analysis, we use the wavelet transform. Also, we use a few extra instructions for the purpose of the signature monitoring to detect control flow errors. We demonstrate that the combination of these three strategies is effective for developing small test programs with high fault coverage in a small test development time. In this case, we only need the instruction set architecture (ISA) and RTL information. Our method not only provides a simple and fast algorithm for on-line self-test applications, also gains the advantages of utilizing lower memory and reducing the test generation time complexities in comparison with proposed methods so far. We focus on the application of this approach for Parwan processor. We develop a self-test routine using our proposed method for Parwan processor and demonstrate the effectiveness of our proposed methodology for on-line testing by presenting experimental results for Parwan processor.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529795,no
177,Evaluating ALPHAN: A Communication Protocol for Haptic Interaction,"In our previous work we introduced a novel application layer protocol, named ALPHAN, for haptic data communication. The protocol is characterized by three distinguished features: first, it is designed at the application layer to enable the application to define and control the networking parameters. Second, it is made highly customizable using XML-based descriptions. Finally, the protocol supports multi-buffering mechanisms to prioritize the communicated information. In this paper, we present a thorough evaluation of the protocol using a collaborative haptic game; the balance ball game that we developed is for this purpose. The performance metrics and the test bed of the protocol evaluation are also discussed. Finally, we comment on our findings and provide directions for prospective research.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479972,no
178,Historical Value-Based Approach for Cost-Cognizant Test Case Prioritization to Improve the Effectiveness of Regression Testing,"Regression testing has been used to support software testing activities and assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive because it requires many test case executions, and the number of test cases increases sharply as the software evolves. In this paper, we propose the Historical Value-Based Approach, which is based on the use of historical information, to estimate the current cost and fault severity for cost-cognizant test case prioritization. We also conducted a controlled experiment to validate the proposed approach, the results of which proved the proposed approachpsilas usefulness. As a result of the proposed approach, software testers who perform regression testing are able to prioritize their test cases so that their effectiveness can be improved in terms of average percentage of fault detected per cost.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579792,yes
179,Incorporating varying requirement priorities and costs in test case prioritization for new and regression testing,"Test case prioritization schedules the test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects the most severe faults at the earliest in its testing life cycle. Test case prioritization techniques have proved to be beneficial for improving regression testing activities. While code coverage based prioritization techniques are found to be taken by most scholars, test case prioritization based on requirements in a cost effective manner has not been taken for study so far. Hence, in this paper, we propose to put forth a model for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software that can also be cost effective and to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the six factors: customer priority, changes in requirement, implementation complexity, usability, application flow and fault impact. The proposed prioritization technique is experimented in three phases with student projects and two sets of industrial projects and the results show convincingly that the proposed prioritization technique improves the rate of severe fault detection.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787662,yes
180,mod kaPoW: Protecting the web with transparent proof-of-work,"Attacks from automated Web clients are a significant problem on the Internet. Web sites often employ Turing tests known as CAPTCHAs to combat automated agents. Unfortunately, such defenses require frequent human user input, are becoming less effective as computer vision techniques improve, and can be subverted by adversaries willing to hire humans to solve challenges. Several alternative defenses based upon cryptographic methods rather than human input have been proposed to achieve the same goals. Such ""proof-of-work"" techniques prioritize clients based on their willingness to solve computational challenges of client-specific difficulty set by the server. Unfortunately, few proof-of-work schemes have been deployed since they require wide-scale adoption of special client software to operate properly. To address these problems we present mocLkaPoW, a novel system that has the efficiency and human-transparency of proof-of-work schemes as well as the software backwards-compatibility of CAPTCHA schemes. The system leverages common Web technologies to deliver a challenge, solve it, and submit the client response, while providing accessibility for legacy clients. This paper describes and evaluates a prototype of this system.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544602,no
181,Model for optimizing software testing period using non homogenous poisson process based on cumulative test case prioritization,"Most of the software organizations has trouble when deciding the release dates their product. This difficulty is due to the fact that an under tested software could lead to many bugs propping up at the client side which would in turn lead to expensive bug-fixes and more importantly loss of customer goodwill. On the other hand, testing beyond certain time would lead to loss of revenue to the organization due to the dilution of the early bird advantage. The aim of our paper is optimizes the time and cost of entire software. In this paper we used non homogeneous Poisson process model based on cumulative priority. Our paper also tries to answer when to release any software.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766422,no
182,"Perspective on Embedded Systems: Challenges, Solutions and Research Priorities","This paper introduces THALES vision and research priorities for embedded systems and illustrates them through presentations of solutions and on-going research projects and initiatives. Thales effort related to mission-critical systems is focused on advanced high-performance embedded computing platforms, on middleware technologies, on software systems design and verification tools for safety and security and on the emergence of open standards in these domains. THALES is also actively contributing to the development of innovation eco-systems: the Joint Undertaking ARTEMIS in Europe; the Pole de Competitivite SYSTEM@TIC PARIS REGION in France.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484650,no
183,Prioritizing User-Session-Based Test Cases for Web Applications Testing,"Web applications have rapidly become a critical part of business for many organizations. However, increased usage of Web applications has not been reciprocated with corresponding increases in reliability. Unique characteristics, such as quick turnaround time, coupled with growing popularity motivate the need for efficient and effective Web application testing strategies. In this paper, we propose several new test suite prioritization strategies for Web applications and examine whether these strategies can improve the rate of fault detection for three Web applications and their preexisting test suites. We prioritize test suites by test lengths, frequency of appearance of request sequences, and systematic coverage of parameter-values and their interactions. Experimental results show that the proposed prioritization criteria often improve the rate of fault detection of the test suites when compared to random ordering of test cases. In general, the best prioritization metrics either (1) consider frequency of appearance of sequences of requests or (2) systematically cover combinations of parameter-values as early as possible.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539541,no
184,Providing the Guideline of Determining Quality Checklists Priorities Based on Evaluation Records of Software Products,"COTS (commercial-off-the-shelf) software products are usually provided in a packaged style without the source code but with many ready-to-use functions. Generally, their vendors are reluctant to disclose the source code. Thus, the major way of quality evaluation and certification requires dynamic behavior testing, essentially black-box testing. Since observing every aspect of external software behavior is almost impossible, it is crucial to designate an adequate range for quality evaluation such as an adequate number of quality checklists or product quality metrics for external behavior testing. Hence, to establish rules of selecting quality evaluation criteria in systematic ways, there have been attempts to analyze and utilize the past records of software evaluation. In this paper, multiple characteristics of software are mapped as nodes to affect and determine the priority ranks of external software quality metrics on Bayesian belief network. The nodes are set to be under the influence of multiple inheritances so that every external characteristic of COTS software is considered thoroughly.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724545,no
185,Quota-constrained test-case prioritization for regression testing of service-centric systems,"Test-case prioritization is a typical scenario of regression testing, which plays an important role in software maintenance. With the popularity of Web services, integrating Web services to build service-centric systems (SCSs) has attracted attention of many researchers and practitioners. During regression testing, as SCSs may use up constituent Web servicespsila request quotas (e.g., the upper limit of the number of requests that a user can send to a Web service during a certain time range), the quota constraint may delay fault exposure and the subsequent debugging. In this paper, we investigate quota-constrained test-case prioritization for SCSs, and propose quota-constrained strategies to maximize testing requirement coverage. We divide the testing time into time slots, and iteratively select and prioritize test cases for each time slot using integer linear programming (ILP). We performed an experimental study on our strategies together with three other strategies, and the results show that with the constraint of request quotas, our strategies can schedule test cases for execution in an order with higher effectiveness in exposing faults and achieving total and additional branch coverage.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658074,yes
186,Ranking Attack-Prone Components with a Predictive Model,"Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. An early security risk analysis that ranks software components by probability of being attacked can provide an affordable means to prioritizing fortification efforts to the highest risk components. We created a predictive model using classification and regression trees and the following internal metrics: quantity of Klocwork static analysis warnings, file coupling, and quantity of changed and added lines of code. We validated the model against pre-release security testing failures on a large commercial telecommunications system. The model assigned a probability of attack to each file where upon ranking the probabilities in descending order we found that 72% of the attack-prone files are in the top 10% of the ranked files and 90% in the top 20% of the files.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700353,no
187,Scheduling Product Line Features for Effective Roadmapping,"Large industrial product lines may produce tens of thousands of variants each year. Each variant typically contains both reusable assets as well as product specific code created by different organizational units. To produce this vast number of variants the organizational resources must be used efficiently. For roadmapping this means an ability to schedule production of reusable assets so that all variants can be completed according to their requirements. When aiming for centralized variability management, roadmapping requires effective management of product line feature dependences and priorities. In this paper, we first introduce the problems haunting feature roadmapping in industrial product lines. Then we investigate how these problems can be solved using a novel approach for organizing product lines based on our practical experiences. Finally, we discuss our experiences and compare our approach with results by other researchers.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724548,no
188,Scheduling Timed Modules for Correct Resource Sharing,"Real-time embedded systems typically include concurrent tasks of different priorities with time-dependent operations accessing common resources. In this context, unsynchronized parallel executions may lead to hazard situations caused by e.g., race conditions. To be able to detect such faulty system behaviors before implementation, we introduce a unified model of resource constrained, scheduled real-time system descriptions, in Alur's and Henzinger's rigorous framework of timed reactive modules. We take a component-based design perspective and construct the realtime system model, by refinement, as a composition of realtime periodic preemptible tasks with encoded functionality, and a fixed-priority scheduler, all modeled as timed modules. For the model, we express the notions of race condition and redundant locking, formally, as invariance properties that can be verified by model-checking.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539537,no
189,Slack-based global multiprocessor scheduling of aperiodic tasks in parallel embedded real-time systems,"We provide a constant time schedulability test and priority assignment algorithm for an on-line multiprocessor server handling aperiodic tasks. Dhall's effect is avoided by dividing tasks in two priority classes based on their utilization: heavy and light. The improvement in this paper is due to assigning priority of light tasks based on slack - not on deadlines. We prove that if the load on the multiprocessor stays below (3 - radic5)/2 ap 38.197%, the server can accept an incoming aperiodic task and guarantee that the deadlines of all accepted tasks will be met. This is better than the current state-of- the-art algorithm where the priorities of light tasks are based on deadlines (the corresponding bound is in that case 35.425%).",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493574,no
190,Software development methods and usability: Perspectives from a survey in the software industry in Norway,"This paper investigates the relationship between software development methodologies and usability. The point of departure is the assumption that two important disciplines in software development, one of software development methods (SDMs) and one of usability work, are not integrated in industrial software projects.Building on previous research we investigate two questions; (1) Will software companies generally acknowledge the importance of usability, but not prioritise it in industrial projects? and (2) To what degree are software development methods and usability perceived by practitioners as being integrated? To this end a survey in the Norwegian IT industry was conducted. From a sample of 259 companies we received responses from 78 companies.In response to our first research question, our findings show that although there is a positive bias towards usability, the importance of usability testing is perceived to be much less than that of usability requirements. Given the strong time and cost pressures associated with the software industry, we believe that these results highlight that there is a gap between intention and reality. Regarding our second research question our survey revealed that companies perceive usability and software development methods to be integrated. This is in contrast to earlier research, which, somewhat pessimistically, has argued for the existence of two different cultures, one of software development and one of usability. The findings give hope for the future, in particular because the general use of system development methods are pragmatic and adaptable.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8149869,no
191,Software Quality model Analysis Program,"It is vital that data is obtained so that actions can be taken to improve the performance. Such improvement can be measured in terms of improved quality, increased customer satisfaction and decreased cost of quality. Different researchers have proposed software quality models to help measure the quality of software products. These models often include metrics for this purpose. Some of the classical and recent models are discussed and analyzed in this paper showing the points of strength and weakness of each model type. A new comprehensive model is proposed and analyzed. A complete solution is discussed through the paper to enable an effective and efficient use of the proposed model to help the development team in prioritizing the important metrics while developing the software products according to some inputs from the user and the objectives of the software being developed. The solution developed is called the quality model analysis program (QAP) and is a fuzzy system that weights the proposed model attributes according to certain rules. The solution enables software project managers to better utilize their resources and take specific actions to better improve the quality of the software produced.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4773015,no
192,Software Quality Requirements: How to Balance Competing Priorities,"The elicitation, analysis, and specification of quality requirements involve careful balancing of a broad spectrum of competing priorities. Developers must therefore focus on identifying qualities and designing solutions that optimize the product's value to its stakeholders.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4455627,no
193,Test Case Prioritization Based on Analysis of Program Structure,"Test case prioritization techniques have been empirically proved to be effective in improving the rate of fault detection in regression testing. However, most of previous techniques assume that all the faults have equal severity, which dose not meet the practice. In addition, because most of the existing techniques rely on the information gained from previous execution of test cases or source code changes, few of them can be directly applied to non-regression testing. In this paper, aiming to improve the rate of severe faults detection for both regression testing and non-regression testing, we propose a novel test case prioritization approach based on the analysis of program structure. The key idea of our approach is the evaluation of testing-importance for each module (e.g., method) covered by test cases. As a proof of concept, we implement $Apros$, a test case prioritization tool, and perform an empirical study on two real, non-trivial Java programs. The experimental result represents that our approach could be a promising solution to improve the rate of severe faults detection.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724580,yes
194,Test Case Prioritization for Multiple Processing Queues,"Test case prioritization is an effective technique that helps to increase the rate of fault detection or code coverage in regression testing. However, all existing methods can only prioritize test cases to a single queue. Once there are two or more machines that participate in testing, all exiting techniques are not applicable any more. To extend the prioritization methods to parallel scenario, this paper defines the prioritization problem in such scenario and applies the task scheduling method to prioritization algorithms to help partitioning a test suite into multiple prioritized subsets. Besides, this paper also discusses the limitation of previous metrics and proposes a new measure of effectiveness of prioritization methods in a parallel scenario. Finally, a case study is performed to illustrate the algorithms and metrics presented in this article.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732476,yes
195,"Testing Optimization for Mission-Critical, Complex, Distributed Systems","The goal of the research was to optimize the regression testing of the software application to address the identified problem of a missing, unclear or even contradictory requirement. The approach was mainly aimed at regression test prioritization and selection of regression test cases per test campaigns. A combination of subjective data based on expert knowledge and objective historical data were the inputs to the model where the output was determining the quality of test selection. Proposed model is aimed at finding newly introduced defects, and it could be extremely useful when the system is in the state of cleaning up or ordering requirements.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591679,yes
196,The use of explicit congestion notification to shape traffic of an intelligent satellite system,"Since the official standardization of explicit congestion notification (ECN) in 2001, the differentiated service (DS) bits 6 and 7 in packets are now classified for purposes of shaping and prioritization. These classifications are used to mark packet streams for controlling traffic flow in an intelligent satellite system (ISS). Using a forward link connection between a transmitting ground terminal to a geostationary satellite (GEO) acting as a relay, to a receiving hub back on the earth; a traffic shaping software which evaluates the ECN type or classification is used to control traffic flow and the results are observed.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463618,no
197,Two Case Studies of User Experience Design and Agile Development,"How can user experience design (UED) practices be leveraged in agile development to improve product usability? UED practices identify the needs and goals of the user through user research and testing. By incorporating UED in agile development, user research and testing can be utilized to prioritize features in the product backlog and to iteratively refine designs to achieve better usability. Furthermore, integrating UED and Agile processes can be accomplished with little or no impact on release schedules. The cases studies presented in this paper describe two examples of UED and agile integration at VeriSign.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599534,no
198,Using Statistical Models to Predict Software Regressions,"Incorrect changes made to the stable parts of a software system can cause failures - software regressions. Early detection of faulty code changes can be beneficial for the quality of a software system when these errors can be fixed before the system is released. In this paper, a statistical model for predicting software regressions is proposed. The model predicts risk of regression for a code change by using software metrics: type and size of the change, number of affected components, dependency metrics, developerpsilas experience and code metrics of the affected components. Prediction results could be used to prioritize testing of changes: the higher is the risk of regression for the change, the more thorough testing it should receive.",2008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700331,no
199,A Hybrid Approach to Build Prioritized Pairwise Interaction Test Suites,"Traditional interaction testing aims to build test suites that cover all t-way interactions of inputs. But in many test scenarios, the entire test suites cannot be fully run due to the limited budget. Therefore it is necessary to take the importance of interactions into account and prioritize these tests of the test suite. In the paper, we use the hybrid approach to build prioritized pairwise interaction test suites (PITS). It adopts a one-test-at-a-time strategy to construct final test suites. But to generate a single test it firstly generates a candidate test and then applies a specific metaheuristic search strategy to enhance this test. Here we experiment four different metaheuristic search strategies. In the experiments, we compare our approach to weighted density algorithm (WDA). Meanwhile, we also analyze the effectiveness of four different search strategies and the effectiveness of the increasing iterations. Empirical results demonstrate the effectiveness of our proposed approach.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365886,no
200,A new method of test data generation for branch coverage in software testing based on EPDG and Genetic Algorithm,"A new method called EPDG-GA which utilizes the edge partitions dominator graph (EPDG) and genetic algorithm (GA) for branch coverage testing is presented in this paper. First, a set of critical branches (CBs) are obtained by analyzing the EPDG of the tested program, while covering all the CBs implies covering all the branches of the control flow graph (CFG). Then, the fitness functions are instrumented in the right position by analyzing the pre-dominator tree (PreDT), and two metrics are developed to prioritize the CBs. Coverage-Table is established to record the CBs information and keeps track of whether a branch is executed or not. GA is used to generate test data to cover CBs so as to cover all the branches. The comparison results show that this approach is more efficient than random testing approach.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276897,no
201,A Survey of Coverage-Based Testing Tools,"Test coverage is sometimes used to measure how thoroughly software is tested and developers and vendors sometimes use it to indicate their confidence in the readiness of their software. This survey studies and compares 17 coverage-based testing tools primarily focusing on, but not restricted to, coverage measurement. We also survey features such as program prioritization for testing, assistance in debugging, automatic generation of test cases and customization of test reports. Such features make tools more useful and practical, especially for large-scale, commercial software applications. Our initial motivations were both to understand the available test coverage tools and to compare them to a tool that we have developed, called eXVantage (a tool suite that includes code coverage testing, debugging, performance profiling and reporting). Our study shows that each tool has some unique features tailored to its application domains. The readers may use this study to help pick the right coverage testing tools for their needs and environment. This paper is also valuable to those who are new to the practice and the art of software coverage testing, as well as those who want to understand the gap between industry and academia.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8130777,no
202,Adaptive Random Test Case Prioritization,"Regression testing assures changed programs against unintended amendments. Rearranging the execution order of test cases is a key idea to improve their effectiveness. Paradoxically, many test case prioritization techniques resolve tie cases using the random selection approach, and yet random ordering of test cases has been considered as ineffective. Existing unit testing research unveils that adaptive random testing (ART) is a promising candidate that may replace random testing (RT). In this paper, we not only propose a new family of coverage-based ART techniques, but also show empirically that they are statistically superior to the RT-based technique in detecting faults. Furthermore, one of the ART prioritization techniques is consistently comparable to some of the best coverage-based prioritization techniques (namely, the ""additional"" techniques) and yet involves much less time cost.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431769,yes
203,Agent Based Replica Placement in a Data Grid Environement,"In a data grid, large quantities of data files are produced and data replication is applied to reduce data access time. Determining when and where to replicate data in order to meet performance goals in grid systems with many users and files, dynamic network and resource characteristics and changing user behavior is difficult. Therefore efficiency and fast access to replicated data are influenced by the location of the resource holding the replica. In this paper, we present an agent based replica placement algorithm to determine the candidate site for the placement of replica. An agent is deployed at each site holding the master copies of the shared data files. To create a replica, each agent prioritizes the resources in the grid based on the resource configuration, bandwidth in the network and the demand for the replica at their sites and then creates a replica at suitable resource locations. We have carried out the simulation using GridSim Toolkit-4.0 for EU Data Grid Testbed1. The simulation results show that the aggregated data transfer time and the execution time for jobs at various resources is less for agent based replica placement.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231873,no
204,BugFix: A learning-based tool to assist developers in fixing bugs,"We present a tool called BugFix that can assist developers in fixing program bugs. Our tool automatically analyzes the debugging situation at a statement and reports a prioritized list of relevant bug-fix suggestions that are likely to guide the developer to an appropriate fix at that statement. BugFix incorporates ideas from machine learning to automatically learn from new debugging situations and bug fixes over time. This enables more effective prediction of the most relevant bug-fix suggestions for newly-encountered debugging situations. The tool takes into account the static structure of a statement, the dynamic values used at that statement by both passing and failing runs, and the interesting value mapping pairs associated with that statement. We present a case study illustrating the efficacy of BugFix in helping developers to fix bugs.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090029,no
205,Building Prioritized Pairwise Interaction Test Suites with Ant Colony Optimization,"Interaction testing offers a stable cost-benefit ratio in identifying faults. But in many testing scenarios, the entire test suite cannot be fully executed due to limited time or cost. In these situations, it is essential to take the importance of interactions into account and prioritize these tests. To tackle this issue, the biased covering array is proposed and the Weighted Density Algorithm (WDA) is developed. To find a better solution, in this paper we adopt ant colony optimization (ACO) to build this prioritized pairwise interaction test suite (PITS). In our research, we propose four concrete test generation algorithms based on Ant System, Ant System with Elitist, Ant Colony System and Max-Min Ant System respectively. We also implement these algorithms and apply them to two typical inputs and report experimental results. The results show the effectiveness of these algorithms.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381411,no
206,Configuration aware prioritization techniques in regression testing,"Configurable software lets users customize applications in many ways, and is becoming increasingly prevalent. Regression testing is an important but expensive way to build confidence that software changes introduce no new faults as software evolves, resulting in many attempts to improve its performance given limited resources. Whereas problems such as test selection and prioritization at the test case level have been extensively researched in the regression testing literature, they have rarely been considered for configurations, though there is evidence that we should not ignore the effects of configurations on regression testing. This research intends to provide a framework for configuration aware prioritization techniques, evaluated through empirical studies.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071025,yes
207,Dynamic admission control and path allocation for SLAs in DiffServ networks,"Today's converged networks are mainly characterized by their support of real-time and high priority traffic requiring a certain level of quality of service (QoS). In this context, traffic classification and prioritization are key features in providing preferential treatments of the traffic in the core of the network. In this paper, we address the joint problem of path allocation and admission control (JPAC) of new Service Level Agreements (SLA) in a DiffServ domain. In order to maximize the resources utilization and the number of admitted SLAs in the network, we consider a statistical bandwidth constraints allowing for a certain overbooking over the network's links. SLAs' admissibility decisions are based on solving to optimality an integer linear programming (ILP) model. When tested by simulations, numerical results confirm that the proposed model can be solved to optimality for real-sized instances within acceptable computation times and substantially reduces the SLAs blocking probability, compared to a the Greedy mechanism proposed in the literature.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090106,no
208,Experimental Comparison of Code-Based and Model-Based Test Prioritization,"During regression testing, a modified system needs to beretested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Code-based test prioritization methods are based on the source code of the system, whereas model-based test prioritization methods are based on system models. System modeling is a widely used technique to model state-based systems. Models can be used not only during software development but also during testing. In this paper, we briefly overview codebased and model-based test prioritization. In addition, we present an experimental study in which the code based test prioritization and the model-based test prioritization are compared.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976373,yes
209,Formal Study of Prioritized Service Compositions,"This paper presents an enhanced derivation procedure to obtain a system of services, from a given choreography. In addition to the basic framework, we introduce several situations where nondeterminism appears and it is resolved by using a dynamic prioritized system. The priority policy is based on several parameters such as the request dispatching, the response time, the quality of the response, etc. These parameters are identified as resources used by a utility function, which determines the priority of each possible option in a nondeterministic choice.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5633969,no
210,From Cradle to Sprint: Creating a Full-Lifecycle Request Pipeline at Nationwide Insurance,"After a successful transition from a prescriptive waterfall process to Scrum and XP, the Corporate Internet Solutions group at Nationwide Insurance found velocity and efficiency stumbling due to the competing and vague priorities of corporate silos. This presentation discusses how the team evolved the traditional Scrum process to better manage 17 dependent projects, and reluctant internal business partners, through a combination of activities including clear Pre-Discovery activities, scenario planning, RITE usability testing, and kanban-style visual management systems.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5261080,no
211,Fundamentals of risk based inspection ƒ?? a practical approach,"New API structural integrity management recommended practice and post-hurricane inspection bulletin, MMS proposed changes to CFR's on decommissioning, and increased interest in developing risk based inspection plans for offshore assets require better understanding of reliability principles. This paper will discuss the fundamentals of RBI from a practical viewpoint and gives details of recently published applicable DNV offshore standards. Offshore structures must be inspected to maintain an acceptable safety level throughout their lifetime. Inspections have traditionally been based upon experience, and judgment of likelihood and consequence of failure. Risk based inspection planning (RBI) for structures, as developed by DNV, represents a systematic, qualitative and quantitative approach which combines theoretical models, test results and in-service experiences. The method is specially developed for application to all types of offshore structures including jackets, jack-ups, TLPs, FPSOs, spars, semi-submersibles, GBSS, and subsea templates. The basis of RBI is to prioritize individual items and systems by considering the associated risks. Attention is given to high risk items, while low risk items receive a more appropriately lesser level of inspection. RBI focuses on cost optimization in all associated activities, to ensure a cost optimal inspection program. The RBI analysis is performed in two steps, comprising a risk screening and a subsequent inspection scheduling. Dedicated software models are utilized to establish the Risk Matrix and to calculate the ?¨time to next inspection?¨. The final deliverable of an RBI analysis is an inspection plan, in which inspection efforts are prioritized from an overall risk perspective.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422330,no
212,Host based intrusion detection using RBF neural networks,A novel approach of host based intrusion detection is suggested in this paper that uses Radial basis Functions Neural Networks as profile containers. The system works by using system calls made by privileged UNIX processes and trains the neural network on its basis. An algorithm is proposed that prioritize the speed and efficiency of the training phase and also limits the false alarm rate. In the detection phase the algorithm provides implementation of window size to detect intrusions that are temporally located. Also a threshold is implemented that is altered on basis of the process behavior. The system is tested with attacks that target different intrusion scenarios. The result shows that the radial Basis Functions Neural Networks provide better detection rate and very low training time as compared to other soft computing methods. The robustness of the training phase is evident by low false alarm rate and high detection capability depicted by the application.,2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353204,no
213,How Well Do Test Case Prioritization Techniques Support Statistical Fault Localization,"In continuous integration, a tight integration of test case prioritization techniques and fault-localization techniques may both expose failures faster and locate faults more effectively. Statistical fault-localization techniques use the execution information collected during testing to locate faults. Executing a small fraction of a prioritized test suite reduces the cost of testing, and yet the subsequent fault localization may suffer. This paper presents the first empirical study to examine the impact of test case prioritization on the effectiveness of fault localization. Among many interesting empirical results, we find that coverage-based and random techniques can be more effective than distribution-based techniques in supporting statistical fault localization.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254274,yes
214,Implementation of the Software Quality Ranks method in the legacy product development environment,"Software quality ranks (SQR) is an important method to manage and improve software quality. Component software quality has a major influence in development project lead time and cost. SQR enables better management and visibility of the quality effort associated with the component implementation. It also provides a roadmap for continuous improvement leading to value add quality attributes like low maintenance, self optimizing software and short development lifecycles. SQR method focuses attention to prioritizing the quality investment on design component level through different quality assurance mechanisms (basic test, code review, desk checks, documentation and other actions). The resulting design delivery to verification phase will be more predictable quality software with shorter lead-time and time-to-market (TTM).",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206363,no
215,Jtop: Managing JUnit Test Cases in Absence of Coverage Information,"Test case management may make the testing process more efficient and thus accelerate software delivery. With the popularity of using JUnit for testing Java software, researchers have paid attention to techniques to manage JUnit test cases in regression testing of Java software. Typically, most existing test case management tools are based on the coverage information. However, coverage information may need extra efforts to obtain. In this paper, we present an Eclipse IDE plug-in (named Jtop) for managing JUnit test cases in absence of coverage information. Jtop statically analyzes the program under test and its corresponding JUnit test cases to perform the following management tasks: regression test case selection, test suite reduction and test case prioritization. Furthermore, Jtop also enables the programmer to manually manipulate test cases through a graphical user interface.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431709,no
216,Keynote: Security Engineering: Developments and Directions,"Security Engineering is a critical component of systems engineering. When complex and large systems are put together, one needs to ensure that the systems are secure. Security engineering methodologies include gathering the security requirements, specifying the security policies, designing the security model, identifying the security critical components of the system design, security verification and validation and security testing. Before installation, one needs to develop a concept of operation (CONOPS) as well as carry out certification and accreditation. Much of the previous work in security engineering has focused on end to end security. That is, the organization needs to ensure that the applications, database systems, operating systems and networks have to be secure. In addition, one needs to ensure security when the subsystems are composed to form a larger system. More recently with open systems and the Web, secure system development is taking a whole new direction. The Office of the Deputy Assistant Secretary of Defense in the United States (Information and Identity Assurance) has stated that ""the Department of Defense's (DoD) policy, planning, and war fighting capabilities are heavily dependent on the information technology foundation provided by the Global Information Grid (GIG). However, the GIG was built for business efficiency instead of mission assurance against sophisticated adversaries who have demonstrated intent and proven their ability to use cyberspace as a tool for espionage and criminal theft of data. GIG mission assurance works to ensure the DoD is able to accomplish its critical missions when networks, services, or information are unavailable, degraded, or distrusted."" To meet the needs of mission assurance challenges, President's (George W. Bush) cyber plan (CNCI) has listed the area of developing multipronged approaches to supply chain risk management as one of the priorities. CNCI states that the reality of global supply chains presents significant challenges in thwarting counterfeit, or maliciously designed hardware and software products. To overcome such challenges and support successful mission assurance we need to design flexible and secure systems whose components may be untrusted or faulty. We need to achieve the secure operation of mission critical systems constructed from untrusted, semitrusted and fully trusted components for successful mission assurance. This keynote address will discuss the developments in security engineering from requirements, to policy to model to design to verification to testing as well as developing CONOPS and conducting certification and accreditation. System evaluation, usability and metrics related issues will also be discussed. Finally we will discuss the changes that have to be made to security engineering to support the next generation of secure systems for mission critical applications.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325402,no
217,Language Identification from an Indian Multilingual Document Using Profile Features,"In order to reach a larger cross section of people, it is necessary that a document should be composed of text contents in different languages. But on the other hand, this causes practical difficulty in OCRing such a document, because the language type of the text should be pre-determined, before employing a particular OCR. In this research work, this problem of recognizing the language of the text content is addressed, however it is perhaps impossible to design a single recognizer which can identify a large number of scripts/languages. As a via media, in this research we have proposed to work on the prioritized requirements of a particular region, for instance in Karnataka state in India,generally any document including official ones, would contain the text in three languages-English-the language of general importance, Hindi-the language of National importance and Kannada -the language of State/Regional importance. We have proposed to learn identifying the language of the text by thoroughly understanding the nature of top and bottom profiles of the printed text lines in these three languages.Experimentation conducted involved 800 text lines for learning and 600 text lines for testing. The performance has turned out to be 95.4%.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804543,no
218,Lightweight Elicitation and Analysis of Software Product Quality Goals: A Multiple Industrial Case Study,"We developed and used a method that gathers relevant stakeholders to elicit, prioritize, and elaborate the quality goals of a software product. It is designed to be lightweight and easy to learn compared to methods for a more comprehensive analysis of non-functional requirements. The method and the resulting quality goals are meant especially for improving the software product management process. We used it in four software product companies, and report lessons learned and evaluation of the method based on practitioners' comments. We found it better to set the goals first for the product in general before discussing a specific release project. In addition to identifying goals that needed improvement, the practitioners considered identifying already achieved goals relevant, but they were neg- lected unless explicitly considered. Using ISO 9126 as a checklist after brainstorming did not add many goals. Prioritization was challenging due to numerous relevant perspectives. Conceiving measures for impor- tant goals seemed to concretize them.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457326,no
219,Measurement and control for risk-based test cases and activities,"Risk-based testing is an approach that consists of a set of activities regarding risk factors identification related to software requirements. Once identified, the risks are prioritized according to its likelihood and impact, and the test cases are projected based on the strategies for treatment of the identified risk factors. Then, test efforts are continuously adjusted according the risk monitoring. Most risk-based testing approaches focuses on activities related to risk identification, analysis and prioritizing. However, metrics are fundamental as they quantify characteristics of a process or product and support software project management activities. In this light, this paper proposes and discusses risk-based testing metrics to measure and control test cases and test activities progress, efforts and costs.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813802,no
220,Mesh your Senses: Multimedia Applications over WiFi-based Wireless Mesh Networks,"This demo aims at (i) validating the design choices we have made in conceiving and deploying the WING testbed, and (ii) showing the capability of out software toolkit to properly support heterogeneous multimedia applications. Additionally, the mesh networking toolkit's fault management features is demonstrated. We hope that our wireless mesh networking toolkit is considered by both researchers and practitioners as platform of choice to test innovative solutions and to provide end-users with wireless connectivity. WING is an experimental multi-radio WMN testbed designed and built exploiting commodity hardware and open-source software components. WING implements a flexible and scalable WMN architecture capable of supporting next-generation Internet services with a particular focus on multimedia applications. The WING project aims at providing an open-platform on top of which innovative solution can be implemented and tested in a realistic environment. Currently, the testbed consist of 10 nodes deployed at CREATE-NET premises and implementing a two-tiers architecture. Other well-known IEEE 802.11-based WMNs include Roofnet, Hyacinth, Microsoft's MCL, and Meraki. We establish the uniqueness of our mesh solution in that it is capable of achieving both service differentiation and performance isolation in IEEE 802.11-based WMNs. While not providing strict QoS performance bounds, the proposed scheme aims at enhancing the perceived quality of experience by combining opportunistic scheduling and packet aggregation and by implementing a DiffServ-like architecture in order to provide traffic prioritization.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172945,no
221,Performance impact analysis with KPP using application response measurement in E-government systems,"In this paper, the performance impact analysis of e-government systems with key performance parameters is being considered. Meaningful impact analysis in sustained government systems is required for considering non-functional requirements and functional requirements. Performance requirements are a critical component of non-functional areas. For example, if a new system change is set to the system, the impact in terms of the response time must be implemented in each sub-system. In this paper, an XML-based framework can be used to analyze performance impacts on sub-systems and can provide a scheme to enhance impact analysis by performance monitoring using application response measurement. Through a health system example as a case study, a performance requirement model to describe extended trees and adapting analysis result of performance monitoring using application response measurement and XML tree representation are addressed. This paper also proposes a scheme for prioritized processing and an algorithm for effectively enhancing impact analysis in a timely fashion.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306282,no
222,Policy-Based Network Management in Home Area Networks: Interim Test Results,"This paper argues that Home Area Networks (HANs) are a good candidate for advanced network management automation techniques, such as Policy-Based Network Management (PBNM). What is proposed is a simple use of policy based network management to introduce some level of Quality of Service (QoS) and Security management in the HAN, whilst hiding this complexity from the home user. In this paper we have presented the interim test results of our research experiments (based on a scenario) using the HAN testbed. After using policies to prioritize different traffic, packet loss decreased to 30% and VoIP quality improved dramatically without employing any intelligent bandwidth allocation technique.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384722,no
223,Predicting Attack-prone Components,"Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. This limitation necessitates security risk management where security efforts are prioritized to the highest risk vulnerabilities that cause the most damage to the end user. We created a predictive model that identifies the software components that pose the highest security risk in order to prioritize security fortification efforts. The input variables to our model are available early in the software life cycle and include security-related static analysis tool warnings, code churn and size, and faults identified by manual inspections. These metrics are validated against vulnerabilities reported by testing and those found in the field. We evaluated our model on a large Cisco software system and found that 75.6% of the system's vulnerable components are in the top 18.6% of the components predicted to be vulnerable. The model's false positive rate is 47.4% of this top 18.6% or 9.1% of the total system components. We quantified the goodness of fit of our model to the Cisco data set using a receiver operating characteristic curve that shows 94.4% of the area is under the curve.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815350,no
224,Prioritization of Scenarios Based on UML Activity Diagrams,"Increased size and complexity of software requires better methods for different activities in the software development lifecycle. Quality assurance of software is primarily done by means of testing, an activity that faces constraints of both time and resources. Hence, there is need to test effectively within the constraints in order to maximize throughput i.e. rate of fault detection, coverage, etc. Test case prioritization involves techniques aimed at finding the best prioritized test suite. In this paper, we propose a prioritization technique based on UML activity diagrams. The constructs of an activity diagram are used to prioritize scenarios. Preliminary results obtained on a case-study indicate that the technique is effective in extracting the critical scenarios from the activity diagram.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231936,no
225,Prioritized Test Generation Strategy for Pair-Wise Testing,"Pair-wise testing is widely used to detect faults in software systems. In many applications where pair-wise testing is needed, the whole test set can not be run completely due to time or budget constraints. In these situations, it is essential to prioritize the tests. In this paper, we drive weight for each value of each parameter, and adapt UWA algorithm to generate an ordered pair-wise coverage test suite. UWA algorithm is to accord weights set for each value of each parameter of the system, then produce ordered pair-wise coverage test set for having generated but unordered one. Finally, a greedy algorithm is adopted to prioritize generated pair-wise coverage test set with driven weights, so that whenever the testing is interrupted, interactions deemed, most important are tested.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368244,no
226,Prioritizing component compatibility tests via user preferences,"Many software systems rely on third-party components during their build process. Because the components are constantly evolving, quality assurance demands that developers perform compatibility testing to ensure that their software systems build correctly over all deployable combinations of component versions, also called configurations. However, large software systems can have many configurations, and compatibility testing is often time and resource constrained. We present a prioritization mechanism that enhances compatibility testing by examining the ldquomost importantrdquo configurations first, while distributing the work over a cluster of computers. We evaluate our new approach on two large scientific middleware systems and examine tradeoffs between the new prioritization approach and a previously developed lowest-cost-configuration-first approach.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306357,yes
227,Prioritizing JUnit test cases in absence of coverage information,"Better orderings of test cases can detect faults in less time with fewer resources, and thus make the debugging process earlier and accelerate software delivery. As a result, test case prioritization has become a hot topic in the research of regression testing. With the popularity of using the JUnit testing framework for developing Java software, researchers also paid attention to techniques for prioritizing JUnit test cases in regression testing of Java software. Typically, most of them are based on coverage information of test cases. However, coverage information may need extra costs to acquire. In this paper, we propose an approach (named Jupta) for prioritizing JUnit test cases in absence of coverage information. Jupta statically analyzes call graphs of JUnit test cases and the software under test to estimate the test ability (TA) of each test case. Furthermore, Jupta provides two prioritization techniques: the total TA based technique (denoted as JuptaT) and the additional TA based technique (denoted as JuptaA). To evaluate Jupta, we performed an experimental study on two open source Java programs, containing 11 versions in total. The experimental results indicate that Jupta is more effective and stable than the untreated orderings and Jupta is approximately as effective and stable as prioritization techniques using coverage information at the method level.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306350,yes
228,Prioritizing test cases for resource constraint environments using historical test case performance data,"Regression testing has been widely used to assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive in that, it requires many test case executions and a large number of test cases. To provide the missing flexibility, researchers introduced prioritization techniques. The aim in this paper has been to prioritize test cases during software regression test. To achieve this, a new equation is presented. The proposed equation considers historical effectiveness of the test cases in fault detection, each test case's execution history in regression test and finally the last priority assigned to the test case. The results of applying the proposed equation to compute the priority of regression test cases for two benchmarks, known as Siemens suite and Space program, demonstrate the relatively faster fault detection in resource and time constrained environments.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234968,yes
229,Prioritizing Use Cases to Aid Ordering of Scenarios,"Models are used as the basis for design and testing of software. The unified modeling language (UML) is used to capture and model the requirements of a software system. One of the major requirements of a development process is to detect defects as early as possible. Effective prioritization of scenarios helps in early detection of defects as well maximize effort and utilization of resources. Use case diagrams are used to represent the requirements of a software system. In this paper, we propose using data captured from the primitives of the use case diagrams to aid in prioritization of scenarios generated from activity diagrams. Interactions among the primitives in the diagrams are used to guide prioritization. Customer prioritization of use cases is taken as one of the factors. Preliminary results on a case study indicate that the technique is effective in prioritization of test scenarios.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358815,yes
230,Quasi-Renewal Time-Delay Fault-Removal Consideration in Software Reliability Modeling,"Software reliability growth models based on a nonhomogeneous Poisson process (NHPP) have been considered as one of the most effective among various models since they integrate the information regarding testing and debugging activities observed in the testing phase into the software reliability model. Although most of the existing NHPP models have progressed successfully in their estimation/prediction accuracies by modifying the assumptions with regard to the testing process, these models were developed based on the instantaneous fault-removal assumption. In this paper, we develop a generalized NHPP software reliability model considering quasi-renewal time-delay fault removal. The quasi-renewal process is employed to estimate the time delay due to identifying and prioritizing the detected faults before actual code change in the software reliability assessment. Model formulation based on the quasi-renewal time-delay assumption is provided, and the generalized mean value function (MVF) for the proposed model is derived by using the method of steps. The general solution of the MVFs for the proposed model is also obtained for some specific existing models. The numerical examples, based on a software failure data set, show that the consideration of quasi-renewal time-delay fault-removal assumption improves the descriptive properties of the model, which means that the length of time delay is getting decreased since testers and programmers adapt themselves to the working environment as testing and debugging activities are in progress.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694128,no
231,Reducing Field Failures in System Configurable Software: Cost-Based Prioritization,"System testing of configurable software is an expensive and resource constrained process. Insufficient testing often leads to escaped faults in the field where failures impact customers and are costly to repair. Prior work has shown that it is possible to efficiently sample configurations for testing using combinatorial interaction testing, and to prioritize these configurations to increase the rate of early fault detection. The underlying assumption to date has been that there is no added complexity to configuring a system level environment over a user configurable one; i.e. the time required to setup and test each individual configuration is nominal. In this paper we examine prioritization of system configurable software driven not only by fault detection but also by the cost of configuration and setup time that moving between different configurations incurs. We present a case study on two releases of an enterprise software system using failures reported in the field. We examine the most effective prioritization technique and conclude that (1) using failure history of configurations can improve the early fault detection rate, but that (2) we must consider fault detection rate over time, not by the number of configurations tested. It is better to test related configurations which incur minimal setup time than to test fewer, more diverse configurations.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362084,no
232,Simple Time-to-Failure Estimation Techniques for Reliability and Maintenance of Equipment,"Proper reliability and maintenance best practice processes have a direct impact on equipment availability, throughput capacity, and spare inventories. The purpose of the time-to-failure estimation (TTFE) technique is to provide a tool for engineers and technicians for risk-based reporting of condition- based maintenance tests and inspections. Through the proper application of this technique, corrective action may be prioritized improving the effectiveness of the maintenance program. Instead of stakeholders being required to make decisions based upon experience only, equipment failure, and repair history can be used to enhance the process, improving the availability of critical equipment.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5191412,no
233,Surviving Insecure IT: Effective Patch Management,"The amount of time to protect enterprise systems against potential vulnerability continues to shrink. Enterprises need an effective patch management mechanism to survive the insecure IT environment. Effective patch management is a systematic and repeatable patch distribution process which includes establishing timely and practical alerts, receiving notification of patches or discovering them, downloading patches and documentation, assessing and prioritizing vulnerabilities, performing testing, deploying patches, and auditing.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804050,no
234,Tag-Based Techniques for Black-Box Test Case Prioritization for Service Testing,"A web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable. The rich interface specifications of a web service, however, provide peer services with a means to formulate black-box testing strategies. In this paper, we formulate new test case prioritization strategies using tags embedded in XML messages to reorder regression test cases, and reveal how the test cases use the interface specifications of services. We evaluate experimentally their effectiveness on revealing regression faults in modified WS-BPEL programs. The results show that the new techniques can have a high probability of outperforming random ordering.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381531,yes
235,Techniques for building excellent operator machine interfaces (OMI),"Establishing a process to continually improve understanding of operator requirements - the why as well as the how - is key to developing an optimal Operator-Machine Interface (OMI) for a large program. The P-8A program is one example of a complex software development, with over 2 million lines of code and a unique and complex OMI. The OMI itself is used as a tool for achieving and is designed in parallel with understanding end goals, operator decision making chains, and thought processes. The OMI can be designed to support and anticipate information needs and alert operators to unusual occurrences. Operator actions and decision-making techniques can be discovered by using expert operators and capturing their knowledge through the use of multiple tools to identify the problem set: storytelling, decision mapping, and is/is not matrices. When a solution set has been identified and agreed to by the customer, resources allocated, and schedule set, a standard process captures and prioritizes further changes, identities their impact, and decides on implementation. Documents describing design and processes and a design description document describing the current version of OMI are made accessible to stakeholders at all times.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5317781,no
236,Test case prioritization based on data reuse an experimental study,"The order in which tests are executed can significantly impact the total test execution time. In this paper, we evaluate two test prioritization techniques (manual and automatic) in the context of mobile phone testing. The manual technique produces test sequences created by test experts, while the automatic one generates sequences mechanically based on the permutation of the tests. Both techniques take into account a data reuse: the more the data is reused among tests, the faster the sequence is executed. In order to evaluate the benefits of these two techniques, we carried out an experiment with 8 testers and 2 test suites arranged in a 2times2 Latin square design replicated four times. The automatic technique reduced approximately 25% of the data generation time and 13.5% of the execution time. The automatic technique is clearly better than the manual one with respect to the generation of sequences. Our experiment showed that the automatic technique also generates sequences whose execution is faster than those created manually by test experts.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5315980,yes
237,Test Selection Prioritization Strategy,"A wide divergence is observed in projects between test activities planned in the test plan and the actual tests that can be executed. Estimates for test execution computed during the planning are inaccurate without test design. The actual time and resources available are usually less than planned. Assuming that time and resources cannot be changed, a dynamic selection of tests for execution that maximizes quality is required.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254084,no
238,Testing Processes in Business-Critical Chain Software Lifecycle,"The business-critical chain lifecycle is an agile software development lifecycle that aims at aligning the software project deliverables to attain the business objectives based on business priorities. The traditional software development projects work on the assumption that all `equal effort consumers' would be treated equally and worked upon. The agile methodology ensures that the delivery cycles are reduced thus introducing agility in the way business is supported by underlying technologies. The proposed lifecycle model introduces new variables pertaining to the business value generation each finished piece of code would produce. Hence the software project processes have to be modified to cater to it. Even the usual agile lifecycle testing strategies need to be modified to suit the proposed model. The test plans, test estimations, test resource management, quality control, regression plans and automation road map and plans have to be customized to cater to the new life cycle model. The secondary project management activities such as risk management, procurement management, etc also may to be modified with respect to the testing processes. My paper aims at using the critical chain principles and proposes a software lifecycle model that can cater to business priorities and aligning the testing processes not only to development cycles but also to the actual business value created. In this paper I would take a case study and compare and contrast when project uses the regular models and this new model. I would also provide guidelines to use this lifecycle model and modify regular project management activities to cater to the new model with emphasis on the testing processes. The paper would try to provide the ideal scenarios; in project teams, in consulting firms and in new customers and expectations; where such a model could provide high impact on the way consulting companies can do successful projects and creating more value to the customers.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319542,no
239,The impact of test case reduction and prioritization on software testing effectiveness,"Software testing is critical but most expensive phase of Software Development Life Cycle (SDLC). Development organizations desire to thoroughly test the software. But this exhaustive testing is impractical due to resource constraints. A large number of test suites are generated using automated tools. But the real challenge is the selection of subset of test cases and/or high order test cases crucial to validate the System Under Test (SUT). Test case reduction and prioritization techniques help test manager to solve this problem at a little cost. In this paper, we investigate their impact on testing process effectiveness using previous empirical studies. The results indicate that these techniques improve the testing effectiveness significantly. At the end, a case study is presented that suggests different useful combinations of these techniques, which are helpful for different testing scenarios.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353136,yes
240,Type Inference for Soft-Error Fault-Tolerance Prediction,"Software systems are becoming increasingly vulnerable to a new class of soft errors, originating from voltage spikes produced by cosmic radiation. The standard technique for assessing the source-level impact of these soft errors, fault injection - essentially a black-box testing technique - provides limited high-level information. Since soft errors can occur anywhere, even control-structured white-box techniques offer little insight. We propose a type-based approach, founded on data-flow structure, to classify the usage pattern of registers and memory cells. To capture all soft errors, the type system is defined at the assembly level, close to the hardware, and allows inferring types in the untyped assembly representation. In a case study, we apply our type inference scheme to a prototype brake-by-wire controller, developed by Volvo Technology, and identify a high correlation between types and fault-injection results. The case study confirms that the inferred types are good predictors for soft-error impact.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431783,no
241,"Uncertainty management in software engineering: Past, present, and future","Software development has significantly matured in the last decade. However, one of the critical challenges today is uncertainty inherent to every aspect of software development including requirement specifications, design, coding, and testing. In this paper, we propose a framework for uncertainty management in software engineering. The framework is used to model uncertainty inherent to software development activities and manage their consequences. The framework consists of four main phases: identification and prioritization, modeling and analysis, management and planning, and monitoring and evaluation. Commercial off-the-shelf (COTS)-based development is selected as an example to illustrate how the proposed framework is used in a simple but intuitive case study to represent uncertainty and manage its consequences.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090081,no
242,Using String Distances for Test Case Prioritisation,"Test case prioritisation aims at finding an ordering which enhances a certain property of an ordered test suite. Traditional techniques rely on the availability of code or a specification of the program under test. In this paper, we propose to use string distances on the text of test cases for their comparison and elaborate a prioritisation algorithm. Such a prioritisation does not require code and can be useful for initial testing and in cases when code is difficult to instrument. We also briefly report on preliminary results of an experiment where the proposed prioritisation technique was compared with random permutations and four classical string distance metrics were evaluated.",2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431745,yes
243,Visualizing the structure of field testing problems,Field testing of a software application prior to general release is an important and essential quality assurance step. Field testing helps identify unforeseen problems. Extensive field testing leads to the reporting of a large number of problems which often overwhelm the allocated resources. Prior efforts focus primarily on studying the reported problems in isolation. We believe that a global view of the interdependencies between these problems will help in rapid understanding and resolution of reported problems. We present a visualization that highlights the commonalities between reported problems. The visualization helps developers identify two patterns that they can use to prioritize and focus their efforts. We demonstrate the applicability of our visualization through a case study on problems reported during field testing efforts for two releases of a large scale enterprise application.,2009,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306297,no
244,A Regression Testing Approach for Software Product Lines Architectures,"In the Software Product Lines (SPL) context, where products are derived from a common platform, the reference architecture can be considered the main asset. In order to maintain its correctness and reliability after modifications, a regression testing approach based on architecture specification and code was developed. It aims to reduce the testing effort, by reusing test cases, execution results, as well as, selecting and prioritizing an effective set of test cases. Taking advantage of SPL architectures similarities, this approach can be applied among product architectures and between the reference and product architecture. This study also presents an evaluation performed in order to calibrate and improve the proposed approach.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631684,no
245,A Simulation Study on Some Search Algorithms for Regression Test Case Prioritization,"Test case prioritization is an approach aiming at increasing the rate of faults detection during the testing phase, by reordering test case execution. Many techniques for regression test case prioritization have been proposed. In this paper, we perform a simulation experiment to study five search algorithms for test case prioritization and compare the performance of these algorithms. The target of the study is to have an in-depth investigation and improve the generality of the comparison results. The simulation study provides two useful guidelines: (1) Two search algorithms, Additional Greedy Algorithm and 2-Optimal Greedy Algorithm, outperform the other three search algorithms in most cases. (2) The performance of the five search algorithms will be affected by the overlap of test cases with regard to test requirements.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562946,yes
246,A source-based risk analysis approach for software test optimization,"In this paper we introduce our proposed technique for software component test prioritization and optimization which is based on a source-code based risk analysis. Software test is one of the most critical steps in the software development. Considering that the time and human resources of a software project are limited, software test should be scheduled and planned very carefully. In this paper we introduce a classification approach that provides the developers with a risk model of the application which is specifically designed to assist the testing process by identifying the most important components and their corresponding test effort estimation. We designed an analyser tool to apply our technique to a test software project and we presented the results in this paper.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485639,yes
247,An Approach for Classifying Program Failures,"In this work, we leverage hardware performance counters-collected data to automatically group program failures that stem from closely related causes into clusters, which can in turn help developers prioritize failures as well as diagnose their causes. Hardware counters have been used for performance analysis of software systems in the past. By contrast, in this paper they are used as abstraction mechanisms for program executions. The results of our feasibility studies conducted on two widely-used applications suggest that hardware counters-collected data can be used to reliably classify failures.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5617204,no
248,An Automatic Configuration Approach to Improve Real-Time Application Throughput While Attaining Determinism,"Determinism and throughput are two important performance measures for Java-based real-time applications, but they often conflict. Therefore, it is significant to improve throughput for Java-based real-time applications while guaranteeing its execution time determinism. In this paper, we propose an automatic configuration approach to assign real-time thread priorities to solve the above-mentioned problem. In this approach, we propose an innovative representation of determinism related with real-time thread priorities using stochastic process. Java-based real-time application's throughput is quantified with thread priorities as parameters. The algorithm of integer programming is used to optimize throughput with boundary conditions of the level of determinism. Finally, the Sweet Factory application is tested to evaluate the effect of our approach. Experiment results show that throughput for Java-based real-time applications could be efficiently improved while keeping the execution time determinism with our approach.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676292,no
249,An Intelligent Approach of Obtaining Feasible Machining Processes and Their Selection Priorities for Features Based on Neural Network,"To obtain all feasible machining processes and their quantitative selection priorities, an intelligent making decision approach combining back-propagation neural network and backward planning is proposed. Uniform design method, which is adapted for the problem of multiple factors and multiple levels, is adopted to build representative sample sets for the neural network. The neural network is trained by an improved back-propagation algorithm which can adjust momentum factor and learning rate simultaneously, and tested by linear regression analysis. A case study has been conducted to demonstrate the effectiveness of the proposed approach.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677004,no
250,Analysis and optimization of software requirements prioritization techniques,"Prioritizing requirements helps the project team to understand which requirements are most important and most urgent. Based on this finding a software engineer can decide what to develop/implement in the first release and what on the coming releases. Prioritization is also a useful activity for decision making in other phases of software engineering like development, testing, and implementation. There are a number of techniques available to prioritize the requirements with their associated strengths and limitations. In this paper we will examine state of the art techniques and analyze their applicability on software requirements domain. At the end we present a framework that will help the software engineer of how to perform prioritization process by combining existing techniques and approaches.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625687,no
251,Analytical survey on automated software test data evaluation,Automated software test data optimization has become a major aspect in quality of any software. For quality different test cases has to be performed for testing. In order to evaluate every aspect of the software program the number of test cases has increased tremendously. In this paper author have tried to evaluate different proposed techniques for automated software test data optimization and emphasize is made to extract the critical factors which need to be present in any technique to make the technique optimized one. These factors are then evaluated on the basis of different papers and concluded some results which are beneficial to work for the creation of an optimized technique.,2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488549,no
252,Arranging software test cases through an optimization method,"During the software testing process, the customers would be invited to review or inspect an ongoing software product. This phase is called the ƒ??in-plantƒ? test, often known as an ƒ??alphaƒ? test. Typically, this test phase lasts for a very short period of time in which the software test engineers or software quality engineers rush to execute a list of software test cases in the test suite with customers. Because of the time constraint, the test cases have to be arranged in terms of test case severities, estimated test time, and customers' demands. As important as the test case arrangement is, this process is mostly performed manually by the project managers and software test engineers together. As the software systems are getting more sophisticated and complex, a greater volume of test cases have to be generated, and the manual arrangement approach may not be the most efficient way to handle this. In this paper, we propose a framework for automating the process of test case arrangement and management through an optimization method. We believe that this framework will help software test engineers facing with the challenges of prioritizing test cases.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602131,yes
253,Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows,"We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062117,no
254,Constructing Prioritized Interaction Test Suite with Interaction Relationship,"Interaction testing has addressed some issues on how to select a small subset of test cases. In many systems where interaction testing is needed, the entire test suite is not executed because of time or budget constraints. It is important to prioritize the test cases in these situations. On the other hand, there are not always interactions among any factors in real systems. Moreover, some factors may need N-way (N&gt;2) testing since there is a closer relationship among them. We present a model for prioritized interaction testing with interaction relationship and propose a greedy algorithm for generating variable strength covering arrays with bias.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459713,no
255,Design and Analysis of Cost-Cognizant Test Case Prioritization Using Genetic Algorithm with Test History,"During software development, regression testing is usually used to assure the quality of modified software. The techniques of test case prioritization schedule the test cases for regression testing in an order that attempts to increase the effectiveness in accordance with some performance goal. The most general goal is the rate of fault detection. It assumes all test case costs and fault severities are uniform. However, those factors usually vary. In order to produce a more satisfactory order, the cost-cognizant metric that incorporates varying test case costs and fault severities is proposed. In this paper, we propose a cost-cognizant test case prioritization technique based on the use of historical records and a genetic algorithm. We run a controlled experiment to evaluate the proposed technique's effectiveness. Experimental results indicate that our proposed technique frequently yields a higher Average Percentage of Faults Detected per Cost (APFDc). The results also show that our proposed technique is also useful in terms of APFDc when all test case costs and fault severities are uniform.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676289,no
256,Efficient Reduction of Model-Based Generated Test Suites through Test Case Pair Prioritization,"During the development and maintenance of software, test suites often reach a size that exceeds the costs allocated for test suite execution. In such a case, the test suite needs to be reduced. Many papers are dedicated to the problem of test suite reduction. Most of them consider the removal or merging of test cases. However, less attention has been paid to the identification of test case pairs, which are eminently suitable for merging. In this paper, we fill this gap by presenting a novel approach that helps identifying those test case pairs within a given set of systematically generated test cases which, when merged, have potential for high test suite reduction. As a result, test suites reduced by our approach are considerably smaller in size than those, whose pairs are selected randomly.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772249,no
257,Evaluating and Enhancing Xen-Based Virtual Routers to Support Real-Time Applications,"Router virtualization seems as the obvious next step to system virtualization and the key to easily deploy and manage next generation overlay virtual networks. In this paper, we investigate the viability of virtual routers on a Xen-based system. We first evaluate the system throughput when achieving forwarding in the virtual routers. Then, we consider the context where virtual routers are dedicated to flows of different types and propose a mechanism to guarantee the required throughput and latency to real time applications while maintaining an optimal aggregated system throughput. We achieved this through both configuring the Xen Credit scheduler and establishing priorities between packets in the driver domain before switching them to the target virtual router.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421620,no
258,Making defect-finding tools work for you,"Given the high costs of software testing and fixing bugs after release, early detection of bugs using static analysis can result in significant savings. However, despite their many benefits, recent availability of many such tools, and evidence of a positive return-on-investment, static-analysis tools are not used widely because of various usability and usefulness problems. The usability inhibitors include the lack of features, such as capabilities to merge reports from multiple tools and view warning deltas between two builds of a system. The usefulness problems are related primarily to the accuracy of the tools: identification of false positives (or, spurious bugs) and uninteresting bugs among the true positives. In this paper, we present the details of an online portal, developed at IBM Research, to address these problems and promote the adoption of static-analysis tools. We report our experience with the deployment of the portal within the IBM developer community. We also highlight the problems that we have learned are important to address, and present our approach toward solving some of those problems.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062143,no
259,Managing Testing Complexity in Dynamically Adaptive Systems: A Model-Driven Approach,"Autonomous systems are increasingly conceived as a means to allow operation in changeable or poorly understood environments. However, granting a system autonomy over its operation removes the ability of the developer to be completely sure of the system's behaviour under all operating contexts. This combination of environmental and behavioural uncertainty makes the achievement of assurance through testing very problematic. This paper focuses on a class of system, called an m-DAS, that uses run-time models to drive run-time adaptations in changing environmental conditions. We propose a testing approach which is itself model-driven, using model analysis to significantly reduce the set of test cases needed to test for emergent behaviour. Limited testing resources may therefore be prioritised for the most likely scenarios in which emergent behaviour may be observed.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463659,no
260,Measuring unmeasurable attributes of software quality using Pragmatic Quality Factor,"Software quality is evolving beyond static measurement to a wider scope of quality definition. Previous studies have indicated the importance of human aspect in software quality. But the quality models have not included comprehensively this aspect together with the behavioural aspect of software quality. This research has proposed a Pragmatic Quality Factors (or PQF) as a software quality measurement and metrics that includes both aspects of quality. These aspects of quality are essential as to balance between technical and non-technical (human) facet. In addition, this model provides flexibility by giving priorities and weights to the quality attributes. The priority and weight are necessary to reflect business requirement in the real business environment. Therefore, it is more practical that suits with different users and purposes. It is implemented through collaborative perspective approach between users, developers and independent assessor. This model shows how the unmeasurable characteristics can be measured indirectly using measures and metrics approach. It has been tested involving assessment and certification exercises in real case studies in Malaysia.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564077,no
261,Notice of Retraction<br>Designing and formulating organization performance evaluation model in AHP method based on EFQM criteria (case study),"Designing and formulating a comprehensive organization performance evaluation model based on European Foundation for Quality Management (EFQM) in AHP (Analytic Hierarchy Process) method is the main aim of the present research study. Evaluation is considered as one of the most important activities in each organization in a way that reformation of processes and procedures of doing activity without evaluation of results will be impossible. At the present research activity, AHP (Analytic Hierarchy Process) has been used as one of MADM (Multi-Attribute Decision Making) methods for the evaluation of performance of organizations through the application of EFQM (European Foundation for Quality Management) excellence model criteria. Also, Municipality of City of Tabriz has been selected as subjects for testing the presented model. In the same direction, seven districts of this municipality were selected as sample model. Necessary and required information were accumulated through questionnaire, interview and also taking advantage of data and library resources, details of which were analyzed and studied through the application of advanced Excel and Expert Choice 11.5 software package system. Eventually, various districts of this organization were evaluated and prioritized in terms of performance.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553098,no
262,"On the Integration of Test Adequacy, Test Case Prioritization, and Statistical Fault Localization","Testing and debugging account for at least 30% of the project effort. Scientific advancements in individual activities or their integration may bring significant impacts to the practice of software development. Fault localization is the foremost debugging sub-activity. Any effective integration between testing and debugging should address how well testing and fault localization can be worked together productively. How likely does a testing technique provide test suites for effective fault localization? To what extent may such a test suite be prioritized so that the test cases having higher priority can be effectively used in a standalone manner to support fault localization? In this paper, we empirically study these two research questions in the context of test data adequacy, test case prioritization and statistical fault localization. Our preliminary postmortem analysis results on 16 test case prioritization techniques and four statistical fault localizations show that branch-adequate test suites on the Siemens suite are unlikely to support effective fault localization. On the other hand, if such a test suite is effective, around 60% of the test cases can be further prioritized to support effective fault localization, which indicates that the potential savings in terms of effort can be significant.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562990,yes
263,Optimizing the Software Architecture for Extensibility in Hard Real-Time Distributed Systems,"We consider a set of control tasks that must be executed on distributed platforms so that end-to-end latencies are within deadlines. We investigate how to allocate tasks to nodes, pack signals to messages, allocate messages to buses, and assign priorities to tasks and messages, so that the design is extensible and robust with respect to changes in task requirements. We adopt a notion of extensibility metric that measures how much the execution times of tasks can be increased without violating end-to-end deadlines. We optimize the task and message design with respect to this metric by adopting a mathematical programming front-end followed by postprocessing heuristics. The proposed algorithm as applied to industrial strength test cases shows its effectiveness in optimizing extensibility and a marked improvement in running time with respect to an approach based on randomized optimization.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535223,no
264,Point-of-Interest Aware Test Case Prioritization: Methods and Experiments,"Location based services personalize their behaviors based on location data. When data kept by a service have evolved or the code has been modified, regression testing can be employed to assure the quality of services. Frequent data update however may lead to frequent regression testing and any faulty implementation of a service may affect many service consumers. Proper test case prioritization helps reveal service problems efficiently. In this paper, we review a set of point-of-interest (POI) aware test case prioritization techniques and report an experiment on such techniques. The empirical results show that these POI-aware techniques are more effective than random ordering and input-guided test case prioritization in terms of APFD. Furthermore, their effectiveness is observed to be quite stable over different sizes of the test suite.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563000,yes
265,Prioritization of Issues and Requirements by Cumulative Voting: A Compositional Data Analysis Framework,"Cumulative Voting (CV), also known as Hundred-Point Method, is a simple and straightforward technique, used in various prioritization studies in software engineering. Multiple stakeholders (users, developers, consultants, marketing representatives or customers) are asked to prioritize issues concerning requirements, process improvements or change management in a ratio scale. The data obtained from such studies contain useful information regarding correlations of issues and trends of the respondents towards them. However, the multivariate and constrained nature of data requires particular statistical analysis. In this paper we propose a statistical framework; the multivariate Compositional Data Analysis (CoDA) for analyzing data obtained from CV prioritization studies. Certain methodologies for studying the correlation structure of variables are applied to a dataset concerning impact analysis issues prioritized by software professionals under different perspectives. These involve filling of zeros, transformation using the geometric mean, principle component analysis on the transformed variables and graphical representation by biplots and ternary plots.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598119,no
266,Prioritization of test case scenarios derived from activity diagram using genetic algorithm,"Software testing involves identifying the test cases which discovers the errors in the program. However, the exhaustive testing is rarely impossible and very time consuming. In this paper, the software testing efficiency is optimized by identifying the critical path clusters. The test case scenarios are derived from the activity diagram and the testing efficiency is optimized by applying the genetic algorithm on the test data. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of activity diagram.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640479,no
267,Prioritizing Mutation Operators Based on Importance Sampling,"Mutation testing is a fault-based testing technique for measuring the adequacy of a test suite. Test suites are assigned scores based on their ability to expose synthetic faults (i.e., mutants) generated by a range of well-defined mathematical operators. The test suites can then be augmented to expose the mutants that remain undetected and are not semantically equivalent to the original code. However, the mutation score can be increased superfluously by mutants that are easy to expose. In addition, it is infeasible to examine all the mutants generated by a large set of mutation operators. Existing approaches have therefore focused on determining the sufficient set of mutation operators and the set of equivalent mutants. Instead, this paper proposes a novel Bayesian approach that prioritizes operators whose mutants are likely to remain unexposed by the existing test suites. Probabilistic sampling methods are adapted to iteratively examine a subset of the available mutants and direct focus towards the more informative operators. Experimental results show that the proposed approach identifies more than 90% of the important operators by examining ? 20% of the available mutants, and causes a 6% increase in the importance measure of the selected mutants.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635074,no
268,Prioritizing State-Based Aspect Tests,"In aspect-oriented programming, aspects are essentially incremental modifications to their base classes. Therefore aspect-oriented programs can be tested in an incremental fashion - we can first test the base classes and then test the base classes and aspects as a whole. This paper demonstrates that, in this incremental testing paradigm, we can prioritize aspect tests so as to report failure earlier. We explore test prioritization for testing aspect-oriented programs against their state models with transition coverage and round-trip coverage. Aspect tests are generated from woven state models obtained by composing aspect models into their base class models. We prioritize aspect tests by identifying the extent to which an aspect modifies its base classes. The modification is measured by the number of new and changed components in state transitions (start state, event, precondition, postcondition, end state). Transitions with more changes have higher priorities for test generation. We evaluate the impact of aspect test prioritization through mutation analysis of two AspectJ programs, where all aspects and their base classes can be modeled by finite state machines. We create aspect mutants of each AspectJ program according to a comprehensive AspectJ fault model. Then we test each mutant with the test suites generated without prioritization and with prioritization, respectively. Our experiment results show that prioritization of aspect tests has accelerated failure report.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477076,yes
269,Prioritizing Tests for Software Fault Localization,"Test prioritization techniques select test cases that maximize the confidence on the correctness of the system when the resources for quality assurance (QA) are limited. In the event of a test failing, the fault at the root of the failure has to be localized, adding an extra debugging cost that has to be taken into account as well. However, test suites that are prioritized for failure detection can reduce the amount of useful information for fault localization. This deteriorates the quality of the diagnosis provided, making the subsequent debugging phase more expensive, and defeating the purpose of the test cost minimization. In this paper we introduce a new test case prioritization approach that maximizes the improvement of the diagnostic information per test. Our approach minimizes the loss of diagnostic quality in the prioritized test suite. When considering QA cost as the combination of testing cost and debugging cost, on the Siemens set, the results of our test case prioritization approach show up to a 53% reduction of the overall QA cost, compared with the next best technique.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562943,yes
270,Prioritizing Unit Test Creation for Test-Driven Maintenance of Legacy Systems,"Test-Driven Development (TDD) is a software development practice that prescribes writing unit tests before writing implementation code. Recent studies have shown that TDD practices can significantly reduce the number of pre-release defects. However, most TDD research thus far has focused on new development. We investigate the adaptation of TDD-like practices for already implemented code, in particular legacy systems. We call this adaptation of TDD-like practices for already implemented code ``Test-Driven Maintenance'' (TDM). In this paper, we present an approach that assists software development and testing managers, who employ TDM, utilize the limited resources they have for testing legacy systems efficiently. The approach leverages the development history of the project to generate a prioritized list of functions that managers should focus their unit test writing resources on. The list is updated dynamically as the development of the legacy system progresses. To evaluate our approach, we conduct a case study on a large commercial legacy software system. Our findings suggest that heuristics based on the function size, modification frequency and bug fixing frequency should be used to prioritize the unit test writing of legacy systems.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562952,no
271,Ranking Attacks Based on Vulnerability Analysis,"Now that multiple-known attacks can affect one software product at the same time, it is necessary to rank and prioritize those attacks in order to establish a better defense. The purpose of this paper is to provide a set of security metrics to rank attacks based on vulnerability analysis. The vulnerability information is retrieved from a vulnerability management ontology, which integrates commonly used standards like CVE, CWE, CVSS, and CAPEC. Among the benefits of ranking attacks through the method proposed here are: a more effective mitigation or prevention of attack patterns against systems, a better foundation to test software products, and a better understanding of vulnerabilities and attacks.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428663,no
272,Regression test cases prioritization using Failure Pursuit Sampling,"The necessity of lowering the execution of system tests' cost is a consensual point in the software development community. The present study presents an optimization of the regression tests' activity, by adapting a test cases prioritization technique called Failure Pursuit Sampling-previously used and validated for the prioritization of tests in general-improving its efficiency for the exclusive execution of regression test. For this purpose, the clustering and sampling phases of the original technique were modified, so that it becomes capable of receive information from tests made on the previous version of a program, and can use this information to drive de efficiency of the new developed technique, for tests made on a present version. The adapted technique was implemented and executed using the Schedule program, of the Siemens suit. By using Average of the Percentage of Faults Detected charts, the modified Failure Pursuit Sampling technique presented a high level of efficiency improvement.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687069,yes
273,Regression Test Generation Approach Based on Tree-Structured Analysis,"Regression test generation is an important process to make sure that changes of program have no unintended side-effects. To achieve full confidence, many projects have to re-run all the test cases for entire program, which makes it a time consuming and expensive activity. In this paper, a code based regression testing approach is proposed to generate selected test suites for unit testing. The framework contains five phases: program change detection phase, logical verification phase, branch pruning phase, test case prioritization phase and test suite generation phase. These five phases can achieve detection of program's modification, coding standard, test case pruning, test case prioritizing and inputs generation for regression test cases respectively. A prototype based on this framework is implemented using logical tree-structured analysis, and the preliminary experiment shows that proposed approach can provide efficient regression test suites.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476640,yes
274,Requirement based test case prioritization,"Test case prioritization involves scheduling test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects faults at the earliest in its testing life cycle. In this paper, an algorithm is proposed for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software and also to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the three factors: customer priority, changes in requirement, implementation complexity. The proposed prioritization technique is validated with two different sets of industrial projects and the results show that the proposed prioritization technique improves the rate of severe fault detection.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670728,yes
275,Requirement-based test case generation and prioritization,"Software release testing is a critical phase in the software development life cycle, as it validates the software against its requirements. Designing comprehensive release test cases that are driven by the software requirements remain the major success factor of the testing phase as far as the software customers are concerned. Further, availing sufficient traceability information to ensure complete coverage of requirements validation in the designed test case suite is significant to software quality assurance. In this paper, we propose a systematic mechanism to derive a set of release test cases from a set of requirements modeled with the Genetic Software Engineering (GSE) method. GSE models functional requirements with a semi-formal visual notation called Behavior Trees (BT). Our algorithm prioritizes the requirements modeled with BTs and derives a set of prioritized release test cases systematically. Additionally, our algorithm provides sufficient traceability information relating test cases to the requirements being tested. This allows for ensuring completeness of test case coverage. We also demonstrate our test case derivation mechanism through a case study.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720443,yes
276,Risk-Based Testing: A Case Study,"This paper describes the application of risk-based testing for a software product evaluation in a real case study. Risk-based testing consists of a set of activities regarding risk factors identification related to software requirements. Once identified, the risks are prioritized according to their likelihood and impact and test cases are designed based on the strategies for treatment of the identified risk factors. Thus, test efforts are continuously adjusted according to risk monitoring. The paper also briefly reviews available risk-based approaches, describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of problems, challenges and future work.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501497,no
277,Risks of unrecognized commonalities in information technology supply chains,"In this paper we examine the interdependencies and common points of failure (and attack) that plague commonly-used system and network hardware and software. The proposed approach requires not only generating inventories of acquiring organizations' equipment and software products, and clear and detailed descriptions of every link in the supply chain, but also the identification of common components and their sources. This information is required not only for manufacturer and OEM supply chains, but also for the services supply chains of maintenance and repair organizations. When such critical components and services have been identified, one must prioritize their importance and apply appropriate security and testing. Such an identification and tracking system is only as good as its ability to incorporate up-to-the-minute changes and additions. This requires extensive real-time reporting and information sharing. The author presents a general description of a proprietary tool that facilitates the collaboration needed for such an approach to be effective.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5654970,no
278,Sequence-based techniques for black-box test case prioritization for composite service testing,"Web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable to web services. In this paper, we formulate new test case prioritization strategies using sequences in XML messages to reorder regression test cases for composite web services, against the tag based techniques given in and reveal how the test cases use the interface specifications of the composite services. The results were evaluated experimentally and the results show that the new techniques can have a high probability of outperforming random ordering and the techniques given in.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705784,yes
279,Stress Testing an AI Based Web Service: A Case Study,"The stress testing of AI-based systems differs from the approach taken for more traditional Web services, both in terms of the design of test cases and the metrics used to measure quality. The expected variability in responses of an AI-based system to the same request adds a level of complexity to stress testing, when compared to more standard systems where the system response is deterministic and any deviations may easily be characterized as product defects. Generating test cases for AI-based systems requires balancing breadth of test cases with depth of response quality: most AI-systems may not return a perfect answer. An example of a machine learning translation system is considered, and the approach used for stress testing it is presented, alongside comparisons with a more traditional approach. The challenges of shipping such a system to support a growing set of features and language pairs necessitate a mature prioritization of test cases. This approach has been successful in shipping a Web service that currently serves millions of users per day.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501500,no
280,Supporting Concern-Based Regression Testing and Prioritization in a Model-Driven Environment,"Traditional regression testing and prioritization approaches are bottom-up (or white-box). They rely on the analysis of the impact of changes in source code artifacts, identifying corresponding parts of software to retest. While effective in minimizing the amount of testing required to validate code changes, they do not leverage on specification-level design and requirements concerns that motivated these changes. Model-based testing approaches support a top-down (or black box) testing approach, where design and requirements models are used in support of test generation. They augment code-based approaches with the ability to test from a higher-level design and requirements perspective. In this paper, we present a model-based regression testing and prioritization approach that efficiently selects test cases for regression testing based on different concerns. It relies on traceability links between models, test cases and code artifacts, together with user-defined properties associated to model elements. In particular we describe how to support concern-based regression testing and prioritization using TDE/UML, an extensible model-based testing environment.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615818,yes
281,Taking Advantage of Service Selection: A Study on the Testing of Location-Based Web Services Through Test Case Prioritization,"Dynamic service compositions pose new verification and validation challenges such as uncertainty in service membership. Moreover, applying an entire test suite to loosely coupled services one after another in the same composition can be too rigid and restrictive. In this paper, we investigate the impact of service selection on service-centric testing techniques. Specifically, we propose to incorporate service selection in executing a test suite and develop a suite of metrics and test case prioritization techniques for the testing of location-aware services. A case study shows that a test case prioritization technique that incorporates service selection can outperform their traditional counterpart - the impact of service selection is noticeable on software engineering techniques in general and on test case prioritization techniques in particular. Further-more, we find that points-of-interest-aware techniques can be significantly more effective than input-guided techniques in terms of the number of invocations required to expose the first failure of a service composition.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552784,yes
282,Test Case Prioritization for Web Service Regression Testing,"Regression testing is necessary to assure the quality of service-oriented business applications in their evolutions. However, because of the constraint of testing resource, entire test suite may not run as a result. Therefore, test case prioritization technique is required to increase the efficiency of Web service application regression testing. In this paper, we propose a dependence analysis based test case prioritization technique. First, we analyze the dependence relationship using control and data flow information in an orchestration language: WS-BPEL. Then we construct a weighted graph and do impact analysis to identify modification-affected elements. After that, we prioritize test cases according to covering more modification-affected elements with the highest weight. Finally we conduct a case study to illustrate the applicability of our method.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569910,yes
283,The Effectiveness of Regression Testing Techniques in Reducing the Occurrence of Residual Defects,"Regression testing is a necessary maintenance activity that can ensure high quality of the modified software system, and a great deal of research on regression testing has been performed. Most of the studies performed to date, however, have evaluated regression testing techniques under the limited context, such as a short-term assessment, which do not fully account for system evolution or industrial circumstances. One important issue associated with a system lifetime view that we have overlooked in past years is the effects of residual defects - defects that persist undetected - across several releases of a system. Depending on an organization's business goals and the type of system being built, residual defects might affect the level of success of the software products. In this paper, we conducted an empirical study to investigate whether regression testing techniques are effective in reducing the occurrence and persistence of residual defects across a system's lifetime, in particular, considering test case prioritization techniques. Our results show that heuristics can be effective in reducing both the occurrence of residual defects and their age. Our results also indicate that residual defects and their age have a strong impact on the cost-benefits of test case prioritization techniques.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477096,yes
284,The Effects of Time Constraints on Test Case Prioritization: A Series of Controlled Experiments,"Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. If this is correct, a better understanding of the effects of time constraints could lead to improved prioritization techniques and improved maintenance and testing processes. We therefore conducted a series of experiments to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our first experiment manipulates time constraint levels and shows that time constraints do play a significant role in determining both the cost-effectiveness of prioritization and the relative cost-benefit trade-offs among techniques. Our second experiment replicates the first experiment, controlling for several threats to validity including numbers of faults present, and shows that the results generalize to this wider context. Our third experiment manipulates the number of faults present in programs to examine the effects of faultiness levels on prioritization and shows that faultiness level affects the relative cost-effectiveness of prioritization techniques. Taken together, these results have several implications for test engineers wishing to cost-effectively regression test their software systems. These include suggestions about when and when not to prioritize, what techniques to employ, and how differences in testing processes may relate to prioritization cost--effectiveness.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482587,yes
285,Time Windows Based Dynamic Routing in Multi-AGV Systems,"This paper presents a dynamic routing method for supervisory control of multiple automated guided vehicles (AGVs) that are traveling within a layout of a given warehouse. In dynamic routing a calculated path particularly depends on the number of currently active AGVs' missions and their priorities. In order to solve the shortest path problem dynamically, the proposed routing method uses time windows in a vector form. For each mission requested by the supervisor, predefined candidate paths are checked if they are feasible. The feasibility of a particular path is evaluated by insertion of appropriate time windows and by performing the windows overlapping tests. The use of time windows makes the algorithm apt for other scheduling and routing problems. Presented simulation results demonstrate efficiency of the proposed dynamic routing. The proposed method has been successfully implemented in the industrial environment in a form of a multiple AGV control system.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907246,no
286,"To Strengthen Security, Change Developers' Incentives","Many of the most common software vulnerabilities, such as buffer overflows, cross-site scripting, and misapplications of cryptography, are wholly avoidable if software makers apply an appropriate level of training, testing, and care.Yet developers today have the ""wrong"" incentives, often leading them to underinvest in security or even to directly harm it. If we can understand these incentives and their causes, we might be able to reshape them and radically improve security.Software makers have shown a dramatic ability to strengthen their products' security given sufficient motivation.The most famous example is Microsoft's transformation over the past decade from a security laughingstock to a leader. In 2002, stung by several widely publicized vulnerabilities across its product line, the company began a major security initiative that produced lasting changes in its priorities, processes, and culture. Gone were the days of ""creating designs and code that emphasize features over security."" Yet changes like these are exceptional. Microsoft's shift was motivated by an intense level of scrutiny and withering global publicity that few firms experience, and it had the unusual luxury of responding with vast engineering resources paid for by monopoly rents. Most developers face far weaker security incentives.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439535,no
287,Using Coverage Information to Guide Test Case Selection in Adaptive Random Testing,"Random Testing (RT) is a fundamental software testing technique. Adaptive Random Testing (ART) improves the fault-detection capability of RT by employing the location information of previously executed test cases. Compared with RT, test cases generated in ART are more evenly spread across the input domain. ART has conventionally been applied to programs that have only numerical input types, because the distance between numerical inputs is readily measurable. The vast majority of computer programs, however, involve non-numerical inputs. To apply ART to these programs requires the development of effective new distance measures. Different from those measures that focus on the concrete values of program inputs, in this paper we propose a method to measure the distance using coverage information. The proposed method enables ART to be applied to all kinds of programs regardless of their input types. Empirical studies are further conducted for the branch coverage Manhattan distance measure using the replace and space programs. Experimental results show that, compared with RT, the proposed method significantly reduces the number of test cases required to detect the first failure. This method can be directly applied to prioritize regression test cases, and can also be incorporated into code-based and model-based test case generation tools.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615787,yes
288,Using Methods & Measures from Network Analysis for GUI Testing,"Graphical user interfaces (GUIs) for today's applications are extremely large. Moreover, they provide many degrees of freedom to the end-user, thus allowing the user to perform a very large number of event sequences on the GUI. The large sizes and degrees of freedom create severe problems for GUI quality assurance, including GUI testing. In this paper, we leverage methods and measures from network analysis to analyze and study GUIs, with the goal of aiding GUI testing activities. We apply these methods and measures on the event-flow graph model of GUIs. Results of a case study show that ""network centrality measures"" are able to identify the most important events in the GUI as well as the most important sequences of events. These events and sequences are good candidates for test prioritization. In addition, the ""betweenness clustering"" method is able to partition the GUI into regions that can be tested separately.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463654,no
289,Web services regression test case prioritization,"Web services and their underlying system grow over time and need to be retested whenever there is a change. This is essential for ensuring uncompromised quality. If we have modified only a small part of the system, it should be possible to reuse the existing test suite. Anyhow, for large modifications or for large systems, retesting the entire test suite will consume large amounts of time and computing resources. In this paper we propose a new method to prioritize test cases in web applications. Our test prioritization technique orders test cases in such a way that the most beneficial is executed first. Most of the existing test prioritization methods are based on the code of the system, but we propose a model-based test prioritization using activity diagram. Our technique identifies difference between original model and modified model. Using this information we plot activity paths for each test case and identify the most promising paths. The test case which covers these paths is considered as the most beneficial test cases. Our approach is effective in revealing the most promising regression test cases. We have applied our method on an online air ticket reservation system in which we could identify the most beneficial test cases from the existing ones.",2010,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5643499,yes
290,A clustering approach to improving test case prioritization: An industrial case study,"Regression testing is an important activity for controlling the quality of a software product, but it accounts for a large proportion of the costs of software. We believe that an understanding of the underlying relationships in data about software systems, including data correlations and patterns, could provide information that would help improve regression testing techniques. We conjecture that if test cases have common properties, then test cases within the same group may have similar fault detection ability. As an initial approach to investigating the relationships in massive data in software repositories, in this paper, we consider a clustering approach to help improve test case prioritization. We implemented new prioritization techniques that incorporate a clustering approach and utilize code coverage, code complexity, and history data on real faults. To assess our approach, we have designed and conducted empirical studies using an industrial software product, Microsoft Dynamics Ax, which contains real faults. Our results show that test case prioritization that utilizes a clustering approach can improve the effectiveness of test case prioritization techniques.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080805,yes
291,A Diagnostic Point of View for the Optimization of Preparation Costs in Runtime Testing,"Runtime testing is emerging as the solution for the validation and acceptance testing of service-oriented systems, where many services are external to the organization, and duplicating the system's components and their context is too complex, if possible at all. In order to perform runtime tests, an additional expense in the test preparation phase is required, both in software development and in hardware. Preparation cost prioritization methods have been based on runtime testability (i.e, coverage) and do not consider whether a good runtime testability is sufficient for a good runtime diagnosis quality in case faults are detected, and whether this diagnosis will be obtained efficiently (i.e., with a low number of test cases). In this paper we show (1) the direct relationship between testability and diagnosis quality, that (2) these two properties do not guarantee an efficient diagnosis, and (3) a measurement that ensures better prediction of efficiency.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954478,no
292,A Fuzzy Logic Approach for Scheduling Preventive Maintenance in ERP System,"Many firms have proceeded to the adoption of Enterprise Resources Planning ERP solutions to maintain competitiveness. ERP is a packaged software system that enables enterprises to integrate operations, business processes and functions through common database. However, the majority of ERP systems do not support Preventive Maintenance (PM) scheduling process. The objective of PM is to minimize equipment downtime using the limited resources of an organization. Therefore, prioritizing PM activities for equipment is essential. In this paper, a fuzzy logic-based system for PM scheduling is proposed to interpret the linguistic variables extracted from expert's knowledge for determining equipment priorities, which could be incorporated as a custom module in ERP systems. The system was tested and proved to be reliable in solving PM scheduling problem.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999330,no
293,A general purpose Ethernet based readout data acquisition system,"A flexible dedicated readout system is one of the most important part of any kind of dedicated detection system, especially for its testing phase as well as when the final system is ready for implementation. An obvious choice is to use a FPGA (apart from dedicated front-end electronics) as a first stage of data storage and processing element. Furthermore the FPGA has to prepare and transfer the incoming/processed data to the host PC. The implementation of the data exchange can be a problem, especially for small groups of developers, who have an option to buy a general solution with its limitations and a price, or to do time-consuming development of their own system practically from scratch. This paper presents a FPGA based general purpose readout solution which lies in between the two opposite approaches. Presented system uses a FPGA mezzanine board equipped with Ethernet Gigabit connection to PC. The FPGA FIFO based readout of a digital data stream is packed directly into the Ethernet frames and send to the destination PC using point-to point connection. The standard Ethernet frames are used in this design, additionally equipped with one byte carrying information on data type. When a high throughput is needed the data type is employed to prioritize them. This moderately simple but very powerful interface is relatively easy to be implemented in many applications [1]. The custom approach chosen for FPGA implementation causes a need to prepare dedicated software suite to process all incoming data in the PC side. The developed software package is called EPPRO (Ethernet Packet PROxy) since it exploits special Ether net frames for data exchange. The core part of EPPRO is a Linux kernel module, responsible for data reception/transmission and dispatching, taking into account their types to filter and prioritize the incoming packets. Overall performance of the whole system has been evaluated in respect to its throughput and reliability, presented test results confirm that all of the design goals have been fulfilled.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154542,no
294,A genetic algorithm based approach for prioritization of test case scenarios in static testing,"White box testing is a test technique that takes into account program code, code structure and internal design flow. White box testing is primarily of two kinds-static and structural. Whereas static testing requires only the source code of the product, not the binaries or executables, in structural testing tests are actually run by the computer on built products. In this paper, we propose a technique for optimizing static testing efficiency by identifying the critical path clusters using genetic algorithm. The testing efficiency is optimized by applying the genetic algorithm on the test data. The test case scenarios are derived from the source code. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of the control flow graph generated from the source code. This research paper is an extension of our previous research paper [18].",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075160,no
295,"A low-cost distributed instrumentation system for monitoring, identifying and diagnosing irregular patterns of behavior in critical ITS components","ITS telecommunication infrastructure and information gathering/distribution equipment such as fiber-optic routing devices, communication huts, uninterrupted power supplies (UPSs), cameras and dynamic message signs (DMSs) are critical to the effective management of resources in emergency situations. Frequently encountered scenarios include evaluating the severity of vehicle collisions to dispatch appropriate law enforcement and ambulatory services, or issuing time-sensitive AMBER alerts to assist in the effort to find missing or kidnapped children. Due to the unacceptably high cost of ITS equipment failures, preventative maintenance and dense operational testing are high priorities. In this paper we present a low-cost distributed instrumentation system (DIS) for continuous monitoring of critical ITS components. Over the last three years, we have developed and deployed the DIS in the Oklahoma Department of Transportation (ODOT) private fiberoptic network that spans several major metropolitan areas in and around Oklahoma City, Tulsa and Lawton. The Oklahoma DIS is responsible for monitoring the health of the entire ODOT ITS communications network as well as the integrity of each camera video signal and the operational status of each DMS. All of the information acquired by the DIS is integrated into an operational summary that is available on a private website for the design and execution of ITS equipment maintenance plans. In Oklahoma, information acquired by the DIS has been successfully integrated into a wide range of operation and maintenance (O&M) planning, which has led to a significant improvement in terms of overall ITS quality of service (QoS) and a quantifiable reduction in wasted costs associated with the premature discarding of energy storage devices.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082868,no
296,A model based prioritization technique for component based software retesting using uml state chart diagram,"Regression testing is the process of testing a modified system using the old test suite. As the test suite size is large, system retesting consumes large amount of time and computing resources. This issue of retesting of software systems can be handled using a good test case prioritization technique. A prioritization technique schedules the test cases for execution so that the test cases with higher priority executed before lower priority. The objective of test case prioritization is to detect fault as early as possible so that the debuggers can begin their work earlier. In this paper we propose a new prioritization technique to prioritize the test cases to perform regression testing for Component Based Software System (CBSS). The components and the state changes for a component based software systems are being represented by UML state chart diagrams which are then converted into Component Interaction Graph (CIG) to describe the interrelation among components. Our prioritization algorithm takes this CIG as input along with the old test cases and generates a prioritized test suit taking into account total number of state changes and total number of database access, both direct and indirect, encountered due to each test case. Our algorithm is found to be very effective in maximizing the objective function and minimizing the cost of system retesting when applied to few JAVA projects.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941719,yes
297,A software safety test approach based on FTA and Bayesian networks,"As an important way to verify software safety, software safety test has caught more attentions in practice. However, it is still an open question that how engineers could make software safety test more efficient. Currently, FTA based method is one of the approaches in software safety test, but it can not utilize the finished software test results, and can not be determined the priorities of all the use cases. In order to solve these problems, this paper gives a quantitative approach of software safety test based on FTA and Bayesian networks. In the approach, top-level events of fault trees are identified from system hazards firstly. Then, fault trees are built using FTA and transferred into Bayesian networks. Finally, test cases of software safety test are determined by the Bayesian networks. Besides, the paper also shows an example using the approach, which could guide software engineers to make software safety test more efficient. The example shows that the approach could take advantage of Bayesian Theorem and FTA methodology together, and give reasonable priorities of use cases in software safety test.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939497,yes
298,A statistical approach to TPS transport optimization,"This paper discusses the statistical challenges of TPS Transport. A TPS is not considered fieldable until all tests are passing. Based on the number of tests in a TPS and the inherent complexity of the transport process, the probability exists that less than 100% of all tests will pass the first time they are tested after undergoing the transport process. Optimizing what types of tests are transported significantly improves the probability that the next TPS will be successfully transported. This paper illustrates that prioritizing which functions to focus on can greatly improve the probability of success while reducing the overall characterization effort.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058752,no
299,A Test Case Design Algorithm Based on Priority Techniques,"Testing is an important step of building e-commerce system. In regression testing, it is the key issue that how to reuse the test suite efficiently. This paper presents a dynamic adjustment prioritization based on the design information of test suite which is a new exploration of regression test prioritization. It improves the shortcoming of the existing technologies which failed to use the design information of test cases effectively. It adjusts the priority of test case by collecting running information, gradually optimizes the test suite and makes the test suite adapted to the current test environment to obtain a better error detection results. Experiments show the error-detected efficiency of it has certain advantages than the existing algorithms.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092632,yes
300,A tool for combination-based prioritization and reduction of user-session-based test suites,"Test suite prioritization and reduction are two approaches to managing large test suites. They play an important role in regression testing, where a large number of tests accumulate over time from previous versions of the system. Accumulation of tests is exacerbated in user-session-based testing of web applications, where field usage data is continually logged and converted into test cases. This paper presents a tool that allows testers to easily collect, prioritize, and reduce user-session-based test cases. Our tool provides four contributions: (1) guidance to users on how to configure their web server to log important usage information, (2) automated parsing of web logs into XML formatted test cases that can be used by test replay tools, (3) automated prioritization of test cases by length-based and combinatorial-based criteria, and (4) automated reduction of test cases by combinatorial coverage.",2011,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080833,yes