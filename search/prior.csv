Document Title,Authors,Author Affiliations,Publication Title,Date Added To Xplore,Year,Volume,Issue,Start Page,End Page,Abstract,ISSN,ISBNs,DOI,Funding Information,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Mesh_Terms,Article Citation Count,Reference Count,Copyright Year,License,Online Date,Issue Date,Meeting Date,Publisher,Document Identifier
Application of Decision Theory to the Testing of Large Systems,P. J. Wong,"Stanford Research Institute Menlo Park, Calif. 94025",IEEE Transactions on Aerospace and Electronic Systems,,1971,AES-7,2,379,384,"A methodology for determining priorities in allocating test resources among the various subsystems within a large system is described. The methodology is based on concepts from applied decision theory. Two versions of the methodology are presented: a complete version, called the extensive form, and an approximate version, called the diminutive form.",0018-9251;1557-9603;2371-9877,,10.1109/TAES.1971.310378,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4103708,,Decision theory;System testing;Aerospace testing;Hardware;Resource management;Software testing;Uncertainty;Software performance;Performance evaluation;Performance analysis,,,,2,7,,,,,,IEEE,IEEE Journals & Magazines
Multiaccess in a Nonqueueing Mailbox Environment,M. J. Ferguson,"INRS T??l??communications, Montreal, P.Q., Canada.",IEEE Transactions on Software Engineering,,1984,SE-10,3,237,243,"A new and flexible solution to the problem of multiple users accessing a single resource, such as communication bandwidth or composite object in memory, is derived. The means of communication consists of sending and receiving messages in known locations (or equivalently, mailboxes without queueing). Any particular user is able to deposit, and hence destroy, previous messages in a mailbox. It is assumed that exclusive access to a mailbox is supplied by an underlying system. The major results of this paper are: 1) a simple tree-based algorithm that guarantees ƒ?› no user or group of users can conspire to prevent access by some other user to the resource; ƒ?› only one user accesses the resource at a time; ƒ?› if there are N users, an individual user is guaranteed access, when requested, to the resource in no more than N-1 turns; Knuth's solution [6] can delay a user up to 2** (N-1)-1 turns; 2) an extension of Dekker's algorithm (2 users) [2] that allows the relative rates of reservations for access to the resource to be proportional to a set of N integers. When a reservation is not being used by its ``owner,'' it will be assigned to another contending request. The assignment is optimal for periodic requests.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.1984.5010232,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5010232,Access reservation priorities;extension of Dekker's algorithm;mailbox communication environment;many user exclusive access;single resource multiaccess;single resource mutually exclusive access;tree-based multiaccess,Bandwidth;Broadcasting;Multiprocessing systems;System testing;Timing;Delay;Time of arrival estimation;Software testing,,,,2,6,,,,,,IEEE,IEEE Journals & Magazines
Practical Priorities in System Testing,N. H. Petschenik,"Bell Communications Research, Inc",IEEE Software,,1985,2,5,18,23,"During the system test phase, ""thorough testing"" can pass the limits of practicality. Test case selection, based on simple priority rules, is one solution to the problem of practicality vs. thoroughness.",0740-7459;1937-4194,,10.1109/MS.1985.231755,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1695401,,System testing;Software testing;Size measurement;Application software;Centralized control;Control systems;Central office;Manuals,,,,16,11,,,,,,IEEE,IEEE Journals & Magazines
Testing and Evaluation of the Defense Data Network,T. R. Dalton; J. Downey; T. L. Hahler,"Intermetrics, Inc., 733 Concord Avenue, Cambridge, MA 02138; Intermetrics, Inc., 733 Concord Avenue, Cambridge, MA 02138; Intermetrics, Inc., 733 Concord Avenue, Cambridge, MA 02138",MILCOM 1987 - IEEE Military Communications Conference - Crisis Communications: The Promise and Reality,,1987,2,,398,403,"The purpose of this document is to establish a concept for test and evaluation (T&E) for the Defense Data Network (DDN) as a whole. This concept is based on the requirements of the DDN in its evolving configurations (e.g., MILNET, SACDIN, DODIIS, ..., DDN); the goals and priorities of the DDN DCS DSD; the needs of the various DDN user communities; knowledge of the ARPANET technology; and experience in T&E of systems and independent verification, validation, and test (IVV&T) of software and hardware. Since 1983 Intermetrics has been a contractor for Independent Validation and Verification (IV&V) support to the Testing branch of the Defense Data Network Defense Communications System Data Systems Directorate (DDN DCS DSD).",,,10.1109/MILCOM.1987.4795240,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4795240,,System testing;Software testing;Distributed control;Performance evaluation;Hardware;Data systems;ARPANET;Time factors;Government;Feedback,,,,,,,,,,,IEEE,IEEE Conferences
Formal methods and iterative design,A. F. Monk,"Dept. of Psychol., York Univ., UK",IEE Colloquium on Formal Methods and Human-Computer Interaction: II,,1988,,,1-Jan,4-Jan,"Formal methods allow a system designer to describe precisely how the system will be. In the area of human-computer interaction this means describing the structure and detail of the user interface. Giving the designer tools to think clearly about the decisions made must be helpful. However, the problem of deciding how the user interface is to be designed remains. In particular, to use such tools effectively a designer must have a good understanding of how the users think about the task, what their expectations and priorities are. This kind of information is best obtained by user testing with prototypes. An initial design is put forward and then refined using feedback from typical users doing typical tasks with a prototype. This refinement should happen in parallel with the refinement of a formal model of the user interface by the application of domain independent principles. In the early stages the cost of this procedure can be minimised by using simulations and mock ups rather than full prototypes. This is known as iterative design. This paper describes, through an example, some techniques which can be used to get insights about how a user approaches a task and the difficulties they have with a particular prototype.<<ETX>>",,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=209309,,Software engineering;User interfaces,software engineering;user interfaces,formal methods;user expectations;user priorities;iterative design;human-computer interaction;user interface;user testing;prototypes;feedback;refinement;formal model;domain independent principles;simulations;mock ups,,,,,,,,,IET,IET Conferences
Real-time adaptive control of knowledge based avionics tasks,R. Cowin; H. F. Krikorian,"Northrop Corp. Hawthorne, CA, USA; Northrop Corp. Hawthorne, CA, USA",Proceedings of the IEEE National Aerospace and Electronics Conference,,1989,,,1175,1184 vol.3,"Advanced decision-making capabilities are being developed to aid the pilots of the next generation of tactical fighters. Due to the limited processing resources available in an avionics suite, efforts have focused on developing a distributed fault-tolerant software architecture that permits the real-time prioritization and scheduling of these tasks. The authors outline the design details of an architecture under development to meet these performance requirements. The system has been tested with a threat-avoidance system, implemented on a testbed of five internetted LISP workstations, to evaluate overall system capabilities including scheduling, task operations, and database accesses. It has a simulation cycle of 50 ms and synchronization between distributed nodes can be achieved within 2 ms. This test case has nine knowledge tasks, one of which is defined as a simulation cycle that drives the test case. This system has been evaluated with the current trace capabilities and runs with a peak of 16 task instances active at any time.<<ETX>>",,,10.1109/NAECON.1989.40357,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=40357,,Adaptive control;Aerospace electronics;System testing;Decision making;Fault tolerance;Software architecture;Computer architecture;Internet;Workstations;Databases,adaptive control;aerospace computer control;aerospace testing;fault tolerant computing;knowledge based systems;military systems;real-time systems;scheduling,real-time adaptive control;knowledge based avionics tasks;distributed fault-tolerant software architecture;threat-avoidance system;scheduling;task operations;database accesses;simulation cycle,,1,5,,,,,,IEEE,IEEE Conferences
Group decision support and multiple criteria optimization,P. H. Iz,"Dept. of Inf. & Quantitative Sci., Baltimore Univ., MD, USA",Proceedings of the Twenty-Fourth Annual Hawaii International Conference on System Sciences,,1991,iii,,678,686 vol.3,The paper proposes a structured group decision aid based on multiple criteria optimization. The procedure is designed to solve optimization problems which involve conflicting objectives and multiple decision-makers with different priorities. Most of the empirical findings regarding the performance of multi criteria techniques involve a single decision-maker. The focus in these algorithms is on determining a compromise solution to a multicriteria problem which best coincides with the preference structure of a decision-maker. The approach taken is to imbed the task of finding a compromise solution in a more general and flexible framework. The underlying concept in this framework is the analytic hierarchy process and the Tchebycheff algorithm is used to solve the multiple criteria problem. The objectives and the alternative solutions to the multiple criteria problem are evaluated through the analytic hierarchy process.<<ETX>>,,,10.1109/HICSS.1991.184201,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=184201,,Decision making;Design optimization;Algorithm design and analysis;Delta modulation;Testing;Aggregates;Spine;Mathematical programming;Application software;Linear programming,decision support systems;groupware;management science;optimisation,Chebychev algorithm;multiple criteria optimization;structured group decision aid;conflicting objectives;multiple decision-makers;compromise solution;preference structure;analytic hierarchy process;Tchebycheff algorithm,,2,33,,,,,,IEEE,IEEE Conferences
Setting maintenance quality objectives and prioritizing maintenance work by using quality metrics,N. F. Schneidewind,"Naval Postgraduate Sch., Monterey, CA, USA",Proceedings. Conference on Software Maintenance 1991,,1991,,,240,249,"Metrics that are collected and validated during development can be used during maintenance to control quality and prioritize maintenance work. Validity criteria are defined mathematically. The approach is based on validating selected metrics against related quality factors during development and using the validated metrics during maintenance to: establish initial quality objectives and quality control criteria and prioritize software components (e.g., module) and allocate resources to maintain them. The author illustrates both a case of passing a validation test (discriminative power) and failing a validation test (tracking).<<ETX>>",,0-8186-2325,10.1109/ICSM.1991.160337,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=160337,,Software quality;Resource management;Software maintenance;Q factor;Quality control;Testing;Gain measurement;Software measurement;Power measurement;Financial management,quality control;resource allocation;software maintenance;software metrics;software reliability,maintenance quality objectives;quality metrics;maintenance work;selected metrics;related quality factors;validated metrics;initial quality objectives;quality control criteria;software components;validation test;discriminative power;tracking,,1,14,,,,,,IEEE,IEEE Conferences
Specification criticism via goal-directed envisionment,K. Downing; S. Fickas,"Dept. of Comput. Sci., Linkoping Univ., Sweden; NA",Proceedings of the Sixth International Workshop on Software Specification and Design,,1991,,,22,30,"Validating a complex system specification is a difficult problem. Generating behaviors and using them to critique a specification is one effective approach. Up until now, symbolic evaluation has been the key technique of behavior generation. Unfortunately, it has drawbacks both in the amount of time it takes to complete a symbolic run, and in the large amount of uninteresting data it produces. The authors propose goal-directed envisionment as an alternative to symbolic evaluation, supplementing the basic envisioning techniques of qualitative physics with behavioral goals. This approach overcomes the problems of symbolic evaluation by generating interpretations in a reasonable amount of time and by exploiting goals to prioritize and analyze the interpretations. The authors describe and evaluate SC, an implemented system which employs goal-directed envisionment to critique specifications.<<ETX>>",,0-8186-2320,10.1109/IWSSD.1991.213080,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=213080,,Automatic testing;Computer science;Physics;Interconnected systems;Buildings;Robustness;Feathers;Resource management;System testing;Storage automation,formal specification,specification criticism;complex system specification;symbolic evaluation;key technique;behavior generation;symbolic run;uninteresting data;goal-directed envisionment;basic envisioning techniques;qualitative physics;behavioral goals;SC;implemented system,,2,16,,,,,,IEEE,IEEE Conferences
Quality Function Deployment (QFD) in testing,A. Rahman,"IBM UK Labs. Ltd., Winchester, UK",IEE Colloquium on Automated Testing and Software Solutions,,1992,,,1-Jun,7-Jun,"QFD is a quality-oriented process that can play an important role in the market-driven, total quality control environment. It can be deployed in almost all areas of product development, test and manufacturing processes. QFD is one way to ensure the reliability of the software products. The author is concerned with the role that software testing can play in increasing the reliability of software. He also examines how Quality Function Deployment (QFD) could be used to achieve this objective. QFD provides integration of the various functions by tying design and process activities together. It priorities product and manufacturing-process characteristics and highlights areas which would require further analysis.<<ETX>>",,,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=168144,,Automatic testing;Quality control;Software reliability,automatic testing;quality control;software reliability,total quality control;reliability;software products;software testing,,,,,,,,,IET,IET Conferences
A method for automatic evaluation of fault effects in the advanced intelligent network,H. Suzuki; H. Kawamura; T. Akiyama; N. Takahashi,"NTT Commun. Switching Labs., Tokyo, Japan; NTT Commun. Switching Labs., Tokyo, Japan; NTT Commun. Switching Labs., Tokyo, Japan; NTT Commun. Switching Labs., Tokyo, Japan",Proceedings of GLOBECOM '93. IEEE Global Telecommunications Conference,,1993,,,234,238 vol.1,"The method described here provides network operators with criteria for deciding the priorities with which services degraded by network faults should be restored. This method consists of four processes: the first lists the unavailable services of customers affected by the fault, the second predicts the mean time needed to repair the fault, the third predicts the traffic trends for the affected services, and the fourth calculates the criteria used to decide the service restoration priorities. This method enables network operators to choose suitable means restoring services and to avoid future congestion due to faults. It also lets them respond more quickly and precisely to customer claims. We are introducing this method into our advanced IN operations systems.<<ETX>>",,0-7803-0917,10.1109/GLOCOM.1993.318129,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=318129,,Intelligent networks;Degradation;Computer architecture;Communication switching;Laboratories;Telecommunication traffic;Stability;Monitoring;Customer service;Application software,intelligent networks;telecommunication network management;automatic test equipment;telecommunications computing;network operating systems;telecommunication traffic,automatic evaluation;fault effects;advanced intelligent network;network faults;unavailable services;mean time to repair;traffic trends prediction;service restoration priorities;operations systems,,7,6,,,,,,IEEE,IEEE Conferences
New probabilistic measures for accelerating the automatic test pattern generation algorithm,B. Phillips; S. Ganesan; C. Bacon,"Software Div. Tinker AFB, USA; Software Div. Tinker AFB, USA; NA",AUTOTESTCON 93,,1993,,,503,512,"In order to approximate the signal controllabilities, the authors introduce new probabilistic measures called signal priorities, whose computation relies on the minimum-value distributions of fanout input variables of a digital circuit. The signal priorities serve the same purpose as do the signal controllabilities. That is, they are used to accelerate the automatic test pattern generation algorithm; however, their computation requires much less effort. This new method is formally defined and tested with several practical example circuits.<<ETX>>",,0-7803-0646,10.1109/AUTEST.1993.396314,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=396314,,Life estimation;Automatic test pattern generation;Circuit testing;Circuit faults;Automatic testing;Acceleration;Combinational circuits;Digital circuits;Electrical fault detection;Fault detection,automatic test software;probability;automatic programming;controllability;fault location;digital circuits;fault diagnosis;logic testing;signal processing,stuck-at faults;digital circuits;automatic test pattern generation algorithm;signal controllabilities;probabilistic measures;signal priorities;minimum-value distributions;fanout input variables;digital circuit,,,8,,,,,,IEEE,IEEE Conferences
Cost-benefit analysis of electric power system reliability,K. N. Tinnium; P. Rastgoufard; P. F. Duvoisin,"Electr. Power Res. Lab., Tulane Univ., New Orleans, LA, USA; Electr. Power Res. Lab., Tulane Univ., New Orleans, LA, USA; Electr. Power Res. Lab., Tulane Univ., New Orleans, LA, USA",Proceedings of 26th Southeastern Symposium on System Theory,,1994,,,468,472,"The purpose of this investigation is to determine an appropriate cost-benefit formula that will help the power system planners in prioritizing transmission system projects. This paper deals with describing the value of increased reliability and security in bulk power systems. Three different approaches used for prioritization of transmission system projects by the electric utilities are discussed and analyzed for two different transmission system alternatives. Utilizing the best approach, transmission alternatives are prioritized and the best alternative is placed on top of the prioritized table. An analysis of the three approaches and a relative comparison is performed on the IEEE 25 bus Reliability Test System. TRELSS (Transmission Reliability Evaluation of Large Scale Systems), a software package developed by EPRI, is utilized in determining the probabilistic indices that are used in the proposed approach.<<ETX>>",0094-2898,0-8186-5320,10.1109/SSST.1994.287831,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=287831,,Cost benefit analysis;Power system reliability;Power system security;Power system analysis computing;Power industry;Performance analysis;Performance evaluation;System testing;Large-scale systems;Software packages,power system reliability;power system analysis computing;costing;economics,electric power system reliability;cost-benefit analysis;power system planners;security;bulk power systems;transmission system projects prioritisation;electric utilities;IEEE 25 bus Reliability Test System;TRELSS;Transmission Reliability Evaluation of Large Scale Systems;software package;probabilistic indices,,2,9,,,,,,IEEE,IEEE Conferences
Priority based data flow testing,R. Gupta; M. L. Soffa,"Dept. of Comput. Sci., Pittsburgh Univ., PA, USA; Dept. of Comput. Sci., Pittsburgh Univ., PA, USA",Proceedings of International Conference on Software Maintenance,,1995,,,348,357,"Software testing is an expensive component of software development and maintenance. For data flow testing, test cases must be found to test the def-use pairs in a program. Since some of the def-use pairs identified through static analysis may be infeasible, no amount of testing effort may result in exhaustive testing of a program. Therefore in practice a fixed amount of effort is spent in testing a program. We develop an approach for assigning priorities to def-use pairs, such that the def-use pairs with higher priorities can be expected to require less effort for test case generation and therefore testing. Thus, by using the priorities as a guide for ordering the def-use pairs for testing, we can maximize the number of def-use pairs tested using a fixed amount of testing effort. We apply the technique to regression testing during the software maintenance phase, in which case the priorities are assigned to capture not only the difficulty in test case generation but also the likelihood that an error introduced by a program change is uncovered by the test case.",1063-6773,0-8186-7141-60-8186-7677,10.1109/ICSM.1995.526556,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=526556,,Software testing;Computer science;Software maintenance;Data analysis,data flow analysis;program diagnostics;program testing;software maintenance;statistical analysis,priority based data flow testing;software testing;software development;software maintenance;def-use pairs;test case generation;regression testing;error;program change,,2,11,,,,,,IEEE,IEEE Conferences
The peer tasking design method,N. R. Howes; J. D. Wood; A. Goforth,"Inst. for Defense Anal., Alexandria, VA, USA; Inst. for Defense Anal., Alexandria, VA, USA; NA",Proceedings of Third Workshop on Parallel and Distributed Real-Time Systems,,1995,,,20,29,"This paper is a preliminary report of an ARPA sponsored study. It focuses on designing real-time command and control or battle management systems for parallel and distributed architectures. Due to delays in other ARPA programs, the targeted architectures were not available during the time frame of the study. The results of the study were, however, tested on more conventional sequential and parallel platforms. The design method discussed here is fundamentally different from those assumed by current real-time scheduling theories, e.g., rate-monotonic, earliest-deadline-first, least-laxity or best-effort. These theories assume that the fundamental unit of prioritization is the task. In this new method, the fundamental unit of prioritization is called a work item. Work items are functions the system performs that have timing requirements (deadlines) associated with them in the requirements specification. Current scheduling theories are applied using artifact deadlines introduced by designers whereas this new method schedules work items to meet specification deadlines (sometimes called end-to-end deadlines) required by the user. With this method, tasks have no priorities. A collection of tasks with no priorities will be called a collection of peer tasks. The study showed that it is possible to schedule work items based on importance rather than urgency while still meeting as many work item deadlines as can be met by scheduling tasks with respect to urgency. Also, it showed that the minimum on-line deadline that can be guaranteed for a work item of highest importance, scheduled at run-time, is approximately the inverse of the throughput, measured in work items per second, for a work load consisting only of work items of that type. Further, it was shown that it provides optimal utilization of a single processor machine, and that its timing behavior is predictable (provable) for both single and multiprocessor machines. Finally, it was shown that throughput is not degraded during overload.<<ETX>>",,0-8186-7099,10.1109/WPDRTS.1995.470511,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=470511,,Design methodology;Timing;Throughput;Peer to peer computing;Real time systems;Command and control systems;Delay effects;Sequential analysis;Processor scheduling;Runtime,delays;processor scheduling;parallel architectures;real-time systems;command and control systems,peer tasking design method;real-time command and control;battle management systems;distributed architectures;parallel architectures;real-time scheduling;work item;timing requirements;end-to-end deadlines;multiprocessor,,2,14,,,,,,IEEE,IEEE Conferences
High-level synthesis of recoverable microarchitectures,S. Y. Ohm; D. M. Blough; F. J. Kurdahi,"Dept. of Electr. & Comput. Eng., California Univ., Irvine, CA, USA; Dept. of Electr. & Comput. Eng., California Univ., Irvine, CA, USA; Dept. of Electr. & Comput. Eng., California Univ., Irvine, CA, USA",Proceedings ED&TC European Design and Test Conference,,1996,,,55,62,Two algorithms that combine the operations of scheduling and recovery point insertion for high-level synthesis of recoverable microarchitectures are presented. The first uses a prioritized cost function in which functional unit cost is minimized first and register cost second. The second algorithm minimizes a weighted sum of functional unit and register costs. Both algorithms are optimal according to their respective cost functions and require less than 10 minutes of CPU time on widely-used high-level synthesis benchmarks. The best previous result reported several hours of CPU time for some of the same benchmarks.,1066-1409,0-8186-7424,10.1109/EDTC.1996.494128,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494128,,High level synthesis;Microarchitecture;Cost function;Scheduling algorithm;Hardware;Registers;Processor scheduling;Digital systems;Fault tolerant systems;Application software,high level synthesis;reconfigurable architectures;scheduling;digital systems;data flow graphs,high-level synthesis;recoverable microarchitectures;scheduling;recovery point insertion;prioritized cost function;functional unit cost;register cost;weighted sum;benchmarks;fault-tolerant digital systems;CDFG;control and data flow graph,,4,15,,,,,,IEEE,IEEE Conferences
Selecting engineering techniques using fuzzy logic based decision support,P. Liggesmeyer,"Corp. Res. & Dev., Software & Eng., Siemens AG, Munich, Germany",Proceedings IEEE Symposium and Workshop on Engineering of Computer-Based Systems,,1996,,,427,434,"The task of selecting software engineering methods, techniques, metrics, and tools is usually performed manually, based on the expertise of individuals. This paper presents a systematic tool supported approach, that bases its suggestions an the technical situation, the existing goals, and constraints of a specific organization or a particular project. A prototype of the decision support system supports the elaboration of test strategies. The approach uses information about the technical situation that is provided by answering predefined questions with fuzzy data. The objective is to assign ""adequacy values"" to combinations of test methods, techniques, metrics, tools, and quantified test situations. The priorities of goals and constraints have assessed by applying a technique that is based on comparing goals in pairs. This permits to check certain consistency criteria by static analysis. A hierarchy of the importance of goals and constraints is calculated, which provides the basis for the determination of the suitability of test methods; techniques, metrics, and tools with respect to goals and constraints.",,0-8186-7355,10.1109/ECBS.1996.494570,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=494570,,Fuzzy logic;Software testing;Software engineering;Prototypes;Decision support systems;System testing;Programming;Process control;Research and development;Design engineering,fuzzy logic;decision support systems;software tools;system monitoring;systems software;software metrics,metrics;fuzzy logic based decision support;software engineering method selection;tools;systematic tool supported approach;technical situation;goals;constraints;decision support system;test strategies;predefined questions;fuzzy data;adequacy values;quantified test situations;goal comparison;consistency criteria checking;static analysis,,2,26,,,,,,IEEE,IEEE Conferences
A high-level synthesis approach to design of fault-tolerant systems,G. Buonanno; M. Pugassi; M. G. Sami,"Dipt. di Elettronica, Politecnico di Milano, Italy; NA; NA",Proceedings. 15th IEEE VLSI Test Symposium (Cat. No.97TB100125),,1997,,,356,361,"Fault-tolerance in embedded systems is a requirement of increasing importance; solutions must achieve a balance between performances and costs that was not usually requested in design of more classical fault-tolerant applications and that involves as a consequence new approaches. A design technique is here proposed supporting fault-tolerance of hardware modules in complex hardware-software systems, fault-tolerance requirements for each hardware-mapped process are specified in terms of time constraints and of relative priorities, and a high-level synthesis methodology allowing to design - for each process - a processor capable of supporting both the nominal execution of the process itself in a fault-free environment and simultaneous execution of a reconfigured pair of processes in a fault-affected environment is defined Performances of the scheduling algorithm, allowing to achieve reconfiguration with minimum resource increase and within the required limits of speed degradation, are evaluated on some relevant instances of algorithms discussed in current literature on high-level synthesis.",1093-0167,0-8186-7810,10.1109/VTEST.1997.600305,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=600305,,High level synthesis;Fault tolerant systems;Algorithm design and analysis;Scheduling algorithm;Embedded system;Costs;Hardware;Time factors;Design methodology;Performance evaluation,fault tolerant computing;high level synthesis;real-time systems;reconfigurable architectures,high-level synthesis;design;fault-tolerant system;embedded system;hardware-software system;cost;processor;scheduling algorithm;reconfiguration,,6,10,,,,,,IEEE,IEEE Conferences
A multi-platform support environment,G. Miyahara; C. P. Satterthwaite,"Hughes Aircraft Co., Los Angeles, CA, USA; NA",Proceedings of the IEEE 1997 National Aerospace and Electronics Conference. NAECON 1997,,1997,2,,815,819 vol.2,"Legacy weapon systems, such as attack aircraft, have taken advantage of embedded computers and software to provide enormous capabilities for flexibility and expandability. The provision of these capabilities has been at a cost, and that is in the dedicated software development facilities which have sprung up to support these legacy systems. Unfortunately, the costs of these dedicated facilities is becoming prohibitive. The Advanced Avionics Multi-Radar Software Support Study (AAMRSSS) project offers experience in handling the above problem. AAMRSSS studied the feasibility of using a dedicated Software Development Facility (SDF) to support multiple software system platforms. Issues of the study were: commonality; unique requirements of the new system to be added; platform priorities; and future expansion. In particular, this study has addressed supporting the AC-130U Gunship's Radar Operational Flight Program (OFF) in the F-15's Radar Software Development Facility.",,0-7803-3725,10.1109/NAECON.1997.622734,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=622734,,Weapons;Embedded software;Software systems;Radar;Costs;Aerospace electronics;Aircraft;Software testing;System testing;Embedded system,military avionics;real-time systems;radar computing;project support environments;military computing;weapons;aircraft computers;aerospace simulation;airborne radar,multi-platform support environment;legacy weapon systems;attack aircraft;AAMRSSS project;dedicated software development facility;multiple software system platforms;commonality;platform priorities;future expansion;radar operational flight program support;radar software development facility;embedded computers;weapon system embedded software;shared platform support;target generator;simulation models;avionics software,,2,5,,,,,,IEEE,IEEE Conferences
A study of effective regression testing in practice,W. E. Wong; J. R. Horgan; S. London; H. Agrawal,"Bellcore, Morristown, NJ, USA; NA; NA; NA",Proceedings The Eighth International Symposium on Software Reliability Engineering,,1997,,,264,274,"The purpose of regression testing is to ensure that changes made to software, such as adding new features or modifying existing features, have not adversely affected features of the software that should not change. Regression testing is usually performed by running some, or all, of the test cases created to test modifications in previous versions of the software. Many techniques have been reported on how to select regression tests so that the number of test cases does not grow too large as the software evolves. Our proposed hybrid technique combines modification, minimization and prioritization-based selection using a list of source code changes and the execution traces from test cases run on previous versions. This technique seeks to identify a representative subset of all test cases that may result in different output behavior on the new software version. We report our experience with a tool called ATAC (Automatic Testing Analysis tool in C) which implements this technique.",,0-8186-8120,10.1109/ISSRE.1997.630875,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=630875,,Software testing;Performance evaluation;Software performance;Software debugging;Computer bugs;Time factors,statistical analysis;configuration management;software maintenance;program testing;software tools;minimisation,regression testing;software changes;software features;test cases;program modifications;previous software versions;hybrid technique;modification-based test selection;prioritization-based selection;source code changes;execution traces;representative subset;output behavior;ATAC;Automatic Testing Analysis tool in C;test set minimization;test set prioritization,,130,25,,,,,,IEEE,IEEE Conferences
A systematic tradeoff analysis for conflicting imprecise requirements,J. Yen; W. A. Tiao,"Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA; NA",Proceedings of ISRE '97: 3rd IEEE International Symposium on Requirements Engineering,,1997,,,87,96,"The need to deal with conflicting system requirements has become increasingly important over the past several years. Often, these requirements are elastic in that they can be satisfied to a degree. The overall goal of this research is to develop a formal framework that facilitates the identification and the tradeoff analysis of conflicting requirements by explicitly capturing their elasticity. Based on a fuzzy set theoretic foundation for representing imprecise requirements, we describe a systematic approach for analyzing the tradeoffs between conflicting requirements using the techniques in decision science. The systematic tradeoff analyses are used for three important tasks in the requirement engineering process: (1) for validating the structure used in aggregating prioritized requirements, (2) for identifying the structures and the parameters of the underlying representation of imprecise requirements and (3) for assessing the priorities of conflicting requirements. We illustrate these techniques using the requirements of a conference room scheduling system.",,0-8186-7740,10.1109/ISRE.1997.566845,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=566845,,Fuzzy logic;Testing;Cost function;Fuzzy sets;Fuzzy systems;Marine vehicles,formal specification;systems analysis,imprecise requirements;tradeoff analysis;fuzzy set theory;requirement engineering process;conference room scheduling system;prioritized requirements,,18,15,,,,,,IEEE,IEEE Conferences
An optimal algorithm for scheduling soft aperiodic tasks in dynamic-priority preemptive systems,I. Ripoll; A. Crespo; A. Garcia-Fornes,"Dept. de Ingenieria de Sistemas, Comput. y Autom., Univ. Politecnica de Valencia, Spain; NA; NA",IEEE Transactions on Software Engineering,,1997,23,6,388,400,"The paper addresses the problem of jointly scheduling tasks with both hard and soft real time constraints. We present a new analysis applicable to systems scheduled using a priority preemptive dispatcher, with priorities assigned dynamically according to the EDF policy. Further, we present a new efficient online algorithm (the acceptor algorithm) for servicing aperiodic work load. The acceptor transforms a soft aperiodic task into a hard one by assigning a deadline. Once transformed, aperiodic tasks are handled in exactly the same way as periodic tasks with hard deadlines. The proposed algorithm is shown to be optimal in terms of providing the shortest aperiodic response time among fixed and dynamic priority schedulers. It always guarantees the proper execution of periodic hard tasks. The approach is composed of two parts: an offline analysis and a run time scheduler. The offline algorithm runs in pseudopolynomial time O(mn), where n is the number of hard periodic tasks and m is the hyperperiod/min deadline.",0098-5589;1939-3520;2326-3881,,10.1109/32.601081,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=601081,,Scheduling algorithm;Dynamic scheduling;Processor scheduling;Real time systems;Testing;Delay;Timing;Computer Society;Runtime,scheduling;real-time systems;minimisation;computational complexity;distributed algorithms,optimal algorithm;soft aperiodic task scheduling;dynamic priority preemptive systems;soft real time constraints;priority preemptive dispatcher;EDF policy;online algorithm;acceptor algorithm;aperiodic work load servicing;hard deadlines;aperiodic response time;dynamic priority schedulers;periodic hard tasks;offline analysis;run time scheduler;pseudopolynomial time;earliest deadline first,,24,13,,,,,,IEEE,IEEE Journals & Magazines
Design and implementation of a real-time switch for segmented Ethernets,C. Venkatramani; Tzi-cker Chiueh,"IBM Thomas J. Watson Res. Center, Hawthorne, NY, USA; NA",Proceedings 1997 International Conference on Network Protocols,,1997,,,152,161,"Providing network bandwidth guarantees over an Ethernet requires coordination of the network nodes for traffic prioritization such that real-time data can have deterministic access to the network. We have shown previously how RETHER, a software based token passing protocol can efficiently provide real-time support over a single shared Ethernet segment. This work extends the token passing mechanism into a switched, multi-segment Ethernet environment. This paper describes the detailed design issues, their solutions, and a fully operational switch implementation built into the FreeBSD kernel. By testing the protocol independently and as the underlying network protocol of the Stony Brook Video Server, we have verified that the bandwidth guarantees are successfully provided, with relatively low run-time overhead, for real-time connections crossing multiple Ethernet segments. This paper also provides a comprehensive performance evaluation of the prototype.",1092-1648,0-8186-8061,10.1109/ICNP.1997.643709,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=643709,,Switches;Ethernet networks;Bandwidth;Telecommunication traffic;Access protocols;Kernel;Testing;Network servers;Runtime;Prototypes,performance evaluation;transport protocols;local area networks;multimedia systems;telecommunication traffic,real-time switch;segmented Ethernets;network bandwidth guarantees;traffic prioritization;RETHER;software based token passing protocol;FreeBSD kernel;Stony Brook Video Server;real-time connections;performance evaluation,,12,17,,,,,,IEEE,IEEE Conferences
Failproof team projects in software engineering courses,A. T. Berztiss,"Dept. of Comput. Sci., Pittsburgh Univ., PA, USA",Proceedings Frontiers in Education 1997 27th Annual Conference. Teaching and Learning in an Era of Change,,1997,2,,1015,1019 vol.2,"The computer science department of the University of Pittsburgh offers two undergraduate and two graduate courses in software engineering in which we emphasize the importance of general engineering principles for software development. For the last ten years the undergraduate courses have been based on team projects. This structure has advantages: students see immediately the relevance of what they learn, and the team setting leads to a better understanding of what they learn. The projects in the two courses are of different types. In one course the result is the formal specification and design of a software system. In the other, the teams implement such a system, but emphasis is on testing rather than on the implementation itself. The success of each project is guaranteed by making it open-ended. A team establishes a list of priorities that is to ensure that a useful product will have been built by the time the term ends. We discuss the nature of team projects, and our evaluation scheme.",0190-5848,0-7803-4086,10.1109/FIE.1997.636027,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=636027,,Software engineering;Computer science;Programming;Design engineering;Computer science education;Teamwork;Formal specifications;Software design;Software systems;System testing,computer science education;formal specification;educational courses,failproof team projects;software engineering courses;Pittsburgh University;graduate courses;undergraduate courses;general engineering principles;course projects;formal specification;software system design;open-ended project,,2,9,,,,,,IEEE,IEEE Conferences
Prioritizing Software Requirements In An Industrial Setting,K. Ryan; J. Karlsson,University of Limerick; NA,Proceedings of the (19th) International Conference on Software Engineering,,1997,,,564,565,,0270-5257,0-89791-914,10.1145/253228.253453,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=610408,,Computer industry;Collaborative software;Costs;Permission;Software testing;System testing;Lead;Telephony;Software engineering;Prototypes,"Requirements, prioritizing, release planning",,,6,2,,,,,,IEEE,IEEE Conferences
READS: a prototyping environment for real-time active applications,Kam-Yiu Lam; T. S. H. Lee; S. H. Son,"Dept. of Comput. Sci., City Univ. of Hong Kong, Kowloon, Hong Kong; NA; NA","Database and Expert Systems Applications. 8th International Conference, DEXA '97. Proceedings",,1997,,,265,270,"We present our Real-time Active Database System, READS, which is a prototyping environment for real-time active database applications on a conventional Unix environment, e.g., Solaris 2.4. In READS, transactions are associated with deadlines and priorities. Priority scheduling is supported by the real-time extensions provided in the underlying operating system. READS can be served as a testbed for evaluating different issues in the design of real-time active database systems (RTADBS). Different approaches for assigning deadlines and priorities to triggered transactions have been suggested and discussed. An application, the programmed trading database system, is implemented with READS and experiments have been performed to study the impact of the deadline constraints on the performance of the deferred and immediate coupling modes for triggering.",,0-8186-8147,10.1109/DEXA.1997.617285,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=617285,,Prototypes;Database systems;Real time systems;Application software;Transaction databases;Timing;Computer science;Operating systems;System testing;Algorithm design and analysis,real-time systems;active databases;deductive databases;software prototyping;Unix;transaction processing;scheduling;software performance evaluation;concurrency control,READS;prototyping environment;real-time active database;Unix environment;Solaris 2.4;transactions;deadlines;priority scheduling;operating system;triggered transactions;trading database system;experiments;deadline constraints;performance;concurrency control,,,13,,,,,,IEEE,IEEE Conferences
Stochastic Petri nets applied to the performance evaluation of static task allocations in heterogeneous computing environments,A. R. McSpadden; N. Lopez-Benitez,"Dept. of Comput. Sci., Texas Tech. Univ., Lubbock, TX, USA; NA",Proceedings Sixth Heterogeneous Computing Workshop (HCW'97),,1997,,,185,194,"A stochastic Petri net (SPN) is systematically constructed from a task graph whose component subtasks are statically allocated onto the processor suite of a heterogeneous computing system (HCS). Given that subtask execution times are exponentially distributed an exponential distribution can be generated for the overall completion time. In particular the enabling functions and rate functions used to specify the SPN model provide needed versatility to integrate processor heterogeneity, task priorities, allocation schemes, communication costs, and other factors characteristic of a HCS into a comprehensive performance analysis. The manner in which these parameters are incorporated into the SPN allows the model to be transformed into a testbed for optimization schemes and heuristics. The proposed approach can be applied to arbitrary task graphs including non-series-parallel.",,0-8186-7879,10.1109/HCW.1997.581420,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=581420,,Stochastic processes;Petri nets;Delay estimation;Stochastic systems;Processor scheduling;Computer science;Educational institutions;Cost function;Testing;Tiles,Petri nets;stochastic processes;resource allocation;software performance evaluation;distributed processing;open systems;exponential distribution;optimisation;heuristic programming,stochastic Petri net;performance evaluation;static task allocations;heterogeneous computing environments;task graph;processor suite;subtask execution times;exponential distribution;completion time;enabling functions;rate functions;SPN model;processor heterogeneity;task priorities;allocation schemes;communication costs;optimization schemes;heuristics;non-series-parallel graphs,,5,23,,,,,,IEEE,IEEE Conferences
Testing for millennium risk management,M. Feord,"Compuware, Netherlands",IEEE Software,,1997,14,3,126,127,"The Year 2000 conversion is a challenge to the economics of both testing and maintenance, but as a whole we are not responding with balanced priorities. Those looking for Year 2000 solutions typically allocate their first energies and budgets to acquiring automated analysis and conversion deals and services. This disturbing tendency ignores two facts that surface in virtually every analysis of the Year 2000 challenge: fifty percent or more of the effort will be in testing; and despite consuming a wealth of resources each year, current testing practices cannot satisfy the demands of current maintenance unrelated to the Y2K conversion. For the moment, most organizations continue to delay action on the Y2K Test problem while wading through the dozens of available analysis and conversion solutions. As a result, many Y2K projects have started on master plans that will need major revision once the true needs and benefits of testing automation become apparent. A growing number of those projects have already corrected course, revising strategy and reallocating budgets once they appreciated the nature of the Y2K testing challenge. Embarrassment will probably be the least of many worries for those that ignore the challenge much longer.",0740-7459;1937-4194,,10.1109/52.589256,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=589256,,Risk management;Automatic testing;System testing;Costs;Production facilities;Software testing;Manufacturing automation;Productivity;Costing;Performance evaluation,program testing;software maintenance;risk management;software development management,millennium risk management testing;Year 2000 conversion;software maintenance;Year 2000 solutions;testing practices;Y2K conversion;Y2K Test problem;conversion solutions;Y2K projects;master plans;budgets;Y2K testing challenge,,,,,,,,,IEEE,IEEE Journals & Magazines
Derivation of an integrated operational profile and use case model,P. Runeson; B. Regnell,"Dept. of Commun. Syst., Lund Univ., Sweden; NA",Proceedings Ninth International Symposium on Software Reliability Engineering (Cat. No.98TB100257),,1998,,,70,79,"Requirements engineering and software reliability engineering both involve model building related to the usage of the intended system; requirements models and test case models respectively are built. Use case modelling for requirements engineering and operational profile testing for software reliability engineering are techniques which are evolving into software engineering practice. Approaches towards integration of the use case model and the operational profile model are proposed. By integrating the derivation of the models, effort may be saved in both development and maintenance of software artifacts. Two integration approaches are presented, transformation and extension. It is concluded that the use case model structure can be transformed into an operational profile model adding the profile information. As a next step, the use case model can be extended to include the information necessary for the operational profile. Through both approaches, modelling and maintenance effort as well as risks for inconsistencies can be reduced. A positive spin-off effect is that quantitative information on usage frequencies is available in the requirements, enabling planning and prioritizing based on that information.",1071-9458,0-8186-8991,10.1109/ISSRE.1998.730843,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=730843,,Computer aided software engineering;System testing;Reliability engineering;Software testing;Software reliability;Costs;Software engineering;Programming;Software quality;Probability,formal specification;software reliability;software maintenance;software maintenance,integrated operational profile/use case model;requirements engineering;software reliability engineering;requirements models;test case models;software artifact development;software artifact maintenance;transformation;extension;quantitative information;usage frequencies;planning;prioritization,,8,16,,,,,,IEEE,IEEE Conferences
Prioritized token-based mutual exclusion for distributed systems,F. Mueller,"Inst. fur Inf., Humboldt-Univ., Berlin, Germany",Proceedings of the First Merged International Parallel Processing Symposium and Symposium on Parallel and Distributed Processing,,1998,,,791,795,"A number of solutions have been proposed for the problem of mutual exclusion in distributed systems. Some of these approaches have since been extended to a prioritized environment suitable for real-time applications but impose a higher message passing overhead than our approach. We present a new protocol for prioritized mutual exclusion in a distributed environment. Our approach uses a token-based model working on a logical tree structure, which is dynamically modified. In addition, we utilize a set of local queues whose union would resemble a single global queue. Furthermore, our algorithm is designed for out-of-order message delivery, handles messages asynchronously and supports multiple requests from one node for multi-threaded nodes. The prioritized algorithm has an average overhead of O(log(n)) messages per request for mutual exclusion with a worst-case overhead of O(n), where n represents the number of nodes in the system. Thus, our prioritized algorithm matches the message complexity of the best non-prioritized algorithms while previous prioritized algorithms have a higher message complexity, to our knowledge. Our concept of local queues can be incorporated into arbitrary token-based protocols with or without priority support to reduce the amount of messages. Performance results indicate that the additional functionality of our algorithm comes at the cost of 30% longer response times within our test environment for distributed execution when compared with an unprioritized algorithm. This result suggests that the algorithm should be used when strict priority ordering is required.",1063-7133,0-8186-8404,10.1109/IPPS.1998.670018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670018,,Protocols;Delay;Testing;Message passing;Tree data structures;Algorithm design and analysis;Cost function;Memory architecture;Hardware;Broadcasting,message passing;real-time systems;communication complexity;protocols;distributed algorithms;software performance evaluation,prioritized token-based mutual exclusion;distributed systems;real-time applications;message passing overhead;protocol;logical tree structure;local queues;global queue;out-of-order message delivery;multiple requests;multithreaded nodes;overhead;message complexity;performance;response times;strict priority ordering,,25,20,,,,,,IEEE,IEEE Conferences
Software testability measurements derived from data flow analysis,Pu-Lin Yeh; Jin-Cherng Lin,"Dept. of Comput. Sci. & Eng., Tatung Inst. of Technol., Taipei, Taiwan; NA",Proceedings of the Second Euromicro Conference on Software Maintenance and Reengineering,,1998,,,96,102,"The purpose of the research is to develop formulations to measure the testability of a program. Testability is a program's property which is introduced with the intention of predicting efforts required for testing the program. A program with a high degree of testability indicates that a selected testing criterion could be achieved with less effort and the existing faults can be revealed more easily during testing. We propose a new program normalization strategy that makes the measurement of testability more precise and reasonable. If the program testability metric derived from data flow analysis could be applied at the beginning of a software testing phase, much more effective testing of resource allocation and prioritizing is possible.",,0-8186-8421,10.1109/CSMR.1998.665760,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=665760,,Software testing;Software measurement;Data analysis;Software metrics;Computer science;Resource management;Electrical capacitance tomography;Size control,program testing;software metrics;resource allocation;data flow analysis,software testability measurements;data flow analysis;program testability;testing criterion;program normalization strategy;program testability metric;software testing phase;resource allocation,,4,19,,,,,,IEEE,IEEE Conferences
Scenario based integration testing for object-oriented software development,Youngchul Kim; C. R. Carlson,"Dept. of Comput. Sci., Illinois Inst. of Technol., Chicago, IL, USA; NA",Proceedings Eighth Asian Test Symposium (ATS'99),,1999,,,283,288,"The adaptive use case methodology for software development proposed by Carlson and Hurlbutt [1997, 1998] forms the backdrop for this paper. Their methodology integrates the software design, development and testing processes through a series of design preserving, algorithmic transformations. This paper focuses on the software testing metrics used in the generation of object oriented test plans as part of that methodology. During the design phase, interaction diagrams are developed from which use case action matrices are then generated. A use case action matrix contains a collection of related scenarios each describing a specific variant of an executable sequence of use case actions. The proposed software testing metrics are employed to improve the productivity of the testing process through scenario prioritization.",1081-7735,0-7695-0315,10.1109/ATS.1999.810764,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=810764,,Software testing;Programming;Algorithm design and analysis;Software algorithms;Electrical capacitance tomography;Software systems;Control systems;Computer science;Scholarships;Software design,object-oriented methods;software engineering;program testing,scenario based integration testing;object-oriented software development;adaptive use case methodology;design preserving algorithmic transformations;testing metrics;object oriented test plans;interaction diagrams;use case action matrices;executable sequence;productivity;scenario prioritization,,5,10,,,,,,IEEE,IEEE Conferences
"Static properties of commercial embedded real-time programs, and their implication for worst-case execution time analysis",J. Engblom,"Dept. of Comput. Syst., Uppsala Univ., Sweden",Proceedings of the Fifth IEEE Real-Time Technology and Applications Symposium,,1999,,,46,55,"We have used a modified C compiler to analyze a large number of commercial real time and embedded applications written in C for 8- and 18-bit processors. Only static aspects of the programs have been studied i.e., such information that can be obtained from the source code without running the programs. The purpose of the study is to provide guidance for the development of worst-case execution time (WCET) analysis tools, and to increase the knowledge about embedded programs in general. Knowing how real programs are written makes it easier to focus research in relevant areas and set priorities. The conclusion is that real time and embedded programs are not necessarily simple just because they are written for small machines. This indicates that real life WCET analysis tools need to handle advanced programming constructions, including function pointer calls and recursion.",1080-1812,0-7695-0194,10.1109/RTTAS.1999.777660,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777660,,Embedded system;Real time systems;Program processors;Software measurement;Benchmark testing;Performance analysis;Embedded computing;Application software;Functional programming;Embedded software,embedded systems;program compilers;C language;system monitoring;programming,static properties;commercial embedded real time programs;modified C compiler;embedded applications;static aspects;source code;worst-case execution time analysis tools;embedded programs;small machines;real life WCET analysis tools;advanced programming constructions;function pointer calls;recursion,,13,21,,,,,,IEEE,IEEE Conferences
Static-priority scheduling of multiframe tasks,S. K. Baruah; Deji Chen; A. Mok,"Vermont Univ., Burlington, VT, USA; NA; NA",Proceedings of 11th Euromicro Conference on Real-Time Systems. Euromicro RTS'99,,1999,,,38,45,The multiframe model of hard-real-time tasks is a generalization of the well-known periodic task model of C. Liu and J. Layland (1973). The feasibility analysis of systems of multiframe tasks which are assigned priorities according to the rate-monotonic priority assignment scheme is studied. An efficient sufficient feasibility test for such systems of multiframe tasks is presented and proved correct-this generalizes a result of A.K. Mok and D. Chen (1997).,1068-3070,0-7695-0240,10.1109/EMRTS.1999.777448,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=777448,,Power system modeling;Scheduling algorithm;Real time systems;Runtime;System testing;Character generation;Processor scheduling,software engineering;multiprogramming;processor scheduling,static-priority scheduling;multiframe tasks;multiframe model;hard-real-time tasks;feasibility analysis;rate-monotonic priority assignment scheme;feasibility test,,8,11,,,,,,IEEE,IEEE Conferences
Teenagers identify causes of violence in schools and develop strategies to eliminate violence using GroupSystems electronic meeting system (EMS),B. F. Marsh,"Comput. Sci. Corp., Huntsville, AL, USA",Proceedings of the 32nd Annual Hawaii International Conference on Systems Sciences. 1999. HICSS-32. Abstracts and CD-ROM of Full Papers,,1999,Track1,,11 pp.,,"Is it possible to engage teenagers in a serious effort to identify the root causes of school violence and to develop strategies to deal with it? If so, will computer-aided group decision support tools add value to the process? Those are the questions we addressed with the 1998 Teen Think Tank on School Violence. While this was neither a formal nor a scientific treatment of the subject, the results of the initial experiment were overwhelmingly impressive. Using the GroupSystems Electronic Meeting System (EMS), sixteen teenagers grappled with the issue of school violence and generated more than 800 different ways to predict, prevent, avoid, protect, react, eliminate, or cope with youth violence. After brainstorming for ideas, they also used EMS to categorize, prioritize, and to reach consensus about their best ideas. Then they developed teen recommendations for students, parents, teachers, school administrators, and law enforcement officers. This was all accomplished in two EMS sessions; and none of the students had any prior knowledge or experience with EMS. This report presents a synopsis of their findings and a brief description of the EMS process.",,0-7695-0001,10.1109/HICSS.1999.772733,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=772733,,Medical services;Educational institutions;Protection;Law enforcement;Software systems;Productivity;Testing;Meetings,group decision support systems;social aspects of automation;teleconferencing,GroupSystems electronic meeting system;school violence;computer-aided group decision support tools;brainstorming,,,,,,,,,IEEE,IEEE Conferences
Test case prioritization: an empirical study,G. Rothermel; R. H. Untch; Chengyun Chu; M. J. Harrold,"Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA; NA; NA",Proceedings IEEE International Conference on Software Maintenance - 1999 (ICSM'99). 'Software Maintenance for Business Change' (Cat. No.99CB36360),,1999,,,179,188,Test case prioritization techniques schedule test cases for execution in an order that attempts to maximize some objective function. A variety of objective functions are applicable; one such function involves rate of fault detection-a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during regression testing can provide faster feedback on a system under regression test and let debuggers begin their work earlier than might otherwise be possible. In this paper we describe several techniques for prioritizing test cases and report our empirical results measuring the effectiveness of these techniques for improving rate of fault detection. The results provide insights into the tradeoffs among various techniques for test case prioritization.,1063-6773,0-7695-0016,10.1109/ICSM.1999.792604,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=792604,,Computer aided software engineering;Fault detection;System testing;Software testing;Computer science;Job shop scheduling;Feedback;Software maintenance;Information science;Ear,program testing,test cases;test case prioritization;objective function;regression test;debuggers;fault detection,,109,21,,,,,,IEEE,IEEE Conferences
The far ultraviolet spectroscopic explorer (FUSE) instrument data system,B. K. Heggestad; R. C. Moore,"Appl. Phys. Lab., Johns Hopkins Univ., Laurel, MD, USA; NA",Gateway to the New Millennium. 18th Digital Avionics Systems Conference. Proceedings (Cat. No.99CH37033),,1999,2,,7.C.2,7.C.2,"This paper describes the architecture for the IDS flight hardware and its real-time embedded flight software. The design uses commercial off-the-shelf (COTS) software components as much as possible, to reduce cost and software development time. The features of the IDS design that provide radiation hardness and fault tolerance are described. Implementation of software to meet the functional requirements is accomplished using a relatively small number of prioritized real-time tasks. A commercial real-time operating system kernel manages and supports these tasks. Inter-task communication is described, as are the software test and validation methods. The paper shows how custom ground support equipment was developed to facilitate software development and testing. Reliable communications between the IDS and the FUSE spacecraft bus are accomplished using a MIL-STD-1553B bus that has an imposed, deterministic real-time protocol. Similarly, communication between the IDS and the other instrument subsystems uses a second MIL-STD-1553B bus that has its own time division multiplex real-time protocol. The design of these real-time protocols is described, with particular attention to reliability and testability.",,0-7803-5749,10.1109/DASC.1999.821995,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=821995,,Spectroscopy;Fuses;Instruments;Intrusion detection;Protocols;Programming;Software testing;Computer architecture;Hardware;Embedded software,aerospace instrumentation;ultraviolet spectroscopy;spectroscopy computing;aerospace computing;embedded systems;software architecture;system buses;computer architecture;radiation hardening (electronics);space vehicle electronics;software fault tolerance;astronomy computing;satellite telemetry,FUSE;architecture;IDS flight hardware;real-time embedded flight software;COTS software;cost;development time;radiation hardness;fault tolerance;prioritized real-time tasks;real-time operating system kernel;inter-task communication;custom ground support equipment;software development;testing;MIL-STD-1553B bus;real-time protocol;time division multiplex real-time protocol;testability;reliability,,,,,,,,,IEEE,IEEE Conferences
A physics/engineering of failure based analysis and tool for quantifying residual risks in hardware,S. L. Cornford; M. Gibbel; M. Feather; D. Oberhettinger,"Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; NA; NA; NA",Annual Reliability and Maintainability Symposium. 2000 Proceedings. International Symposium on Product Quality and Integrity (Cat. No.00CH37055),,2000,,,382,388,"NASA Code Q is supporting efforts to improve the verification and validation and the risk management processes for spaceflight projects. A physics-of-failure based Defect Detection and Prevention (DDP) methodology previously developed has been integrated into a software tool and is currently being implemented on various NASA projects and as part of NASA's new model-based spacecraft development environment. The DDP methodology begins with prioritizing the risks (or failure modes, FMs) relevant to a mission which need to be addressed. These risks can be reduced through the implementation of a set of detection and prevention activities referred to herein as PACTs (preventative measures, analyses, process controls and tests). Each of these PACTs has some effectiveness against one or more FMs but also has an associated resource cost. The FMs can be weighted according to their likelihood of occurrence and their mission impact should they occur. The net effectiveness of various combinations of PACTs can then be evaluated against these weighted FMs to obtain the residual risk for each of these FMs and the associated resource costs to achieve these risk levels. The process thus identifies the project-relevant ""tall pole"" FMs and design drivers and allows real time tailoring with the evolution of the design and technology content. The DDP methodology allows risk management in its truest sense: it identifies and assesses risk, provides options and tools for risk decision making and mitigation and allows for real-time tracking of current risk status.",0149-144X,0-7803-5848,10.1109/RAMS.2000.816338,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=816338,,Physics;Failure analysis;Flexible manufacturing systems;NASA;Risk management;Costs;Aerospace engineering;Software tools;Space vehicles;Risk analysis,space vehicles;risk management;failure analysis,failure based analysis;hardware residual risks quantification;NASA Code Q;risk management processes;spaceflight projects;physics-of-failure;Defect Detection and Prevention methodology;software tool;model-based spacecraft development environment;risks prioritisation;failure modes;prevention activities;resource cost;mission impact;risk management;risk decision making,,1,7,,,,,,IEEE,IEEE Conferences
Architecture of the simplified Chinese embedded system STARTH,P. X. Mao; Zhizhong Tang; M. Chen; Youming Zhang; Vern Zhang,"Dept. of Comput. Sci., Tsinghua Univ., Beijing, China; NA; NA; NA; NA",Proceedings Fourth International Conference/Exhibition on High Performance Computing in the Asia-Pacific Region,,2000,2,,1167,1170 vol.2,"There is a trend for the information products that are integrated by computer, communication, and consumer electronics. The OS is required more compact and practical. An embedded system STARTH is developed based on the core of the Motorola PPSM that is a real time 32-bit kernel with prioritized interrupt scheduling. All tasks are interrupt-driven. The PPSM kernel does not access hardware device directly. The kernel controls all peripherals indirectly, through software device drivers. The PPSM tools consist of pen input, graphics, database, text, character input, system and communication. The PPSM toolsets, together with its device drivers, provides the basic contro of the LCD, the drawing functions, the real time clock and the UART. The architecture of the embedded system STARTH on the Dragon Ball EZ platform is discussed in parts in details. The development environments of both software and hardware are described. The system is analyzed from its initialization, registration to system management, even the applications programming. The STARTH is tested and run on the hardware system of Dragon Ball platform. It is found that STARTH is practical and reliable for personal information devices.",,0-7695-0590-20-7695-0589,10.1109/HPC.2000.843622,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=843622,,Embedded system;Kernel;Hardware;Computer architecture;Consumer electronics;Operating systems;Real time systems;Processor scheduling;Communication system control;Graphics,personal information systems;consumer electronics;embedded systems;application generators;application program interfaces;parallel processing,Chinese embedded system STARTH;information products;Motorola PPSM;real time 32-bit kernel;prioritized interrupt scheduling;hardware device;software device drivers;pen input;graphics;UART;Dragon Ball EZ platform;development environments;system management;applications programming;personal information devices,,,9,,,,,,IEEE,IEEE Conferences
Building an IP network quality-of-service testbed,D. T. McWherter; J. Sevy; W. C. Regli,"Drexel Univ., Philadelphia, PA, USA; NA; NA",IEEE Internet Computing,,2000,4,4,65,73,The Drexel Network Toolkit is a software package for testing various approaches to QoS on IP-based networks. It uses Linux and DiffServ packet-marking primitives to classify and prioritize packets. DNT was used in a project to evaluate satellite based IP delivery for multimedia applications in telemedicine and telemaintenance.,1089-7801;1941-0131,,10.1109/4236.865089,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=865089,,IP networks;Quality of service;Testing;Traffic control;Delay;Internet;Routing;Application software;Linux;Satellites,Internet;protocols;quality of service;Unix;packet switching;multimedia systems;telecommunication computing;telemedicine,IP network quality-of-service testbed;Drexel Network Toolkit;software package;QoS;IP based networks;Linux;DiffServ packet-marking primitives;packet classification;satellite based IP delivery;multimedia applications;telemedicine;telemaintenance,,5,13,,,,,,IEEE,IEEE Journals & Magazines
GBS IP multicast tunneling,L. Allen; C. Murray; M. DiFrancisco; C. Ellis; S. Hughes; M. Blanding,"Booz, Allen & Hamilton Inc., McLean, VA, USA; NA; NA; NA; NA; NA",MILCOM 2000 Proceedings. 21st Century Military Communications. Architectures and Technologies for Information Superiority (Cat. No.00CH37155),,2000,2,,699,703 vol.2,"Internet protocol (IP) multicast over the DoD's Global Broadcast Service (GBS) provides a one-way communications path to the warfighter as part of an interactive communications system. GBS provides the capability to deliver information products of varying size, timeliness requirements, and security levels. The products share satellite resources based on CINC/JTF commanders priorities, operational locations, availability of the satellite resources, and platform capabilities of the deployed users. Currently, IP multicast (IPMC) is not supported by the Defense Information Infrastructure (DII). This document defines a real-time IP solution for tunneling multicast data across the Defense Information Services Network (DISN), through GBS components, and to the end-user LANs. This paper describes a cost-effective, operationally sound, incremental approach for rapid prototyping and integration of these technologies. It outlines each of the implementation approaches researched to date, the designs selected for systematic testing, and an overview of the plan for testing selected designs. The initial architecture is based on commercial-off-the-shelf IP routers, ATM switches, multicast software, and VPN techniques.",,0-7803-6521,10.1109/MILCOM.2000.904019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=904019,,Tunneling;Satellite broadcasting;Multicast protocols;System testing;Web and internet services;Communication system security;Information security;Availability;Software prototyping;Prototypes,transport protocols;LAN interconnection;information networks;military communication;telecommunication network routing;multicast communication;asynchronous transfer mode;packet switching;interactive systems;broadcasting;direct broadcasting by satellite;military computing,GBS IP multicast tunneling;DoD;Global Broadcast Service;one-way communications path;interactive communications system;information products;security levels;satellite resource sharing;operational locations;satellite resource availability;Defense Information Infrastructure;DII;real-time IP solution;multicast data tunneling;Defense Information Services Network;DISN;LAN;rapid prototyping;systematic testing;commercial-off-the-shelf IP routers;ATM switches;multicast software;VPN techniques,,,3,,,,,,IEEE,IEEE Conferences
Generating test cases for GUI responsibilities using complete interaction sequences,L. White; H. Almezen,"Dept. of Electr. Eng. & Comput. Sci., Case Western Reserve Univ., Cleveland, OH, USA; NA",Proceedings 11th International Symposium on Software Reliability Engineering. ISSRE 2000,,2000,,,110,121,"Testing graphical user interfaces (GUI) is a difficult problem due to the fact that the GUI possesses a large number of states to be tested, the input space is extremely large due to different permutations of inputs and events which affect the GUI, and complex GUI dependencies which may exist. There has been little systematic study of this problem yielding a resulting strategy which is effective and scalable. The proposed method concentrates upon user sequences of GUI objects and selections which collaborate, called complete interaction sequences (CIS), that produce a desired response for the user. A systematic method to test these CIS utilizes a finite-state model to generate tests. The required tests can be substantially reduced by identifying components of the CIS that can be tested separately. Since consideration is given to defects totally within each CIS, and the components reduce required testing further, this approach is scalable. An empirical investigation of this method shows that substantial reduction in tests can still detect the defects in the GUI. Future research will prioritize testing related to the CIS testing for maximum benefit if testing time is limited.",1071-9458,0-7695-0807,10.1109/ISSRE.2000.885865,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885865,,Computer aided software engineering;Graphical user interfaces;Computational Intelligence Society;System testing;Collaboration;Application software;Logic;Explosions,program testing;graphical user interfaces;finite state machines,test case generation;GUI;complete interaction sequences;graphical user interfaces;finite-state model;software testing,,64,14,,,,,,IEEE,IEEE Conferences
The Digital Library for Earth System Education: implementing the DLESE Community Plan,M. R. Marlino; D. W. Fulker; G. Horton,"UCAR, Boulder, CO, USA; NA; NA",IGARSS 2000. IEEE 2000 International Geoscience and Remote Sensing Symposium. Taking the Pulse of the Planet: The Role of Remote Sensing in Managing the Environment. Proceedings (Cat. No.00CH37120),,2000,1,,46,48 vol.1,"Over the past year, geoscience educators, librarians and information technologists have made substantial progress in initiating the construction ofa Digital Library for Earth System Education (DLESE). Two major efforts, the Portal to the Future Workshop and the Geoscience Digital Library (GDL) project, have established a vision for the library, a governance process to enable community ownership, management, and construction, and have begun development of a testbed collection, discovery system, and user interface. The DLESE Community Plan lays out in detail the need for this facility, a community vision for its goals and priorities, and a strategy for initial construction of the library. From this initial work, two conclusions emerge as paramount in moving forward with the library. First, it is essential that development of the library community and the building of the technological infrastructure for the library go hand in hand. Second, the library will be most effectively built as a highly coordinated, but distributed community effort. In this way, the full range of talents in the community can be leveraged and rapid development of the library is possible. This paper briefly reviews our technical accomplishments to date and outlines their plans for further development. Progress and plans can be charted in the following three areas: 1. Community-centered design and use case development 2. Discovery system, metadata, and collection testbeds 3. System architecture and interoperability.",,0-7803-6359,10.1109/IGARSS.2000.860416,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=860416,,Software libraries;Earth;Process design;Prototypes;Usability;Geoscience;System testing;User interfaces;Design methodology;Inspection,education;educational technology;teaching;geology;remote sensing;terrain mapping;information science,education;teaching;Earth science;geology;Digital Library for Earth System Education;DLESE;Community Plan;geoscience;Portal to the Future Workshop;Geoscience Digital Library;terrain mapping;remote sensing;land surface;community-centered design;discovery system;metadata;collection testbed;system architecture;interoperability;information science,,,15,,,,,,IEEE,IEEE Conferences
A hybrid genetic algorithm for generating optimal synthetic aperture radar target servicing strategies,B. Jackson; J. Norgard,"Ball Aerosp. & Technol. Corp., Boulder, CO, USA; NA",2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542),,2001,2,,2/709,2/718 vol.2,"The purpose of this research was to develop a software tool for generating optimal target servicing strategies for imaging fixed ground targets with a spaceborne SAR. Given a list of targets and their corresponding geographic locations and relative priorities, this tool generates a target servicing strategy that maximizes the overall collection utility based on the number of targets successfully imaged weighted by their relative priorities. This tool is specifically designed to maximize sensor utility in the case of a target-rich environment. For small numbers of targets, a target servicing strategy is unnecessary, and the targets may be imaged in any order without paying any particular attention to geographic proximity or target priority. However, for large, geographically diverse target decks, the order in which targets are serviced is of great importance. The target servicing problem is shown to be of the class NP-hard, and thus cannot be solved to optimality in polynomial time. Therefore, global search techniques such as genetic algorithms are called for. A unique hybrid algorithm that combines genetic algorithms with simulated annealing has been developed to generate optimized target servicing strategies. The performance of this hybrid algorithm was compared against that of three different greedy algorithms in a series of 20 test cases. Preliminary results indicate consistent performance improvements over greedy algorithms for target-rich environments. Over the course of 20 trials, the hybrid optimizing algorithm produced weighted collection scores that were on average 10% higher than the best greedy algorithm.",,0-7803-6599,10.1109/AERO.2001.931250,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931250,,Genetic algorithms;Hybrid power systems;Synthetic aperture radar;Greedy algorithms;Spaceborne radar;Image sensors;Software algorithms;Space technology;Business;Springs,synthetic aperture radar;genetic algorithms;target tracking;simulated annealing;radar imaging,hybrid genetic algorithm;synthetic aperture radar;target servicing strategies;fixed ground targets;spaceborne SAR;geographic locations;overall collection utility;sensor utility;target-rich environment;geographic proximity;geographically diverse target decks;NP-hard;global search techniques;simulated annealing;greedy algorithms;target-rich environments;hybrid optimizing algorithm;weighted collection scores,,2,10,,,,,,IEEE,IEEE Conferences
A selective software testing method based on priorities assigned to functional modules,M. Hirayama; T. Yamamoto; J. Okayasu; O. Mizuno; T. Kikuno,"R&D Center Syst. Eng. Lab, Toshiba Corp., Japan; NA; NA; NA; NA",Proceedings Second Asia-Pacific Conference on Quality Software,,2001,,,259,267,"As software systems have been introduced to many advanced applications, the size of software systems increases so much. Simultaneously, the lifetime of software systems becomes very small and thus their development is required within a relatively short period. We propose a novel selective software testing method that aims to attain the requirement of short period development. The proposed method consists of 3 steps: assign priorities to functional modules (Step 1), derive a test specification (Step 2), and construct a test plan (Step 3) according to the priorities. In Step 1, for development of functional modules, we select both product and process properties to calculate priorities. Then, in Step 2, we generate detailed test items for each module according to its priority. Finally, in Step 3, we manage test resources including time and developer's skill to attain the requirement. As a result of experimental application, we can show the superiority of the proposed testing method compared to the conventional testing method.",,0-7695-1287,10.1109/APAQS.2001.990028,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=990028,,Software testing;System testing;Embedded software;Software systems;Fault detection;Application software;Programming;Research and development;Systems engineering and theory;Resource management,program testing;formal specification;resource allocation,selective software testing method;functional modules;priority assignment;software systems lifetime;short period development;test specification;test plan;process properties;product properties;test resources;design of testing specification,,2,11,,,,,,IEEE,IEEE Conferences
Air traffic control improvement using prioritized CSMA,D. C. Robinson,"Glenn Res. Center, NASA, Cleveland, OH, USA",2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542),,2001,3,,3/1359,3/1365 vol.3,"Version 7 simulations of the industry-standard network simulation software ""OPNET"" are presented of two applications of the Aeronautical Telecommunications Network (ATN), Controller Pilot Data Link Communications (CPDLC) and Automatic Dependent Surveillance-Broadcast mode (ADS-B), over VHF Data Link mode 2 (VDL-2). Communication is modeled for air traffic between just three cities. All aircraft are assumed to have the same equipage. The simulation involves Air Traffic Control (ATC) ground stations and 105 aircraft taking off, flying realistic free-flight trajectories, and landing in a 24-hr period. All communication is modeled as unreliable. Collision-less, prioritized carrier sense multiple access (CSMA) is successfully tested. The statistics presented include latency, queue length, and packet loss. This research may show that a communications system simpler than the currently accepted standard envisioned may not only suffice, but also surpass performance of the standard at a lower cost of deployment.",,0-7803-6599,10.1109/AERO.2001.931366,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931366,,Air traffic control;Multiaccess communication;Aircraft;Communication standards;Aerospace industry;Communication industry;Computer industry;Industrial control;Application software;Automatic control,carrier sense multiple access;air traffic control;aerospace simulation,air traffic control;OPNET Version 7 software;computer simulation;Aeronautical Telecommunications Network;Controller Pilot Data Link Communications;Automatic Dependent Surveillance-Broadcast mode;VHF Data Link mode 2;collisionless prioritized CSMA;latency;queue length;packet loss;communication network,,1,3,,,,,,IEEE,IEEE Conferences
Applying Moore's technology adoption life cycle model to quality of EDA software,G. Ben-Yaacov; E. P. Stone; R. Goldman,"Synopsys Inc., USA; NA; NA",Proceedings of the IEEE 2001. 2nd International Symposium on Quality Electronic Design,,2001,,,76,80,"This paper describes a methodology for allocating priority levels and resources to quality activities during the development of EDA software projects. Geoffrey Moore's technology adoption life cycle model is used to provide a baseline understanding of what the market and the target users require at any point in time during the product life cycle. Applying this model, EDA software development teams can make choices and prioritize quality objectives which are based on the customer segment that they are targeting at any point in time during the product life cycle.",,0-7695-1025,10.1109/ISQED.2001.915209,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915209,,Electronic design automation and methodology;Software quality;Time to market;Product development;Software tools;Software maintenance;Customer satisfaction;Resource management;Software testing;Filling,electronic design automation;software quality,Moore's technology adoption life cycle model;EDA software quality;priority level allocation;quality activities;EDA software projects;software development teams;product life cycle,,1,,,,,,,IEEE,IEEE Conferences
DDP-a tool for life-cycle risk management,S. L. Cornford; M. S. Feather; K. A. Hicks,"Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; NA; NA",2001 IEEE Aerospace Conference Proceedings (Cat. No.01TH8542),,2001,1,,1/441,1/451 vol.1,"At JPL we have developed, and implemented, a process for achieving life-cycle risk management. This process has been embodied in a software tool and is called Defect Detection and Prevention (DDP). The DDP process can be succinctly stated as: determine where we want to be, what could get in the way and how we will get there. The 'determine where we want to be' is captured as trees of requirements and the 'what could get in the way' is captured as trees of potential failure modes. Scoring the impacts of these failure modes on the requirements results in a prioritized set of failure modes. The user then selects from a set of PACTs (Preventative measures, Analyses, process Controls and Tests) each of which has an effectiveness versus the various failure modes. It is the goal of the DDP process to optimally select the subset of the PACTs/spl Dagger/ which minimizes the residual risk subject to the project resource constraints.",,0-7803-6599,10.1109/AERO.2001.931736,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=931736,,Risk management;Software tools;Propulsion;Laboratories;Application software;Testing;Tree graphs;Feathers;Quality assurance;Failure analysis,life cycle costing;risk management;failure analysis;project management;software tools,DDP;life-cycle risk management;JPL;software tool;defect detection and prevention;trees;failure modes;PACTs;residual risk;project resource constraints,,27,8,,,,,,IEEE,IEEE Conferences
Failure modes and effects analysis for software reliability,Dong Nguyen,"Sextant In-Flight Syst., Thomson-CSF, Irvine, CA, USA",Annual Reliability and Maintainability Symposium. 2001 Proceedings. International Symposium on Product Quality and Integrity (Cat. No.01CH37179),,2001,,,219,222,"This paper presents a systematic problem solving approach, which is based on the failure modes and effects analysis (FMEA), to system software reliability. This approach will practically: (a) ensure that all of conceivable failure modes and their effects on operational success of the software system have been considered; (b) list potential failures, and identify the magnitude of their effects; (c) develop criteria for test planning, design of the tests, and checkout systems (e.g., logging mechanism); (d) provide a basis for quantitative reliability and availability analysis; and (e) provide a basis for establishing corrective action priorities. This approach was created for software reliability analysis and testing in the multimedia digital distribution system (MDDS) at Thomson-CSF Sextant In-Flight Systems. First it was used to improve the software reliability for the ISDN Communication Control Unit (CCU) subsystem of the MDDS, and then globally applied to the software reliability analysis MDDS and improvement for the whole MDDS. It has been proven to be an effective and efficient approach to system software reliability.",0149-144X,0-7803-6615,10.1109/RAMS.2001.902470,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=902470,,Failure analysis;Software reliability;Software testing;System testing;System software;Problem-solving;Software systems;Availability;Multimedia systems;ISDN,software reliability;failure analysis;multimedia communication;digital communication,software reliability;failure modes and effects analysis;system software reliability;failure modes;operational success;software system;potential failures listing;test planning;design of the tests;checkout systems;logging mechanism;quantitative reliability analysis;quantitative availability analysis;corrective action priorities;software reliability analysis;multimedia digital distribution system;Thomson-CSF Sextant In-Flight Systems;ISDN Communication Control Unit,,3,2,,,,,,IEEE,IEEE Conferences
Incorporating varying test costs and fault severities into test case prioritization,S. Elbaum; A. Malishevsky; G. Rothermel,"Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; NA; NA",Proceedings of the 23rd International Conference on Software Engineering. ICSE 2001,,2001,,,329,338,"Test case prioritization techniques schedule test cases for regression testing in an order that increases their ability to meet some performance goal. One performance goal, rate of fault detection, measures how quickly faults are detected within the testing process. In previous work (S. Elbaum et al., 2000; G. Rothermel et al., 1999), we provided a metric, APFD, for measuring rate of fault detection, and techniques for prioritizing test cases to improve APFD, and reported the results of experiments using those techniques. This metric and these techniques, however, applied only in cases in which test costs and fault severity are uniform. We present a new metric for assessing the rate of fault detection of prioritized test cases that incorporates varying test case and fault costs. We present the results of a case study illustrating the application of the metric. This study raises several practical questions that might arise in applying test case prioritization; we discuss how practitioners could go about answering these questions.",0270-5257,0-7695-1050,10.1109/ICSE.2001.919106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=919106,,Costs;Computer aided software engineering;Fault detection;Software testing;Job shop scheduling;Computer science;System testing;Processor scheduling;Application software;Frequency,program testing;software metrics;software cost estimation,varying test costs;fault severities;test case prioritization;regression testing;performance goal;fault detection rate;testing process;APFD;fault severity;prioritized test cases;varying test case costs;fault costs;case study,,98,13,,,,,,IEEE,IEEE Conferences
"Integrated reliability analysis, diagnostics and prognostics for critical power systems",F. Tu; M. S. Azam; Y. Shlapak; K. Pattipati; R. Karanam; S. Amin,"Dept. of Electr. & Comput. Eng., Connecticut Univ., Storrs, CT, USA; NA; NA; NA; NA; NA",2001 IEEE Autotestcon Proceedings. IEEE Systems Readiness Technology Conference. (Cat. No.01CH37237),,2001,,,416,440,"Critical power systems, such as data centers and communication switching facilities, have very high availability requirements (5 min./year downtime). A data center that consumes electricity at a rate of 3 MW can have a downtime cost of $300,000 an hour. Even a momentary interruption of two seconds may cause a loss of two hours of data processing. Consequently, power quality has emerged as an issue of significant importance in the operation of these systems. In this paper, we address three issues of power quality: real-time detection and diagnosis of power quality problems, reliability and availability evaluation, and capacity margin analysis. The objective of real-time detection and diagnosis is to provide a seamless on-line monitoring and off-line maintenance process. The techniques are being applied to monitor the power quality of a few facilities at the University of Connecticut. Reliability analysis, based on a computationally efficient sum of disjoint products, enables analysts to decide on the optimum levels of redundancy, aids operators in prioritizing the maintenance options within a given budget, and in monitoring the system for capacity margin. Capacity margin analysis helps operators to plan for additional loads and to schedule repair/replacement activities. The resulting analytical and software tool is demonstrated on a sample data center.",1080-7225,0-7803-7094,10.1109/AUTEST.2001.949035,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=949035,,Power system reliability;Power quality;Monitoring;Availability;Maintenance;Power system analysis computing;Communication switching;Energy consumption;Costs;Data processing,power system reliability;maintenance engineering;condition monitoring;scheduling;redundancy;power system measurement;power system analysis computing;software tools;computerised monitoring,integrated reliability analysis;diagnostics;prognostics;integrated reliability analysis/diagnostics/prognostics;critical power systems;data centers;communication switching facilities;availability requirements;downtime;data center;electricity consumption;downtime cost;momentary interruption;data processing loss;power quality;system operation;real-time detection;real-time diagnosis;power quality problems;reliability evaluation;availability evaluation;capacity margin analysis;on-line monitoring;off-line maintenance process;reliability analysis;computationally efficient disjoint product sum;redundancy;maintenance option priorities;maintenance budget;capacity margin;monitoring;repair/replacement scheduling;software tool,,,13,,,,,,IEEE,IEEE Conferences
"Managing the maintenance of ported, outsourced, and legacy software via orthogonal defect classification",K. Bassin; P. Santhanam,"Center for Software Eng., IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA",Proceedings IEEE International Conference on Software Maintenance. ICSM 2001,,2001,,,726,734,"From the perspective of maintenance, software systems that include COTS software, legacy, ported or outsourced code pose a major challenge. The dynamics of enhancing or adapting a product to address evolving customer usage and the inadequate documentation of these changes over a period of time (and several generations) are just two of the factors which may have a debilitating effect on the maintenance effort. While many approaches and solutions have been offered to address the underlying problems, few offer methods which directly affect a team's ability to quickly identify and prioritize actions targeting the product which is already in front of them. The paper describes a method to analyze the information contained in the form of defect data and arrive at technical actions to address explicit product and process weaknesses which can be feasibly addressed in the current effort. The defects are classified using Orthogonal Defect Classification (ODC) and actual case studies are used to illustrate the key points.",1063-6773,0-7695-1189,10.1109/ICSM.2001.972791,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972791,,Software maintenance;Costs;Electrical capacitance tomography;Engineering management;Software engineering;Inspection;Software testing;Stability;Measurement standards;Standards development,software maintenance;outsourcing;program testing;software development management,legacy software maintenance;orthogonal defect classification;software systems;COTS software;ported code;outsourced code;evolving customer usage;documentation;maintenance effort;defect data;technical actions;explicit product;Orthogonal Defect Classification;case studies;software evaluation;ODC,,4,9,,,,,,IEEE,IEEE Conferences
Prioritizing test cases for regression testing,G. Rothermel; R. H. Untch; Chengyun Chu; M. J. Harrold,"Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA; NA; NA",IEEE Transactions on Software Engineering,,2001,27,10,929,948,"Test case prioritization techniques schedule test cases for execution in an order that attempts to increase their effectiveness at meeting some performance goal. Various goals are possible; one involves rate of fault detection, a measure of how quickly faults are detected within the testing process. An improved rate of fault detection during testing can provide faster feedback on the system under test and let software engineers begin correcting faults earlier than might otherwise be possible. One application of prioritization techniques involves regression testing, the retesting of software following modifications; in this context, prioritization techniques can take advantage of information gathered about the previous execution of test cases to obtain test case orderings. We describe several techniques for using test execution information to prioritize test cases for regression testing, including: 1) techniques that order test cases based on their total coverage of code components; 2) techniques that order test cases based on their coverage of code components not previously covered; and 3) techniques that order test cases based on their estimated ability to reveal faults in the code components that they cover. We report the results of several experiments in which we applied these techniques to various test suites for various programs and measured the rates of fault detection achieved by the prioritized test suites, comparing those rates to the rates achieved by untreated, randomly ordered, and optimally ordered suites.",0098-5589;1939-3520;2326-3881,,10.1109/32.962562,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962562,,Computer aided software engineering;Software testing;Fault detection;Costs;Computer Society;System testing;Software maintenance;Processor scheduling;Feedback;Application software,program testing;program debugging,test case prioritization;regression testing;software testing;test case scheduling;software fault detection rate;software fault correction;code component coverage;experiments;cost-benefit analysis,,513,38,,,,,,IEEE,IEEE Journals & Magazines
Providing absolute differentiated services for real-time applications in static-priority scheduling networks,Shengquan Wang; Dong Xuan; Riccardo Bettati; Wei Zhao,"Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA; NA; NA; NA",Proceedings IEEE INFOCOM 2001. Conference on Computer Communications. Twentieth Annual Joint Conference of the IEEE Computer and Communications Society (Cat. No.01CH37213),,2001,2,,669,678 vol.2,"We propose and analyze a methodology for providing absolute differentiated services for real-time applications in networks that use static-priority schedulers. We extend previous work on worst-case delay analysis and develop a method that can be used to derive delay bounds without specific information on flow population. With this new method, we are able to successfully employ a utilization-based admission control approach for flow admission. This approach does not require explicit delay computation at admission time and hence is scalable to large systems. We assume the underlying network to use static-priority schedulers. We design and analyze several priority assignment algorithms, and investigate their ability to achieve higher utilization bounds. Traditionally, schedulers in differentiated services networks assign priorities on a class-by-class basis, with the same priority for each class on each router. We show that relaxing this requirement, that is, allowing different routers to assign different priorities to classes, achieves significantly higher utilization bounds.",0743-166X,0-7803-7016,10.1109/INFCOM.2001.916255,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=916255,,Admission control;Job shop scheduling;Diffserv networks;Delay effects;Processor scheduling;Algorithm design and analysis;Bandwidth;Application software;System testing;Intelligent networks,Internet;telecommunication congestion control;delays;telecommunication network routing;quality of service;telecommunication traffic;performance evaluation,absolute differentiated services;real-time applications;static-priority scheduling networks;worst-case delay analysis;delay bounds;utilization-based admission control;flow admission;static-priority schedulers;priority assignment algorithms;utilization bounds;differentiated services networks;router;Internet;heuristic algorithms;QoS architecture;performance evaluation;traffic model,,15,23,,,,,,IEEE,IEEE Conferences
Test-suite reduction and prioritization for modified condition/decision coverage,J. A. Jones; M. J. Harrold,"Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; NA",Proceedings IEEE International Conference on Software Maintenance. ICSM 2001,,2001,,,92,101,"Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique, and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows, and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software, according to some criterion, as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. The paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of a case study of the test-suite reduction algorithm.",1063-6773,0-7695-1189,10.1109/ICSM.2001.972715,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=972715,,Software testing;Costs;Software algorithms;System testing;Software maintenance;Educational institutions;Safety;Electronic switching systems;FAA;Performance evaluation,program testing;program verification;software prototyping,test-suite reduction;modified condition/decision coverage;software testing;high-assurance software development;commercial airborne systems;Federal Aviation Administration;verification technique;safety faults;test cases;regression testing;test-suite size problem;test-suite prioritization algorithms;prioritization techniques;MC/DC adequate test suites;case study,,25,13,,,,,,IEEE,IEEE Conferences
The importance of quality requirements in software platform development-a survey,E. Johansson; A. Wesslen; L. Bratthall; M. Host,"Ericsson Mobile Commun., Lund, Sweden; Ericsson Mobile Commun., Lund, Sweden; NA; NA",Proceedings of the 34th Annual Hawaii International Conference on System Sciences,,2001,,,10 pp.,,"This paper presents a survey where some quality requirements that commonly affect software architecture have been prioritized with respect to cost and lead-time impact when developing software platforms and when using them. Software platforms are the basis for a product-line, i.e. a collection of functionality that a number of products is based on. The survey has been carried out in two large software development organizations using 34 senior participants. The prioritization was carried out using the Incomplete Pairwise Comparison method (IPC). The analysis shows that there are large differences between the importance of the quality requirements studied. The differences between the views of different stakeholders are also analysed and it is found to be less than the difference between the quality requirements. Yet this is identified as a potential source of negative impact on product development cost and lead-time, and rules of thumb for reducing the impact are given.",,0-7695-0981,10.1109/HICSS.2001.927252,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=927252,,Software quality;Communication system software;Costs;Mobile communication;Application software;Data analysis;Product development;Testing;Electrical capacitance tomography;Internet,software architecture;software quality;software cost estimation,software quality requirements;software platform development;survey;software architecture;product development cost;lead-time impact;product-line;software development organizations;Incomplete Pairwise Comparison method;software development cost,,15,28,,,,,,IEEE,IEEE Conferences
Understanding and measuring the sources of variation in the prioritization of regression test suites,S. Elbaum; D. Gable; G. Rothermel,"Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; NA; NA",Proceedings Seventh International Software Metrics Symposium,,2001,,,169,179,"Test case prioritization techniques let testers order their test cases so that those with higher priority, according to some criterion, are executed earlier than those with lower priority. In previous work (1999, 2000), we examined a variety of prioritization techniques to determine their ability to improve the rate of fault detection of test suites. Our studies showed that the rate of fault detection of test suites could be significantly improved by using more powerful prioritization techniques. In addition, they indicated that rate of fault detection was closely associated with the target program. We also observed a large quantity of unexplained variance, indicating that other factors must be affecting prioritization effectiveness. These observations motivate the following research questions. (1) Are there factors other than the target program and the prioritization technique that consistently affect the rate of fault detection of test suites? (2) What metrics are most representative of each factor? (3) Can the consideration of additional factors lead to more efficient prioritization techniques? To address these questions, we performed a series of experiments exploring three factors: program structure, test suite composition and change characteristics. This paper reports the results and implications of those experiments.",1530-1435,0-7695-1043,10.1109/METRIC.2001.915525,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=915525,,Fault detection;Performance evaluation;Feedback;System testing;Debugging;Data analysis,program testing;software metrics;statistical analysis,regression test suites;test case prioritization techniques;variation sources;fault detection rate;software metrics;software testing;program structure;test suite composition;change characteristics,,8,20,,,,,,IEEE,IEEE Conferences
Yemanja-a layered event correlation engine for multi-domain server farms,K. Appleby; G. Goldszmidt; M. Steinder,"IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA; NA; NA",2001 IEEE/IFIP International Symposium on Integrated Network Management Proceedings. Integrated Network Management VII. Integrated Management Strategies for the New Millennium (Cat. No.01EX470),,2001,,,329,344,"Yemanja is a model-based event correlation engine for multi-layer fault diagnosis. It targets complex propagating fault scenarios, and can smoothly correlate low-level network events with high-level application performance alerts related to quality of service violations. Entity-models that represent devices or abstract components encapsulate entity behavior. Distantly associated entities are not explicitly aware of each other, and communicate through event propagation chains. Yemanja's state-based engine supports generic scenario definitions, prioritization of alternate solutions, integrated problem-state and device testing, and simultaneous analysis of overlapping problems. The system of correlation rules was developed based on device, layer, and dependency analysis, and reveals the layered structure of computer networks. The primary objectives of this research include the development of reusable, configuration independent, correlation scenarios; adaptability and the extensibility of the engine to match the constantly changing topology of a multi-domain server farm; and the development of a concise specification language that is relatively simple yet powerful.",,0-7803-6719,10.1109/INM.2001.918051,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=918051,,Engines;Fault diagnosis;Testing;Network servers;Resource management;Milling machines;Rivers;Application software;Quality of service;Independent component analysis,quality of service;computer network management;network servers;correlation methods;computer network reliability;specification languages,Yemanja;layered event correlation engine;multi-domain server farms;multi-layer fault diagnosis;complex propagating fault scenarios;low-level network events;high-level application performance alerts;quality of service violations;entity-models;abstract components;entity behavior;event propagation chains;state-based engine;generic scenario definitions;prioritization;problem-state testing;device testing;overlapping problem;correlation rules;dependency analysis;layered structure;computer networks;adaptability;extensibility;topology;specification language,,16,,,,,,,IEEE,IEEE Conferences
A history-based test prioritization technique for regression testing in resource constrained environments,Jung-Min Kim; A. Porter,"Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA",Proceedings of the 24th International Conference on Software Engineering. ICSE 2002,,2002,,,119,129,"Regression testing is an expensive and frequently executed maintenance process used to revalidate modified software. To improve it, regression test selection (RTS) techniques strive to lower costs without overly reducing effectiveness by carefully selecting a subset of the test suite. Under certain conditions, some can even guarantee that the selected test cases perform no worse than the original test suite. This ignores certain software development realities such as resource and time constraints that may prevent using RTS techniques as intended (e.g., regression testing must be done overnight, but RTS selection returns two days worth of tests). In practice, testers work around this by prioritizing the test cases and running only those that fit within existing constraints. Unfortunately this generally violates key RTS assumptions, voiding RTS technique guarantees and making regression testing performance unpredictable. Despite this, existing prioritization techniques are memoryless, implicitly assuming that local choices can ensure adequate long run performance. Instead, we propose a new technique that bases prioritization on historical execution data. We conducted an experiment to assess its effects on the long run performance of resource constrained regression testing. Our results expose essential tradeoffs that should be considered when using these techniques over a series of software releases.",,1-58113-472,10.1109/ICSE.2002.1007961,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1007961,,Software testing;Time factors;Educational institutions;Permission;System testing;Computer science;Software maintenance;Costs;Performance evaluation;Programming,program testing;software maintenance;program verification,history-based test prioritization;regression testing;resource constrained environments;software maintenance;modified software revalidation;costs;performance;experiment;software releases;program testing,,28,22,,,,,,IEEE,IEEE Conferences
"Analysis of preemptive periodic real-time systems using the (max, plus) algebra with applications in robotics",F. Baccelli; B. Gaujal; D. Simon,"ENS, Paris, France; NA; NA",IEEE Transactions on Control Systems Technology,,2002,10,3,368,380,"We present the model of a system of periodic real-time tasks with fixed priorities, preemption and synchronization, performed by a robot controller, using marked graphs. Then, with the help of the (max, plus) algebra, we derive simple tests to check real-time constraints on those tasks such as response times and the respect of deadlines. This method takes into account the precedence and synchronization constraints and is not limited to a particular scheduling policy.",1063-6536;1558-0865;2374-0159,,10.1109/87.998024,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=998024,,Real time systems;Algebra;Delay;Synchronization;Robot control;Computational complexity;Performance analysis;Testing;Clocks;Time measurement,robots;synchronisation;periodic control;real-time systems;control system analysis computing;graph theory,fixed priority preemption;marked graphs;max plus algebra;periodic systems;real-time systems;synchronization;robotics;ORCCAD software;response times,,6,19,,,,,,IEEE,IEEE Journals & Magazines
Dual purpose simulation: new data link test and comparison with VDL-2,D. C. Robinson,"NASA Glenn Res. Center, Cleveland, OH, USA",Proceedings. The 21st Digital Avionics Systems Conference,,2002,1,,3C6,3C6,"While the results of this paper are similar to those of previous research, in this paper the technical difficulties present previously are eliminated, producing better results, enabling one to more readily see the benefits of Prioritized CSMA (PCSMA). A new analysis section also helps to generalize this research so that it is not limited to exploration of the new concept of PCSMA. Commercially available network simulation software, OPNET version 7.0, simulations are presented involving an important application of the Aeronautical Telecommunications Network (ATN), Controller Pilot Data Link Communications (CPDLC) over the Very High Frequency Data Link Mode 2 (VDL-2). Communication is modeled for essentially all incoming and outgoing nonstop air-traffic for just three United States cities: Cleveland, Cincinnati, and Detroit. Collision-less PCSMA is successfully tested and compared with the traditional CSMA typically associated with VDL-2. The performance measures include latency, throughput, and packet loss. As expected, PCSMA is much quicker and more efficient than traditional CSMA. These simulation results show the potency of PCSMA for implementing low latency, high throughput and efficient connectivity. We are also testing a new and better data link that could replace CSMA with relative ease.",,0-7803-7367,10.1109/DASC.2002.1067949,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1067949,,Testing;Multiaccess communication;Delay;Throughput;Application software;Communication system control;Telecommunication control;Frequency;Cities and towns;Performance loss,carrier sense multiple access;aircraft communication;air traffic control;digital simulation;data communication;digital radio;telecommunication computing,prioritized CSMA;collision-less PCSMA;network simulation software;OPNET version 7.0;Aeronautical Telecommunications Network;ATN;controller-pilot data link communications;CPDLC;latency;throughput;packet loss;performance measures;retransmission analysis;very high frequency data link;VHF data link Mode 2;VDL-2;communication modelling;ATC ground stations;air traffic control,,,7,,,,,,IEEE,IEEE Conferences
Elimination of crucial faults by a new selective testing method,M. Hirayama; T. Yamamoto; J. Okayasu; O. Mizuno; T. Kikuno,"R&D Center, Toshiba Corp., Japan; R&D Center, Toshiba Corp., Japan; R&D Center, Toshiba Corp., Japan; NA; NA",Proceedings International Symposium on Empirical Software Engineering,,2002,,,183,191,"Recent software systems contain a lot of functions to provide various services. According to this tendency, software testing becomes more difficult than before and cost of testing increases so much, since many test items are required. In this paper we propose and discuss such a new selective software testing method that is constructed from previous testing method by simplifying testing specification. We have presented, in the previous work, a selective testing method to perform highly efficient software testing. The selective testing method has introduced an idea of functional priority testing and generated test items according to their functional priorities. Important functions with high priorities are tested in detail, and functions with low priorities are tested less intensively. As a result, additional cost for generating testing instructions becomes relatively high. In this paper in order to reduce its cost, we change the way of giving information, with respect to priorities. The new method gives the priority only rather than generating testing instructions to each test item, which makes the testing method quite simple and results in cost reduction. Except for this change, the new method is essentially the same as the previous method. We applied this new method to actual development of software tool and evaluated its effectiveness. From the result of the application experiment, we confirmed that many crucial faults can be detected by using the proposed method.",,0-7695-1796,10.1109/ISESE.2002.1166937,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166937,,Software testing;Costs;Performance evaluation;System testing;Application software;Software quality;Research and development;Systems engineering and theory;Information science;Fault detection,program testing;software fault tolerance,crucial faults elimination;selective testing method;software systems;software testing;testing specification;functional priority testing,,1,15,,,,,,IEEE,IEEE Conferences
Extreme programming modified: embrace requirements engineering practices,J. Nawrocki; M. Jasinski; B. Walter; A. Wojciechowski,"Poznan Univ. of Technol., Poland; Poznan Univ. of Technol., Poland; Poznan Univ. of Technol., Poland; Poznan Univ. of Technol., Poland",Proceedings IEEE Joint International Conference on Requirements Engineering,,2002,,,303,310,"Extreme programming (XP) is an agile (lightweight) software development methodology and it becomes more and more popular. XP proposes many interesting practices, but it also has some weaknesses. From the software engineering point of view the most important issues are: maintenance problems resulting from very limited documentation (XP relies on code and test cases only), and lack of wider perspective of a system to be built. Moreover, XP assumes that there is only one customer representative. In many cases there are several representatives (each one with his own view of the system and different priorities) and then some XP practices should be modified. In the paper we assess XP from two points of view: the capability maturity model and the Sommerville-Sawyer model (1997). We also propose how to introduce documented requirements to XP, how to modify the planning game to allow many customer representatives and how to get a wider perspective of a system to be built at the beginning of the project lifecycle.",1090-705X,0-7695-1465,10.1109/ICRE.2002.1048543,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1048543,,Capability maturity model;Documentation;Programming profession;Software engineering;Software testing;Software maintenance;Oral communication;Automatic testing;System testing;Electronic mail,formal specification;software engineering,extreme programming;XP;requirements engineering practices;agile software development methodology;software engineering;software maintenance;documentation;capability maturity model;Sommerville-Sawyer model;planning game,,10,,,,,,,IEEE,IEEE Conferences
How much information is needed for usage-based reading? A series of experiments,T. Thelin; P. Runeson; C. Wohlin; T. Olsson; C. Andersson,"Dept. of Commun. Syst., Lund Univ., Sweden; Dept. of Commun. Syst., Lund Univ., Sweden; NA; NA; NA",Proceedings International Symposium on Empirical Software Engineering,,2002,,,127,138,"Software inspections are regarded as an important technique to detect faults throughout the software development process. The individual preparation phase of software inspections has enlarged its focus from only comprehension to also include fault searching. Hence, reading techniques to support the reviewers on fault detection are needed. Usage-based reading (UBR) is a reading technique, which focuses on the important parts of a software document by using prioritized use cases. This paper presents a series of three UBR experiments on design specifications, with focus on the third. The first experiment evaluates the prioritization of UBR and the second compares UBR against checklist-based reading. The third experiment investigates the amount of information needed in the use cases and whether a more active approach helps the reviewers to detect more faults. The third study was conducted at two different places with a total of 82 subjects. The general result from the experiments is that UBR works as intended and is efficient as well as effective in guiding reviewers during the preparation phase of software inspections. Furthermore, the results indicate that use cases developed in advance are preferable compared to developing them as part of the preparation phase of the inspection.",,0-7695-1796,10.1109/ISESE.2002.1166932,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166932,,Inspection;Software engineering;Bioreactors;Communication system software;Computer science;Technology planning;Fault detection;Testing;Engineering management,software engineering;inspection;program debugging,usage-based reading;experiments;software inspections;software development fault detection;software document;prioritized use cases;design specifications;checklist-based reading,,8,23,,,,,,IEEE,IEEE Conferences
ICT in Japanese university language education: a case study,M. H. Field,"Waseda Univ., Tokyo, Japan","International Conference on Computers in Education, 2002. Proceedings.",,2002,,,929,933 vol.2,This paper reports part of a study conducted at a university in Japan. The belief that ICT provides students with more opportunities to negotiate target forms has been used to justify its use in language education. This study could not substantiate that ICT will change the way language is used in Japan or that CALL is providing new ways for learning and acquiring a new language. Students were experiencing difficulties in prioritising their learning repertoire between the acquisition of computer skills and language proficiency. Individuals experienced positive and negative coding and de-coding filters when communicating in ICT and this was related to the validity and reliability applied to the text. The value attached to the ICT interaction may influence the degree to which the ICT event influences face-to-face communicative acts.,,0-7695-1509,10.1109/CIE.2002.1186116,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1186116,,Natural languages;Computer aided software engineering;Computer science education;Communications technology;Cultural differences;Filters;Communication system control;Boring;Materials testing;Drilling,natural languages;computer aided instruction,Japanese university language education;ICT;Japan;computer aided language learning;learning repertoire;computer skills acquisition;language proficiency;negative de-coding filters;negative coding filters;reliability,,1,18,,,,,,IEEE,IEEE Conferences
Maintaining software with a security perspective,K. Jiwnani; M. Zelkowitz,"Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA; Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA","International Conference on Software Maintenance, 2002. Proceedings.",,2002,,,194,203,"Testing for software security is a lengthy, complex and costly process. Currently, security testing is done using penetration analysis and formal verification of security kernels. These methods are not complete and are difficult to use. Hence it is essential to focus testing effort in areas that have a greater number of security vulnerabilities to develop secure software as well as meet budget and time constraints. We propose a testing strategy based on a classification of vulnerabilities to develop secure and stable systems. This taxonomy will enable a system testing and maintenance group to understand the distribution of security vulnerabilities and prioritize their testing effort according to the impact the vulnerabilities have on the system. This is based on Landwehr's (1994) classification scheme for security flaws and we evaluated it using a database of 1360 operating system vulnerabilities. This analysis indicates vulnerabilities tend to be focused in relatively few areas and associated with a small number of software engineering issues.",1063-6773,0-7695-1819,10.1109/ICSM.2002.1167766,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167766,,Software maintenance;System testing;Software testing;Data security;Formal verification;Kernel;Time factors;Taxonomy;Databases;Operating systems,security of data;program testing;software maintenance;operating systems (computers),software security testing;budget constraints;time constraints;vulnerability classification;stable systems;software maintenance;security flaw classification scheme;operating system vulnerabilities;software engineering,,17,25,,,,,,IEEE,IEEE Conferences
Modeling the cost-benefits tradeoffs for regression testing techniques,A. G. Malishevsky; G. Rothermel; S. Elbaum,"Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; Dept. of Comput. Sci., Oregon State Univ., Corvallis, OR, USA; NA","International Conference on Software Maintenance, 2002. Proceedings.",,2002,,,204,213,"Regression testing is an expensive activity that can account for a large proportion of the software maintenance budget. Because engineers add tests into test suites as software evolves, over time, increased test suite size makes revalidation of the software more expensive. Regression test selection, test suite reduction, and test case prioritization techniques can help with this, by reducing the number of regression tests that must be run and by helping testers meet testing objectives more quickly. These techniques, however can be expensive to employ and may not reduce overall regression testing costs. Thus, practitioners and researchers could benefit from cost models that would help them assess the cost-benefits of techniques. Cost models have been proposed for this purpose, but some of these models omit important factors, and others cannot truly evaluate cost-effectiveness. In this paper, we present new cost-benefits models for regression test selection, test suite reduction, and test case prioritization, that capture previously omitted factors, and support cost-benefits analyses where they were not supported before. We present the results of an empirical study assessing these models.",1063-6773,0-7695-1819,10.1109/ICSM.2002.1167767,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1167767,,Software testing;Software maintenance;Fault detection;Computer science;Performance evaluation;Cost benefit analysis;Software performance;Maintenance engineering,cost-benefit analysis;software maintenance;program testing;software development management,cost-benefit tradeoff models;regression testing;software maintenance;regression test selection;regression test suite reduction;regression test case prioritization,,26,22,,,,,,IEEE,IEEE Conferences
Parameter optimization tool for enhancing on-chip network performance,J. Riihimaki; E. Salminen; K. Kuusilinna; T. Hamalainen,"Inst. of Digital & Comput. Syst., Tampere Univ. of Technol., Finland; Inst. of Digital & Comput. Syst., Tampere Univ. of Technol., Finland; Inst. of Digital & Comput. Syst., Tampere Univ. of Technol., Finland; Inst. of Digital & Comput. Syst., Tampere Univ. of Technol., Finland",2002 IEEE International Symposium on Circuits and Systems. Proceedings (Cat. No.02CH37353),,2002,4,,IV,IV,"In this paper, we present a tool to be used in the optimization of interconnection parameters in order to achieve optimal performance and implementation with minimal costs. The optimization tool uses an iterative algorithm to optimize the interconnection parameters, such as data width, priorities, and the time an agent can reserve the interconnection, to fulfill the given constraints. In the used test case, the required area decreased 50% while 85% of the original bandwidth was obtained. This was due to an improved arbitration process.",,0-7803-7448,10.1109/ISCAS.2002.1010388,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1010388,,Network-on-a-chip;System-on-a-chip;Optimization methods;Constraint optimization;Testing;Cost function;Iterative algorithms;Bandwidth;Digital systems;Complex networks,circuit optimisation;software tools;circuit CAD;iterative methods;integrated circuit design;integrated circuit interconnections;integrated circuit metallisation,parameter optimization;on-chip network performance enhancement;interconnection parameters;optimal performance;optimal implementation;minimal costs;iterative algorithm;data width;interconnection priorities;agent interconnection reservation time;interconnection constraints;interconnection test;interconnection bandwidth;arbitration process,,2,8,,,,,,IEEE,IEEE Conferences
Simulation of restaurant operations using the Restaurant Modeling Studio,D. M. Brann; B. C. Kulick,"Autom. Associates Inc., Solana Beach, CA, USA; Autom. Associates Inc., Solana Beach, CA, USA",Proceedings of the Winter Simulation Conference,,2002,2,,1448,1453 vol.2,"The operation of quick service restaurants (QSR) is a highly engineered process, with many factors coming into play: physical layout, equipment availability, and worker staffing levels, positioning, and priorities. The Restaurant Modeling Studio (RMS) provides an analysis platform for investigating the impacts of these factors on critical performance metrics, especially speed of service and service capacity. The key components of the RMS are a simulation engine built in Arena, and two custom applications built on Microsoft Visio - the Kitchen and Process Designers. The simulation engine supports a large number of behaviors, including parallel operations, inventory replenishment, prioritized task selection and many more. The Kitchen Designer and Process Designer provide the user with powerful tools for specifying the physical layout and order fulfillment processes. This paper presents the components of the RMS and its use in an analysis kitchen design comparison and labor deployment standards.",,0-7803-7614,10.1109/WSC.2002.1166417,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1166417,,Engines;Process design;Testing;Computer architecture;User interfaces;Application software;Marketing and sales;Analytical models;Automation;Availability,catering industry;digital simulation;personnel;CAD,quick service restaurants;physical layout;equipment availability;worker staffing levels;positioning;priorities;Restaurant Modeling Studio;performance metrics;service speed;service capacity;Arena;Microsoft Visio;Process Designer;Kitchen Designer;parallel operations;inventory replenishment;prioritized task selection;labor deployment standards,,2,,,,,,,IEEE,IEEE Conferences
Test case prioritization: a family of empirical studies,S. Elbaum; A. G. Malishevsky; G. Rothermel,"Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; NA; NA",IEEE Transactions on Software Engineering,,2002,28,2,159,182,"To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of such prioritization is to increase a test suite's rate of fault detection. Previous work reported results of studies that showed that prioritization techniques can significantly improve rate of fault detection. Those studies, however, raised several additional questions: 1) Can prioritization techniques be effective when targeted at specific modified versions; 2) what trade-offs exist between fine granularity and coarse granularity prioritization techniques; 3) can the incorporation of measures of fault proneness into prioritization techniques improve their effectiveness? To address these questions, we have performed several new studies in which we empirically compared prioritization techniques using both controlled experiments and case studies.",0098-5589;1939-3520;2326-3881,,10.1109/32.988497,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=988497,,Computer aided software engineering;Fault detection;Software testing;System testing;Costs;Software measurement;Radio access networks;Feedback;Debugging;Instruments,program testing,test case prioritization;regression testing;software testing;fault detection rate;fine granularity prioritization techniques;coarse granularity prioritization techniques;fault proneness measures,,364,35,,,,,,IEEE,IEEE Journals & Magazines
A comparison of coverage-based and distribution-based techniques for filtering and prioritizing test cases,D. Leon; A. Podgurski,"Electr. Eng. & Comput. Sci. Dept. Case, Western Reserve Univ., Cleveland, OH, USA; Electr. Eng. & Comput. Sci. Dept. Case, Western Reserve Univ., Cleveland, OH, USA","14th International Symposium on Software Reliability Engineering, 2003. ISSRE 2003.",,2003,,,442,453,"This paper presents an empirical comparison of four different techniques for filtering large test suites: test suite minimization, prioritization by additional coverage, cluster filtering with one-per-cluster sampling, and failure pursuit sampling. The first two techniques are based on selecting subsets that maximize code coverage as quickly as possible, while the latter two are based on analyzing the distribution of the tests' execution profiles. These techniques were compared with data sets obtained from three large subject programs: the GCC, Jikes, and javac compilers. The results indicate that distribution-based techniques can be as efficient or more efficient for revealing defects than coverage-based techniques, but that the two kinds of techniques are also complementary in the sense that they find different defects. Accordingly, some simple combinations of these techniques were evaluated for use in test case prioritization. The results indicate that these techniques can create more efficient prioritizations than those generated using prioritization by additional coverage.",1071-9458,0-7695-2007,10.1109/ISSRE.2003.1251065,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1251065,,Filtering;Computer aided software engineering;Costs;Automatic testing;Sampling methods;System testing;Software testing;Computer science;Java;Program processors,program testing;program compilers;Java;software engineering,coverage-based technique;distribution-based technique;test case prioritization;test case filtering;test suite minimization;cluster filtering;one-per-cluster sampling;failure pursuit sampling;subset selection;code coverage maximization;test execution profile distribution;GCC compiler;Jikes compiler;javac compiler;defect finding,,72,28,,,,,,IEEE,IEEE Conferences
A task space redundancy-based scheme for motion planning,Yixin Chen; J. E. McInroy,"Dept. of Comput. Sci. & Eng., Pennsylvania State Univ., University Park, PA, USA; NA","Proceedings of the 2003 American Control Conference, 2003.",,2003,4,,3435,3441 vol.4,"In many applications, the manipulations require only part of the degrees of freedom (DOFs) of the end-effector, or some DOFs are more important than the rest. We name these applications prioritized manipulations. The end-effectors DOFs are divided into those which are critical and must be controlled as precisely as possible, and those which have loose specifications, so their tracking performance can traded-off to achieve other needs. In this paper, we derive a formulation for partitioning the task space into major and secondary task directions and finding the velocity and static force mappings that precisely accomplish the major task and locally optimize some secondary goals. The techniques are tested on a 6-DOF parallel robot performing a 2-DOF tracking task.",0743-1619,0-7803-7896,10.1109/ACC.2003.1244063,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1244063,,Welding;Manipulators;Cameras;Application software;Redundancy;Trajectory;Computer science;Testing;Parallel robots;Performance evaluation,path planning;redundant manipulators;redundancy;end effectors;tracking;velocity;force,task space redundancy-based scheme;motion planning;degrees of freedom;end-effector DOF;prioritized manipulation;tracking performance;velocity;static force mapping;6-DOF parallel robot;2-DOF tracking task;secondary task direction,,,20,,,,,,IEEE,IEEE Conferences
"Generating, selecting and prioritizing test cases from specifications with tool support",Y. T. Yu; S. P. Ng; E. Y. K. Chan,"Dept. of Comput. Sci., City Univ. of Hong Kong, China; NA; NA","Third International Conference on Quality Software, 2003. Proceedings.",,2003,,,83,90,"The classification-tree method provides a systematic way for software testers to derive test cases by considering important relevant aspects that are identified from the specification. The method has been used in many real-life applications and shown to be effective. This paper presents several enhancements to the method by annotating the classification tree with additional information to reduce manual effort in the generation, selection and prioritization of test cases. A tool for supporting this enhanced process is also described.",,0-7695-2015,10.1109/QSIC.2003.1319089,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1319089,,Computer aided software engineering;Software testing;Classification tree analysis;System testing;Computer science;Software systems;Automatic testing;Life testing;Tree graphs;Information technology,program testing;formal specification,test case generation;test case selection;test case prioritization;classification-tree method;test case derivation;classification tree annotation;black box testing;partition testing;software testing;specification-based testing,,8,17,,,,,,IEEE,IEEE Conferences
Global implementation of ERP software - critical success factors on upgrading technical infrastructure,S. Ghosh,"IEEE, Columbia, MD, USA",IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change,,2003,,,320,324,"Implementing an Enterprise Resource planning (ERP) software in a global environment, executive sponsors face two key challenges. While business processes are to be re-engineered to align with the ERP software best practices, technical architecture and infrastructure needs to be in place globally as per specifications of the packaged software. In the legacy environment, different countries or different business units use different systems, based on local standards supported by local resources. In the new ERP world, globally all the countries must conform to same technical infrastructure. Technical managers face multiple critical issues implementing a global solution. Most of the ERP software are developed in technically advanced countries, standards are often too high for under developed or developing countries. In an effort to bring the global organization to a common platform different countries needs different levels of upgrades. In this paper the authors review key technical issues faced is a global upgrade process to support a global ERP implementation and how to resolve those. We conclude although technical infrastructure and business process reengineering both are equally important and each implementation is unique, but following some simple steps it is easy to prioritize each ones during different phases of the project. Also time lines of two sub-projects must converge after initial phase and must follow a common plan for the project to be successful. Multiple scenarios are described to facilitate the process.",,0-7803-8150,10.1109/IEMC.2003.1252285,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1252285,,Enterprise resource planning;Business process re-engineering;Project management;Internet;USA Councils;Computer architecture;Software packages;Packaging;Life testing;Environmental management,enterprise resource planning;business process re-engineering;software packages;globalisation;international trade;technology management,enterprise resource planning software;business process reengineering;software package;business managers;technology management;business processes life cycle;global data traffic;global business;global organization;global implementation;technical infrastructure;technical architecture;legacy environment;business units,,6,6,,,,,,IEEE,IEEE Conferences
Global multiprocessor scheduling of aperiodic tasks using time-independent priorities,L. Lundberg; H. Lennerstad,"Dept. of Software Eng. & Comput. Sci., Blekinge Inst. of Technol., Ronneby, Sweden; Dept. of Software Eng. & Comput. Sci., Blekinge Inst. of Technol., Ronneby, Sweden","The 9th IEEE Real-Time and Embedded Technology and Applications Symposium, 2003. Proceedings.",,2003,,,170,180,"We provide a constant time schedulability test for a multiprocessor server handling aperiodic tasks. Dhall's effect is avoided by dividing the tasks in two priority classes based on task utilization: heavy and light. We prove that if the load on the multiprocessor server stays below U/sub threshold/ = 3 - /spl radic/7 = 35.425%, the server can accept incoming aperiodic tasks and guarantee that the deadlines of all accepted tasks will be met. 35.425% utilization is also a threshold for a task to be characterized as heavy. The bound U/sub threshold/ = 3 - /spl radic/7 = 35.425% is easy-to-use, but not sharp if we know the number of processors in the multiprocessor. For a server with m processors, we calculate a formula for the sharp bound U/sub threshold/(m), which converges to Uthreshold from above as m - -. The results are based on a utilization function u/sub m/(x) = 2(1 - x)/(2 + /spl radic/(2 + 2x)) + x/m. By using this function, the performance of the multiprocessor can in some cases be improved beyond U/sub threshold/ (m) by paying the extra overhead of monitoring the individual utilization of the current tasks.",1545-3421,0-7695-1956,10.1109/RTTAS.2003.1203049,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1203049,,Processor scheduling;Delay;Admission control;Software engineering;Computer science;Software testing;Monitoring;Real time systems,processor scheduling;computational complexity;real-time systems,multiprocessor scheduling;time-independent priority;multiprocessor server,,5,7,,,,,,IEEE,IEEE Conferences
Market-based task allocation for dynamic processing environments,M. P. Wellman; S. -. Cheng,"Michigan Univ., Ann Arbor, MI, USA; Michigan Univ., Ann Arbor, MI, USA",IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502),,2003,,,109,114,"Flexible and large-scale information processing across enterprises entails dynamic and decentralized control of workflow through adaptive allocation of knowledge and processing resources. Markets comprise a well-understood class of mechanisms for decentralized resource allocation, where agents interacting through a price system direct resources toward their most valued uses as indicated by these prices. The information-processing domain presents several challenges for market-based approaches, including (1) representing knowledge-intensive tasks and capabilities, (2) propagating price signals across multiple levels of information processing, (3) handling dynamic task arrival and changing priorities, and (4) accommodating the increasing-returns and public-good characteristics of information products. A market gaming environment provides a methodology for testing alternative market structures and agent strategies, and evaluating proposed solutions in a realistic decentralized manner.",,0-7803-7958,10.1109/KIMAS.2003.1245031,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245031,,Information processing;Resource management;Distributed control;Centralized control;Communication system control;Aggregates;Optimal control;Programmable control;Adaptive control;Signal processing,task analysis;resource allocation;multi-agent systems;electronic commerce;software agents,market-based task allocation;dynamic processing environments;large-scale information processing;decentralized resource allocation;knowledge-intensive tasks;price signals propagation;information products;market gaming environment,,,23,,,,,,IEEE,IEEE Conferences
Prioritized use cases as a vehicle for software inspections,T. Thelin; P. Runeson; C. Wohlin,"Dept. of Commun. Syst., Lund Univ., Sweden; Dept. of Commun. Syst., Lund Univ., Sweden; NA",IEEE Software,,2003,20,4,30,33,"The usage-based reading technique combines traditional inspection principles, use cases, and operational profile testing to create an efficient user-oriented software inspection reading technique. UBR can find faults more effectively and efficiently than the traditional checklist-based method.",0740-7459;1937-4194,,10.1109/MS.2003.1207451,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1207451,,Computer aided software engineering;Inspection;Computer industry;Software testing;Vehicle dynamics;Software quality;Software design;Performance analysis;Industrial relations;Software engineering,program testing;program verification;software engineering,software engineering;object-oriented design;software testing;inspection;verification;UBR;usage-based reading,,18,13,,,,,,IEEE,IEEE Journals & Magazines
Putting your best tests forward,G. Rothermel; S. Elbaum,"Oregon State Univ., Corvallis, OR, USA; NA",IEEE Software,,2003,20,5,74,77,"Test case prioritization orders tests so that they help you meet your testing goals earlier during regression testing. Prioritization techniques can, for example, order tests to achieve coverage at the fastest rate possible, exercise features in order of expected frequency of use, or reveal faults as early as possible. We focus on the last goal, which we describe as ""increasing a test suite's rate of fault detection"" or the speed with which the test suite reveals faults. A faster fault detection rate during regression testing provides earlier feedback on a system under test, supporting earlier strategic decisions about release schedules and letting engineers begin debugging sooner. Also, if testing time is limited or unexpectedly reduced, prioritization increases the chance that testing resources will have been spent as cost effectively as possible in the available time.",0740-7459;1937-4194,,10.1109/MS.2003.1231157,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1231157,,Software testing;Costs;Fault detection;System testing;Feedback;History;Software quality;Irrigation;Frequency;Debugging,program testing;program debugging;software engineering,regression testing;software testing;test case prioritization;coverage;fault detection rate;release schedules;debugging,,12,10,,,,,,IEEE,IEEE Journals & Magazines
Smart debugging software architectural design in SDL,W. E. Wong; T. Sugeta; Yu Qi; J. C. Maldonado,"Dept. of Comput. Sci., Texas Univ., Richardson, TX, USA; Dept. of Comput. Sci., Texas Univ., Richardson, TX, USA; Dept. of Comput. Sci., Texas Univ., Richardson, TX, USA; NA",Proceedings 27th Annual International Computer Software and Applications Conference. COMPAC 2003,,2003,,,41,47,"Statistical data show that it is much less expensive to correct software bugs at the early design stage rather than the late stage of the development process when the final system has already been implemented and integrated together. The use of slicing and execution histories as an aid in software debugging is well established for programming languages like C and C++; however, it is rarely applied in the field of software design specification. We propose a solution by applying the source code level technologies to debugging software designs represented in a high-level specification and description language such as SDL. More specifically, we extend execution slice-based heuristics from source code-based debugging to the software design specification level. Suspicious locations in an SDL specification are prioritized by their likelihood of containing faults. Locations with a higher priority should be examined first rather than those with a lower priority as the former are more likely to contain the faults. A debugging tool, SmartD/sub DSL/, with user-friendly interfaces was developed to support our method. An illustration is provided to demonstrate the feasibility of using our method to effectively debug an architectural design.",0730-3157,0-7695-2020,10.1109/CMPSAC.2003.1245320,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1245320,,Software debugging;Software design;Testing;Computer bugs;Computer science;Delay;History;Computer languages;Fault detection;Programming profession,software architecture;software process improvement;formal specification;specification languages;software tools;software fault tolerance;program debugging,smart debugging;software architecture;statistical data;software bugs;development process;program slicing;execution histories;software debugging;programming languages;C;C++;software design specification;software designs;high-level specification;description language;execution slice-based heuristics;source code-based debugging;design specification level;SDL specification;containing faults;debugging tool;SmartD/sub DSL/;user-friendly interfaces;architectural design;program execution slicing;software fault detection;SmartDSDL,,1,20,,,,,,IEEE,IEEE Conferences
Test-suite reduction and prioritization for modified condition/decision coverage,J. A. Jones; M. J. Harrold,"Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA; Coll. of Comput., Georgia Inst. of Technol., Atlanta, GA, USA",IEEE Transactions on Software Engineering,,2003,29,3,195,209,"Software testing is particularly expensive for developers of high-assurance software, such as software that is produced for commercial airborne systems. One reason for this expense is the Federal Aviation Administration's requirement that test suites be modified condition/decision coverage (MC/DC) adequate. Despite its cost, there is evidence that MC/DC is an effective verification technique and can help to uncover safety faults. As the software is modified and new test cases are added to the test suite, the test suite grows and the cost of regression testing increases. To address the test-suite size problem, researchers have investigated the use of test-suite reduction algorithms, which identify a reduced test suite that provides the same coverage of the software according to some criterion as the original test suite, and test-suite prioritization algorithms, which identify an ordering of the test cases in the test suite according to some criteria or goals. Existing test-suite reduction and prioritization techniques, however, may not be effective in reducing or prioritizing MC/DC-adequate test suites because they do not consider the complexity of the criterion. This paper presents new algorithms for test-suite reduction and prioritization that can be tailored effectively for use with MC/DC. The paper also presents the results of empirical studies of these algorithms.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2003.1183927,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1183927,,Software testing;Software algorithms;Costs;System testing;Software maintenance;Computer Society;Safety;FAA;Performance evaluation;Software performance,program testing;performance evaluation,critical software;software testing;commercial airborne systems;test suites;modified condition/decision coverage;test-suite prioritization,,144,15,,,,,,IEEE,IEEE Journals & Magazines
Web-technology of university's evaluation rating,I. A. Botygin; I. I. Kalyatsky,"Tomsk Polytech. Univ., Russia; Tomsk Polytech. Univ., Russia","7th Korea-Russia International Symposium on Science and Technology, Proceedings KORUS 2003. (IEEE Cat. No.03EX737)",,2003,2,,410,415 vol.2,"At present there is no a unified standard system of both qualitative and quantitative criteria to estimate objectively the conformity of the University activities and its infrastructure with the requirements of the public state attestation and accreditation. So the use of heuristic and expert procedures is considered to be necessary. In the suggested model of the University (institute, faculty, department) rating, the fragments of the models used by the Ministry of Education of Russia, the Association for Technical Universities of Russia, the Independent Accreditation Center, the advanced technical universities of Russia, as well as long-term developments of the Tomsk Polytechnic University in quality control of the University activities and its departments, modern tendencies and priorities of the universities developments, are accepted as the basic ones. 55 indices have been determined. Numerical values of indices are normalized on the maximum value achieved in the objects under consideration that provides a commensurability of indices by their essential spread in various objects. The data convolution by sections and subsections is carried out linearly taking into account their importance. The value of the index is determined by an expert method taking into account its relative importance within the limits of each section (subsection). The numerical value of each rating index is determined by its relation to the ""base"". The ""base"" is determined as the on-budget salary fund of the University (institute, faculty, department) staff, providing educational process, and the on-budget salary fund of the staff carrying out state budgetary research. The software for the collective interactive remote input of the indices, as well as for the centralized calculation and operative analysis of the universities rating has been developed. The server Apache, the database server MySQL, and the scripting language PHP have been used for the above work to be carried out.",,89-7868-617,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1222647,,Accreditation;State estimation;Remuneration;Convolution;Databases;Region 8;Engineering education;Educational institutions;Statistics;Testing,Web sites;quality control;educational institutions;budgeting data processing;operations research,web-technology;qualitative criteria;quantitative criteria;university activities;long-term developments;quality control;data convolution;educational process;on-budget salary;state budgetary research;rating index;centralized calculation;operative analysis;scripting language,,,,,,,,,IEEE,IEEE Conferences
Agile development: evaluation and experience,W. F. Tichy,"Karlsruhe Univ., Germany",Proceedings. 26th International Conference on Software Engineering,,2004,,,692,,"Agile methods such as Extreme Programming, Crystal, Scrum, and others have attracted a lot of attention recently. Agile methods stress early and continuous delivery of software, welcome changing requirements, and value early feedback from customers. Agile methods seek to cut out inefficiency, bureaucracy, and anything that adds no value to a software product. Proponents of agile methods often see software specification and documentation as adding no value, which has led observers to conclude that agile development is nothing but unprincipled hacking, perhaps even an anarchic counter-reaction to bureaucratic, heavyweight software processes that demand ever more intermediate deliverables from developers. The purpose of this panel is to discuss under what circumstances agile methods work and don't work. Some of the key practices of agile methods are: scheduling according to feature priorities, incremental delivery of software, feedback from expert users, emphasis on face-to-face communication, pair development, minimalist design combined with refactoring, test-driven development, automated regression testing, daily integration, self-organizing teams, and periodic tuning of the methods. Working software is the primary measure of success. Find out what the latest practical experience with agile methods is and learn about the latest thinking in this area.",0270-5257,0-7695-2163,10.1109/ICSE.2004.1317492,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1317492,,Feedback;Automatic testing;Software testing;Stress;Documentation;Computer crime;Tuning;Software measurement;Software engineering,software engineering;program testing,agile development;Extreme Programming;Crystal;Scrum;software specification;program documentation;scheduling;software incremental delivery;face-to-face communication;test-driven development;automated regression testing,,2,,,,,,,IEEE,IEEE Conferences
An approach to generate the thin-threads from the UML diagrams,Xiaoqing Bai; C. P. Lam; Huaizhong Li,"Coll. of Electr. Eng., Guangxi Univ., Nanning, China; NA; NA","Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.",,2004,,,546,552 vol.1,"Software testing plays a crucial role in assuring software quality. One of the most important issues in software testing research is the generation of the test cases. For scenario-based software testing, the thin-threads, which are the usage scenarios in a software system from the end user's point of view, are frequently used to generate test cases. However, the generation of the thin-threads is not an easy task. A scenario-based business model has to be manually derived or labor-intensive business analysis has to be manually carried out in order to extract the thin-threads from a software system. In this work, we propose an automated approach to directly generate thin-threads from the UML artifacts. The generated thin-threads can be used to generate and to prioritize the test cases for scenario-based software testing.",0730-3157,0-7695-2209,10.1109/CMPSAC.2004.1342893,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342893,,Unified modeling language;Software testing;Software systems;System testing;Object oriented modeling;Information science;Software quality;Educational institutions;Computer industry;Software standards,Unified Modeling Language;program testing;object-oriented programming;diagrams;software quality,software quality;UML diagrams;test case generation;scenario-based software testing,,3,20,,,,,,IEEE,IEEE Conferences
An execution slice and inter-block data dependency-based approach for fault localization,W. E. Wong; Y. Qi,"Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA; Dept. of Comput. Sci., Texas Univ., Dallas, TX, USA",11th Asia-Pacific Software Engineering Conference,,2004,,,366,373,"Localizing a fault in a program is a complex and time-consuming process. In this paper we present a novel approach using execution slice and inter-block data dependency to effectively identify the locations of program faults. An execution slice with respect to a given test case is the set of code executed by this test, and two blocks are data dependent if one block contains a definition that is used by another block or vice versa. Not only can our approach reduce the search domain for program debugging, but also prioritize suspicious locations in the reduced domain based on their likelihood of containing faults. More specifically, the likelihood of a piece of code containing a specific fault is inversely proportional to the number of successful tests that execute it. In addition, the likelihood also depends on whether this piece of code is data dependent on other suspicious code. A debugging tool, DESiD, was developed to support our method. A case study that shows the effectiveness of our method in locating faults on an application developed for the European Space Agency is also reported.",1530-1362,0-7695-2245,10.1109/APSEC.2004.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1371939,Software testing;fault localization;program debugging;execution slice;inter-block data dependency,Debugging;Fault diagnosis;Software testing;Computer bugs;Computer science;Application software;Life testing;Programming,program slicing;program debugging;program testing;fault diagnosis;software quality,software testing;execution slice;inter-block data dependency;fault localization;program debugging,,5,14,,,,,,IEEE,IEEE Conferences
An improved AHP method in performance assessment,Yidan Bao; Yanping Wu; Yong He; Xiaofeng Ge,"Coll. of Biosyst. Eng. & Food Sci., Zhejiang Univ., Hangzhou, China; Coll. of Biosyst. Eng. & Food Sci., Zhejiang Univ., Hangzhou, China; Coll. of Biosyst. Eng. & Food Sci., Zhejiang Univ., Hangzhou, China; Coll. of Biosyst. Eng. & Food Sci., Zhejiang Univ., Hangzhou, China",Fifth World Congress on Intelligent Control and Automation (IEEE Cat. No.04EX788),,2004,1,,177,180 Vol.1,"In order to reduce subjective errors in the traditional analytic hierarchy process (AHP), an improved AHP method integrated with orthogonal experimental design was presented in this paper. The new method combined both AHP and orthogonal design principles to make decisions, objectives in assessment system and values computed through mapping and satisfying function respectively as the corresponding factors, levels and results in orthogonal experimental design. With software SAS analysis, priorities on performance were to be ranked out accordingly. Application has showed that the method can improve the accuracy of AHP in performance assessment.",,0-7803-8273,10.1109/WCICA.2004.1340551,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1340551,,Design for experiments;Decision making;Performance analysis;Analysis of variance;Testing;Helium;Educational institutions;Design engineering;Software performance;Synthetic aperture sonar,operations research;decision making;design of experiments;decision theory,AHP method;analytic hierarchy process;performance assessment system;error reduction;orthogonal experimental design;orthogonal design principles;decision making;SAS software analysis,,2,6,,,,,,IEEE,IEEE Conferences
Assessing staffing needs for a software maintenance project through queuing simulation,G. Antoniol; A. Cimitile; G. A. Di Lucca; M. Di Penta,"Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy; Dept. of Eng., Univ. of Sannio, Benevento, Italy",IEEE Transactions on Software Engineering,,2004,30,1,43,58,"We present an approach based on queuing theory and stochastic simulation to help planning, managing, and controlling the project staffing and the resulting service level in distributed multiphase maintenance processes. Data from a Y2K massive maintenance intervention on a large COBOL/JCL financial software system were used to simulate and study different service center configurations for a geographically distributed software maintenance project. In particular, a monolithic configuration corresponding to the customer's point-of-view and more fine-grained configurations, accounting for different process phases as well as for rework, were studied. The queuing theory and stochastic simulation provided a means to assess staffing, evaluate service level, and assess the likelihood to meet the project deadline while executing the project. It turned out to be an effective staffing tool for managers, provided that it is complemented with other project-management tools, in order to prioritize activities, avoid conflicts, and check the availability of resources.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2004.1265735,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1265735,,Software maintenance;Queueing analysis;Computational modeling;Stochastic processes;Project management;Costs;Computer simulation;Software systems;Counting circuits;Computer Society,software maintenance;queueing theory;discrete event simulation;personnel;project management;stochastic processes;program testing,queuing theory;stochastic simulation;distributed multiphase maintenance process;Y2K massive maintenance;financial software system;distributed software maintenance project;project-management tool;software maintenance staffing;discrete-event simulation;process simulation;schedule estimation,,39,37,,,,,,IEEE,IEEE Journals & Magazines
Aviation application over IPv6: performance issues,V. Srivastava; C. Wargo; S. Lai,"Comput. Networks & Software, Springfield, VA, USA; Comput. Networks & Software, Springfield, VA, USA; Comput. Networks & Software, Springfield, VA, USA",2004 IEEE Aerospace Conference Proceedings (IEEE Cat. No.04TH8720),,2004,3,,,1670 Vol.3,"Aviation industries in United States and in Europe are undergoing a major paradigm shift in the introduction of new network technologies. In the US, NASA is also actively investigating the feasibility of IPv6 based networks for the aviation needs of the United States. In Europe, the Eurocontrol lead, Internet protocol for aviation exchange (iPAX) Working Group is actively investigating the various ways of migrating the aviation authorities backbone infrastructure from X.25 based networks to an IPv6 based network. For the last 15 years, the global aviation community has pursued the development and implementation of an industry-specific set of communications standards known as the aeronautical telecommunications network (ATN). These standards are now beginning to affect the emerging military global air traffic management (GATM) community as well as the commercial air transport community. Efforts are continuing to gain a full understanding of the differences and similarities between ATN and Internet architectures as related to communications, navigation, and surveillance (CNS) infrastructure choices. This research paper describes the implementation of the IPv6 testbed at Computer Networks & Software, Inc. and it's interface connection mechanism to Eurocontrol and NASA's (Cleveland) testbed in the first phase of the project. In the second phase this research work investigates the performance issues of aviation applications such as controller to pilot data link communication (CPDLC), on an IPv6 based backbone network. Aviation applications are grouped into different priority levels. Desired quality of service (QoS) to each priority level is implemented via Diffserv implementation. This research work looks into the possibility of providing similar QoS performance for aviation application in an IPv6 network as is provided in an ATN based network. The testbed consists of three autonomous systems. The autonomous system represents CNS domain, NASA domain and a EUROCONTROL domain. The primary mode of connection between CNS IPv6 testbed and NASA and EUROCONTROL IPv6 testbed is initially a set of IPv6 over IPv4 tunnels. The aviation application under test (CPDLC) consists of two processes running on different IPv6 enabled machines. These processes communicate with each other over the IPv6 network. One machine resides on the CNS portion of the testbed and other may reside in NASA (Cleveland) and/or in Eurocontrol. The IPv6 packets between Eurocontrol, NASA and CNS testbeds would be carried on IPv6 over IPv4 tunnels. We present some results, which suggest that IPv6 QoS has matured enough, so as to provide the QoS service, which is similar in capability to die ATN architecture. We implemented three basic priorities of flow: (1) command & control; (2) surveillance; and (3) general traffic. Various parameters like throughput, packet loss and delay are investigated. The results are analyzed to get a conceptual view of the effect of IPv6 based network on the aviation applications.",1095-323X,0-7803-8155,10.1109/AERO.2004.1367941,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1367941,,NASA;Quality of service;Europe;Spine;Military standards;Surveillance;Software testing;Application software;Space technology;IP networks,aircraft communication;data communication;IP networks;Internet;aerospace simulation;protocols;air traffic control;telecommunication links;quality of service;telecommunication standards,aviation application;aviation industries;IPv6 based networks;United States;Europe;Eurocontrol;Internet protocol for aviation exchange;iPAX Working Group;aviation authorities backbone infrastructure;X.25 based networks;communications standards;aeronautical telecommunications network;military global air traffic management;air transport community;Internet architectures;CNS infrastructure;IPv6 testbed;Computer Networks & Software Inc;controller to pilot data link communication;quality of service;Diffserv implementation;autonomous systems;CNS domain;NASA domain;EUROCONTROL domain;IPv6 packets;IPv4 tunnels,,3,14,,,,,,IEEE,IEEE Conferences
Best practices for a FRACAS implementation,E. J. Hallquist; T. Schick,"Relex Software Corp., Greensurgh, PA, USA; Relex Software Corp., Greensurgh, PA, USA","Annual Symposium Reliability and Maintainability, 2004 - RAMS",,2004,,,663,667,"Many companies use a FRACAS (failure reporting analysis and corrective action system) process, better known as a closed-loop analysis and correction action process, to track and report problems or failures. Very few companies, however, fully realize all of the possible benefits of a FRACAS process, such as improving quality and productivity while reducing costs. Although many issues may prevent an effective implementation, there are three areas of concern that may cause negative impacts irrespective of the best-intentioned technology. In particular, complex organization interaction, inefficient and ineffective data tracking, and a lack of prioritized goals prevent the dramatic results that can be achieved with a FRACAS. By following the suggested eight step methodology and best practices presented, the potential for implementing a high-performance FRACAS can be greatly increased.",,0-7803-8215,10.1109/RAMS.2004.1285523,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1285523,,Best practices;Guidelines;Failure analysis;Databases;Testing;Data engineering;Reliability engineering;Productivity;Costs;Product development,failure analysis;closed loop systems,failure reporting analysis;corrective action system;closed-loop analysis;complex organization interaction;data tracking,,,2,,,,,,IEEE,IEEE Conferences
Code-coverage guided prioritized test generation,J. J. Li; H. Yee,"Avaya Labs., Basking Ridge, NJ, USA; Avaya Labs., Basking Ridge, NJ, USA","Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.",,2004,2,,178,181 vol.2,"With Internet applications spreading like wildfire, software testing is challenged with new topics related to the distributed nature of Web applications. We apply code based testing techniques to the testing of Web applications, specifically Java programs. Source code based automatic test generation is difficult because most previous methods use constraint satisfaction models as a solution, which is an NP complete problem [M. J. Gallagher et al. (1997)]. We present a method of guiding users through test case generations. Instead of automating the entire procedure, our method aims at generating a framework of test cases and providing instructions for users to instantiate the framework into actual executable test cases. An early experimental study of this method shows the effectiveness of this method.",0730-3157,0-7695-2209,10.1109/CMPSAC.2004.1342705,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1342705,,Application software;Automatic testing;Software testing;Java;System testing;Internet;Network servers;Protocols;Automatic generation control;Humans,program testing;Internet;Java;automatic testing;object-oriented methods;program compilers,code-coverage guided prioritized test generation;Internet applications;software testing;Java programs;source code based automatic test generation;constraint satisfaction models;NP complete problem;test case generations,,,7,,,,,,IEEE,IEEE Conferences
Empirical studies of test case prioritization in a JUnit testing environment,H. Do; G. Rothermel; A. Kinneer,"Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Lincoln, NE, USA",15th International Symposium on Software Reliability Engineering,,2004,,,113,124,"Test case prioritization provides a way to run test cases with the highest priority earliest. Numerous empirical studies have shown that prioritization can improve a test suite's rate of fault detection, but the extent to which these results generalize is an open question because the studies have all focused on a single procedural language, C, and a few specific types of test suites, in particular, Java and the JUnit testing framework are being used extensively in practice, and the effectiveness of prioritization techniques on Java systems tested under JUnit has not been investigated. We have therefore designed and performed a controlled experiment examining whether test case prioritization can be effective on Java programs tested under JUnit, and comparing the results to those achieved in earlier studies. Our analyses show that test case prioritization can significantly improve the rate of fault detection of JUnit test suites, but also reveal differences with respect to previous studies that can be related to the language and testing paradigm.",1071-9458,0-7695-2215,10.1109/ISSRE.2004.18,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1383111,,System testing;Java;Software testing;Fault detection;Performance evaluation;Software systems;Computer science;Systems engineering and theory;Feedback;Software performance,program testing;software fault tolerance;Java,test case prioritization;JUnit testing environment;software fault detection;Java program;object-oriented system,,,33,,,,,,IEEE,IEEE Conferences
New optical switches enable automated testing with true flexibility,M. Bitting,"Polatis Ltd., McKinney, TX, USA",Proceedings AUTOTESTCON 2004.,,2004,,,361,366,"The proliferation of fiber optic systems in military and avionics platforms is driven by the ever increasing need for higher data rates to support multi-sensor data fusion. Traditionally, the test systems to support these optical deployments are manual and inefficient. Increasingly fast optical components require optical test equipment that is very expensive. To make cost effective test suites, it is essential that these high value resources be used efficiently. This is most effectively accomplished through test architectures that are remotely controlled and automatically scheduled. These test architectures also enable a diverse set of testing applications to be simultaneously executed within an optical test lab or manufacturing environment. The advent of optical matrix switching technology with sub 1dB insertion loss performance and repeatability measured in milli dB's opens up new doors for highly efficient, remotely controlled, automated test systems. The ultra low loss aspects of these switches enable distributed test architectures that were previously unrealizable. Distributed test architectures create a test environment where expensive test equipment can be leveraged over a greater number of test samples in a more timely and automated fashion. This allows the lab manager to prioritize and schedule tests across many users, DUTs, and test equipment bays in an operation that can run 24/7. This paper explores the enabling photonic switch technology and a couple generic test architectures that can be applied in a variety of automated applications to increase test equipment usage and efficiency, thus lowering end costs for deployable fiber optic components and systems.",1088-7725,0-7803-8449,10.1109/AUTEST.2004.1436883,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1436883,,Optical switches;Automatic testing;Test equipment;System testing;Optical fibers;Optical devices;Costs;Automatic control;Job shop scheduling;Optical losses,photonic switching systems;automatic test equipment;automatic test software;optical engineering computing;optical fibre testing,optical switch;automated testing;optical test equipment;optical matrix switching technology;distributed test architecture;photonic switch technology,,1,,,,,,,IEEE,IEEE Conferences
Requirements driven software evolution,L. Tahvildari; K. Kontogiannis,"Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada; Dept. of Electr. & Comput. Eng., Waterloo Univ., Ont., Canada","Proceedings. 12th IEEE International Workshop on Program Comprehension, 2004.",,2004,,,258,259,"Software evolution is an integral part of the software life cycle. Furthermore in the recent years the issue of keeping legacy systems operational in new platforms has become critical and one of the top priorities in IT departments worldwide. The research community and the industry have responded to these challenges by investigating and proposing techniques for analyzing, transforming, integrating, and porting software systems to new platforms, languages, and operating environments. However, measuring and ensuring that compliance of the migrant system with specific target requirements have not been formally and thoroughly addressed. We believe that issues such as the identification, measurement, and evaluation of specific re-engineering and transformation strategies and their impact on the quality of the migrant system pose major challenges in the software re-engineering community. Other related problems include the verification, validation, and testing of migrant systems, and the design of techniques for keeping various models (architecture, design, source code) during evolution, synchronized. In this working session, we plan to assess the state of the art in these areas, discuss on-going work, and identify further research issues.",1092-8138,0-7695-2149,10.1109/WPC.2004.1311070,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1311070,,Software quality;Computer architecture;System testing;Computer industry;Software systems;Software measurement;Software tools;Application software;Software design;Algorithm design and analysis,software prototyping;integrated software;systems re-engineering;formal verification;program testing;software architecture,requirements driven software evolution;software life cycle;legacy system;integrated software;operating environment;software re-engineering;formal verification;program testing;software architecture,,,13,,,,,,IEEE,IEEE Conferences
A controlled experiment assessing test case prioritization techniques via mutation faults,H. Do; G. Rothermel,"Dept. of Comput. Sci. & Eng., Nebraska Univ., Omaha, NE, USA; Dept. of Comput. Sci. & Eng., Nebraska Univ., Omaha, NE, USA",21st IEEE International Conference on Software Maintenance (ICSM'05),,2005,,,411,420,"Regression testing is an important part of software maintenance, but it can also be very expensive. To reduce this expense, software testers may prioritize their test cases so that those that are more important are run earlier in the regression testing process. Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited to hand-seeded faults, primarily due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults. We have therefore designed and performed a controlled experiment to assess the ability of prioritization techniques to improve the rate of fault detection techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. We also compare our results to those collected earlier with respect to the relationship between hand-seeded faults and mutation faults, and the implications this has for researchers performing empirical studies of prioritization.",1063-6773,0-7695-2368,10.1109/ICSM.2005.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510136,,Computer aided software engineering;Genetic mutations;Software testing;Fault detection;Software maintenance;System testing;Performance evaluation;Computer science;Automatic testing;Automatic control,program testing;software maintenance,test case prioritization technique;mutation fault;regression testing;software maintenance;fault detection;hand-seeded faults,,26,28,,,,,,IEEE,IEEE Conferences
Concealment of whole-frame losses for wireless low bit-rate video based on multiframe optical flow estimation,S. Belfiore; M. Grangetto; E. Magli; G. Olmo,"Dipt. di Elettronica, Center for Multimedia Radio Commun., Torino, Italy; Dipt. di Elettronica, Center for Multimedia Radio Commun., Torino, Italy; Dipt. di Elettronica, Center for Multimedia Radio Commun., Torino, Italy; Dipt. di Elettronica, Center for Multimedia Radio Commun., Torino, Italy",IEEE Transactions on Multimedia,,2005,7,2,316,329,"In low bit-rate packet-based video communications, video frames may have very small size, so that each frame fills the payload of a single network packet; thus, packet losses correspond to whole-frame losses, to which the existing error concealment algorithms are badly suited and generally not applicable. In this paper, we deal with the problem of concealment of whole frame-losses, and propose a novel technique which is capable of handling this very critical case. The proposed technique presents other two major innovations with respect to the state-of-the-art: i) it is based on optical flow estimation applied to error concealment and ii) it performs multiframe estimation, thus optimally exploiting the multiple reference frame buffer featured by the most modern video coders such as H.263+ and H.264. If data partitioning is employed, by e.g., sending headers, motion vectors, and coding modes in prioritized packets as can be done in the DiffServ network model, the algorithm is capable of exploiting the motion vectors to improve the error concealment results. The algorithm has been embedded in the H.264 test model software, and tested under both independent and correlated packet loss models with parameters typical of the wireless environment. Results show that the proposed algorithm significantly outperforms other techniques by several dBs in peak signal-to-noise ratio (PSNR), provides good visual quality, and has a rather low complexity, which makes it possible to perform real-time operation with reasonable computational resources.",1520-9210;1941-0077,,10.1109/TMM.2005.843347,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1407904,Optical flow estimation;packet-based video transmission;video error concealment;whole-frame losses;wireless communications,Optical losses;Image motion analysis;Optical buffering;State estimation;Partitioning algorithms;Software testing;PSNR;Payloads;Technological innovation;Estimation error,video coding;video streaming;video databases;DiffServ networks;image sequences;wireless LAN;packet switching;error statistics,multiframe optical flow estimation;error concealment algorithms;multiple reference frame buffer;video coders;data partition;DiffServ network model;packet loss models;wireless communications;signal-to-noise ratio;visual quality;packet-based video transmission;video error concealment;whole-frame losses,,66,37,,,,,,IEEE,IEEE Journals & Magazines
Current results from a rover science data analysis system,R. Castano; M. Judd; T. Estlin; R. C. Anderson; D. Gaines; A. Castano; B. Bornstein; T. Stough; K. Wagstaff,"Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA",2005 IEEE Aerospace Conference,,2005,,,356,365,"The Onboard Autonomous Science Investigation System (OASIS) evaluates geologic data gathered by a planetary rover. This analysis is used to prioritize the data for transmission, so that the data with the highest science value is transmitted to Earth. In addition, the onboard analysis results are used to identify science opportunities. A planning and scheduling component of the system enables the rover to take advantage of the identified science opportunity. OASIS is a NASA-funded research project that is currently being tested on the FIDO rover at JPL for use on future missions. In this paper, we provide a brief overview of the OASIS system, and then describe our recent successes in integrating with and using rover hardware. OASIS currently works in a closed loop fashion with onboard control software (e.g., navigation and vision) and has the ability to autonomously perform the following sequence of steps: analyze gray scale images to find rocks, extract the properties of the rocks, identify rocks of interest, retask the rover to take additional imagery of the identified target and then allow the rover to continue on its original mission. We also describe the early 2004 ground test validation of specific OASIS components on selected Mars exploration rover (MER) images. These components include the rock-finding algorithm, RockIT, and the rock size feature extraction code. Our team also developed the RockIT GUI, an interface that allows users to easily visualize and modify the rock-finder results. This interface has allowed us to conduct preliminary testing and validation of the rock-finder's performance.",1095-323X,0-7803-8870,10.1109/AERO.2005.1559328,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559328,,Data analysis;Testing;Geology;Geoscience;Hardware;Software performance;Navigation;Image analysis;Image sequence analysis;Performance analysis,planetary rovers;feature extraction;Mars;geophysical signal processing;geophysical techniques;image processing;rocks;graphical user interfaces,rover science data analysis system;Onboard Autonomous Science Investigation System;OASIS;planetary rover;onboard analysis;NASA;FIDO rover;Mars exploration rover images;rock-finding algorithm;feature extraction code;RockIT GUI,,11,4,,,,,,IEEE,IEEE Conferences
Early community building: a critical success factor for XP projects,G. Broza,"Ind. Logic (Canada), Toronto, Ont., Canada",Agile Development Conference (ADC'05),,2005,,,167,172,"Extreme programming (XP) literature and discussions often view successful projects only as customer-driven product development: planning, coding and testing an unfolding series of prioritized units of vertical functionality. I claim, however, that a successful project also requires a prospering community, comprising an introspective group of committed professionals communicating effectively, and using a well-understood, stable process. Weakness on any of these fronts presents a high risk of failure; therefore, I advise every XP project's members to actively engage in building their community, such that it reaches its critical level of development already by the first internal release. To help in this endeavor, I provide a comprehensive list of activities and attitudes to practice and avoid during the first release.",,0-7695-2487,10.1109/ADC.2005.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1609817,XP;community building;first release;risk,Programming profession;Logic programming;Logic testing;Project management;Collaboration;Gas insulated transmission lines;Functional programming;Product development;Buildings;Software systems,software engineering,early community building;extreme programming;customer-driven product development;product planning;product testing,,,19,,,,,,IEEE,IEEE Conferences
Empirical results from an experiment on value-based review (VBR) processes,K. Lee; B. Boehm,"Center for Software Eng., Univ. of Southern California, Los Angeles, CA, USA; Center for Software Eng., Univ. of Southern California, Los Angeles, CA, USA","2005 International Symposium on Empirical Software Engineering, 2005.",,2005,,,10 pp.,,"As part of our research on value-based software engineering, we conducted an experiment on the use of value-based review (VBR) processes. We developed a set of VBR checklists with issues ranked by success-criticality, and a set of VBR processes prioritized by issue criticality and stakeholder-negotiated product capability priorities. The experiment involved 28 independent verification and validation (IV&V) subjects (full-time working professionals taking a distance learning course) reviewing specifications produced by 18 real-client, full-time student e-services projects. The IV&V subjects were randomly assigned to use either the VBR approach or our previous value-neutral checklist-based reading (CBR) approach. The difference between groups was not statistically significant for number of issues reported, but was statistically significant for number of issues per review hour, total issue impact, and cost effectiveness in terms of total issue impact per review hour. For the latter, the VBRs were roughly twice as cost-effective as the CBRs.",,0-7803-9507,10.1109/ISESE.2005.1541809,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541809,,Software engineering;Costs;Computer aided instruction;Bioreactors;Software quality;Computer architecture;Software performance;Application software;Software testing;Investments,software engineering;statistical analysis,empirical analysis;value-based review process;VBR checklist;value-based software engineering;full-time student e-services project;value-neutral checklist-based reading approach;CBR approach,,2,16,,,,,,IEEE,IEEE Conferences
Identification of test process improvements by combining fault trigger classification and faults-slip-through measurement,L. -. Damm; L. Lundberg,"Ericsson AB, Karlskrona, Sweden; NA","2005 International Symposium on Empirical Software Engineering, 2005.",,2005,,,10 pp.,,"Successful software process improvement depends on the ability to analyze past projects and determine which parts of the process that could become more efficient. One source of such an analysis is the faults that are reported during development. This paper proposes how a combination of two existing techniques for fault analysis can be used to identify where in the test process improvements are needed, i.e. to pinpoint which activities in which phases that should be improved. This was achieved by classifying faults after which test activities that triggered them and which phase each fault should have been found in, i.e. through a combination of orthogonal defect classification (ODC) and faults-slip-through measurement. As a part of the method, the paper proposes a refined classification scheme due to identified problems when trying to apply ODC classification schemes in practice. The feasibility of the proposed method was demonstrated by applying it on an industrial software development project at Ericsson AB. The obtained measures resulted in a set of quantified and prioritized improvement areas to address in consecutive projects.",,0-7803-9507,10.1109/ISESE.2005.1541824,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541824,,Fault diagnosis;Testing;Feedback;Software measurement;Phase measurement;Programming;Refining;Computer industry;Area measurement;Costs,software process improvement;software development management;software fault tolerance,fault trigger classification;faults-slip-through measurement;software process improvement;fault analysis;orthogonal defect classification;industrial software development project,,3,30,,,,,,IEEE,IEEE Conferences
Integrated Product Policy and distributed supplier structures: SME and sound LCA data in conflict,F. Mandorli; M. Germani; H. E. Otto,"Department of Mechanical Engineering, The University of Ancona, Via Brecce Bianche, Ancona I-60131 ITALY, f.mandorli@univpm.it; NA; NA",2005 4th International Symposium on Environmentally Conscious Design and Inverse Manufacturing,,2005,,,430,437,"The sustainable development of our societies is one of the priorities of the European Commission. Through its new Integrated Product Policy (IPP), the European Commission is developing a series of measures that influence the supply and demand of environmentally sound products. Some IPP tools are based on product and process self-declarations, while others require the performance of a Life-Cycle Assessment (LCA). Life-Cycle Inventory (LCI) data availability is the fundamental premise in order to be able to perform an LCA. In this paper, the work is to investigate the diffusion of required LCA data along the supplier chain with the aim of identifying strategies to increase the awareness of Small and Medium Enterprises (SMEs) in respect to LCA, to suggest methodologies to facilitate the collection of sound LCI data and to test available low-cost software tools to support LCA, with particular reference to the production phase is reported",,1-4244-0081,10.1109/ECODIM.2005.1619261,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1619261,Environmental Product Declaration;Life-Cycle Assessment data;Small and Medium Enterprise,Production;Acoustical engineering;Data engineering;Certification;Availability;Sustainable development;Environmental economics;ISO;Supply and demand;Supply chains,legislation;product development;product life cycle management;small-to-medium enterprises;societies;software tools;supply and demand;supply chains;sustainable development,integrated product policy tools;life-cycle assessment data;small-to-medium enterprises;sustainable development;societies;European Commission;supply and demand;environmental products;life-cycle inventory data;supply chain;software tools,,,8,,,,,,IEEE,IEEE Conferences
Plug and play testbed to enable responsive space missions,J. Summers,"MicroSat Syst. Inc., Littleton, CO, USA",2005 IEEE Aerospace Conference,,2005,,,557,563,"There exists a growing need in the DOD for a tactical or responsive space asset to support real-time battlefield intelligence, surveillance, and reconnaissance. The desired attributes include being: 1. responsive /sub e/ployable in days; 2. affordable - expendable tactical resources at a cost comparable to other tactical systems; 3. employable - assets must support the joint force commander (JFC); 4. integrated space/air/terrestrial system-of-systems - full network connectivity, bandwidth on demand, and augment other assets. To the warfighter, a responsive space asset would provide the capability to respond to unanticipated military needs in days, providing flexibility of response to rapidly field tailored payloads and coverage. This capability could also provide rapid reconstitution after a loss from attack or failure and counteract enemy adaptation, through denial or deception, to existing space capabilities. Most importantly the short deployment times and low cost would provide the United States a means for efficiently using the versatility, and relative safety of space to provide real-time support to the war fighter. To be responsive the space element must possess a modular design supporting ""plug and play"" (PnP) architecture, leveraging commercial parts and standards. Lending itself to a lean production and integration environment again utilizing standard interfaces and taking advantage of pre-qualified inventoried subsystems. Rapid deployment of these elements will make use of ""canned"" mission planning tools, tailored orbits for a given theater, built-in health and status monitoring, and autonomous test and checkout software and operations. The two emerging responsive mission objectives include space control and tailored, tactical intelligence, surveillance, reconnaissance (ISR). Space control involves a situational awareness to sense threats against, and provide protection for, US space assets. The tailored, tactical missions offer high tempo ISR operations such as target characterization and emitter location in theater, perform real-time blue force tracking, and provide gap-filler, specialized communications support. The challenge is to develop and qualify the satellite technologies and rapid integration and test processes to support an operational responsive system in the next five years. Under an Air Force Research Laboratory SBIR program, MicroSat Systems is developing a PnP testbed to enable an operational responsive capability through development and ground validation of the various elements. Those elements include the mission definition and CONOPS specification processes, space segment, and the operational prioritization, tasking, processing, exploitation, and dissemination (PTPED) process/infrastructure. Specific to providing an end-to-end mission simulation, the testbed should be equipped with the modeling and simulation tools to develop tactical satellite CONOPS and provide the warfighter with a front end tool for training. To validate utility to the end-user the testbed must be equipped with the capability to simulate the data processing and dissemination infrastructure envisioned to provide the battlefield commander with real-time data. The space segment of the testbed should consist of the hardware and software elements required to simulate the operations of a fully functional satellite system. The approach to developing a responsive mission testbed includes defining requirements, hardware/software architecture, technology development roadmap, starting with a core capability, and incrementally integrating and validating the developing components and processes as they emerge.",1095-323X,0-7803-8870,10.1109/AERO.2005.1559345,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1559345,,Plugs;Space missions;Space technology;Software testing;Satellites;System testing;Surveillance;Reconnaissance;Costs;Hardware,command and control systems;military aircraft;aerospace simulation;automatic test software;aerospace safety;software tools;hardware-software codesign;surveillance,plug and play testbed;responsive space missions;tactical systems;joint force commander;integrated space-air-terrestrial system-of-systems;responsive space asset;real-time support;war fighter;mission planning tools;health monitoring;status monitoring;autonomous test software;checkout software;space control;tactical intelligence;tactical surveillance;tactical reconnaissance;situational awareness;tactical missions;satellite technologies;operational responsive system;mission simulation;simulation tools;tactical satellite;CONOPS;data processing;dissemination infrastructure;battlefield commander;hardware-software architecture;technology development,,2,6,,,,,,IEEE,IEEE Conferences
Prioritize code for testing to improve code coverage of complex software,J. J. Li,"Avaya Labs Res., Basking Ridge, NJ, USA",16th IEEE International Symposium on Software Reliability Engineering (ISSRE'05),,2005,,,10 pp.,84,"Code prioritization for testing promises to achieve the maximum testing coverage with the least cost. This paper presents an innovative method to provide hints on which part of code should be tested first to achieve best code coverage. This method claims two major contributions. First it takes into account a ""global view"" of the execution of a program being tested, by considering the impact of calling relationship among methods/functions of complex software. It then relaxes the ""guaranteed"" condition of traditional dominator analysis to be ""at least"" relationship among dominating nodes, which makes dominator calculation much simpler without losing its accuracy. It also then expands this modified dominator analysis to include global impact of code coverage, i.e. the coverage of the entire software other than just the current function. We implemented two versions of code prioritization methods, one based on original dominator analysis and the other on relaxed dominator analysis with global view. Our comparison study shows that the latter is consistently better in terms of identifying code for testing to increase code coverage",1071-9458;2332-6549,0-7695-2482,10.1109/ISSRE.2005.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1544723,Testing;code coverage;dominator analysis;coverage priority,Software testing;Costs;Software reliability;Software design;Programming;History;Software engineering;Area measurement;Guidelines;Frequency,partial evaluation (compilers);program diagnostics;program testing,code coverage;software testing;code prioritization;program execution;dominating nodes;relaxed dominator analysis,,6,10,,,,,,IEEE,IEEE Conferences
System test case prioritization of new and regression test cases,H. Srikanth; L. Williams; J. Osborne,"Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC, USA; NA","2005 International Symposium on Empirical Software Engineering, 2005.",,2005,,,10 pp.,,"Test case prioritization techniques have been shown to be beneficial for improving regression-testing activities. With prioritization, the rate of fault detection is improved, thus allowing testers to detect faults earlier in the system-testing phase. Most of the prioritization techniques to date have been code coverage-based. These techniques may treat all faults equally. We build upon prior test case prioritization research with two main goals: (1) to improve user-perceived software quality in a cost effective way by considering potential defect severity and (2) to improve the rate of detection of severe faults during system-level testing of new code and regression testing of existing code. We present a value-driven approach to system-level test case prioritization called the prioritization of requirements for test (PORT). PORT prioritizes system test cases based upon four factors: requirements volatility, customer priority, implementation complexity, and fault proneness of the requirements. We conducted a PORT case study on four projects developed by students in advanced graduate software testing class. Our results show that PORT prioritization at the system level improves the rate of detection of severe faults. Additionally, customer priority was shown to be one of the most important prioritization factors contributing to the improved rate of fault detection.",,0-7803-9507,10.1109/ISESE.2005.1541815,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1541815,,System testing;Computer aided software engineering;Software testing;Fault detection;Software quality;Costs;Software engineering;Computer science;Statistical analysis;Phase detection,program testing;software quality;software fault tolerance;regression analysis,system-level test case prioritization;regression test case;fault detection;code coverage-based prioritization technique;software quality;software defect severity;value-driven approach;prioritization of requirements for test;PORT;requirements volatility;customer priority;implementation complexity;fault proneness;software testing,,46,32,,,,,,IEEE,IEEE Conferences
"Systematic incremental development of agent systems, using Prometheus",M. Perepletchikov; L. Padgham,"Sch. of Comput. Sci. & Inf. Technol., R. Melbourne Inst. of Technol., Vic., Australia; Sch. of Comput. Sci. & Inf. Technol., R. Melbourne Inst. of Technol., Vic., Australia",Fifth International Conference on Quality Software (QSIC'05),,2005,,,413,418,"This paper presents a mechanism for dividing an agent oriented application into the three IEEE defined scoping levels of essential, conditional and optional. This mechanism is applied after the initial system specification, and is then used to direct incremental development with three separate releases. The scoping described can be applied at any stage of a project, in order to guide consistent scoping back if such is needed. The three levels of scoping that are used are consistent with the approach used in many companies. The approach to scoping requires that scenarios are prioritised manually on a five point scale. All other aspects are then prioritised automatically, based on this information. The approach used allows a developer to indicate what size partitions - based on number of scenarios - are required for each scoping level. The mechanisms are applied to the Prometheus development methodology and are integrated into the Prometheus design tool (PDT).",1550-6002;2332-662X,0-7695-2472,10.1109/QSIC.2005.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1579165,,Programming;Testing;Iterative methods;Computer science;Information technology;Application software;Software engineering;Software quality,multi-agent systems;formal specification;software agents,system specification;Prometheus development methodology;Prometheus design tool;agent systems incremental development,,2,14,,,,,,IEEE,IEEE Conferences
Test factoring: focusing test suites for the task at hand,D. Saff; M. D. Ernst,"Comput. Sci. & Artificial Intelligence Lab, MIT, USA; Comput. Sci. & Artificial Intelligence Lab, MIT, USA","Proceedings. 27th International Conference on Software Engineering, 2005. ICSE 2005.",,2005,,,656,,"Frequent execution of a test suite during software maintenance can catch regression errors early, indicate whether progress is being made, and improve productivity. However, if the test suite takes a long time to produce feedback, the developer is slowed down, and the benefit of frequent testing is reduced. After a program is edited, ideally, only changed code would be tested. Any time spent executing previously tested, unchanged parts of the code is wasted. For a large test suite containing many small unit tests, test selection and prioritization can be effective. Test selection runs only those tests that are possibly affected by the most recent change, and test prioritization can run first the tests that are most likely to reveal a recently-introduced error.",0270-5257;1558-1225,1-59593-963,10.1109/ICSE.2005.1553636,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1553636,,Automatic testing;System testing;Software testing;Computer bugs;Costs;Instruments;Computer science;Artificial intelligence;Debugging;Algorithm design and analysis,software maintenance;program testing,test factoring;software maintenance;regression error;unit testing;test selection;test prioritization;mock object,,1,3,,,,,,IEEE,IEEE Conferences
Test prioritization using system models,B. Korel; L. H. Tahat; M. Harman,"Dept. of Comput. Sci., Illinois Inst. of Technol., Chicago, IL, USA; NA; NA",21st IEEE International Conference on Software Maintenance (ICSM'05),,2005,,,559,568,"During regression testing, a modified system is retested using the existing test suite. Because the size of the test suite may be very large, testers are interested in detecting faults in the system as early as possible during the retesting process. Test prioritization tries to order test cases for execution so the chances of early detection of faults during retesting are increased. The existing prioritization methods are based on the code of the system. System modeling is a widely used technique to model state-based systems. In this paper, we present methods of test prioritization based on state-based models after changes to the model and the system. The model is executed for the test suite and information about model execution is used to prioritize tests. Execution of the model is inexpensive as compared to execution of the system; therefore the overhead associated with test prioritization is relatively small. In addition, we present an analytical framework for evaluation of test prioritization methods. This framework may reduce the cost of evaluation as compared to the existing evaluation framework that is based on experimentation (observation). We have performed an experimental study in which we compared different test prioritization methods. The results of the experimental study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.",1063-6773,0-7695-2368,10.1109/ICSM.2005.87,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1510150,,System testing;Fault detection;Modeling;Software systems;Software maintenance;Software testing;Computer science;Technological innovation;Educational institutions;Costs,program testing;software fault tolerance,regression testing;system modeling;state-based model;test prioritization method;fault detection,,37,18,,,,,,IEEE,IEEE Conferences
Using occurrence properties of defect report data to improve requirements,K. S. Wasson; K. N. Schmid; R. R. Lutz; J. C. Knight,"Dept. of Comput. Sci., Virginia Univ., Charlottesville, VA, USA; NA; NA; NA",13th IEEE International Conference on Requirements Engineering (RE'05),,2005,,,253,262,"Defect reports generated for faults found during testing provide a rich source of information regarding problematic phrases used in requirements documents. These reports indicate that faults often derive from instances of ambiguous, incorrect or otherwise deficient language. In this paper, we report on a method combining elements of linguistic theory and information retrieval to guide the discovery of problematic phrases throughout a requirements specification, using defect reports and correction requests generated during testing to seed our detection process. We found that phrases known from these materials to be problematic have occurrence properties in requirements documents that both allow the direction of resources to prioritize their correction, and generate insights characterizing more general locations of difficulty within the requirements. Our findings lead to some recommendations for more efficiently and effectively managing certain natural language issues in the creation and maintenance of requirements specifications.",1090-705X;2332-6441,0-7695-2425,10.1109/RE.2005.77,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1531046,,Natural languages;Computer science;System testing;Software systems;Propulsion;Laboratories;Information resources;Information retrieval;Character generation;Software testing,natural languages;information retrieval;formal specification;system documentation;computational linguistics,defect report data occurrence properties;linguistic theory;information retrieval;requirements specification;natural languages,,6,15,,,,,,IEEE,IEEE Conferences
A Dynamic Partitioning Approach for GUI Testing,K. Cai; L. Zhao; F. Wang,"Beijing University of Aeronautics and Astronautics, China; Beijing University of Aeronautics and Astronautics, China; Beijing University of Aeronautics and Astronautics, China",30th Annual International Computer Software and Applications Conference (COMPSAC'06),,2006,2,,223,228,"Previous works on GUI testing are mainly concerned with how to define or generate GUI test cases. The issue of how to employ generated GUI test cases or primitive actions is seldom discussed. In this paper we propose a dynamic partitioning approach for GUI testing to address the issue. In this approach, the given GUI primitive actions are dynamically partitioned into two disjoint classes: one comprising prioritized primitive actions and the other comprising non-prioritized ones. The testing process is divided into two stages and contains two feedback loops. The first stage prioritizes primitive actions and the second stage selects and performs prioritized primitive actions. The first feedback loop is local and occurs in the second stage, which adjusts the memberships of primitive actions after they are performed. The second feedback loop is global and occurs between the first and second stages. It switches GUI testing from the second stage to the first stage upon no prioritized primitive actions are available. Two testing experiments with real GUI applications show that the proposed dynamic partitioning approach can really work in practice and may significantly outperform the random testing approach in the sense that the dynamic partitioning approach uses fewer primitive actions to achieve given testing goals and behaves more stable. The dynamic partitioning approach adopts explicit feedback mechanisms and contributes to the emerging area of software cybernetics that explores the interplay between software and control",0730-3157;0730-3157,0-7695-2655,10.1109/COMPSAC.2006.94,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020171,,Graphical user interfaces;Aerodynamics;Automatic testing;Feedback loop;Computational Intelligence Society;Application software;Switches;Cybernetics;Software testing;Materials testing,automatic test pattern generation;graphical user interfaces;program control structures;program testing,dynamic partitioning;GUI testing;feedback loops;random testing;software cybernetics,,3,12,,,,,,IEEE,IEEE Conferences
A Feature-Oriented Requirements Tracing Method: A Study of Cost-benefit Analysis,S. Ahn; K. Chong,"Soongsil University, Seoul, Korea; Soongsil University, Seoul, Korea",2006 International Conference on Hybrid Information Technology,,2006,2,,611,616,"Establishing and maintaining traceability links places a big burden since complex systems have especially yield an enormous number of various artifacts although traceability links is useful for requirements change impact analysis, requirements conflict analysis, and requirements consistency checking. Hence, we propose a feature-oriented requirements tracing method including value consideration and intermediate catalysis. To achieve our goal in this paper, we present (1) a meta-model of feature-oriented requirements tracing, (2) a featureoriented requirement tracing process overview, and (3) cost-benefits analysis. The meta-model is a formalization of feature-oriented requirement tracing using UML notation. The feature-oriented requirement tracing process consists of requirements definition, feature modeling, feature prioritization, requirements linking, and traceability links evaluation. We also carry out cost-benefit analysis through a case study to demonstrate the feasibility of our approach.",,0-7695-2674,10.1109/ICHIT.2006.253670,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021275,,Cost benefit analysis;Software engineering;Joining processes;Unified modeling language;Software testing;Software reusability;Programming;Computer architecture;Software systems;Maintenance engineering,,,,2,14,,,,,,IEEE,IEEE Conferences
A hardware implementation of layer 2 MPLS,R. Peterkin; D. Ionescu,"Sch. of Inf. Technol. & Eng., Ottawa Univ., Ont., Canada; Sch. of Inf. Technol. & Eng., Ottawa Univ., Ont., Canada","Third IEEE International Workshop on Electronic Design, Test and Applications (DELTA'06)",,2006,,,2 pp.,404,"This paper presents a hardware architecture for layer 2 Multi Protocol Label Switching (MPLS). MPLS is a protocol framework used primarily to prioritize internet traffic and improve bandwidth utilization. Furthermore it increases the performance of internet applications and overall efficiency. However, most existing MPLS solutions are entirely software based which decreases performance. MPLS performance can be enhanced by executing core tasks in hardware while allowing other tasks to be executed in software to guard against performance degradation. This paper proposes a hardware design of MPLS on an FPGA for increased performance and efficiency.",,0-7695-2500,10.1109/DELTA.2006.3,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1581247,,Hardware;Multiprotocol label switching;Protocols;Internet;Software performance;Computer architecture;Bandwidth;Application software;Degradation;Field programmable gate arrays,multiprotocol label switching;Internet;field programmable gate arrays,hardware implementation;layer 2 MPLS;multi protocol label switching;protocol framework;Internet traffic;bandwidth utilization;hardware design;FPGA,,,,,,,,,IEEE,IEEE Conferences
An Add-On for Managing Behaviours with Priority in JADE,J. A. Suarez-romero; A. Alonso-Betanzos; B. Guijarro-Berdinas,"University of A Coruna, Spain; University of A Coruna, Spain; University of A Coruna, Spain",2006 IEEE/WIC/ACM International Conference on Intelligent Agent Technology,,2006,,,713,716,"In this article, two new implementations for behaviours in JADE are presented. These new behaviours, while being able to reproduce the functioning of the old JADE's behaviours, allow the user to define priorities. This fact is of vital importance for several multiagent applications. Finally, a test was developed to show that the performance of the new behaviours is similar to the original ones.",,0-7695-2748,10.1109/IAT.2006.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4053000,,Java;Processor scheduling;Programming profession;Computer science;Sequential analysis;Yarn;Concurrent computing;Round robin;Intelligent agent;Dynamic scheduling,Java;multi-agent systems;program diagnostics;software agents;software engineering,Java agent development framework;JADE behaviour;multiagent application;add-on application,,,3,,,,,,IEEE,IEEE Conferences
An Agent-Based E-Learning Assessing and Instructing System,Z. Liu; Z. Wang; Z. Fang,"Software College, Zhejiang University of Technology, Hangzhou, P.R. China, 310014. lzhi@zjut.edu.cn; Software College, Zhejiang University of Technology, Hangzhou, P.R. China, 310014. lzhi@zjut.edu.cn; Network Centre, Zhejiang University of Technology, Hangzhou, P.R. China, 310014. fzl@zjut.edu.cn",2006 10th International Conference on Computer Supported Cooperative Work in Design,,2006,,,1,6,"The research work on e-learning has become very important field in the education. Many demands for instructing strategies in the adaptive learning have turn out. Base on these instructing strategies, the learners could reduce their blindness in the learning process. This paper proposes a method to build a Bayesian networks model in order to assess the learner's knowledge level and instruct the learner. This e-learning assessing and instructing system is designed and implemented based on multi-agent systems (ELAIS). In this system, the knowledge cognitive level, the learning priorities and weakness of a learner could be analyzed and assessed through the tracking information. Then the corresponding instruction will give to the learner in order to improve the learner's learning efficiency. The parameters assessment is achieved using EM algorithm and the assessment results in this system reveal the pretty accuracy to the real testing situation",,1-4244-0164-X1-4244-0165,10.1109/CSCWD.2006.253194,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4019230,E-Learning;Bayesian networks;multi-agent systems,Electronic learning;Bayesian methods;Multiagent systems;Collaborative work;Information analysis;Intelligent agent;Educational institutions;Blindness;System testing;Computer aided instruction,belief networks;computer aided instruction;multi-agent systems,Bayesian networks;e-learning assessing and instructing system;multi-agent systems;EM algorithm,,2,10,,,,,,IEEE,IEEE Conferences
Applying CMMI and Strategy to ATE Development,S. T. Stevens,"V I Engineering, Inc., 27300 Haggerty Rd., # F10, Farmington Hills, MI 48331. (248)489-1200, sstevens@viengineering.com",2006 IEEE Autotestcon,,2006,,,813,818,"This paper provides a viewpoint of the capability maturity model integration (CMMI<sup>SM</sup> ) from the perspective of automated test equipment (ATE) development and test engineering. ATE development is a specialized segment of product development and shares many of the same issues. Requirements for the test equipment are very dependent on continually evolving product characteristics. Even with the best planning, lead times for ATE development are typically eroded by late changes to product requirements and designs, and eventually the critical path leads right through test! Without a solid process foundation, chaos ensues. The CMMI process models provide a framework for the integration of best practices in many disciplines. Portions of the systems engineering, software engineering, Integrated Product and Process Development and Supplier Sourcing models all offer important perspectives which affect ATE developers. This paper focuses on the CMMI processes and best practices which yield the greatest impact to test organizations and groups that provide ATE. The overall Test Strategy should help prioritize the process areas which deserve the most attention. Mature ATE organizations use a Balanced Scorecard approach to provide alignment with corporate and program level goals. Metrics monitor their progress against their corporate goals. At the program level, they apply a risk- driven approach to selectively apply resources that achieve the highest ROI for test dollars. From this business-oriented vantage point, organizations are likely to see increased efficiencies that will decrease overall system development costs by streamlining the testing component of their budgets.",1088-7725;1558-4550,1-4244-0052-X1-4244-0051,10.1109/AUTEST.2006.283769,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062482,,Capability maturity model;Test equipment;Best practices;Samarium;Automatic testing;Product development;Path planning;Product design;Solids;Chaos,automatic test equipment;investment;software engineering;systems engineering,automated test equipment;ATE development;capability maturity model integration;test engineering;product requirements;systems engineering;software engineering;integrated product;process development;supplier sourcing models;test strategy;balanced scorecard;return on investment;business-oriented vantage point,,3,9,,,,,,IEEE,IEEE Conferences
DDP: a tool for life-cycle risk management,S. L. Cornford; M. S. Feather; K. A. Hicks,"Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA; Jet Propulsion Lab., California Inst. of Technol., Pasadena, CA, USA",IEEE Aerospace and Electronic Systems Magazine,,2006,21,6,13,22,"At JPL we have developed and implemented a process for achieving life-cycle risk management. This process has been embodied in a software tool and is called defect detection and prevention (DDP). The DDP process can be succinctly stated as: determine where we want to be, what could get in the way and how we will get there. The ""determine where we want to be"" is captured as trees of requirements and the ""what could get in the way"" is captured as trees of potential failure modes. Scoring the impacts of these failure modes on the requirements results in a prioritized set of failure modes. The user then selects from a set of preventative measures, analyses, process controls and tests (PACTs) each of which has an effectiveness versus the various failure modes. It is the goal of the DDP process to optimally select the subset of the PACTs which minimizes the residual risk subject to the project resource constraints. The DDP process is intended to facilitate risk management over the entire project life cycle beginning with architectural and advanced technology decisions all the way through operation. As the project design, technology content, and implementation approach matures, the requirements and failure mode trees are elaborated upon to accommodate the additional information. Thus, the DDP process is a systematic, continuous, top-down approach to managing risk. Implementation of the DDP process requires a critical mass of expertise (usually the project team and a few specialists) and captures both their engineering judgement as well as available quantitative data. This additional data may result from models, layouts, prototype testing, other focused risk evaluations and institutional experiences. The DDP process also identifies areas where additional information would be advantageous, thus allowing a project to target critical areas of risk or risk uncertainty. This also allows the project to identify those areas which would benefit the most from application of other quantitative tools and methods (e.g. Monte Carlo simulations, FMECAs, fault trees). The software tool supports the DDP process by providing guidance for implementing the process steps, graphical visualizations of the various trees, their interrelationships and the current risk landscape. The tool is capable of supporting on-the-fly knowledge elicitation as well as integrating off-line deliberations. There are a variety of available outputs including graphs, trees and reports as well as clear identification of the driving requirements, ""tall-pole"" residual risks and the PACTs which have been selected and agreed upon. The DDP process has been applied at various levels of assembly including the system and subsystem levels, as well as down to the component level. Recently, significant benefits have been realized from application to advanced technologies, where the focus has been on increasing the infusion rates of these technologies by identification and mitigation of risks prior to delivery to a project",0885-8985;1557-959X,,10.1109/MAES.2006.1662004,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1662004,,Risk management;Software tools;Testing;Tree graphs;Failure analysis;Process control;Data engineering;Design engineering;Prototypes;Uncertainty,aerospace computing;failure analysis;project management;risk management;software tools,life-cycle risk management;software tool;defect detection and prevention;failure modes;preventative measures;process controls;project resource constraints;project design;technology content;risk uncertainty;on-the-fly knowledge elicitation,,4,8,,,,,,IEEE,IEEE Journals & Magazines
"Expert System for Quality Cost Planning, Monitoring and Control",S. Brad; M. Fulea; B. Mocan,"Technical University of Cluj-Napoca, 15 Ctin Daicoviciu Street, Cluj-Napoca, 400020; Technical University of Cluj-Napoca, 15 Ctin Daicoviciu Street, Cluj-Napoca, 400020; Technical University of Cluj-Napoca, 15 Ctin Daicoviciu Street, Cluj-Napoca, 400020","2006 IEEE International Conference on Automation, Quality and Testing, Robotics",,2006,2,,53,58,"On the market, there are some commercial available software tools for quality cost management. However, these tools do not incorporate specialized agents for handling complex tasks related to the current needs in quality cost planning, like interpretation of the results over a horizon of time and automatic generation of reliable guidelines to prioritize resources in order to improve the quality of the business processes. This paper is going to introduce the results of some researches performed by the authors in designing and developing an expert system for comprehensive monitoring, controlling and planning of quality costs within business processes. Results are already successful implemented in a large enterprise from chemical industry",,1-4244-0360-X1-4244-0361,10.1109/AQTR.2006.254599,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4022922,,Expert systems;Costs;Monitoring;Control systems;Automatic control;Process planning;Business;Software tools;Quality management;Guidelines,business data processing;costing;expert systems;quality control,expert system;quality cost planning;quality cost monitoring;quality cost control;business processes,,1,17,,,,,,IEEE,IEEE Conferences
How International Standards Such as ATML and IEEE 1641 STD can Make the Realisation of an Open System Architecture on a Common Test Platform a Reality,C. Gorringe,"MIEEE, EADS Test & Services (UK) Ltd., 29-31 Cobham Road, Wimborne, Dorset BH21 7PF, +44 (0)1202 872800, chris.gorringe@eads-ts.com",2006 IEEE Autotestcon,,2006,,,731,738,"A perspective on how the DoD and MoD are integrating open standards into their ATS frameworks and policy in the search for a common test platform architecture solution for use on all test platforms. The paper examines the two approaches being taken and draws on their commonality to propose how open standards can help meet both their aims and circumstances. Benefits such as TPS interoperability, re-host and re-use are examined and contrasted for open systems versus common architecture to identify the practical implication for real systems. The life cycle cost of support for the system is identified and the trade-off in cost between fast optimal TPSs and fully interoperable TPSs is considered. The paper goes on to show the difference between using information models utilizing a development process versus the use of run time interfaces and how they can lead to different solutions to the same basic problem but with different peripheral benefits. In conclusion an approach to maximize benefit between the two framework groups is considered whilst maintaining individual priorities.",1088-7725;1558-4550,1-4244-0052-X1-4244-0051,10.1109/AUTEST.2006.283756,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4062469,,Open systems;System testing;Automatic testing;Software testing;Cost function;Logistics;Computer industry;Defense industry;Software standards;Standards organizations,automatic test equipment;IEEE standards;military systems;software architecture,international standards;ATML;IEEE 1641 STD;open system architecture;DoD;MoD;ATS frameworks;common test platform architecture;life cycle support cost;run time interfaces,,,2,,,,,,IEEE,IEEE Conferences
Improving Testing Efficiency using Cumulative Test Analysis,I. Holden; D. Dalton,"IBM, UK; NA",Testing: Academic & Industrial Conference - Practice And Research Techniques (TAIC PART'06),,2006,,,152,158,"It can be impossible to thoroughly test complex software projects with a large library of tests to be run in many environments and configurations. The cumulative test analysis (CTA) technique described reduces the time to find defects by prioritising and minimising the testing. Tests are chosen to target the product areas having the highest risk of defects. Test effectiveness, test code coverage, product code changes and changes to dependencies are monitored and analysed to prioritise the testing. Test results from build to build are accumulated. Build reports clearly identify areas at risk, test results, and the tests that must be run. Experiences with a prototype tool are discussed and conclusions drawn from the use of CTA show that defects are found sooner, more time is available for writing new tests and the focus of test execution moves towards product quality instead of simply test results",,0-7695-2672,10.1109/TAIC-PART.2006.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691682,,Software testing;Product codes;Monitoring;Prototypes;Software libraries;Writing;Performance evaluation;Software prototyping;Software tools;Feedback,program testing;project management;risk management;software management,test efficiency improvement;cumulative test analysis;software project testing;CTA technique;risk areas,,,5,,,,,,IEEE,IEEE Conferences
On the Use of Mutation Faults in Empirical Assessments of Test Case Prioritization Techniques,Hyunsook Do; G. Rothermel,"Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE 68588-0115; Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE 68588-0115",IEEE Transactions on Software Engineering,,2006,32,9,733,752,"Regression testing is an important activity in the software life cycle, but it can also be very expensive. To reduce the cost of regression testing, software testers may prioritize their test cases so that those which are more important, by some measure, are run earlier in the regression testing process. One potential goal of test case prioritization techniques is to increase a test suite's rate of fault detection (how quickly, in a run of its test cases, that test suite can detect faults). Previous work has shown that prioritization can improve a test suite's rate of fault detection, but the assessment of prioritization techniques has been limited primarily to hand-seeded faults, largely due to the belief that such faults are more realistic than automatically generated (mutation) faults. A recent empirical study, however, suggests that mutation faults can be representative of real faults and that the use of hand-seeded faults can be problematic for the validity of empirical results focusing on fault detection. We have therefore designed and performed two controlled experiments assessing the ability of prioritization techniques to improve the rate of fault detection of test case prioritization techniques, measured relative to mutation faults. Our results show that prioritization can be effective relative to the faults considered, and they expose ways in which that effectiveness can vary with characteristics of faults and test suites. More importantly, a comparison of our results with those collected using hand-seeded faults reveals several implications for researchers performing empirical studies of test case prioritization techniques in particular and testing techniques in general",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2006.92,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1707670,Regression testing;test case prioritization;program mutation;empirical studies.,Genetic mutations;Computer aided software engineering;Fault detection;Software testing;System testing;Life testing;Software maintenance;Performance evaluation;Costs;Software measurement,program diagnostics;program testing,regression testing;software life cycle;test case prioritization technique;fault detection;hand-seeded fault;mutation fault,,97,41,,,,,,IEEE,IEEE Journals & Magazines
Optimizing the selection of representative configurations in verification of evolving product lines of distributed embedded systems,K. D. Scheidemann,"BMW Car IT GmbH, Munchen, Germany",10th International Software Product Line Conference (SPLC'06),,2006,,,75,84,"Electronics and computer science play a more and more prominent role in automotive technology. In the future the prevalence of those new technologies and the customers' demand for individuality leads to tremendously large configuration spaces of vehicle control systems. To cope with the resulting complexity in verification, new strategies need to be explored. One likely future challenge is to determine a set of vehicle configurations, such that the successful verification of this small set implies the correctness of the entire product family. This paper presents a method to address this task, based on exploiting communalities in architecture and requirements. We introduce efficient algorithms with provable quality guarantees for the optimization problems of choosing the minimum set of configurations necessary to verify all possible configurations and choosing the best k configurations to maximize the verification coverage of the entire product family. We discuss extensions of our method which allow requirement priorities and the consideration of configuration costs, and present a technique for automatically determining communalities in architecture and requirements which can be exploited by our optimization methods. We demonstrate the effectiveness of our method on an indicator light system product family. In this example a configuration reduction by 60% can be achieved",,0-7695-2599,10.1109/SPLINE.2006.1691579,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1691579,,Embedded system;Space technology;Control systems;Space vehicles;System testing;Automotive engineering;Computer architecture;Production;Hardware;Computer science,automobiles;automotive electronics;configuration management;control engineering computing;embedded systems;program testing;program verification,representative configurations;evolving product lines;distributed embedded system;automotive technology;vehicle control system;vehicle configurations;verification coverage,,4,15,,,,,,IEEE,IEEE Conferences
Prioritizing Software Inspection Results using Static Profiling,C. Boogerd; L. Moonen,"Delft University of Technology, The Netherlands; Delft University of Technology, The Netherlands",2006 Sixth IEEE International Workshop on Source Code Analysis and Manipulation,,2006,,,149,160,"Static software checking tools are useful as an additional automated software inspection step that can easily be integrated in the development cycle and assist in creating secure, reliable and high quality code. However, an often quoted disadvantage of these tools is that they generate an overly large number of warnings, including many false positives due to the approximate analysis techniques. This information overload effectively limits their usefulness. In this paper we present ELAN, a technique that helps the user prioritize the information generated by a software inspection tool, based on a demand-driven computation of the likelihood that execution reaches the locations for which warnings are reported. This analysis is orthogonal to other prioritization techniques known from literature, such as severity levels and statistical analysis to reduce false positives. We evaluate feasibility of our technique using a number of case studies and assess the quality of our predictions by comparing them to actual values obtained by dynamic profiling.",,0-7695-2353,10.1109/SCAM.2006.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4026864,,Inspection;Software quality;Software tools;Statistical analysis;Documentation;Costs;Software testing;Security;Embedded system;Economic forecasting,,,,18,41,,,,,,IEEE,IEEE Conferences
Prognostics usefulness criteria,J. K. Line; N. S. Clements,"Lockheed Martin Aeronaut., Fort Worth, TX, USA; Lockheed Martin Aeronaut., Fort Worth, TX, USA",2006 IEEE Aerospace Conference,,2006,,,7 pp.,,"Prognostics and health management (PHM) can provide remarkable insight for maintenance management of large systems, but must be implemented with a healthy respect for the end user and a practical view of the hardware and software capabilities. The F-35 Joint Strike Fighter (JSF) program is implementing a comprehensive PHM system to maximize the supportability of the air system. The prognostic algorithms must have a defined minimum capability to aid implementation and verification. However, the complexity of the air system precludes creating exact requirements for remaining useful life and confidence. Instead, ""usefulness criteria"" are created to link the user need with the minimum capability of the algorithm. The usefulness criteria are a list of goals related to aircraft supportability which can be used to define the minimum acceptable time to maintenance indication for the prognostic algorithm. The goals in the usefulness criteria were applied to each prognostic algorithm in the F-35 PHM system. When assigned, these usefulness criteria provide a means to measure the improved performance of the aircraft and fleet maintenance as well as prioritize the implementation of the prognostic algorithms. The development and implementation of the algorithms in relation to these usefulness criteria are still in process, but it is expected that most algorithms exceeds the criteria. Those which do not meet the criteria are re-evaluated with a trade study to determine if further efforts in hardware and software development are warranted. This process of usefulness criteria development and application can be rigorously applied to the development of any PHM system. This paper covers the development of the usefulness criteria for the F-35 program and the implementation results to date",1095-323X,0-7803-9545,10.1109/AERO.2006.1656123,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656123,,Prognostics and health management;Algorithm design and analysis;Software algorithms;Postal services;Hardware;Aircraft;Vehicles;Monitoring;Propulsion;Software testing,aircraft maintenance;program verification;remaining life assessment;software engineering,prognostics usefulness criteria;prognostics and health management;maintenance management;F-35 Joint Strike Fighter;prognostic algorithms;aircraft supportability;maintenance indication;aircraft maintenance;fleet maintenance,,1,5,,,,,,IEEE,IEEE Conferences
"Recorders, Reasoners and Artificial Intelligence - Integrated Diagnostics on Military Transport Aircraft",B. Chidambaram; P. Pigg; P. L. Horn; M. A. Talbot; D. D. Gilbertson; K. C. Cerise,"5301 Bolsa Ave., MC H013-B319, Huntington Beach, CA 92647, 714-896-7017, balaguruna.chidambaram@boeing.com; NA; NA; NA; NA; NA",2006 IEEE Aerospace Conference,,2006,,,1,9,"A research group at Boeing has developed a prototype for an integrated diagnostic system to optimize maintenance on military transport aircraft by decreasing maintenance costs and increasing aircraft availability. The integrated diagnostic system comprises an on-board recorder, and a ground-based reasoner that analyzes the recorded data, to optimize maintenance. The functions of the ground-based reasoner (GBR) include identification of root-cause, filtering of false alarms, and prioritization of maintenance actions. The technologies used include expert systems/fast state recognition methods, data mining technologies and Bayesian analyses. The ground-based reasoner provides an open plug-n-play software framework for incorporating these technologies into a software tool, for field maintenance. The tool has a simple, intuitive graphic user interface that is designed to help the end-user, the maintenance technician, with everyday maintenance tasks. The integrated diagnostic system prototype is currently undergoing testing on pre-delivery test flights for the C-17 military transport, at a Boeing facility, and initial results in applying the system to the aerial delivery subsystem and the hydraulic subsystem are discussed",1095-323X,0-7803-9545,10.1109/AERO.2006.1656074,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1656074,,Military aircraft;Artificial intelligence;Prototypes;Software tools;System testing;Cost function;Data analysis;Filtering;Diagnostic expert systems;Data mining,aerial equipment;artificial intelligence;Bayes methods;data mining;diagnostic reasoning;hydraulic systems;military aircraft;military computing;recorders,artificial intelligence;military transport aircraft;integrated diagnostic system;maintenance costs;aircraft availability;on-board recorder;ground-based reasoner;root-cause identification;false alarm filtering;maintenance action prioritization;expert systems recognition methods;fast state recognition methods;data mining technologies;Bayesian analyses;open plug-n-play software framework;software tool;field maintenance;graphic user interface;maintenance technician;maintenance tasks;pre-delivery test flights;C-17 military transport;aerial delivery subsystem;hydraulic subsystem,,1,4,,,,,,IEEE,IEEE Conferences
Search Based Approaches to Component Selection and Prioritization for the Next Release Problem,P. Baker; M. Harman; K. Steinhofel; A. Skaliotis,"Motorola Labs, Viables Estate, UK; Motorola Labs, Viables Estate, UK; Motorola Labs, Viables Estate, UK; Motorola Labs, Viables Estate, UK",2006 22nd IEEE International Conference on Software Maintenance,,2006,,,176,185,This paper addresses the problem of determining the next set of releases in the course of software evolution. It formulates both ranking and selection of candidate software components as a series of feature subset selection problems to which search based software engineering can be applied. The approach is automated using greedy and simulated annealing algorithms and evaluated using a set of software components from the component base of a large telecommunications organization. The results are compared to those obtained by a panel of (human) experts. The results show that the two automated approaches convincingly outperform the expert judgment approach,1063-6773,0-7695-2354,10.1109/ICSM.2006.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021335,,Simulated annealing;Software engineering;Educational institutions;Software algorithms;Robustness;Humans;Programming;Software systems;Software testing;Planing,greedy algorithms;object-oriented programming;simulated annealing;software maintenance;software selection,software component selection;software component prioritization;next release problem;search based software engineering;software evolution;software ranking;feature subset selection problem;greedy algorithm;simulated annealing algorithm;large telecommunications organization,,38,19,,,,,,IEEE,IEEE Conferences
Test Case Prioritization Using Relevant Slices,D. Jeffrey; N. Gupta,"The University of Arizona, USA; The University of Arizona, USA",30th Annual International Computer Software and Applications Conference (COMPSAC'06),,2006,1,,411,420,"Software testing and retesting occurs continuously during the software development lifecycle to detect errors as early as possible. The sizes of test suites grow as software evolves. Due to resource constraints, it is important to prioritize the execution of test cases so as to increase chances of early detection of faults. Prior techniques for test case prioritization are based on the total number of coverage requirements exercised by the test cases. In this paper, we present a new approach to prioritize test cases based on the coverage requirements present in the relevant slices of the outputs of test cases. We present experimental results comparing the effectiveness of our prioritization approach with that of existing techniques that only account for total requirement coverage, in terms of ability to achieve high rate of fault detection. Our results present interesting insights into the effectiveness of using relevant slices for test case prioritization",0730-3157;0730-3157,0-7695-2655,10.1109/COMPSAC.2006.80,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4020103,,Software testing;Fault detection;Computer science;Programming;Life testing;Computer errors;Time factors,program testing;software fault tolerance,test case prioritization;software testing;software development lifecycle;fault detection,,41,17,,,,,,IEEE,IEEE Conferences
The Incremental Evolution of Attack Agents in Xpilot,G. B. Parker; M. Parker,"Computer Science, Connecticut College, parker@conncoll.edu; NA",2006 IEEE International Conference on Evolutionary Computation,,2006,,,969,975,"In the research presented in this paper, we use incremental evolution to learn multifaceted neural network (NN) controllers for agents operating in the space game Xpilot. Behavioral components specific to the accomplishment of specific tasks, such as bullet-dodging, shooting, and closing on an enemy, are learned in the first increment. These behavioral components are used in the second increment to evolve a NN that prioritizes the output of a two-layer NN depending on that agent's current situation.",1089-778X;1941-0026,0-7803-9487,10.1109/CEC.2006.1688415,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1688415,,Neural networks;System testing;Humans;Artificial intelligence;Intelligent networks;Control systems;Autonomous agents;Genetic algorithms;Artificial neural networks;Open source software,computer games;evolutionary computation;learning (artificial intelligence);neural nets;software agents,incremental evolution;attack agents;Xpilot;multifaceted neural network;space game,,2,7,,,,,,IEEE,IEEE Conferences
Towards Interactive Fault Localization Using Test Information,D. Hao; L. Zhang; H. Mei; J. Sun,Peking University; Peking University; Peking University; Peking University,2006 13th Asia Pacific Software Engineering Conference (APSEC'06),,2006,,,277,284,"Finding the location of a fault is a central task of debugging. Typically, a developer employs an interactive process for fault localization. To accelerate this task, several approaches have been proposed to automate fault localization. In practice, testing-based fault localization (TBFL), which uses test information to locate faults, has become a research focus. However, experimental results reported in the literature showed that current automation of fault localization can only serve as a means to confirming the search space and prioritizing search sequences, not a substitute of the interactive fault localization process. In this paper, we propose an approach based on test information to support the entire interactive fault localization process. During this process, the information gathered from previous interaction steps can be used to provide the ranking of suspicious statements for the current interaction step. As a feasibility study of our approach, we performed an experiment on applying our approach together with some other TBFL approaches on the Siemens programs, which have been used in the literature. Our experimental results show the effectiveness of our approach.",1530-1362;1530-1362,0-7695-2685,10.1109/APSEC.2006.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4137428,,Software testing;Humans;Electronic equipment testing;System testing;Software systems;Sun;Software debugging;Computer science;Acceleration;Automation,program debugging;program testing;software engineering,interactive fault localization;test information;debugging;testing-based fault localization;Siemens programs,,2,28,,,,,,IEEE,IEEE Conferences
Using the Case-Based Ranking Methodology for Test Case Prioritization,P. Tonella; P. Avesani; A. Susi,"ITC-irst, Trento, Italy; ITC-irst, Trento, Italy; ITC-irst, Trento, Italy",2006 22nd IEEE International Conference on Software Maintenance,,2006,,,123,133,"The test case execution order affects the time at which the objectives of testing are met. If the objective is fault detection, an inappropriate execution order might reveal most faults late, thus delaying the bug fixing activity and eventually the delivery of the software. Prioritizing the test cases so as to optimize the achievement of the testing goal has potentially a positive impact on the testing costs, especially when the test execution time is long. Test engineers often possess relevant knowledge about the relative priority of the test cases. However, this knowledge can be hardly expressed in the form of a global ranking or scoring. In this paper, we propose a test case prioritization technique that takes advantage of user knowledge through a machine learning algorithm, case-based ranking (CBR). CBR elicits just relative priority information from the user, in the form of pairwise test case comparisons. User input is integrated with multiple prioritization indexes, in an iterative process that successively refines the test case ordering. Preliminary results on a case study indicate that CBR overcomes previous approaches and, for moderate suite size, gets very close to the optimal solution",1063-6773,0-7695-2354,10.1109/ICSM.2006.74,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4021329,,Software testing;System testing;Fault detection;Machine learning algorithms;Application software;Automatic testing;Humans;Delay;Software debugging;Cost function,learning (artificial intelligence);program testing,case-based ranking;test case prioritization;test case execution order;fault detection;software testing;machine learning;test case ordering,,20,16,,,,,,IEEE,IEEE Conferences
A histogram-matching approach to the evolution of bin-packing strategies,R. Poli; J. Woodward; E. K. Burke,"Department of Computer Science, University of Essex, UK; Department of Computer Science and IT, University of Nottingham, UK; Department of Computer Science and IT, University of Nottingham, UK",2007 IEEE Congress on Evolutionary Computation,,2007,,,3500,3507,"We present a novel algorithm for the one- dimension offline bin packing problem with discrete item sizes based on the notion of matching the item-size histogram with the bin-gap histogram. The approach is controlled by a constructive heuristic function which decides how to prioritise items in order to minimise the difference between histograms. We evolve such a function using a form of linear register-based genetic programming system. We test our evolved heuristics and compare them with hand-designed ones, including the well- known best fit decreasing heuristic. The evolved heuristics are human-competitive, generally being able to outperform high- performance human-designed heuristics.",1089-778X;1941-0026,978-1-4244-1339-3978-1-4244-1340,10.1109/CEC.2007.4424926,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424926,,Heuristic algorithms;Humans;Genetic programming;Algorithm design and analysis;Histograms;Computer science;Genetic algorithms;Testing;Computer industry;Application software,bin packing;genetic algorithms,histogram-matching approach;bin-packing strategies evolution;one-dimension offline bin packing problem;bin-gap histogram;constructive heuristic function;linear register-based genetic programming system,,13,11,,,,,,IEEE,IEEE Conferences
A model to predict anti-regressive effort in Open Source Software,A. Capiluppi; J. Fernandez-Ramil,"Centre of Research on Open Source Software Department of Computing and Informatics, University of Lincoln, UK, acapiluppi@lincoln.ac.uk; Computing Department, The Open University, UK, j.f.ramil@open.ac.uk",2007 IEEE International Conference on Software Maintenance,,2007,,,194,203,"Accumulated changes on a software system are not uniformly distributed: some elements are changed more often than others. For optimal impact, the limited time and effort for complexity control, called anti-regressive work, should be applied to the elements of the system which are frequently changed and are complex. Based on this, we propose a maintenance guidance model (MGM) which is tested against real-world data. MGM takes into account several dimensions of complexity: size, structural complexity and coupling. Results show that maintainers of the eight open source systems studied tend, in general, to prioritize their anti-regressive work in line with the predictions given by our MGM, even though, divergences also exist. MGM offers a history-based alternative to existing approaches to the identification of elements for anti-regressive work, most of which use static code characteristics only.",1063-6773,978-1-4244-1255-6978-1-4244-1256,10.1109/ICSM.2007.4362632,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362632,ANTI-REGRESSIVE WORK;COUPLING;EMPIRICAL STUDIES;MCCABE CYCLOMATIC COMPLEXITY;MAINTENANCE;METRICS;OPEN SOURCE;SoFTWARE EVOLUTION,Predictive models;Open source software;Software systems;Informatics;Optimal control;Control systems;Software testing;System testing;Documentation;Knowledge management,public domain software;software metrics,open source software;complexity control;antiregressive work;maintenance guidance model,,2,28,,,,,,IEEE,IEEE Conferences
A Multi-Agent Solution to Distribution Systems Restoration,J. M. Solanki; S. Khushalani; N. N. Schulz,NA; NA; NA,IEEE Transactions on Power Systems,,2007,22,3,1026,1034,"The goal to provide faster and faster restoration after a fault is pushing the technical envelope related to new algorithms. While many approaches use centralized strategies, the concept of multi-agent systems (MAS) is creating a new option related to distributed analyses for restoration. This paper provides details on a MAS that restores a power system after a fault. The development of agents and behaviors of the agents are described, including communication of agents. The MAS is tested on two test systems and facilitates both full and partial restoration, including load prioritization and shedding.",0885-8950;1558-0679,,10.1109/TPWRS.2007.901280,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4282055,Decentralized solution;intelligent agents;multi-agent system (MAS),Power system restoration;Intelligent agent;Multiagent systems;Power system faults;System testing;Switches;Knowledge based systems;Software agents;Power system analysis computing;Automation,load shedding;multi-agent systems;power distribution faults;power engineering computing;power system restoration,distribution systems restoration;multiagent systems;partial restoration;load prioritization;load shedding,,173,21,,,,,,IEEE,IEEE Journals & Magazines
A Multipurpose Code Coverage Tool for Java,R. Lingampally; A. Gupta; P. Jalote,"Indian Institute of Technology, India; Indian Institute of Technology, India; Indian Institute of Technology, India",2007 40th Annual Hawaii International Conference on System Sciences (HICSS'07),,2007,,,261b,261b,"Most test coverage analyzers help in evaluating the effectiveness of testing by providing data on statement and branch coverage achieved during testing. If made available, the coverage information can be very useful for many other related activities, like, regression testing, test case prioritization, test-suite augmentation, test-suite minimization, etc. In this paper, we present a Java-based tool JavaCodeCoverage for test coverage reporting. It supports testing and related activities by recording the test coverage for various code-elements and updating the coverage information when the code being tested is modified. The tool maintains the test coverage information for a set of test cases on individual as well as test suite basis and provides effective visualization for the same. Open source database support of the tool makes it very useful for software testing research",1530-1605,0-7695-2755,10.1109/HICSS.2007.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4076910,,Java;Software testing;System testing;Performance evaluation;Visualization;Visual databases;Binary codes;Costs;Fault detection;Information analysis,Java;program testing,multipurpose code coverage tool;Java;branch coverage;software testing;regression testing;test case prioritization;test-suite augmentation;test-suite minimization;test coverage reporting;open source database,,6,32,,,,,,IEEE,IEEE Conferences
A Novel Approach of Prioritizing Use Case Scenarios,D. Kundu; D. Samanta,NA; NA,14th Asia-Pacific Software Engineering Conference (APSEC'07),,2007,,,542,549,"Modern softwares are very large and complex. As the size and complexity of software increases, software developers feel an urgent need for a better management of different activities during the course of software development. In this paper, we present an approach of use case scenario prioritization suitable for project planning at an early phase of the software development. We consider only use case model in our work. For prioritization, we focus on how critical a scenario path is, which essentially depends on density of overlapping of sub path of a scenario path with other scenario path(s) of a use case. Our proposed approach provides an analytical solution on use case scenario prioritization and is very much effective in project management related activities as substantiated by our experimental results.",1530-1362;1530-1362,0-7695-3057,10.1109/ASPEC.2007.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4425898,Use case model;UML;software quality;scenario prioritization;project management,Programming;Unified modeling language;Software quality;Software engineering;Information technology;Project management;Computer industry;Software testing;Software development management;Documentation,formal specification;project management;software cost estimation;software development management;software metrics;software prototyping;software quality;Unified Modeling Language,use case scenario prioritization;software complexity;software project planning;software development life cycle;software project management;software quality metric;software cost metric;UML use case model,,6,25,,,,,,IEEE,IEEE Conferences
A process framework for customising software quality models,Mbusi Sibisi; C. C. van Waveren,"Research and Development Department of Altech UEC Technologies (Pty) Ltd, South Africa; Department of Engineering and Technology Management at the University of Pretoria, South Africa",AFRICON 2007,,2007,,,1,8,The quality objective of many software organisations is to deliver software products that meet and or exceed customer expectations. The key to achieving this is to capture these expectations at the beginning of the project by clearly defining all quality requirements. The characteristics particularly defined in ISO/IEC 9126-1 (2001) provide the framework for specifying quality requirements. The ISO/IEC 9126-1 quality model is intended to be applicable to any type of software product or intermediate product. Before application this model needs to be tailored to a specific software and specific need. Since these characteristics cannot be directly measured this makes it difficult to directly prioritise and choose the most relevant characteristics and sub-characteristics. Hence a process framework that will link these characteristics and sub- characteristics to user needs is required. This will in turn help customise software quality models like ISO/IEC 9126-1 (2001) and other general software quality models. A process framework for customising software quality models is proposed in the text and it is further shown how this framework was applied in a real working environment in an attempt to quantitatively validate it. The results collected in the study showed that the framework could be used reliably in customising a generic software quality model at characteristic level only. The deviations at sub-characteristic level were due to unclear questions in the generated Generic Quality Questionnaire that resulted in misunderstandings. And the metrics used to create these questions were not fully tested for validity and reliability due to time constraints. Enhancements are discussed in the study and it is further shown how reliability can also be achieved at sub-characteristic level.,2153-0025;2153-0033,978-1-4244-0986-0978-1-4244-0987,10.1109/AFRCON.2007.4401495,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4401495,Software Quality;Software Quality Models;Software Measurement;Software Metrics;Software Quality Standards,Software quality;ISO standards;IEC standards;Software measurement;Software standards;Standards publication;Design engineering;Africa;Application software;Testing,formal specification;software metrics;software process improvement;software quality,process framework;software quality model;software product;customer expectation;software quality requirement specification,,5,18,,,,,,IEEE,IEEE Conferences
A Visualization Framework for Web Service Discovery and Selection Based on Quality of Service,F. F. Chua; H. Yuan; S. D. Kim,NA; NA; NA,The 2nd IEEE Asia-Pacific Service Computing Conference (APSCC 2007),,2007,,,312,319,"The visualization of Web service execution process is an emerging research in service-oriented computing (SOC) area. This paper presents a practical visualization framework in putting service discovery and selection process based on quality of service (QoS) attributes into a visual context. The proposed practical architecture serves as a foundation for designing the novel GUIs for different users. By considering the preferences and priorities for service consumers and service providers, we adopt different application UI design criteria and design patterns which tailored to service-based visualization design. Based on that, evaluation is carried out to test the usability, effectiveness and acceptability of the proposed visualization framework.",,0-7695-3051,10.1109/APSCC.2007.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4414476,,Web services;Quality of service;Data visualization;Service oriented architecture;Application software;Graphical user interfaces;Context-aware services;Computer architecture;Displays;Navigation,data visualisation;quality of service;Web services,visualization framework;Web service discovery;quality of service;Web service execution process;service-oriented computing;QoS,,3,8,,,,,,IEEE,IEEE Conferences
Applying Interface-Contract Mutation in Regression Testing of Component-Based Software,S. Hou; L. Zhang; T. Xie; H. Mei; J. Sun,"Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, Beijing, 100871, China, houss@sei.pku.edu.cn; Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, Beijing, 100871, China, zhanglu@sei.pku.edu.cn; Department of Computer Science, North Carolina State University, Raleigh, NC 27695, xie@csc.ncsu.edu; Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, Beijing, 100871, China, meih@sei.pku.edu.cn; Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China; Key laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, Beijing, 100871, China, sjs@sei.pku.edu.cn",2007 IEEE International Conference on Software Maintenance,,2007,,,174,183,"Regression testing, which plays an important role in software maintenance, usually relies on test adequacy criteria to select and prioritize test cases. However, with the wide use and reuse of black-box components, such as reusable class libraries and COTS components, it is challenging to establish test adequacy criteria for testing software systems built on components whose source code is not available. Without source code or detailed documents, the misunderstanding between the system integrators and component providers has become a main factor of causing faults in component-based software. In this paper, we apply mutation on interface contracts, which can describe the rights and obligations between component users and providers, to simulate the faults that may occur in this way of software development. The mutation adequacy score for killing the mutants of interface contracts can serve as a test adequacy criterion. We performed an experimental study on three subject systems to evaluate the proposed approach together with four other existing criteria. The experimental results show that our adequacy criterion is helpful for both selecting good-quality test cases and scheduling test cases in an order of exposing faults quickly in regression testing of component-based software.",1063-6773,978-1-4244-1255-6978-1-4244-1256,10.1109/ICSM.2007.4362630,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362630,,Genetic mutations;Software testing;Contracts;Electronic equipment testing;Software maintenance;System testing;Computer science;Software libraries;Software systems;Programming,object-oriented programming;program testing;software maintenance;software packages;software quality,interface-contract mutation;regression testing;component-based software;software maintenance;black-box component;COTS component;software system testing;source code;software development,,7,35,,,,,,IEEE,IEEE Conferences
Combinatorial Interaction Regression Testing: A Study of Test Case Generation and Prioritization,X. Qu; M. B. Cohen; K. M. Woolf,"Department of Computer Science and Engineering, University of Nebraska-Lincoln, xqu@cse.unl.edu; Department of Computer Science and Engineering, University of Nebraska-Lincoln, myra@cse.unl.edu; Department of Computer Science and Engineering, University of Nebraska-Lincoln, kwoolf@cse.unl.edu",2007 IEEE International Conference on Software Maintenance,,2007,,,255,264,"Regression testing is an expensive part of the software maintenance process. Effective regression testing techniques select and order (or prioritize) test cases between successive releases of a program. However, selection and prioritization are dependent on the quality of the initial test suite. An effective and cost efficient test generation technique is combinatorial interaction testing, CIT, which systematically samples all t-way combinations of input parameters. Research on CIT, to date, has focused on single version software systems. There has been little work that empirically assesses the use of CIT test generation as the basis for selection or prioritization. In this paper we examine the effectiveness of CIT across multiple versions of two software subjects. Our results show that CIT performs well in finding seeded faults when compared with an exhaustive test set. We examine several CIT prioritization techniques and compare them with a re-generation/prioritization technique. We find that prioritized and re-generated/prioritized CIT test suites may find faults earlier than unordered CIT test suites, although the re-generated/prioritized test suites sometimes exhibit decreased fault detection.",1063-6773,978-1-4244-1255-6978-1-4244-1256,10.1109/ICSM.2007.4362638,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362638,,Software testing;Fault detection;System testing;Life testing;Software systems;Computer science;Software maintenance;Costs;Performance evaluation;Software quality,configuration management;program diagnostics;program testing;software fault tolerance;software maintenance;software quality,combinatorial interaction regression testing;test case generation;software maintenance;test case prioritization;software quality;software version;fault detection,,44,19,,,,,,IEEE,IEEE Conferences
Compatibility and Regression Testing of COTS-Component-Based Software,L. Mariani; S. Papagiannakis; M. Pezze,"Universita degli studi di Milano Bicocca, Italy; Universita degli studi di Milano Bicocca, Italy; Universita degli studi di Milano Bicocca, Italy",29th International Conference on Software Engineering (ICSE'07),,2007,,,85,95,"Software engineers frequently update COTS components integrated in component-based systems, and can often chose among many candidates produced by different vendors. This paper tackles both the problem of quickly identifying components that are syntactically compatible with the interface specifications, but badly integrate in target systems, and the problem of automatically generating regression test suites. The technique proposed in this paper to automatically generate compatibility and prioritized test suites is based on behavioral models that represent component interactions, and are automatically generated while executing the original test suites on previous versions of target systems.",0270-5257;1558-1225,0-7695-2828,10.1109/ICSE.2007.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4222571,,Software testing;System testing;Automatic testing;Binary codes;Software systems;Natural languages;Java;Control systems;Lead;XML,formal specification;object-oriented programming;program testing;software packages,COTS component-based software;commecial off-the-shelf;compatibility testing;regression testing;interface specification;component interaction representation,,20,25,,,,,,IEEE,IEEE Conferences
Control unit for a laboratory motor test bench for monitoring and controlling PMSM and induction motors,M. Ganchev,"Arsenal Research, Giefinggasse 2, A-1210 Vienna, Austria",2007 European Conference on Power Electronics and Applications,,2007,,,1,8,"The work presents a state-of-art control unit for pulse width modulated inverter-fed AC motors. The unit is suitable for a wide range of voltage source inverters with frequencies ranging from 1 KHz to up to 100 KHz. The hardware is characterized by a powerful floating point DSP, FPGA unit, asynchronous serial and IEEE 1394 communication interface, and 12 channels Analog/Digital interface with sample and conversion times together equaling 250 ns. The control software is managed by a specially designed real-time multitasking operating system. The operating system guarantees less than 300 ns time duration when jumping from one task to another upon internal or external event. The operating system can be adapted easily for arbitrary number of tasks with various prioritization levels and triggering events, and therefore suitable for interfacing hardware in the loop (HIL) simulation environments. The on-line interaction between the user and the running control software is implemented by a specially designed IEEE1394 driver for Windows XP and a graphical user interface (GUI). This allows graphical and numerical monitoring of software variables and their modification at will.",,978-92-75815-10,10.1109/EPE.2007.4417431,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4417431,Test bench;Real time processing;Real time simulation;Software;System Integration;Electrical Drive;DSP;Control of Drive;Fault Handling Strategy,Induction motors;Laboratories;Testing;Monitoring;Pulse width modulation inverters;Operating systems;AC motors;Communication system control;Hardware;Graphical user interfaces,computerised monitoring;control engineering computing;graphical user interfaces;induction motors;machine control;operating systems (computers);permanent magnet motors;power engineering computing;synchronous motors;testing,laboratory motor test bench;permanent magnet synchronous motor monitoring;permanent magnet synchronous motor control;induction motor;pulse width modulated inverter-fed AC motor;voltage source inverter;floating point DSP;FPGA unit;asynchronous serial communication;IEEE 1394 communication interface;analog-digital interface;control software;real-time multitasking operating system;hardware-in-the-loop simulation;Windows XP;graphical user interface;numerical monitoring;fault handling strategy;electrical drive;system integration;real time simulation;frequency 1 kHz to 100 kHz,,2,13,,,,,,IEEE,IEEE Conferences
Data Mining Static Code Attributes to Learn Defect Predictors,T. Menzies; J. Greenwald; A. Frank,"Lane Department of Computer Science and Electrical Engineering, West Virginia University, Morgantown, WV; Department of Computer Science, Portland State University, Portland, OR; Department of Computer Science, Portland State University, Portland, OR",IEEE Transactions on Software Engineering,,2007,33,1,2,13,"The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of ""McCabes versus Halstead versus lines of code counts"" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2007.256941,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4027145,Data mining detect prediction;McCabe;Halstead;artifical intelligence;empirical;naive Bayes.,Data mining;Bayesian methods;Artificial intelligence;Software testing;System testing;Learning systems;Art;Software quality;Software systems;Financial management,data mining;learning (artificial intelligence);program diagnostics;program testing;software quality,data mining;static code attributes;defect predictor learning;McCabes versus Halstead;lines of code counts;resource-bound exploration,,484,49,,,,,,IEEE,IEEE Journals & Magazines
Effective Fault Localization using Code Coverage,W. E. Wong; Y. Qi; L. Zhao; K. Cai,University of Texas at Dallas; University of Texas at Dallas; University of Texas at Dallas; Beijing University of Aeronautics,31st Annual International Computer Software and Applications Conference (COMPSAC 2007),,2007,1,,449,456,"Localizing a bug in a program can be a complex and time- consuming process. In this paper we propose a code coverage-based fault localization method to prioritize suspicious code in terms of its likelihood of containing program bugs. Code with a higher risk should be examined before that with a lower risk, as the former is more suspicious (i.e., more likely to contain program bugs) than the latter. We also answer a very important question: how can each additional test case that executes the program successfully help locate program bugs? We propose that with respect to a piece of code, the aid introduced by the first successful test that executes it in computing its likelihood of containing a bug is larger than or equal to that of the second successful test that executes it, which is larger than or equal to that of the third successful test that executes it, etc. A tool, chiDebug, was implemented to automate the computation of the risk of the code and the subsequent prioritization of suspicious code for locating program bugs. A case study using the Siemens suite was also conducted. Data collected from our study support the proposal described above. They also indicate that our method (in particular Heuristics III (c), (d), and (e)) can effectively reduce the search domain for locating program bugs.",0730-3157,0-7695-2870,10.1109/COMPSAC.2007.109,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291037,,Computer bugs;Proposals;Computer science;Automatic testing;Data visualization;Software testing;Software debugging;Software design;Java;Application software,codes;fault tolerant computing;program debugging,program bug localization;code coverage-based fault localization;suspicious code;program bug location;Siemens suite;search domain,,55,22,,,,,,IEEE,IEEE Conferences
Enhancing the Efficiency of Regression Testing through Intelligent Agents,T. M. S. U. Salima; A. Askarunisha; N. Ramaraj,NA; NA; NA,International Conference on Computational Intelligence and Multimedia Applications (ICCIMA 2007),,2007,1,,103,108,"Software testing is indispensable for all software development. As all mature engineering disciplines need to have systematic testing methodologies, software testing is a very important subject of software engineering. In software development practice, testing accounts for as much as 50% of total development efforts. Testing can be manual, automated, or a combination of both.Manual testing is the process of executing the application and manually interacting with the application, specifying inputs and observing outputs. Manually testing the software is inefficient and costly. It is imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process, which contains many testing related activities using various techniques and methods. In order to automate the process, we have to have some ways to generate oracles from the specification, and generate test cases to test the target software against the oracles to decide their correctness. Today we still don't have a full-scale system that has achieved this goal. In general, significant amount of human intervention is still needed in testing. The degree of automation remains at the automated test script level. This paper therefore provides a timely summary and enhancement of agent theory in software testing ,which motivates recent efforts in adapting concepts and methodologies for agent oriented software testing to complex system which has not previously done. Agent technologies facilitate the automated software testing by virtue of their high level decomposition, independency and parallel activation[4]. Usage of agent based regression testing reduces the complexity involved in prioritizing the testcases.With the ability of agents to act autonomously, monitoring code changes and generating test cases for the changed version of the code can be done dynamically. Agent-Oriented Software testing (AOST) is a nascent but active field of research . A comprehensive methodology that plays an essential role in Software testing must be robust but easy-to use. Moreover, it should provide a roadmap to guide engineers in creating Agent-based systems. The agent based regression testing(ABRT) proposed here is to offer a definition for encompassing to cover the regression testing phenomena based on agents, yet sufficiently tight that it can rule out complex systems that are clearly not agent based.",,0-7695-3050,10.1109/ICCIMA.2007.294,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426561,,Intelligent agent;Software testing;Automatic testing;System testing;Programming;Application software;Software engineering;Costs;Humans;Automation,,,,5,15,,,,,,IEEE,IEEE Conferences
"Increased Mars Rover Autonomy using AI Planning, Scheduling and Execution",T. Estlin; D. Gaines; C. Chouinard; R. Castano; B. Bornstein; M. Judd; I. Nesnas; R. Anderson,"Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Tara.Estlin@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Daniel.Gaines@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Caroline.Chouinard@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Rebecca.Castano@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Benjamin.Bornstein@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Michele.Judd@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Issa.Nesnas@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109 USA. email: Robert.Anderson@jpl.nasa.gov",Proceedings 2007 IEEE International Conference on Robotics and Automation,,2007,,,4911,4918,"This paper presents technology for performing autonomous commanding of a planetary rover. Through the use of AI planning, scheduling and execution techniques, the OASIS autonomous science system provides capabilities for the automated generation of a rover activity plan based on science priorities, the handling of opportunistic science, including new science targets identified by onboard data analysis software, other dynamic decision-making such as modifying the rover activity plan in response to problems or other state and resource changes. We first describe some of the particular challenges this work has begun to address and then describe our system approach. Finally, we report on our experience testing this software with a Mars rover prototype.",1050-4729,1-4244-0602-11-4244-0601,10.1109/ROBOT.2007.364236,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4209854,,Mars;Artificial intelligence;Data analysis;Navigation;Monitoring;Software safety;Robotics and automation;Image analysis;System testing;Hardware,aerospace robotics;data analysis;mobile robots;planetary rovers;planning (artificial intelligence);scheduling,Mars rover autonomy;artificial intelligent planning;scheduling;planetary rover;OASIS autonomous science system;opportunistic science;data analysis,,7,13,,,,,,IEEE,IEEE Conferences
Indexing Noncrashing Failures: A Dynamic Program Slicing-Based Approach,C. Liu; X. Zhang; J. Han; Y. Zhang; B. K. Bhargava,"Department of Computer Science, University of Illinois-UC, Urbana, IL 61801 USA, chaoliu@cs.uiuc.edu; Department of Computer Science, Purdue University, West Lafayette, IN 47907 USA, xyzhang@cs.purdue.edu; Dept of Computer Science, University of Illinois-UC, Urbana, IL 61801 USA, hanj@cs.uiuc.edu; Dept of Computer Science, Purdue University, West Lafayette, IN 47907 USA, zhangyu@cs.purdue.edu; Dept of Computer Science, Purdue University, West Lafayette, IN 47907 USA, bb@cs.purdue.edu",2007 IEEE International Conference on Software Maintenance,,2007,,,455,464,"Recent software systems usually feature an automated failure reporting component, with which a huge number of failures are collected from software end-users. With a proper support of failure indexing, which identifies failures due to the same fault, the collected failure data can help developers prioritize failure diagnosis, among other utilities of the failure data. Since crashing failures can be effectively indexed by program crashing venues, current practice has seen great success in prioritizing crashing failures. A recent study of bug characteristics indicates that as excellent memory checking tools are widely adopted, semantic bugs and the resulting noncrashing failures have become dominant. Unfortunately, the problem of how to index non-crashing failures has not been seriously studied before. In previous study, two techniques have been proposed to index noncrashing failures, and they are T-Proximity and R-Proximity. However, as T-Proximity indexes failures by the profile of the entire execution, it is generally not effective because most information in the profile is fault-irrelevant. On the other hand, although R-Proximity is more effective than T-Proximity, it relies on a sufficient number of correct executions that may not be available in practice. In this paper, we propose a dynamic slicing-based approach, which does not require any correct executions, and is comparably effective as R-Proximity. A detailed case study with gzip is reported, which clearly strates the advantages of the proposed approach.",1063-6773,978-1-4244-1255-6978-1-4244-1256,10.1109/ICSM.2007.4362658,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4362658,,Indexing;Fault diagnosis;Computer science;Computer crashes;Software testing;Chaos;Computer bugs;Software systems;Java;Failure analysis,program debugging;program slicing,dynamic program slicing-based approach;software system;automated failure reporting component;failure diagnosis;bug characteristic;memory checking tool;program noncrashing failure indexing;R-Proximity index failure;T-Proximity index failure,,2,23,,,,,,IEEE,IEEE Conferences
Model Checking Safety-Critical Systems Using Safecharts,P. Hsiung; Y. Chen; Y. Lin,"Department of Computer Science and Information Engineering, National Chung Cheng University, 168, University Road, Min-Hsiung, Chiayi, Taiwan-621, ROC; Department of Computer Science and Information Engineering, National Chung Cheng University, 168, University Road, Min-Hsiung, Chiayi, Taiwan-621, ROC; National Chiao Tung University, Hsinchu, Taiwan- 704, ROC",IEEE Transactions on Computers,,2007,56,5,692,705,"With rapid developments in science and technology, we now see the ubiquitous use of different types of safety-critical systems in our daily lives such as in avionics, consumer electronics, and medical systems. In such systems, unintentional design faults might result in injury or even death to human beings. To make sure that safety-critical systems are really safe, there is a need to verify them formally. However, the verification of such systems is getting more and more difficult because designs are becoming very complex. To cope with high design complexity, currently, model-driven architecture design is becoming a well-accepted trend. However, existing methods of testing and standards conformance are restricted to implementation code, so they do not fit very well with model-based approaches. To bridge this gap, we propose a model-based formal verification technique for safety-critical systems. In this work, the model-checking paradigm is applied to the Safecharts model, which was used for modeling but not yet used for verification. Our contributions listed are as follows: first, the safety constraints in Safecharts are mapped to semantic equivalents in timed automata for verification. Second, the theory for safety constraint verification is proven and implemented in a compositional model checker (that is, the state-graph manipulator (SGM)). Third, prioritized and urgent transitions are implemented in SGM to model the risk semantics in Safecharts. Finally, it is shown that the priority-based approach to mutual exclusion of resource usage in the original Safecharts is unsafe and corresponding solutions are proposed. Application examples show the feasibility and benefits of the proposed model-driven verification of safety-critical systems",0018-9340;1557-9956;2326-3814,,10.1109/TC.2007.1021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4141241,Safety-critical systems;model checking;Safecharts;extended timed automaton.,,automata theory;computational linguistics;formal verification;graph theory;safety-critical software,model checking;safety-critical systems;Safecharts;design complexity;model-driven architecture design;model-based formal verification technique;semantic equivalents;timed automata;safety constraint verification;compositional model checker;state-graph manipulator;risk semantics;mutual exclusion;resource usage,,14,29,,,,,,IEEE,IEEE Journals & Magazines
Multi - Layered Best Basis Image Compression,D. M. Yadav; D. S. Bormane,NA; NA,International Conference on Computational Intelligence and Multimedia Applications (ICCIMA 2007),,2007,3,,79,83,"Software testing is indispensable for all software development. As all mature engineering disciplines need to have systematic testing methodologies, software testing is a very important subject of software engineering. In software development practice, testing accounts for as much as 50% of total development efforts. Testing can be manual, automated, or a combination of both. Manual testing is the process of executing the application and manually interacting with the application, specifying inputs and observing outputs. Manually testing the software is inefficient and costly. It is imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process, which contains many testing related activities using various techniques and methods. In order to automate the process, we have to have some ways to generate oracles from the specification, and generate test cases to test the target software against the oracles to decide their correctness. Today we still don't have a full-scale system that has achieved this goal. In general, significant amount of human intervention is still needed in testing. The degree of automation remains at the automated test script level. This paper therefore provides a timely summary and enhancement of agent theory in software testing ,which motivates recent efforts in adapting concepts and methodologies for agent oriented software testing to complex system which has not previously done. Agent technologies facilitate the automated software testing by virtue of their high level decomposition, independency and parallel activation. Usage of agent based regression testing reduces the complexity involved in prioritizing the test cases.With the ability of agents to act autonomously, monitoring code changes and generating test cases for the changed version of the code can be done dynamically. Agent-oriented software testing (AOST) is a nascent but active field of research . A comprehensive methodology that plays an essential role in software testing must be robust but easy-to use. Moreover, it should provide a roadmap to guide engineers in creating agent-based systems. The agent based regression testing (ABRT) proposed here is to offer a definition for encompassing to cover the regression testing phenomena based on agents, yet sufficiently tight that it can rule out complex systems that are clearly not agent based.",,0-7695-3050,10.1109/ICCIMA.2007.293,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4426345,,Image coding;Software testing;Automatic testing;System testing;Programming;Application software;Software engineering;Costs;Humans;Automation,approximation theory;data compression;image coding;image representation;image sequences;image texture;wavelet transforms,multi layered best basis image compression;multilayered image representation technique;image encoding;image residual sequence;image textured pattern-brushlet;localized oriented textured feature;coarse main approximation method;wavelet packet bases,,,12,,,,,,IEEE,IEEE Conferences
Prioritization of Regression Tests using Singular Value Decomposition with Empirical Change Records,M. Sherriff; M. Lake; L. Williams,NA; NA; NA,The 18th IEEE International Symposium on Software Reliability (ISSRE '07),,2007,,,81,90,"During development and testing, changes made to a system to repair a detected fault can often inject a new fault into the code base. These injected faults may not be in the same files that were just changed, since the effects of a change in the code base can have ramifications in other parts of the system. We propose a methodology for determining the effect of a change and then prioritizing regression test cases by gathering software change records and analyzing them through singular value decomposition. This methodology generates clusters of files that historically tend to change together. Combining these clusters with test case information yields a matrix that can be multiplied by a vector representing a new system modification to create a prioritized list of test cases. We performed a post hoc case study using this technique with three minor releases of a software product at IBM. We found that our methodology suggested additional regression tests in 50% of test runs and that the highest-priority suggested test found an additional fault 60% of the time.",1071-9458;2332-6549,0-7695-3024-9978-0-7695-3024,10.1109/ISSRE.2007.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402199,,Singular value decomposition;System testing;Software testing;Programming;Fault detection;Matrix decomposition;Software reliability;Reliability engineering;Lakes;Software performance,program testing;singular value decomposition;software maintenance,regression tests prioritization;singular value decomposition;empirical change records;detected fault repair;software change records;system modification;software product;regression tests,,25,25,,,,,,IEEE,IEEE Conferences
Prioritized Constraints with Data Sampling Scores for Automatic Test Data Generation,X. Ma; J. J. Li; D. M. Weiss,Avaya Labs Research; Avaya Labs Research; Avaya Labs Research,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",,2007,3,,1129,1134,"Many automatic test data generation approaches use constraint solvers to find data values. One problem with this method is that it cannot generate test data when the constraints are not solvable, either because there is no solution or the constraints are too complex. We propose a constraint prioritization method using data sampling scores to generate valid test data even when a set of constraints is not solvable. Our case study illustrates the effectiveness of this method.",,0-7695-2909-7978-0-7695-2909,10.1109/SNPD.2007.523,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4288019,Test case generation;test automation;constraint;solving;software reliability.,Sampling methods;Automatic testing;Software testing;System testing;Writing;Automation;Costs;Programming;Software engineering;Artificial intelligence,automatic test software;constraint handling;program testing;software reliability,data sampling scores;automatic test data generation;constraint solving;constraint prioritization;software reliability,,,9,,,,,,IEEE,IEEE Conferences
Prioritizing Coverage-Oriented Testing Process - An Adaptive-Learning-Based Approach and Case Study,F. Belli; M. Eminov; N. Gokce,"University of Paderborn, Germany; Mugla University, Turkey.; Mugla University, Turkey",31st Annual International Computer Software and Applications Conference (COMPSAC 2007),,2007,2,,197,203,"This paper proposes a graph-model-based approach to prioritizing the test process. Tests are ranked according to their preference degrees which are determined indirectly, i.e., through classifying the events. To construct the groups of events, unsupervised neural network is trained by adaptive competitive learning algorithm. A case study demonstrates and validates the approach.",0730-3157,0-7695-2870,10.1109/COMPSAC.2007.169,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291124,,System testing;Costs;Mathematical model;Neural networks;Time factors;Robustness;Constraint optimization,graph theory;neural nets;program testing;unsupervised learning,coverage-oriented testing process;adaptive learning;graph-model-based approach;unsupervised neural network;adaptive competitive learning algorithm,,4,19,,,,,,IEEE,IEEE Conferences
Search Algorithms for Regression Test Case Prioritization,Z. Li; M. Harman; R. M. Hierons,NA; NA; NA,IEEE Transactions on Software Engineering,,2007,33,4,225,237,"Regression testing is an expensive, but important, process. Unfortunately, there may be insufficient resources to allow for the reexecution of all test cases during regression testing. In this situation, test case prioritization techniques aim to improve the effectiveness of regression testing by ordering the test cases so that the most beneficial are executed first. Previous work on regression test case prioritization has focused on greedy algorithms. However, it is known that these algorithms may produce suboptimal results because they may construct results that denote only local minima within the search space. By contrast, metaheuristic and evolutionary search algorithms aim to avoid such problems. This paper presents results from an empirical study of the application of several greedy, metaheuristic, and evolutionary search algorithms to six programs, ranging from 374 to 11,148 lines of code for three choices of fitness metric. The paper addresses the problems of choice of fitness metric, characterization of landscape modality, and determination of the most suitable search technique to apply. The empirical results replicate previous results concerning greedy algorithms. They shed light on the nature of the regression testing search space, indicating that it is multimodal. The results also show that genetic algorithms perform well, although greedy approaches are surprisingly effective, given the multimodal nature of the landscape",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2007.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4123325,Search techniques;test case prioritization;regression testing.,Greedy algorithms;Cost function;Genetic algorithms;Software testing;Libraries;Fault detection,genetic algorithms;greedy algorithms;program testing;search problems,regression testing;test case prioritization technique;greedy algorithm;metaheuristics;evolutionary search algorithm;fitness metric;genetic algorithm,,259,22,,,,,,IEEE,IEEE Journals & Magazines
Software Fault Localization Based on Testing Requirement and Program Slice,J. Sun; Z. Li; J. Ni; F. Yin,"Sichuan University, China; Sichuan Radio and TV University, China; Sichuan University, China; Sichuan University, China; Southwest University for Nationalities, China","2007 International Conference on Networking, Architecture, and Storage (NAS 2007)",,2007,,,168,176,"A heuristic approach is proposed to locate a fault according to the priority. To a given test case wt, fault localization has to be proceeded when its output wrong. Firstly, four assistant test cases, one failed and three successful test cases, are selected out according to the biggest cardinality of Req(wt,t<sub>i</sub>), which stand for the common testing requirements both covered by wt and t<sub>i</sub>. Then, code prioritization methodology is put forward based on program slice technique. Dynamic slice technique is taken for wt and execution slice technique for four assistant test cases. Some dices are constructed with different priority which means the possibility of containing bug and is evaluated according to the occurrences in the selected slices. Thirdly, the key algorithm including two procedures, refining and augmenting, is followed here to fault localization based on priority. In the refining phase, the most suspicious codes am checked step by step; in the augmenting phase, more codes will be gradually considered on the basis of direct data dependency. At last, experimental studies are performed to illustrate the effectiveness of the technique.",,0-7695-2927,10.1109/NAS.2007.51,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4286423,,Software testing;Computer science;Sun;Educational institutions;TV;Failure analysis;Computer bugs;Societies,program slicing;program testing;software reliability,software fault localization;testing requirement;program slice;heuristic approach;dynamic slice technique;augmenting phase;direct data dependency,,2,9,,,,,,IEEE,IEEE Conferences
Speaking Truth to Power,G. Booch,IBM,IEEE Software,,2007,24,2,12,13,"Whenever the author conducts an architectural assessment for software development projects, he endeavors to speak truth to power: those with true power never fear the truth. Sam Guckenheimer has observed that in software code there is truth. Code represents the stark reality of a software development organization's labor. There is also truth to be found in a system's architecture. Every system's architecture is molded by the forces that swirl around it, and the collective concerns of all the stakeholders represent the most dynamic forces shaping a system. The software development organization's unique task is to address all the essential concerns of all the important stakeholders and to ensure that they aren't blindsided by unexpected problems and stakeholders. This is why employing a process that incrementally and iteratively grows a system's architecture through the regular release of testable executables is so important. Such a process lets the software team engage the right stakeholders at the right time and to make the right decisions, neither too early nor too late. In creating a software-intensive system that's both relevant and beautiful, every stakeholder, no matter how close or how far from the code, deserves the truth",0740-7459;1937-4194,,10.1109/MS.2007.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4118643,architectural assessment;stakeholder prioritization,Computer architecture;System testing;Orbits;Social network services;Real time systems;Privacy;Humans;Electronic mail;Aging;History,software architecture;software development management,architectural assessment;truth to power;soft-ware development;system architecture;stakeholders;testable executables;software team;software-intensive system,,1,,,,,,,IEEE,IEEE Journals & Magazines
Techniques for building excellent Operator Machine Interfaces (OMI),P. Gorman; N. Pappas,"The Boeing Company, Seattle, WA, USA; The Boeing Company, Seattle, WA, USA",2007 IEEE/AIAA 26th Digital Avionics Systems Conference,,2007,,,6.A.1-1,6.A.1-8,"Establishing a process to continually improve understanding of operator requirements -the why as well as the how-is key to developing an optimal Operator-Machine Interface (OMI) for a large program. The P-8A program is one example of a complex software development, with over 2 million lines of code and a unique and complex OMI. The OMI itself is used as a tool for achieving, and is designed in parallel with understanding end goals, operator decision making chains, and thought processes. The OMI can be designed to support and anticipate information needs, and alert operators to unusual occurrences. Operator actions and decision making techniques can be discovered by using expert operators and capturing their knowledge through the use of multiple tools to identify the problem set: storytelling, decision mapping, is/is not matrices, etc. When a solution set has been identified and agreed to by the customer, resources allocated, and schedule set, a standard process captures and prioritizes further changes, identifies their impact, and decides on implementation. Documents describing design and processes and a Design Description Document describing the current version of the OMI are made accessible to stakeholders at all times. ""What's important is not that we can conceive the idea, but that when we actually test it on people you discover it doesn't work... your intuition is wrong."" -Daniel M. Russell (IBM Almaden / Xerox PARC).",2155-7195;2155-7209,978-1-4244-1107-8978-1-4244-1108,10.1109/DASC.2007.4391947,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4391947,,Companies;Decision making;Process design;Aerospace testing;Logic;Programming;Resource management;Humans;Computer errors;Software performance,man-machine systems;user interfaces,operator machine interfaces;operator decision making;expert operator;documents describing design;design description document,,,7,,,,,,IEEE,IEEE Conferences
Test Case Prioritization Based on Varying Testing Requirement Priorities and Test Case Costs,X. Zhang; C. Nie; B. Xu; B. Qu,"Southeast University, China; Southeast University, China; Southeast University, China; Southeast University, China",Seventh International Conference on Quality Software (QSIC 2007),,2007,,,15,24,"Test case prioritization is an effective and practical technique in regression testing. It schedules test cases in order of precedence that increases their ability to meet some performance goals, such as code coverage, rate of fault detection. In previous work, the test case prioritization techniques and metrics usually assumed that testing requirement priorities and test case costs are uniform. In this paper, basing on varying testing requirement priorities and test case costs, we present a new, general test case prioritization technique and an associated metric. The case study illustrates that the rate of ""units-oftesting-requirement-priority-satisfied- per-unit-test-case-cost"" can be increased, and then the testing quality and customer satisfaction can be improved.",1550-6002;2332-662X,0-7695-3035-4978-0-7695-3035,10.1109/QSIC.2007.4385476,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4385476,,Costs;Software testing;Fault detection;Software quality;Life testing;Programming;Feedback;Computer science;Processor scheduling;Customer satisfaction,program testing;software reliability,test case prioritization;varying testing requirement priorities;test case costs;regression testing;fault detection;code coverage;testing requirement priorities;customer satisfaction;testing quality;software testing,,14,23,,,,,,IEEE,IEEE Conferences
Test Case Prioritization for Black Box Testing,B. Qu; C. Nie; B. Xu; X. Zhang,"Southeast University, Nanjing, China; Southeast University, Nanjing, China; Southeast University, Nanjing, China; Southeast University, Nanjing, China",31st Annual International Computer Software and Applications Conference (COMPSAC 2007),,2007,1,,465,474,"Test case prioritization is an effective and practical technique that helps to increase the rate of regression fault detection when software evolves. Numerous techniques have been reported in the literature on prioritizing test cases for regression testing. However, existing prioritization techniques implicitly assume that source or binary code is available when regression testing is performed, and therefore cannot be implemented when there is no program source or binary code to be analyzed. In this paper, we presented a new technique for black box regression testing, and we performed an experiment to measure our technique. Our results show that the new technique is helpful to improve the effectiveness of fault detection when performing regression test in black box environment.",0730-3157,0-7695-2870,10.1109/COMPSAC.2007.209,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4291039,,Fault detection;Software testing;Binary codes;Performance evaluation;History;Computer science;Runtime;Performance analysis;Life testing;Sorting,fault diagnosis;program testing;regression analysis,test case prioritization;regression fault detection;black box regression testing,,19,15,,,,,,IEEE,IEEE Conferences
The Effect of Organization Process Focus and Organizational Learning on Project Performance: An Examination of Taiwan's Companies,C. Wu; S. Wang; K. Fang,"National Yunlin University of Science & Technology, Department of Information Management, Yunlin, Taiwan, R.O.C.; National Formosa University, Department of Information Management, Yunlin, Taiwan, R.O.C.; National Formosa University, Department of Information Management, Yunlin, Taiwan, R.O.C.; National Yunlin University of Science & Technology, Department of Information Management, Yunlin, Taiwan, R.O.C.",PICMET '07 - 2007 Portland International Conference on Management of Engineering & Technology,,2007,,,1083,1089,"The impact of organizational learning on project performance has received a great deal of attention in recent years. Process focus is recognized as one of five factors which help to promote organizational learning through out the process. A theoretical model is derived based upon prior researches in literature to examine the effects of organizational learning and process focus on project performance. The structural equation modeling was adopted to test the proposed hypotheses, and Taiwanese corporate IS companies served as examples. The results revealed that organization process focus has a positive impact on organizational learning, which in turn has a positive influence on project performance. Both organization process focus and organizational learning play the influence on project performance. These findings should give valuable information for managers to revisit their priorities in terms of the relative efforts in organization process focus and organization learning.",2159-5100;2159-5119,978-1-8908-4315,10.1109/PICMET.2007.4349429,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4349429,,Project management;Information management;Testing;Software performance;Software quality;Companies;Programming;Scheduling;Equations;Planning,organisational aspects;project management,organization process focus;organizational learning;project performance;Taiwan companies;structural equation modeling;information systems,,2,62,,,,,,IEEE,IEEE Conferences
The Impact of Organizational Learning on Lack of Team's Expertise Risk in Information Systems Projects,C. Wu; K. Fang,NA; NA,IEEE International Conference on e-Business Engineering (ICEBE'07),,2007,,,738,743,"During the past decade, information systems investment has grown rapidly worldwide and information systems project development has become one of the most important targets in e-business. Yet, the failure of information systems projects is a common occurrence in many organizations around the world. A theoretical model is derived based upon organizational learning theory and prior research in order to examine the effects of organizational learning on lack of team's expertise risk. A survey method is applied to test the hypotheses proposed by the research model, and Taiwanese corporate companies serve as examples. After survey by questionnaire and analyze the data by structure equation modeling, the result reveals that organizational learning has significantly negative impacts on all of the lack of development expertise risk, lack of domain expertise risk, and lack of general expertise risk. These findings support information systems managers with valuable information to revisit their priorities in terms of the relative efforts in organization learning.",,0-7695-3003-6978-0-7695-3003,10.1109/ICEBE.2007.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4402174,Organizational learning;Lack of team's expertise;Project risk;Project failure,Information systems;Project management;Risk management;Management information systems;Information management;Investments;Design methodology;Software development management;Software tools;Application software,electronic commerce;human resource management;information systems;project management;risk management,team expertise risk;information systems projects;information systems investment;e-business;structure equation modeling;organizational learning,,,35,,,,,,IEEE,IEEE Conferences
The need for self-managed access nodes in grid environments.,R. Nou; F. Julia; J. Torres,"Technical University of Catalonia, Barcelona, Spain; Technical University of Catalonia, Barcelona, Spain; Technical University of Catalonia, Barcelona, Spain",Fourth IEEE International Workshop on Engineering of Autonomic and Autonomous Systems (EASe'07),,2007,,,5,13,"The Grid is constantly growing and it is being used by more and more applications. In this scenario the entry node is an important component in the whole architecture and will become a contention point. In this paper we will demonstrate that the use of a self-managed layer on the entry node of a grid is necessary. A self-managed system can allow more jobs to be accepted and finished correctly. Since it's not acceptable for a grid middleware layer to lose jobs, we would normally need to prioritize the finishing/acceptance of jobs over the response time or the throughput. A prototype of what could be considered an autonomous system, is presented and tested over an installation of Globus Toolkit (GT4) and shows that we can greatly improve the performance of the original middleware by a factor of 30%. In this paper GT is used as an example but it could be added to any grid middleware",,0-7695-2809,10.1109/EASE.2007.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148878,,Middleware;Computer architecture;Prototypes;Mathematical model;Application software;Finishing;Delay;Throughput;System testing;Network topology,grid computing;middleware,self-managed access node;grid environment;self-managed system;grid middleware layer;Globus Toolkit,,1,16,,,,,,IEEE,IEEE Conferences
The TeraPaths Testbed: Exploring End-to-End Network QoS,D. Katramatos; D. Yu; B. Gibbard; S. McKee,"RHIC/ATLAS Computing Facility, Physics Department, Brookhaven National Laboratory, Upton, NY 11793. dkat@bnl.gov; RHIC/ATLAS Computing Facility, Physics Department, Brookhaven National Laboratory, Upton, NY 11793. dtyu@bnl.gov; RHIC/ATLAS Computing Facility, Physics Department, Brookhaven National Laboratory, Upton, NY 11793. gibbard@bnl.gov; Physics Department, University of Michigan, Ann Arbor, MI 48109. smckee@umich.edu",2007 3rd International Conference on Testbeds and Research Infrastructure for the Development of Networks and Communities,,2007,,,1,7,"The TeraPaths project at Brookhaven National Laboratory (BNL) investigates the combination of DiffServ-based LAN QoS with WAN MPLS tunnels in creating end-to-end (host-to-host) virtual paths with bandwidth guarantees. These virtual paths prioritize, protect, and throttle network flows in accordance with site agreements and user requests, and prevent the disruptive effects that conventional network flows can cause in one another. This paper focuses on the TeraPaths testbed, a collection of end-site subnets connected through high-performance WANs, serving the research and software development needs of the TeraPaths project. The testbed is rapidly evolving towards a multiple end-site infrastructure, dedicated to QoS networking research, and it offers unique opportunities for experimentation with minimal or no impact on regular, production networking operations.",,978-1-4244-0738-5978-1-4244-0739,10.1109/TRIDENTCOM.2007.4444698,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4444698,network;end-to-end;QoS;DiffServ;MPLS,Wide area networks;Local area networks;Bandwidth;Software testing;Physics;Laboratories;Multiprotocol label switching;Protection;System testing;Control systems,DiffServ networks;local area networks;multiprotocol label switching;quality of service;wide area networks,TeraPaths Testbed;end-to-end network QoS;DiffServ-based LAN;WAN MPLS tunnels;virtual paths;throttle network flows;end-site subnets,,3,15,,,,,,IEEE,IEEE Conferences
Toward the Use of Automated Static Analysis Alerts for Early Identification of Vulnerability- and Attack-prone Components,Michael; L. Williams,North Carolina State University; North Carolina State University,Second International Conference on Internet Monitoring and Protection (ICIMP 2007),,2007,,,18,18,"Extensive research has shown that software metrics can be used to identify fault- and failure-prone components. These metrics can also give early indications of overall software quality. We seek to parallel the identification and prediction of fault- and failure-prone components in the reliability context with vulnerability- and attack-prone components in the security context. Our research will correlate the quantity and severity of alerts generated by source code static analyzers to vulnerabilities discovered by manual analyses and testing. A strong correlation may indicate that automated static analyzers (ASA), a potentially early technique for vulnerability identification in the development phase, can identify high risk areas in the software system. Based on the alerts, we may be able to predict the presence of more complex and abstract vulnerabilities involved with the design and operation of the software system. An early knowledge of vulnerability can allow software engineers to make informed risk management decisions and prioritize redesign, inspection, and testing efforts. This paper presents our research objective and methodology.",,0-7695-2911,10.1109/ICIMP.2007.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4271764,,Fault diagnosis;Software systems;Software metrics;Software quality;Security;Testing;Risk analysis;Knowledge engineering;Risk management;Inspection,program diagnostics;program testing;security of data;software metrics;software quality;software reliability,automated static analysis;software metrics;failure-prone component identification;software quality;reliability context;security context;software testing;vulnerability identification,,8,43,,,,,,IEEE,IEEE Conferences
Towards an empirical method of efficiency testing of system parts: A methodological study,W. Brinkman; R. Haakma; D. G. Bouwhuis,NA; NA; NA,Interacting with Computers,,2007,19,3,342,356,"Current usability evaluation methods are essentially holistic in nature. However, engineers that apply a component-based software engineering approach might also be interested in understanding the usability of individual parts of an interactive system. This paper examines the efficiency dimension of usability by describing a method, which engineers can use to test, empirically and objectively, the physical interaction effort to operate components in a single device. The method looks at low-level events, such as button clicks, and attributes the physical effort associated with these interaction events to individual components in the system. This forms the basis for engineers to prioritise their improvement effort. The paper discusses face validity, content validity, criterion validity, and construct validity of the method. The discussion is set within the context of four usability tests, in which 40 users participated to evaluate the efficiency of four different versions of a mobile phone. The results of the study show that the method can provide a valid estimation of the physical interaction event effort users made when interacting with a specific part of a device.",0953-5438;1873-7951,,10.1016/j.intcom.2007.01.002,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8147077,Efficiency;Usability testing;HCI methodology;Usability evaluation method;Log file analysis;Empirical method,,,,,1,,,,,,,OUP,OUP Journals & Magazines
Value-Oriented Requirements Prioritization in a Small Development Organization,J. Azar; R. K. Smith; D. Cordes,Orasi Software; University of Alabama; University of Alabama,IEEE Software,,2007,24,1,32,37,"Requirements engineering, especially requirements prioritization and selection, plays a critical role in overall project development. In small companies, this often difficult process can affect not only project success but also overall company survivability. A value-oriented prioritization (VOP) framework can help this process by clarifying and quantifying the selection and prioritization issues. A case study of a small development company shows a successful VOP deployment that improved communications and saved time by focusing requirements decisions for new product releases on core company values",0740-7459;1937-4194,,10.1109/MS.2007.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4052549,requirements/specifications;software engineering;value-based software engineering,Marketing and sales;Project management;Costs;Aerospace testing;Software development management;Programming;Scheduling;Risk analysis;Process planning;Decision making,DP industry;formal specification;formal verification;software development management,value-oriented requirement prioritization;small development organization;requirement engineering;requirement selection;project development;value-based software engineering,,38,13,,,,,,IEEE,IEEE Journals & Magazines
Verification and Validation of (Real Time) COTS Products using Fault Injection Techniques,R. Barbosa; N. Silva; J. Duraes; H. Madeira,"Critical Software, S.A., Coimbra, Portugal; Critical Software, S.A., Coimbra, Portugal; CISUC, University of Coimbra, Portugal; CISUC, University of Coimbra, Portugal",2007 Sixth International IEEE Conference on Commercial-off-the-Shelf (COTS)-Based Software Systems (ICCBSS'07),,2007,,,233,242,"With the goal of reducing time to market and project costs, the current trend of real time business and mission critical systems is evolving from the development of custom made applications to the use of commercial off the shelf (COTS) products. Obviously, the same confidence and quality of the custom made software components is expected from the commercial applications. In most cases, such products (COTS) are not designed with stringent timing and/or safety requirements as priorities. Thus, to decrease the gap between the use of custom made components and COTS components, this paper presents a methodology for evaluating COTS products in the scope of dependable, real time systems, through the application of fault injection techniques at key points of the software engineering process. By combining the use of robustness testing (fault injection at interface level) with software fault injection (using educated fault injection operators), a COTS component can be assessed in the context of the system it will belong to, with special emphasis given to timing and safety constraints that are usually imposed by the target real time dependable environment. In the course of this work, three case studies have been performed to assess the methodology using realistic scenarios that used common COTS products. Results for one case study are presented",,0-7695-2785,10.1109/ICCBSS.2007.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4127318,,Real time systems;Application software;Timing;Software safety;Time to market;Costs;Business;Mission critical systems;Software quality;Product safety,program testing;program verification;software fault tolerance;software packages,software verification;software validation;commercial off the shelf;software components;software engineering;robustness testing;software fault injection,,7,6,,,,,,IEEE,IEEE Conferences
"""It's Not the Pants, it's the People in the Pants"" Learnings from the Gap Agile Transformation What Worked, How We Did it, and What Still Puzzles Us",D. Goodman; M. Elbaz,NA; NA,Agile 2008 Conference,,2008,,,112,115,"After 7 years of traditional IT delivery, Gap Inc.Direct decided to adopt Agile. This experience report discusses three key factors that contributed to our successful (and ongoing) Agile transformation: 1. Ambitious Pilot Project, 2. Massive Investment in Continuous Integration, 3. Rethinking our Assets. The choices we made might seem risky and even counter-intuitive, but understanding them could help other organizations consider different points of view and priorities as they embark on the transition to Agile. Additionally, we will identify ongoing challenges and what is left in our transformation backlog.",,978-0-7695-3321,10.1109/Agile.2008.87,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599459,Gap;Agile Transformation;Continuos Integration;Test Automation;Iterative Development,Business;Organizations;Programming;Testing;Software;Writing;Project management,project management;software engineering;software management,Gap Inc;agile transformation;IT delivery;ambitious pilot project;massive investment;continuous integration;assets rethinking,,4,,,,,,,IEEE,IEEE Conferences
A dynamic scheduler for balancing HPC applications,C. Boneti; R. Gioiosa; F. J. Cazorla; M. Valero,"Universitat Politecnica de Catalunya, Spain; Barcelona Supercomputing Center, Spain; Barcelona Supercomputing Center, Spain; Barcelona Supercomputing Center, Spain",SC '08: Proceedings of the 2008 ACM/IEEE Conference on Supercomputing,,2008,,,1,12,"Load imbalance cause significant performance degradation in High Performance Computing applications. In our previous work we showed that load imbalance can be alleviated by modern MT processors that provide mechanisms for controlling the allocation of processors internal resources. In that work, we applied static, hand-tuned resource allocations to balance HPC applications, providing improvements for benchmarks and real applications. In this paper we propose a dynamic process scheduler for the Linux kernel that automatically and transparently balances HPC applications according to their behavior. We tested our new scheduler on an IBM POWER5 machine, which provides a software-controlled prioritization mechanism that allows us to bias the processor resource allocation. Our experiments show that the scheduler reduces the imbalance of HPC applications, achieving results similar to the ones obtained by hand-tuning the applications (up to 16%). Moreover, our solution reduces the application's execution time combining effect of load balance and high responsive scheduling.",2167-4329;2167-4337,978-1-4244-2834-2978-1-4244-2835,10.1109/SC.2008.5217785,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5217785,,Dynamic scheduling;Processor scheduling;Application software;Resource management;Degradation;High performance computing;Automatic control;Benchmark testing;Linux;Kernel,dynamic scheduling;Linux;operating system kernels;processor scheduling;resource allocation,dynamic scheduler;HPC applications;load imbalance;performance degradation;high performance computing applications;MT processors;hand-tuned resource allocations;dynamic process scheduler;Linux kernel;IBM POWER5 machine;software-controlled prioritization mechanism;processor resource allocation,,20,30,,,,,,IEEE,IEEE Conferences
Adaptive Test Question Selection for Web-Based Educational System,O. Voz?­r; M. Bielikov?­,NA; NA,2008 Third International Workshop on Semantic Media Adaptation and Personalization,,2008,,,164,169,"In this paper we present a method proposed to select test questions adapting to individual needs of students in the context of Web-based educational system. It functions as a combination of three particular methods. First one is based on course structure and focuses on the selection of the most appropriate topic for learning, second uses the Item Response Theory to select k-best questions with adequate difficulty for particular learner and the last is based on usage history and prioritizes questions according to specific strategies, e.g. to filter out the questions that was recently asked. We describe how these methods evaluate user answers to gather information concerning their characteristics for more precise selection of further questions. We evaluated proposed method within our Web-based system called Flip on domain of functional programming.",,978-0-7695-3444,10.1109/SMAP.2008.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724867,adaptive test question selection;web-based educational system;item response theory;lerning programming,System testing;Adaptive systems;Feedback;Software testing;Informatics;History;Software engineering;Information technology;Educational technology;Filtering theory,courseware;Internet,Web-based educational system;adaptive test question selection;course structure;item response theory;functional programming;k-best question selection,,4,9,,,,,,IEEE,IEEE Conferences
Addressing Low Base Rates in Intrusion Detection via Uncertainty-Bounding Multi-Step Analysis,R. J. Cole; P. Liu,NA; NA,2008 Annual Computer Security Applications Conference (ACSAC),,2008,,,269,278,"Existing approaches to characterizing intrusion detection systems focus on performance under test conditions. While it is well-understood that operational conditions may differ from test conditions, little attention has been paid to the question of assessing the effect on IDS results of parameter estimation errors resulting from these differences. In this paper we consider this question in the context of multi-step attacks. We derive simulated distributions of the posterior probability of exploit given the observation of a series of alerts and bounds on the posterior uncertainty given a particular distribution of the model parameters. Knowledge of such bounds introduces the novel prospect of a confidence versus agility tradeoff in IDS administration. Such a tradeoff could give administrators flexibility in IDS configuration, allowing them to choose detection confidence at the price of detection latency, according to organizational priorities.",1063-9527,978-0-7695-3447,10.1109/ACSAC.2008.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4721564,Intrusion detection;Bayesian network;Probabilistic inference,Intrusion detection;Bayesian methods;Computer security;Expert systems;Uncertainty;Phase detection;Application software;Information analysis;Performance analysis;System testing,inference mechanisms;parameter estimation;probability;security of data,intrusion detection system;low base rate address;uncertainty-bounding multistep analysis;test condition;parameter estimation error;posterior probability distribution;inference uncertainty,,,11,,,,,,IEEE,IEEE Conferences
Agent multiplication: An economical large-scale testing environment for system management solutions,K. D. Ryu; D. Daly; M. Seminara; S. Song; P. G. Crumley,"IBM T.J. Watson Research Center Yorktown Heights, NY, USA; IBM T.J. Watson Research Center Yorktown Heights, NY, USA; IBM T.J. Watson Research Center Yorktown Heights, NY, USA; Department of Computer Science University of Maryland, College Park, USA; IBM T.J. Watson Research Center Yorktown Heights, NY, USA",2008 IEEE International Symposium on Parallel and Distributed Processing,,2008,,,1,8,"System management solutions are designed to scale to thousands or more machines and networked devices. However, it is challenging to test and verify the proper operation and scalability of management software given the limited resources of a testing lab. We have developed a method called agent multiplication, in which one physical testing machine is used to represent hundreds of client machines. This provides the necessary client load to test the performance and scalability of the management software and server within limited resources. In addition, our approach guarantees that the test environment remains consistent between test runs, ensuring that test results can be meaningfully compared. We used agent multiplication to test and verify the operation of a server managing 4,000 systems. We exercised the server functions with only 8 test machines. Applying this test environment to an early version of a real enterprise system management solution we were able to uncover critical bugs, resolve race conditions, and examine and adjust thread prioritization levels for improved performance.",1530-2075,978-1-4244-1693-6978-1-4244-1694,10.1109/IPDPS.2008.4536552,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4536552,,Environmental economics;Large-scale systems;System testing;Environmental management;Software testing;Scalability;Resource management;Software performance;Computer bugs;Yarn,client-server systems;program debugging;program testing;program verification;software agents;software management;systems analysis,agent multiplication;economical large-scale testing environment;system management solutions;management software;physical testing machine;client machines;real enterprise system management solution;critical bugs,,1,17,,,,,,IEEE,IEEE Conferences
An Empirical Study on Bayesian Network-based Approach for Test Case Prioritization,S. Mirarab; L. Tahvildari,NA; NA,"2008 1st International Conference on Software Testing, Verification, and Validation",,2008,,,278,287,"A cost effective approach to regression testing is to prioritize test cases from a previous version of a software system for the current release. We have previously introduced a new approach for test case prioritization using Bayesian Networks (BN) which integrates different types of information to estimate the probability of each test case finding bugs. In this paper, we enhance our BN-based approach in two ways. First, we introduce a feedback mechanism and a new change information gathering strategy. Second, a comprehensive empirical study is performed to evaluate the performance of the approach and to identify the effects of using different parameters included in the technique. The study is performed on five open source Java objects. The obtained results show relative advantage of using feedback mechanism for some objects in terms of early fault detection. They also provide insight into costs and benefits of the various parameters used in the approach.",2159-4848,978-0-7695-3127,10.1109/ICST.2008.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539555,Test Case Prioritization;Empirical Study;Regression Testing,Bayesian methods;Software testing;System testing;Feedback;Costs;Software systems;Computer bugs;Performance evaluation;Java;Fault detection,Bayes methods;Java;program testing;regression analysis,Bayesian network-based approach;test case prioritization;regression testing;information gathering strategy;open source Java;feedback mechanism,,17,22,,,,,,IEEE,IEEE Conferences
Application of system models in regression test suite prioritization,B. Korel; G. Koutsogiannakis; L. H. Tahat,"Computer Science Department, Illinois Institute of Technology, Chicago, 60616, USA; Computer Science Department, Illinois Institute of Technology, Chicago, 60616, USA; Gulf University for Science & Tech., P.O. Box 7207, Hawally 32093, Kuwait",2008 IEEE International Conference on Software Maintenance,,2008,,,247,256,"During regression testing, a modified system needs to be retested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Most of the existing test prioritization methods are based on the code of the system, but model-based test prioritization has been recently proposed. System modeling is a widely used technique to model state-based systems. The existing model based test prioritization methods can only be used when models are modified during system maintenance. In this paper, we present model-based prioritization for a class of modifications for which models are not modified (only the source code is modified). After identification of elements of the model related to source-code modifications, information collected during execution of a model is used to prioritize tests for execution. In this paper, we discuss several model-based test prioritization heuristics. The major motivation to develop these heuristics was simplicity and effectiveness in early fault detection. We have conducted an experimental study in which we compared model-based test prioritization heuristics. The results of the study suggest that system models may improve the effectiveness of test prioritization with respect to early fault detection.",1063-6773,978-1-4244-2613-3978-1-4244-2614,10.1109/ICSM.2008.4658073,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658073,,Fault detection;Testing;Heuristic algorithms;Modeling;Life estimation;Software systems;USA Councils,program diagnostics;program testing;software maintenance,fault detection;regression test suite prioritization;state-based system modeling;system maintenance;source-code modification,,24,24,,,,,,IEEE,IEEE Conferences
Application-Level QoS: Improving Video Conferencing Quality through Sending the Best Packet Next,I. McDonald; R. Nelson,NA; NA,"2008 The Second International Conference on Next Generation Mobile Applications, Services, and Technologies",,2008,,,507,513,"In a traditional network stack, data from an application is transmitted in the order that it is received. An algorithm is proposed where information about the priority of packets and expiry times is used by the transport layer to reorder or discard packets at the time of transmission to optimise the use of available bandwidth. This can be used for video conferencing to prioritise important data. This scheme is implemented and compared to unmodified datagram congestion control protocol (DCCP). This algorithm is implemented as an interface to DCCP and tested using traffic modelled on video conferencing software. The results show improvement can be made to video conferencing during periods of congestion - substantially more audio packets arrive on time with the algorithm, which leads to higher quality video conferencing. In many cases video packet arrival rate also increases and adopting the algorithm gives improvements to video conferencing that are better than using unmodified queuing for DCCP. The algorithm proposed is implemented on the server only, so benefits can be obtained on the client without changes being required to the client.",2161-2889;2161-2897,978-0-7695-3333,10.1109/NGMAST.2008.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4756479,,Videoconference;Streaming media;Traffic control;Testing;Telecommunication traffic;Next generation networking;Mobile computing;Computer networks;Bandwidth;Protocols,data communication;quality of service;telecommunication congestion control;telecommunication traffic;teleconferencing;transport protocols,application-level QoS;video conferencing;datagram congestion control protocol;traffic modelling;congestion;audio packets;video packet arrival rate;unmodified queuing;data transmission;send best packet next algorithm,,3,29,,,,,,IEEE,IEEE Conferences
Applying Particle Swarm Optimization to Prioritizing Test Cases for Embedded Real Time Software Retesting,K. H. S. Hla; Y. Choi; J. S. Park,NA; NA; NA,2008 IEEE 8th International Conference on Computer and Information Technology Workshops,,2008,,,527,532,"In recent years, complex embedded systems are used in every device that is infiltrating our daily lives. Since most of the embedded systems are multi-tasking real time systems, the task interleaving issues, dead lines and other factors needs software units retesting to follow the subsequence changes. Regression testing is used for the software maintenance that revalidates the old functionality of the software unit. Testing is one of the most complex and time-consuming activities, in which running of all combination of test cases in test suite may require a large amount of efforts. Test case prioritization techniques can take advantage that orders test cases, which attempts to increase effectiveness in regression testing. This paper proposes to use particle swarm optimization (PSO) algorithm to prioritize the test cases automatically based on the modified software units. Regarding to the recent investigations, PSO is a multi-object optimization technique that can find out the best positions of the objects. The goal is to prioritize the test cases to the new best order, based on modified software components, so that test cases, which have new higher priority, can be selected in the regression testing process. The empirical results show that by using the PSO algorithm, the test cases can be prioritized in the test suites with their new best positions effectively and efficiently.",,978-0-7695-3242-4978-0-7695-3239,10.1109/CIT.2008.Workshops.104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4568558,embedded system;real time software system;test case coverage;evolutionary structural testing;regression testing;particle swarm optimization,Particle swarm optimization;Software testing;Embedded software;Aerospace testing;System testing;Embedded system;Embedded computing;Real time systems;Aerospace engineering;Interleaved codes,embedded systems;multiprogramming;particle swarm optimisation;program testing;software maintenance,particle swarm optimization;embedded real time software retesting;complex embedded systems;multitasking real time systems;software units;regression testing;software maintenance;multiobject optimization technique,,14,13,,,,,,IEEE,IEEE Conferences
Automated Generation and Assessment of Autonomous Systems Test Cases,K. J. Barltrop; K. H. Friberg; G. A. Horvath,"Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-354-6412, Kevin.J.Barltrop@jpl.nasa.gov; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-393-4077, Kenneth.H.Friberg@jpl.nasa.gov, Friberg.Autonomy@gmail.com; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Drive, Pasadena, CA 91109. 818-39 3-6234, Gregory.A.Horvath@jpl.nasa.gov",2008 IEEE Aerospace Conference,,2008,,,1,10,"Verification and validation testing of autonomous spacecraft routinely culminates in the exploration of anomalous or faulted mission-like scenarios. Prioritizing which scenarios to develop usually comes down to focusing on the most vulnerable areas and ensuring the best return on investment of test time. Rules-of-thumb strategies often come into play, such as injecting applicable anomalies prior to, during, and after system state changes; or, creating cases that ensure good safety-net algorithm coverage. Although experience and judgment in test selection can lead to high levels of confidence about the majority of a system's autonomy, it's likely that important test cases are overlooked. One method to fill in potential test coverage gaps is to automatically generate and execute test cases using algorithms that ensure desirable properties about the coverage. For example, generate cases for all possible fault monitors, and across all state change boundaries. Of course, the scope of coverage is determined by the test environment capabilities, where a faster-than-real-time, high-fidelity, software-only simulation would allow the broadest coverage. Even real-time systems that can be replicated and run in parallel, and that have reliable set-up and operations features provide an excellent resource for automated testing. Making detailed predictions for the outcome of such tests can be difficult, and when algorithmic means are employed to produce hundreds or even thousands of cases, generating predicts individually is impractical, and generating predicts with tools requires executable models of the design and environment that themselves require a complete test program. Therefore, evaluating the results of large number of mission scenario tests poses special challenges. A good approach to address this problem is to automatically score the results based on a range of metrics. Although the specific means of scoring depends highly on the application, the use of formal scoring metrics has high value in identifying and prioritizing anomalies, and in presenting an overall picture of the state of the test program. In this paper we present a case study based on automatic generation and assessment of faulted test runs for the Dawn mission, and discuss its role in optimizing the allocation of resources for completing the test program.",1095-323X,978-1-4244-1487-1978-1-4244-1488,10.1109/AERO.2008.4526484,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4526484,,Automatic testing;System testing;Space vehicles;Orbital robotics;Power system protection;Propulsion;Laboratories;Space technology;Investments;Software testing,aerospace testing;automatic testing;space vehicles,automated autonomous systems test case generation;validation testing;verification testing;faulted mission-like scenarios;rules-of-thumb strategies;safety-net algorithm coverage;software-only simulation;automated testing;formal scoring metrics;Dawn mission,,3,1,,,,,,IEEE,IEEE Conferences
Change Priority Determination in IT Service Management Based on Risk Exposure,J. Sauve; R. Santos; R. Reboucas; A. Moura; C. Bartolini,Member; NA; NA; NA; Member,IEEE Transactions on Network and Service Management,,2008,5,3,178,187,"In the Change Management process within IT Service Management, some activities need to evaluate the risk exposure associated with changes to be made to the infrastructure and services. The paper presents a method to evaluate risk exposure associated with a change. Further, we show how to use the risk exposure metric to automatically assign priorities to changes. The formal model developed for this purpose captures the business perspective by using financial metrics in the evaluation of risk. Thus the method is an example of Business-Driven IT Management. A case study, performed in conjunction with a large IT service provider, is reported and provides good results when compared to decisions made by human managers.",1932-4537;2373-7379,,10.1109/TNSM.2009.031105,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4805134,"Change management, risk, change prioritization, IT service management, business-driven IT management",Risk management;Quality management;Scheduling;Information technology;Testing;Computational Intelligence Society;Software systems;Embedded software;Humans;Process control,business data processing;management of change;risk analysis,change priority determination;IT service management;risk exposure metric;financial metrics;business-driven IT management,,16,20,,,,,,IEEE,IEEE Journals & Magazines
Choosing the Right Prioritisation Method,S. Hatton,NA,19th Australian Conference on Software Engineering (aswec 2008),,2008,,,517,526,"There are many methods available for prioritising software requirements. Choosing the most suitable one can often be quite difficult. A number of factors need to be considered such as the project development methodology being used, the amount of time available, the amount of information known about requirements, the stage of the project and the degree of information about priority required. This paper examines the type of information available at different stages in a project and matches it to the properties of prioritisation methods. It then recommends the usage of specific prioritisation methods at certain stages of a project.",1530-0803;2377-5408,978-0-7695-3100,10.1109/ASWEC.2008.4483241,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4483241,Requirements Prioritisation;Software Engineering;Analytical Hierarchy Process;Hundred Dollar Method;Simple Ranking;MoSCoW;Efficient Prioritisation,Australia;Software engineering;Iterative methods;Dynamic programming;Computer science;Road transportation;Scheduling;Decision making;Computer industry;System testing,software engineering;systems analysis,software requirements;prioritisation methods;software development process,,8,30,,,,,,IEEE,IEEE Conferences
Collaborative group membership and access control for JXTA,,,2008 3rd International Conference on Communication Systems Software and Middleware and Workshops (COMSWARE '08),,2008,,,159,166,"This paper presents a proposal for group membership and access control services for JXTA, both based on the principle of self-organization and collaboration of peer group members. The need for collaboration strengthens the resistance against free riding and eases management of revocation data. The proposal prioritizes group autonomy and makes use of the concepts of web of trust and small world phenomenon in order to achieve its ends, distancing itself from approaches based on centralized PKI models or trusted third parties external to the group. It also offers an alternative to the basic group membership services distributed with the JXTA platform implementations.",,978-1-4244-1796-4978-1-4244-1797,10.1109/COMSWA.2008.4554399,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4554399,Access control;peer-to-peer;security;group membership;JXTA;distributed systems;web of trust;Java,Collaboration;Access control;Authentication;Proposals;Peer to peer computing;Guidelines;Access protocols;Identity management systems;Graphical user interfaces;Testing,authorisation;formal specification;groupware;peer-to-peer computing;public key cryptography,collaborative group membership;access control;JXTA specification framework;peer group members;group autonomy;centralized PKI models,,1,16,,,,,,IEEE,IEEE Conferences
Creating Agile Streams for Business & Technical Value,Z. Hunter; D. Spann,NA; NA,Agile 2008 Conference,,2008,,,144,147,"Have you ever played the role of business owner and found yourself between ""a rock and a hard place"" of organizational politics when prioritizing backlog features? The Agile Stream approach negates those politics by dedicating development teams to organizational units and allowing those teams to continue working, iteration after iteration, as long as they continue delivering business value.",,978-0-7695-3321,10.1109/Agile.2008.93,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599466,Agile;Stream;Executive;Process;Flow,Business;Testing;Companies;Planning;Lead;Best practices;Resource management,software engineering,Agile streams;organizational politics;dedicating development teams;organizational units;business value,,,,,,,,,IEEE,IEEE Conferences
Effective RTL Method to Develop On-Line Self-Test Routine for the Processors Using the Wavelet Transform,A. Asghari; S. A. Motamedi; S. Attarchi,NA; NA; NA,Seventh IEEE/ACIS International Conference on Computer and Information Science (icis 2008),,2008,,,33,38,"In this paper, we introduce a new efficient register transfer level (RTL) method to develop on-line self- test routines. We consider some prioritizations to select the components and instructions of the processor. In addition, we choose test patterns based on spectral RTL test pattern generation (TPG) strategy. For the purpose of spectral analysis, we use the wavelet transform. Also, we use a few extra instructions for the purpose of the signature monitoring to detect control flow errors. We demonstrate that the combination of these three strategies is effective for developing small test programs with high fault coverage in a small test development time. In this case, we only need the instruction set architecture (ISA) and RTL information. Our method not only provides a simple and fast algorithm for on-line self-test applications, also gains the advantages of utilizing lower memory and reducing the test generation time complexities in comparison with proposed methods so far. We focus on the application of this approach for Parwan processor. We develop a self-test routine using our proposed method for Parwan processor and demonstrate the effectiveness of our proposed methodology for on-line testing by presenting experimental results for Parwan processor.",,978-0-7695-3131,10.1109/ICIS.2008.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4529795,Processor testing;On-line low-cost testing;Spectral test pattern generating;Software-based self-testing (SBST);Register transfer level (RTL),Built-in self-test;Wavelet transforms;Automatic testing;Registers;Test pattern generators;Spectral analysis;Wavelet analysis;Monitoring;Error correction;Instruction sets,automatic test pattern generation;circuit complexity;error detection;fault diagnosis;instruction sets;integrated circuit testing;microprocessor chips;wavelet transforms,register transfer level method;Parwan processor online self-test routine;spectral RTL test pattern generation strategy;wavelet transform;signature monitoring;control flow error detection;fault coverage;instruction set architecture;test generation time complexity,,,14,,,,,,IEEE,IEEE Conferences
Evaluating ALPHAN: A Communication Protocol for Haptic Interaction,H. A. Osman; M. Eid; A. E. Saddik,"Multimedia Communications Research Laboratory - MCRLab, School of Information Technology and Engineering - University of Ottawa, Ottawa, Ontario, K1N 6N5, Canada, HALOsman@mcrlab.uottawa.ca; Multimedia Communications Research Laboratory - MCRLab, School of Information Technology and Engineering - University of Ottawa, Ottawa, Ontario, K1N 6N5, Canada, eid@mcrlab.uottawa.ca; Multimedia Communications Research Laboratory - MCRLab, School of Information Technology and Engineering - University of Ottawa, Ottawa, Ontario, K1N 6N5, Canada, abed@mcrlab.uottawa.ca",2008 Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems,,2008,,,361,366,"In our previous work we introduced a novel application layer protocol, named ALPHAN, for haptic data communication. The protocol is characterized by three distinguished features: first, it is designed at the application layer to enable the application to define and control the networking parameters. Second, it is made highly customizable using XML-based descriptions. Finally, the protocol supports multi-buffering mechanisms to prioritize the communicated information. In this paper, we present a thorough evaluation of the protocol using a collaborative haptic game; the balance ball game that we developed is for this purpose. The performance metrics and the test bed of the protocol evaluation are also discussed. Finally, we comment on our findings and provide directions for prospective research.",2324-7347;2324-7355,978-1-4244-2005,10.1109/HAPTICS.2008.4479972,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4479972,Tele-Haptics;networking protocol;XML descriptions;collaborative haptics;H.5.2 [Information Interfaces and Presentation]: User Interfaces ?¨ Haptic I/O,Haptic interfaces;Collaboration;Transport protocols;Jitter;Application software;Data communication;Communication system control;Delay;Quality of service;Multimedia communication,computer games;data communication;groupware;haptic interfaces;transport protocols;XML,communication protocol;haptic interaction;application layer protocol;haptic data communication;XML-based descriptions;multibuffering mechanisms;collaborative haptic game,,12,19,,,,,,IEEE,IEEE Conferences
Historical Value-Based Approach for Cost-Cognizant Test Case Prioritization to Improve the Effectiveness of Regression Testing,H. Park; H. Ryu; J. Baik,NA; NA; NA,2008 Second International Conference on Secure System Integration and Reliability Improvement,,2008,,,39,46,"Regression testing has been used to support software testing activities and assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive because it requires many test case executions, and the number of test cases increases sharply as the software evolves. In this paper, we propose the Historical Value-Based Approach, which is based on the use of historical information, to estimate the current cost and fault severity for cost-cognizant test case prioritization. We also conducted a controlled experiment to validate the proposed approach, the results of which proved the proposed approachpsilas usefulness. As a result of the proposed approach, software testers who perform regression testing are able to prioritize their test cases so that their effectiveness can be improved in terms of average percentage of fault detected per cost.",,978-0-7695-3266,10.1109/SSIRI.2008.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4579792,Test Case Prioritization;Regression Testing;Software Testing;Software Engineering,Testing;Software;Software systems;Fault detection;Software testing;Current measurement;Probability,program testing;quality assurance;software cost estimation;software fault tolerance;software quality,historical value-based approach;cost-cognizant test case prioritization;regression testing;software testing activity;software program quality assurance;software program version;software fault severity detection;software cost estimation,,20,22,,,,,,IEEE,IEEE Conferences
Incorporating varying requirement priorities and costs in test case prioritization for new and regression testing,K. Ramasamy; S. Arul Mary,"Department of Information Technology, Bharathidasan Institute of Technology, Bharathidasan University, Trichy-24, Tamilnadu, India; Department of Information Technology, Bharathidasan Institute of Technology, Bharathidasan University, Trichy-24, Tamilnadu, India","2008 International Conference on Computing, Communication and Networking",,2008,,,1,9,"Test case prioritization schedules the test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects the most severe faults at the earliest in its testing life cycle. Test case prioritization techniques have proved to be beneficial for improving regression testing activities. While code coverage based prioritization techniques are found to be taken by most scholars, test case prioritization based on requirements in a cost effective manner has not been taken for study so far. Hence, in this paper, we propose to put forth a model for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software that can also be cost effective and to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the six factors: customer priority, changes in requirement, implementation complexity, usability, application flow and fault impact. The proposed prioritization technique is experimented in three phases with student projects and two sets of industrial projects and the results show convincingly that the proposed prioritization technique improves the rate of severe fault detection.",,978-1-4244-3594-4978-1-4244-3595,10.1109/ICCCNET.2008.4787662,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4787662,Fault severity;Rate of fault detection;Regression testing;Requirement weight;Requirement cost;Test case prioritization;system testing;customer satisfaction,Costs;Fault detection;System testing;Software testing;Software quality;Life testing;Usability;Application software;Software engineering;Job shop scheduling,formal specification;program testing;regression analysis;scheduling;software fault tolerance;software quality,varying requirement priorities;test case prioritization scheduling;regression testing;fault detection;code coverage based prioritization techniques;software requirement specification;user satisfaction;software quality;customer priority;implementation complexity;application flow;fault impact,,5,30,,,,,,IEEE,IEEE Conferences
mod kaPoW: Protecting the web with transparent proof-of-work,E. Kaiser; Wu-chang Feng,"Portland State University, USA; Portland State University, USA",IEEE INFOCOM Workshops 2008,,2008,,,1,6,"Attacks from automated Web clients are a significant problem on the Internet. Web sites often employ Turing tests known as CAPTCHAs to combat automated agents. Unfortunately, such defenses require frequent human user input, are becoming less effective as computer vision techniques improve, and can be subverted by adversaries willing to hire humans to solve challenges. Several alternative defenses based upon cryptographic methods rather than human input have been proposed to achieve the same goals. Such ""proof-of-work"" techniques prioritize clients based on their willingness to solve computational challenges of client-specific difficulty set by the server. Unfortunately, few proof-of-work schemes have been deployed since they require wide-scale adoption of special client software to operate properly. To address these problems we present mocLkaPoW, a novel system that has the efficiency and human-transparency of proof-of-work schemes as well as the software backwards-compatibility of CAPTCHA schemes. The system leverages common Web technologies to deliver a challenge, solve it, and submit the client response, while providing accessibility for legacy clients. This paper describes and evaluates a prototype of this system.",,978-1-4244-2219,10.1109/INFOCOM.2008.4544602,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4544602,,Protection;Uniform resource locators;Humans;Access protocols;Filters;Web server;Java;Internet;Automatic testing;Robotics and automation,client-server systems;Internet;telecommunication security,mod_kaPoW;World Wide Web;transparent proof-of-work;Web client attacks;Internet;Web site;software backwards-compatibility;CAPTCHA scheme,,1,16,,,,,,IEEE,IEEE Conferences
Model for optimizing software testing period using non homogenous poisson process based on cumulative test case prioritization,P. R. Srivastava,"Computer Science and Information Systems Group, Birla Institute of Technology and Science (BITS) Pilani-333031, INDIA",TENCON 2008 - 2008 IEEE Region 10 Conference,,2008,,,1,6,"Most of the software organizations has trouble when deciding the release dates their product. This difficulty is due to the fact that an under tested software could lead to many bugs propping up at the client side which would in turn lead to expensive bug-fixes and more importantly loss of customer goodwill. On the other hand, testing beyond certain time would lead to loss of revenue to the organization due to the dilution of the early bird advantage. The aim of our paper is optimizes the time and cost of entire software. In this paper we used non homogeneous Poisson process model based on cumulative priority. Our paper also tries to answer when to release any software.",2159-3442;2159-3450,978-1-4244-2408-5978-1-4244-2409,10.1109/TENCON.2008.4766422,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4766422,Non Homogenous Poisson Process;Cumulative Priority;Software Life Cycle Length;Software Release Time;Optimal Testing Policy,Software testing;Computer bugs;Cost function;Logic testing;Stochastic processes;System testing;Computer science;Information systems;Birds;Life testing,higher order statistics;program debugging;program testing;software cost estimation;stochastic processes,software testing period optimization;nonhomogenous Poisson process model;cumulative test case prioritization;software organization;software release date;software bug fixing;software cost optimization;software time optimization,,1,14,,,,,,IEEE,IEEE Conferences
"Perspective on Embedded Systems: Challenges, Solutions and Research Priorities",D. Vernay,"CTO, THALES, Paris, France","2008 Design, Automation and Test in Europe",,2008,,,2,2,"This paper introduces THALES vision and research priorities for embedded systems and illustrates them through presentations of solutions and on-going research projects and initiatives. Thales effort related to mission-critical systems is focused on advanced high-performance embedded computing platforms, on middleware technologies, on software systems design and verification tools for safety and security and on the emergence of open standards in these domains. THALES is also actively contributing to the development of innovation eco-systems: the Joint Undertaking ARTEMIS in Europe; the Pole de Competitivite SYSTEM@TIC PARIS REGION in France.",1530-1591;1558-1101,978-3-9810801-3-1978-3-9810801-4,10.1109/DATE.2008.4484650,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4484650,,Embedded system;Mission critical systems;Embedded computing;Middleware;Software systems;Software design;Software safety;Security;Software standards;Technological innovation,embedded systems;middleware;program verification;software tools,THALES vision;mission-critical systems;advanced high-performance embedded computing platforms;middleware technologies;software systems design;verification tools,,,,,,,,,IEEE,IEEE Conferences
Prioritizing User-Session-Based Test Cases for Web Applications Testing,S. Sampath; R. C. Bryce; G. Viswanath; V. Kandimalla; A. G. Koru,NA; NA; NA; NA; NA,"2008 1st International Conference on Software Testing, Verification, and Validation",,2008,,,141,150,"Web applications have rapidly become a critical part of business for many organizations. However, increased usage of Web applications has not been reciprocated with corresponding increases in reliability. Unique characteristics, such as quick turnaround time, coupled with growing popularity motivate the need for efficient and effective Web application testing strategies. In this paper, we propose several new test suite prioritization strategies for Web applications and examine whether these strategies can improve the rate of fault detection for three Web applications and their preexisting test suites. We prioritize test suites by test lengths, frequency of appearance of request sequences, and systematic coverage of parameter-values and their interactions. Experimental results show that the proposed prioritization criteria often improve the rate of fault detection of the test suites when compared to random ordering of test cases. In general, the best prioritization metrics either (1) consider frequency of appearance of sequences of requests or (2) systematically cover combinations of parameter-values as early as possible.",2159-4848,978-0-7695-3127,10.1109/ICST.2008.42,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539541,web application testing;pair-wise interaction coverage;pair-wise interaction testing;user-session-based testing;test case prioritization,System testing;Fault detection;Software testing;Application software;Frequency;Information systems;Computer science;Robustness;Computer bugs;Web server,Internet;program testing;software fault tolerance,user-session-based test case;Web application testing;application reliability;test suite prioritization;fault detection;prioritization metrics;request sequence,,30,30,,,,,,IEEE,IEEE Conferences
Providing the Guideline of Determining Quality Checklists Priorities Based on Evaluation Records of Software Products,C. Lee; B. Lee; C. Wu,NA; NA; NA,2008 15th Asia-Pacific Software Engineering Conference,,2008,,,169,176,"COTS (commercial-off-the-shelf) software products are usually provided in a packaged style without the source code but with many ready-to-use functions. Generally, their vendors are reluctant to disclose the source code. Thus, the major way of quality evaluation and certification requires dynamic behavior testing, essentially black-box testing. Since observing every aspect of external software behavior is almost impossible, it is crucial to designate an adequate range for quality evaluation such as an adequate number of quality checklists or product quality metrics for external behavior testing. Hence, to establish rules of selecting quality evaluation criteria in systematic ways, there have been attempts to analyze and utilize the past records of software evaluation. In this paper, multiple characteristics of software are mapped as nodes to affect and determine the priority ranks of external software quality metrics on Bayesian belief network. The nodes are set to be under the influence of multiple inheritances so that every external characteristic of COTS software is considered thoroughly.",1530-1362;1530-1362,978-0-7695-3446,10.1109/APSEC.2008.75,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724545,software quality;checklist;priority,Guidelines;Software quality;Computer science;Certification;Software testing;Bayesian methods;Embedded software;Application software;Software engineering;Software packages,belief networks;software metrics;software packages;software quality,commercial-off-the-shelf software;dynamic behavior testing;black-box testing;quality evaluation criteria;software quality metrics;Bayesian belief network,,,14,,,,,,IEEE,IEEE Conferences
Quota-constrained test-case prioritization for regression testing of service-centric systems,Shan-Shan Hou; Shan-Shan Hou; Lu Zhang; Lu Zhang; Tao Xie; Jia-Su Sun; Jia-Su Sun,"Key Lab. of High Confidence Software Technol., Peking Univ., Beijing; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing; Dept. of Comput. Sci., North Carolina State Univ., Raleigh, NC; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing; Key Lab. of High Confidence Software Technol., Peking Univ., Beijing",2008 IEEE International Conference on Software Maintenance,,2008,,,257,266,"Test-case prioritization is a typical scenario of regression testing, which plays an important role in software maintenance. With the popularity of Web services, integrating Web services to build service-centric systems (SCSs) has attracted attention of many researchers and practitioners. During regression testing, as SCSs may use up constituent Web servicespsila request quotas (e.g., the upper limit of the number of requests that a user can send to a Web service during a certain time range), the quota constraint may delay fault exposure and the subsequent debugging. In this paper, we investigate quota-constrained test-case prioritization for SCSs, and propose quota-constrained strategies to maximize testing requirement coverage. We divide the testing time into time slots, and iteratively select and prioritize test cases for each time slot using integer linear programming (ILP). We performed an experimental study on our strategies together with three other strategies, and the results show that with the constraint of request quotas, our strategies can schedule test cases for execution in an order with higher effectiveness in exposing faults and achieving total and additional branch coverage.",1063-6773,978-1-4244-2613-3978-1-4244-2614,10.1109/ICSM.2008.4658074,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4658074,,Testing;Web services;Software;Partitioning algorithms;Integer linear programming;Computer science;Laboratories,integer programming;linear programming;program testing;regression analysis;software maintenance;Web services,quota-constrained test-case prioritization;regression testing;service-centric systems;software maintenance;Web services;quota constraint;integer linear programming,,10,22,,,,,,IEEE,IEEE Conferences
Ranking Attack-Prone Components with a Predictive Model,M. Gegick; L. Williams,NA; NA,2008 19th International Symposium on Software Reliability Engineering (ISSRE),,2008,,,315,316,"Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. An early security risk analysis that ranks software components by probability of being attacked can provide an affordable means to prioritizing fortification efforts to the highest risk components. We created a predictive model using classification and regression trees and the following internal metrics: quantity of Klocwork static analysis warnings, file coupling, and quantity of changed and added lines of code. We validated the model against pre-release security testing failures on a large commercial telecommunications system. The model assigned a probability of attack to each file where upon ranking the probabilities in descending order we found that 72% of the attack-prone files are in the top 10% of the ranked files and 90% in the top 20% of the files.",1071-9458;2332-6549,978-0-7695-3405,10.1109/ISSRE.2008.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700353,Attack-prone,Predictive models;Security;Performance analysis;Input variables;Buffer overflow;Software systems;Risk analysis;Classification tree analysis;Regression tree analysis;Failure analysis,probability;program diagnostics;regression analysis;security of data;trees (mathematics),attack-prone component;predictive model;security risk analysis;classification trees;regression trees;Klocwork static analysis warnings;file coupling,,2,4,,,,,,IEEE,IEEE Conferences
Scheduling Product Line Features for Effective Roadmapping,J. Savolainen; J. Kuusela,NA; NA,2008 15th Asia-Pacific Software Engineering Conference,,2008,,,195,202,"Large industrial product lines may produce tens of thousands of variants each year. Each variant typically contains both reusable assets as well as product specific code created by different organizational units. To produce this vast number of variants the organizational resources must be used efficiently. For roadmapping this means an ability to schedule production of reusable assets so that all variants can be completed according to their requirements. When aiming for centralized variability management, roadmapping requires effective management of product line feature dependences and priorities. In this paper, we first introduce the problems haunting feature roadmapping in industrial product lines. Then we investigate how these problems can be solved using a novel approach for organizing product lines based on our practical experiences. Finally, we discuss our experiences and compare our approach with results by other researchers.",1530-1362;1530-1362,978-0-7695-3446,10.1109/APSEC.2008.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724548,Product line;reuse;roadmapping;feature modelling,Job shop scheduling;Software engineering;Computer industry;Production;Organizing;Testing;Technology management;Pricing;Portfolios,formal specification;product development;scheduling;software development management;software maintenance;software reusability,software product line feature dependency management;reusable asset;product specific code;organizational resource;requirements scheduling;centralized variability management;product line roadmapping;software product line evolution,,,13,,,,,,IEEE,IEEE Conferences
Scheduling Timed Modules for Correct Resource Sharing,C. Seceleanu; P. Pettersson; H. Hansson,NA; NA; NA,"2008 1st International Conference on Software Testing, Verification, and Validation",,2008,,,102,111,"Real-time embedded systems typically include concurrent tasks of different priorities with time-dependent operations accessing common resources. In this context, unsynchronized parallel executions may lead to hazard situations caused by e.g., race conditions. To be able to detect such faulty system behaviors before implementation, we introduce a unified model of resource constrained, scheduled real-time system descriptions, in Alur's and Henzinger's rigorous framework of timed reactive modules. We take a component-based design perspective and construct the realtime system model, by refinement, as a composition of realtime periodic preemptible tasks with encoded functionality, and a fixed-priority scheduler, all modeled as timed modules. For the model, we express the notions of race condition and redundant locking, formally, as invariance properties that can be verified by model-checking.",2159-4848,978-0-7695-3127,10.1109/ICST.2008.70,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4539537,real-time systems;component-based design;timed modules;resource management,Resource management;Real time systems;Scheduling;Embedded system;Protocols;Delay;Concurrent computing;Timing;Encoding;Software testing,embedded systems;parallel processing;peer-to-peer computing;program verification,resource sharing;real-time embedded systems;time-dependent operations;unsynchronized parallel executions;resource constrained unified model;component-based design;fixed-priority scheduler;invariance properties;model-checking verification,,,20,,,,,,IEEE,IEEE Conferences
Slack-based global multiprocessor scheduling of aperiodic tasks in parallel embedded real-time systems,L. Lundberg; H. Lennerstad,"Department of Systems and Software, School of Engineering, Blekinge Institute of Technology, 372 25 Ronneby, Sweden; Department of Systems and Software, School of Engineering, Blekinge Institute of Technology, 372 25 Ronneby, Sweden",2008 IEEE/ACS International Conference on Computer Systems and Applications,,2008,,,465,472,"We provide a constant time schedulability test and priority assignment algorithm for an on-line multiprocessor server handling aperiodic tasks. Dhall's effect is avoided by dividing tasks in two priority classes based on their utilization: heavy and light. The improvement in this paper is due to assigning priority of light tasks based on slack - not on deadlines. We prove that if the load on the multiprocessor stays below (3 - radic5)/2 ap 38.197%, the server can accept an incoming aperiodic task and guarantee that the deadlines of all accepted tasks will be met. This is better than the current state-of- the-art algorithm where the priorities of light tasks are based on deadlines (the corresponding bound is in that case 35.425%).",2161-5322;2161-5330,978-1-4244-1967-8978-1-4244-1968,10.1109/AICCSA.2008.4493574,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4493574,,Processor scheduling;Real time systems;Scheduling algorithm;Delay;Embedded software;Software systems;Software testing;System testing;Software algorithms;Admission control,embedded systems;multiprocessing systems;processor scheduling,slack-based global multiprocessor scheduling;aperiodic tasks;parallel embedded real-time systems;time schedulability test;priority assignment algorithm,,1,10,,,,,,IEEE,IEEE Conferences
Software development methods and usability: Perspectives from a survey in the software industry in Norway,B. Bygstad; G. Ghinea; E. Brevik,NA; NA; NA,Interacting with Computers,,2008,20,3,375,385,"This paper investigates the relationship between software development methodologies and usability. The point of departure is the assumption that two important disciplines in software development, one of software development methods (SDMs) and one of usability work, are not integrated in industrial software projects.Building on previous research we investigate two questions; (1) Will software companies generally acknowledge the importance of usability, but not prioritise it in industrial projects? and (2) To what degree are software development methods and usability perceived by practitioners as being integrated? To this end a survey in the Norwegian IT industry was conducted. From a sample of 259 companies we received responses from 78 companies.In response to our first research question, our findings show that although there is a positive bias towards usability, the importance of usability testing is perceived to be much less than that of usability requirements. Given the strong time and cost pressures associated with the software industry, we believe that these results highlight that there is a gap between intention and reality. Regarding our second research question our survey revealed that companies perceive usability and software development methods to be integrated. This is in contrast to earlier research, which, somewhat pessimistically, has argued for the existence of two different cultures, one of software development and one of usability. The findings give hope for the future, in particular because the general use of system development methods are pragmatic and adaptable.",0953-5438;1873-7951,,10.1016/j.intcom.2007.12.001,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8149869,Software development methods;Usability;Software industry;Survey,,,,,6,,,,,,,OUP,OUP Journals & Magazines
Software Quality model Analysis Program,A. A. Hamada; M. N. Moustafa; H. I. Shaheen,"EDS, 29 Emtedad Ramses st., Nasr city, Cairo 11471, Egypt; Ain-Shams University, Faculty of Engineering, Computer and Systems Department, 1 Elsarayat st., Abassia, Cairo 11571, Egypt; Ain-Shams University, Faculty of Engineering, Computer and Systems Department, 1 Elsarayat st., Abassia, Cairo 11571, Egypt",2008 International Conference on Computer Engineering & Systems,,2008,,,296,300,"It is vital that data is obtained so that actions can be taken to improve the performance. Such improvement can be measured in terms of improved quality, increased customer satisfaction and decreased cost of quality. Different researchers have proposed software quality models to help measure the quality of software products. These models often include metrics for this purpose. Some of the classical and recent models are discussed and analyzed in this paper showing the points of strength and weakness of each model type. A new comprehensive model is proposed and analyzed. A complete solution is discussed through the paper to enable an effective and efficient use of the proposed model to help the development team in prioritizing the important metrics while developing the software products according to some inputs from the user and the objectives of the software being developed. The solution developed is called the quality model analysis program (QAP) and is a fuzzy system that weights the proposed model attributes according to certain rules. The solution enables software project managers to better utilize their resources and take specific actions to better improve the quality of the software produced.",,978-1-4244-2115-2978-1-4244-2116,10.1109/ICCES.2008.4773015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4773015,,Software quality;Software measurement;Costs;ISO standards;IEC standards;Software testing;Q factor;Performance analysis;Cities and towns;Data engineering,software metrics;software quality,software quality model analysis program;customer satisfaction;comprehensive model;software products;software project managers,,2,12,,,,,,IEEE,IEEE Conferences
Software Quality Requirements: How to Balance Competing Priorities,J. D. Blaine; J. Cleland-Huang,independent consultant; DePaul University,IEEE Software,,2008,25,2,22,24,"The elicitation, analysis, and specification of quality requirements involve careful balancing of a broad spectrum of competing priorities. Developers must therefore focus on identifying qualities and designing solutions that optimize the product's value to its stakeholders.",0740-7459;1937-4194,,10.1109/MS.2008.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4455627,Quality requirements;non-functional requirements;product value;architectural qualities,Software quality;Security;Safety;Usability;Testing;Terminology;Dictionaries;Taxonomy;Delay;Costs,,,,19,5,,,,,,IEEE,IEEE Journals & Magazines
Test Case Prioritization Based on Analysis of Program Structure,Z. Ma; J. Zhao,NA; NA,2008 15th Asia-Pacific Software Engineering Conference,,2008,,,471,478,"Test case prioritization techniques have been empirically proved to be effective in improving the rate of fault detection in regression testing. However, most of previous techniques assume that all the faults have equal severity, which dose not meet the practice. In addition, because most of the existing techniques rely on the information gained from previous execution of test cases or source code changes, few of them can be directly applied to non-regression testing. In this paper, aiming to improve the rate of severe faults detection for both regression testing and non-regression testing, we propose a novel test case prioritization approach based on the analysis of program structure. The key idea of our approach is the evaluation of testing-importance for each module (e.g., method) covered by test cases. As a proof of concept, we implement $Apros$, a test case prioritization tool, and perform an empirical study on two real, non-trivial Java programs. The experimental result represents that our approach could be a promising solution to improve the rate of severe faults detection.",1530-1362;1530-1362,978-0-7695-3446,10.1109/APSEC.2008.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4724580,Test case prioritization;call graph;program analysis,Fault detection;Software testing;Computer science;Computer bugs;Software engineering;Performance evaluation;Java;Information analysis;Fault location;System testing,Java;program testing;software engineering,test case prioritization;program structure analysis;regression testing;nontrivial Java programs,,11,21,,,,,,IEEE,IEEE Conferences
Test Case Prioritization for Multiple Processing Queues,B. Qu; C. Nie; B. Xu,NA; NA; NA,2008 International Symposium on Information Science and Engineering,,2008,2,,646,649,"Test case prioritization is an effective technique that helps to increase the rate of fault detection or code coverage in regression testing. However, all existing methods can only prioritize test cases to a single queue. Once there are two or more machines that participate in testing, all exiting techniques are not applicable any more. To extend the prioritization methods to parallel scenario, this paper defines the prioritization problem in such scenario and applies the task scheduling method to prioritization algorithms to help partitioning a test suite into multiple prioritized subsets. Besides, this paper also discusses the limitation of previous metrics and proposes a new measure of effectiveness of prioritization methods in a parallel scenario. Finally, a case study is performed to illustrate the algorithms and metrics presented in this article.",2160-1283;2160-1291,978-0-7695-3494-7978-1-4244-2727,10.1109/ISISE.2008.106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4732476,Regression testing;parallel scenario;metrics;test case prioritization,Partitioning algorithms;Scheduling algorithm;Information science;Computer science;Fault detection;Life testing;Software testing;Performance evaluation;Education;Feedback,parallel processing;program diagnostics;program testing;scheduling;software metrics;software prototyping,multiple processing queue;test case prioritization;fault detection;regression testing;parallel scenario;task scheduling;software metrics;software life cycle,,,10,,,,,,IEEE,IEEE Conferences
"Testing Optimization for Mission-Critical, Complex, Distributed Systems",M. G. Stochel; R. Sztando,NA; NA,2008 32nd Annual IEEE International Computer Software and Applications Conference,,2008,,,847,852,"The goal of the research was to optimize the regression testing of the software application to address the identified problem of a missing, unclear or even contradictory requirement. The approach was mainly aimed at regression test prioritization and selection of regression test cases per test campaigns. A combination of subjective data based on expert knowledge and objective historical data were the inputs to the model where the output was determining the quality of test selection. Proposed model is aimed at finding newly introduced defects, and it could be extremely useful when the system is in the state of cleaning up or ordering requirements.",0730-3157;0730-3157,978-0-7695-3262,10.1109/COMPSAC.2008.96,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4591679,mission critical;testing optimization;six sigma;DMAIC;public safety,Testing;Software;Data models;Knowledge engineering;Stability analysis;Organizations;Optimization,distributed processing;program testing;regression analysis,mission-critical systems;complex systems;distributed systems;regression testing optimization;software application;regression test prioritization;regression test case selection,,3,9,,,,,,IEEE,IEEE Conferences
The use of explicit congestion notification to shape traffic of an intelligent satellite system,W. Almuhtadi; D. R. Murphy; B. Cheng,"Intelligent Satellite System Research, Ottawa, Ontario, Algonquin College, EION Inc., Canada; Intelligent Satellite System Research, Ottawa, Ontario, Algonquin College, EION Inc., Canada; Intelligent Satellite System Research, Ottawa, Ontario, Algonquin College, EION Inc., Canada",2008 IEEE Radio and Wireless Symposium,,2008,,,819,822,"Since the official standardization of explicit congestion notification (ECN) in 2001, the differentiated service (DS) bits 6 and 7 in packets are now classified for purposes of shaping and prioritization. These classifications are used to mark packet streams for controlling traffic flow in an intelligent satellite system (ISS). Using a forward link connection between a transmitting ground terminal to a geostationary satellite (GEO) acting as a relay, to a receiving hub back on the earth; a traffic shaping software which evaluates the ECN type or classification is used to control traffic flow and the results are observed.",2164-2958;2164-2974,978-1-4244-1462-8978-1-4244-1463,10.1109/RWS.2008.4463618,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463618,ISS;ECN;Differentiated Service;Traffic Shaping;Congestion Control,Shape;Intelligent systems;Satellites;Traffic control;Testing;Throughput;Yarn;Open source software;Monitoring;Relays,DiffServ networks;satellite communication;telecommunication congestion control;telecommunication traffic,explicit congestion notification;intelligent satellite system;differentiated service;packet streams;traffic flow control;forward link connection;geostationary satellite,,2,7,,,,,,IEEE,IEEE Conferences
Two Case Studies of User Experience Design and Agile Development,M. Najafi; L. Toyoshiba,NA; NA,Agile 2008 Conference,,2008,,,531,536,"How can user experience design (UED) practices be leveraged in agile development to improve product usability? UED practices identify the needs and goals of the user through user research and testing. By incorporating UED in agile development, user research and testing can be utilized to prioritize features in the product backlog and to iteratively refine designs to achieve better usability. Furthermore, integrating UED and Agile processes can be accomplished with little or no impact on release schedules. The cases studies presented in this paper describe two examples of UED and agile integration at VeriSign.",,978-0-7695-3321,10.1109/Agile.2008.67,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4599534,Agile;User Experience;VeriSign;Usability;User Experience Design;UED;Sprint;Scrum;security,Testing;Web sites;Usability;Book reviews;Planning;Schedules;Security,program testing;software development management;software engineering;software performance evaluation,user experience design;agile development;product usability;VeriSign,,14,7,,,,,,IEEE,IEEE Conferences
Using Statistical Models to Predict Software Regressions,A. Tarvo,NA,2008 19th International Symposium on Software Reliability Engineering (ISSRE),,2008,,,259,264,"Incorrect changes made to the stable parts of a software system can cause failures - software regressions. Early detection of faulty code changes can be beneficial for the quality of a software system when these errors can be fixed before the system is released. In this paper, a statistical model for predicting software regressions is proposed. The model predicts risk of regression for a code change by using software metrics: type and size of the change, number of affected components, dependency metrics, developerpsilas experience and code metrics of the affected components. Prediction results could be used to prioritize testing of changes: the higher is the risk of regression for the change, the more thorough testing it should receive.",1071-9458;2332-6549,978-0-7695-3405,10.1109/ISSRE.2008.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4700331,software metrics;software regression;testing;statistical model,Predictive models;Software systems;Software metrics;Computer bugs;Manufacturing;Software testing;Software reliability;Reliability engineering;Fault detection;Computer industry,program debugging;software metrics;software quality;statistical analysis,statistical models;software regressions;software system;faulty code;software quality;software metrics;dependency metrics,,2,12,,,,,,IEEE,IEEE Conferences
A Hybrid Approach to Build Prioritized Pairwise Interaction Test Suites,X. Chen; Q. Gu; X. Wang; A. Li; D. Chen,NA; NA; NA; NA; NA,2009 International Conference on Computational Intelligence and Software Engineering,,2009,,,1,4,"Traditional interaction testing aims to build test suites that cover all t-way interactions of inputs. But in many test scenarios, the entire test suites cannot be fully run due to the limited budget. Therefore it is necessary to take the importance of interactions into account and prioritize these tests of the test suite. In the paper, we use the hybrid approach to build prioritized pairwise interaction test suites (PITS). It adopts a one-test-at-a-time strategy to construct final test suites. But to generate a single test it firstly generates a candidate test and then applies a specific metaheuristic search strategy to enhance this test. Here we experiment four different metaheuristic search strategies. In the experiments, we compare our approach to weighted density algorithm (WDA). Meanwhile, we also analyze the effectiveness of four different search strategies and the effectiveness of the increasing iterations. Empirical results demonstrate the effectiveness of our proposed approach.",,978-1-4244-4507,10.1109/CISE.2009.5365886,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5365886,,Software testing;Greedy algorithms;Simulated annealing;Laboratories;Computer science;Statistical analysis;Programming;Costs;Application software;Ant colony optimization,program testing,prioritized pairwise interaction test suites;t-way interactions;metaheuristic search strategy;weighted density algorithm,,1,11,,,,,,IEEE,IEEE Conferences
A new method of test data generation for branch coverage in software testing based on EPDG and Genetic Algorithm,Ciyong Chen; Xiaofeng Xu; Yan Chen; Xiaochao Li; Donghui Guo,"The Department of Physics, Xiamen University, 361005, China; The Department of Physics, Xiamen University, 361005, China; The School of Information Science and Technology, Xiamen University, 361005, China; The School of Information Science and Technology, Xiamen University, 361005, China; The Department of Physics, Xiamen University, 361005, China","2009 3rd International Conference on Anti-counterfeiting, Security, and Identification in Communication",,2009,,,307,310,"A new method called EPDG-GA which utilizes the edge partitions dominator graph (EPDG) and genetic algorithm (GA) for branch coverage testing is presented in this paper. First, a set of critical branches (CBs) are obtained by analyzing the EPDG of the tested program, while covering all the CBs implies covering all the branches of the control flow graph (CFG). Then, the fitness functions are instrumented in the right position by analyzing the pre-dominator tree (PreDT), and two metrics are developed to prioritize the CBs. Coverage-Table is established to record the CBs information and keeps track of whether a branch is executed or not. GA is used to generate test data to cover CBs so as to cover all the branches. The comparison results show that this approach is more efficient than random testing approach.",2163-5048;2163-5056,978-1-4244-3883-9978-1-4244-3884,10.1109/ICASID.2009.5276897,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5276897,Test Data Generation;Edge Partitions;Genetic Algorithm;Branch Coverage,Software testing;Genetic algorithms;Tree graphs;Instruments;Partitioning algorithms;Costs;Automatic testing;Physics;Information science;Flow graphs,flow graphs;genetic algorithms;graph theory;program testing,test data generation method;branch coverage software testing;edge partitions dominator graph;genetic algorithm;critical branch;control flow graph;fitness function;pre-dominator tree,,1,11,,,,,,IEEE,IEEE Conferences
A Survey of Coverage-Based Testing Tools,Q. Yang; J. J. Li; D. M. Weiss,NA; NA; NA,The Computer Journal,,2009,52,5,589,597,"Test coverage is sometimes used to measure how thoroughly software is tested and developers and vendors sometimes use it to indicate their confidence in the readiness of their software. This survey studies and compares 17 coverage-based testing tools primarily focusing on, but not restricted to, coverage measurement. We also survey features such as program prioritization for testing, assistance in debugging, automatic generation of test cases and customization of test reports. Such features make tools more useful and practical, especially for large-scale, commercial software applications. Our initial motivations were both to understand the available test coverage tools and to compare them to a tool that we have developed, called eXVantage (a tool suite that includes code coverage testing, debugging, performance profiling and reporting). Our study shows that each tool has some unique features tailored to its application domains. The readers may use this study to help pick the right coverage testing tools for their needs and environment. This paper is also valuable to those who are new to the practice and the art of software coverage testing, as well as those who want to understand the gap between industry and academia.",0010-4620;1460-2067,,10.1093/comjnl/bxm021,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8130777,code coverage;coverage-based testing tool;prioritization;test case generation;dominator analysis,,,,,15,,,,,,,OUP,OUP Journals & Magazines
Adaptive Random Test Case Prioritization,B. Jiang; Z. Zhang; W. K. Chan; T. H. Tse,NA; NA; NA; NA,2009 IEEE/ACM International Conference on Automated Software Engineering,,2009,,,233,244,"Regression testing assures changed programs against unintended amendments. Rearranging the execution order of test cases is a key idea to improve their effectiveness. Paradoxically, many test case prioritization techniques resolve tie cases using the random selection approach, and yet random ordering of test cases has been considered as ineffective. Existing unit testing research unveils that adaptive random testing (ART) is a promising candidate that may replace random testing (RT). In this paper, we not only propose a new family of coverage-based ART techniques, but also show empirically that they are statistically superior to the RT-based technique in detecting faults. Furthermore, one of the ART prioritization techniques is consistently comparable to some of the best coverage-based prioritization techniques (namely, the ""additional"" techniques) and yet involves much less time cost.",1938-4300,978-1-4244-5259-0978-0-7695-3891,10.1109/ASE.2009.77,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431769,Adaptive random testing;test case prioritization,Software testing;Subspace constraints;Fault detection;Automatic testing;Programming;Greedy algorithms;Software engineering;Costs;Australia Council;Computer science,fault diagnosis;program testing;regression analysis;software engineering,adaptive random test case prioritization;regression testing;random selection;coverage based ART techniques;fault detection,,75,28,,,,,,IEEE,IEEE Conferences
Agent Based Replica Placement in a Data Grid Environement,S. Naseera; K. V. M. Murthy,NA; NA,"2009 First International Conference on Computational Intelligence, Communication Systems and Networks",,2009,,,426,430,"In a data grid, large quantities of data files are produced and data replication is applied to reduce data access time. Determining when and where to replicate data in order to meet performance goals in grid systems with many users and files, dynamic network and resource characteristics and changing user behavior is difficult. Therefore efficiency and fast access to replicated data are influenced by the location of the resource holding the replica. In this paper, we present an agent based replica placement algorithm to determine the candidate site for the placement of replica. An agent is deployed at each site holding the master copies of the shared data files. To create a replica, each agent prioritizes the resources in the grid based on the resource configuration, bandwidth in the network and the demand for the replica at their sites and then creates a replica at suitable resource locations. We have carried out the simulation using GridSim Toolkit-4.0 for EU Data Grid Testbed1. The simulation results show that the aggregated data transfer time and the execution time for jobs at various resources is less for agent based replica placement.",,978-0-7695-3743,10.1109/CICSYN.2009.77,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231873,Data Grid;Data Replication;Candidate Site;Resource Location;GridSim Toolkit-4.0,Delay;Testing;Costs;Seismic measurements;Computational intelligence;Computer science;Data engineering;Bandwidth;Availability;Time measurement,digital simulation;grid computing;software agents,data grid environment;data replication;dynamic network;resource characteristics;agent based replica placement algorithm;GridSim Toolkit-4.0;EU Data Grid Testbed1,,4,21,,,,,,IEEE,IEEE Conferences
BugFix: A learning-based tool to assist developers in fixing bugs,D. Jeffrey; M. Feng; Neelam Gupta; R. Gupta,"University of California, CSE Department, Riverside, USA; University of California, CSE Department, Riverside, USA; University of California, CSE Department, Riverside, USA; University of California, CSE Department, Riverside, USA",2009 IEEE 17th International Conference on Program Comprehension,,2009,,,70,79,"We present a tool called BugFix that can assist developers in fixing program bugs. Our tool automatically analyzes the debugging situation at a statement and reports a prioritized list of relevant bug-fix suggestions that are likely to guide the developer to an appropriate fix at that statement. BugFix incorporates ideas from machine learning to automatically learn from new debugging situations and bug fixes over time. This enables more effective prediction of the most relevant bug-fix suggestions for newly-encountered debugging situations. The tool takes into account the static structure of a statement, the dynamic values used at that statement by both passing and failing runs, and the interesting value mapping pairs associated with that statement. We present a case study illustrating the efficacy of BugFix in helping developers to fix bugs.",1092-8138,978-1-4244-3998-0978-1-4244-3997,10.1109/ICPC.2009.5090029,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090029,,Computer bugs;Software debugging;Testing;Machine learning;Programming;Robustness;Fault diagnosis;Failure analysis;Runtime;Error correction,learning (artificial intelligence);program debugging,BugFix;learning-based tool;program bugs;machine learning,,25,36,,,,,,IEEE,IEEE Conferences
Building Prioritized Pairwise Interaction Test Suites with Ant Colony Optimization,X. Chen; Q. Gu; X. Zhang; D. Chen,NA; NA; NA; NA,2009 Ninth International Conference on Quality Software,,2009,,,347,352,"Interaction testing offers a stable cost-benefit ratio in identifying faults. But in many testing scenarios, the entire test suite cannot be fully executed due to limited time or cost. In these situations, it is essential to take the importance of interactions into account and prioritize these tests. To tackle this issue, the biased covering array is proposed and the Weighted Density Algorithm (WDA) is developed. To find a better solution, in this paper we adopt ant colony optimization (ACO) to build this prioritized pairwise interaction test suite (PITS). In our research, we propose four concrete test generation algorithms based on Ant System, Ant System with Elitist, Ant Colony System and Max-Min Ant System respectively. We also implement these algorithms and apply them to two typical inputs and report experimental results. The results show the effectiveness of these algorithms.",1550-6002;2332-662X,978-1-4244-5913-1978-1-4244-5912-4978-0-7695-3828,10.1109/QSIC.2009.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381411,Software testing;prioritized interaction testing;ant colony optimization,Ant colony optimization;Software testing;Software quality;Concrete;System testing;Partitioning algorithms;Laboratories;Computer science;Fault diagnosis;Costs,minimax techniques;program testing,prioritized pairwise interaction test suites;ant colony optimization;biased covering array;weighted density algorithm;ant system with elitist;ant colony system;max-min ant system,,12,19,,,,,,IEEE,IEEE Conferences
Configuration aware prioritization techniques in regression testing,Xiao Qu,"Department of Computer Science and Engineering, University of Nebraska - Lincoln, 68588-0115, USA",2009 31st International Conference on Software Engineering - Companion Volume,,2009,,,375,378,"Configurable software lets users customize applications in many ways, and is becoming increasingly prevalent. Regression testing is an important but expensive way to build confidence that software changes introduce no new faults as software evolves, resulting in many attempts to improve its performance given limited resources. Whereas problems such as test selection and prioritization at the test case level have been extensively researched in the regression testing literature, they have rarely been considered for configurations, though there is evidence that we should not ignore the effects of configurations on regression testing. This research intends to provide a framework for configuration aware prioritization techniques, evaluated through empirical studies.",,978-1-4244-3495,10.1109/ICSE-COMPANION.2009.5071025,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5071025,,System testing;Fault detection;Sampling methods;Software testing;Software performance;Web pages;Software maintenance;Computer science;Application software;Internet,program testing;software engineering,configuration aware prioritization technique;regression testing;configurable software;test selection,,1,17,,,,,,IEEE,IEEE Conferences
Dynamic admission control and path allocation for SLAs in DiffServ networks,H. A. Harhira; S. Pierre,"Departement of Software and Computer Engineering, Ecole Polytechnique de Montreal, Canada; Departement of Software and Computer Engineering, Ecole Polytechnique de Montreal, Canada",2009 Canadian Conference on Electrical and Computer Engineering,,2009,,,132,136,"Today's converged networks are mainly characterized by their support of real-time and high priority traffic requiring a certain level of quality of service (QoS). In this context, traffic classification and prioritization are key features in providing preferential treatments of the traffic in the core of the network. In this paper, we address the joint problem of path allocation and admission control (JPAC) of new Service Level Agreements (SLA) in a DiffServ domain. In order to maximize the resources utilization and the number of admitted SLAs in the network, we consider a statistical bandwidth constraints allowing for a certain overbooking over the network's links. SLAs' admissibility decisions are based on solving to optimality an integer linear programming (ILP) model. When tested by simulations, numerical results confirm that the proposed model can be solved to optimality for real-sized instances within acceptable computation times and substantially reduces the SLAs blocking probability, compared to a the Greedy mechanism proposed in the literature.",0840-7789,978-1-4244-3509-8978-1-4244-3508,10.1109/CCECE.2009.5090106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090106,DiffServ;dynamic admission control;Quality of Service (QoS);Dynamic routing,Admission control;Quality of service;Diffserv networks;Telecommunication traffic;Communication system traffic control;Bandwidth;Resource management;Probes;Software quality;Computer networks,DiffServ networks;greedy algorithms;integer programming;IP networks;linear programming;quality of service;resource allocation;telecommunication traffic,dynamic admission control;SLA;DiffServ networks;quality of service;QoS;traffic classification;traffic prioritization;joint problem of path allocation and admission control;Service Level Agreements;resources utilization;integer linear programming;blocking probability;Greedy mechanism,,,10,,,,,,IEEE,IEEE Conferences
Experimental Comparison of Code-Based and Model-Based Test Prioritization,B. Korel; G. Koutsogiannakis,NA; NA,"2009 International Conference on Software Testing, Verification, and Validation Workshops",,2009,,,77,84,"During regression testing, a modified system needs to beretested using the existing test suite. Since test suites may be very large, developers are interested in detecting faults in the system as early as possible. Test prioritization orders test cases for execution to increase potentially the chances of early fault detection during retesting. Code-based test prioritization methods are based on the source code of the system, whereas model-based test prioritization methods are based on system models. System modeling is a widely used technique to model state-based systems. Models can be used not only during software development but also during testing. In this paper, we briefly overview codebased and model-based test prioritization. In addition, we present an experimental study in which the code based test prioritization and the model-based test prioritization are compared.",,978-0-7695-3671-2978-1-4244-4356,10.1109/ICSTW.2009.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4976373,Test prioritization;Model Based Test Prioritization;Code Based Test Prioritization,System testing;Software testing;Fault detection;Modeling;Conferences;Computer science;USA Councils;Programming;Embedded system;Automata,program testing;software fault tolerance,code-based test prioritization;model-based test prioritization;regression testing;fault detection;software development,,10,25,,,,,,IEEE,IEEE Conferences
Formal Study of Prioritized Service Compositions,C. Andres; G. Diaz; E. Martinez; Y. Zhang,NA; NA; NA; NA,2009 Fifth International Conference on Signal Image Technology and Internet Based Systems,,2009,,,355,362,"This paper presents an enhanced derivation procedure to obtain a system of services, from a given choreography. In addition to the basic framework, we introduce several situations where nondeterminism appears and it is resolved by using a dynamic prioritized system. The priority policy is based on several parameters such as the request dispatching, the response time, the quality of the response, etc. These parameters are identified as resources used by a utility function, which determines the priority of each possible option in a nondeterministic choice.",,978-1-4244-5741-0978-1-4244-5740-3978-0-7695-3959,10.1109/SITIS.2009.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5633969,Testing;formal methods,Web services;Software reliability;Security;Software systems;Proposals,distributed processing;formal verification,prioritized service composition;nondeterminism;dynamic prioritized system;priority policy;utility function;formal method,,,8,,,,,,IEEE,IEEE Conferences
From Cradle to Sprint: Creating a Full-Lifecycle Request Pipeline at Nationwide Insurance,K. G. Fisher; A. Bankston,NA; NA,2009 Agile Conference,,2009,,,223,228,"After a successful transition from a prescriptive waterfall process to Scrum and XP, the Corporate Internet Solutions group at Nationwide Insurance found velocity and efficiency stumbling due to the competing and vague priorities of corporate silos. This presentation discusses how the team evolved the traditional Scrum process to better manage 17 dependent projects, and reluctant internal business partners, through a combination of activities including clear Pre-Discovery activities, scenario planning, RITE usability testing, and kanban-style visual management systems.",,978-0-7695-3768,10.1109/AGILE.2009.72,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5261080,Scrum;Product Management;User Experience;User Centered Design;Business Alignment;Product Owner,Pipelines;Insurance;Companies;Project management;Usability;Web and internet services;Collaboration;Process planning;System testing;Loans and mortgages,kanban;software development management;software houses,full lifecycle request pipeline;waterfall process;Nationwide Insurance;corporate silos;Scrum process;business partners;scenario planning;usability testing;kanban-style visual management system;Corporate Internet Solution,,2,2,,,,,,IEEE,IEEE Conferences
Fundamentals of risk based inspection ƒ?? a practical approach,G. A. Ghoneim; G. Sigurdsson,"Det Norske Veritas, 400 Ravello Drive, Katy, TX 77449 USA; Det Norske Veritas NO-1322 H?¨vik, Norway",OCEANS 2009,,2009,,,1,9,"New API structural integrity management recommended practice and post-hurricane inspection bulletin, MMS proposed changes to CFR's on decommissioning, and increased interest in developing risk based inspection plans for offshore assets require better understanding of reliability principles. This paper will discuss the fundamentals of RBI from a practical viewpoint and gives details of recently published applicable DNV offshore standards. Offshore structures must be inspected to maintain an acceptable safety level throughout their lifetime. Inspections have traditionally been based upon experience, and judgment of likelihood and consequence of failure. Risk based inspection planning (RBI) for structures, as developed by DNV, represents a systematic, qualitative and quantitative approach which combines theoretical models, test results and in-service experiences. The method is specially developed for application to all types of offshore structures including jackets, jack-ups, TLPs, FPSOs, spars, semi-submersibles, GBSS, and subsea templates. The basis of RBI is to prioritize individual items and systems by considering the associated risks. Attention is given to high risk items, while low risk items receive a more appropriately lesser level of inspection. RBI focuses on cost optimization in all associated activities, to ensure a cost optimal inspection program. The RBI analysis is performed in two steps, comprising a risk screening and a subsequent inspection scheduling. Dedicated software models are utilized to establish the Risk Matrix and to calculate the ?¨time to next inspection?¨. The final deliverable of an RBI analysis is an inspection plan, in which inspection efforts are prioritized from an overall risk perspective.",0197-7385,978-1-4244-4960-6978-0-933957-38,10.23919/OCEANS.2009.5422330,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5422330,,Inspection;Cost function;Risk analysis;Asset management;Risk management;Standards publication;Maintenance;Safety;System testing;Performance analysis,condition monitoring;failure analysis;inspection;offshore installations;production planning;risk analysis;structural engineering,risk based inspection planning;API structural integrity management;post-hurricane inspection bulletin;minerals management services;MMS;offshore assets;DNV offshore standards;offshore structures;failure;jackets;jack-ups;semisubmersible platforms;subsea templates;cost optimization;inspection scheduling,,,14,,,,,,IEEE,IEEE Conferences
Host based intrusion detection using RBF neural networks,U. Ahmed; A. Masood,"Computer Science Department, Military College of Signals, National University of Sciences and Technology, Rawalpindi, Pakistan; Computer Science Department, Military College of Signals, National University of Sciences and Technology, Rawalpindi, Pakistan",2009 International Conference on Emerging Technologies,,2009,,,48,51,A novel approach of host based intrusion detection is suggested in this paper that uses Radial basis Functions Neural Networks as profile containers. The system works by using system calls made by privileged UNIX processes and trains the neural network on its basis. An algorithm is proposed that prioritize the speed and efficiency of the training phase and also limits the false alarm rate. In the detection phase the algorithm provides implementation of window size to detect intrusions that are temporally located. Also a threshold is implemented that is altered on basis of the process behavior. The system is tested with attacks that target different intrusion scenarios. The result shows that the radial Basis Functions Neural Networks provide better detection rate and very low training time as compared to other soft computing methods. The robustness of the training phase is evident by low false alarm rate and high detection capability depicted by the application.,,978-1-4244-5630-7978-1-4244-5631,10.1109/ICET.2009.5353204,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353204,intrusion detection;Host Based Intrusion Detection;neural networks;RBF neural networks,Intrusion detection;Neural networks;Radial basis function networks;Phase detection;Military computing;Application software;Monitoring;Computer science;Educational institutions;Computer networks,algorithm theory;radial basis function networks;security of data;stability;Unix,host based intrusion detection;RBF neural networks;radial basis functions;UNIX processes;speed efficiency algorithm;soft computing methods;robustness,,11,21,,,,,,IEEE,IEEE Conferences
How Well Do Test Case Prioritization Techniques Support Statistical Fault Localization,B. Jiang; Z. Zhang; T. H. Tse; T. Y. Chen,NA; NA; NA; NA,2009 33rd Annual IEEE International Computer Software and Applications Conference,,2009,1,,99,106,"In continuous integration, a tight integration of test case prioritization techniques and fault-localization techniques may both expose failures faster and locate faults more effectively. Statistical fault-localization techniques use the execution information collected during testing to locate faults. Executing a small fraction of a prioritized test suite reduces the cost of testing, and yet the subsequent fault localization may suffer. This paper presents the first empirical study to examine the impact of test case prioritization on the effectiveness of fault localization. Among many interesting empirical results, we find that coverage-based and random techniques can be more effective than distribution-based techniques in supporting statistical fault localization.",0730-3157;0730-3157,978-0-7695-3726,10.1109/COMPSAC.2009.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254274,Continuous integration;software process integration;test case prioritization;fault localization,Software testing;Application software;Costs;Software debugging;Computer applications;Resumes;Australia Council;Computer science;Feedback,cost reduction;program compilers;program debugging;program testing;random processes;software cost estimation;software fault tolerance;statistical distributions,test case prioritization technique;statistical fault localization technique;continuous integration;prioritized test suite;testing cost reduction;empirical study;coverage-based technique;distribution-based technique;random technique;software process integration;debugging technique;code compilation,,23,29,,,,,,IEEE,IEEE Conferences
Implementation of the Software Quality Ranks method in the legacy product development environment,L. Hribar; A. Burilovic; D. Huljenic,"Ericsson Nikola Tesla R&D Center, Croatia; Ericsson Nikola Tesla R&D Center, Croatia; Ericsson Nikola Tesla R&D Center, Croatia",2009 10th International Conference on Telecommunications,,2009,,,141,145,"Software quality ranks (SQR) is an important method to manage and improve software quality. Component software quality has a major influence in development project lead time and cost. SQR enables better management and visibility of the quality effort associated with the component implementation. It also provides a roadmap for continuous improvement leading to value add quality attributes like low maintenance, self optimizing software and short development lifecycles. SQR method focuses attention to prioritizing the quality investment on design component level through different quality assurance mechanisms (basic test, code review, desk checks, documentation and other actions). The resulting design delivery to verification phase will be more predictable quality software with shorter lead-time and time-to-market (TTM).",,978-953-184-130-6978-953-184-131,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5206363,,Software quality;Product development;Quality management;Software maintenance;Time to market;Costs;Continuous improvement;Investments;Quality assurance;Testing,quality assurance;software development management;software quality,software quality ranks method;legacy product development;component software quality;design component level;quality assurance mechanisms,,1,16,,,,,,IEEE,IEEE Conferences
Jtop: Managing JUnit Test Cases in Absence of Coverage Information,L. Zhang; J. Zhou; D. Hao; L. Zhang; H. Mei,NA; NA; NA; NA; NA,2009 IEEE/ACM International Conference on Automated Software Engineering,,2009,,,677,679,"Test case management may make the testing process more efficient and thus accelerate software delivery. With the popularity of using JUnit for testing Java software, researchers have paid attention to techniques to manage JUnit test cases in regression testing of Java software. Typically, most existing test case management tools are based on the coverage information. However, coverage information may need extra efforts to obtain. In this paper, we present an Eclipse IDE plug-in (named Jtop) for managing JUnit test cases in absence of coverage information. Jtop statically analyzes the program under test and its corresponding JUnit test cases to perform the following management tasks: regression test case selection, test suite reduction and test case prioritization. Furthermore, Jtop also enables the programmer to manually manipulate test cases through a graphical user interface.",1938-4300,978-1-4244-5259-0978-0-7695-3891,10.1109/ASE.2009.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431709,regression test case selection;test case prioritization;test suite reduction,Software testing;Electronic equipment testing;Automatic testing;Graphical user interfaces;Displays;Java;Conference management;Technology management;Engineering management;Software engineering,graphical user interfaces;Java;program testing;software management,Jtop;managing JUnit test cases;coverage Information;test case management;software delivery;Java software;test case selection;test suite reduction;test case prioritization;graphical user interface;software testing,,2,10,,,,,,IEEE,IEEE Conferences
Keynote: Security Engineering: Developments and Directions,P. B. Thuraisingham,NA,2009 Third IEEE International Conference on Secure Software Integration and Reliability Improvement,,2009,,,2,3,"Security Engineering is a critical component of systems engineering. When complex and large systems are put together, one needs to ensure that the systems are secure. Security engineering methodologies include gathering the security requirements, specifying the security policies, designing the security model, identifying the security critical components of the system design, security verification and validation and security testing. Before installation, one needs to develop a concept of operation (CONOPS) as well as carry out certification and accreditation. Much of the previous work in security engineering has focused on end to end security. That is, the organization needs to ensure that the applications, database systems, operating systems and networks have to be secure. In addition, one needs to ensure security when the subsystems are composed to form a larger system. More recently with open systems and the Web, secure system development is taking a whole new direction. The Office of the Deputy Assistant Secretary of Defense in the United States (Information and Identity Assurance) has stated that ""the Department of Defense's (DoD) policy, planning, and war fighting capabilities are heavily dependent on the information technology foundation provided by the Global Information Grid (GIG). However, the GIG was built for business efficiency instead of mission assurance against sophisticated adversaries who have demonstrated intent and proven their ability to use cyberspace as a tool for espionage and criminal theft of data. GIG mission assurance works to ensure the DoD is able to accomplish its critical missions when networks, services, or information are unavailable, degraded, or distrusted."" To meet the needs of mission assurance challenges, President's (George W. Bush) cyber plan (CNCI) has listed the area of developing multipronged approaches to supply chain risk management as one of the priorities. CNCI states that the reality of global supply chains presents significant challenges in thwarting counterfeit, or maliciously designed hardware and software products. To overcome such challenges and support successful mission assurance we need to design flexible and secure systems whose components may be untrusted or faulty. We need to achieve the secure operation of mission critical systems constructed from untrusted, semitrusted and fully trusted components for successful mission assurance. This keynote address will discuss the developments in security engineering from requirements, to policy to model to design to verification to testing as well as developing CONOPS and conducting certification and accreditation. System evaluation, usability and metrics related issues will also be discussed. Finally we will discuss the changes that have to be made to security engineering to support the next generation of secure systems for mission critical applications.",,978-0-7695-3758,10.1109/SSIRI.2009.74,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5325402,,Data security;Design engineering;Certification;Accreditation;Supply chains;Mission critical systems;Systems engineering and theory;System testing;Database systems;Operating systems,military computing;program testing;program verification;security of data;software metrics;software reusability,complex system;large system;security engineering methodology;security requirement;security policy;security verification;security validation;CONOPS system;operation concept;secure system development;DoD policy;war fighting capability;global information grid;GIG mission assurance;CNCI cyber plan;supply chain risk management;flexible system;mission critical system;system evaluation;system usability;system metric;database system;operating system,,1,,,,,,,IEEE,IEEE Conferences
Language Identification from an Indian Multilingual Document Using Profile Features,M. C. Padma; P. A. Vijaya; P. Nagabhushan,NA; NA; NA,2009 International Conference on Computer and Automation Engineering,,2009,,,332,335,"In order to reach a larger cross section of people, it is necessary that a document should be composed of text contents in different languages. But on the other hand, this causes practical difficulty in OCRing such a document, because the language type of the text should be pre-determined, before employing a particular OCR. In this research work, this problem of recognizing the language of the text content is addressed, however it is perhaps impossible to design a single recognizer which can identify a large number of scripts/languages. As a via media, in this research we have proposed to work on the prioritized requirements of a particular region, for instance in Karnataka state in India,generally any document including official ones, would contain the text in three languages-English-the language of general importance, Hindi-the language of National importance and Kannada -the language of State/Regional importance. We have proposed to learn identifying the language of the text by thoroughly understanding the nature of top and bottom profiles of the printed text lines in these three languages.Experimentation conducted involved 800 text lines for learning and 600 text lines for testing. The performance has turned out to be 95.4%.",,978-0-7695-3569,10.1109/ICCAE.2009.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804543,Document Image Processing;Multi-lingual document;Language Identification;Top Profile;Bottom Profile;Feature extraction.,Natural languages;Optical character recognition software;Feature extraction;Educational institutions;Text recognition;Automation;Testing;Document image processing;Books;Text analysis,document handling;natural language processing;text analysis,language identification;Indian multilingual document;OCRing;text content;English;Hindi;Kannada,,6,11,,,,,,IEEE,IEEE Conferences
Lightweight Elicitation and Analysis of Software Product Quality Goals: A Multiple Industrial Case Study,J. Vanhanen; M. V. M??ntyl??; J. Itkonen,NA; NA; NA,2009 Third International Workshop on Software Product Management,,2009,,,42,52,"We developed and used a method that gathers relevant stakeholders to elicit, prioritize, and elaborate the quality goals of a software product. It is designed to be lightweight and easy to learn compared to methods for a more comprehensive analysis of non-functional requirements. The method and the resulting quality goals are meant especially for improving the software product management process. We used it in four software product companies, and report lessons learned and evaluation of the method based on practitioners' comments. We found it better to set the goals first for the product in general before discussing a specific release project. In addition to identifying goals that needed improvement, the practitioners considered identifying already achieved goals relevant, but they were neg- lected unless explicitly considered. Using ISO 9126 as a checklist after brainstorming did not add many goals. Prioritization was challenging due to numerous relevant perspectives. Conceiving measures for impor- tant goals seemed to concretize them.",,978-1-4244-7693-0978-0-7695-4098,10.1109/IWSPM.2009.5,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5457326,,Software quality;Computer industry;Programming;Computer architecture;Usability;Product design;Quality management;ISO standards;Software maintenance;Software testing,formal specification;ISO standards;software management;software quality,lightweight elicitation;software product quality analysis;multiple industrial case study;nonfunctional requirements;software product management process;ISO 9126,,6,20,,,,,,IEEE,IEEE Conferences
Measurement and control for risk-based test cases and activities,E. Souza; C. Gusmao; K. Alves; J. Venancio; R. Melo,"Department of Systems and Computing at Pernambuco University, Rua Benfica, 455, Madalena, 50750-410, Recife - PE, Brasil; Department of Systems and Computing at Pernambuco University, Rua Benfica, 455, Madalena, 50750-410, Recife - PE, Brasil; Department of Systems and Computing at Pernambuco University, Rua Benfica, 455, Madalena, 50750-410, Recife - PE, Brasil; Department of Systems and Computing at Pernambuco University, Rua Benfica, 455, Madalena, 50750-410, Recife - PE, Brasil; Department of Systems and Computing at Pernambuco University, Rua Benfica, 455, Madalena, 50750-410, Recife - PE, Brasil",2009 10th Latin American Test Workshop,,2009,,,1,6,"Risk-based testing is an approach that consists of a set of activities regarding risk factors identification related to software requirements. Once identified, the risks are prioritized according to its likelihood and impact, and the test cases are projected based on the strategies for treatment of the identified risk factors. Then, test efforts are continuously adjusted according the risk monitoring. Most risk-based testing approaches focuses on activities related to risk identification, analysis and prioritizing. However, metrics are fundamental as they quantify characteristics of a process or product and support software project management activities. In this light, this paper proposes and discusses risk-based testing metrics to measure and control test cases and test activities progress, efforts and costs.",2373-0862,978-1-4244-4207-2978-1-4244-4206,10.1109/LATW.2009.4813802,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813802,Software Testing;Risk-based Testing;Metrics;Control and Measurement,Software testing;Costs;Software quality;System testing;Monitoring;Software measurement;Risk analysis;Control systems;Paramagnetic resonance;Project management,program testing;software metrics;software quality,risk factors identification;software requirements;software project management;risk-based testing metrics,,5,21,,,,,,IEEE,IEEE Conferences
Mesh your Senses: Multimedia Applications over WiFi-based Wireless Mesh Networks,R. Riggio; K. Gomez; T. Rasheed; M. Gerola; D. Miorandi,NA; NA; NA; NA; NA,"2009 6th IEEE Annual Communications Society Conference on Sensor, Mesh and Ad Hoc Communications and Networks Workshops",,2009,,,1,3,"This demo aims at (i) validating the design choices we have made in conceiving and deploying the WING testbed, and (ii) showing the capability of out software toolkit to properly support heterogeneous multimedia applications. Additionally, the mesh networking toolkit's fault management features is demonstrated. We hope that our wireless mesh networking toolkit is considered by both researchers and practitioners as platform of choice to test innovative solutions and to provide end-users with wireless connectivity. WING is an experimental multi-radio WMN testbed designed and built exploiting commodity hardware and open-source software components. WING implements a flexible and scalable WMN architecture capable of supporting next-generation Internet services with a particular focus on multimedia applications. The WING project aims at providing an open-platform on top of which innovative solution can be implemented and tested in a realistic environment. Currently, the testbed consist of 10 nodes deployed at CREATE-NET premises and implementing a two-tiers architecture. Other well-known IEEE 802.11-based WMNs include Roofnet, Hyacinth, Microsoft's MCL, and Meraki. We establish the uniqueness of our mesh solution in that it is capable of achieving both service differentiation and performance isolation in IEEE 802.11-based WMNs. While not providing strict QoS performance bounds, the proposed scheme aims at enhancing the perceived quality of experience by combining opportunistic scheduling and packet aggregation and by implementing a DiffServ-like architecture in order to provide traffic prioritization.",2155-5486;2155-5494,978-1-4244-3938,10.1109/SAHCNW.2009.5172945,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5172945,,Wireless mesh networks;Telecommunication traffic;Computer architecture;Ethernet networks;Software testing;Open source hardware;Open source software;Web and internet services;Application software;Design engineering,fault tolerance;Internet;multimedia communication;scheduling;telecommunication computing;telecommunication network management;telecommunication traffic;wireless LAN,heterogeneous multimedia application;WiFi-based wireless mesh network;software toolkit;fault management;hardware-open-source software component;next-generation Internet service;opportunistic scheduling;packet aggregation;WING platform,,5,12,,,,,,IEEE,IEEE Conferences
Performance impact analysis with KPP using application response measurement in E-government systems,N. Yoo,"DoD/HA, Falls Church, Virginia, USA",2009 IEEE International Conference on Software Maintenance,,2009,,,503,506,"In this paper, the performance impact analysis of e-government systems with key performance parameters is being considered. Meaningful impact analysis in sustained government systems is required for considering non-functional requirements and functional requirements. Performance requirements are a critical component of non-functional areas. For example, if a new system change is set to the system, the impact in terms of the response time must be implemented in each sub-system. In this paper, an XML-based framework can be used to analyze performance impacts on sub-systems and can provide a scheme to enhance impact analysis by performance monitoring using application response measurement. Through a health system example as a case study, a performance requirement model to describe extended trees and adapting analysis result of performance monitoring using application response measurement and XML tree representation are addressed. This paper also proposes a scheme for prioritized processing and an algorithm for effectively enhancing impact analysis in a timely fashion.",1063-6773,978-1-4244-4897-5978-1-4244-4828,10.1109/ICSM.2009.5306282,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306282,,Performance analysis;Electronic government;Systems engineering and theory;Delay;Monitoring;XML;Reliability engineering;Time measurement;System testing;Algorithm design and analysis,government data processing;software performance evaluation;tree data structures;XML,performance impact analysis;application response measurement;e-government systems;XML-based framework;performance monitoring;extended trees;prioritized processing;key performance parameter,,,5,,,,,,IEEE,IEEE Conferences
Policy-Based Network Management in Home Area Networks: Interim Test Results,A. I. Rana; M. O Foghlu,NA; NA,"2009 3rd International Conference on New Technologies, Mobility and Security",,2009,,,1,3,"This paper argues that Home Area Networks (HANs) are a good candidate for advanced network management automation techniques, such as Policy-Based Network Management (PBNM). What is proposed is a simple use of policy based network management to introduce some level of Quality of Service (QoS) and Security management in the HAN, whilst hiding this complexity from the home user. In this paper we have presented the interim test results of our research experiments (based on a scenario) using the HAN testbed. After using policies to prioritize different traffic, packet loss decreased to 30% and VoIP quality improved dramatically without employing any intelligent bandwidth allocation technique.",2157-4952;2157-4960,978-1-4244-6273-5978-1-4244-4765-7978-1-4244-6272,10.1109/NTMS.2009.5384722,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5384722,,Testing;Home automation;Telecommunication traffic;Energy management;Quality management;Quality of service;Internet;Engineering management;Software systems;Channel allocation,computer network management;computer network security;home computing;quality of service,policy-based network management;home area networks;interim test results;advanced network management automation;quality of service;security management,,6,7,,,,,,IEEE,IEEE Conferences
Predicting Attack-prone Components,M. Gegick; P. Rotella; L. Williams,NA; NA; NA,2009 International Conference on Software Testing Verification and Validation,,2009,,,181,190,"Limited resources preclude software engineers from finding and fixing all vulnerabilities in a software system. This limitation necessitates security risk management where security efforts are prioritized to the highest risk vulnerabilities that cause the most damage to the end user. We created a predictive model that identifies the software components that pose the highest security risk in order to prioritize security fortification efforts. The input variables to our model are available early in the software life cycle and include security-related static analysis tool warnings, code churn and size, and faults identified by manual inspections. These metrics are validated against vulnerabilities reported by testing and those found in the field. We evaluated our model on a large Cisco software system and found that 75.6% of the system's vulnerable components are in the top 18.6% of the components predicted to be vulnerable. The model's false positive rate is 47.4% of this top 18.6% or 9.1% of the total system components. We quantified the goodness of fit of our model to the Cisco data set using a receiver operating characteristic curve that shows 94.4% of the area is under the curve.",2159-4848,978-1-4244-3775-7978-0-7695-3601,10.1109/ICST.2009.36,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4815350,Security;metric;predict;attack-prone;classification and regression tree,Predictive models;Software systems;Data security;Reliability engineering;Software testing;Input variables;Inspection;Costs;System testing;Risk management,inspection;object-oriented programming;program diagnostics;program testing;security of data;software metrics;software tools,attack-prone component prediction;software engineering;vulnerability fixing;security risk management;predictive model;software life cycle;static analysis tool warnings;manual inspections;metrics;Cisco software system,,14,33,,,,,,IEEE,IEEE Conferences
Prioritization of Scenarios Based on UML Activity Diagrams,S. P.G.; H. Mohanty,NA; NA,"2009 First International Conference on Computational Intelligence, Communication Systems and Networks",,2009,,,271,276,"Increased size and complexity of software requires better methods for different activities in the software development lifecycle. Quality assurance of software is primarily done by means of testing, an activity that faces constraints of both time and resources. Hence, there is need to test effectively within the constraints in order to maximize throughput i.e. rate of fault detection, coverage, etc. Test case prioritization involves techniques aimed at finding the best prioritized test suite. In this paper, we propose a prioritization technique based on UML activity diagrams. The constructs of an activity diagram are used to prioritize scenarios. Preliminary results obtained on a case-study indicate that the technique is effective in extracting the critical scenarios from the activity diagram.",,978-0-7695-3743,10.1109/CICSYN.2009.74,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5231936,UML;Prioritization,Unified modeling language;Software testing;Programming;Software quality;Fault detection;Throughput;Computational intelligence;Computer networks;Communication system software;Quality assurance,program testing;software development management;software metrics;software quality;Unified Modeling Language,UML activity diagrams;software complexity;software development lifecycle;quality assurance;software testing;test case prioritization;prioritization technique,,2,11,,,,,,IEEE,IEEE Conferences
Prioritized Test Generation Strategy for Pair-Wise Testing,J. Gao; J. Zhu,NA; NA,2009 15th IEEE Pacific Rim International Symposium on Dependable Computing,,2009,,,99,102,"Pair-wise testing is widely used to detect faults in software systems. In many applications where pair-wise testing is needed, the whole test set can not be run completely due to time or budget constraints. In these situations, it is essential to prioritize the tests. In this paper, we drive weight for each value of each parameter, and adapt UWA algorithm to generate an ordered pair-wise coverage test suite. UWA algorithm is to accord weights set for each value of each parameter of the system, then produce ordered pair-wise coverage test set for having generated but unordered one. Finally, a greedy algorithm is adopted to prioritize generated pair-wise coverage test set with driven weights, so that whenever the testing is interrupted, interactions deemed, most important are tested.",,978-0-7695-3849,10.1109/PRDC.2009.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5368244,pair-wise testing;test prioritization;weights,System testing;Software testing;Fault detection;Computer science;Greedy algorithms;Software systems;Application software;Time factors;Costs;Fault diagnosis,program testing,prioritized test generation;pair-wise testing;software fault detection;UWA algorithm;updating weight algorithm;ordered pair-wise coverage test,,,16,,,,,,IEEE,IEEE Conferences
Prioritizing component compatibility tests via user preferences,I. Yoon; A. Sussman; A. Memon; A. Porter,"Department of Computer Science, University of Maryland, College Park, MD, 20742 USA; Department of Computer Science, University of Maryland, College Park, MD, 20742 USA; Department of Computer Science, University of Maryland, College Park, MD, 20742 USA; Department of Computer Science, University of Maryland, College Park, MD, 20742 USA",2009 IEEE International Conference on Software Maintenance,,2009,,,29,38,"Many software systems rely on third-party components during their build process. Because the components are constantly evolving, quality assurance demands that developers perform compatibility testing to ensure that their software systems build correctly over all deployable combinations of component versions, also called configurations. However, large software systems can have many configurations, and compatibility testing is often time and resource constrained. We present a prioritization mechanism that enhances compatibility testing by examining the ldquomost importantrdquo configurations first, while distributing the work over a cluster of computers. We evaluate our new approach on two large scientific middleware systems and examine tradeoffs between the new prioritization approach and a previously developed lowest-cost-configuration-first approach.",1063-6773,978-1-4244-4897-5978-1-4244-4828,10.1109/ICSM.2009.5306357,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306357,,System testing;Software systems;Software testing;Quality assurance;Costs;Performance evaluation;Middleware;Software libraries;Computer science;Educational institutions,middleware;object-oriented programming;program testing;software engineering,user preferences;compatibility testing prioritization;third-party components;component configurations;software systems;middleware systems;computer clusters,,,14,,,,,,IEEE,IEEE Conferences
Prioritizing JUnit test cases in absence of coverage information,L. Zhang; J. Zhou; D. Hao; L. Zhang; H. Mei,"Key Laboratory of High Confidence Software Technologies, Ministry of Education, Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Institute of Software, School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, P. R. China",2009 IEEE International Conference on Software Maintenance,,2009,,,19,28,"Better orderings of test cases can detect faults in less time with fewer resources, and thus make the debugging process earlier and accelerate software delivery. As a result, test case prioritization has become a hot topic in the research of regression testing. With the popularity of using the JUnit testing framework for developing Java software, researchers also paid attention to techniques for prioritizing JUnit test cases in regression testing of Java software. Typically, most of them are based on coverage information of test cases. However, coverage information may need extra costs to acquire. In this paper, we propose an approach (named Jupta) for prioritizing JUnit test cases in absence of coverage information. Jupta statically analyzes call graphs of JUnit test cases and the software under test to estimate the test ability (TA) of each test case. Furthermore, Jupta provides two prioritization techniques: the total TA based technique (denoted as JuptaT) and the additional TA based technique (denoted as JuptaA). To evaluate Jupta, we performed an experimental study on two open source Java programs, containing 11 versions in total. The experimental results indicate that Jupta is more effective and stable than the untreated orderings and Jupta is approximately as effective and stable as prioritization techniques using coverage information at the method level.",1063-6773,978-1-4244-4897-5978-1-4244-4828,10.1109/ICSM.2009.5306350,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306350,,Software testing;Electronic equipment testing;Java;Software debugging;Fault detection;Costs;Laboratories;Educational technology;Computer science education;Computer science,Java;program testing;software engineering,JUnit test cases prioritization;coverage information;Java software development;debugging process;regression testing;test ability estiimation;Jupta approach;TA prioritization techniques,,14,22,,,,,,IEEE,IEEE Conferences
Prioritizing test cases for resource constraint environments using historical test case performance data,Y. Fazlalizadeh; A. Khalilian; M. A. Azgomi; S. Parsa,"Computer Engineering Department, Iran University of Science and Technology, Tehran, Iran; Computer Engineering Department, Iran University of Science and Technology, Tehran, Iran; Computer Engineering Department, Iran University of Science and Technology, Tehran, Iran; Computer Engineering Department, Iran University of Science and Technology, Tehran, Iran",2009 2nd IEEE International Conference on Computer Science and Information Technology,,2009,,,190,195,"Regression testing has been widely used to assure the acquirement of appropriate quality through several versions of a software program. Regression testing, however, is too expensive in that, it requires many test case executions and a large number of test cases. To provide the missing flexibility, researchers introduced prioritization techniques. The aim in this paper has been to prioritize test cases during software regression test. To achieve this, a new equation is presented. The proposed equation considers historical effectiveness of the test cases in fault detection, each test case's execution history in regression test and finally the last priority assigned to the test case. The results of applying the proposed equation to compute the priority of regression test cases for two benchmarks, known as Siemens suite and Space program, demonstrate the relatively faster fault detection in resource and time constrained environments.",,978-1-4244-4519-6978-1-4244-4520,10.1109/ICCSIT.2009.5234968,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5234968,software regression test;test case prioritization;history-based prioritization;historical fault detection effectiveness,Software testing;Equations;Fault detection;Time factors;Software performance;Life testing;History;Data engineering;Information analysis;Performance analysis,program testing;regression analysis;software fault tolerance;software quality,resource constraint environment;historical test case performance data;software program quality;prioritization techniques;software regression test;Space program;Siemens suite;fault detection,,6,18,,,,,,IEEE,IEEE Conferences
Prioritizing Use Cases to Aid Ordering of Scenarios,S. P.G.; H. Mohanty,NA; NA,2009 Third UKSim European Symposium on Computer Modeling and Simulation,,2009,,,136,141,"Models are used as the basis for design and testing of software. The unified modeling language (UML) is used to capture and model the requirements of a software system. One of the major requirements of a development process is to detect defects as early as possible. Effective prioritization of scenarios helps in early detection of defects as well maximize effort and utilization of resources. Use case diagrams are used to represent the requirements of a software system. In this paper, we propose using data captured from the primitives of the use case diagrams to aid in prioritization of scenarios generated from activity diagrams. Interactions among the primitives in the diagrams are used to guide prioritization. Customer prioritization of use cases is taken as one of the factors. Preliminary results on a case study indicate that the technique is effective in prioritization of test scenarios.",,978-1-4244-5345-0978-0-7695-3886,10.1109/EMS.2009.78,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5358815,UML;use case diagram;activity diagram;scenarios;prioritization,Software testing;Unified modeling language;Software systems;Fault detection;Computational modeling;Computer simulation;Costs;Performance evaluation;Visualization;Machine learning algorithms,program testing;Unified Modeling Language,use case diagram;software design;software testing;unified modeling language;customer prioritization,,1,15,,,,,,IEEE,IEEE Conferences
Quasi-Renewal Time-Delay Fault-Removal Consideration in Software Reliability Modeling,S. Hwang; H. Pham,NA; NA,"IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans",,2009,39,1,200,209,"Software reliability growth models based on a nonhomogeneous Poisson process (NHPP) have been considered as one of the most effective among various models since they integrate the information regarding testing and debugging activities observed in the testing phase into the software reliability model. Although most of the existing NHPP models have progressed successfully in their estimation/prediction accuracies by modifying the assumptions with regard to the testing process, these models were developed based on the instantaneous fault-removal assumption. In this paper, we develop a generalized NHPP software reliability model considering quasi-renewal time-delay fault removal. The quasi-renewal process is employed to estimate the time delay due to identifying and prioritizing the detected faults before actual code change in the software reliability assessment. Model formulation based on the quasi-renewal time-delay assumption is provided, and the generalized mean value function (MVF) for the proposed model is derived by using the method of steps. The general solution of the MVFs for the proposed model is also obtained for some specific existing models. The numerical examples, based on a software failure data set, show that the consideration of quasi-renewal time-delay fault-removal assumption improves the descriptive properties of the model, which means that the length of time delay is getting decreased since testers and programmers adapt themselves to the working environment as testing and debugging activities are in progress.",1083-4427;1558-2426,,10.1109/TSMCA.2008.2007982,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4694128,Mean value function (MVF);nonhomogeneous Poisson process (NHPP);quasi-renewal process;software reliability engineering;time-delay fault removal;Mean value function (MVF);nonhomogeneous Poisson process (NHPP);quasi-renewal process;software reliability engineering;time-delay fault removal,Software reliability;Software testing;Predictive models;Delay effects;Accuracy;Delay estimation;Fault diagnosis;Fault detection;Software debugging;Programming profession,delays;estimation theory;fault diagnosis;program debugging;program testing;software fault tolerance;stochastic processes,software reliability growth model;quasi renewal time-delay fault removal assumption;nonhomogeneous Poisson process;software testing;software debugging activity;time delay estimation;fault detection;software reliability assessment;model formulation;generalized mean value function,,27,31,,,,,,IEEE,IEEE Journals & Magazines
Reducing Field Failures in System Configurable Software: Cost-Based Prioritization,H. Srikanth; M. B. Cohen; X. Qu,NA; NA; NA,2009 20th International Symposium on Software Reliability Engineering,,2009,,,61,70,"System testing of configurable software is an expensive and resource constrained process. Insufficient testing often leads to escaped faults in the field where failures impact customers and are costly to repair. Prior work has shown that it is possible to efficiently sample configurations for testing using combinatorial interaction testing, and to prioritize these configurations to increase the rate of early fault detection. The underlying assumption to date has been that there is no added complexity to configuring a system level environment over a user configurable one; i.e. the time required to setup and test each individual configuration is nominal. In this paper we examine prioritization of system configurable software driven not only by fault detection but also by the cost of configuration and setup time that moving between different configurations incurs. We present a case study on two releases of an enterprise software system using failures reported in the field. We examine the most effective prioritization technique and conclude that (1) using failure history of configurations can improve the early fault detection rate, but that (2) we must consider fault detection rate over time, not by the number of configurations tested. It is better to test related configurations which incur minimal setup time than to test fewer, more diverse configurations.",1071-9458;2332-6549,978-1-4244-5375-7978-0-7695-3878,10.1109/ISSRE.2009.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5362084,configuration prioritization;regression testing;combinatorial interaction testing,Software systems;System testing;Costs;Fault detection;Software testing;Databases;Operating systems;Environmental economics;Programming;Application software,electronic commerce;program testing;software fault tolerance,system configurable software;cost-based prioritization;configurable software system testing;resource constrained process;combinatorial interaction testing;fault detection;system level environment;enterprise software system;prioritization technique,,14,20,,,,,,IEEE,IEEE Conferences
Simple Time-to-Failure Estimation Techniques for Reliability and Maintenance of Equipment,H. W. Penrose,"Dreisilker Electric Motors, Inc.",IEEE Electrical Insulation Magazine,,2009,25,4,14,18,"Proper reliability and maintenance best practice processes have a direct impact on equipment availability, throughput capacity, and spare inventories. The purpose of the time-to-failure estimation (TTFE) technique is to provide a tool for engineers and technicians for risk-based reporting of condition- based maintenance tests and inspections. Through the proper application of this technique, corrective action may be prioritized improving the effectiveness of the maintenance program. Instead of stakeholders being required to make decisions based upon experience only, equipment failure, and repair history can be used to enhance the process, improving the availability of critical equipment.",0883-7554;1558-4402,,10.1109/MEI.2009.5191412,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5191412,"Reliability and maintenance, time-to-failure estimation, reactive maintenance practices, predictive maintenance practices",Predictive maintenance;Costs;Economic indicators;Electric motors;Reliability engineering;Tribology;Aging;Electric breakdown;Coordinate measuring machines;Application software,condition monitoring;failure analysis;inspection;machine tools;maintenance engineering,time-to-failure estimation;equipment maintenance;equipment availability;equipment reliability;throughput capacity;spare inventories;condition-based maintenance;inspection;equipment failure,,6,10,,,,,,IEEE,IEEE Journals & Magazines
Surviving Insecure IT: Effective Patch Management,S. Liu; R. Kuhn; H. Rossman,US National Library of Medicine; US National Institute of Standards and Technology; Science Applications International Corporation,IT Professional,,2009,11,2,49,51,"The amount of time to protect enterprise systems against potential vulnerability continues to shrink. Enterprises need an effective patch management mechanism to survive the insecure IT environment. Effective patch management is a systematic and repeatable patch distribution process which includes establishing timely and practical alerts, receiving notification of patches or discovering them, downloading patches and documentation, assessing and prioritizing vulnerabilities, performing testing, deploying patches, and auditing.",1520-9202;1941-045X,,10.1109/MITP.2009.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4804050,IT Professional;security;vulnerability;patch management;vulnerability alerts;vulnerability scan;vulnerability assessment,Application software;IEEE news;Information security;Mission critical systems;Databases;National security;Monitoring;Subscriptions;Documentation;Software development management,security of data;software maintenance,enterprise systems;patch management mechanism;insecure IT environment;patch distribution process,,3,,,,,,,IEEE,IEEE Journals & Magazines
Tag-Based Techniques for Black-Box Test Case Prioritization for Service Testing,L. Mei; W. K. Chan; T. H. Tse; R. G. Merkel,NA; NA; NA; NA,2009 Ninth International Conference on Quality Software,,2009,,,21,30,"A web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable. The rich interface specifications of a web service, however, provide peer services with a means to formulate black-box testing strategies. In this paper, we formulate new test case prioritization strategies using tags embedded in XML messages to reorder regression test cases, and reveal how the test cases use the interface specifications of services. We evaluate experimentally their effectiveness on revealing regression faults in modified WS-BPEL programs. The results show that the new techniques can have a high probability of outperforming random ordering.",1550-6002;2332-662X,978-1-4244-5913-1978-1-4244-5912-4978-0-7695-3828,10.1109/QSIC.2009.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381531,test case prioritization;black-box regression testing;WS-BPEL;service testing;encapsulation testing,Web services;Software testing;XML;Fault detection;Logic testing;Collaborative software;Councils;Software quality;Australia;International collaboration,program testing;regression analysis;Web services,tag-based technique;black-box test case prioritization;service testing;Web service;service composition;collaborative agreement;peer services;code-based regression testing;black-box testing;XML messages;WS-BPEL programs,,11,27,,,,,,IEEE,IEEE Conferences
Techniques for building excellent operator machine interfaces (OMI),P. Gorman; N. Pappas,"The Boeing Company, Seattle, WA, USA.; The Boeing Company, Seattle, WA, USA.",IEEE Aerospace and Electronic Systems Magazine,,2009,24,10,17,22,"Establishing a process to continually improve understanding of operator requirements - the why as well as the how - is key to developing an optimal Operator-Machine Interface (OMI) for a large program. The P-8A program is one example of a complex software development, with over 2 million lines of code and a unique and complex OMI. The OMI itself is used as a tool for achieving and is designed in parallel with understanding end goals, operator decision making chains, and thought processes. The OMI can be designed to support and anticipate information needs and alert operators to unusual occurrences. Operator actions and decision-making techniques can be discovered by using expert operators and capturing their knowledge through the use of multiple tools to identify the problem set: storytelling, decision mapping, and is/is not matrices. When a solution set has been identified and agreed to by the customer, resources allocated, and schedule set, a standard process captures and prioritizes further changes, identities their impact, and decides on implementation. Documents describing design and processes and a design description document describing the current version of OMI are made accessible to stakeholders at all times.",0885-8985;1557-959X,,10.1109/MAES.2009.5317781,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5317781,,Companies;Decision making;Process design;Logic;Military aircraft;Programming;Resource management;Aerospace testing;Marketing and sales;Humans,aerospace computing;decision making;decision support systems;formal specification;man-machine systems;resource allocation;user interfaces,operator machine interface;OMI;operator requirement;P-8A program;complex software development;operator decision making chain;resource allocation;design description document;aerospace community;expert operators;storytelling problem;decision mapping problem;is/is not matrices problem,,,7,,,,,,IEEE,IEEE Journals & Magazines
Test case prioritization based on data reuse an experimental study,L. Lima; J. Iyoda; A. Sampaio; E. Aranha,"Centro de Inform?­tica, Universidade Federal de Pernambuco, Recife-PE, Brazil; Centro de Inform?­tica, Universidade Federal de Pernambuco, Recife-PE, Brazil; Centro de Inform?­tica, Universidade Federal de Pernambuco, Recife-PE, Brazil; Escola de Ci?¦ncia e Tecnologia, Universidade Federal do Rio Grande do Norte, Natal-RN, Brazil",2009 3rd International Symposium on Empirical Software Engineering and Measurement,,2009,,,279,290,"The order in which tests are executed can significantly impact the total test execution time. In this paper, we evaluate two test prioritization techniques (manual and automatic) in the context of mobile phone testing. The manual technique produces test sequences created by test experts, while the automatic one generates sequences mechanically based on the permutation of the tests. Both techniques take into account a data reuse: the more the data is reused among tests, the faster the sequence is executed. In order to evaluate the benefits of these two techniques, we carried out an experiment with 8 testers and 2 test suites arranged in a 2times2 Latin square design replicated four times. The automatic technique reduced approximately 25% of the data generation time and 13.5% of the execution time. The automatic technique is clearly better than the manual one with respect to the generation of sequences. Our experiment showed that the automatic technique also generates sequences whose execution is faster than those created manually by test experts.",1949-3770;1949-3789,978-1-4244-4842-5978-1-4244-4841,10.1109/ESEM.2009.5315980,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5315980,,Automatic testing;Software testing;Manuals;Software engineering;Software measurement;Time measurement;Mobile handsets;Computer industry;Automation;Costs,mobile computing;program testing;software reusability,test case prioritization;data reuse;experimental study;manual testing;automatic testing;mobile phone testing,,4,20,,,,,,IEEE,IEEE Conferences
Test Selection Prioritization Strategy,R. Subramanyan; C. J. Budnik,NA; NA,2009 33rd Annual IEEE International Computer Software and Applications Conference,,2009,2,,545,549,"A wide divergence is observed in projects between test activities planned in the test plan and the actual tests that can be executed. Estimates for test execution computed during the planning are inaccurate without test design. The actual time and resources available are usually less than planned. Assuming that time and resources cannot be changed, a dynamic selection of tests for execution that maximizes quality is required.",0730-3157;0730-3157,978-0-7695-3726,10.1109/COMPSAC.2009.190,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5254084,test selection;test prioritization;test strategy,System testing;Software testing;Fault detection;Life testing;Unified modeling language;Planning;USA Councils;Project management;Tellurium;Computer applications,program testing;project management;software metrics;software quality,test selection prioritization strategy;test activity planning;test execution;quality maximization;complexity metrics;software project,,1,10,,,,,,IEEE,IEEE Conferences
Testing Processes in Business-Critical Chain Software Lifecycle,G. R. N.,NA,2009 WRI World Congress on Software Engineering,,2009,4,,238,242,"The business-critical chain lifecycle is an agile software development lifecycle that aims at aligning the software project deliverables to attain the business objectives based on business priorities. The traditional software development projects work on the assumption that all `equal effort consumers' would be treated equally and worked upon. The agile methodology ensures that the delivery cycles are reduced thus introducing agility in the way business is supported by underlying technologies. The proposed lifecycle model introduces new variables pertaining to the business value generation each finished piece of code would produce. Hence the software project processes have to be modified to cater to it. Even the usual agile lifecycle testing strategies need to be modified to suit the proposed model. The test plans, test estimations, test resource management, quality control, regression plans and automation road map and plans have to be customized to cater to the new life cycle model. The secondary project management activities such as risk management, procurement management, etc also may to be modified with respect to the testing processes. My paper aims at using the critical chain principles and proposes a software lifecycle model that can cater to business priorities and aligning the testing processes not only to development cycles but also to the actual business value created. In this paper I would take a case study and compare and contrast when project uses the regular models and this new model. I would also provide guidelines to use this lifecycle model and modify regular project management activities to cater to the new model with emphasis on the testing processes. The paper would try to provide the ideal scenarios; in project teams, in consulting firms and in new customers and expectations; where such a model could provide high impact on the way consulting companies can do successful projects and creating more value to the customers.",,978-0-7695-3570,10.1109/WCSE.2009.424,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5319542,,Life testing;Software testing;Automatic testing;Project management;Programming;Risk management;Life estimation;Resource management;Quality control;Automation,business data processing;life testing;program testing;project management;software quality,testing processes;business-critical chain software lifecycle;agile software development lifecycle;business priorities;software development projects;business value generation;software project processes;agile lifecycle testing strategies;test plans;test estimations;test resource management;quality control;regression plans;automation road map;secondary project management,,,1,,,,,,IEEE,IEEE Conferences
The impact of test case reduction and prioritization on software testing effectiveness,S. ur Rehman Khan; I. ur Rehman; S. U. R. Malik,"Department of Computer Science COMSATS Institute of Information Technology Islamabad, Pakistan; Department of Computer Science COMSATS Institute of Information Technology Islamabad, Pakistan; Department of Computer Science COMSATS Institute of Information Technology Islamabad, Pakistan",2009 International Conference on Emerging Technologies,,2009,,,416,421,"Software testing is critical but most expensive phase of Software Development Life Cycle (SDLC). Development organizations desire to thoroughly test the software. But this exhaustive testing is impractical due to resource constraints. A large number of test suites are generated using automated tools. But the real challenge is the selection of subset of test cases and/or high order test cases crucial to validate the System Under Test (SUT). Test case reduction and prioritization techniques help test manager to solve this problem at a little cost. In this paper, we investigate their impact on testing process effectiveness using previous empirical studies. The results indicate that these techniques improve the testing effectiveness significantly. At the end, a case study is presented that suggests different useful combinations of these techniques, which are helpful for different testing scenarios.",,978-1-4244-5630-7978-1-4244-5631,10.1109/ICET.2009.5353136,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5353136,Software testing;exhaustive testing;test case reduction;test case prioritization,Software testing;Costs;System testing;Automatic testing;Life testing;Computer science;Information technology;Programming;Standards development;Computer aided software engineering,program testing;software engineering,test case reduction;software testing;software development life cycle;prioritization techniques,,2,26,,,,,,IEEE,IEEE Conferences
Type Inference for Soft-Error Fault-Tolerance Prediction,G. Munkby; S. Schupp,NA; NA,2009 IEEE/ACM International Conference on Automated Software Engineering,,2009,,,65,75,"Software systems are becoming increasingly vulnerable to a new class of soft errors, originating from voltage spikes produced by cosmic radiation. The standard technique for assessing the source-level impact of these soft errors, fault injection - essentially a black-box testing technique - provides limited high-level information. Since soft errors can occur anywhere, even control-structured white-box techniques offer little insight. We propose a type-based approach, founded on data-flow structure, to classify the usage pattern of registers and memory cells. To capture all soft errors, the type system is defined at the assembly level, close to the hardware, and allows inferring types in the untyped assembly representation. In a case study, we apply our type inference scheme to a prototype brake-by-wire controller, developed by Volvo Technology, and identify a high correlation between types and fault-injection results. The case study confirms that the inferred types are good predictors for soft-error impact.",1938-4300,978-1-4244-5259-0978-0-7695-3891,10.1109/ASE.2009.61,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431783,fault injection;assembly type system;test selection and prioritization,Fault tolerance;Computer errors;Assembly systems;Software systems;Voltage;Testing;Error correction;Registers;Hardware;Prototypes,data flow computing;program testing;reasoning about programs;software fault tolerance;type theory,type inference;soft-error fault tolerance prediction;software systems;voltage spikes;cosmic radiation;source level impact;soft errors;fault injection;black-box testing;high-level information;data flow structure;usage pattern;memory cells;untyped assembly representation;brake-by-wire controller,,,24,,,,,,IEEE,IEEE Conferences
"Uncertainty management in software engineering: Past, present, and future",H. Ibrahim; B. H. Far; A. Eberlein; Y. Daradkeh,"University of Calgary, 2500 university Drive NW, AB, T2N 1N4, Canada; University of Calgary, 2500 university Drive NW, AB, T2N 1N4, Canada; Department of Computer Science & Engineering, American University of Sharjah, UAE; University of Calgary, 2500 university Drive NW, AB, T2N 1N4, Canada",2009 Canadian Conference on Electrical and Computer Engineering,,2009,,,7,12,"Software development has significantly matured in the last decade. However, one of the critical challenges today is uncertainty inherent to every aspect of software development including requirement specifications, design, coding, and testing. In this paper, we propose a framework for uncertainty management in software engineering. The framework is used to model uncertainty inherent to software development activities and manage their consequences. The framework consists of four main phases: identification and prioritization, modeling and analysis, management and planning, and monitoring and evaluation. Commercial off-the-shelf (COTS)-based development is selected as an example to illustrate how the proposed framework is used in a simple but intuitive case study to represent uncertainty and manage its consequences.",0840-7789,978-1-4244-3509-8978-1-4244-3508,10.1109/CCECE.2009.5090081,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5090081,Uncertainty management;Software Engineering;COTS-Based Development,Uncertainty;Engineering management;Software engineering;Programming;Software development management;Software design;Monitoring;Decision making;Bayesian methods;Software systems,software development management;uncertainty handling,software engineering;uncertainty management;software development;model uncertainty;commercial off-the-shelf based development,,2,20,,,,,,IEEE,IEEE Conferences
Using String Distances for Test Case Prioritisation,Y. Ledru; A. Petrenko; S. Boroday,NA; NA; NA,2009 IEEE/ACM International Conference on Automated Software Engineering,,2009,,,510,514,"Test case prioritisation aims at finding an ordering which enhances a certain property of an ordered test suite. Traditional techniques rely on the availability of code or a specification of the program under test. In this paper, we propose to use string distances on the text of test cases for their comparison and elaborate a prioritisation algorithm. Such a prioritisation does not require code and can be useful for initial testing and in cases when code is difficult to instrument. We also briefly report on preliminary results of an experiment where the proposed prioritisation technique was compared with random permutations and four classical string distance metrics were evaluated.",1938-4300,978-1-4244-5259-0978-0-7695-3891,10.1109/ASE.2009.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5431745,Software Engineering;Testing and Debugging;Testing tools;Test case prioritisation;String distance,Software testing;Hamming distance;Automatic testing;Application software;Software engineering;Instruments;Computer industry;Software debugging;Software tools;Costs,program testing;software tools,string distance metrics;test case prioritisation algorithm;ordered test suite;program testing;random permutations,,11,5,,,,,,IEEE,IEEE Conferences
Visualizing the structure of field testing problems,B. Chan; Y. Zou; A. E. Hassan; A. Sinha,"Dept. of Elec. and Comp. Engineering, Queen's University, Kingston, Ontario, Canada; Dept. of Elec. and Comp. Engineering, Queen's University, Kingston, Ontario, Canada; School of Computing, Queen's University, Kingston, Ontario, Canada; Handheld Software Research In Motion (REVI) Waterloo, Ontario, Canada",2009 IEEE International Conference on Software Maintenance,,2009,,,429,432,Field testing of a software application prior to general release is an important and essential quality assurance step. Field testing helps identify unforeseen problems. Extensive field testing leads to the reporting of a large number of problems which often overwhelm the allocated resources. Prior efforts focus primarily on studying the reported problems in isolation. We believe that a global view of the interdependencies between these problems will help in rapid understanding and resolution of reported problems. We present a visualization that highlights the commonalities between reported problems. The visualization helps developers identify two patterns that they can use to prioritize and focus their efforts. We demonstrate the applicability of our visualization through a case study on problems reported during field testing efforts for two releases of a large scale enterprise application.,1063-6773,978-1-4244-4897-5978-1-4244-4828,10.1109/ICSM.2009.5306297,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5306297,,Visualization;Testing,data visualisation;software development management,field testing problem visualisation;large scale enterprise application;software application;quality assurance;unforeseen problem identification;problem resolution,,,9,,,,,,IEEE,IEEE Conferences
A Regression Testing Approach for Software Product Lines Architectures,P. A. d. M. S. Neto; I. d. C. Machado; Y. C. Cavalcanti; E. S. d. Almeida; V. C. Garcia; S. R. d. L. Meira,NA; NA; NA; NA; NA; NA,"2010 Fourth Brazilian Symposium on Software Components, Architectures and Reuse",,2010,,,41,50,"In the Software Product Lines (SPL) context, where products are derived from a common platform, the reference architecture can be considered the main asset. In order to maintain its correctness and reliability after modifications, a regression testing approach based on architecture specification and code was developed. It aims to reduce the testing effort, by reusing test cases, execution results, as well as, selecting and prioritizing an effective set of test cases. Taking advantage of SPL architectures similarities, this approach can be applied among product architectures and between the reference and product architecture. This study also presents an evaluation performed in order to calibrate and improve the proposed approach.",,978-1-4244-8707-3978-0-7695-4259,10.1109/SBCARS.2010.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5631684,Software Product Lines;Regression Testing;Reference Architecture.,Testing;Software;Computer architecture;Maintenance engineering;Context;Planning;Feature extraction,product development;regression analysis;software architecture;software reliability,regression testing approach;software product lines architectures;architecture specification,,4,31,,,,,,IEEE,IEEE Conferences
A Simulation Study on Some Search Algorithms for Regression Test Case Prioritization,S. Li; N. Bian; Z. Chen; D. You; Y. He,NA; NA; NA; NA; NA,2010 10th International Conference on Quality Software,,2010,,,72,81,"Test case prioritization is an approach aiming at increasing the rate of faults detection during the testing phase, by reordering test case execution. Many techniques for regression test case prioritization have been proposed. In this paper, we perform a simulation experiment to study five search algorithms for test case prioritization and compare the performance of these algorithms. The target of the study is to have an in-depth investigation and improve the generality of the comparison results. The simulation study provides two useful guidelines: (1) Two search algorithms, Additional Greedy Algorithm and 2-Optimal Greedy Algorithm, outperform the other three search algorithms in most cases. (2) The performance of the five search algorithms will be affected by the overlap of test cases with regard to test requirements.",2332-662X;1550-6002;1550-6002,978-1-4244-8078-4978-0-7695-4131,10.1109/QSIC.2010.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562946,regression testing;test case prioritization;simulation study;search algorithms,Greedy algorithms;Testing;Software algorithms;Software;Arrays;Algorithm design and analysis;Fault detection,greedy algorithms;program testing;regression analysis;search problems;software fault tolerance,search algorithms;faults detection;regression test case prioritization;additional greedy algorithm;optimal greedy algorithm,,7,16,,,,,,IEEE,IEEE Conferences
A source-based risk analysis approach for software test optimization,A. Hosseingholizadeh,"Department of Computer Science, Ryerson University, Toronto, Canada",2010 2nd International Conference on Computer Engineering and Technology,,2010,2,,V2-601,V2-604,"In this paper we introduce our proposed technique for software component test prioritization and optimization which is based on a source-code based risk analysis. Software test is one of the most critical steps in the software development. Considering that the time and human resources of a software project are limited, software test should be scheduled and planned very carefully. In this paper we introduce a classification approach that provides the developers with a risk model of the application which is specifically designed to assist the testing process by identifying the most important components and their corresponding test effort estimation. We designed an analyser tool to apply our technique to a test software project and we presented the results in this paper.",,978-1-4244-6349-7978-1-4244-6347-3978-1-4244-6348,10.1109/ICCET.2010.5485639,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5485639,Software Engineering;Software Risk;Software Test;Metric;Code-Analysis,Risk analysis;Software testing;Programming;Automatic testing;Risk management;Software quality;Computer science;Humans;Application software;Software tools,pattern classification;program testing;risk analysis;software reliability,source code-based risk analysis approach;software test optimization;software component test prioritization;software development;software project;classification approach,,2,12,,,,,,IEEE,IEEE Conferences
An Approach for Classifying Program Failures,B. Ozcelik; K. Kalkan; C. Yilmaz,NA; NA; NA,2010 Second International Conference on Advances in System Testing and Validation Lifecycle,,2010,,,93,98,"In this work, we leverage hardware performance counters-collected data to automatically group program failures that stem from closely related causes into clusters, which can in turn help developers prioritize failures as well as diagnose their causes. Hardware counters have been used for performance analysis of software systems in the past. By contrast, in this paper they are used as abstraction mechanisms for program executions. The results of our feasibility studies conducted on two widely-used applications suggest that hardware counters-collected data can be used to reliably classify failures.",,978-1-4244-7784-5978-0-7695-4146,10.1109/VALID.2010.10,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5617204,failure classification;debugging aids;hardware performance counters,Radiation detectors;Hardware;Clustering algorithms;Sockets;Accuracy;Algorithm design and analysis;Flexible printed circuits,pattern classification;program debugging;software reliability,program failure classification approach;hardware performance counters;program diagnosis;software systems,,2,17,,,,,,IEEE,IEEE Conferences
An Automatic Configuration Approach to Improve Real-Time Application Throughput While Attaining Determinism,X. Zhang; D. Cao; Y. Gao; X. Chen; H. Mei,NA; NA; NA; NA; NA,2010 IEEE 34th Annual Computer Software and Applications Conference,,2010,,,443,452,"Determinism and throughput are two important performance measures for Java-based real-time applications, but they often conflict. Therefore, it is significant to improve throughput for Java-based real-time applications while guaranteeing its execution time determinism. In this paper, we propose an automatic configuration approach to assign real-time thread priorities to solve the above-mentioned problem. In this approach, we propose an innovative representation of determinism related with real-time thread priorities using stochastic process. Java-based real-time application's throughput is quantified with thread priorities as parameters. The algorithm of integer programming is used to optimize throughput with boundary conditions of the level of determinism. Finally, the Sweet Factory application is tested to evaluate the effect of our approach. Experiment results show that throughput for Java-based real-time applications could be efficiently improved while keeping the execution time determinism with our approach.",0730-3157;0730-3157;0730-3157,978-1-4244-7513-1978-1-4244-7512-4978-0-7695-4085,10.1109/COMPSAC.2010.73,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676292,real-time applications;throughput;determinism;stochastic process;integer programming,Real time systems;Throughput;Time factors;Instruction sets;Stochastic processes;Production facilities;Java,integer programming;Java;real-time systems;software architecture;virtual machines,determinism;Java;real-time thread priority;stochastic process;integer programming,,,15,,,,,,IEEE,IEEE Conferences
An Intelligent Approach of Obtaining Feasible Machining Processes and Their Selection Priorities for Features Based on Neural Network,G. Hua; X. Fan,NA; NA,2010 International Conference on Computational Intelligence and Software Engineering,,2010,,,1,4,"To obtain all feasible machining processes and their quantitative selection priorities, an intelligent making decision approach combining back-propagation neural network and backward planning is proposed. Uniform design method, which is adapted for the problem of multiple factors and multiple levels, is adopted to build representative sample sets for the neural network. The neural network is trained by an improved back-propagation algorithm which can adjust momentum factor and learning rate simultaneously, and tested by linear regression analysis. A case study has been conducted to demonstrate the effectiveness of the proposed approach.",,978-1-4244-5391-7978-1-4244-5392,10.1109/CISE.2010.5677004,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5677004,,Artificial neural networks;Planning;Boring;Training;Surface roughness;Materials,backpropagation;computer aided manufacturing;intelligent manufacturing systems;machining;neural nets;process planning;regression analysis,feasible machining process;intelligent making decision;backpropagation neural network;backward planning;momentum factor;learning rate;linear regression analysis;computer aided process planning,,,10,,,,,,IEEE,IEEE Conferences
Analysis and optimization of software requirements prioritization techniques,M. Aasem; M. Ramzan; A. Jaffar,"University Institute of Information Technology, Arid Agriculture University Rawalpindi, Pakistan; University Institute of Information Technology, Arid Agriculture University Rawalpindi, Pakistan; National University of Computer and Emerging Sciences Islamabad, Pakistan",2010 International Conference on Information and Emerging Technologies,,2010,,,1,6,"Prioritizing requirements helps the project team to understand which requirements are most important and most urgent. Based on this finding a software engineer can decide what to develop/implement in the first release and what on the coming releases. Prioritization is also a useful activity for decision making in other phases of software engineering like development, testing, and implementation. There are a number of techniques available to prioritize the requirements with their associated strengths and limitations. In this paper we will examine state of the art techniques and analyze their applicability on software requirements domain. At the end we present a framework that will help the software engineer of how to perform prioritization process by combining existing techniques and approaches.",,978-1-4244-8003-6978-1-4244-8001-2978-1-4244-8002,10.1109/ICIET.2010.5625687,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5625687,Requirements Engineering;Prioritization Framework,Software;Decision making;Humans;Current measurement;Software engineering;Planning;Manuals,formal specification;formal verification;software development management;systems analysis,software requirement prioritization technique;project team;software engineering;decision making;software development;software testing;software implementation,,12,31,,,,,,IEEE,IEEE Conferences
Analytical survey on automated software test data evaluation,A. Mansoor,"Shaheed Zulfikar Ali Bhutto Institute of Science and Technologies, Islamabad, Pakistan",4th International Conference on New Trends in Information Science and Service Science,,2010,,,580,585,Automated software test data optimization has become a major aspect in quality of any software. For quality different test cases has to be performed for testing. In order to evaluate every aspect of the software program the number of test cases has increased tremendously. In this paper author have tried to evaluate different proposed techniques for automated software test data optimization and emphasize is made to extract the critical factors which need to be present in any technique to make the technique optimized one. These factors are then evaluated on the basis of different papers and concluded some results which are beneficial to work for the creation of an optimized technique.,,978-89-88678-17-6978-1-4244-6982,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5488549,Test Case Generation (TCG);Test Case Reduction (TCR);Test Case Prioritization (TCP);Echelon (Prioritization System),Automatic testing;Software testing;Software quality;Application software;System testing;Benchmark testing;Performance evaluation;Data mining,program testing,software test data optimization;software program;software test data evaluation,,,17,,,,,,IEEE,IEEE Conferences
Arranging software test cases through an optimization method,G. Y. Chen; J. Rogers,"Chung Yuan Christian University, Industrial & Systems Engineering, Chung Li, Taiwan; University of Texas - Arlington, Industrial & Mfg. Systems Engineering, Arlington, TX - USA",PICMET 2010 TECHNOLOGY MANAGEMENT FOR GLOBAL ECONOMIC GROWTH,,2010,,,1,5,"During the software testing process, the customers would be invited to review or inspect an ongoing software product. This phase is called the ƒ??in-plantƒ? test, often known as an ƒ??alphaƒ? test. Typically, this test phase lasts for a very short period of time in which the software test engineers or software quality engineers rush to execute a list of software test cases in the test suite with customers. Because of the time constraint, the test cases have to be arranged in terms of test case severities, estimated test time, and customers' demands. As important as the test case arrangement is, this process is mostly performed manually by the project managers and software test engineers together. As the software systems are getting more sophisticated and complex, a greater volume of test cases have to be generated, and the manual arrangement approach may not be the most efficient way to handle this. In this paper, we propose a framework for automating the process of test case arrangement and management through an optimization method. We believe that this framework will help software test engineers facing with the challenges of prioritizing test cases.",2159-5100,978-1-4244-8203-0978-1-890843-21,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5602131,,Software;Software testing;Measurement;Ant colony optimization;Programming;Manuals,program testing;software quality,software testing process;software product;software quality engineers;software test engineers;project managers;in-plant test;alpha test,,,10,,,,,,IEEE,IEEE Conferences
Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows,P. J. Guo; T. Zimmermann; N. Nagappan; B. Murphy,Stanford University; Microsoft Research; Microsoft Research; Microsoft Research,2010 ACM/IEEE 32nd International Conference on Software Engineering,,2010,1,,495,504,"We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.",1558-1225;0270-5257,978-1-60558-719,10.1145/1806799.1806871,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062117,,Computer bugs;Buildings;Databases;Open source software;Testing;Measurement,operating systems (computers);program debugging;statistical analysis,Windows Vista;Windows 7;Windows bugs;bug fixing;statistical model;probability,,49,42,,,,,,IEEE,IEEE Conferences
Constructing Prioritized Interaction Test Suite with Interaction Relationship,J. Yuan; Z. Jiang,NA; NA,2010 Second International Workshop on Education Technology and Computer Science,,2010,3,,181,184,"Interaction testing has addressed some issues on how to select a small subset of test cases. In many systems where interaction testing is needed, the entire test suite is not executed because of time or budget constraints. It is important to prioritize the test cases in these situations. On the other hand, there are not always interactions among any factors in real systems. Moreover, some factors may need N-way (N&gt;2) testing since there is a closer relationship among them. We present a model for prioritized interaction testing with interaction relationship and propose a greedy algorithm for generating variable strength covering arrays with bias.",,978-1-4244-6389-3978-1-4244-6388,10.1109/ETCS.2010.239,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5459713,covering arrays;software interaction testing;test prioritization;greedy algorithm,System testing;Software testing;Computer science education;Greedy algorithms;Costs;Educational technology;Computer science;Laboratories;Embedded system;Embedded computing,greedy algorithms;program testing,prioritized interaction test suite;interaction relationship;interaction testing;budget constraints;greedy algorithm;variable strength;software testing,,,8,,,,,,IEEE,IEEE Conferences
Design and Analysis of Cost-Cognizant Test Case Prioritization Using Genetic Algorithm with Test History,Y. Huang; C. Huang; J. Chang; T. Chen,NA; NA; NA; NA,2010 IEEE 34th Annual Computer Software and Applications Conference,,2010,,,413,418,"During software development, regression testing is usually used to assure the quality of modified software. The techniques of test case prioritization schedule the test cases for regression testing in an order that attempts to increase the effectiveness in accordance with some performance goal. The most general goal is the rate of fault detection. It assumes all test case costs and fault severities are uniform. However, those factors usually vary. In order to produce a more satisfactory order, the cost-cognizant metric that incorporates varying test case costs and fault severities is proposed. In this paper, we propose a cost-cognizant test case prioritization technique based on the use of historical records and a genetic algorithm. We run a controlled experiment to evaluate the proposed technique's effectiveness. Experimental results indicate that our proposed technique frequently yields a higher Average Percentage of Faults Detected per Cost (APFDc). The results also show that our proposed technique is also useful in terms of APFDc when all test case costs and fault severities are uniform.",0730-3157;0730-3157;0730-3157,978-1-4244-7513-1978-1-4244-7512-4978-0-7695-4085,10.1109/COMPSAC.2010.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5676289,regression testing;cost-cognizant test case prioritizaion;test cost;fault severity;rate of fault detection;Average Percentage of Faults Detected pre Cost(APFDc),Testing;Biological cells;Fault detection;Measurement;Gallium;Search problems;Schedules,genetic algorithms;program testing;regression analysis;scheduling;software fault tolerance;software metrics;software quality,cost-cognizant test case prioritization technique;genetic algorithm;test history;software development;regression testing;modified software quality;test case prioritization schedule technique;fault detection;cost-cognizant metric;average percentage of faults detected per cost;APFDc,,6,12,,,,,,IEEE,IEEE Conferences
Efficient Reduction of Model-Based Generated Test Suites through Test Case Pair Prioritization,H. Cichos; T. S. Heinze,NA; NA,"2010 Workshop on Model-Driven Engineering, Verification, and Validation",,2010,,,37,42,"During the development and maintenance of software, test suites often reach a size that exceeds the costs allocated for test suite execution. In such a case, the test suite needs to be reduced. Many papers are dedicated to the problem of test suite reduction. Most of them consider the removal or merging of test cases. However, less attention has been paid to the identification of test case pairs, which are eminently suitable for merging. In this paper, we fill this gap by presenting a novel approach that helps identifying those test case pairs within a given set of systematically generated test cases which, when merged, have potential for high test suite reduction. As a result, test suites reduced by our approach are considerably smaller in size than those, whose pairs are selected randomly.",,978-0-7695-4384,10.1109/MoDeVVa.2010.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5772249,model-based testing;test suite reduction,Merging;Fault detection;Unified modeling language;Testing;Iron;Barium;Computer aided software engineering,program testing;software maintenance,model-based generated test suites;test case pair prioritization;software development;software maintenance;systematically generated test cases;high test suite reduction,,3,9,,,,,,IEEE,IEEE Conferences
Evaluating and Enhancing Xen-Based Virtual Routers to Support Real-Time Applications,M. Bourguiba; K. Haddadou; G. Pujolle,NA; NA; NA,2010 7th IEEE Consumer Communications and Networking Conference,,2010,,,1,5,"Router virtualization seems as the obvious next step to system virtualization and the key to easily deploy and manage next generation overlay virtual networks. In this paper, we investigate the viability of virtual routers on a Xen-based system. We first evaluate the system throughput when achieving forwarding in the virtual routers. Then, we consider the context where virtual routers are dedicated to flows of different types and propose a mechanism to guarantee the required throughput and latency to real time applications while maintaining an optimal aggregated system throughput. We achieved this through both configuring the Xen Credit scheduler and establishing priorities between packets in the driver domain before switching them to the target virtual router.",2331-9852;2331-9860,978-1-4244-5175-3978-1-4244-5176,10.1109/CCNC.2010.5421620,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5421620,,Throughput;Laboratories;Hardware;Computer architecture;Real time systems;Application virtualization;Delay;Protocols;Testing;Software performance,network servers;virtual machines;virtual private networks,Xen-based virtual routers;router virtualization;system virtualization;overlay virtual networks;Xen credit scheduler,,,7,,,,,,IEEE,IEEE Conferences
Making defect-finding tools work for you,M. G. Nanda; M. Gupta; S. Sinha; S. Chandra; D. Schmidt; P. Balachandran,IBM Research - India; IBM Research - India; IBM Research - India; IBM Research - TJ Watson Research Center; IBM Tivoli; IBM Rational,2010 ACM/IEEE 32nd International Conference on Software Engineering,,2010,2,,99,108,"Given the high costs of software testing and fixing bugs after release, early detection of bugs using static analysis can result in significant savings. However, despite their many benefits, recent availability of many such tools, and evidence of a positive return-on-investment, static-analysis tools are not used widely because of various usability and usefulness problems. The usability inhibitors include the lack of features, such as capabilities to merge reports from multiple tools and view warning deltas between two builds of a system. The usefulness problems are related primarily to the accuracy of the tools: identification of false positives (or, spurious bugs) and uninteresting bugs among the true positives. In this paper, we present the details of an online portal, developed at IBM Research, to address these problems and promote the adoption of static-analysis tools. We report our experience with the deployment of the portal within the IBM developer community. We also highlight the problems that we have learned are important to address, and present our approach toward solving some of those problems.",1558-1225;0270-5257,978-1-60558-719,10.1145/1810295.1810310,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062143,defect differencing;defect merging;defect prioritization;defect views;static analysis portal,Computer bugs;Portals;Usability;Java;Open source software;Servers;Null value,portals;program diagnostics,defect-finding tools;software testing;bug fixing;static-analysis tools;usability problems;usefulness problems;online portal;IBM Research;IBM developer community,,10,14,,,,,,IEEE,IEEE Conferences
Managing Testing Complexity in Dynamically Adaptive Systems: A Model-Driven Approach,K. Welsh; P. Sawyer,NA; NA,"2010 Third International Conference on Software Testing, Verification, and Validation Workshops",,2010,,,290,298,"Autonomous systems are increasingly conceived as a means to allow operation in changeable or poorly understood environments. However, granting a system autonomy over its operation removes the ability of the developer to be completely sure of the system's behaviour under all operating contexts. This combination of environmental and behavioural uncertainty makes the achievement of assurance through testing very problematic. This paper focuses on a class of system, called an m-DAS, that uses run-time models to drive run-time adaptations in changing environmental conditions. We propose a testing approach which is itself model-driven, using model analysis to significantly reduce the set of test cases needed to test for emergent behaviour. Limited testing resources may therefore be prioritised for the most likely scenarios in which emergent behaviour may be observed.",,978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050,10.1109/ICSTW.2010.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463659,component;model-driven;model-directed;testing;autonomous;dynamically adaptive systems,System testing;Adaptive systems;Runtime environment;Software testing;Environmental management;Switches;Middleware;Logic;Conference management;Uncertainty,program testing;software engineering,complexity testing;dynamically adaptive systems;model-driven approach;autonomous systems;environmental uncertainty;behavioural uncertainty;m-DAS system;run-time models,,2,25,,,,,,IEEE,IEEE Conferences
Measuring unmeasurable attributes of software quality using Pragmatic Quality Factor,J. H. Yahaya; A. Deraman,"College of Arts and Sciences, Building of Information Technology, Universiti Utara Malaysia (UUM), 06010, Sintok, Kedah, Malaysia; Vice Chancellor Office, Universiti Malaysia Terengganu (UMT), 21030, Kuala Terengganu, Malaysia",2010 3rd International Conference on Computer Science and Information Technology,,2010,1,,197,202,"Software quality is evolving beyond static measurement to a wider scope of quality definition. Previous studies have indicated the importance of human aspect in software quality. But the quality models have not included comprehensively this aspect together with the behavioural aspect of software quality. This research has proposed a Pragmatic Quality Factors (or PQF) as a software quality measurement and metrics that includes both aspects of quality. These aspects of quality are essential as to balance between technical and non-technical (human) facet. In addition, this model provides flexibility by giving priorities and weights to the quality attributes. The priority and weight are necessary to reflect business requirement in the real business environment. Therefore, it is more practical that suits with different users and purposes. It is implemented through collaborative perspective approach between users, developers and independent assessor. This model shows how the unmeasurable characteristics can be measured indirectly using measures and metrics approach. It has been tested involving assessment and certification exercises in real case studies in Malaysia.",,978-1-4244-5540-9978-1-4244-5537-9978-1-4244-5539,10.1109/ICCSIT.2010.5564077,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5564077,software quality;pragmatic quality factor;software measurement,Measurement;Accuracy;Usability;Art;Stability analysis,groupware;software metrics;software quality,unmeasurable attribute measuring;pragmatic quality factor;static measurement;software quality measurement;software metrics;business requirement;collaborative perspective approach;assessment exercises;certification exercises,,3,28,,,,,,IEEE,IEEE Conferences
Notice of Retraction<br>Designing and formulating organization performance evaluation model in AHP method based on EFQM criteria (case study),S. Iranzadeh; F. Chakherlouy,"Islamic Azad university,Tabriz Branch, Iran; Tabriz Business Training Center, Iran",2010 IEEE International Conference on Advanced Management Science(ICAMS 2010),,2010,1,,606,609,"Designing and formulating a comprehensive organization performance evaluation model based on European Foundation for Quality Management (EFQM) in AHP (Analytic Hierarchy Process) method is the main aim of the present research study. Evaluation is considered as one of the most important activities in each organization in a way that reformation of processes and procedures of doing activity without evaluation of results will be impossible. At the present research activity, AHP (Analytic Hierarchy Process) has been used as one of MADM (Multi-Attribute Decision Making) methods for the evaluation of performance of organizations through the application of EFQM (European Foundation for Quality Management) excellence model criteria. Also, Municipality of City of Tabriz has been selected as subjects for testing the presented model. In the same direction, seven districts of this municipality were selected as sample model. Necessary and required information were accumulated through questionnaire, interview and also taking advantage of data and library resources, details of which were analyzed and studied through the application of advanced Excel and Expert Choice 11.5 software package system. Eventually, various districts of this organization were evaluated and prioritized in terms of performance.;Notice of Retraction<br><br>After careful and considered review of the content of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.<br><br>We hereby retract the content of this paper. Reasonable effort should be made to remove all past references to this paper.<br><br>The presenting author of this paper has the option to appeal this decision by contacting TPII@ieee.org.<br><br>",,978-1-4244-6932-1978-1-4244-6931-4978-1-4244-6930,10.1109/ICAMS.2010.5553098,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5553098,Evaluation of Performance;EFQM (European Foundation for Quality Management);Analytic Hierarchy Process (AHP),Integrated circuits;Europe;Lead;Analytical models;Films,decision making;organisational aspects;quality management,organization performance evaluation model;European Foundation for quality management;analytic hierarchy process method;multiattribute decision making method;excellence model criteria;Tabriz;Excel;Expert Choice 11.5 software package system,,,8,,,,,,IEEE,IEEE Conferences
"On the Integration of Test Adequacy, Test Case Prioritization, and Statistical Fault Localization",B. Jiang; W. K. Chan,NA; NA,2010 10th International Conference on Quality Software,,2010,,,377,384,"Testing and debugging account for at least 30% of the project effort. Scientific advancements in individual activities or their integration may bring significant impacts to the practice of software development. Fault localization is the foremost debugging sub-activity. Any effective integration between testing and debugging should address how well testing and fault localization can be worked together productively. How likely does a testing technique provide test suites for effective fault localization? To what extent may such a test suite be prioritized so that the test cases having higher priority can be effectively used in a standalone manner to support fault localization? In this paper, we empirically study these two research questions in the context of test data adequacy, test case prioritization and statistical fault localization. Our preliminary postmortem analysis results on 16 test case prioritization techniques and four statistical fault localizations show that branch-adequate test suites on the Siemens suite are unlikely to support effective fault localization. On the other hand, if such a test suite is effective, around 60% of the test cases can be further prioritized to support effective fault localization, which indicates that the potential savings in terms of effort can be significant.",2332-662X;1550-6002;1550-6002,978-1-4244-8078-4978-0-7695-4131,10.1109/QSIC.2010.64,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562990,debugging;testing;continuous integration,Equations;Servers;Measurement;Testing;Debugging;Software;Inspection,continuous improvement;program debugging;program testing;software metrics;software quality;statistical analysis,test adequacy;test case prioritization;statistical fault localization;software debugging;software development;branch adequate test suite;Siemens suite;continuous integration,,4,23,,,,,,IEEE,IEEE Conferences
Optimizing the Software Architecture for Extensibility in Hard Real-Time Distributed Systems,Q. Zhu; Y. Yang; M. Natale; E. Scholte; A. Sangiovanni-Vincentelli,"Intel Corporation; EECS Department, University of California at Berkeley; Scuola Superiore S. Anna, Pisa; United Technologies Research Center; EECS Department, University of California at Berkeley",IEEE Transactions on Industrial Informatics,,2010,6,4,621,636,"We consider a set of control tasks that must be executed on distributed platforms so that end-to-end latencies are within deadlines. We investigate how to allocate tasks to nodes, pack signals to messages, allocate messages to buses, and assign priorities to tasks and messages, so that the design is extensible and robust with respect to changes in task requirements. We adopt a notion of extensibility metric that measures how much the execution times of tasks can be increased without violating end-to-end deadlines. We optimize the task and message design with respect to this metric by adopting a mathematical programming front-end followed by postprocessing heuristics. The proposed algorithm as applied to industrial strength test cases shows its effectiveness in optimizing extensibility and a marked improvement in running time with respect to an approach based on randomized optimization.",1551-3203;1941-0050,,10.1109/TII.2010.2053938,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5535223,Design space exploration;distributed system;extensibility;platform-based design;real-time,Software architecture;Real time systems;Robustness;Design optimization;Control system synthesis;Automatic control;Supply chains;Production;Sensor systems;Control systems,distributed processing;optimisation;software architecture,software architecture;hard real-time distributed systems;end-to-end latency;extensibility metric notion;randomized optimization;mathematical programming front-end,,30,22,,,,,,IEEE,IEEE Journals & Magazines
Point-of-Interest Aware Test Case Prioritization: Methods and Experiments,K. Zhai; W. K. Chan,NA; NA,2010 10th International Conference on Quality Software,,2010,,,449,456,"Location based services personalize their behaviors based on location data. When data kept by a service have evolved or the code has been modified, regression testing can be employed to assure the quality of services. Frequent data update however may lead to frequent regression testing and any faulty implementation of a service may affect many service consumers. Proper test case prioritization helps reveal service problems efficiently. In this paper, we review a set of point-of-interest (POI) aware test case prioritization techniques and report an experiment on such techniques. The empirical results show that these POI-aware techniques are more effective than random ordering and input-guided test case prioritization in terms of APFD. Furthermore, their effectiveness is observed to be quite stable over different sizes of the test suite.",2332-662X;1550-6002;1550-6002,978-1-4244-8078-4978-0-7695-4131,10.1109/QSIC.2010.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5563000,test case prioritization;location-based service,Global Positioning System;Measurement;Entropy;Testing;Book reviews;Cities and towns;Software,mobile computing;program testing;regression analysis,point-of-interest aware test case prioritization;location based services;quality of services;frequent regression testing;faulty implementation;POI,,1,29,,,,,,IEEE,IEEE Conferences
Prioritization of Issues and Requirements by Cumulative Voting: A Compositional Data Analysis Framework,P. Chatzipetrou; L. Angelis; P. Rovegard; C. Wohlin,NA; NA; NA; NA,2010 36th EUROMICRO Conference on Software Engineering and Advanced Applications,,2010,,,361,370,"Cumulative Voting (CV), also known as Hundred-Point Method, is a simple and straightforward technique, used in various prioritization studies in software engineering. Multiple stakeholders (users, developers, consultants, marketing representatives or customers) are asked to prioritize issues concerning requirements, process improvements or change management in a ratio scale. The data obtained from such studies contain useful information regarding correlations of issues and trends of the respondents towards them. However, the multivariate and constrained nature of data requires particular statistical analysis. In this paper we propose a statistical framework; the multivariate Compositional Data Analysis (CoDA) for analyzing data obtained from CV prioritization studies. Certain methodologies for studying the correlation structure of variables are applied to a dataset concerning impact analysis issues prioritized by software professionals under different perspectives. These involve filling of zeros, transformation using the geometric mean, principle component analysis on the transformed variables and graphical representation by biplots and ternary plots.",2376-9505;1089-6503,978-1-4244-7901,10.1109/SEAA.2010.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5598119,Prioritization;Compositional Data Analysis;Cumulative Voting;Hundred-Point Method;Hundred-Dollar test,Correlation;Statistical analysis;Software;Data analysis;Context;Covariance matrix;Software engineering,data analysis;principal component analysis;software engineering,cumulative voting;multivariate compositional data analysis;hundred-point method;software engineering;statistical analysis;CoDA;correlation structure;geometric mean;principle component analysis;graphical representation,,10,36,,,,,,IEEE,IEEE Conferences
Prioritization of test case scenarios derived from activity diagram using genetic algorithm,S. Sabharwal; R. Sibal; C. Sharma,"Department of computer Science and IT, Netaji Subhas Institute of Technology, Delhi, India; Department of computer Science and IT, Netaji Subhas Institute of Technology, Delhi, India; Department of computer Science and IT, Netaji Subhas Institute of Technology, Delhi, India",2010 International Conference on Computer and Communication Technology (ICCCT),,2010,,,481,485,"Software testing involves identifying the test cases which discovers the errors in the program. However, the exhaustive testing is rarely impossible and very time consuming. In this paper, the software testing efficiency is optimized by identifying the critical path clusters. The test case scenarios are derived from the activity diagram and the testing efficiency is optimized by applying the genetic algorithm on the test data. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of activity diagram.",,978-1-4244-9034-9978-1-4244-9033-2978-1-4244-9032,10.1109/ICCCT.2010.5640479,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5640479,software testing;genetic algorithm;activity diagram;CFG,Biological cells;Gallium;Unified modeling language;Software testing;Software;Evolutionary computation,genetic algorithms;program testing,software testing;program error;critical path cluster;test case scenario;activity diagram;genetic algorithm;information flow metric;information flow complexity,,4,17,,,,,,IEEE,IEEE Conferences
Prioritizing Mutation Operators Based on Importance Sampling,M. Sridharan; A. S. Namin,NA; NA,2010 IEEE 21st International Symposium on Software Reliability Engineering,,2010,,,378,387,"Mutation testing is a fault-based testing technique for measuring the adequacy of a test suite. Test suites are assigned scores based on their ability to expose synthetic faults (i.e., mutants) generated by a range of well-defined mathematical operators. The test suites can then be augmented to expose the mutants that remain undetected and are not semantically equivalent to the original code. However, the mutation score can be increased superfluously by mutants that are easy to expose. In addition, it is infeasible to examine all the mutants generated by a large set of mutation operators. Existing approaches have therefore focused on determining the sufficient set of mutation operators and the set of equivalent mutants. Instead, this paper proposes a novel Bayesian approach that prioritizes operators whose mutants are likely to remain unexposed by the existing test suites. Probabilistic sampling methods are adapted to iteratively examine a subset of the available mutants and direct focus towards the more informative operators. Experimental results show that the proposed approach identifies more than 90% of the important operators by examining ? 20% of the available mutants, and causes a 6% increase in the importance measure of the selected mutants.",1071-9458;1071-9458;2332-6549,978-1-4244-9056-1978-0-7695-4255,10.1109/ISSRE.2010.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5635074,Mutation Testing;Testing Effectiveness;Importance Sampling;Bayesian Reasoning,Testing;Monte Carlo methods;Entropy;Probability distribution;Equations;Probabilistic logic;Stochastic processes,Bayes methods;importance sampling;program testing;software fault tolerance,mutation operators;importance sampling;mutation testing;fault-based testing technique;Bayesian approach;probabilistic sampling method,,11,22,,,,,,IEEE,IEEE Conferences
Prioritizing State-Based Aspect Tests,D. Xu; J. Ding,NA; NA,"2010 Third International Conference on Software Testing, Verification and Validation",,2010,,,265,274,"In aspect-oriented programming, aspects are essentially incremental modifications to their base classes. Therefore aspect-oriented programs can be tested in an incremental fashion - we can first test the base classes and then test the base classes and aspects as a whole. This paper demonstrates that, in this incremental testing paradigm, we can prioritize aspect tests so as to report failure earlier. We explore test prioritization for testing aspect-oriented programs against their state models with transition coverage and round-trip coverage. Aspect tests are generated from woven state models obtained by composing aspect models into their base class models. We prioritize aspect tests by identifying the extent to which an aspect modifies its base classes. The modification is measured by the number of new and changed components in state transitions (start state, event, precondition, postcondition, end state). Transitions with more changes have higher priorities for test generation. We evaluate the impact of aspect test prioritization through mutation analysis of two AspectJ programs, where all aspects and their base classes can be modeled by finite state machines. We create aspect mutants of each AspectJ program according to a comprehensive AspectJ fault model. Then we test each mutant with the test suites generated without prioritization and with prioritization, respectively. Our experiment results show that prioritization of aspect tests has accelerated failure report.",2159-4848,978-1-4244-6436-4978-1-4244-6435-7978-0-7695-3990,10.1109/ICST.2010.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477076,software testing;aspect-oriented programming;model-based testing;finite state machine;test prioritization,Object oriented modeling;Automata;Software testing;Protection;Computer science;Genetic mutations;Life estimation;Quality assurance,aspect-oriented programming;fault tolerant computing;finite state machines;program testing,state based aspect test;aspect oriented programming;incremental testing paradigm;aspect test prioritization;mutation analysis;AspectJ program;finite state machine;AspectJ fault model,,5,33,,,,,,IEEE,IEEE Conferences
Prioritizing Tests for Software Fault Localization,A. Gonzalez-Sanchez; E. Piel; H. Gross; A. J. C. van Gemund,NA; NA; NA; NA,2010 10th International Conference on Quality Software,,2010,,,42,51,"Test prioritization techniques select test cases that maximize the confidence on the correctness of the system when the resources for quality assurance (QA) are limited. In the event of a test failing, the fault at the root of the failure has to be localized, adding an extra debugging cost that has to be taken into account as well. However, test suites that are prioritized for failure detection can reduce the amount of useful information for fault localization. This deteriorates the quality of the diagnosis provided, making the subsequent debugging phase more expensive, and defeating the purpose of the test cost minimization. In this paper we introduce a new test case prioritization approach that maximizes the improvement of the diagnostic information per test. Our approach minimizes the loss of diagnostic quality in the prioritized test suite. When considering QA cost as the combination of testing cost and debugging cost, on the Siemens set, the results of our test case prioritization approach show up to a 53% reduction of the overall QA cost, compared with the next best technique.",2332-662X;1550-6002;1550-6002,978-1-4244-8078-4978-0-7695-4131,10.1109/QSIC.2010.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562943,testing;test prioritization;debugging;diagnosis,Software,fault location;program debugging;program testing;software quality,software fault localization;test prioritization technique;quality assurance;failure detection;diagnosis quality;test cost minimization;test case prioritization approach,,14,18,,,,,,IEEE,IEEE Conferences
Prioritizing Unit Test Creation for Test-Driven Maintenance of Legacy Systems,E. Shihab; Z. M. Jiang; B. Adams; A. E. Hassan; R. Bowerman,NA; NA; NA; NA; NA,2010 10th International Conference on Quality Software,,2010,,,132,141,"Test-Driven Development (TDD) is a software development practice that prescribes writing unit tests before writing implementation code. Recent studies have shown that TDD practices can significantly reduce the number of pre-release defects. However, most TDD research thus far has focused on new development. We investigate the adaptation of TDD-like practices for already implemented code, in particular legacy systems. We call this adaptation of TDD-like practices for already implemented code ``Test-Driven Maintenance'' (TDM). In this paper, we present an approach that assists software development and testing managers, who employ TDM, utilize the limited resources they have for testing legacy systems efficiently. The approach leverages the development history of the project to generate a prioritized list of functions that managers should focus their unit test writing resources on. The list is updated dynamically as the development of the legacy system progresses. To evaluate our approach, we conduct a case study on a large commercial legacy software system. Our findings suggest that heuristics based on the function size, modification frequency and bug fixing frequency should be used to prioritize the unit test writing of legacy systems.",2332-662X;1550-6002;1550-6002,978-1-4244-8078-4978-0-7695-4131,10.1109/QSIC.2010.74,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5562952,,Computer bugs;Writing;Measurement;History;Time division multiplexing;Maintenance engineering;Data mining,program testing;software development management;software maintenance,test-driven maintenance;test-driven development;software development practice;unit test writing resources;legacy software system;modification frequency;bug fixing frequency,,3,45,,,,,,IEEE,IEEE Conferences
Ranking Attacks Based on Vulnerability Analysis,J. A. Wang; H. Wang; M. Guo; L. Zhou; J. Camargo,NA; NA; NA; NA; NA,2010 43rd Hawaii International Conference on System Sciences,,2010,,,1,10,"Now that multiple-known attacks can affect one software product at the same time, it is necessary to rank and prioritize those attacks in order to establish a better defense. The purpose of this paper is to provide a set of security metrics to rank attacks based on vulnerability analysis. The vulnerability information is retrieved from a vulnerability management ontology, which integrates commonly used standards like CVE, CWE, CVSS, and CAPEC. Among the benefits of ranking attacks through the method proposed here are: a more effective mitigation or prevention of attack patterns against systems, a better foundation to test software products, and a better understanding of vulnerabilities and attacks.",1530-1605,978-1-4244-5510-2978-1-4244-5509,10.1109/HICSS.2010.313,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5428663,,Information security;Software tools;Software testing;Data security;Ontologies;Software systems;Programming;Computer industry;Software standards;Information retrieval,ontologies (artificial intelligence);program testing;security of data,ranking attacks;vulnerability analysis;software product;vulnerability management ontology,,7,19,,,,,,IEEE,IEEE Conferences
Regression test cases prioritization using Failure Pursuit Sampling,C. Simons; E. C. Paraiso,"Postgraduate Program in Informatics, Pontifical Catholic University of Parana, Curitiba, Brazil; Postgraduate Program in Informatics, Pontifical Catholic University of Parana, Curitiba, Brazil",2010 10th International Conference on Intelligent Systems Design and Applications,,2010,,,923,928,"The necessity of lowering the execution of system tests' cost is a consensual point in the software development community. The present study presents an optimization of the regression tests' activity, by adapting a test cases prioritization technique called Failure Pursuit Sampling-previously used and validated for the prioritization of tests in general-improving its efficiency for the exclusive execution of regression test. For this purpose, the clustering and sampling phases of the original technique were modified, so that it becomes capable of receive information from tests made on the previous version of a program, and can use this information to drive de efficiency of the new developed technique, for tests made on a present version. The adapted technique was implemented and executed using the Schedule program, of the Siemens suit. By using Average of the Percentage of Faults Detected charts, the modified Failure Pursuit Sampling technique presented a high level of efficiency improvement.",2164-7143;2164-7151,978-1-4244-8136-1978-1-4244-8134-7978-1-4244-8135,10.1109/ISDA.2010.5687069,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5687069,regression tests;test case prioritization;failure pursuit sampling;clustering,Nearest neighbor searches;Schedules;Software;Fault detection;Optimization;Intelligent systems;Clustering algorithms,optimisation;program testing;regression analysis;sampling methods;software development management,regression test cases prioritization;failure pursuit sampling;system test cost;software development community;optimization;regression tests;Siemens suit,,1,26,,,,,,IEEE,IEEE Conferences
Regression Test Generation Approach Based on Tree-Structured Analysis,Z. Zhang; J. Huang; B. Zhang; J. Lin; X. Chen,NA; NA; NA; NA; NA,2010 International Conference on Computational Science and Its Applications,,2010,,,244,249,"Regression test generation is an important process to make sure that changes of program have no unintended side-effects. To achieve full confidence, many projects have to re-run all the test cases for entire program, which makes it a time consuming and expensive activity. In this paper, a code based regression testing approach is proposed to generate selected test suites for unit testing. The framework contains five phases: program change detection phase, logical verification phase, branch pruning phase, test case prioritization phase and test suite generation phase. These five phases can achieve detection of program's modification, coding standard, test case pruning, test case prioritizing and inputs generation for regression test cases respectively. A prototype based on this framework is implemented using logical tree-structured analysis, and the preliminary experiment shows that proposed approach can provide efficient regression test suites.",,978-1-4244-6462-3978-1-4244-6461-6978-0-7695-3999,10.1109/ICCSA.2010.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5476640,regression test;automated test generation;white-box test;software verification,Regression tree analysis;Logic testing;Software testing;Automatic testing;Phase detection;Life testing;Software maintenance;Prototypes;Computer architecture;Tree data structures,program diagnostics;regression analysis,regression test generation approach;code based regression testing;program change detection phase;logical verification phase;branch pruning phase;test case prioritization phase;test suite generation phase;program modification detection;coding standard;test case pruning;logical tree-structured analysis,,,16,,,,,,IEEE,IEEE Conferences
Requirement based test case prioritization,R. Kavitha; V. R. Kavitha; N. Suresh Kumar,"Computer Science and Engineering, VCET, Madurai, India; Master of Computer Applications, VCET, Madurai, India; Electronics and Communication Engineering, VCET, Madurai, India",2010 INTERNATIONAL CONFERENCE ON COMMUNICATION CONTROL AND COMPUTING TECHNOLOGIES,,2010,,,826,829,"Test case prioritization involves scheduling test cases in an order that increases the effectiveness in achieving some performance goals. One of the most important performance goals is the rate of fault detection. Test cases should run in an order that increases the possibility of fault detection and also that detects faults at the earliest in its testing life cycle. In this paper, an algorithm is proposed for system level test case prioritization (TCP) from software requirement specification to improve user satisfaction with quality software and also to improve the rate of severe fault detection. The proposed model prioritizes the system test cases based on the three factors: customer priority, changes in requirement, implementation complexity. The proposed prioritization technique is validated with two different sets of industrial projects and the results show that the proposed prioritization technique improves the rate of severe fault detection.",,978-1-4244-7770-8978-1-4244-7769-2978-1-4244-7768,10.1109/ICCCCT.2010.5670728,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5670728,Test case;test case Prioritization,Fault detection;Complexity theory;Software algorithms;Software;Software testing;Algorithm design and analysis,formal specification;program testing;software fault tolerance;software quality,requirement based test case prioritization;test case scheduling;fault detection;testing life cycle;system level test case prioritization;software requirement specification;user satisfaction;industrial projects,,4,15,,,,,,IEEE,IEEE Conferences
Requirement-based test case generation and prioritization,Y. I. Salem; R. Hassan,"College of Computing and Information Systems, Arab Academy for Science and Technology, Cairo, Egypt; College of Computing and Information Systems, Arab Academy for Science and Technology, Cairo, Egypt",2010 International Computer Engineering Conference (ICENCO),,2010,,,152,157,"Software release testing is a critical phase in the software development life cycle, as it validates the software against its requirements. Designing comprehensive release test cases that are driven by the software requirements remain the major success factor of the testing phase as far as the software customers are concerned. Further, availing sufficient traceability information to ensure complete coverage of requirements validation in the designed test case suite is significant to software quality assurance. In this paper, we propose a systematic mechanism to derive a set of release test cases from a set of requirements modeled with the Genetic Software Engineering (GSE) method. GSE models functional requirements with a semi-formal visual notation called Behavior Trees (BT). Our algorithm prioritizes the requirements modeled with BTs and derives a set of prioritized release test cases systematically. Additionally, our algorithm provides sufficient traceability information relating test cases to the requirements being tested. This allows for ensuring completeness of test case coverage. We also demonstrate our test case derivation mechanism through a case study.",,978-1-61284-185-4978-1-61284-184-7978-1-61284-183,10.1109/ICENCO.2010.5720443,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720443,requirements-driven testig test case prioritization;test case generation,Unified modeling language;Semantics;Software;Systematics,program testing;project management;quality assurance;software quality;trees (mathematics),software release testing;software development life cycle;software requirements;software customers;software quality assurance;genetic software engineering;GSE models;behavior trees;test case generation;test case prioritization,,1,15,,,,,,IEEE,IEEE Conferences
Risk-Based Testing: A Case Study,E. Souza; C. Gusm?œo; J. Ven?›ncio,NA; NA; NA,2010 Seventh International Conference on Information Technology: New Generations,,2010,,,1032,1037,"This paper describes the application of risk-based testing for a software product evaluation in a real case study. Risk-based testing consists of a set of activities regarding risk factors identification related to software requirements. Once identified, the risks are prioritized according to their likelihood and impact and test cases are designed based on the strategies for treatment of the identified risk factors. Thus, test efforts are continuously adjusted according to risk monitoring. The paper also briefly reviews available risk-based approaches, describes the benefits that are likely to accrue from the growing body of work in this area and provides a set of problems, challenges and future work.",,978-1-4244-6271-1978-1-4244-6270-4978-0-7695-3984,10.1109/ITNG.2010.203,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501497,Case Study;Risk-Based Testing;Risk Management;Software Testing;Testing Process,Software testing;System testing;Monitoring;Risk management;Software quality;Costs;Programming;Risk analysis;Automatic testing;Information technology,program testing;risk management,risk-based testing;software product evaluation;risk factor identification;software requirements,,5,12,,,,,,IEEE,IEEE Conferences
Risks of unrecognized commonalities in information technology supply chains,C. W. Axelrod,"Delta Risk LLC, Great Neck, New York",2010 IEEE International Conference on Technologies for Homeland Security (HST),,2010,,,495,499,"In this paper we examine the interdependencies and common points of failure (and attack) that plague commonly-used system and network hardware and software. The proposed approach requires not only generating inventories of acquiring organizations' equipment and software products, and clear and detailed descriptions of every link in the supply chain, but also the identification of common components and their sources. This information is required not only for manufacturer and OEM supply chains, but also for the services supply chains of maintenance and repair organizations. When such critical components and services have been identified, one must prioritize their importance and apply appropriate security and testing. Such an identification and tracking system is only as good as its ability to incorporate up-to-the-minute changes and additions. This requires extensive real-time reporting and information sharing. The author presents a general description of a proprietary tool that facilitates the collaboration needed for such an approach to be effective.",,978-1-4244-6048-9978-1-4244-6047-2978-1-4244-6046,10.1109/THS.2010.5654970,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5654970,supply chain;dependencies;complexity;risk mitigation;common points of failure;IT outsourcing;computer hardware;computer software,Supply chains;Software;Complexity theory;Security;Organizations;Hardware;Testing,information technology;production engineering computing;risk management;supply chain management,unrecognized commonalities;information technology supply chains;plague commonly-used system;network hardware;network software;OEM supply chains;repair organizations;tracking system;identification system;information sharing,,1,7,,,,,,IEEE,IEEE Conferences
Sequence-based techniques for black-box test case prioritization for composite service testing,A. Askarunisa; A. M. Abirami; K. A. J. Punitha; B. K. Selvakumar; R. Arunkumar,"Department of Computer Science Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science Engineering, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science Engineering, Thiagarajar College of Engineering, Madurai, India",2010 IEEE International Conference on Computational Intelligence and Computing Research,,2010,,,1,4,"Web service may evolve autonomously, making peer web services in the same service composition uncertain as to whether the evolved behaviors may still be compatible to its originally collaborative agreement. Although peer services may wish to conduct regression testing to verify the original collaboration, the source code of the former service can be inaccessible to them. Traditional code-based regression testing strategies are inapplicable to web services. In this paper, we formulate new test case prioritization strategies using sequences in XML messages to reorder regression test cases for composite web services, against the tag based techniques given in and reveal how the test cases use the interface specifications of the composite services. The results were evaluated experimentally and the results show that the new techniques can have a high probability of outperforming random ordering and the techniques given in.",,978-1-4244-5967-4978-1-4244-5965,10.1109/ICCIC.2010.5705784,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5705784,Black-box Regression Testing;Composite service Testing;Test case prioritization,Testing;Web services;XML;Fault detection;Meteorology;IEEE Computer Society Press;Software,groupware;identification technology;peer-to-peer computing;program testing;regression analysis;Web services;XML,peer Web service;collaborative agreement;regression testing;source code;test case prioritization strategies;XML message sequence;tag based techniques;interface specification;black-box test case prioritization;composite service testing,,2,11,,,,,,IEEE,IEEE Conferences
Stress Testing an AI Based Web Service: A Case Study,A. Chakravarty,NA,2010 Seventh International Conference on Information Technology: New Generations,,2010,,,1004,1008,"The stress testing of AI-based systems differs from the approach taken for more traditional Web services, both in terms of the design of test cases and the metrics used to measure quality. The expected variability in responses of an AI-based system to the same request adds a level of complexity to stress testing, when compared to more standard systems where the system response is deterministic and any deviations may easily be characterized as product defects. Generating test cases for AI-based systems requires balancing breadth of test cases with depth of response quality: most AI-systems may not return a perfect answer. An example of a machine learning translation system is considered, and the approach used for stress testing it is presented, alongside comparisons with a more traditional approach. The challenges of shipping such a system to support a growing set of features and language pairs necessitate a mature prioritization of test cases. This approach has been successful in shipping a Web service that currently serves millions of users per day.",,978-1-4244-6271-1978-1-4244-6270-4978-0-7695-3984,10.1109/ITNG.2010.149,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5501500,Internet applications;measurement techniques;software engineering;software performance testing;software testing,Artificial intelligence;Web services;System testing;Software testing;Stress measurement;Application software;Software performance;Software measurement;Machine learning algorithms;Information technology,learning (artificial intelligence);program testing;Web services,stress testing;artificial intelligence;AI-based system;Web service;machine learning translation system,,1,10,,,,,,IEEE,IEEE Conferences
Supporting Concern-Based Regression Testing and Prioritization in a Model-Driven Environment,R. S. S. Filho; C. J. Budnik; W. M. Hasling; M. McKenna; R. Subramanyan,NA; NA; NA; NA; NA,2010 IEEE 34th Annual Computer Software and Applications Conference Workshops,,2010,,,323,328,"Traditional regression testing and prioritization approaches are bottom-up (or white-box). They rely on the analysis of the impact of changes in source code artifacts, identifying corresponding parts of software to retest. While effective in minimizing the amount of testing required to validate code changes, they do not leverage on specification-level design and requirements concerns that motivated these changes. Model-based testing approaches support a top-down (or black box) testing approach, where design and requirements models are used in support of test generation. They augment code-based approaches with the ability to test from a higher-level design and requirements perspective. In this paper, we present a model-based regression testing and prioritization approach that efficiently selects test cases for regression testing based on different concerns. It relies on traceability links between models, test cases and code artifacts, together with user-defined properties associated to model elements. In particular we describe how to support concern-based regression testing and prioritization using TDE/UML, an extensible model-based testing environment.",,978-1-4244-8089-0978-0-7695-4105,10.1109/COMPSACW.2010.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615818,Model-driven testing;test development;regression testing;test prioritization,,program testing;regression analysis,supporting concern based regression testing;model driven environment;source code artifacts;specification level design;model based testing approaches;code based approaches;user defined properties;UML,,6,17,,,,,,IEEE,IEEE Conferences
Taking Advantage of Service Selection: A Study on the Testing of Location-Based Web Services Through Test Case Prioritization,K. Zhai; B. Jiang; W. K. Chan; T. H. Tse,NA; NA; NA; NA,2010 IEEE International Conference on Web Services,,2010,,,211,218,"Dynamic service compositions pose new verification and validation challenges such as uncertainty in service membership. Moreover, applying an entire test suite to loosely coupled services one after another in the same composition can be too rigid and restrictive. In this paper, we investigate the impact of service selection on service-centric testing techniques. Specifically, we propose to incorporate service selection in executing a test suite and develop a suite of metrics and test case prioritization techniques for the testing of location-aware services. A case study shows that a test case prioritization technique that incorporates service selection can outperform their traditional counterpart - the impact of service selection is noticeable on software engineering techniques in general and on test case prioritization techniques in particular. Further-more, we find that points-of-interest-aware techniques can be significantly more effective than input-guided techniques in terms of the number of invocations required to expose the first failure of a service composition.",,978-1-4244-8146-0978-0-7695-4128,10.1109/ICWS.2010.98,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5552784,test case prioritization;location-based web service;service-centric testing;service selection,Testing;Web services;Measurement;Entropy;Global Positioning System;Cities and towns;Sorting,mobile computing;program testing;program verification;Web services,service selection;location-based Web services;test case prioritization;dynamic service composition;verification;validation;service membership;service-centric testing;location-aware services;software engineering,,14,24,,,,,,IEEE,IEEE Conferences
Test Case Prioritization for Web Service Regression Testing,L. Chen; Z. Wang; L. Xu; H. Lu; B. Xu,NA; NA; NA; NA; NA,2010 Fifth IEEE International Symposium on Service Oriented System Engineering,,2010,,,173,178,"Regression testing is necessary to assure the quality of service-oriented business applications in their evolutions. However, because of the constraint of testing resource, entire test suite may not run as a result. Therefore, test case prioritization technique is required to increase the efficiency of Web service application regression testing. In this paper, we propose a dependence analysis based test case prioritization technique. First, we analyze the dependence relationship using control and data flow information in an orchestration language: WS-BPEL. Then we construct a weighted graph and do impact analysis to identify modification-affected elements. After that, we prioritize test cases according to covering more modification-affected elements with the highest weight. Finally we conduct a case study to illustrate the applicability of our method.",,978-1-4244-7326-7978-1-4244-7327-4978-0-7695-4081,10.1109/SOSE.2010.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5569910,Web service;regression testing;test case prioritization;impact analysis;dependence analysis,Testing;Web services;Complexity theory;Business;Flow graphs;Computational modeling;History,business data processing;quality assurance;quality of service;regression analysis;software architecture;Web services,Web service regression testing;quality of service;testing resource constraint;dependence analysis based test case prioritization technique;data flow information;orchestration language;WS-BPEL;weighted graph;modification-affected elements,,8,18,,,,,,IEEE,IEEE Conferences
The Effectiveness of Regression Testing Techniques in Reducing the Occurrence of Residual Defects,P. Nagahawatte; H. Do,NA; NA,"2010 Third International Conference on Software Testing, Verification and Validation",,2010,,,79,88,"Regression testing is a necessary maintenance activity that can ensure high quality of the modified software system, and a great deal of research on regression testing has been performed. Most of the studies performed to date, however, have evaluated regression testing techniques under the limited context, such as a short-term assessment, which do not fully account for system evolution or industrial circumstances. One important issue associated with a system lifetime view that we have overlooked in past years is the effects of residual defects - defects that persist undetected - across several releases of a system. Depending on an organization's business goals and the type of system being built, residual defects might affect the level of success of the software products. In this paper, we conducted an empirical study to investigate whether regression testing techniques are effective in reducing the occurrence and persistence of residual defects across a system's lifetime, in particular, considering test case prioritization techniques. Our results show that heuristics can be effective in reducing both the occurrence of residual defects and their age. Our results also indicate that residual defects and their age have a strong impact on the cost-benefits of test case prioritization techniques.",2159-4848,978-1-4244-6436-4978-1-4244-6435-7978-0-7695-3990,10.1109/ICST.2010.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5477096,Regression testing;test case prioritization;residual defects;empirical study,Testing,program testing;software maintenance;software quality,regression testing;residual defects;software maintenance;software quality;modified software system;short-term assessment;test case prioritization,,2,31,,,,,,IEEE,IEEE Conferences
The Effects of Time Constraints on Test Case Prioritization: A Series of Controlled Experiments,H. Do; S. Mirarab; L. Tahvildari; G. Rothermel,"North Dakota State University, Fargo; IBM, Vancouver; University of Waterloo, Waterloo; University of Nebraskaƒ??Lincoln, Lincoln",IEEE Transactions on Software Engineering,,2010,36,5,593,617,"Regression testing is an expensive process used to validate modified software. Test case prioritization techniques improve the cost-effectiveness of regression testing by ordering test cases such that those that are more important are run earlier in the testing process. Many prioritization techniques have been proposed and evidence shows that they can be beneficial. It has been suggested, however, that the time constraints that can be imposed on regression testing by various software development processes can strongly affect the behavior of prioritization techniques. If this is correct, a better understanding of the effects of time constraints could lead to improved prioritization techniques and improved maintenance and testing processes. We therefore conducted a series of experiments to assess the effects of time constraints on the costs and benefits of prioritization techniques. Our first experiment manipulates time constraint levels and shows that time constraints do play a significant role in determining both the cost-effectiveness of prioritization and the relative cost-benefit trade-offs among techniques. Our second experiment replicates the first experiment, controlling for several threats to validity including numbers of faults present, and shows that the results generalize to this wider context. Our third experiment manipulates the number of faults present in programs to examine the effects of faultiness levels on prioritization and shows that faultiness level affects the relative cost-effectiveness of prioritization techniques. Taken together, these results have several implications for test engineers wishing to cost-effectively regression test their software systems. These include suggestions about when and when not to prioritize, what techniques to employ, and how differences in testing processes may relate to prioritization cost--effectiveness.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2010.58,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5482587,Regression testing;test case prioritization;cost-benefits;Bayesian networks;empirical studies.,Time factors;Software testing;Automatic testing;Maintenance engineering;Programming;System testing;Computer Society;Software systems;Bayesian methods;Software quality,belief networks;program testing;regression analysis;software fault tolerance,time constraints;test case prioritization techniques;regression testing;various software development processes;cost-benefit trade-offs;Bayesian networks,,65,62,,,,,,IEEE,IEEE Journals & Magazines
Time Windows Based Dynamic Routing in Multi-AGV Systems,N. Smolic-Rocak; S. Bogdan; Z. Kovacic; T. Petrovic,NA; NA; NA; NA,IEEE Transactions on Automation Science and Engineering,,2010,7,1,151,155,"This paper presents a dynamic routing method for supervisory control of multiple automated guided vehicles (AGVs) that are traveling within a layout of a given warehouse. In dynamic routing a calculated path particularly depends on the number of currently active AGVs' missions and their priorities. In order to solve the shortest path problem dynamically, the proposed routing method uses time windows in a vector form. For each mission requested by the supervisor, predefined candidate paths are checked if they are feasible. The feasibility of a particular path is evaluated by insertion of appropriate time windows and by performing the windows overlapping tests. The use of time windows makes the algorithm apt for other scheduling and routing problems. Presented simulation results demonstrate efficiency of the proposed dynamic routing. The proposed method has been successfully implemented in the industrial environment in a form of a multiple AGV control system.",1545-5955;1558-3783,,10.1109/TASE.2009.2016350,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4907246,Automated guided vehicles;dynamic routing;path feasibility;time windows,Routing;Vehicle dynamics;Supervisory control;Vehicles;Shortest path problem;Performance evaluation;Testing;Scheduling algorithm;Job shop scheduling;Electrical equipment industry,automatic guided vehicles;integrated software;path planning,time windows based dynamic routing;multi AGV systems;supervisory control;automated guided vehicles;shortest path problem;vector form;predefined candidate paths;windows overlapping tests;industrial environment,,39,29,,,,,,IEEE,IEEE Journals & Magazines
"To Strengthen Security, Change Developers' Incentives",J. A. Halderman,University of Michigan,IEEE Security & Privacy,,2010,8,2,79,82,"Many of the most common software vulnerabilities, such as buffer overflows, cross-site scripting, and misapplications of cryptography, are wholly avoidable if software makers apply an appropriate level of training, testing, and care.Yet developers today have the ""wrong"" incentives, often leading them to underinvest in security or even to directly harm it. If we can understand these incentives and their causes, we might be able to reshape them and radically improve security.Software makers have shown a dramatic ability to strengthen their products' security given sufficient motivation.The most famous example is Microsoft's transformation over the past decade from a security laughingstock to a leader. In 2002, stung by several widely publicized vulnerabilities across its product line, the company began a major security initiative that produced lasting changes in its priorities, processes, and culture. Gone were the days of ""creating designs and code that emphasize features over security."" Yet changes like these are exceptional. Microsoft's shift was motivated by an intense level of scrutiny and withering global publicity that few firms experience, and it had the unusual luxury of responding with vast engineering resources paid for by monopoly rents. Most developers face far weaker security incentives.",1540-7993;1558-4046,,10.1109/MSP.2010.85,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5439535,security economics;developers' incentives;transparency;liability;security and privacy,Security;Buffer overflow;Cryptography;Software testing;Monopoly,cryptography;operating systems (computers),strengthen security;change developers incentives;software vulnerabilities;cross site scripting;cryptography misapplications;security improvement;software makers;products security;Microsoft transformation;publicized vulnerabilities;engineering resources,,1,5,,,,,,IEEE,IEEE Journals & Magazines
Using Coverage Information to Guide Test Case Selection in Adaptive Random Testing,Z. Q. Zhou,NA,2010 IEEE 34th Annual Computer Software and Applications Conference Workshops,,2010,,,208,213,"Random Testing (RT) is a fundamental software testing technique. Adaptive Random Testing (ART) improves the fault-detection capability of RT by employing the location information of previously executed test cases. Compared with RT, test cases generated in ART are more evenly spread across the input domain. ART has conventionally been applied to programs that have only numerical input types, because the distance between numerical inputs is readily measurable. The vast majority of computer programs, however, involve non-numerical inputs. To apply ART to these programs requires the development of effective new distance measures. Different from those measures that focus on the concrete values of program inputs, in this paper we propose a method to measure the distance using coverage information. The proposed method enables ART to be applied to all kinds of programs regardless of their input types. Empirical studies are further conducted for the branch coverage Manhattan distance measure using the replace and space programs. Experimental results show that, compared with RT, the proposed method significantly reduces the number of test cases required to detect the first failure. This method can be directly applied to prioritize regression test cases, and can also be incorporated into code-based and model-based test case generation tools.",,978-1-4244-8089-0978-0-7695-4105,10.1109/COMPSACW.2010.43,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5615787,software testing;adaptive random testing;adaptive random sequence;test case prioritization;distance measure,,complete computer programs;program testing;random sequences;software engineering,coverage information;test case selection;adaptive random testing;fault detection capability;location information;test case generation;ART;computer program;nonnumerical input;software testing technique;branch coverage Manhattan distance measure;space program;regression test case;code based test case generation;model based test case generation,,17,27,,,,,,IEEE,IEEE Conferences
Using Methods & Measures from Network Analysis for GUI Testing,E. Elsaka; W. E. Moustafa; B. Nguyen; A. Memon,NA; NA; NA; NA,"2010 Third International Conference on Software Testing, Verification, and Validation Workshops",,2010,,,240,246,"Graphical user interfaces (GUIs) for today's applications are extremely large. Moreover, they provide many degrees of freedom to the end-user, thus allowing the user to perform a very large number of event sequences on the GUI. The large sizes and degrees of freedom create severe problems for GUI quality assurance, including GUI testing. In this paper, we leverage methods and measures from network analysis to analyze and study GUIs, with the goal of aiding GUI testing activities. We apply these methods and measures on the event-flow graph model of GUIs. Results of a case study show that ""network centrality measures"" are able to identify the most important events in the GUI as well as the most important sequences of events. These events and sequences are good candidates for test prioritization. In addition, the ""betweenness clustering"" method is able to partition the GUI into regions that can be tested separately.",,978-1-4244-6774-7978-1-4244-6773-0978-0-7695-4050,10.1109/ICSTW.2010.61,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5463654,GUI testing;event-flow graphs;network analysis;test prioritization;software testing;network centrality;betweenness clustering,Graphical user interfaces;Delay;Application software;Java;Software testing;Humans;Automatic testing;Time measurement;Informatics;Smart phones,graphical user interfaces;program testing;software quality,network analysis;GUI testing;graphical user interfaces;event sequences;GUI quality assurance;event-flow graph model;network centrality measures;betweenness clustering method,,2,17,,,,,,IEEE,IEEE Conferences
Web services regression test case prioritization,B. Athira; P. Samuel,"Department of Computer Science, Cochin University of Science and Technology, India; Information Technology, School of Engineering, Cochin University of Science and Technology, India",2010 International Conference on Computer Information Systems and Industrial Management Applications (CISIM),,2010,,,438,443,"Web services and their underlying system grow over time and need to be retested whenever there is a change. This is essential for ensuring uncompromised quality. If we have modified only a small part of the system, it should be possible to reuse the existing test suite. Anyhow, for large modifications or for large systems, retesting the entire test suite will consume large amounts of time and computing resources. In this paper we propose a new method to prioritize test cases in web applications. Our test prioritization technique orders test cases in such a way that the most beneficial is executed first. Most of the existing test prioritization methods are based on the code of the system, but we propose a model-based test prioritization using activity diagram. Our technique identifies difference between original model and modified model. Using this information we plot activity paths for each test case and identify the most promising paths. The test case which covers these paths is considered as the most beneficial test cases. Our approach is effective in revealing the most promising regression test cases. We have applied our method on an online air ticket reservation system in which we could identify the most beneficial test cases from the existing ones.",,978-1-4244-7818-7978-1-4244-7817-0978-1-4244-7816,10.1109/CISIM.2010.5643499,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5643499,Web services;Test Prioritization;Activity Diagram;Activity paths,Unified modeling language;Biological system modeling;Business;Testing;Software;Computers;Information systems,model-based reasoning;program testing;regression analysis;software maintenance;Web services,Web service;test case prioritization;model based prioritization;regression test case;online air ticket reservation system,,5,18,,,,,,IEEE,IEEE Conferences
A clustering approach to improving test case prioritization: An industrial case study,R. Carlson; H. Do; A. Denton,"Microsoft, Fargo, ND, USA; Department of Computer Science, North Dakota State University, Fargo, USA; Department of Computer Science, North Dakota State University, Fargo, USA",2011 27th IEEE International Conference on Software Maintenance (ICSM),,2011,,,382,391,"Regression testing is an important activity for controlling the quality of a software product, but it accounts for a large proportion of the costs of software. We believe that an understanding of the underlying relationships in data about software systems, including data correlations and patterns, could provide information that would help improve regression testing techniques. We conjecture that if test cases have common properties, then test cases within the same group may have similar fault detection ability. As an initial approach to investigating the relationships in massive data in software repositories, in this paper, we consider a clustering approach to help improve test case prioritization. We implemented new prioritization techniques that incorporate a clustering approach and utilize code coverage, code complexity, and history data on real faults. To assess our approach, we have designed and conducted empirical studies using an industrial software product, Microsoft Dynamics Ax, which contains real faults. Our results show that test case prioritization that utilizes a clustering approach can improve the effectiveness of test case prioritization techniques.",1063-6773;1063-6773,978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662,10.1109/ICSM.2011.6080805,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080805,Regression testing;test case prioritization tecniques;clustering approach;industrial case study,Testing;Complexity theory;History;Fault detection;Measurement;Software systems,costing;pattern clustering;program testing;quality control;regression analysis;software fault tolerance;software quality;statistical testing,clustering approach;test case prioritization;industrial case study;regression testing;quality control;software cost;data correlation;fault detection;software repository;code coverage;code complexity;industrial software product;Microsoft Dynamics Ax,,22,26,,,,,,IEEE,IEEE Conferences
A Diagnostic Point of View for the Optimization of Preparation Costs in Runtime Testing,A. G. Sanchez; ??. Piel; H. Gross; A. J. C. van Gemund,NA; NA; NA; NA,"2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops",,2011,,,654,660,"Runtime testing is emerging as the solution for the validation and acceptance testing of service-oriented systems, where many services are external to the organization, and duplicating the system's components and their context is too complex, if possible at all. In order to perform runtime tests, an additional expense in the test preparation phase is required, both in software development and in hardware. Preparation cost prioritization methods have been based on runtime testability (i.e, coverage) and do not consider whether a good runtime testability is sufficient for a good runtime diagnosis quality in case faults are detected, and whether this diagnosis will be obtained efficiently (i.e., with a low number of test cases). In this paper we show (1) the direct relationship between testability and diagnosis quality, that (2) these two properties do not guarantee an efficient diagnosis, and (3) a measurement that ensures better prediction of efficiency.",,978-0-7695-4345-1978-1-4577-0019,10.1109/ICSTW.2011.21,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954478,runtime testing;diagnosis;preparation costs,Runtime;Testing;Correlation;Authentication;Databases;Sensitivity,costing;program testing;software development management,runtime testing;service-oriented systems;acceptance testing;preparation cost prioritization method;preparation cost optimization;runtime testability;software development,,,32,,,,,,IEEE,IEEE Conferences
A Fuzzy Logic Approach for Scheduling Preventive Maintenance in ERP System,R. H. Fouad; M. Samhouri,NA; NA,2011 International Conference on Management and Service Science,,2011,,,1,4,"Many firms have proceeded to the adoption of Enterprise Resources Planning ERP solutions to maintain competitiveness. ERP is a packaged software system that enables enterprises to integrate operations, business processes and functions through common database. However, the majority of ERP systems do not support Preventive Maintenance (PM) scheduling process. The objective of PM is to minimize equipment downtime using the limited resources of an organization. Therefore, prioritizing PM activities for equipment is essential. In this paper, a fuzzy logic-based system for PM scheduling is proposed to interpret the linguistic variables extracted from expert's knowledge for determining equipment priorities, which could be incorporated as a custom module in ERP systems. The system was tested and proved to be reliable in solving PM scheduling problem.",,978-1-4244-6581-1978-1-4244-6579-8978-1-4244-6580,10.1109/ICMSS.2011.5999330,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5999330,,Job shop scheduling;Preventive maintenance;Availability;Fuzzy logic,enterprise resource planning;fuzzy logic;fuzzy reasoning;preventive maintenance;scheduling;software packages,fuzzy logic approach;preventive maintenance scheduling;ERP system;enterprise resources planning;packaged software system;database;equipment downtime minimization;PM scheduling problem;fuzzy inference system,,,15,,,,,,IEEE,IEEE Conferences
A general purpose Ethernet based readout data acquisition system,B. Mindur; L. Jachymczyk,"Faculty of Physics and Applied Computer Science, AGH University of Science and Technology, al. Mickiewicza 30, 30-059 Krak??w, Poland; Faculty of Physics and Applied Computer Science, AGH University of Science and Technology, al. Mickiewicza 30, 30-059 Krak??w, Poland",2011 IEEE Nuclear Science Symposium Conference Record,,2011,,,800,806,"A flexible dedicated readout system is one of the most important part of any kind of dedicated detection system, especially for its testing phase as well as when the final system is ready for implementation. An obvious choice is to use a FPGA (apart from dedicated front-end electronics) as a first stage of data storage and processing element. Furthermore the FPGA has to prepare and transfer the incoming/processed data to the host PC. The implementation of the data exchange can be a problem, especially for small groups of developers, who have an option to buy a general solution with its limitations and a price, or to do time-consuming development of their own system practically from scratch. This paper presents a FPGA based general purpose readout solution which lies in between the two opposite approaches. Presented system uses a FPGA mezzanine board equipped with Ethernet Gigabit connection to PC. The FPGA FIFO based readout of a digital data stream is packed directly into the Ethernet frames and send to the destination PC using point-to point connection. The standard Ethernet frames are used in this design, additionally equipped with one byte carrying information on data type. When a high throughput is needed the data type is employed to prioritize them. This moderately simple but very powerful interface is relatively easy to be implemented in many applications [1]. The custom approach chosen for FPGA implementation causes a need to prepare dedicated software suite to process all incoming data in the PC side. The developed software package is called EPPRO (Ethernet Packet PROxy) since it exploits special Ether net frames for data exchange. The core part of EPPRO is a Linux kernel module, responsible for data reception/transmission and dispatching, taking into account their types to filter and prioritize the incoming packets. Overall performance of the whole system has been evaluated in respect to its throughput and reliability, presented test results confirm that all of the design goals have been fulfilled.",1082-3654;1082-3654,978-1-4673-0120-6978-1-4673-0118-3978-1-4673-0119,10.1109/NSSMIC.2011.6154542,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6154542,FPGA;Ethernet;network protocol;kernel driver;readout system,Field programmable gate arrays;Sockets;Indexes;Payloads;Clocks;Decoding;Random access memory,data acquisition;data communication;digital storage;electronic data interchange;field programmable gate arrays;flexible electronics;high energy physics instrumentation computing;Linux;local area networks;microcomputers;nuclear electronics;readout electronics;software packages,readout data acquisition system;detection system;FPGA;front-end electronics;data processing element;incoming data;data processing;data exchange;FPGA mezzanine board;ethernet gigabit connection;FPGA FIFO;digital data stream;PC;standard Ethernet frame;EPPRO software package;Linux kernel module;data reception;data transmission;data dispatching;incoming data packet,,5,8,,,,,,IEEE,IEEE Conferences
A genetic algorithm based approach for prioritization of test case scenarios in static testing,S. Sabharwal; R. Sibal; C. Sharma,"Department of computer Science and IT, Netaji Subhas Institute of Technology Delhi, India; Department of computer Science and IT, Netaji Subhas Institute of Technology Delhi, India; Department of computer Science and IT, Netaji Subhas Institute of Technology Delhi, India",2011 2nd International Conference on Computer and Communication Technology (ICCCT-2011),,2011,,,304,309,"White box testing is a test technique that takes into account program code, code structure and internal design flow. White box testing is primarily of two kinds-static and structural. Whereas static testing requires only the source code of the product, not the binaries or executables, in structural testing tests are actually run by the computer on built products. In this paper, we propose a technique for optimizing static testing efficiency by identifying the critical path clusters using genetic algorithm. The testing efficiency is optimized by applying the genetic algorithm on the test data. The test case scenarios are derived from the source code. The information flow metric is adopted in this work for calculating the information flow complexity associated with each node of the control flow graph generated from the source code. This research paper is an extension of our previous research paper [18].",,978-1-4577-1386-6978-1-4577-1385-9978-1-4577-1384,10.1109/ICCCT.2011.6075160,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6075160,software testing;genetic algorithm;Information flow metric;CFG,Testing;Biological cells;Genetic algorithms;Complexity theory;Computers;Measurement;Communications technology,flow graphs;genetic algorithms;program testing;software metrics,genetic algorithm;test case scenario prioritization;white box testing;program code;code structure;internal design flow;source code;structural testing;static testing efficiency optimisation;information flow metric;information flow complexity;control flow graph,,6,18,,,,,,IEEE,IEEE Conferences
"A low-cost distributed instrumentation system for monitoring, identifying and diagnosing irregular patterns of behavior in critical ITS components",E. Vorakitolan; N. Mould; J. P. Havlicek; R. D. Barnes; A. R. Stevenson,"University of Oklahoma School of Electrical and Computer Engineering, 110 W. Boyd Street, Devon Energy Hall 150, Norman, OK, USA 73019-1102; University of Oklahoma School of Electrical and Computer Engineering, 110 W. Boyd Street, Devon Energy Hall 150, Norman, OK, USA 73019-1102; University of Oklahoma School of Electrical and Computer Engineering, 110 W. Boyd Street, Devon Energy Hall 150, Norman, OK, USA 73019-1102; University of Oklahoma School of Electrical and Computer Engineering, 110 W. Boyd Street, Devon Energy Hall 150, Norman, OK, USA 73019-1102; Oklahoma Department of Transportation, Technology Services Division, Oklahoma City, OK, USA 73105",2011 14th International IEEE Conference on Intelligent Transportation Systems (ITSC),,2011,,,2045,2050,"ITS telecommunication infrastructure and information gathering/distribution equipment such as fiber-optic routing devices, communication huts, uninterrupted power supplies (UPSs), cameras and dynamic message signs (DMSs) are critical to the effective management of resources in emergency situations. Frequently encountered scenarios include evaluating the severity of vehicle collisions to dispatch appropriate law enforcement and ambulatory services, or issuing time-sensitive AMBER alerts to assist in the effort to find missing or kidnapped children. Due to the unacceptably high cost of ITS equipment failures, preventative maintenance and dense operational testing are high priorities. In this paper we present a low-cost distributed instrumentation system (DIS) for continuous monitoring of critical ITS components. Over the last three years, we have developed and deployed the DIS in the Oklahoma Department of Transportation (ODOT) private fiberoptic network that spans several major metropolitan areas in and around Oklahoma City, Tulsa and Lawton. The Oklahoma DIS is responsible for monitoring the health of the entire ODOT ITS communications network as well as the integrity of each camera video signal and the operational status of each DMS. All of the information acquired by the DIS is integrated into an operational summary that is available on a private website for the design and execution of ITS equipment maintenance plans. In Oklahoma, information acquired by the DIS has been successfully integrated into a wide range of operation and maintenance (O&M) planning, which has led to a significant improvement in terms of overall ITS quality of service (QoS) and a quantifiable reduction in wasted costs associated with the premature discarding of energy storage devices.",2153-0017;2153-0009;2153-0009,978-1-4577-2197-7978-1-4577-2198-4978-1-4577-2196,10.1109/ITSC.2011.6082868,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6082868,,Instruments;Monitoring;Maintenance engineering;Batteries;Temperature sensors;Software;Resource management,automated highways;computer network management;computerised monitoring;maintenance engineering;optical fibre networks;telecommunication equipment;telecommunication network planning,low cost distributed instrumentation system;irregular pattern monitoring;irregular pattern identification;irregular pattern diagnostics;critical ITS components;ITS telecommunication infrastructure;information gathering equipment;information distribution equipment;emergency situation;Oklahoma Department of Transportation;private fiber optic network;metropolitan area;operation planning;maintenance planning;ITS quality of service;QoS;intelligent transportation system,,,16,,,,,,IEEE,IEEE Conferences
A model based prioritization technique for component based software retesting using uml state chart diagram,S. Mohanty; A. A. Acharya; D. P. Mohapatra,"School of Computer Engineering, KIIT University, Bhubaneswar, India; School of Computer Engineering, KIIT University, Bhubaneswar, India; Department of Computer Science, &amp; Engineering National Institute of Technology, Rourkela, India",2011 3rd International Conference on Electronics Computer Technology,,2011,2,,364,368,"Regression testing is the process of testing a modified system using the old test suite. As the test suite size is large, system retesting consumes large amount of time and computing resources. This issue of retesting of software systems can be handled using a good test case prioritization technique. A prioritization technique schedules the test cases for execution so that the test cases with higher priority executed before lower priority. The objective of test case prioritization is to detect fault as early as possible so that the debuggers can begin their work earlier. In this paper we propose a new prioritization technique to prioritize the test cases to perform regression testing for Component Based Software System (CBSS). The components and the state changes for a component based software systems are being represented by UML state chart diagrams which are then converted into Component Interaction Graph (CIG) to describe the interrelation among components. Our prioritization algorithm takes this CIG as input along with the old test cases and generates a prioritized test suit taking into account total number of state changes and total number of database access, both direct and indirect, encountered due to each test case. Our algorithm is found to be very effective in maximizing the objective function and minimizing the cost of system retesting when applied to few JAVA projects.",,978-1-4244-8679-3978-1-4244-8678,10.1109/ICECTECH.2011.5941719,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941719,regression testing;software components;state chart diagram;CBSS;CIG,Databases;Testing;Software systems;Fault detection;Unified modeling language;Computational modeling,object-oriented programming;program testing;regression analysis;software fault tolerance;Unified Modeling Language,model based prioritization technique;component based software retesting;UML state chart diagram;regression testing;test case prioritization technique;fault detection;component interaction graph;objective function;JAVA,,1,7,,,,,,IEEE,IEEE Conferences
A software safety test approach based on FTA and Bayesian networks,X. He; X. Tao,"Dept. of Quality Engineering, China Aero-Polytechnology Establishment, Aviation Industry Corporation of China, Beijing, China; Dept. of Aerospace Application, Academy of Opto-Electronics, Chinese Academy of Sciences, Beijing, China",2011 Prognostics and System Health Managment Confernece,,2011,,,1,5,"As an important way to verify software safety, software safety test has caught more attentions in practice. However, it is still an open question that how engineers could make software safety test more efficient. Currently, FTA based method is one of the approaches in software safety test, but it can not utilize the finished software test results, and can not be determined the priorities of all the use cases. In order to solve these problems, this paper gives a quantitative approach of software safety test based on FTA and Bayesian networks. In the approach, top-level events of fault trees are identified from system hazards firstly. Then, fault trees are built using FTA and transferred into Bayesian networks. Finally, test cases of software safety test are determined by the Bayesian networks. Besides, the paper also shows an example using the approach, which could guide software engineers to make software safety test more efficient. The example shows that the approach could take advantage of Bayesian Theorem and FTA methodology together, and give reasonable priorities of use cases in software safety test.",2166-563X;2166-5656,978-1-4244-7950-4978-1-4244-7951-1978-1-4244-7949,10.1109/PHM.2011.5939497,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5939497,software safety test;FTA;Bayesian networks,Hazards;Bayesian methods;Software algorithms;Software,Bayes methods;belief networks;fault trees;program testing;program verification;software engineering,software safety test;FTA network;Bayesian network;top level event;fault trees;system hazard;software engineers,,1,15,,,,,,IEEE,IEEE Conferences
A statistical approach to TPS transport optimization,J. L. Orlet; G. L. Murdock,"The Boeing Company, Ground Support Systems, St. Louis, MO, USA; The Boeing Company, Ground Support Systems, St. Louis, MO, USA",2011 IEEE AUTOTESTCON,,2011,,,66,69,"This paper discusses the statistical challenges of TPS Transport. A TPS is not considered fieldable until all tests are passing. Based on the number of tests in a TPS and the inherent complexity of the transport process, the probability exists that less than 100% of all tests will pass the first time they are tested after undergoing the transport process. Optimizing what types of tests are transported significantly improves the probability that the next TPS will be successfully transported. This paper illustrates that prioritizing which functions to focus on can greatly improve the probability of success while reducing the overall characterization effort.",1558-4550;1088-7725;1088-7725,978-1-4244-9363-0978-1-4244-9362-3978-1-4244-9361,10.1109/AUTEST.2011.6058752,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6058752,TPS;Transport;Optimization,Hardware;Testing;Runtime;Power supplies;Schedules;Emulation;Voltage measurement,automatic test equipment;automatic testing;optimisation;probability;software maintenance;statistical analysis,statistical approach;TPS transport optimization;transport process;probability;legacy system,,,3,,,,,,IEEE,IEEE Conferences
A Test Case Design Algorithm Based on Priority Techniques,H. Xian,NA,2011 Fifth International Conference on Management of e-Commerce and e-Government,,2011,,,57,62,"Testing is an important step of building e-commerce system. In regression testing, it is the key issue that how to reuse the test suite efficiently. This paper presents a dynamic adjustment prioritization based on the design information of test suite which is a new exploration of regression test prioritization. It improves the shortcoming of the existing technologies which failed to use the design information of test cases effectively. It adjusts the priority of test case by collecting running information, gradually optimizes the test suite and makes the test suite adapted to the current test environment to obtain a better error detection results. Experiments show the error-detected efficiency of it has certain advantages than the existing algorithms.",,978-1-4577-1659,10.1109/ICMeCG.2011.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6092632,test case;algorithm;priority techniques,Testing;Algorithm design and analysis;Heuristic algorithms;Complexity theory;History;Software algorithms;Correlation,electronic commerce;program testing,test case design algorithm;e-commerce system;regression testing,,,16,,,,,,IEEE,IEEE Conferences
A tool for combination-based prioritization and reduction of user-session-based test suites,S. Sampath; R. C. Bryce; S. Jain; S. Manchester,"Information Systems, UMBC, Baltimore, MD 21250, USA; Computer Science, Utah State University, Logan, 84341, USA; Information Systems, UMBC, Baltimore, MD 21250, USA; Computer Science, Utah State University, Logan, 84341, USA",2011 27th IEEE International Conference on Software Maintenance (ICSM),,2011,,,574,577,"Test suite prioritization and reduction are two approaches to managing large test suites. They play an important role in regression testing, where a large number of tests accumulate over time from previous versions of the system. Accumulation of tests is exacerbated in user-session-based testing of web applications, where field usage data is continually logged and converted into test cases. This paper presents a tool that allows testers to easily collect, prioritize, and reduce user-session-based test cases. Our tool provides four contributions: (1) guidance to users on how to configure their web server to log important usage information, (2) automated parsing of web logs into XML formatted test cases that can be used by test replay tools, (3) automated prioritization of test cases by length-based and combinatorial-based criteria, and (4) automated reduction of test cases by combinatorial coverage.",1063-6773;1063-6773,978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662,10.1109/ICSM.2011.6080833,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080833,,XML;Web servers;Educational institutions;Databases;Engines;Software testing,combinatorial mathematics;Internet;program testing;regression analysis;XML,combinatorial-based prioritization;user-session-based test suite;test suite prioritization;test suite reduction;regression testing;Web application;field usage data;user-session-based test cases;Web server;Web log;XML formatted test case;automated prioritization;length-based criteria;combinatorial-based criteria;combinatorial coverage,,6,11,,,,,,IEEE,IEEE Conferences
A Workflow Scheduling Algorithm for Optimizing Energy-Efficient Grid Resources Usage,F. Coutinho; L. A. V. d. Carvalho; R. Santana,NA; NA; NA,"2011 IEEE Ninth International Conference on Dependable, Autonomic and Secure Computing",,2011,,,642,649,"Grid computing represents the main solution to integrate distributed and heterogeneous resources in global scale. However, the infrastructure necessary for maintaining a global grid in production is huge. Such fact has led to excessive power consumption. On the other hand, most green strategies for data centers are DVS (Dynamic Voltage Scaling)-based and become difficult to implement them in global grids. This paper proposes the HGreen heuristic (Heavier Tasks on Maximum Green Resource) and defines a workflow scheduling algorithm in order to implement it on global grids. HGreen algorithm aims to prioritize energy-efficient resources and explores workflow application profiles. Simulation results have shown that the proposed algorithm can significantly reduce the power consumption in global grids.",,978-1-4673-0006-3978-0-7695-4612,10.1109/DASC.2011.115,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6119053,Grid computing;scheduling;scientific workflow;green computing,Green products;Energy efficiency;Air pollution;Benchmark testing;Algorithm design and analysis;Scheduling algorithm;Educational institutions,computer centres;grid computing;optimisation;power aware computing;scheduling;workflow management software,workflow scheduling algorithm;energy-efficient grid resource usage optimization;heterogeneous resource;global grid computing;power consumption;data center;dynamic voltage scaling;HGreen heuristic;HGreen algorithm,,7,40,,,,,,IEEE,IEEE Conferences
Adaptive Regression Testing Strategy: An Empirical Study,M. J. Arafeen; H. Do,NA; NA,2011 IEEE 22nd International Symposium on Software Reliability Engineering,,2011,,,130,139,"When software systems evolve, different amounts and types of code modifications can be involved in different versions. These factors can affect the costs and benefits of regression testing techniques in different ways, and thus, there may be no single regression testing technique that is the most cost-effective technique to use on every version. To date, many regression testing techniques have been proposed, but no research has been done on the problem of helping practitioners systematically choose appropriate techniques on new versions as systems evolve. To address this problem, we propose adaptive regression testing (ART) strategies that attempt to identify the regression testing techniques that will be the most cost-effective for each regression testing session considering organization's situations and testing environment. To assess our approach, we conducted an experiment focusing on test case prioritization techniques. Our results show that prioritization techniques selected by our approach can be more cost-effective than those used by the control approaches.",2332-6549;1071-9458;1071-9458,978-1-4577-2060-4978-0-7695-4568,10.1109/ISSRE.2011.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132961,Regression testing;adaptive regression testing strategy;test case prioritization;Analytical Hierarchy Process,Testing;Software;Mathematical model;Equations;Subspace constraints;Java;Fault detection,program testing;regression analysis;software process improvement,adaptive regression testing strategy;software systems;code modifications;cost-effective technique;ART strategy;test case prioritization technique,,5,34,,,,,,IEEE,IEEE Conferences
Agent-Based Test Case Prioritization,C. Malz; P. G??hner,NA; NA,"2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops",,2011,,,149,152,In this paper an Adaptive Test Management System (ATMS) based on software agents is presented which prioritizes test cases considering available information from test teams and from developments teams about the software system and the test cases. The goal of the ATMS is to increase the number of found faults in the available test time with the determined prioritization order.,,978-0-7695-4345-1978-1-4577-0019,10.1109/ICSTW.2011.81,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954404,test case prioritization;software agents;fuzzy logic,Asynchronous transfer mode;Software agents;Databases;Software systems;Computer architecture;Fuzzy logic,program testing;software agents,agent-based test case prioritization;adaptive test management system;software agents,,6,5,,,,,,IEEE,IEEE Conferences
An Empirical Study on the Relation between Dependency Neighborhoods and Failures,T. Zimmerman; N. Nagappan; K. Herzig; R. Premraj; L. Williams,NA; NA; NA; NA; NA,"2011 Fourth IEEE International Conference on Software Testing, Verification and Validation",,2011,,,347,356,"Changing source code in large software systems is complex and requires a good understanding of dependencies between software components. Modification to components with little regard to dependencies may have an adverse impact on the quality of the latter, i.e., increase their risk to fail. We conduct an empirical study to understand the relationship between the quality of components and the characteristics of their dependencies such as their frequency of change, their complexity, number of past failures and the like. Our study has been conducted on two large software systems: Microsoft VISTA and ECLIPSE. Our results show that components that have outgoing dependencies to components with higher object-oriented complexity tend to have fewer field failures for VISTA, but the opposite relation holds for ECLIPSE. Likewise, other notable observations have been made through our study that (a) confirm that certain characteristics of components increase the risk of their dependencies to fail and (b) some of the characteristics are project specific while some were also found to be common. We expect that such results can be leveraged for use to provide new directions for research in defect prediction, test prioritization and related research fields that utilize code dependencies in their empirical analysis. Additionally, these results provide insights to engineers on the potential reliability impacts of new component dependencies based upon the characteristics of the component.",2159-4848,978-1-61284-174-8978-0-7695-4342,10.1109/ICST.2011.39,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770624,software quality;defects;dependency;empirical software engineering,Measurement;Complexity theory;Software systems;Servers;Couplings;Java,object-oriented programming;program testing;software quality;systems analysis,dependency neighborhood;source code changing;software component;component quality;Microsoft VISTA;ECLIPSE;object-oriented complexity;field failure;defect prediction;test prioritization;code dependency;empirical analysis,,11,26,,,,,,IEEE,IEEE Conferences
An empirical validation of FindBugs issues related to defects,A. Vetro; M. Morisio; M. Torchiano,Politecnico di Torino; Politecnico di Torino; Politecnico di Torino,15th Annual Conference on Evaluation & Assessment in Software Engineering (EASE 2011),,2011,,,144,153,"Background: Effective use of bug finding tools promise to speed up the process of source code verification and to move a portion of discovered defects from testing to coding phase. However, many problems related to their usage, especially the large number of false positives, could easily hinder the potential benefits of such tools. Aims: Assess the percentage and type of issues of a popular bugfinding tool (FindBugs) that are actual defects. Method: We analyzed 301 Java Projects developed at a university with FindBugs, collecting the issues signalled on the source code. Afterwards, we checked the precision of issues with information on changes, we ranked and validated them using both manual inspection and validation with tests failures. Results: We observed that a limited set of issues have high precision and conversely we identified those issues characterized by low precision. We compared findings first with our previous experiment and then to related work: results are consistent with both of them. Conclusions: Since our and other empirical studies demonstrated that few issues are related to real defects with high precision, developers could enable only them (or prioritize), reducing the information overload of FindBugs and having the possibility to discover defects earlier. Furthermore, the technique presented in the paper can be adopted to other tools on a code base with tests to find issues with high precision that can be checked on code in production to find defects earlier.",,978-1-84919-509,10.1049/ic.2011.0018,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6083173,,,data flow analysis;Java;program debugging;program verification;software tools,FindBugs issues;bug finding tools;source code verification;coding phase;Java projects,,6,,,,,,,IET,IET Conferences
An Improved Metric for Test Case Prioritization,X. Zhang; B. Qu,NA; NA,2011 Eighth Web Information Systems and Applications Conference,,2011,,,125,130,"Test case prioritization is an effective and practical technique of regression testing. To illustrate its effectiveness, many test metrics were proposed. In this paper, the physical meanings of these metrics were explained and their limitations were pointed out. Then, an improved metric and its extension for test case prioritization were proposed. The case study indicates that, compared with existing metrics, our new metric can provide much more precise illustration of the effectiveness of test case prioritization techniques.",,978-1-4577-1812,10.1109/WISA.2011.31,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6093578,Software testing;Regression testing;Test case prioritization;Metrics,Information systems,program testing;regression analysis;software metrics,test case prioritization;regression testing;test metrics,,1,9,,,,,,IEEE,IEEE Conferences
Black box test case prioritization techniques for semantic based composite web services using OWL-S,A. Askarunisa; K. A. J. Punitha; A. M. Abirami,"Department of Computer Science, Thiagarajar College of Engineering, Madurai, India; Department of Computer Science, Thiagarajar College of Engineering, Madurai, India; Thiagarajar College of Engineering Madurai, Tamilnadu, India",2011 International Conference on Recent Trends in Information Technology (ICRTIT),,2011,,,1215,1220,"Web services are the basic building blocks for the business which is different from web applications. Testing of web services is difficult and increases the cost due to the unavailability of source code. Researchers have, web services are tested based on the syntactic structure using Web Service Description Language (WSDL) for atomic web services. This paper proposes an automated testing framework for composite web services based on semantics where the domain knowledge of the web services is described using prot??g?? tool and the behaviour of the entire business operation flow for the composite web service is described by Ontology Web Language for services (OWL-S). Prioritization of test cases is performed based on various coverage criteria for composite web services. Series of experiments were conducted to assess the effectiveness of prioritization and empirical results shown that prioritization techniques perform well in detecting faults compared to traditional techniques.",,978-1-4577-0590-8978-1-4577-0588-5978-1-4577-0589,10.1109/ICRTIT.2011.5972354,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5972354,Average percent of Faults Detected;Composite web services;Harmonic Mean of TF;Harmonic Mean of Sequence Invocation;Ontology Web Language for services;prot??g??;Test Case Prioritization,Web services;Ontologies;Testing;Semantics;Fault detection;XML;Unified modeling language,automatic test software;knowledge representation languages;semantic Web;Web services,black box test case prioritization technique;semantic based composite Web services;OWL-S;source code;syntactic structure;Web service description language;atomic Web services;automated testing framework;domain knowledge;prot??g?? tool;business operation flow;ontology web language for service,,7,15,,,,,,IEEE,IEEE Conferences
Calculating Prioritized Interaction Test Sets with Constraints Using Binary Decision Diagrams,E. Salecker; R. Reicherdt; S. Glesner,NA; NA; NA,"2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops",,2011,,,278,285,"Combinatorial interaction testing has become an established technique to systematically determine test sets for highly-configurable software systems. The generation of minimal test sets that fullfill the demanded coverage criteria is an NP-complete problem. Constraint handling and integrated test case prioritization, features necessary for practical use, further complicate the problem. We present a novel algorithm that exploits our observation that the combinatorial interaction testing problem with constraints can be modelled as a single propositional logic formula. Our test set calculation algorithm uses binary decision diagrams as efficient data structure for this formula. The algorithm supports constraints and prioritization. Our evaluation results prove its cost effectiveness. For many benchmark problems the algorithm calculates the best results compared to other greedy approaches.",,978-0-7695-4345-1978-1-4577-0019,10.1109/ICSTW.2011.79,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954420,combinatorial interaction testing;constraints;priorities;binary decision diagrams,Data structures;Boolean functions;Calibration;Testing;Color;Keyboards;Transforms,binary decision diagrams;computational complexity;formal logic;program testing,interaction test set;binary decision diagram;combinatorial interaction testing;highly-configurable software system;NP-complete problem;constraint handling;integrated test case prioritization;propositional logic formula,,5,16,,,,,,IEEE,IEEE Conferences
Challenges in Audit Testing of Web Services,C. D. Nguyen; A. Marchetto; P. Tonella,NA; NA; NA,"2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops",,2011,,,103,106,"Audit testing of Web services is a form of regression testing that applies to the integration of external services in a service composition. When such services change, the application that integrates them needs to be regressiontested, to ensure that the existing functionalities are not broken by the service changes. This means the external services must be audited for preserved compatibility with the integration. In this paper, we analyze the main research challenges behind audit testing of services. Such challenges descend from the limited testability of external services and encompass traditional regression testing problems, such as test case selection and prioritization, for which existing solutions are nonetheless inadequate. Moreover, in the paper we discuss in detail one such challenge, i.e., the problem of defining an oracle by making the integrator's assumptions explicit.",,978-0-7695-4345-1978-1-4577-0019,10.1109/ICSTW.2011.104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954397,Webservice Composition;Audit Testing;Regression Testing,Testing;Minimization;Conferences;Service oriented architecture;Protocols;Business,program testing;Web services,audit testing;Web services;regression testing;service composition;test case selection;test case prioritization,,1,8,,,,,,IEEE,IEEE Conferences
"Challenges, benefits and opportunities in operating cabled ocean observatories: Perspectives from NEPTUNE Canada",C. R. Barnes; M. M. R. Best; F. R. Johnson; L. Pautet; B. Pirenne,"NEPTUNE Canada, University of Victoria, PO Box 1700 STN CSC, Victoria British Columbia, V8W2Y2 Canada; NEPTUNE Canada, University of Victoria, PO Box 1700 STN CSC, Victoria British Columbia, V8W2Y2 Canada; NEPTUNE Canada, University of Victoria, PO Box 1700 STN CSC, Victoria British Columbia, V8W2Y2 Canada; NEPTUNE Canada, University of Victoria, PO Box 1700 STN CSC, Victoria British Columbia, V8W2Y2 Canada; NEPTUNE Canada, University of Victoria, PO Box 1700 STN CSC, Victoria British Columbia, V8W2Y2 Canada",2011 IEEE Symposium on Underwater Technology and Workshop on Scientific Use of Submarine Cables and Related Technologies,,2011,,,1,7,"The advent of the first cabled ocean observatories, with several others being planned, demonstrates the challenges, benefits and opportunities for ocean science and commercial applications. Examples are drawn primarily from NEPTUNE Canada (NC), which completed installation of the subsea infrastructure and 60 diverse instruments in 2009, with 40 more in 2010, thereby establishing the world's first regional cabled ocean observatory, northeast Pacific Ocean, off British Columbia's coast. Initial data flow started in December 2009. Another 30 instruments will be deployed in 2011-12. Introducing abundant power and high bandwidth communications into a range of ocean environments allows discrimination between short and long-term events, interactive experiments, real time data and imagery, and complex multidisciplinary teams interrogating a vast database over the observatory's 25-year design life. Scientific priorities and observatory node sites were identified through workshops. Alcatel-Lucent Submarine Networks designed, manufactured and installed the 800km backbone cable and five nodes (stepping 10kV DC to 400V DC). Node sites are located at the coast (Folger Passage), continental slope (ODP 889; Barkley Canyon), abyssal plain (ODP 1027), and ocean-spreading ridge (Endeavour), in water depths of 100-2660m. Principal scientific themes are: plate tectonic processes and earthquake dynamics; dynamic processes of seabed fluid fluxes and gas hydrates; regional ocean/climate dynamics and effects on marine biota; deep-sea ecosystem dynamics; and engineering and computational research. The Data Management and Archive System (DMAS) provides controls for the observatory network and transparent access to other data providers using interoperability techniques within a Web 2.0 environment. Users can perform data visualization and analysis on-line with either default or custom processing code, as well as simultaneously interacting with each other. Oceans 2.0 is adding tools to perform software-aided feature detection and classification of sounds in acoustic data streams. New knowledge and scientific interpretations are addressing important science applications of the observatory: ocean/climate change, ocean acidification, recognizing and mitigating natural hazards, non-renewable and renewable natural resources. Challenges are considerable: technical innovations, enlarging the user base, management, funding, maximizing educational/outreach activities. Socio-economic benefits are substantial: not only the transformation of ocean sciences but with many applications in sectors such as sovereignty, security, transportation, data services, and public policy. Opportunities for commercialization of technologies and data services/products are being facilitated by the Centre of Enterprise and Engagement (www.onccee.ca) within Ocean Networks Canada (www.networkscanada.ca) that manages the NC and VENUS observatories (www.neptunecanada.ca; www.uvic.venus.ca). Cabled ocean observatories are transforming the ocean sciences and will result in a progressive wiring of the oceans. They are designed to be expandable in footprint, nodes and instruments, and the range of scientific questions, and to provide facilities for testing technology prototypes. They will provide a wealth of new research opportunities and socio-economic benefits.",,978-1-4577-0164-1978-1-4577-0165-8978-1-4577-0163,10.1109/UT.2011.5774134,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5774134,,Oceans;Observatories;Instruments;Real time systems;Venus;Optical fiber cables;Monitoring,geophysics computing;open systems;submarine cables,cabled ocean observatories;NEPTUNE Canada;ocean science;subsea infrastructure;high bandwidth communications;backbone cable;continental slope;abyssal plain;ocean-spreading ridge;plate tectonic processes;earthquake dynamics;seabed fluid fluxes;gas hydrates;regional ocean dynamics;climate dynamics;data management and archive system;interoperability;Web 2.0;Oceans 2.0;size 800 km;depth 100 m to 2660 m,,1,5,,,,,,IEEE,IEEE Conferences
Change Sensitivity Based Prioritization for Audit Testing of Webservice Compositions,C. D. Nguyen; A. Marchetto; P. Tonella,NA; NA; NA,"2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops",,2011,,,357,365,"Modern software systems have often the form of Web service compositions. They take advantage of the availability of a variety of external Web services to provide rich and complex functionalities, obtained as the integration of external services. However, Web services change at a fast pace and while syntactic changes are easily detected as interface incompatibilities, other more subtle changes are harder to detect and may give raise to faults. They occur when the interface is compatible with the composition, but the semantics of the service response has changed. This typically involves undocumented or implicit aspects of the service interface. Audit testing of services is the process by which the service integrator makes sure that the service composition continues to work properly with the new versions of the integrated services. Audit testing of services is conducted under strict (sometimes extreme) time and budget constraints. Hence, prioritizing the audit test cases so as to execute the most important ones first becomes of fundamental importance. We propose a test case prioritization method specifically tailored for audit testing of services. Our method is based on the idea that the most important test cases are those that have the highest sensitivity to changes injected into the service responses (mutations). In particular, we consider only changes that do not violate the explicit contract with the service (i.e., the WSDL), but may violate the implicit assumptions made by the service integrator.",,978-0-7695-4345-1978-1-4577-0019,10.1109/ICSTW.2011.50,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954434,Audit Testing;Change Sensitivity;Test Prioritization;Webservice Composition,Testing;Sensitivity;XML;Web services;Monitoring;Semantics;Syntactics,program testing;Web services,change sensitivity based prioritization;audit testing;Web service composition;software system;test case prioritization method;WSDL;service integrator;service interface,,4,24,,,,,,IEEE,IEEE Conferences
Code Hot Spot: A tool for extraction and analysis of code change history,W. Snipes; B. Robinson; E. Murphy-Hill,"Industrial Software Systems, ABB Corporate Research, Raleigh, NC USA; Industrial Software Systems, ABB Corporate Research, Raleigh, NC USA; North Carolina State University, Department of Computer Science, Raleigh, USA",2011 27th IEEE International Conference on Software Maintenance (ICSM),,2011,,,392,401,"Commercial software development teams have limited time available to focus on improvements to their software. These teams need a way to quickly identify areas of the source code that would benefit from improvement, as well as quantifiable data to defend the selected improvements to management. Past research has shown that mining configuration management systems for change information can be useful in determining faulty areas of the code. We present a tool named Code Hot Spot, which mines change records out of Microsoft's TFS configuration management system and creates a report of hot spots. Hot spots are contiguous areas of the code that have higher values of metrics that are indicators of faulty code. We present a study where we use this tool to study projects at ABB to determine areas that need improvement. The resulting data have been used to prioritize areas for additional code reviews and unit testing, as well as identifying change prone areas in need of refactoring.",1063-6773;1063-6773,978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662,10.1109/ICSM.2011.6080806,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080806,Decision Support;Metrics;Refactoring;Verification;Quality;Configuration Management,Measurement;History;Complexity theory;Data mining;Correlation;Software;Couplings,data flow analysis;software maintenance;software management;software metrics;software tools,code change history extraction tool;code change history analysis tool;commercial software development teams;source code;software management;configuration management system mining;change information;Code Hot Spot;Microsoft TFS configuration management system;metrics;faulty code;ABB,,,18,,,,,,IEEE,IEEE Conferences
Compiling SyncCharts to Synchronous C,C. Traulsen; T. Amende; R. von Hanxleden,"Department of Computer Science, Christian-Albrechts-Universit&#x00E4;t zu Kiel; Department of Computer Science, Christian-Albrechts-Universit&#x00E4;t zu Kiel; Department of Computer Science, Christian-Albrechts-Universit&#x00E4;t zu Kiel","2011 Design, Automation & Test in Europe",,2011,,,1,4,"SyncCharts are a synchronous Statechart variant to model reactive systems with a precise and deterministic semantics. The simulation and software synthesis for SyncCharts usually involve the compilation into Esterel, which is then further compiled into C code. This can produce efficient code, but has two principal drawbacks: 1) the arbitrary control flow that can be expressed with SyncChart transitions cannot be mapped directly to Esterel, and 2) it is very difficult to map the resulting C code back to the original SyncChart, which hampers traceability. This paper presents an alternative software synthesis approach for SyncCharts that compiles SyncCharts directly into Synchronous C (SC). The compilation preserves the structure of the original SyncChart, which is advantageous for validation and possibly certification. We present a static thread-scheduling scheme that reflects data dependencies and optimizes both the number of used threads as well as the maximal used priorities. This results in SC code with competitive speed and little memory requirements.",1558-1101;1530-1591;1530-1591,978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7,10.1109/DATE.2011.5763284,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763284,,Semantics;Computational modeling;Instruction sets;Writing;Concurrent computing;Schedules;Optimization,flowcharting;multi-threading;processor scheduling;program compilers;synchronisation,SyncChart compiling;Synchronous C;synchronous statechart variant;reactive systems;deterministic semantics;software synthesis;Esterel;C code;arbitrary control flow;SyncChart transition;static thread-scheduling scheme,,2,10,,,,,,IEEE,IEEE Conferences
Cost Optimizations in Runtime Testing and Diagnosis of Systems of Systems,A. Gonzalez-Sanchez,NA,"2011 Fourth IEEE International Conference on Software Testing, Verification and Validation",,2011,,,439,442,"In practically all development processes tests are used to detect the presence of faults. This is not an exception for critical and high-availability systems. However, these systems cannot be taken offline or duplicated for testing in some cases. This makes runtime testing necessary. This paper presents work aimed at optimizing the three main sources of testing cost: preparation, execution and diagnosis. First, preparation cost is optimized by defining a metric of the runtime testability of the system, used to elaborate an implementation plan of preparative work for runtime testing. Second, the interrelated nature of test execution cost and diagnostic cost is highlighted and a new diagnostic test prioritization is introduced.",2159-4848,978-1-61284-174-8978-0-7695-4342,10.1109/ICST.2011.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770638,runtime testing;diagnostic prioritization,Software testing;Conferences,program diagnostics;program testing,cost optimizations;runtime testing;runtime diagnosis;systems of systems;testing cost;preparation;runtime testability;implementation plan;test execution cost;diagnostic cost;diagnostic test prioritization,,,19,,,,,,IEEE,IEEE Conferences
"CRANE: Failure Prediction, Change Analysis and Test Prioritization in Practice -- Experiences from Windows",J. Czerwonka; R. Das; N. Nagappan; A. Tarvo; A. Teterev,NA; NA; NA; NA; NA,"2011 Fourth IEEE International Conference on Software Testing, Verification and Validation",,2011,,,357,366,"Building large software systems is difficult. Maintaining large systems is equally hard. Making post-release changes requires not only thorough understanding of the architecture of a software component about to be changed but also its dependencies and interactions with other components in the system. Testing such changes in reasonable time and at a reasonable cost is a difficult problem as infinitely many test cases can be executed for any modification. It is important to obtain a risk assessment of impact of such post-release change fixes. Further, testing of such changes is complicated by the fact that they are applicable to hundreds of millions of users, even the smallest mistakes can translate to a very costly failure and re-work. There has been significant amount of research in the software engineering community on failure prediction, change analysis and test prioritization. Unfortunately, there is little evidence on the use of these techniques in day-to-day software development in industry. In this paper, we present our experiences with CRANE: a failure prediction, change risk analysis and test prioritization system at Microsoft Corporation that leverages existing research for the development and maintenance of Windows Vista. We describe the design of CRANE, validation of its useful-ness and effectiveness in practice and our learnings to help enable other organizations to implement similar tools and practices in their environment.",2159-4848,978-1-61284-174-8978-0-7695-4342,10.1109/ICST.2011.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770625,Software Reliability;Failure Prediction,Testing;Maintenance engineering;Cranes;Operating systems;Software maintenance;Measurement,object-oriented programming;operating systems (computers);program diagnostics;program testing;risk management;software architecture;software maintenance;software performance evaluation,CRANE;failure prediction;change analysis;large software systems;software component architecture;risk assessment;post-release change fixes;software engineering community;day-to-day software development;change risk analysis;test prioritization system;Microsoft Corporation;Windows Vista;organizations,,19,24,,,,,,IEEE,IEEE Conferences
Critical component analyzer ƒ?? A novel test prioritization framework for component based real time systems,M. R. Praba; D. J. Mala,"Dept. of Computer Applications, KLN College of Information Technology, Madurai, Tamilnadu, India; Dept. of Computer Applications, Thiagarajar College of Engineering, Madurai. Tamilnadu, India",2011 Malaysian Conference in Software Engineering,,2011,,,281,286,"Component based software development system is composed of many components and it uses the reusable components as the building blocks for constructing the complex software system. The major challenges in CBS are testing component dependency that is; it is a tricky task to test each and every component for each possible input data which will lead to exhaustive testing. To reduce the cost, the industries are following some stopping criteria and release the product to the customer side. These stopping criteria will at times lead to skipping up of some of the components from rigorous testing. This will lead to hazardous side effects such as loss in terms of revenue, human life and resources. This insight leads to the need to identify critical components which have the higher dependability measure in terms of functionality and receives higher priority in testing with rigorous test procedures. Hence, this paper proposes a novel method for identifying the critical components from the Software under Test (SUT) and prioritizes them for testing with at most care based on various dependency metrics and measures among the components with the help of Component Execution Sequence Graph (CESG).",,978-1-4577-1531-0978-1-4577-1530-3978-1-4577-1529,10.1109/MySEC.2011.6140684,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140684,Software Testing;Test prioritization;Component based Testing;Critical component;metrics and measures,Couplings;Software;Software testing;Software measurement;Programming,graph theory;object-oriented programming;program testing;real-time systems;software cost estimation;software metrics;software reusability,critical component analyzer;test prioritization framework;component based real time systems;component based software development system;reusable components;complex software system;testing component dependency;exhaustive testing;cost reduction;stopping criteria;hazardous side effects;rigorous test procedures;software under test;SUT;dependency metrics;component execution sequence graph;CESG,,,13,,,,,,IEEE,IEEE Conferences
Critical components identification and verification for effective software test prioritization,D. J. Mala; M. R. Praba,"Dept. of Computer Applications, Thiagarajar College of Engineering, Madurai; Dept. of MCA, KLN College of Information Technology, Madurai",2011 Third International Conference on Advanced Computing,,2011,,,181,186,"Nowadays, software complexity increases as the number of components in a component based system (CBS) increases. As the complexity level increases, the testing and verification of components also increases. This in turn rose up the testing time and cost which thus made industries to skip off some of the components due to the hard timeline and resource limitations especially during maintenance. This leads to hazardous effects if some of these missed components are critical in term of their core functionality and dependability with other components. Hence, a regression testing which is usually performed during maintenance phase should be developed meticulously to identify and test these critical components rigorously before releasing the software on to the customer side. This paper proposed a novel regression testing method based on the criticality measure calculated by means of dependability metrics and internal complexity metrics. Also, this paper compares the performance of the proposed approach with existing approaches and concluded that the proposed framework outperforms them.",2377-6927,978-1-4673-0671-3978-1-4673-0670-6978-1-4673-0669,10.1109/ICoAC.2011.6165171,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165171,Software Testing;Test prioritization;Component based Testing;Critical component;dependency metrics and Component metrics,Measurement;Software;Complexity theory;Software testing;Programming;Couplings,object-oriented programming;program testing;program verification;regression analysis;software maintenance;software metrics;statistical testing,software test prioritization;critical component identification;critical component verification;software complexity;component based system;software maintenance;core functionality;regression testing;criticality measure;dependability metrics,,,16,,,,,,IEEE,IEEE Conferences
Dealing with Test Automation Debt at Microsoft,J. Hartmann,NA,2011 IEEE 35th Annual Computer Software and Applications Conference Workshops,,2011,,,136,136,"At Microsoft, substantial time and resources are expended in test case development, execution and verification. Thousands of new tests are added to existing test suites without any kind of review regarding their unique contribution to test suite effectiveness or impact on test suite efficiency. This talk describes how we leverage existing code coverage data, together with reduction and prioritization techniques, to help each test team analyze its test suite and guide them in improving their suite's effectiveness and efficiency. The analysis focuses on identifying and deprecating/prioritizing groups of tests cases, given specific tactical goals for example, increasing current test suite stability and reliability, better structuring of test suite migration efforts, reducing test suite execution time and testing with limited hardware resources.",,978-1-4577-0980-7978-0-7695-4459,10.1109/COMPSACW.2011.99,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032226,,Conferences;Software quality;Automation;Hardware;Reliability;Internet,data reduction;program testing;program verification;software reliability,test automation debt;Microsoft;test case development;test case execution;test case verification;test suite effectiveness;test suite efficiency;code coverage data;reduction techniques;prioritization techniques;test suite stability;test suite reliability;test suite migration efforts;test suite execution time;hardware resources,,,,,,,,,IEEE,IEEE Conferences
Design of mice maze based on PLC control unit,J. Ding; S. Zhang,"School of Information and Electrical, Zhejiang University City College, Hangzhou, Zhejiang province, China; School of Information and Electrical, Zhejiang University City College, Hangzhou, Zhejiang province, China","2011 International Conference on Electronics, Communications and Control (ICECC)",,2011,,,2007,2010,"In recent years, there are a number of mouse maze devices developed. In this paper, we introduce the Y-maze which is based on the traditional programmable logic controller as a control center to the new Y-maze. In the subject, the maze of automatic control system will be in addition to the sensor controller sub- section also part and actuator part. This paper mainly discusses the new Maze automatic control system, describes the software design process and priorities, hardware selection and layout. Among them, the interface circuit of the programmable controller, I/O port assignments, infrared sensor module and the maze rotation module are described in detail The maze of automatic control system to achieve the experimental data in mice can reduce errors, improve test efficiency and reduce human labor, side by side, in addition to the traditional Y-maze caused by the interference of some unavoidable factors, such as mice, leaving the smell of mice in the experimental results, the reliability of the experiment can be further improved.",,978-1-4577-0321-8978-1-4577-0320-1978-1-4577-0319,10.1109/ICECC.2011.6067621,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6067621,Y-maze;PLC;infrared sensors;automatic control,Mice;Noise;Educational institutions;Presses;Switches,biocontrol;control engineering computing;programmable controllers;software engineering,mice maze design;PLC control unit;mouse maze devices;Y-maze;programmable logic controller;maze automatic control system;software design process;hardware selection;hardware layout;I-O port assignments;infrared sensor module;maze rotation module,,,7,,,,,,IEEE,IEEE Conferences
Design Principles for Integration of Model-Driven Quality Assurance Tools,O. Crelier; R. S. S. Filho; W. M. Hasling; C. J. Budnik,NA; NA; NA; NA,"2011 Fifth Brazilian Symposium on Software Components, Architectures and Reuse",,2011,,,100,109,"The engineering of software systems is supported by tools in different phases of the software development. The integration of these tools is crucial to assure the trace ability of existing models and artifacts, and to support the automation of critical software development phases such as software testing and validation. In particular, the integration of novel software quality assurance tools into existing environments must be performed in a way that minimizes its impact on existing software process, while the benefits of the tool are leveraged. This guarantees the adoption of new methodologies with minimal interference in existing production workflow. In this paper we discuss our experience in integrating a model-driven software testing tool developed within SIEMENS with a widely-adopted model-driven design tool. In particular, we establish a set of design principles from the lessons learned in this integration. We conclude showing a design that prioritizes data integration over control and presentation that achieves a high degree of tool integration while minimizing the integration development effort.",,978-0-7695-4626-1978-1-4673-0208,10.1109/SBCARS.2011.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6114546,Tool Integration;Software Interoperability;Quality Assurance Tools;UML;TDE/UML;Model-based Testing,Unified modeling language;Protocols;Testing;Data models;Software tools;Software engineering,data integration;program diagnostics;program testing;software quality;software tools,model-driven quality assurance tools;software system engineering;software development;software traceability;critical software development;software validation;software quality assurance tools;model-driven software testing tool;SIEMENS;model-driven software design tool;data integration;integration development,,2,15,,,,,,IEEE,IEEE Conferences
Designing VM schedulers for embedded real-time applications,A. Masrur; T. Pfeuffer; M. Geier; S. Dr??ssler; S. Chakraborty,"Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany; ReliaTec GmbH, Garching, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany",2011 Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS),,2011,,,29,38,"Virtual Machines (VMs) allow for platform-independent software development and their use in embedded systems is increasing. In particular, VMs are rewarding in the context of mixed-criticality applications to provide isolation between critical and non-critical tasks running on the same processor. In this paper, we study the design of a real-time system based on a VM monitor/hypervisor that supports multiple VMs/domains. Since each VM in the system runs several real-time tasks, scheduling the VMs leads to a hierarchical scheduling problem. So far, most published techniques for analyzing hierarchical scheduling deal with the schedulabil-ity problem, i.e., for a given hierarchical scheduler, testing whether a set of real-time tasks meet their deadlines. In this paper, we are rather concerned with the synthesis of hier-archical/VM schedulers; that is, how to design a scheduler such that all real-time tasks running on the different VMs meet their deadlines. We consider a setup where the tasks are scheduled on multiple VMs under fixed priorities according to the Deadline Monotonic (DM) policy. The VMs are scheduled under fixed priorities on a Rate Monotonic (RM) basis using one or more processors. A partitioned scheduling of VMs is considered, i.e., VMs are not allowed to migrate from one processor to the other. In this context, we propose a method for selecting optimum time slices and periods for each VM in the system. Our goal is to configure the VM scheduler such that not only all tasks are schedulable but also the minimum possible resources are used. Finally, to illustrate the proposed design technique, we present a case study based on automotive control applications.",,978-1-4503-0715-4978-1-4503-0715-4978-1-4503-0712,10.1145/2039370.2039378,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6062302,Real-Time Scheduling;Virtual Machines;Embedded Software,Real time systems;Scheduling;Program processors;Processor scheduling;Time factors;Delta modulation;Monitoring,automotive engineering;control engineering computing;embedded systems;processor scheduling;program testing;virtual machines,VM scheduler design;embedded real-time application;virtual machines;platform-independent software development;mixed-criticality application;noncritical tasks;critical tasks;VM monitor;hierarchical scheduling problem;schedulability problem;multiple VM;deadline monotonic policy;rate monotonic basis;partitioned scheduling;automotive control applications,,3,23,,,,,,IEEE,IEEE Conferences
Developing a Single Model and Test Prioritization Strategies for Event-Driven Software,R. C. Bryce; S. Sampath; A. M. Memon,"Utah State University, Logan; University of Maryland, Baltimore; University of Maryland, College Park",IEEE Transactions on Software Engineering,,2011,37,1,48,64,"Event-Driven Software (EDS) can change state based on incoming events; common examples are GUI and Web applications. These EDSs pose a challenge to testing because there are a large number of possible event sequences that users can invoke through a user interface. While valuable contributions have been made for testing these two subclasses of EDS, such efforts have been disjoint. This work provides the first single model that is generic enough to study GUI and Web applications together. In this paper, we use the model to define generic prioritization criteria that are applicable to both GUI and Web applications. Our ultimate goal is to evolve the model and use it to develop a unified theory of how all EDS should be tested. An empirical study reveals that the GUI and Web-based applications, when recast using the new model, show similar behavior. For example, a criterion that gives priority to all pairs of event interactions did well for GUI and Web applications; another criterion that gives priority to the smallest number of parameter value settings did poorly for both. These results reinforce our belief that these two subclasses of applications should be modeled and studied together.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2010.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5401169,Combinatorial interaction testing;covering arrays;event-driven software (EDS);t-way interaction coverage;test suite prioritization;user-session testing;Web application testing;GUI testing.,Software testing;Graphical user interfaces;Application software;Computer science;User interfaces;Protocols;Embedded software;Information systems;Educational institutions;Abstracts,graphical user interfaces;Internet;program testing;service-oriented architecture,event-driven software;test prioritization strategy;EDS;GUI testing;Web application testing;graphical user interface,,63,28,,,,,,IEEE,IEEE Journals & Magazines
Dynamic performance stubs to simulate the main memory behavior of applications,P. Trapp; M. Meyer; C. Facchi,"University of Applied Sciences, Ingolstadt, Ingolstadt, Germany; University of Applied Sciences, Ingolstadt, Ingolstadt, Germany; University of Applied Sciences, Ingolstadt, Ingolstadt, Germany",2011 International Symposium on Performance Evaluation of Computer & Telecommunication Systems,,2011,,,127,134,"Dynamic performance stubs provide a framework to simulate the performance behavior of software modules and functions. Hence, they can be used as an extension to software performance engineering methodologies. The methodology of dynamic performance stubs targets to gain oriented performance improvement. Other applications include the identification of ""hidden"" bottlenecks and the prioritization of optimization alternatives. Main memory stubs have been developed to extend the simulation possibilities of the dynamic performance stubs framework. They are able to simulate the heap and stack behavior of software modules or functions. This paper evaluates an algorithm to generate the simulation data file, which serves as input for the main memory stubs simulation algorithm. Moreover, it presents an automatic error correction algorithm to consider the results from the calibration functions to improve the simulation results. Additionally, a proof of concept is given to depict the results of the simulation data file generation and the automatic error correction algorithm. This paper shows that, it is possible to generate the simulation data file as well as to optimize the simulation data to compensate inaccuracies in order to create main memory stubs.",,978-1-4577-0139-9978-1-61782-309,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5984857,Memory Systems;Software Performance;Evaluation and Testing;Modeling;Performance Optimization;Bounds;Models;Case Studies,,optimisation;software performance evaluation;storage management,dynamic performance stubs;main memory behavior;performance behavior;software modules;software functions;software performance engineering;performance improvement;optimization;automatic error correction;calibration functions,,,13,,,,,,IEEE,IEEE Conferences
Dynamic Prioritization in Regression Testing,N. Kaushik; M. Salehie; L. Tahvildari; S. Li; M. Moore,NA; NA; NA; NA; NA,"2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops",,2011,,,135,138,"Although used extensively in industry, regression testing is challenging from both a process management as well as a resource management perspective. In literature, proposed test case prioritization techniques assume a constant pool of test cases with non-changing coverage during the regression testing process, and therefore they work with a fixed, prioritized test suite. However, in practice, test cases and their coverage metrics may change during regression testing due to modifications of software artefacts (e.g. due to bug fixing). For example, modifying obsolete test cases or source code may change the coverage metrics during the process. This may lead to some changes in test case priorities. Dealing with manual tests cases, scheduling test case execution in shared environments and other constraints in practice may cause the same effect. In this paper, we highlight these challenges in industrial regression testing and propose a paradigm called Dynamic Prioritization, which uses in-process events and the most up-to-date test suite to re-order test cases.",,978-0-7695-4345-1978-1-4577-0019,10.1109/ICSTW.2011.68,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954402,regression testing;test case prioritization;software testing,Testing;Measurement;Couplings;Software;Complexity theory;Manuals;Minimization,program testing,regression testing;test case prioritization technique;test case coverage metrics;dynamic prioritization paradigm,,3,14,,,,,,IEEE,IEEE Conferences
"Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities",Y. Shin; A. Meneely; L. Williams; J. A. Osborne,"DePaul University, Chicago; North Carolina State University, Raleigh; North Carolina State University, Raleigh; North Carolina State University, Raleigh",IEEE Transactions on Software Engineering,,2011,37,6,772,787,"Security inspection and testing require experts in security who think like an attacker. Security experts need to know code locations on which to focus their testing and inspection efforts. Since vulnerabilities are rare occurrences, locating vulnerable code locations can be a challenging task. We investigated whether software metrics obtained from source code and development history are discriminative and predictive of vulnerable code locations. If so, security experts can use this prediction to prioritize security inspection and testing efforts. The metrics we investigated fall into three categories: complexity, code churn, and developer activity metrics. We performed two empirical case studies on large, widely used open-source projects: the Mozilla Firefox web browser and the Red Hat Enterprise Linux kernel. The results indicate that 24 of the 28 metrics collected are discriminative of vulnerabilities for both projects. The models using all three types of metrics together predicted over 80 percent of the known vulnerable files with less than 25 percent false positives for both projects. Compared to a random selection of files for inspection and testing, these models would have reduced the number of files and the number of lines of code to inspect or test by over 71 and 28 percent, respectively, for both projects.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2010.81,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5560680,Fault prediction;software metrics;software security;vulnerability prediction.,Fault diagnosis;Software security;Complexity theory;Predictive models;Charge coupled devices,Linux;online front-ends;program testing;public domain software;software fault tolerance;software metrics,code churn;software vulnerabilities;developer activity metrics;security inspection;software metrics;source code;vulnerable code locations;open-source projects;Mozilla Firefox Web browser;Red Hat enterprise Linux kernel,,110,43,,,,,,IEEE,IEEE Journals & Magazines
Fast Start-up for Spartan-6 FPGAs using Dynamic Partial Reconfiguration,J. Meyer; J. Noguera; M. H?¬bner; L. Braun; O. Sander; R. M. Gil; R. Stewart; J. Becker,"Institute for Information Processing Technology, Karlsruhe Institute of Technology, Karlsruhe, Germany; Xilinx Inc., Ireland; Institute for Information Processing Technology, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute for Information Processing Technology, Karlsruhe Institute of Technology, Karlsruhe, Germany; Institute for Information Processing Technology, Karlsruhe Institute of Technology, Karlsruhe, Germany; Departamento de Electr??nica, Universidad de Alcal?­, Madrid, Spain; Xilinx Inc., Ireland; Institute for Information Processing Technology, Karlsruhe Institute of Technology, Karlsruhe, Germany","2011 Design, Automation & Test in Europe",,2011,,,1,6,"This paper introduces the first available tool flow for Dynamic Partial Reconfiguration on the Spartan-6 family. In addition, the paper proposes a new configuration method called Fast Start-up targeting modern FPGA architectures, where the FPGA is configured in two-steps, instead of using a single (monolithic) full device configuration. In this novel approach, only the timing-critical modules are loaded at power-up using the first high-priority bitstream, while the non-timing critical modules are loaded afterwards. This two-step or prioritized FPGA start-up is used in order to meet the extremely tight startup timing specifications found in many modern applications, like PCI-express or automotive applications. Finally, the developed tool flow and methods for Fast Start-up have been used and tested to implement a CAN-based automotive ECU on a Spartan-6 evaluation board (i.e., SP605). By using this novel approach, it was possible to decrease the initial bitstream size and hence, achieve a configuration time speed-up of up to 4.5??, when compared to a standard configuration solution.",1558-1101;1530-1591;1530-1591,978-3-9810801-8-6978-1-61284-208-0978-3-9810801-7,10.1109/DATE.2011.5763244,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5763244,,Field programmable gate arrays;Vehicle dynamics;Clocks;Timing;Control systems;Software,field programmable gate arrays;monolithic integrated circuits,fast start-up;Spartan-6 FPGA;dynamic partial reconfiguration;tool flow;configuration method;monolithic full device configuration;timing-critical modules;high-priority bitstream,,6,13,,,,,,IEEE,IEEE Conferences
GeTeX: A Tool for Testing Real-Time Embedded Systems Using CAN Applications,M. S. AbouTrab; S. Counsell; R. M. Hierons,NA; NA; NA,2011 18th IEEE International Conference and Workshops on Engineering of Computer-Based Systems,,2011,,,61,70,"Real-Time Embedded Systems (RTES) have an increasing role in controlling the IT that we use on a day-to-day basis. The behaviour of an RTES is not based solely on the interactions it might have with its surrounding environment, but also on the timing requirements it induces. As a result, ensuring that an RTES behaves correctly is non-trivial, especially after adding time as a new dimension to the complexity of the testing process. In previous research, we introduced a 'priority-based' approach which tested the logical and timing behaviour of an RTES modeled formally as UPPAAL Timed Automata (UTA). The 'priority-based' approach was based on producing sets of timed test traces by achieving timing constraints coverage according to three sets of priorities, namely boundary, out-boundary and in-boundary. In this paper, we introduce a new testing tool 'GeTeX' that deploys the ""priority-based"" testing approach. GeTeX is a complete testing tool which generates timed test-cases from UTA models and executes them on the System Under Test (SUT) to identify faults. In its current version, GeTeX supports Control Area Network (CAN) applications.",,978-0-7695-4379-6978-1-4577-0065,10.1109/ECBS.2011.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5934805,Real-time embedded systems;real-time model-based testing;testing tool;CAN,Testing;Clocks;Timing;Automata;Real time systems;Cost accounting;Semantics,automata theory;controller area networks;embedded systems;program testing;software tools,GeTeX;real-time embedded system testing;CAN;priority-based approach;UPPAAL timed automata;timing constraints;out-boundary priority;in-boundary priority;priority-based testing;testing tool;system under test;control area network;boundary priority,,1,30,,,,,,IEEE,IEEE Conferences
Goal-Oriented Test Case Selection and Prioritization for Product Line Feature Models,A. Ensan; E. Bagheri; M. Asadi; D. Gasevic; Y. Biletskiy,NA; NA; NA; NA; NA,2011 Eighth International Conference on Information Technology: New Generations,,2011,,,291,298,"The software product line engineering paradigm is amongst the widely used means for capturing and handling the commonalities and variabilities of the many applications of a target domain. The large number of possible products and complex interactions between software product line features makes the effective testing of them a challenge. To conquer the time and space complexity involved with testing a product line, an intuitive approach is the reduction of the test space. In this paper, we propose an approach to reduce the product line test space. We introduce a goal-oriented approach for the selection of the most desirable features from the product line. Such an approach allows us to identify the features that are more important and need to be tested more comprehensively from the perspective of the domain stakeholders. The more important features and the configurations that contain them will be given priority over the less important configurations, hence providing a hybrid test case reduction and prioritization strategy for testing software product lines.",,978-1-61284-427-5978-0-7695-4367,10.1109/ITNG.2011.58,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945249,Feature Models;Test Case Prioritization;Test Case Selection;Product Lines;Goal Models,Testing;Software;Cognition;Maintenance engineering;Marketing and sales;Complexity theory;Analytical models,computational complexity;product development;program testing;software reusability,goal-oriented test case selection;product line feature model prioritization;software product line engineering paradigm;time complexity;space complexity;software product line testing,,4,20,,,,,,IEEE,IEEE Conferences
How to Shop for Free Online -- Security Analysis of Cashier-as-a-Service Based Web Stores,R. Wang; S. Chen; X. Wang; S. Qadeer,NA; NA; NA; NA,2011 IEEE Symposium on Security and Privacy,,2011,,,465,480,"Web applications increasingly integrate third-party services. The integration introduces new security challenges due to the complexity for an application to coordinate its internal states with those of the component services and the web client across the Internet. In this paper, we study the security implications of this problem to merchant websites that accept payments through third-party cashiers (e.g., PayPal, Amazon Payments and Google Checkout), which we refer to as Cashier-as-a-Service or CaaS. We found that leading merchant applications (e.g., NopCommerce and Interspire), popular online stores (e.g., Buy.com and JR.com) and a prestigious CaaS provider (Amazon Payments) all contain serious logic flaws that can be exploited to cause inconsistencies between the states of the CaaS and the merchant. As a result, a malicious shopper can purchase an item at an arbitrarily low price, shop for free after paying for one item, or even avoid payment. We reported our findings to the affected parties. They either updated their vulnerable software or continued to work on the fixes with high priorities. We further studied the complexity in finding this type of logic flaws in typical CaaS-based checkout systems, and gained a preliminary understanding of the effort that needs to be made to improve the security assurance of such systems during their development and testing processes.",2375-1207;1081-6011,978-0-7695-4402-1978-1-4577-0147,10.1109/SP.2011.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5958046,e-Commerce security;web API;Cashier-as-a-Service;logic bug;program verification,Security;Servers;Software;Browsers;Complexity theory;Web services;Google,electronic commerce;financial data processing;Internet;retail data processing;security of data;Web sites,security analysis;cashier-as-a-service based Web stores;third-party services;Internet;merchant Web sites;third-party cashiers;PayPal;Amazon Payments;Google Checkout;NopCommerce;Interspire;Buy.com;JR.com;CaaS-based checkout systems,,31,39,,,,,,IEEE,IEEE Conferences
Hybrid regression testing technique: A multi layered approach,V. Gupta; D. S. Chauhan,"Research Scholar, Uttarakhand Technical University, Dehradun, Uttarakhand, India; Vice Chancellor, Uttarakhand Technical University, Dehradun, Uttarakhand, India",2011 Annual IEEE India Conference,,2011,,,1,5,Software needs to be delivered well in time and within budgets. One way of doing this is performing incremental delivery of the software with each increment being adding new features along with changes requests. This incremental delivery is supported with requirement prioritization and needs to be tested for checking the reliability and quality. Testing of this increment calls for testing of not only of old newly added functionality but also of existing features so as to make sure that old parts that works perfectly well do not malfunctions after new code is added. Thus a new hybrid technique is proposed in this paper that clusters the test cases and prioritizes the clusters on basis of priorities of requirements represented by the clusters and series of selections and prioritizations at levels of test cases reduces the number of test cases to manageable level and execution of these test cases guarantees the testing of highest priority requirements associated with statements that are often associated with failures or has highest number of parents or sibling's statements dependent on it and is likely to be influenced by changes or failures in this statement.,2325-940X;2325-9418,978-1-4577-1109-1978-1-4577-1110-7978-1-4577-1108,10.1109/INDCON.2011.6139363,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6139363,Regression testing;hybrid approach;parallelism;clusters,Testing;Software;Arrays;Visualization;Debugging;Software engineering;Information processing,program testing;program verification;regression analysis;software quality;software reliability,hybrid regression testing technique;multilayered approach;incremental delivery;requirement prioritization;software reliability checking;software quality checking;increment calls;test case execution,,6,11,,,,,,IEEE,IEEE Conferences
Impact Analysis of Configuration Changes for Test Case Selection,X. Qu; M. Acharya; B. Robinson,NA; NA; NA,2011 IEEE 22nd International Symposium on Software Reliability Engineering,,2011,,,140,149,"Testing configurable systems, which are becoming prevalent, is expensive due to the large number of configurations and test cases. Existing approaches reduce this expense by selecting or prioritizing configurations. However, these approaches redundantly run the full test suite for the selected configurations. To address this redundancy, we propose a test case selection approach by analyzing the impact of configuration changes with static program slicing. Given an existing test suite T used for testing a system S under a configuration C, our approach decides for each t in T if t has to be used for testing S under a different configuration C'. We have evaluated our approach on a large industrial system within ABB with promising results.",2332-6549;1071-9458;1071-9458,978-1-4577-2060-4978-0-7695-4568,10.1109/ISSRE.2011.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6132962,configuration testing;test case selection;program slicing;static impact analysis,Testing;Safety;Equations;Google;Data structures;Approximation methods;Switches,program slicing;program testing,impact analysis;configuration changes;test case selection;configurable system testing;static program slicing,,7,31,,,,,,IEEE,IEEE Conferences
Improving independence in the community for stroke survivors: The role of biomechanics visualisation in ankle-foot orthosis tuning,B. Carse; R. Bowers; B. Meadows; P. Rowe,"Bioengineering, Department, University of Strathclyde, Glasgow, UK; National Centre for Prosthetics and Orthotics, University of Strathclyde, Glasgow, UK; WestMARC, Southern General Hospital, Glasgow, UK; Bioengineering, Department, University of Strathclyde, Glasgow, UK",2011 5th International Conference on Pervasive Computing Technologies for Healthcare (PervasiveHealth) and Workshops,,2011,,,399,403,"One of the key priorities for stroke survivors in their rehabilitation process is regaining their ability to walk. Evidence has shown that provision of ankle-foot orthoses (AFOs) can have a positive impact on walking. This paper discusses the role of gait analysis in the provision of AFOs for stroke survivors. A discussion of the shortcomings of gait analysis techniques is included, with a description of how these might be overcome during the AFO tuning process through the ongoing development of data visualisation software. The design of a randomised controlled trial in conjunction with a series of qualitative measures is described, which will be used to test the efficacy of the visualisation software.",2153-1633;2153-1641,978-1-936968-15-2978-1-61284-767-2978-1-936968-14,10.4108/icst.pervasivehealth.2011.246128,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6038838,stroke;biomechanics;ankle-foot orthoses;gait analysis;visualisation;randomised controlled trial;rehabilitation,Data visualization;Tuning;Biomechanics;Three dimensional displays;Decision making;Software;Interviews,data visualisation;gait analysis;medical computing;orthotics;patient rehabilitation,stroke survivors;biomechanics visualisation;ankle-foot orthosis tuning;rehabilitation process;gait analysis techniques;AFO tuning process;data visualisation software,,,25,,,,,,IEEE,IEEE Conferences
Improving Regression Testing Transparency and Efficiency with History-Based Prioritization -- An Industrial Case Study,E. Engstr??m; P. Runeson; A. Ljung,NA; NA; NA,"2011 Fourth IEEE International Conference on Software Testing, Verification and Validation",,2011,,,367,376,"Background: History based regression testing was proposed as a basis for automating regression test selection, for the purpose of improving transparency and test efficiency, at the function test level in a large scale software development organization. Aim: The study aims at investigating the current manual regression testing process as well as adopting, implementing and evaluating the effect of the proposed method. Method: A case study was launched including: identification of important factors for prioritization and selection of test cases, implementation of the method, and a quantitative and qualitative evaluation. Results: 10 different factors, of which two are history-based, are identified as important for selection. Most of the information needed is available in the test management and error reporting systems while some is embedded in the process. Transparency is increased through a semi-automated method. Our quantitative evaluation indicates a possibility to improve efficiency, while the qualitative evaluation supports the general principles of history-based testing but suggests changes in implementation details.",2159-4848,978-1-61284-174-8978-0-7695-4342,10.1109/ICST.2011.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770626,regression testing;history-based prioritization;regression test selection;regression test prioritization;empirical evaluation;industrial case study;function testing,Testing;Software;History;Equations;Interviews;Context;Systematics,program testing;software houses,regression testing transparency;history-based prioritization;history based regression testing;regression test selection automation;large scale software development organization;manual regression testing process;test management systems;error reporting systems,,8,22,,,,,,IEEE,IEEE Conferences
IMS Threat and Attack Surface Analysis Using Common Vulnerability Scoring System,S. Petajasoja; H. Kortti; A. Takanen; J. Tirila,NA; NA; NA; NA,2011 IEEE 35th Annual Computer Software and Applications Conference Workshops,,2011,,,68,73,"For the purposes of this study, IMS specifications and public sources were analyzed using the general attack surface analysis methodology. These findings were verified and augmented by active scanning and passive analysis of the available real-world IMS test setups that were investigated during the project. As various tests and security probes were performed against the test setups, the system behaviour was analyzed for previously undetermined interactions and transient attack surfaces. After the IMS attack vectors had been identified, the Common Vulnerability Scoring System version 2 (CVSSv2) Base Scores were used to prioritize the IMS attack surface interfaces. CVSS is an industry standard for classifying vulnerabilities. It must be noted however that the idea of applying CVSS scoring to an a priori comparison of vulnerability categories and potential attack surfaces is original research by the authors of this study.",,978-1-4577-0980-7978-0-7695-4459,10.1109/COMPSACW.2011.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032214,next generation networks;security;threat analysis;attack surface analysis,Measurement;Protocols;Availability;Authentication;Complexity theory;Surface treatment,computer network security;IP networks;multimedia systems,IMS threat;IMS attack surface analysis;IMS specifications;public sources;general attack surface analysis methodology;active scanning;passive analysis;IMS attack vectors;common vulnerability scoring system version 2 base scores;IMS attack surface interfaces;IP multimedia subsystem,,4,6,,,,,,IEEE,IEEE Conferences
Increasing test coverage using human-based approach of fault injection testing,N. S. M. Yusop; W. F. Abbas; H. Haron; K. A. Kamaruddin,"Faculty of Computer &amp; Mathematical Sciences, Universiti Teknologi Mara, Selangor, Malaysia; Faculty of Computer &amp; Mathematical Sciences, Universiti Teknologi Mara, Selangor, Malaysia; Faculty of Computer &amp; Mathematical Sciences, Universiti Teknologi Mara, Selangor, Malaysia; Faculty of Computer &amp; Mathematical Sciences, Universiti Teknologi Mara, Selangor, Malaysia",2011 Malaysian Conference in Software Engineering,,2011,,,287,291,"Fault injection testing (FIT) approach validates system's fault tolerance mechanism by actively injecting software faults into the targeted areas in the system in order to accelerate its failure rate. This highly complements other testing approaches such as requirements and regression testing implemented during the same testing phase. During testing, it is impossible to run all possible test scenarios. It is especially difficult to predict how the user might use the system functionality correctly as per design. The human interaction through the system may be varies and will leads to the functionality loophole. It is therefore important to have strategic testing approach for evaluating the dependability of computer systems especially in human errors. This paper proposed on applying Knowledge-Based, Fault Prediction Model and Test Case Prioritization approaches that can be combined to increase the test coverage. The goal of this paper is to highlight the needs and advantages of the selected approaches in performing FIT as one of effective testing techniques in the ongoing quest for increased software quality.",,978-1-4577-1531-0978-1-4577-1530-3978-1-4577-1529,10.1109/MySEC.2011.6140685,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140685,Fault Injection Testing;Software Quality,Software engineering;Humans;Predictive models;Software testing;Software quality,program testing;regression analysis;software fault tolerance,test coverage;human-based approach;fault injection testing;fault tolerance;software faults;regression testing;functionality loophole;strategic testing approach;computer systems;knowledge-based fault prediction model;test case prioritization,,,23,,,,,,IEEE,IEEE Conferences
Industrial experiences with automated regression testing of a legacy database application,E. Rogstad; L. Briand; R. Dalberg; M. Rynning; E. Arisholm,"Simula Research Laboratory, Lysaker, University of Oslo, Dept. of Informatics, Norway; Simula Research Laboratory, Lysaker, University of Oslo, Dept. of Informatics, Norway; The Norwegian Tax Department, Oslo, Norway; The Norwegian Tax Department, Oslo, Norway; Testify AS, University of Oslo, Department of Informatics, Norway",2011 27th IEEE International Conference on Software Maintenance (ICSM),,2011,,,362,371,"This paper presents a practical approach and tool (DART) for functional black-box regression testing of complex legacy database applications. Such applications are important to many organizations, but are often difficult to change and consequently prone to regression faults during maintenance. They also tend to be built without particular considerations for testability and can be hard to control and observe. We have therefore devised a practical solution for functional regression testing that captures the changes in database state (due to data manipulations) during the execution of a system under test. The differences in changed database states between consecutive executions of the system under test, on different system versions, can help identify potential regression faults. In order to make the regression test approach scalable for large, complex database applications, classification tree models are used to prioritize test cases. The test case prioritization can be applied to reduce test execution costs and analysis effort. We report on how DART was applied and evaluated on business critical batch jobs in a legacy database application in an industrial setting, namely the Norwegian Tax Accounting System (SOFIE) at the Norwegian Tax Department (NTD). DART has shown promising fault detection capabilities and cost-effectiveness and has contributed to identify many critical regression faults for the past eight releases of SOFIE.",1063-6773;1063-6773,978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662,10.1109/ICSM.2011.6080803,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080803,regression testing;legacy database applications;industrial context,Databases;Testing;Mice;Keyboards;Unified modeling language,database management systems;organisational aspects;regression analysis;software maintenance;taxation,industrial experiences;automated regression testing;legacy database application;functional black-box regression testing;organizations;maintenance;regression faults;DART;Norwegian Tax Accounting System;SOFIE;Norwegian Tax Department;NTD;fault detection capabilities,,14,12,,,,,,IEEE,IEEE Conferences
Lower bounds for single machine subproblems occurring in weighted tardiness oriented shifting bottleneck procedures,R. Braune; G. Z??pfel; M. Affenzeller,"Institute for Production and Logistics Management, Johannes Kepler University, Linz, Austria; Institute for Production and Logistics Management, Johannes Kepler University, Linz, Austria; Department of Software Engineering, Upper Austria University of Applied Sciences, Hagenberg, Austria",2011 IEEE Symposium on Computational Intelligence in Scheduling (SCIS),,2011,,,25,32,"In this paper, we propose lower bounds for single machine scheduling problems which occur during a run of a shifting bottleneck procedure for total weighted tardiness job shops. The specific structure of this kind of problem and its objective function in particular prevent an immediate transfer or an adaption of existing lower bounds from ƒ??conventionalƒ? single machine problems with tardiness related objectives. Hence it has been necessary to develop bounding approaches which are to some extent conceptually new. Potential application scenarios range from exact subproblem solution methods or machine prioritization criteria in a shifting bottleneck procedure to branch-and-bound algorithms for job shops with total weighted tardiness objective. In order to provide a significant evaluation of the proposed lower bounds regarding their effectiveness and efficiency, we tested them based on problem instances which actually have been generated in a shifting bottleneck procedure applied to benchmark job shop problems.",,978-1-61284-196-0978-1-61284-195-3978-1-61284-194,10.1109/SCIS.2011.5976548,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5976548,,Schedules;Indexes;Single machine scheduling;Cost function;Processor scheduling;Equations;Complexity theory,integer programming;job shop scheduling;single machine scheduling;tree searching,weighted tardiness oriented shifting bottleneck procedure;single machine scheduling problem;job shops;tardiness related objective;bounding approach;machine prioritization;branch-and-bound algorithm;integer programming,,1,30,,,,,,IEEE,IEEE Conferences
Making the Case for MORTO: Multi Objective Regression Test Optimization,M. Harman,NA,"2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops",,2011,,,111,114,This paper argues that regression test optimization problems such as selection and prioritization require multi objective optimization in order to adequately cater for real world regression testing scenarios. The paper presents several examples of costs and values that could be incorporated into such a Multi Objective Regression Test Optimization (MORTO) approach.,,978-0-7695-4345-1978-1-4577-0019,10.1109/ICSTW.2011.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954399,regression testing;SBSE;testing;Search Based Software Engineering;test selection;test prioritization,Testing;Optimization;Minimization;Software engineering;USA Councils;IEEE Computer Society Press;Conferences,optimisation;program testing;regression analysis,multiobjective regression test optimization;test selection;test prioritization,,18,30,,,,,,IEEE,IEEE Conferences
Modeling the Diagnostic Efficiency of Regression Test Suites,A. Gonzalez-Sanchez; H. Gross; A. J. C. van Gemund,NA; NA; NA,"2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops",,2011,,,634,643,"Diagnostic performance, measured in terms of the manual effort developers have to spend after faults are detected, is not the only important quality of a diagnosis. Efficiency, i.e., the number of tests and the rate of convergence to the final diagnosis is a very important quality of a diagnosis as well. In this paper we present an analytical model and a simulation model to predict the diagnostic efficiency of test suites when prioritized with the information gain algorithm. We show that, besides the size of the system itself, an optimal coverage density and uniform coverage distribution are needed to achieve an efficient diagnosis. Our models allow us to decide whether using IG with our current test suite will provide a good diagnostic efficiency, and enable us to define criteria for the generation or improvement of test suites.",,978-0-7695-4345-1978-1-4577-0019,10.1109/ICSTW.2011.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954476,test prioritization;diagnosis;diagnostic effectiveness,Predictive models;Analytical models;Fault diagnosis;Entropy;Testing;Mathematical model;Bayesian methods,program testing;regression analysis,regression test suites;diagnostic efficiency;information gain algorithm;optimal coverage density;uniform coverage distribution,,3,35,,,,,,IEEE,IEEE Conferences
On Practical Adequate Test Suites for Integrated Test Case Prioritization and Fault Localization,B. Jiang; W. K. Chan; T. H. Tse,NA; NA; NA,2011 11th International Conference on Quality Software,,2011,,,21,30,"An effective integration between testing and debugging should address how well testing and fault localization can work together productively. In this paper, we report an empirical study on the effectiveness of using adequate test suites for fault localization. We also investigate the integration of test case prioritization and statistical fault localization with a postmortem analysis approach. Our results on 16 test case prioritization techniques and four statistical fault localization techniques show that, although much advancement has been made in the last decade, test adequacy criteria are still insufficient in supporting effective fault localization. We also find that the use of branch-adequate test suites is more likely than statement-adequate test suites in the effective support of statistical fault localization.",2332-662X;1550-6002;1550-6002,978-1-4577-0754-4978-0-7695-4468,10.1109/QSIC.2011.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004308,Debugging;testing;continuous integration,Testing;Subspace constraints;Schedules;Debugging;Flexible printed circuits;Cities and towns;Measurement,program debugging;program testing;software fault tolerance;statistical analysis,integrated test case prioritization;debugging;program testing;postmortem analysis approach;statistical fault localization techniques;branch-adequate test suites;statement-adequate test suites,,7,26,,,,,,IEEE,IEEE Conferences
Pragmatic prioritization of software quality assurance efforts,E. Shihab,"Queen's University, Kingston, ON, Canada",2011 33rd International Conference on Software Engineering (ICSE),,2011,,,1106,1109,"A plethora of recent work leverages historical data to help practitioners better prioritize their software quality assurance efforts. However, the adoption of this prior work in practice remains low. In our work, we identify a set of challenges that need to be addressed to make previous work on quality assurance prioritization more pragmatic. We outline four guidelines that address these challenges to make prior work on software quality assurance more pragmatic: 1) Focused Granularity (i.e., small prioritization units), 2) Timely Feedback (i.e., results can be acted on in a timely fashion), 3) Estimate Effort (i.e., estimate the time it will take to complete tasks), and 4) Evaluate Generality (i.e., evaluate findings across multiple projects and multiple domains). We present two approaches, at the code and change level, that demonstrate how prior approaches can be more pragmatic.",1558-1225;0270-5257,978-1-4503-0445-0978-1-4503-0445,10.1145/1985793.1986007,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032601,change risk;software metrics;unit testing,Software;Pragmatics;Measurement;Testing;Guidelines;Data mining;History,computational linguistics;software metrics;software quality,software quality assurance;pragmatic prioritization,,,18,,,,,,IEEE,IEEE Conferences
Prioritising Refactoring Using Code Bad Smells,M. Zhang; N. Baddoo; P. Wernick; T. Hall,NA; NA; NA; NA,"2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops",,2011,,,458,464,"We investigated the relationship between six of Fowler et al.'s Code Bad Smells (Duplicated Code, Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man) and software faults. In this paper we discuss how our results can be used by software developers to prioritise refactoring. In particular we suggest that source code containing Duplicated Code is likely to be associated with more faults than source code containing the other five Code Bad Smells. As a consequence, Duplicated Code should be prioritised for refactoring. Source code containing Message Chains seems to be associated with a high number of faults in some situations. Consequently it is another Code Bad Smell which should be prioritised for refactoring. Source code containing only one of the Data Clumps, Switch Statements, Speculative Generality, or Middle Man Bad Smell is not likely to be fault-prone. As a result these Code Bad Smells could be put into a lower refactoring priority.",,978-0-7695-4345-1978-1-4577-0019,10.1109/ICSTW.2011.69,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954447,Code Bad Smells;Refactoring;Fault,Software;Switches;Fault diagnosis;Encoding;Taxonomy;Systematics;Java,software engineering;system recovery,code bad smells;duplicated code;data clumps;switch statements;speculative generality;message chains;middle man;software faults;source code,,2,21,,,,,,IEEE,IEEE Conferences
Prioritizing interaction test suite for t-way testing,S. K. Said; R. R. Othman; K. Z. Zamli,"School of Electrical and Electronic Engineering, Universiti Sains Malaysia Enginering Campus, 14300 Nibong Tebal, Penang, Malaysia; School of Electrical and Electronic Engineering, Universiti Sains Malaysia Enginering Campus, 14300 Nibong Tebal, Penang, Malaysia; School of Electrical and Electronic Engineering, Universiti Sains Malaysia Enginering Campus, 14300 Nibong Tebal, Penang, Malaysia",2011 Malaysian Conference in Software Engineering,,2011,,,292,297,"In recent years, many new t-way interaction based strategies (where t indicates the interaction strength), particularly based on covering arrays, have been developed in the literature. In search of an optimal strategy that generates the most minimum number of tests, many of existing t-way strategies have not sufficiently dealt with test prioritization (i.e. in terms of maximizing new interaction coverage per test). Addressing this issue, this paper highlights a useful prioritization algorithm to reorganize the test cases in order to improve the rate of interaction coverage. This algorithm takes a pre-generated t-way test suite as input and automatically generates a priority ordered test suite as output. In order to demonstrate its applicability, this paper demonstrates the use of the algorithm to help prioritize the test suite generated by existing t-way strategy, MC-MIPOG.",,978-1-4577-1531-0978-1-4577-1530-3978-1-4577-1529,10.1109/MySEC.2011.6140686,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6140686,Interaction Testing;Test Prioritization,Software testing;Algorithm design and analysis;Software;Software engineering;Generators;Arrays,program testing,interaction test suite;t-way testing;t-way interaction based strategy;optimal strategy;test prioritization;pregenerated t-way test suite;MC-MIPOG,,2,16,,,,,,IEEE,IEEE Conferences
Prioritizing Requirements-Based Regression Test Cases: A Goal-Driven Practice,M. Salehie; S. Li; L. Tahvildari; R. Dara; S. Li; M. Moore,NA; NA; NA; NA; NA; NA,2011 15th European Conference on Software Maintenance and Reengineering,,2011,,,329,332,"Any changes for maintenance or evolution purposes may break existing working features, or may violate the requirements established in the previous software releases. Regression testing is essential to avoid these problems, but it may be ended up with executing many time-consuming test cases. This paper tries to address prioritizing requirements-based regression test cases. To this end, system-level testing is focused on two practical issues in industrial environments: i) addressing multiple goals regarding quality, cost and effort in a project, and ii) using non-code metrics due to the lack of detailed code metrics in some situations. This paper reports a goal-driven practice at Research In Motion (RIM) towards prioritizing requirements-based test cases regarding these issues. Goal-Question-Metric (GQM) is adopted in identifying metrics for prioritization. Two sample goals are discussed to demonstrate the approach: detecting bugs earlier and maintaining testing effort. We use two releases of a prototype Web-based email client to conduct a set of experiments based on the two mentioned goals. Finally, we discuss lessons learned from applying the goal-driven approach and experiments, and we propose few directions for future research.",1534-5351;1534-5351,978-1-61284-259-2978-0-7695-4343,10.1109/CSMR.2011.46,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5741354,Test case prioritization;Software regression testing;Requirements-based test cases;Goal-driven approaches,Testing;Measurement;Computer bugs;Humans;Complexity theory;Software;Electronic mail,program testing;regression analysis;software maintenance;software metrics,requirement prioritization;regression test cases;system-level testing maintenance;noncode metrics;Research In Motion;goal-question-metric;bug detection;Web-based email client,,5,7,,,,,,IEEE,IEEE Conferences
Prioritizing tests for fault localization through ambiguity group reduction,A. Gonzalez-Sanchez; R. Abreu; H. Gross; A. J. C. van Gemund,"Delft University of Technology, Software Technology Department, Mekelweg 4, 2628 CD, The Netherlands; University of Porto, Departament of Informatics Engineering, Rua Dr. Roberto Frias, 4200-465, Portugal; Delft University of Technology, Software Technology Department, Mekelweg 4, 2628 CD, The Netherlands; Delft University of Technology, Software Technology Department, Mekelweg 4, 2628 CD, The Netherlands",2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011),,2011,,,83,92,"In practically all development processes, regression tests are used to detect the presence of faults after a modification. If faults are detected, a fault localization algorithm can be used to reduce the manual inspection cost. However, while using test case prioritization to enhance the rate of fault detection of the test suite (e.g., statement coverage), the diagnostic information gain per test is not optimal, which results in needless inspection cost during diagnosis. We present RAPTOR, a test prioritization algorithm for fault localization, based on reducing the similarity between statement execution patterns as the testing progresses. Unlike previous diagnostic prioritization algorithms, RAPTOR does not require false negative information, and is much less complex. Experimental results from the Software Infrastructure Repository's benchmarks show that RAPTOR is the best technique under realistic conditions, with average cost reductions of 40% with respect to the next best technique, with negligible impact on fault detection capability.",1938-4300,978-1-4577-1639-3978-1-4577-1638,10.1109/ASE.2011.6100153,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6100153,,Inspection;Testing;Complexity theory;Software;Subspace constraints;Estimation;Bayesian methods,program testing;software reliability,ambiguity group reduction;regression tests;fault localization algorithm;RAPTOR;test prioritization algorithm;software infrastructure repository;software reliability,,14,46,,,,,,IEEE,IEEE Conferences
Ranking of technology transfer barriers in developing countries; case study of Iran's biotechnology industry,K. Yazdani; K. Y. Rashvanlouei; K. Ismail,"Faculty of Management and Human Resource Development, University Technology Malaysia, Johor Bahru, Malaysia; Graduate School of Management and Economics, Sharif University of Technology, Tehran, Iran; Faculty of Management and Human Resource Development, University Technology Malaysia, Johor Bahru, Malaysia",2011 IEEE International Conference on Industrial Engineering and Engineering Management,,2011,,,1602,1606,"In this paper, first we shall define a list of technology transfer barriers in the biotechnology field through interviews and literature reviews. Next, we shall categorize them into four major categories of technology, and at last introduce all of the hypotheses on the existence of relationship between each part of technology and failure in the technology transfer process. Questionnaires consisting of two parts were created and handed out to all of the specialists and biotechnology idea-holders in the Iranian National Research center of Biotechnology, Lidco Co, Iran Pastor Institute, Biotechnology Department of University of Tehran, etc. Going on, we've evaluated the factors' validity through statistical tests and the recognized barriers' priorities in each category, which were generally, identified by Analytical Hierarchy Process (AHP) via the software Expert Choice. These priorities describe the high importance of organization-ware (34.1%), information-ware (27%), technique-ware (20.7%) and eventually human-ware (18.1%) respectively.",2157-362X;2157-3611;2157-3611,978-1-4577-0739-1978-1-4577-0740-7978-1-4577-0738,10.1109/IEEM.2011.6118187,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6118187,Management of Technology;Technology Transfer;Developing Countries;Analytic Hierarchy Process (AHP),Technology transfer;Biotechnology;Reliability;Industries;Educational institutions;Humans;Organizations,biotechnology;decision making;technology transfer,technology transfer barrier ranking;biotechnology industry;Iran;technology transfer process;analytical hierarchy process;Expert Choice software;organization-ware;information-ware;technique-ware;human-ware,,2,10,,,,,,IEEE,IEEE Conferences
Regression testing in Software as a Service: An industrial case study,H. Srikanth; M. B. Cohen,"IBM Lotus Division, Littleton, MA, USA; Dept. of Computer Science &amp; Engineering, University of Nebraska-Lincoln, USA",2011 27th IEEE International Conference on Software Maintenance (ICSM),,2011,,,372,381,"Many organizations are moving towards a business model of Software as a Service (SaaS), where customers select and pay for services dynamically via the web. In SaaS, service providers face the challenge of delivering and maintaining high quality software solutions which must continue to work under an enormous number of scenarios; customers can easily subscribe and unsubscribe from services at any point. To date, there has been little research on unique approaches for regression test methodologies for testing in a SaaS environment. In this paper, we present an industrial case study of a regression testing approach to improve test effectiveness and efficiency in SaaS. We model service level use cases from field failures as abstract events and then generate sequences of these for testing to provide a broad coverage of the possible use cases. In subsequent releases of the system we prioritize the tests to improve time to detection of faults in the modified system. We have applied our technique to two releases of a large industrial enterprise level SaaS application and demonstrate that using our approach (1) we could have uncovered escaped faults prior to the system release in both versions of the system; (2) using a priority order we could have improved the efficiency of testing in the first version; and (3) prioritization based on failure history from the first version increases the fault detection rate in the new version, suggesting a correlation between the important sequences in versions that can be leveraged for regression testing.",1063-6773;1063-6773,978-1-4577-0664-6978-1-4577-0663-9978-1-4577-0662,10.1109/ICSM.2011.6080804,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6080804,Regression Testing;Prioritization;Software as a Service;Cloud Computing,Testing;Software;Companies;Fault detection;Arrays;Maintenance engineering,cloud computing;fault diagnosis;program testing;regression analysis;software fault tolerance;software maintenance,regression testing approach;software-as-a-service;SaaS;industrial case study;Web;sequence generation;fault detection;industrial enterprise level;failure history,,7,25,,,,,,IEEE,IEEE Conferences
Research and implementation of resource management system based on Xen virtual machine,Gang Ning; Yongqing Sun,"School of Software, Shanghai Jiaotong University, #800 Rd Dongchuan, China; Network Security Center, The Third Institute of Ministry of Public Security, #339 Rd Bisheng, Shanghai, China",Proceedings of 2011 International Conference on Computer Science and Network Technology,,2011,3,,1371,1374,"With the development of computer technology, there is gradually abundant resource available for computer systems. Virtualization technology provides a viable solution for effective management and rational allocation of system resources. Xen virtual machine is an excellent open source virtual machine, so attracting widespread attention, with broad application prospects. However, the traditional resource management of Xen virtual machine focuses on sharing processor resources fairly, while ignoring the effect of the virtual machines with the different priorities. This would cause the practicality and the performance issues in using virtual machine. This paper proposes a resource management system model based on Xen virtual machine. The model monitors guest domain and analyzes runtime information for automating resource allocation. It mainly take into account the virtual machines with different priorities. Through a series of comparative tests, the results verify that this model can enhance practicality.",,978-1-4577-1587-7978-1-4577-1586-0978-1-4577-1585,10.1109/ICCSNT.2011.6182220,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6182220,Xen;virtual technology;resource management,Prototypes;Monitoring;Analytical models;Educational institutions,resource allocation;virtual machines;virtualisation,resource management system;Xen virtual machine;virtualization technology;system resource rational allocation;open source virtual machine;processor resource sharing;guest domain,,2,8,,,,,,IEEE,IEEE Conferences
Risk-Based Testing of Safety-Critical Embedded Systems Driven by Fault Tree Analysis,J. Kloos; T. Hussain; R. Eschbach,NA; NA; NA,"2011 IEEE Fourth International Conference on Software Testing, Verification and Validation Workshops",,2011,,,26,33,"One important aspect of the quality assurance process of safety-critical embedded systems is verifying the appropriateness, correctness of the implementation and effectiveness of safety functions. Due to the rapid growth in complexity, manual verification activities are no longer feasible. This holds especially for testing. A popular method for testing such complex systems is model-based testing. Recent techniques for model-based testing do not sufficiently take into consideration the information derived from the safety analyses like Failure Mode and Effect Analysis and Fault Tree Analyses (FTA). In this paper, we describe an approach to use the results of FTA during the construction of test models, such that test cases can be derived, selected and prioritized according to the severity of the identified risks and the number of basic events that cause it. This approach is demonstrated on an example from the automation domain, namely a modular production system. We find that the method provides a significant increase in coverage of safety functions, compared to regular model based testing.",,978-0-7695-4345-1978-1-4577-0019,10.1109/ICSTW.2011.90,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5954386,Safety;Model-based Testing;Fault-Tree Analysis;Risk-based Testing,Testing;Safety;Fault trees;Sensors;Manuals;Embedded systems,embedded systems;program testing;safety-critical software;software quality,risk-based testing;safety-critical embedded systems;fault tree analysis;quality assurance process;model-based testing;failure mode and effect analysis,,12,15,,,,,,IEEE,IEEE Conferences
"Robotic test bed for autonomous surface exploration of Titan, Mars, and other planetary bodies",W. Fink; M. A. Tarbell; R. Furfaro; L. Powers; J. S. Kargel; V. R. Baker; J. Lunine,"Visual and Autonomous Exploration Systems Research Laboratory, Departments of Electrical &amp; Computer Engineering, Biomedical Engineering, and Ophthalmology and Vision Science, University of Arizona, Tucson, 85721, USA; Visual and Autonomous Exploration Systems Research Laboratory, Departments of Electrical &amp; Computer Engineering, Biomedical Engineering, and Ophthalmology and Vision Science, University of Arizona, Tucson, 85721, USA; Systems and Industrial Engineering Department, University of Arizona, Tucson, USA; Department of Electrical &amp; Computer Engineering, University of Arizona, Tucson, USA; Department of Hydrology and Water Resources, University of Arizona, Tucson, USA; Department of Hydrology and Water Resources, University of Arizona, Tucson, USA; Lunar and Planetary Laboratory, University of Arizona, Tucson, USA",2011 Aerospace Conference,,2011,,,1,11,"Tier-scalable robotic reconnaissance missions are called for in extreme space environments, including planetary atmospheres, surfaces (both solid and liquid), and subsurfaces (e.g., oceans), as well as in potentially hazardous or inaccessible operational areas on Earth. Such future missions will require increasing degrees of operational autonomy: (1) Automatic mapping of an operational area from different vantages (i.e., spaceborne, airborne, surface, subsurface); (2) automatic sensor deployment and sensor data gathering; (3) automatic feature extraction and target/region-of-interest/anomaly identification within the mapped operational area; (4) automatic target prioritization for follow-up or close-up (in-situ) examination; and (5) subsequent automatic, targeted deployment and navigation/relocation of agents/sensors (e.g., to follow up on transient events). We report on recent progress in developing an Earth-based (outdoors) robotic test bed for Tier-scalable Reconnaissance at the University of Arizona and Caltech for distributed, science-driven, and significantly less constrained (compared to state-of-the-art) reconnaissance of prime locations on a variety of planetary bodies, with particular focus on Saturn's moon Titan with its methane/hydrocarbon lakes and Mars. The test bed currently comprises several computer-controlled robotic surface vehicles, i.e., rovers and lake landers/boats equipped with a variety of sensors. To achieve a fully operational Tier-scalable Reconnaissance test bed, aerial platforms will be integrated as a next step. The robotic surface vehicles can be interactively or automatically controlled from anywhere in the world in near real-time via the Internet. The test bed enables the implementation, field-testing, and validation of algorithms and strategies for navigation, exploration, sensor deployment, sensor data gathering, feature extraction, anomaly detection, and science goal prioritization for autonomous planetary exploration. Furthermore, it permits field-testing of novel instruments and sensor technologies, as well as testing of cooperative multi-agent scenarios and distributed scientific exploration of operational areas. As such the robotic test bed enables the development, implementation, field-testing, and validation of software packages for inter-agent communication and coordination to navigate and explore operational areas with greatly reduced reliance on (ultimately without assistance from) ground operators, thus affording the degree of mission autonomy/flexibility necessary to support future missions to Titan, Mars, and other planetary bodies, including asteroids.",1095-323X;1095-323X,978-1-4244-7351-9978-1-4244-7350-2978-1-4244-7349,10.1109/AERO.2011.5747267,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5747267,,Robot sensing systems;Sea surface;Cameras;Reconnaissance;Boats;Lakes,Earth;geophysical prospecting;Internet;Mars;mobile robots;Moon;multi-agent systems;planetary rovers;remote sensing;Saturn;sensors;software packages,autonomous surface exploration;Titan;Mars;planetary body;tier scalable robotic reconnaissance mission;space environment;planetary atmosphere;operational autonomy;automatic mapping;automatic sensor deployment;sensor data gathering;automatic feature extraction;anomaly identification;mapped operational area;automatic target prioritization;Earth based robotic test bed;University of Arizona;Caltech;Saturn moon;computer controlled robotic surface vehicle;Internet;cooperative multiagent scenario;software package;inter agent communication,,12,24,,,,,,IEEE,IEEE Conferences
Scenario Driven Testing,K. Sivashanmugam; D. Lin; S. Palanisamy,NA; NA; NA,2011 Eighth International Conference on Information Technology: New Generations,,2011,,,299,303,"Software testing has traditionally focused on evaluating the functionality of implemented modules against feature specifications. This approach assumes that customer requirements and usage scenarios are accurately translated into specifications and that individual modules implemented using the feature specifications would work seamlessly and coherently to solve business problems meant to be addressed by the software under test. To ensure software built would help customers solve their business problems as intended, test teams have to go beyond traditional feature driven testing approach and test software for quality and completeness with respect to targeted customer scenarios. For this, test teams have to adopt scenario driven test methodology which involves understanding the targeted customer scenarios and use them along with feature specifications for the intended software solution to translate them into test specifications, prioritization of test work items and use them throughout project for shared understanding of tradeoffs and making decisions. In this short paper, we describe scenario driven testing and share how it was applied to test a feature-set developed for a successful product line at Microsoft??.",,978-1-61284-427-5978-0-7695-4367,10.1109/ITNG.2011.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945250,scenario driven testing;scenario driven engineering;variation modeling;customer oriented testing;test case generation,Testing;Software;Automation;Context;Planning;Business;Object oriented modeling,program testing;software quality,scenario driven testing;software testing;customer requirements;feature specifications;business problems;feature driven testing approach;software quality;Microsoft??,,3,6,,,,,,IEEE,IEEE Conferences
"Security Requirements Analysis, Specification, Prioritization and Policy Development in Cyber-Physical Systems",K. K. Fletcher; X. Liu,NA; NA,2011 Fifth International Conference on Secure Software Integration and Reliability Improvement - Companion,,2011,,,106,113,"In recent past, the security of cyber-physical systems (CPSs) has been the subject of major concern. One of the reasons is that, CPSs are often applied to mission-critical processes. Also, the automation CPSs bring in managing physical processes, and the detail of information available to them for carrying out their tasks, make securing them a prime importance. Securing CPSs is a difficult task as systems are interconnected. In order to achieve a continuous secured CPS environment, there is the need for an integrated methodology to analyze, specify and prioritize security requirements and also to develop policies to meet them. First, CPS assets are represented using high-order object models. Second, swim lane diagrams are extended to include malactivities and prevention or mitigation options to decompose use cases. We analyze security threats pertaining to the hardware components, software components and the hardware-software interaction. Security requirements are then specified, and an analytical prioritization approach, based on relative priority analysis is employed to prioritize them. Finally, security policies are then developed to meet the requirements. To demonstrate its effectiveness and evaluate its application, the proposed methodology is applied in a structured approach to a test bed - Ayushman, a Pervasive Health Monitoring System (PHMS).",,978-1-4577-0781-0978-0-7695-4454,10.1109/SSIRI-C.2011.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004511,cyber-physical systems (CPS);CPS security requirements;high order object oriented modeling technique;CPS security requirements prioritization;hardware-software interaction,Security;Object oriented modeling;Analytical models;Unified modeling language;Monitoring;Software;Object recognition,formal specification;health care;medical computing;object-oriented programming;security of data;systems analysis;ubiquitous computing,security requirements analysis;security requirement specification;security requirement prioritization;policy development;cyber-physical system;mission-critical process;swim lane diagram;malactivity;prevention options;mitigation options;security threat analysis;hardware components;software components;hardware-software interaction;relative priority analysis;Ayushman;Pervasive Health Monitoring System,,5,15,,,,,,IEEE,IEEE Conferences
Test Case Generation and Prioritization from UML Models,A. Gantait,NA,2011 Second International Conference on Emerging Applications of Information Technology,,2011,,,345,350,"This paper proposes a novel approach to generating test cases from UML 2.0 activity diagrams and prioritizing those test cases using model information encapsulated in the activity diagrams. The test cases generated according to our approach are suitable for system level testing of the application. For prioritization of test cases, we propose a method based on coverage of all transitions in the activity diagram and usage probability of a particular flow in the activity model. We also propose an approach for selecting test data based on analysis of the branch conditions of the decision nodes in the activity diagrams.",,978-1-4244-9683,10.1109/EAIT.2011.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5734956,UML;Model;Activity Diagram;Test case;Prioritization;Test data,Unified modeling language;Object oriented modeling;Testing;Software;Programming;Data models;Business,probability;program testing;Unified Modeling Language,test case generation;test case prioritization;UML 2.0 activity diagrams;UML models;usage probability,,3,17,,,,,,IEEE,IEEE Conferences
Test case prioritization for regression testing based on fault dependency,M. I. Kayes,"Quality Assurance Engineer, SoftwarePeople, Dhaka, Bangladesh",2011 3rd International Conference on Electronics Computer Technology,,2011,5,,48,52,"Test case prioritization techniques involve scheduling test cases for regression testing in an order that increases their effectiveness at meeting some performance goal. This is inefficient to re execute all the test cases in regression testing following the software modifications. Using information obtained from previous test case execution, prioritization techniques order the test cases for regression testing so that most beneficial are executed first thus allows an improved effectiveness of testing. One performance goal, rate of dependency detected among faults, measures how quickly dependency among faults are detected within the regression testing process. An improved rate of fault dependency can provide faster feedback on software and let developers start debugging on the severe faults that cause other faults to appear later. This paper presents the new metric for assessing rate of fault dependency detection and an algorithm to prioritize test cases. Using the new metric the effectiveness of this prioritization is shown comparing it with non-prioritized test case. Analysis proves that prioritized test cases are more effective in detecting dependency among faults.",,978-1-4244-8679-3978-1-4244-8678,10.1109/ICECTECH.2011.5941954,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5941954,Software Engineering;Software Testing;Test Case Prioritization;Regression Testing;Average Percentage of Fault Dependency Detected (APFDD),Measurement;Software;Software testing;Fault detection;Software algorithms;Debugging,computer aided software engineering;program testing;regression analysis;scheduling,test case prioritization;scheduling test;test case execution;prioritization techniques;regression testing process;fault dependency detection,,7,10,,,,,,IEEE,IEEE Conferences
Test Case Prioritization Technique Based on Genetic Algorithm,W. Jun; Z. Yan; J. Chen,NA; NA; NA,2011 International Conference on Internet Computing and Information Services,,2011,,,173,175,"With the rapid development of information technology, software testing, as a software quality assurance, is becoming more and more important. In the software life cycle, each time the code has changed need to be regression testing. The huge test case library makes running a full test case library being challenged. To this end, we designed a genetic algorithm-based test case prioritization algorithm and improved the genetic algorithm proposed software test case prioritization algorithm.",,978-1-4577-1561,10.1109/ICICIS.2011.50,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6063222,Software Testing;Test Case Prioritization;Genetic Algorithms;APBC,,genetic algorithms;program testing;quality assurance;regression analysis;software quality,information technology;software testing;software quality assurance;software life cycle;regression testing;test case library;genetic algorithm-based test case prioritization algorithm,,3,5,,,,,,IEEE,IEEE Conferences
Test Generation for X-machines with Non-terminal States and Priorities of Operations,K. Bogdanov,NA,"2011 Fourth IEEE International Conference on Software Testing, Verification and Validation",,2011,,,130,139,"Testing methods aiming to demonstrate that an implementation behaves the same as a specification X-machine (extended finite-state machine) usually assume that (1) all states are terminal states and (2) there are no priorities associated with operations on transitions. The considered model for the machine is such that outputs for transitions leading to non-terminal states will be buffered and contents of buffers will only be made observable when terminal states are entered. The X-machine testing method has been extended in this work to handle such an extension of X-machines (EFSM).Priorities of operations determine the order in which guards of transitions are evaluated. This makes it possible to reduce the size of a test suite. For instance, if testing has shown that a transition with a specific guard g has been implemented from some state, then no lower-priority transition with a guard implied by g may ever be executed from that state. It is hence not necessary to test for the presence of such a lower-priority transition.",2159-4848,978-1-61284-174-8978-0-7695-4342,10.1109/ICST.2011.59,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770602,Test Generation;Finite-State Machines;Extended Finite-State Machines;FSM;EFSM;X-machines,Testing;Schedules;Automata;Context;Unified modeling language;Data models,finite state machines;program testing,test generation;X-machines;nonterminal states;operation priority;extended finite-state machine,,,22,,,,,,IEEE,IEEE Conferences
Towards Impact Analysis of Test Goal Prioritization on the Efficient Execution of Automatically Generated Test Suites Based on State Machines,S. Wei??leder,NA,2011 11th International Conference on Quality Software,,2011,,,150,155,"Test prioritization aims at reducing test execution costs. There are several approaches to prioritize test cases based on collected data of previous test runs, e.g., in regression testing. In this paper, we present a new approach to test prioritization for efficient test execution that is focused on the artifacts used in model-based test generation from state machines. We propose heuristics for test goal prioritizations and evaluate them using two different test models. Our finding is that the prioritizations can have a positive impacton the test execution efficiency. This impact, however, is hard to predict for a concrete situation. Thus, the question for the general gain of test goal prioritizations is still open.",2332-662X;1550-6002;1550-6002,978-1-4577-0754-4978-0-7695-4468,10.1109/QSIC.2011.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004322,Model-Based Testing;Coverage Criteria;Test Goal Prioritization;Test Execution Efficiency,Testing;Unified modeling language;Fault detection;Redundancy;Generators;Search problems;Couplings,automatic test pattern generation;finite state machines;regression analysis;Unified Modeling Language,test goal prioritization;automatically generated test suite;state machine;test execution cost reduction;regression testing;model-based test generation,,,19,,,,,,IEEE,IEEE Conferences
Usability Testing Methodology: Effectiveness of Heuristic Evaluation in E-Government Website Development,A. Sivaji; A. Abdullah; A. G. Downe,NA; NA; NA,2011 Fifth Asia Modelling Symposium,,2011,,,68,72,"Software development organizations consist of marketing, project management, development, design, and quality assurance team. It is important for the various teams within the organization to understand the benefits and limitation of incorporating various usability testing methods within the software development life cycle. Some of the reasons for poor usability include effort prioritization conflicts from project management, development and design team. The role of the usability engineer is to get involved as the heuristic evaluator and facilitate the development and design efforts are based on usability principles and at the same time adhering to the project time line. Two of the common usability inspection methods consist of user experience testing and expert review or more commonly known as Heuristic Evaluation (HE). This paper focuses on understanding the effectiveness of HE as a methodology for defect detection. The results show the effectiveness of the HE as a usability testing methodology in capturing defects and prioritizing development and design efforts. The results also reinforce the need for integrating traditional heuristics with modified heuristics customized to the domain or field of the project being tested such as E-Government.",2376-1164;2376-1172,978-1-4577-0193,10.1109/AMS.2011.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5961243,Usability Testing Methodology;Heuristice Evaluation;Defect Detection;Iterative Testing,Usability;Helium;Testing;Electronic government;Navigation;Inspection,government data processing;program testing;software development management;Web sites,usability testing methodology;e-government Website development;software development life cycle;effort prioritization conflicts;user experience testing;expert review;heuristic evaluation;defect detection,,12,13,,,,,,IEEE,IEEE Conferences
Using SQL Hotspots in a Prioritization Heuristic for Detecting All Types of Web Application Vulnerabilities,B. Smith; L. Williams,NA; NA,"2011 Fourth IEEE International Conference on Software Testing, Verification and Validation",,2011,,,220,229,"Development organizations often do not have time to perform security fortification on every file in a product before release. One way of prioritizing security efforts is to use metrics to identify core business logic that could contain vulnerabilities, such as database interaction code. Database code is a source of SQL injection vulnerabilities, but importantly may be home to unrelated vulnerabilities. The goal of this research is to improve the prioritization of security fortification efforts by investigating the ability of SQL hotspots to be used as the basis for a heuristic for prediction of all vulnerability types. We performed empirical case studies of 15 releases of two open source PHP web applications: Word Press, a blogging application, and WikkaWiki, a wiki management engine. Using statistical analysis, we show that the more SQL hotspots a file contains per line of code, the higher the probability that file will contain any type of vulnerability.",2159-4848,978-1-61284-174-8978-0-7695-4342,10.1109/ICST.2011.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5770611,sql;hotspots;sql injection;prioritization;empirical;wordpress;wikkawiki,Security;Predictive models;Measurement;Mathematical model;Complexity theory;Software;Databases,Internet;probability;search engines;security of data;software metrics;SQL;statistical analysis;Web sites,SQL hotspots;prioritization heuristic;Web application vulnerability;development organizations;core business logic;database interaction code;database code;SQL injection;security fortification efforts;open source PHP Web applications;Word Press;blogging application;WikkaWiki;wiki management engine;statistical analysis;probability,,11,17,,,,,,IEEE,IEEE Conferences
Which Crashes Should I Fix First?: Predicting Top Crashes at an Early Stage to Prioritize Debugging Efforts,D. Kim; X. Wang; S. Kim; A. Zeller; S. C. Cheung; S. Park,"Sogang University, Seoul; The Hong Kong University of Science and Technology, Hong Kong; The Hong Kong University of Science and Technology, Hong Kong; Saarland University, Saarbr?¬cken; The Hong Kong University of Science and Technology, Hong Kong; Sogang University, Seoul",IEEE Transactions on Software Engineering,,2011,37,3,430,447,"Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these ƒ??top crashesƒ? thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2011.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5711013,Top crash;machine learning;crash reports;social network analysis;data mining.,Fires;Feature extraction;Software;Testing;Computer bugs;Training,program debugging;software maintenance;software quality;system recovery,debugging;software systems;software failures;Firefox crash report databases;Thunderbird crash report databases;software quality;software maintenance,,35,59,,,,,,IEEE,IEEE Journals & Magazines
A Framework to Support Research in and Encourage Industrial Adoption of Regression Testing Techniques,J. M. Kauffman; G. M. Kapfhammer,NA; NA,"2012 IEEE Fifth International Conference on Software Testing, Verification and Validation",,2012,,,907,908,"When software developers make changes to a program, it is possible that they will introduce faults into previously working parts of the code. As software grows, a regression test suite is run to ensure that the old functionality still works as expected. Yet, as the number of test cases increases, it becomes more expensive to execute the test suite. Reduction and prioritization techniques enable developers to manage large and unwieldy test suites. However, practitioners and researchers do not always use and study these methods due, in part, to a lack of availability. In response to this issue, this paper describes an already released open-source framework that supports both research and practice in regression testing. The sharing of this framework will enable the replication of empirical studies in regression testing and encourage faster industrial adoption of these useful, yet rarely used, techniques.",2159-4848,978-0-7695-4670-4978-1-4577-1906,10.1109/ICST.2012.194,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200107,open-source framework;regression testing,Testing;Open source software;Monitoring;Timing;Algorithm design and analysis;Java,fault diagnosis;program testing;public domain software;regression analysis,industrial adoption;regression testing techniques;software developers;regression test suite;old functionality;test cases;reduction techniques;prioritization techniques;open-source framework,,,8,,,,,,IEEE,IEEE Conferences
A Heuristic Model-Based Test Prioritization Method for Regression Testing,X. Han; H. Zeng; H. Gao,NA; NA; NA,"2012 International Symposium on Computer, Consumer and Control",,2012,,,886,889,"Due to the resource and time constraints for re-executing large test suites in regression testing, developers are interested in detecting faults in the system as early as possible. Test case prioritization seeks to order test cases in such a way that early fault detection is maximized. In this paper, we present a model-based heuristic method to prioritize test cases for regression testing, which takes into account two types of information collected during execution of the modified model on the test suite. The experiment shows that our algorithm has better effectiveness of early fault detection.",,978-1-4673-0767-3978-0-7695-4655,10.1109/IS3C.2012.226,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228450,Test prioritization;regression testing;model-based test prioritization;early fault detection,Fault detection;Testing;Computational modeling;Minimization;Software maintenance;Software systems,fault diagnosis;program testing;software maintenance,heuristic model-based test prioritization method;regression testing;fault detection;large test suites;software maintenance,,1,9,,,,,,IEEE,IEEE Conferences
A Multi-Objective Technique to Prioritize Test Cases Based on Latent Semantic Indexing,M. M. Islam; A. Marchetto; A. Susi; G. Scanniello,NA; NA; NA; NA,2012 16th European Conference on Software Maintenance and Reengineering,,2012,,,21,30,"To early discover faults in source code, test case ordering has to be properly chosen. To this aim test prioritization techniques can be used. Several of these techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test prioritization technique that determines sequences of test cases that maximize the number of discovered faults that are both technical and business critical. The technique uses the information related to the code and requirements coverage, as well as the execution cost of each test case. The approach also uses recovered trace ability links among source code and system requirements via the Latent Semantic Indexing technique. We evaluated our proposal against both a random prioritization technique and two single-objective prioritization techniques on two Java applications. The results indicate that our proposal outperforms the baseline techniques and that additional improvements are still possible.",1534-5351,978-0-7695-4666-7978-1-4673-0984,10.1109/CSMR.2012.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6178873,Regression Testing;Requirements;Testing;Test Case Prioritization;Traceability,Software;Testing;Large scale integration;Weight measurement;Indexing;Semantics;Business,Java;program testing;software fault tolerance,test case prioritization;latent semantic indexing;fault discovery;source code;single objective function;multiobjective test prioritization technique;traceability links;system requirements;random prioritization technique;single-objective prioritization techniques;Java applications,,6,34,,,,,,IEEE,IEEE Conferences
A Remote Sensing Approach for Landslide Hazard Assessment on Engineered Slopes,P. E. Miller; J. P. Mills; S. L. Barr; S. J. Birkinshaw; A. J. Hardy; G. Parkin; S. J. Hall,"School of Civil Engineering and Geosciences, Newcastle University, Newcastle upon Tyne, U.K.; School of Civil Engineering and Geosciences, Newcastle University, Newcastle upon Tyne, U.K.; School of Civil Engineering and Geosciences, Newcastle University, Newcastle upon Tyne, U.K.; School of Civil Engineering and Geosciences, Newcastle University, Newcastle upon Tyne, U.K.; Institute of Geography and Earth Sciences, Aberystwyth University, Aberystwyth, U.K.; School of Civil Engineering and Geosciences, Newcastle University, Newcastle upon Tyne, U.K.; Network Rail (LNE), York, U.K.",IEEE Transactions on Geoscience and Remote Sensing,,2012,50,4,1048,1056,"Earthworks such as embankments and cuttings are integral to road and rail networks but can be prone to instability, necessitating rigorous and continual monitoring. To date, the potential of remote sensing for earthwork hazard assessment has been largely overlooked. However, techniques such as airborne laser scanning (ALS) are now ripe for addressing these challenges. This research presents the development of a novel hazard assessment strategy, combining high-resolution remote sensing with a numerical modeling approach. The research was implemented at a railway test site located in northern England, U.K.; ALS data and multispectral aerial imagery facilitated the determination of key slope stability variables, which were then used to parameterize a coupled hydrological-geotechnical model, in order to simulate slope behavior under current and future climates. A software toolset was developed to integrate the core elements of the methodology and determine resultant slope failure hazard which could then be mapped and queried within a geographical information system environment. Results indicate that the earthworks are largely stable, which is in broad agreement with the management company's slope hazard grading data, and in terms of morphological analysis, the remote methodology was able to correctly identify 99% of earthworks classed as embankments and 100% of cuttings. The developed approach provides an effective and practicable method for remotely quantifying slope failure hazard at fine spatial scales (0.5 m) and for prioritizing and reducing on-site inspection.",0196-2892;1558-0644,,10.1109/TGRS.2011.2165547,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6032742,Airborne laser scanning (ALS);hazard mapping;railway engineering;remote sensing,Hazards;Meteorology;Remote sensing;Stability analysis;Rail transportation;Numerical stability;Vegetation mapping,geographic information systems;geomorphology;geophysical techniques;geotechnical engineering;hazardous areas;numerical analysis;railways;remote sensing;roads,remote sensing approach;landslide hazard assessment;engineered slopes;earthworks;embankments;cuttings;road networks;rail networks;airborne laser scanning;numerical modeling approach;northern England;U.K;ALS data;multispectral aerial imagery;coupled hydrological-geotechnical model;slope behavior;software toolset;slope failure hazard;geographical information system;morphological analysis,,12,38,,,,,,IEEE,IEEE Journals & Magazines
"A Scalable, Lightweight WebOS Application Framework",D. A. Ostrowski,NA,2012 IEEE First International Conference on Internet Operating Systems,,2012,,,5,8,"Frequently, as applications scale, they are considered in the context of a web OS-based architecture. In support of this goal, we present a lightweight framework designed as a middleware application. This architecture is highly influenced by hypermedia-based techniques, leveraging metadata in the context of HTML5. Our framework relies on a novel incorporation of a number of open source technologies including node.js and couchDB to support priorities of fast-prototyping, scalability and maintainability. Initial experiments have demonstrated that our approach performs effectively among the dynamics of our environment.",,978-1-4673-5092-1978-0-7695-4936,10.1109/ICIOS.2012.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424523,Web OS;Internet Operating System;Hyperdata API;metadata,Computer architecture;Testing;Servers;Databases;Conferences;Internet;Security,hypermedia markup languages;Internet;meta data;middleware;network operating systems;software prototyping,lightweight WebOS application framework;Web OS-based architecture;middleware application;hypermedia-based techniques;metadata leveraging;HTML5;open source technologies;node.js;couchDB,,,43,,,,,,IEEE,IEEE Conferences
A Static Approach to Prioritizing JUnit Test Cases,H. Mei; D. Hao; L. Zhang; L. Zhang; J. Zhou; G. Rothermel,"Peking University, Beijing; Peking University, Beijing; Peking University, Beijing; Peking University, Beijing; Peking University, Beijing; University of Nebraska, Lincoln",IEEE Transactions on Software Engineering,,2012,38,6,1258,1275,"Test case prioritization is used in regression testing to schedule the execution order of test cases so as to expose faults earlier in testing. Over the past few years, many test case prioritization techniques have been proposed in the literature. Most of these techniques require data on dynamic execution in the form of code coverage information for test cases. However, the collection of dynamic code coverage information on test cases has several associated drawbacks including cost increases and reduction in prioritization precision. In this paper, we propose an approach to prioritizing test cases in the absence of coverage information that operates on Java programs tested under the JUnit framework-an increasingly popular class of systems. Our approach, JUnit test case Prioritization Techniques operating in the Absence of coverage information (JUPTA), analyzes the static call graphs of JUnit test cases and the program under test to estimate the ability of each test case to achieve code coverage, and then schedules the order of these test cases based on those estimates. To evaluate the effectiveness of JUPTA, we conducted an empirical study on 19 versions of four Java programs ranging from 2K-80K lines of code, and compared several variants of JUPTA with three control techniques, and several other existing dynamic coverage-based test case prioritization techniques, assessing the abilities of the techniques to increase the rate of fault detection of test suites. Our results show that the test suites constructed by JUPTA are more effective than those in random and untreated test orders in terms of fault-detection effectiveness. Although the test suites constructed by dynamic coverage-based techniques retain fault-detection effectiveness advantages, the fault-detection effectiveness of the test suites constructed by JUPTA is close to that of the test suites constructed by those techniques, and the fault-detection effectiveness of the test suites constructed by some of JUPTA's variants is better than that of the test suites constructed by several of those techniques.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2011.106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6363461,Software testing;regression testing;test case prioritization;JUnit;call graph,Software testing;Regression analysis;Scheduling,Java;program testing;regression analysis;software fault tolerance,static approach;regression testing;test case prioritization techniques;dynamic code coverage information;Java programs;JUnit test case prioritization techniques operating in the absence of coverage information;JUPTA;static call graphs;fault-detection effectiveness;dynamic coverage-based techniques,,47,44,,,,,,IEEE,IEEE Journals & Magazines
A Two-Level Prioritization Approach for Regression Testing of Web Applications,D. Garg; A. Datta; T. French,NA; NA; NA,2012 19th Asia-Pacific Software Engineering Conference,,2012,2,,150,153,A test case prioritization technique reschedules test cases for regression testing in an order to achieve specific goals like early fault detection. We propose a new two level prioritization approach to prioritize test cases for web applications as a whole. Our approach automatically selects modified functionalities in a web application and executes test cases on the basis of the impact of modified functionalities. We suggest several new prioritization strategies for web applications and examine whether these prioritization strategies improve the rate of fault detection for web applications. We propose a new automated test suite prioritization model for web applications that selects test cases related to modified functionalities and reschedules them using our new prioritization strategies to detect faults early in test suite execution.,1530-1362;1530-1362;1530-1362,978-1-4673-4930-7978-0-7695-4922,10.1109/APSEC.2012.34,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462796,Regression testing;Test case prioritization;Web applications;Two level prioritization,Unified modeling language;Software;Conferences;Software testing;Software engineering;Educational institutions,Internet;program testing;regression analysis,two-level prioritization;regression testing;Web applications;test case prioritization,,2,13,,,,,,IEEE,IEEE Conferences
Appropriate placement of series compensators to improve transient stability of power system,A. Nasri; M. Ghandhari; R. Eriksson,"Department of Electric Power Systems, School of Electrical Engineering at KTH Royal Institute of Technology, Sweden; Department of Electric Power Systems, School of Electrical Engineering at KTH Royal Institute of Technology, Sweden; Department of Electric Power Systems, School of Electrical Engineering at KTH Royal Institute of Technology, Sweden",IEEE PES Innovative Smart Grid Technologies,,2012,,,1,6,"Trajectory sensitivity analysis is used to find the best places for installation of thyristor controlled series capacitors (TCSC) to improve transient stability of the power system. Based on the rotor angles of generators, an equivalent angle (??<sub>eq</sub>) is defined by determining accelerating and decelerating machines, and then using trajectory sensitivities of this angle with respect to the impedances of the transmission lines in the post-fault system, appropriate locations for placing TCSC will be found. Severity of the faults is also considered in this calculation. This method is applied to the IEEE 3-machine 9-bus test system to find the priorities of the transmission lines for installation of TCSC. Simulation with industrial software verifies the obtained results.",2378-8534;2378-8542,978-1-4673-1220-2978-1-4673-1221-9978-1-4673-1219,10.1109/ISGT-Asia.2012.6303243,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6303243,Trajectory sensitivity analysis (TSA);Thyristor Controlled Series Capacitor (TCSC);Transient Stability;Critical Clearing Time (CCT),Power system stability;Trajectory;Circuit faults;Transient analysis;Sensitivity;Power transmission lines,electric generators;IEEE standards;power system transient stability;power transmission lines;rotors;thyristor applications,series compensators;power system improve transient stability;thyristor controlled series capacitors;TCSC;generator rotor angles;decelerating machines;accelerating machines;transmission lines;IEEE 3-machine 9-bus test system,,2,13,,,,,,IEEE,IEEE Conferences
Automated prediction of defect severity based on codifying design knowledge using ontologies,M. Iliev; B. Karasneh; M. R. V. Chaudron; E. Essenius,"Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands; Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands; Leiden Institute of Advanced Computer Science, Leiden University, Leiden, The Netherlands; Technical Software Engineering, Logica Nederland B. V., Rotterdam, The Netherlands",2012 First International Workshop on Realizing AI Synergies in Software Engineering (RAISE),,2012,,,7,11,"Assessing severity of software defects is essential for prioritizing fixing activities as well as for assessing whether the quality level of a software system is good enough for release. In filling out defect reports, developers routinely fill out default values for the severity levels. The purpose of this research is to automate the prediction of defect severity. Our aim is to research how this severity prediction can be achieved through reasoning about the requirements and the design of a system using ontologies. In this paper we outline our approach based on an industrial case study.",,978-1-4673-1753-5978-1-4673-1752,10.1109/RAISE.2012.6227962,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6227962,severity;defect;design;ontology;automatic classification,Ontologies;Testing;IEEE standards;Cognition;Software systems,inference mechanisms;ontologies (artificial intelligence);program compilers;software quality,automated defect severity prediction;design knowledge codification;ontologies;software defects severity;fixing activities;software system quality level;reasoning;industrial case study,,3,7,,,,,,IEEE,IEEE Conferences
Case based reasoning approach for adaptive test suite optimization,B. Narendra Kumar Rao; A. RamaMohan Reddy,"Dept. of CSE, SVEC, Tirupati, Andhra Pradesh, India; Dept. of CSE, SVEC, Tirupati, Andhra Pradesh, India","2012 Third International Conference on Computing, Communication and Networking Technologies (ICCCNT'12)",,2012,,,1,5,"Case-based reasoning is an approach to problem solving and learning that has got a lot of attention over the last few years. This paper provides an overview of the foundational issues related to case-based reasoning, describing some of the leading methodological approaches within the field, and exemplifying the current state through pointers to some systems. The framework influences the recent methodologies for knowledge level descriptions of intelligent systems. The methods for case retrieval reuse, solution testing, and learning are summarized, and realization is discussed with few example systems that represent different CBR approaches. Regression testing occurs during the maintenance stage of the software life cycle, however, it requires large amounts of test cases to assure the attainment of a certain degree of quality. So, test suite sizes may grow significantly. This paper focuses primarily on application of CBR to test suite optimization.",,,10.1109/ICCCNT.2012.6395870,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6395870,Retrieve;Reuse;Revise;Retain;Test suite reduction;Test case Prioritization;Test suite Optimization,Optimization;Maintenance engineering;Algorithm design and analysis;Manuals;Computers,case-based reasoning;learning (artificial intelligence);problem solving;program testing;regression analysis;software maintenance;statistical testing,adaptive test suite optimization;problem solving;case-based reasoning approach;learning;methodological approaches;knowledge level descriptions;intelligent systems;case retrieval reuse methods;solution testing;CBR approaches;regression testing;software life cycle,,,17,,,,,,IEEE,IEEE Conferences
Code coverage-based regression test selection and prioritization in WebKit,?. Besz??des; T. Gergely; L. Schrettner; J. J?­sz; L. Lang??; T. Gyim??thy,"University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary; University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary; University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary; University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary; University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary; University of Szeged, Department of Software Engineering & HAS Research Group on AI, Szeged, Hungary",2012 28th IEEE International Conference on Software Maintenance (ICSM),,2012,,,46,55,"Automated regression testing is often crucial in order to maintain the quality of a continuously evolving software system. However, in many cases regression test suites tend to grow too large to be suitable for full re-execution at each change of the software. In this case selective retesting can be applied to reduce the testing cost while maintaining similar defect detection capability. One of the basic test selection methods is the one based on code coverage information, where only those tests are included that cover some parts of the changes. We experimentally applied this method to the open source web browser engine project WebKit to find out the technical difficulties and the expected benefits if this method is to be introduced into the actual build process. Although the principle is simple, we had to solve a number of technical issues, so we report how this method was adapted to be used in the official build environment. Second, we present results about the selection capabilities for a selected set of revisions of WebKit, which are promising. We also applied different test case prioritization strategies to further reduce the number of tests to execute. We explain these strategies and compare their usefulness in terms of defect detection and test suite reduction.",1063-6773,978-1-4673-2312-3978-1-4673-2313,10.1109/ICSM.2012.6405252,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405252,Regression testing;test case selection;code coverage;test prioritization;test quality;WebKit,Testing;Databases;Reliability;Conferences;Software maintenance;Communities;Instruments,Internet;program testing;public domain software;regression analysis,code coverage-based regression testing;regression test selection;regression test prioritization;WebKit;continuously evolving software system;software change;selective retesting;testing cost reduction;defect detection capability;code coverage information;open source web browser engine project;test suite reduction,,7,16,,,,,,IEEE,IEEE Conferences
Dependency-Based Test Case Selection and Prioritization in Embedded Systems,P. Caliebe; T. Herpel; R. German,NA; NA; NA,"2012 IEEE Fifth International Conference on Software Testing, Verification and Validation",,2012,,,731,735,"Embedded systems in automotive engineering are getting more and more complex due to a higher rate of integration and shared usage of sensor signals. A common solution to testing these systems is deriving test cases from models, so called model-based testing. In practice, generated test suites are typically very huge and have to be reduced by methods of regression-test selection and prioritization. In our field of application, we additionally suffer from the lack of knowledge on system internals like the source code. Therefore, our approach is based on dependences between the components of embedded systems. The model we use is derived from the system architecture and system requirements. We are using graph algorithms for selecting and prioritizing test cases to run on a certain system version. First statistical evaluations and the current implementation already have shown promising reduction of test-cases for regression testing.",2159-4848,978-0-7695-4670-4978-1-4577-1906,10.1109/ICST.2012.164,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200176,Software Testing;Component-Based;Dependences;Structure-Based;Regression Testing;Verification;Validation,Testing;Unified modeling language;Automotive engineering;Embedded systems;Integrated circuit modeling,automotive engineering;embedded systems;graph theory;mechanical engineering computing;program testing;regression analysis,dependency-based test case selection;embedded systems;automotive engineering;test cases;model-based testing;regression-test selection;system architecture;system requirements;graph algorithms;regression testing,,10,18,,,,,,IEEE,IEEE Conferences
Diffusion of Software Features: An Exploratory Study,F. Thung; D. Lo; L. Jiang,NA; NA; NA,2012 19th Asia-Pacific Software Engineering Conference,,2012,1,,368,373,"New features are frequently proposed in many software libraries. These features include new methods, classes, packages, etc. These features are utilized in many open source and commercial software systems. Some of these features are adopted very quickly, while others take a long time to be adopted. Each feature takes much resource to develop, test, and document. Library developers and managers need to decide what feature to prioritize and what to develop next. As a first step to aid these stakeholders, we perform an exploratory study on the diffusion or rate of adoption of features in Java Development Kit (JDK) library. Our empirical study proposes such questions as how many new features are adopted by client applications, how long it takes for a new feature to spread to various software products, what features are diffused quickly, and what features are diffused widely. We perform an exploratory study with new features in Java Development Kit (JDK, from version 1.3 to 1.6) and provide empirical findings to answer the above research questions.",1530-1362;1530-1362;1530-1362,978-1-4673-4930-7978-0-7695-4922,10.1109/APSEC.2012.139,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462682,Diffusion;Exploratory Study;Empirical Software Engineering,Feature extraction;Java;Software;XML;Software libraries;Communities,Java;public domain software;software libraries,software feature diffusion;software libraries;open source software systems;commercial software systems;library developers;library managers;Java development kit library;JDK;software products,,,26,,,,,,IEEE,IEEE Conferences
Diversity maximization speedup for fault localization,L. Gong; D. Lo; L. Jiang; H. Zhang,"Tsinghua University, China; Singapore Management University, Singapore; Singapore Management University, Singapore; Tsinghua University, China",2012 Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering,,2012,,,30,39,"Fault localization is useful for reducing debugging effort. However, many fault localization techniques require non-trivial number of test cases with oracles, which can determine whether a program behaves correctly for every test input. Test oracle creation is expensive because it can take much manual labeling effort. Given a number of test cases to be executed, it is challenging to minimize the number of test cases requiring manual labeling and in the meantime achieve good fault localization accuracy. To address this challenge, this paper presents a novel test case selection strategy based on Diversity Maximization Speedup (DMS). DMS orders a set of unlabeled test cases in a way that maximizes the effectiveness of a fault localization technique. Developers are only expected to label a much smaller number of test cases along this ordering to achieve good fault localization results. Our experiments with more than 250 bugs from the Software-artifact Infrastructure Repository show (1) that DMS can help existing fault localization techniques to achieve comparable accuracy with on average 67% fewer labeled test cases than previously best test case prioritization techniques, and (2) that given a labeling budget (i.e., a fixed number of labeled test cases), DMS can help existing fault localization techniques reduce their debugging cost (in terms of the amount of code needed to be inspected to locate faults). We conduct hypothesis test and show that the saving of the debugging cost we achieve for the real C programs are statistically significant.",,978-1-4503-1204-2978-1-4503-1204,10.1145/2351676.2351682,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494903,Fault Localization;Test Case Prioritization,,C language;program debugging;program testing,C programs;test case prioritization techniques;software-artifact infrastructure repository;DMS;test case selection strategy;manual labeling;test oracle creation;fault localization techniques;debugging effort reduction;diversity maximization speedup,,7,32,,,,,,IEEE,IEEE Conferences
Dynamic Fault Visualization Tool for Fault-based Testing and Prioritization,P. Daniel; K. Y. Sim,NA; NA,2012 International Conference on Advanced Computer Science Applications and Technologies (ACSAT),,2012,,,301,306,"Fault-based testing has been proven to be a cost effective testing technique for software logics and rules expressed in Boolean expressions. It can guarantee the elimination of common faults without exhaustive testing. However, average software testing practitioners may not have in-depth knowledge on Boolean algebra and complex logic derivations required to apply existing fault-based testing techniques. In this paper, a dynamic fault visualization tool has been proposed. This tool allows its user to visualize fault-based testing and prioritize test inputs with a simple greedy method. The performance evaluation of this tool has been done on Boolean expressions extracted from a real life aviation tool. The results show that it can achieve significant performance improvements compared to ordinary sequential order test execution and existing static technique. The proposed visualization tool could also identify possible faults to guide the debugging process.",,978-0-7695-4959-0978-1-4673-5832,10.1109/ACSAT.2012.55,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6516370,Software Testing;Fault-based Testing;Fault Visualization,,data visualisation;program debugging;program testing;software fault tolerance,dynamic fault visualization tool;fault-based testing;fault-based prioritization;software logic;software rule;Boolean expression;Boolean algebra;complex logic derivation;greedy method;aviation tool;sequential order test execution;static technique;debugging process,,1,14,,,,,,IEEE,IEEE Conferences
G-RankTest: Regression testing of controller applications,L. Mariani; O. Riganelli; M. Santoro; M. Ali,"University of Milano Bicocca, Viale Sarca, 336, Italy; University of Milano Bicocca, Viale Sarca, 336, Italy; University of Milano Bicocca, Viale Sarca, 336, Italy; VTT Technical Research Centre of Finland, Tekniikankatu 1, FI-33101, Tampere, Finland",2012 7th International Workshop on Automation of Software Test (AST),,2012,,,131,137,"Since controller applications must typically satisfy real-time constraints while manipulating real-world variables, their implementation often results in programs that run extremely fast and manipulate numerical inputs and outputs. These characteristics make them particularly suitable for test case generation. In fact a number of test cases can be easily created, due to the simplicity of numerical inputs, and executed, due to the speed of computations. In this paper we present G-RankTest, a technique for test case generation and prioritization. The key idea is that test case generation can run for long sessions (e.g., days) to accurately sample the behavior of a controller application and then the generated test cases can be prioritized according to different strategies, and used for regression testing every time the application is modified. In this work we investigate the feasibility of using the gradient of the output as a criterion for selecting the test cases that activate the most tricky behaviors, which we expect easier to break when a change occurs, and thus deserve priority in regression testing.",,978-1-4673-1822-8978-1-4673-1821,10.1109/IWAST.2012.6228981,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228981,regression testing;test case prioritization;test case generation;test automation,Testing;Control systems;Embedded software;Input variables;Coordinate measuring machines;Real time systems,control engineering computing;gradient methods;program testing;regression analysis;statistical testing,G-RankTest;regression testing;controller applications;variable manipulation;numerical input manipulation;numerical output manipulation;test case generation;test case prioritization;output gradient;test case selection criterion,,1,15,,,,,,IEEE,IEEE Conferences
Graph-Based Optimization Algorithm and Software on Kidney Exchanges,Y. Chen; Y. Li; J. D. Kalbfleisch; Y. Zhou; A. Leichtman; P. X. -. Song,"Department of Biostatistics, University of Michigan, Ann Arbor, USA; Department of Biostatistics, University of Michigan, Ann Arbor, USA; Department of Biostatistics, University of Michigan, Ann Arbor, USA; Department of Biostatistics, University of Michigan, Ann Arbor, USA; Department of Internal Medicine , University of Michigan, Ann Arbor, USA; Department of Biostatistics, University of Michigan, Ann Arbor, USA",IEEE Transactions on Biomedical Engineering,,2012,59,7,1985,1991,"Kidney transplantation is typically the most effective treatment for patients with end-stage renal disease. However, the supply of kidneys is far short of the fast-growing demand. Kidney paired donation (KPD) programs provide an innovative approach for increasing the number of available kidneys. In a KPD program, willing but incompatible donor-candidate pairs may exchange donor organs to achieve mutual benefit. Recently, research on exchanges initiated by altruistic donors (ADs) has attracted great attention because the resultant organ exchange mechanisms offer advantages that increase the effectiveness of KPD programs. Currently, most KPD programs focus on rule-based strategies of prioritizing kidney donation. In this paper, we consider and compare two graph-based organ allocation algorithms to optimize an outcome-based strategy defined by the overall expected utility of kidney exchanges in a KPD program with both incompatible pairs and ADs. We develop an interactive software-based decision support system to model, monitor, and visualize a conceptual KPD program, which aims to assist clinicians in the evaluation of different kidney allocation strategies. Using this system, we demonstrate empirically that an outcome-based strategy for kidney exchanges leads to improvement in both the quantity and quality of kidney transplantation through comprehensive simulation experiments.",0018-9294;1558-2531,,10.1109/TBME.2012.2195663,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6188515,Kidney exchanges;optimal matches;software,Kidney;Resource management;Computational modeling;Optimal matching;Blood;Uncertainty;Graphical user interfaces,decision support systems;diseases;graph theory;interactive systems;kidney;knowledge based systems;medical computing;optimisation;patient monitoring;patient treatment,graph-based optimization algorithm;kidney exchanges;kidney transplantation;patient treatment;end-stage renal disease;kidney paired donation programs;altruistic donors;organ exchange mechanisms;rule-based strategy;graph-based organ allocation algorithms;outcome-based strategy;interactive software-based decision support system;conceptual KPD program,"Algorithms;Databases, Factual;Histocompatibility Testing;Humans;Kidney Transplantation;Living Donors;Medical Informatics Applications;Software;User-Computer Interface",11,19,,,,,,IEEE,IEEE Journals & Magazines
Guiding Testing Activities by Predicting Defect-Prone Parts Using Product and Inspection Metrics,F. Elberzhager; S. Kremer; J. M?¬nch; D. Assmann,NA; NA; NA; NA,2012 38th Euromicro Conference on Software Engineering and Advanced Applications,,2012,,,406,413,"Product metrics, such as size or complexity, are often used to identify defect-prone parts or to focus quality assurance activities. In contrast, quality information that is available early, such as information provided by inspections, is usually not used. Currently, only little experience is documented in the literature on whether data from early defect detection activities can support the identification of defect prone parts later in the development process. This article compares selected product and inspection metrics commonly used to predict defect-prone parts. Based on initial experience from two case studies performed in different environments, the suitability of different metrics for predicting defect-prone parts is illustrated. These studies revealed that inspection defect data seems to be a suitable predictor, and a combination of certain inspection and product metrics led to the best prioritizations in our contexts.",1089-6503;2376-9505,978-0-7695-4790-9978-1-4673-2451,10.1109/SEAA.2012.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328182,inspection metrics;product metrics;comparison;case study;focusing,Measurement;Inspection;Testing;Complexity theory;Context;Focusing;Quality assurance,inspection;program testing;quality assurance;software metrics;software quality,defect-prone parts prediction;product metrics;inspection metrics;defect-prone parts identification;quality assurance;quality information;early defect detection activities;inspection defect data,,4,8,,,,,,IEEE,IEEE Conferences
High Performance Memory Requests Scheduling Technique for Multicore Processors,W. El-Reedy; A. A. El-Moursy; H. A. H. Fahmy,NA; NA; NA,2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems,,2012,,,127,134,"In modern computer systems, long memory latency is one of the main bottlenecks micro-architects are facing for leveraging the system performance especially for memory-intensive applications. This emphasises the importance of the memory access scheduling to efficiently utilize memory bandwidth. Moreover, in recent micro-processors, multithread and multicore is turned to be the default choice for their design. This resulted in more contention on memory. Hence, the effect of memory access scheduling schemes is more critical to the overall performance boost. Although memory access scheduling techniques have been recently proposed for performance improvement, most of them have overlooked the fairness among the running applications. Achieving both high-throughput and fairness simultaneously is challenging. In this paper, we focus on the basic idea of memory requests scheduling, which includes how to assign priorities to threads, what request should be served first, and how to achieve fairness among the running applications for multicore microprocessors. We propose two new memory access scheduling techniques FLRMR, and FIQMR. Compared to recently proposed techniques, on average, FLRMR achieves 8.64% speedup relative to LREQ algorithm, and FIQMR achieves 11.34% speedup relative to IQ-based algorithm. FLRMR outperforms the best of the other techniques by 8.1% in 8-cores workloads. Moreover, FLRMR improves fairness over LREQ by 77.2% on average.",,978-1-4673-2164-8978-0-7695-4749,10.1109/HPCC.2012.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332168,Computer architecture;Memory management;Multicore processing,Instruction sets;Benchmark testing;Multicore processing;Throughput;Scheduling;Scheduling algorithms,coprocessors;multiprocessing systems;multi-threading;processor scheduling;storage management,high performance memory requests scheduling;multicore microprocessor;memory latency;microarchitect;memory intensive application;memory access scheduling;memory bandwidth;multithread processor;throughput;priority assignment;FLRMR;FIQMR,,3,18,,,,,,IEEE,IEEE Conferences
Impact Analysis in the Presence of Dependence Clusters Using Static Execute after in WebKit,L. Schrettner; J. J?­sz; T. Gergely; ?. Besz??des; T. Gyim??thy,NA; NA; NA; NA; NA,2012 IEEE 12th International Working Conference on Source Code Analysis and Manipulation,,2012,,,24,33,"Impact analysis based on code dependence can be an integral part of software quality assurance by providing opportunities to identify those parts of the software system that are affected by a change. Because changes usually have far reaching effects in programs, effective and efficient impact analysis is vital, which has different applications including change propagation and regression testing. Static Execute After (SEA) is a relation on program elements (procedures) that is efficiently computable and accurate enough to be a candidate for use in impact analysis in practice. To assess the applicability of SEA in terms of capturing real defects, we present results on integrating it into the build system of Web Kit, a large, open source software system, and on related experiments. We show that a large number of real defects can be captured by impact sets computed by SEA, albeit many of them are large. We demonstrate that this is not an issue in applying it to regression test prioritization, but generally it can be an obstacle in the path to efficient use of impact analysis. We believe that the main reason for large impact sets is the formation of dependence clusters in code. As apparently dependence clusters cannot be easily avoided in the majority of cases, we focus on determining the effects these clusters have on impact analysis.",,978-0-7695-4783-1978-1-4673-2398,10.1109/SCAM.2012.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392099,Change impact analysis;Source code analysis;Static Execute After;Regression testing;Dependence clusters,Testing;Algorithm design and analysis;Software systems;Software algorithms;Sea measurements;Approximation methods,Internet;software quality,impact analysis;dependence clusters;WebKit;code dependence;software quality assurance;software system;static execute after;SEA,,3,29,,,,,,IEEE,IEEE Conferences
Investment optimization methodology applied to investments on non-technical losses reduction actions,S. S. Ribeiro; T. Cazes; R. F. Mano; D. Maia,"Choice Technologies S.A., Rio de Janeiro, Brazil; NA; NA; NA",2012 IEEE Symposium on Computers and Communications (ISCC),,2012,,,354,360,"This electronic document presents the research developed on a R&D Project for the Energy Recovery Department of Rio de Janeiro's distribution company, Light S.E.S.A. The main purpose is prioritizing the grid investments on non-technical losses reduction actions. The work was developed along eighteen months, and resulted in experimental software. It analyses the historical results of the actions, allocating optimally the resources to a pre-defined period of time. The methodology consists of a statistic model based on historical results processed through a decision three optimization algorithm in order to maximize the objective function. It was tested on decision making process regarding grid investments to reduce non-technical losses, and the Return-on-Investment results were quite satisfactory.",1530-1346;1530-1346,978-1-4673-2713-8978-1-4673-2712-1978-1-4673-2711,10.1109/ISCC.2012.6249321,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249321,optimization;statistic models energy losses recovery;energy theft;decision support methods,Investments;Inspection;Optimization;Estimation;Software;Databases;Geospatial analysis,electricity supply industry;investment;power distribution economics;power grids;statistical analysis,investment optimization methodology;nontechnical losses reduction actions;electronic document;R&D Project;Rio de Janeiro;distribution company;grid investments;pre-defined period of time;statistic model;optimization algorithm,,2,15,,,,,,IEEE,IEEE Conferences
Log-based approach for performance requirements elicitation and prioritization,O. M. Mendizabal; M. Spier; R. Saad,"Centro de Ci?¦ncias Computacionais, Universidade Federal do Rio Grande - FURG, Rio Grande, Brazil; Expedia Worldwide Engineering, Expedia Inc., Bellevue, WA, USA; Performance Engineering & Testing Practice, Dell Inc, Porto Alegre, Brazil",2012 20th IEEE International Requirements Engineering Conference (RE),,2012,,,297,302,"Requirements engineering activities are a critical part of a project's lifecycle. Success of subsequent project phases is highly dependent on good requirements definition. However, eliciting and achieving consensus on priority between all stakeholders is a complex task. Considering software development of large scale global applications, the challenges increase by the need of managing discussions between groups of stakeholders with different roles and background. This paper presents a practical approach for requirements elicitation and prioritization based on realistic user behaviors observation. It uses basic statistic analysis and application usage information to automatically identify the most relevant requirements for majority of stakeholders. An industry case illustrates the feasibility and efficiency of our approach.",2332-6441;1090-705X;1090-750X,978-1-4673-2785-5978-1-4673-2783-1978-1-4673-2784,10.1109/RE.2012.6345818,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345818,Performance Requirements;Requirements Elicitation;Requirements Prioritization;Case Study,Time factors;Servers;Companies;Measurement;Software;Tuning,formal verification;software performance evaluation;statistical analysis,log-based approach;performance requirements elicitation;requirements engineering activities;good requirements definition;software development;large scale global applications;requirements prioritization;realistic user behaviors observation;basic statistic analysis;application usage information,,1,13,,,,,,IEEE,IEEE Conferences
Managing Software Quality Requirements,L. B. Phillips; A. Aurum; R. B. Svensson,NA; NA; NA,2012 38th Euromicro Conference on Software Engineering and Advanced Applications,,2012,,,349,356,"This research study explores current quality requirements (QR) management practices in Australian organisations focusing on the elicitation, handling processes, challenges faced, quantification methods used and interdependency management. This research was conducted through six mini case studies, examining organizations that varied in size, structure, industry and function. A mixed methodology was utilised through an online survey for gathering quantitative data and semi-structured interviews for gathering explanatory qualitative data. The results found that five out of the six organisations studied did not have a formal and defined process for the handling of QRs. Large organisations treated QRs are part of their overall project specifications, while smaller organisations saw the management of QRs as more ad hoc. When prioritising QRs, Accuracy was considered the most important priority followed by Security and Reliability. The main challenges that organisations face in their management of QRs is defining and quantifying these requirements.",1089-6503;2376-9505,978-0-7695-4790-9978-1-4673-2451,10.1109/SEAA.2012.65,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328174,Software quality requirements;non-functional requirements;industrial practice;prioritization,Companies;Software;Interviews;Accuracy;Testing;Industries,software quality;systems analysis,software quality requirements;QR management practices;Australian organisations;mixed methodology;semi-structured interviews;quantitative data,,3,27,,,,,,IEEE,IEEE Conferences
Modular based multiple test case prioritization,N. Prakash; T. R. Rangaswamy,"Department of Information Technology, B.S.A Crescent Engineering College, Chennai, India; Dean Academic, B.S.A Crescent Engineering College, Chennai, India",2012 IEEE International Conference on Computational Intelligence and Computing Research,,2012,,,1,7,"Cost and time effective reliable test case prioritization technique is the need for present software industries. The test case prioritization for the entire program consumes more time and the selection of test case for entire software is also affecting the test performance. In order to alleviate the above problem a new methodology using modular based test case prioritization is proposed for regression testing. In this method the program is divided into multiple modules. The test cases corresponding to each module is prioritized first. In the second stage, the individual modular based prioritized test suites are combined together and further prioritized for the whole program. This method is verified for fault coverage and compared with overall program test case prioritization method. The proposed method is assessed using three standard applications namely University Students Monitoring System, Hospital Management System, and Industrial Process Operation System. The empirical studies show that the proposed algorithm is significantly performed well. The superiority of the proposed method is also highlighted.",,978-1-4673-1344-5978-1-4673-1342-1978-1-4673-1343,10.1109/ICCIC.2012.6510205,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510205,Modular program;regression testing;test case prioritization,,program testing;regression analysis;statistical testing,industrial process operation system;hospital management system;university students monitoring system;program test case prioritization method;fault coverage;regression testing;test performance;program test;software industry;modular based test case prioritization,,,17,,,,,,IEEE,IEEE Conferences
MOTCP: A tool for the prioritization of test cases based on a sorting genetic algorithm and Latent Semantic Indexing,M. M. Islam; A. Marchetto; A. Susi; F. B. Kessler; G. Scanniello,"Trento, Italy; Trento, Italy; Trento, Italy; Trento, Italy; Dipartimento di Matematica e Informatica, Universit&#x00E0; della Basilicata, Potenza, Italy",2012 28th IEEE International Conference on Software Maintenance (ICSM),,2012,,,654,657,"Test prioritization techniques can be used to determine test case ordering and early discover faults in source code. Several of these techniques exploit a single objective function, e.g., code or requirements coverage. In this tool demo paper, we present MOTCP, a software tool that implements a multi-objective test prioritization technique based on the information related to the code and requirements coverage, as well as the execution cost of each test case. To establish users' and system requirements coverage, the MOTCP uses Latent Semantic Indexing to recover traceability links among application source code and requirements specifications. The test case ordering is then obtained by applying a non-dominated sorting genetic algorithm.",1063-6773,978-1-4673-2312-3978-1-4673-2313,10.1109/ICSM.2012.6405346,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405346,Genetic algorithms;multi-objective Optimization;test prioritization techniques;traceability,Conferences;Software maintenance;Genetic algorithms;Optimization;Sociology;Statistics,genetic algorithms;indexing;Pareto optimisation;program testing;software fault tolerance;software tools;sorting,MOTCP tool;software tool;sorting genetic algorithm;latent semantic indexing;test case ordering;early fault discovery;objective function;multiobjective test case prioritization technique;system requirements coverage;execution cost;traceability link recovery;application source code;software testing;multiobjective optimization,,3,15,,,,,,IEEE,IEEE Conferences
On Capturing Effects of Modifications as Data Dependencies,H. Ural; H. Yenig?¬n,NA; NA,2012 IEEE 36th Annual Computer Software and Applications Conference,,2012,,,350,351,"Dependence analysis on an Extended Finite State Machine (EFSM) representation of the requirements of a system under test has been used in requirements-based regression testing for regression test suite (RTS) reduction (reducing the size of a given test suite by eliminating redundancies), for RTS prioritization (ordering test cases in a given test suite for early fault detection) or for RTS selection (selecting a subset of a test suite covering the identified dependencies). These particular uses of dependence analysis are based on definitions of various types of control and data dependencies (between transitions in an EFSM) caused by a given set of modifications on the requirements. This abstract considers the definitions of data dependencies, gives examples of incompleteness of existing definitions, and presents insights on completing these definitions.",0730-3157;0730-3157;0730-3157,978-1-4673-1990-4978-0-7695-4736,10.1109/COMPSAC.2012.50,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340172,model-based testing;regression testing;data dependence,Testing;Analytical models;Data models;Educational institutions;Abstracts;Software maintenance;Conferences,data reduction;finite state machines;program testing;regression analysis,data dependency;extended finite state machine;requirements-based regression testing;regression test suite;RTS reduction;RTS prioritization;RTS selection;EFSM;system under test,,,2,,,,,,IEEE,IEEE Conferences
On the Fault-Detection Capabilities of Adaptive Random Test Case Prioritization: Case Studies with Large Test Suites,Z. Q. Zhou; A. Sinaga; W. Susilo,NA; NA; NA,2012 45th Hawaii International Conference on System Sciences,,2012,,,5584,5593,"An adaptive random (AR) testing strategy has recently been developed and examined by a growing body of research. More recently, this strategy has been applied to prioritizing regression test cases based on code coverage using the concepts of Jaccard Distance (JD) and Coverage Manhattan Distance (CMD). Code coverage, however, does not consider frequency, furthermore, comparison between JD and CMD has not yet been made. This research fills the gap by first investigating the fault-detection capabilities of using frequency information for AR test case prioritization, and then comparing JD and CMD. Experimental results show that ""coverage"" was more useful than ""frequency"" although the latter can sometimes complement the former, and that CMD was superior to JD. It is also found that, for certain faults, the conventional ""additional"" algorithm (widely accepted as one of the best algorithms for test case prioritization) could perform much worse than random testing on large test suites.",1530-1605;1530-1605,978-1-4577-1925-7978-0-7695-4525,10.1109/HICSS.2012.454,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6149572,,Subspace constraints;Frequency measurement;Software;Power capacitors;Software testing;Educational institutions,fault diagnosis;program testing,fault-detection capabilities;adaptive random test case prioritization;adaptive random testing strategy;code coverage;Jaccard distance concept;coverage Manhattan distance;frequency information;AR test case prioritization,,10,29,,,,,,IEEE,IEEE Conferences
Oracle-Centric Test Case Prioritization,M. Staats; P. Loyola; G. Rothermel,NA; NA; NA,2012 IEEE 23rd International Symposium on Software Reliability Engineering,,2012,,,311,320,"Recent work in testing has demonstrated the benefits of considering test oracles in the testing process. Unfortunately, this work has focused primarily on developing techniques for generating test oracles, in particular techniques based on mutation testing. While effective for test case generation, existing research has not considered the impact of test oracles in the context of regression testing tasks. Of interest here is the problem of test case prioritization, in which a set of test cases are ordered to attempt to detect faults earlier and to improve the effectiveness of testing when the entire set cannot be executed. In this work, we propose a technique for prioritizing test cases that explicitly takes into account the impact of test oracles on the effectiveness of testing. Our technique operates by first capturing the flow of information from variable assignments to test oracles for each test case, and then prioritizing to ``cover'' variables using the shortest paths possible to a test oracle. As a result, we favor test orderings in which many variables impact the test oracle's result early in test execution. Our results demonstrate improvements in rate of fault detection relative to both random and structural coverage based prioritization techniques when applied to faulty versions of three synchronous reactive systems.",1071-9458;1071-9458;2332-6549,978-1-4673-4638-2978-0-7695-4888,10.1109/ISSRE.2012.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405379,Software testing;software metrics,Testing;Fault detection;Measurement;Instruments;Java;Aircraft;Educational institutions,program testing;regression analysis,Oracle-centric test case prioritization;generating test oracles;mutation testing;regression testing tasks;fault detection;information flow;structural coverage based prioritization techniques;synchronous reactive systems,,6,44,,,,,,IEEE,IEEE Conferences
Predicting the priority of a reported bug using machine learning techniques and cross project validation,M. Sharma; P. Bedi; K. K. Chaturvedi; V. B. Singh,"Department of Computer Science, University of Delhi, India; Department of Computer Science, University of Delhi, India; Department of Computer Science, University of Delhi, India; Delhi College of Arts &amp; Commerce, University of Delhi, India",2012 12th International Conference on Intelligent Systems Design and Applications (ISDA),,2012,,,539,545,"In bug repositories, we receive a large number of bug reports on daily basis. Managing such a large repository is a challenging job. Priority of a bug tells that how important and urgent it is for us to fix. Priority of a bug can be classified into 5 levels from PI to P5 where PI is the highest and P5 is the lowest priority. Correct prioritization of bugs helps in bug fix scheduling/assignment and resource allocation. Failure of this will result in delay of resolving important bugs. This requires a bug prediction system which can predict the priority of a newly reported bug. Cross project validation is also an important concern in empirical software engineering where we train classifier on one project and test it for prediction on other projects. In the available literature, we found very few papers for bug priority prediction and none of them dealt with cross project validation. In this paper, we have evaluated the performance of different machine learning techniques namely Support Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbors (KNN) and Neural Network (NNet) in predicting the priority of the newly coming reports on the basis of different performance measures. We performed cross project validation for 76 cases of five data sets of open office and eclipse projects. The accuracy of different machine learning techniques in predicting the priority of a reported bug within and across project is found above 70% except Naive Bayes technique.",2164-7143;2164-7143;2164-7151,978-1-4673-5119-5978-1-4673-5117-1978-1-4673-5118,10.1109/ISDA.2012.6416595,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416595,Bug repositories;Bug priority;Triager;Classifiers;10-fold Cross Validation;SVM;KNN;Naive Bayes;Neural Net,Intelligent systems;Hafnium compounds;Decision support systems,belief networks;learning (artificial intelligence);neural nets;pattern classification;program debugging;program testing;project management;resource allocation;support vector machines,reported bug priority prediction;machine learning techniques;cross project validation;bug repositories;bug priority classification;bug prioritization;resource allocation;bug fix scheduling;bug prediction system;empirical software engineering;support vector machine;SVM;Naive Bayes classifier;k-nearest neighbor classifier;neural network;NNet;NB;KNN;open office projects;eclipse projects,,13,25,,,,,,IEEE,IEEE Conferences
Preemptive Regression Test Scheduling Strategies: A New Testing Approach to Thriving on the Volatile Service Environments,L. Mei; K. Zhai; B. Jiang; W. K. Chan; T. H. Tse,NA; NA; NA; NA; NA,2012 IEEE 36th Annual Computer Software and Applications Conference,,2012,,,72,81,"A workflow-based web service may use ultra-late binding to invoke external web services to concretize its implementation at run time. Nonetheless, such external services or the availability of recently used external services may evolve without prior notification, dynamically triggering the workflow-based service to bind to new replacement external services to continue the current execution. Any integration mismatch may cause a failure. In this paper, we propose Preemptive Regression Testing (PRT), a novel testing approach that addresses this adaptive issue. Whenever such a late-change on the service under regression test is detected, PRT preempts the currently executed regression test suite, searches for additional test cases as fixes, runs these fixes, and then resumes the execution of the regression test suite from the preemption point.",0730-3157;0730-3157;0730-3157,978-1-4673-1990-4978-0-7695-4736,10.1109/COMPSAC.2012.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6340125,adaptive service composition;adaptive regression test?ªing;preemptive regression testing;test case prioritization,Testing;Educational institutions;Availability;Books;Service oriented architecture,program testing;regression analysis;Web services,preemptive regression test scheduling strategies;volatile service environments;dynamically triggered workflow-based external Web service;ultralate binding;run time;integration mismatch;PRT;test cases;preemption point;adaptive service composition,,3,26,,,,,,IEEE,IEEE Conferences
Prioritization of Test Cases Using Software Agents and Fuzzy Logic,C. Malz; N. Jazdi; P. Gohner,NA; NA; NA,"2012 IEEE Fifth International Conference on Software Testing, Verification and Validation",,2012,,,483,486,"Limited test time and restricted number of test resources confront test managers with big challenges, especially in the system test. Consequently, the test manager has to prioritize test cases before each test cycle. There is much information available for determining a reasonable prioritization order in software projects. However, due to the complexity of current software systems and the high number of existing test cases, the abundance of information relevant for prioritization is not manageable for the test manager, even with high effort. In this paper we present a concept for an automated prioritization of test cases using software agents and fuzzy logic. Our prioritization system determines the prioritization order which increases the test effectiveness and the fault detection rate.",2159-4848,978-0-7695-4670-4978-1-4577-1906,10.1109/ICST.2012.131,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200143,test case prioritization;software agent;fuzzy logic,Software agents;Fault detection;Estimation;Fuzzy logic;Testing;Software systems,fuzzy logic;program testing;software agents;software fault tolerance,test case;software agent;fuzzy logic;test resource;system test;test manager;test cycle;prioritization order;software project;software system complexity;automated prioritization;prioritization system;test effectiveness;fault detection rate,,12,12,,,,,,IEEE,IEEE Conferences
Prioritizing demand response programs from reliability aspect,M. Nikzad; M. Bashirvand; B. Mozafari; A. M. Ranjbar,"Department of Electrical Engineering, Islamshahr Branch, Islamic Azad University, Tehran, Iran; Department of Electrical and Computer Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Department of Electrical and Computer Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Department of Electrical Engineering, Sharif University of Technology, Tehran, Iran",2012 11th International Conference on Environment and Electrical Engineering,,2012,,,229,234,"In this paper, the impact of demand response programs (DRPs) on reliability improvement of the restructured power systems is quantified. In this regard, the demand response (DR) model which treats consistently the main characteristics of the demand curve is developed for modeling. In proposed model, some penalties for customers in case of no responding to load reduction and incentives for customers who respond to reducing their loads are considered. In order to make analytical evaluation of the reliability, a mixed integer DCOPF is proposed by which load curtailments and generation re-dispatches for each contingency state are determined. Both transmission and generation failures are considered in contingency enumeration. The proposed technique is modeled in the GAMS software and solved using CPLEX. Reliability indices for generation-side, transmission network and whole system are calculated using this technique. Different DRPs based on the DR model are implemented over the IEEE RTS 24-bus test system, and reliability indices for different parties are calculated. Afterward, using proposed performance index, the priority of the considered programs is determined from view point of different market participants.",,978-1-4577-1829-8978-1-4577-1830-4978-1-4577-1828,10.1109/EEEIC.2012.6221578,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221578,demand response programs;analytical reliability evaluation techniques;mixed integer linear programming;expected energy not supplied;expected interruption cost,Reliability;Electricity;Power system reliability;Mathematical model;Load modeling;Load management;Elasticity,demand side management;load dispatching;power system reliability,demand response programs;reliability aspect;restructured power systems;main characteristics;load curtailments;generation re-dispatches;GAMS software;CPLEX;performance index,,2,13,,,,,,IEEE,IEEE Conferences
Quality Playbook: Ensuring Release to Release Improvement,P. Rotella; S. Chulani; S. Pradhan,NA; NA; NA,2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops,,2012,,,53,53,"Summary form only given. Before a major feature release is made available to customers, it is important to be able to anticipate if the release will be of lesser quality than its predecessor release. Our research group has developed models that use development and test times, resource levels, code added, and bugs found and fixed (or not fixed) to predict whether or not a new feature release will achieve a key quality goal - to be of better quality than its predecessor release. If the release quality prediction models, developed early in the development branches integration phase, indicate a likely upcoming quality problem in the field, another set of predictive models ('playbook' models) are then developed and used by our team to identify development or test practices that are in need of improvement. These playbook models are key components of what we call 'quality playbooks,' that are designed to address several objectives: . Identify 'levers' that positively influence feature release quality. Levers are in-process engineering metrics that are associated with specific development or test processes/practices and measure their adoption and effectiveness. . If possible, identify levers that can be invoked early in the lifecycle, to enable the development and test teams to improve deficient practices and remediate the current release under development. If it is not possible to identify early levers but possible to identify levers later in the lifecycle, we can only change deficient practices to improve the quality of future successor releases. . Determine the potential quality impact of changes suggested by the profile of significant levers. Low impact levers are likely not to be addressed by development teams. . Determine the resource and schedule investments needed to change and implement practices: Training, disruption, additional engineering time, etc. . Using impact and investment calculations identify which practices to change, either for the current release or just for subsequent releases. Develop a prioritization/ROI scheme to provide planning guidance to development and test teams. . Identify specific practice changes needed, or new practices to adopt. . Design and plan pilot programs to test the models, including the impact and investment components. Using this 'playbook' approach, our team has developed models for 31 major feature releases that are resident on 11 different hardware platforms. These models have identified six narrowly-defined classes of metrics that include both actionable levers and 'indicator' metrics that correlate well with release quality. (Indicator metrics do also correlate well, but are less specifically actionable.) The models for these six classes of metrics (and their associated practices) include strong levers and strong indicators for all releases and platforms thus far examined. Impact and investment results are also described in this paper, as are pilot programs that have tested the validity of the modeling and business calculation results. Two additional large-scale pilots of the 'playbook' approach are underway, and these are also described.",,978-1-4673-5048-8978-0-7695-4928,10.1109/ISSREW.2012.104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405417,,Predictive models;Investments;Abstracts;Computer bugs;Current measurement;Schedules,economic indicators;investment;prediction theory;quality management;resource allocation;scheduling,quality playbook;predecessor release improvement;resource levels;test times;bugs;key quality goal;quality prediction models;development branches integration phase;predictive models;development identification;test practices;in-process engineering metrics;test processes;test teams;future successor releases quality;potential quality impact;significant lever profile;schedule investments;resource investments;investment calculation identification;prioritization-ROI scheme;investment components;hardware platforms;actionable levers;indicator metrics;business calculation;modeling validity;large-scale pilots,,,,,,,,,IEEE,IEEE Conferences
Resilient system design for Prognosis and Health Monitoring of an ocean power generator,A. Marcus; I. Cardei; T. Tavtilov; G. Alsenas,"Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, Florida, United States of America; Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, Florida, United States of America; Department of Computer and Electrical Engineering and Computer Science, Florida Atlantic University, Boca Raton, Florida, United States of America; Southeast National Marine Renewable Energy Center, Florida Atlantic University, Boca Raton, Florida, United States of America",2012 IEEE International Systems Conference SysCon 2012,,2012,,,1,8,"In this paper we introduce a new methodology that integrates system resilience engineering and hazard analysis into complex system design. We then demonstrate its performance by applying it to the design of a Prognosis and Health Monitoring (PHM) system for an ocean current power generator. Three common methodologies for system hazard analysis were tested by applying them to the PHM system's network topology architecture; STAMP-based Process Analysis (STPA), Hazard and Operability Analysis (HAZOP), and a Resilience Engineering, Heuristic-based approach. While all three approaches adequately revealed most PHM system hazards, which assisted in identifying the means with which to mitigate them, none of the approaches fully addressed the multi-state dimensionality of the sub-components of the system, missing risky and hazardous scenarios. We developed the System Hazard Indication and Extraction Learning Diagnosis (SHIELD) methodology for system hazard analysis and resilient design. SHIELD integrates state space analysis into the hazard analysis process in order to facilitate the location of undiscovered hazard scenarios. Our approach uses recursive, top-down system decomposition with subsystem, interface, and process cycle identification. Then, a bottom-up recursive evaluation is completed where we analyze the subsystem state space and state transitions with regard to hazards/failures in process cycles. This yields a comprehensive list of failure states and scenarios. Finally, a top-down prioritized application of resilient engineering heuristics which address hazard scenarios is prescribed. This final phase results in a comprehensive, complete analysis of complex system architectures forcing resilience into the final system design.",,978-1-4673-0750-5978-1-4673-0748-2978-1-4673-0749,10.1109/SysCon.2012.6189490,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189490,risk analysis;hazard analysis;system resilience engineering,Hazards;Prognostics and health management;Resilience;Software;Turbines;Monitoring,computerised monitoring;condition monitoring;hazards;hydroelectric power stations;ocean waves;wave power generation,resilient system design;complex system design;system resilience engineering;ocean current power generator;PHM;system network topology architecture;STAMP-based process analysis;hazard and operability analysis;HAZOP;heuristic-based approach;system hazard indication and extraction learning diagnosis;system hazard analysis;SHIELD;state space analysis;top-down system;process cycle identification;bottom-up recursive evaluation;state transition;prognosis and health monitoring,,2,20,,,,,,IEEE,IEEE Conferences
School that has a vision,V. Mitick?«,"Tatransk?­ akad??mia, n.o. Ul.29.augusta 4812, 058 01 Poprad, Slovakia",2012 IEEE 10th International Conference on Emerging eLearning Technologies and Applications (ICETA),,2012,,,275,278,"The article is devoted to presentation of the concept and results of the Private secondary professional school in Poprad that is actually realization of the vision of two high school teachers on how education might look like at high school. Based on the inclusion of Private secondary professional school into the school system and the subsequent approval of the experimental verification of branches of study the founder began to build a school, which is based on two main priorities - high quality educational process and modern hardware and software. The school where students have their own laptops, use electronic books along with an educational portal, do test and homework via the internet and where parents can check the results by means of the electronic register of students' grades. The school which offers their students new perspective branches of study tailor-made according to the current and future requirements of the market, i.e. without the school subjects that are not relevant anymore, and with an ongoing innovative curriculum; the school with enthusiastic highly qualified teachers using new methods and forms of teaching including modern technique. The school that partners with the university sector to ensure the follow-up study of the graduates provides video conferencing for specialized subjects and conversational courses with schools abroad. Private secondary professional school, Ul.29.augusta 4812 in Poprad, officially began on 01.09.2008, the founder of which is Tatransk?­ Akad??mia.",,978-1-4673-5122-5978-1-4673-5120-1978-1-4673-5121,10.1109/ICETA.2012.6418322,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6418322,,Educational institutions;Graphics;Computer networks;Software;Portable computers;Motion pictures,computer aided instruction;educational courses;educational institutions;electronic publishing;Internet;portals;teaching;teleconferencing;video communication,private secondary professional school;Poprad;high school teachers;school system;high quality educational process;modern hardware;modern software;electronic books;educational portal;Internet;student grade electronic register;teaching;video conferencing;conversational course;Tatransk?­ Akad??mia,,,3,,,,,,IEEE,IEEE Conferences
Selecting an appropriate framework for value-based requirements prioritization,N. Kukreja; B. Boehm; S. S. Payyavula; S. Padmanabhuni,"Center for Systems and Software Engineering (CSSE), University of Southern California, Los Angeles, USA; Center for Systems and Software Engineering (CSSE), University of Southern California, Los Angeles, USA; Infosys Labs, Infosys Technologies, Bangalore, India; Infosys Labs, Infosys Technologies, Bangalore, India",2012 20th IEEE International Requirements Engineering Conference (RE),,2012,,,303,308,"There are usually more requirements than feasible in a given schedule. Thus, it's imperative to be able to choose the most valuable ones for implementation to ensure the delivery of a high value software system. There are myriad requirements prioritization frameworks and selecting the most appropriate one is a decision problem in its own right. In this paper we present our approach in selecting the most appropriate value based requirements prioritization framework as per the requirements of our stakeholders. Based on our analysis a single framework was selected, validated by requirements engineers and project managers and deployed for company-wide use by a major IT player in India.",2332-6441;1090-705X;1090-750X,978-1-4673-2785-5978-1-4673-2783-1978-1-4673-2784,10.1109/RE.2012.6345819,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345819,requirements prioritization;decision making;framework;value-based software engineering,Testing;Planning;Companies;Surface acoustic waves;Vectors;Software engineering,decision making;formal verification;project management;value engineering,value-based requirement prioritization;high value software system;requirement engineers;project managers;IT player;India;decision problem,,6,39,,,,,,IEEE,IEEE Conferences
Size-Constrained Regression Test Case Selection Using Multicriteria Optimization,S. Mirarab; S. Akhlaghi; L. Tahvildari,"University of Texas at Austin, Austin; Shahed University, Tehran; University of Waterloo, Waterloo",IEEE Transactions on Software Engineering,,2012,38,4,936,956,"To ensure that a modified software system has not regressed, one approach is to rerun existing test cases. However, this is a potentially costly task. To mitigate the costs, the testing effort can be optimized by executing only a selected subset of the test cases that are believed to have a better chance of revealing faults. This paper proposes a novel approach for selecting and ordering a predetermined number of test cases from an existing test suite. Our approach forms an Integer Linear Programming problem using two different coverage-based criteria, and uses constraint relaxation to find many close-to-optimal solution points. These points are then combined to obtain a final solution using a voting mechanism. The selected subset of test cases is then prioritized using a greedy algorithm that maximizes minimum coverage in an iterative manner. The proposed approach has been empirically evaluated and the results show significant improvements over existing approaches for some cases and comparable results for the rest. Moreover, our approach provides more consistency compared to existing approaches.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2011.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5928351,Software regression testing;test case selection;integer programming;Pareto optimality,Testing;Software;Time factors;Fault detection;Optimization;Estimation;IP networks,greedy algorithms;integer programming;linear programming;program testing;regression analysis,size constrained regression test case selection;multicriteria optimization;modified software system;integer linear programming problem;voting mechanism;greedy algorithm;iterative manner,,25,61,,,,,,IEEE,IEEE Journals & Magazines
Software testing suite prioritization using multi-criteria fitness function,A. A. Ahmed; M. Shaheen; E. Kosba,"Computer Engineering Department., Alexandria High Institute of Engineering and Technology (A.I.E.T), Alexandria, Egypt; College of Computing and Information Technology, Arab Academy for Science and Technology &amp; Maritime Transport., Alexandria, Egypt; College of Computing and Information Technology, Arab Academy for Science and Technology &amp; Maritime Transport., Alexandria, Egypt",2012 22nd International Conference on Computer Theory and Applications (ICCTA),,2012,,,160,166,"Regression testing is the process of validating modifications introduced in a system during software maintenance. It is an expensive, yet an important process. As the test suite size is very large, system retesting consumes large amount of time and computing resources. Unfortunately, there may be insufficient resources to allow for the re-execution of all test cases during regression testing. Testcase prioritization techniques aim to improve the effectiveness of regression testing, by ordering the testcases so that the most beneficial are executed first with higher priority. The objective of test case prioritization is to detect faults as early as possible. An approach for automating the test case prioritization process using genetic algorithm with Multi-Criteria Fitness function is presented. It uses multiple control flow coverage metrics. These metrics measure the degree of coverage of conditions, multiple conditions and statements that the test case covers. Theses metrics are weighted by the number of faults revealed and their severity. The proposed Multi-criteria technique showed superior results compared to similar work.",,978-1-4673-2824-1978-1-4673-2823-4978-1-4673-2822,10.1109/ICCTA.2012.6523563,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523563,Genetic algorithm;Regression testing;Testcase prioritization,,genetic algorithms;program testing;regression analysis;software maintenance,software testing suite prioritization;multicriteria fitness function;regression testing;modification validation;software maintenance;computing resource;test case prioritization process;genetic algorithm;multiple control flow coverage metrics;software fault,,5,14,,,,,,IEEE,IEEE Conferences
Test Case Prioritization Due to Database Changes in Web Applications,D. Garg; A. Datta,NA; NA,"2012 IEEE Fifth International Conference on Software Testing, Verification and Validation",,2012,,,726,730,"A regression test case prioritization (TCP) technique reorders test cases for regression testing to achieve early fault detection. Most TCP techniques have been developed for regression testing of source code in an application. Most web applications rely on a database server for serving client requests. Any changes in the database result in erroneous client interactions and may bring down the entire web application. However, most prioritization techniques are unsuitable for prioritizing test suites for early detection of changes in databases. There are very few proposals in the literature for prioritization of test cases that can detect faults in the database early. We propose a new automated TCP technique for web applications that automatically identifies the database changes, prioritizes test cases related to database changes and executes them in priority order to detect faults early.",2159-4848,978-0-7695-4670-4978-1-4577-1906,10.1109/ICST.2012.163,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6200175,Regression testing;Test case prioritization;Database schema;Functional dependency graph,Databases;Testing;Software;Conferences;Unified modeling language;Editorials;Fault detection,database management systems;Internet;regression analysis;statistical testing,database change;regression test case prioritization technique;regression testing;fault detection;Web application;client request;client interaction,,2,23,,,,,,IEEE,IEEE Conferences
Test Case Prioritization for Regression Testing Based on Function Call Path,Z. Zhang; Y. Mu; Y. Tian,NA; NA; NA,2012 Fourth International Conference on Computational and Information Sciences,,2012,,,1372,1375,"Test case prioritization is an effective and practical technique of regression testing. It is helpful to increase the efficiency of regression testing by sorting and executing test cases according to their importance. Static paths on function call obtained by analyzing the source code, combined with the dynamic path after executing test cases, the correspondence is built between test cases and the static paths, identifying the changes which software developers modify program to correct defects, giving different priority to test case based on path coverage , test cases are selected in accordance with their priorities in regression testing. Firstly, the background and related concept of test case prioritization are introduced. And then, the relevant research work is outlined, a set of new prioritization algorithms are proposed; implementation and analysis of the algorithm are given finally.",,978-1-4673-2406-9978-0-7695-4789,10.1109/ICCIS.2012.312,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6301421,test case prioritization;function call;regression testing;algorithm,Testing;Algorithm design and analysis;Software;Software algorithms;Heuristic algorithms;Conferences;Sorting,program testing;regression analysis,test case prioritization;regression testing;function call path;static paths;source code;dynamic path;software developers,,,14,,,,,,IEEE,IEEE Conferences
Test case prioritization incorporating ordered sequence of program elements,K. Wu; C. Fang; Z. Chen; Z. Zhao,"State Key Laboratory for Novel Software Technology, Nanjing University, 210093, China; State Key Laboratory for Novel Software Technology, Nanjing University, 210093, China; State Key Laboratory for Novel Software Technology, Nanjing University, 210093, China; State Key Laboratory for Novel Software Technology, Nanjing University, 210093, China",2012 7th International Workshop on Automation of Software Test (AST),,2012,,,124,130,"Test suites often grow very large over many releases, such that it is impractical to re-execute all test cases within limited resources. Test case prioritization, which rearranges test cases, is a key technique to improve regression testing. Code coverage information has been widely used in test case prioritization. However, other important information, such as the ordered sequence of program elements measured by execution frequencies, was ignored by previous studies. It raises a risk to lose detections of difficult-to-find bugs. Therefore, this paper improves the similarity-based test case prioritization using the ordered sequence of program elements measured by execution counts. The empirical results show that our new technique can increase the rate of fault detection more significantly than the coverage-based ART technique. Moreover, our technique can detect bugs in loops more quickly and be more cost-benefits than the traditional ones.",,978-1-4673-1822-8978-1-4673-1821,10.1109/IWAST.2012.6228980,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6228980,Test case prioritization;similarity;ordered sequence;edit distance;farthest-first algorithm,Computer bugs;Subspace constraints,fault diagnosis;program testing;regression analysis,program elements;test suites;regression testing;code coverage information;similarity-based test case prioritization;ordered sequence;fault detection,,,26,,,,,,IEEE,IEEE Conferences
Towards reliable web applications: ISO 19761,M. A. Talib; E. Mendes; A. Khelifi,"College of Information Technology, Zayed University, UAE; College of Information Technology, Zayed University, UAE; Software Engineering Department, ALHOSN University, UAE",IECON 2012 - 38th Annual Conference on IEEE Industrial Electronics Society,,2012,,,3144,3148,"This research adopts a new scenario-based black box testing methodology for testing web applications. It combines a black box testing strategy with the functions (scenarios) measured by the COSMIC-FFP measurement procedure (ISO/IEC 19761 standard) to produce an optimal set of test cases. This testing approach shows its applicability during all the development phases. Moreover, it can be applied during the early development phase once the specifications have been documented as well as after the development phase where we don't have the access to the code. This paper also considers the use of a functional complexity measure for assigning priorities to the generated test cases. Finally, those concepts have been applied on part of Online Banking System as a case study.",1553-572X;1553-572X,978-1-4673-2421-2978-1-4673-2419-9978-1-4673-2420,10.1109/IECON.2012.6389396,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6389396,Web applications testing;COSMIC-FFP;ISO 19761,ISO;Indexes;Ice;IEC standards,IEC standards;Internet;ISO standards;program testing;software metrics,Web application testing;scenario-based black box testing;COSMIC-FFP measurement procedure;ISO/IEC 19761 standard;functional complexity measure;online banking system,,1,12,,,,,,IEEE,IEEE Conferences
Use of embedded intelligence in tactical grids for energy surety and fuel conservation,D. D. Massie; P. S. Curtiss; M. A. Miller,"IPERC, Fort Montgomery, NY; IPERC, Fort Montgomery, NY; IPERC, Fort Montgomery, NY",2012 IEEE Power and Energy Society General Meeting,,2012,,,1,5,"This paper describes a system for creating an energy-sharing infrastructure, effectively creating redundant sources of energy supply and significantly reducing the logistical burdens associated with providing power. An intelligent power management and power grid system has been developed and tested. This system optimizes performance and efficiency through local and system-level autonomous controls. The grid system was based on existing military, trailer-mounted, mobile power equipment. A reduction in fuel consumption of 36 percent was observed. In addition, prioritized load shedding was demonstrated as a means to prevent the generators from being overloaded.",1932-5517;1944-9925,978-1-4673-2729-9978-1-4673-2727-5978-1-4673-2728,10.1109/PESGM.2012.6345122,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6345122,Energy conservation;smart grid;intelligent control;power optimization,Generators;Monitoring;Computers;Heating;Fuels;Software;Cooling,control engineering computing;electric generators;embedded systems;graphical user interfaces;intelligent control;load shedding;military equipment;power apparatus;power engineering computing;power grids;power system management,embedded intelligence;tactical grids;energy surety;fuel conservation;energy-sharing infrastructure;energy supply sources;intelligent power management;power grid system;system-level autonomous controls;mobile power equipment;trailer-mounted eqipment;military equipment;load shedding;generators;fuel consumption,,1,,,,,,,IEEE,IEEE Conferences
Using Non-redundant Mutation Operators and Test Suite Prioritization to Achieve Efficient and Scalable Mutation Analysis,R. Just; G. M. Kapfhammer; F. Schweiggert,NA; NA; NA,2012 IEEE 23rd International Symposium on Software Reliability Engineering,,2012,,,11,20,"Mutation analysis is a powerful and unbiased technique to assess the quality of input values and test oracles. However, its application domain is still limited due to the fact that it is a time consuming and computationally expensive method, especially when used with large and complex software systems. Addressing these challenges, this paper makes several contributions to significantly improve the efficiency of mutation analysis. First, it investigates the decrease in generated mutants by applying a reduced, yet sufficient, set of mutants for replacing conditional (COR) and relational (ROR) operators. The analysis of ten real-world applications, with 400,000 lines of code and more than 550,000 generated mutants in total, reveals a reduction in the number of mutants created of up to 37% and more than 25% on average. Yet, since the isolated use of non-redundant mutation operators does not ensure that mutation analysis is efficient and scalable, this paper also presents and experimentally evaluates an optimized workflow that exploits the redundancies and runtime differences of test cases to reorder and split the corresponding test suite. Using the same ten open-source applications, an empirical study convincingly demonstrates that the combination of non-redundant operators and prioritization leveraging information about the runtime and mutation coverage of tests reduces the total cost of mutation analysis further by as much as 65%.",1071-9458;1071-9458;2332-6549,978-1-4673-4638-2978-0-7695-4888,10.1109/ISSRE.2012.31,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6405400,,Runtime;Redundancy;Testing;Educational institutions;Connectors;Computer science;Software systems,program diagnostics;program testing,nonredundant operators;open-source applications;real-world application analysis;complex software systems;large software systems;computationally expensive method;test oracles;scalable mutation analysis;test suite prioritization;nonredundant mutation operators,,17,17,,,,,,IEEE,IEEE Conferences
Using Prioritized Disk Service to Expedite Program Execution,T. Yeh; L. Yang,NA; NA,2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems,,2012,,,1027,1032,"Computer systems often host multiple programs in execution simultaneously. Among those running programs, some may be important and time-critical, which users would expect them to finish their execution as soon as possible. Generally speaking, the course of program execution includes CPU operation and hard disk operation (disk I/O). For the CPU operation, modern computer systems have the ability to adjust the CPU scheduling sequence according to program priority. However, most computer systems do not have effective ways to conduct disk I/O based on program priority. The Linux operating system has been widely used in many areas. It supports several disk schedulers. The Complete Fair Queuing (CFQ) and the Anticipatory Scheduling (AS) are among those most well-known. Currently, CFQ is the default disk scheduler in the Linux operating system. AS is the predecessor of CFQ. Unfortunately, CFQ only offers prioritized disk I/O to some extent through the tool ""ionice"", while AS does not provide any prioritized service at all. We propose and implement a new disk scheduler, namely Prioritized Anticipatory Scheduling (PAS), by adding schemes of supporting prioritized disk I/O into AS in the Linux kernel. Our experimental results show that PAS surpasses CFQ with ionice for the vast majority of all test cases. Compared with AS, PAS can improve the performance of programs with high disk I/O priority by up to 71.88%.",,978-1-4673-2164-8978-0-7695-4749,10.1109/HPCC.2012.150,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332286,disk performance;disk scheduling,Linux;Kernel;Computers;Time factors;Hard disks;Processor scheduling,hard discs;input-output programs;Linux;multiprocessing systems;queueing theory;scheduling,prioritized disk service;multiple program execution course;CPU operation;hard disk operation;modern computer systems;CPU scheduling sequence;program priority-based disk I/O;Linux operating system;complete fair queuing;CFQ;anticipatory scheduling;AS;Linux operating system;CFQ predecessor;prioritized anticipatory scheduling;PAS;Linux kernel;high disk I-O priority,,,18,,,,,,IEEE,IEEE Conferences
Value-Based Coverage Measurement in Requirements-Based Testing: Lessons Learned from an Approach Implemented in the TOSCA Testsuite,R. Ramler; T. Kopetzky; W. Platz,NA; NA; NA,2012 38th Euromicro Conference on Software Engineering and Advanced Applications,,2012,,,363,366,"Testing is one of the most widely practiced quality assurance measures and also one of the most resource-intensive activities in software development. Still, however, most of the available methods, techniques and tools for software testing are value-neutral and do not realize the potential value contribution of testing. In this paper we present an approach for value-based coverage measurement that can be used to align the testing effort with the achievable value associated with requirements and functional units. It has been implemented as part of a commercial test tool and was successfully applied in real-world projects. The results demonstrated its ability to adequately capture the distribution of the business value and risks involved in different requirements. The paper concludes with sharing important lessons learned from developing value-based coverage measurement in the practical setting of commercial tool development and real-world test projects.",1089-6503;2376-9505,978-0-7695-4790-9978-1-4673-2451,10.1109/SEAA.2012.43,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6328176,Value-Based Software Testing;Test Case Prioritization;Coverage Measurement;Requirements Coverage,Software;Business;Software testing;Software engineering;Standards;Software measurement,program testing;software quality,value-based coverage measurement;requirements-based testing;TOSCA testsuite;quality assurance measures;software development;software testing;commercial test tool;business value;business risks;real-world test projects;commercial tool development,,1,10,,,,,,IEEE,IEEE Conferences
A Comparison of Different Defect Measures to Identify Defect-Prone Components,T. D. Oyetoyan; R. Conradi; D. S. Cruzes,NA; NA; NA,2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement,,2013,,,181,190,"(Background) Defect distribution in software systems has been shown to follow the Pareto rule of 20-80. This motivates the prioritization of components with the majority of defects for testing activities. (Research goal) Are there significant variations between defective components and architectural hotspots identified by other defect measures? (Approach) We have performed a study using post-release data of an industrial Smart Grid application with a well-maintained defect tracking system. Using the Pareto principle, we identify and compare defect-prone and hotspots components based on four defect metrics. Furthermore, we validated the quantitative results against qualitative data from the developers. (Results) Our results show that at the top 25% of the measures 1) significant variations exist between the defective components identified by the different defect metrics and that some of the components persist as defective across releases 2) the top defective components based on number of defects could only identify about 40% of critical components in this system 3) other defect metrics identify about 30% additional critical components 4) additional quality challenges of a component could be identified by considering the pair wise intersection of the defect metrics. (Discussion and Conclusion) Since a set of critical components in the system is missed by using largest-first or smallest-first prioritization approaches, this study, therefore, makes a case for an all-inclusive metrics during defect model construction such as number of defects, defect density, defect severity and defect correction effort to make us better understand what comprises defect-prone components and architectural hotspots, especially in critical applications.",,978-0-7695-5078,10.1109/IWSM-Mensura.2013.34,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693238,defect distribution;defect measures;defect metrics;defect severity;defect correction effort;defect density;defect-prone component;Smart Grid;critical system;architectural hotspots,Measurement;Software;Smart grids;Predictive models;Testing;Maintenance engineering;Object oriented modeling,object-oriented programming;Pareto distribution;power engineering computing;program testing;smart power grids;software architecture;software metrics;software quality,defect measure;defect-prone component identification;defect distribution;software system;Pareto rule;component prioritization;testing activities;architectural hotspot;post-release data;industrial smart grid application;well-maintained defect tracking system;Pareto principle;defect metrics;quality challenges;largest-first prioritization approach;smallest-first prioritization approach;defect density;defect severity;defect correction;critical application,,,43,,,,,,IEEE,IEEE Conferences
A Fuzzy Expert System for Cost-Effective Regression Testing Strategies,A. Schwartz; H. Do,NA; NA,2013 IEEE International Conference on Software Maintenance,,2013,,,1,10,"Different testing environments and software change characteristics can affect the choice of regression testing techniques. In our prior work, we developed adaptive regression testing (ART) strategies to investigate this problem. While the ART strategies showed promising results, we also found that the multiple criteria decision making processes required for the ART strategies are time-consuming, often inaccurate and inconsistent, and limited in their scalability. To address these issues, in this research, we develop and empirically study a fuzzy expert system (FESART) to aid decision makers in choosing the most cost-effective technique for a particular software version. The results of our study show that FESART is consistently more cost-effective than the previously proposed ART strategies. One of the biggest contributors to FESART being more cost-effective is the reduced time required to apply the strategy. This contribution has significant impact because a strategy that is less time-consuming will be easier for researchers and practitioners to adopt, and will provide even greater cost-savings for regression testing sessions.",1063-6773,978-0-7695-4981,10.1109/ICSM.2013.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676871,Regression testing;test case prioritization;adaptive regression testing strategy;AHP;fuzzy AHP;empirical studies,Expert systems;Testing;Subspace constraints;Fuzzy logic;Software;Decision making;Fuzzy sets,expert systems;fuzzy set theory;program testing,fuzzy expert system;cost-effective regression testing strategies;software change characteristics;testing environments;adaptive regression testing strategies;ART strategies;multiple criteria decision making processes;FESART;software version,,1,40,,,,,,IEEE,IEEE Conferences
A novel approach for test case prioritization,R. U. Maheswari; D. JeyaMala,"Department of computer Applications, K.L.N. College of Engineering, Sivagangai, India; Department of computer Applications, Thiagarajar College of Engineering, Madurai, India",2013 IEEE International Conference on Computational Intelligence and Computing Research,,2013,,,1,5,"The process of verifying the modified software in the maintenance phase is called Regression Testing. The size of the regression test suite and its selection process is a complex task for regression testers because of time and budget constraints. In this research paper, new Prioritization technique based on hamming distance has been proposed. It is illustrated using an example and found that it produces good results. Average Percentage of Fault Detection (APFD) metrics and charts has been used to show the effectiveness of proposed algorithm.",,978-1-4799-1597-2978-1-4799-1594-1978-1-4799-1595,10.1109/ICCIC.2013.6724209,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724209,APFD;Fault based Test Suite prioritization;Hamming Distance,Fault detection;Testing;Hamming distance;Bismuth;Conferences;Measurement;Software engineering,program testing;software maintenance,test case prioritization;modified software;maintenance phase;regression testing;regression test suite;time constraint;budget constraint;prioritization technique;hamming distance;average percentage of fault detection metrics;APFD metrics,,3,12,,,,,,IEEE,IEEE Conferences
A refactoring-based approach for test case selection and prioritization,E. L. G. Alves; P. D. L. Machado; T. Massoni; S. T. C. Santos,"SPLab - Software Practices Laboratory, Federal University of Campina Grande, UFCG, Campina Grande, Brazil; SPLab - Software Practices Laboratory, Federal University of Campina Grande, UFCG, Campina Grande, Brazil; SPLab - Software Practices Laboratory, Federal University of Campina Grande, UFCG, Campina Grande, Brazil; SPLab - Software Practices Laboratory, Federal University of Campina Grande, UFCG, Campina Grande, Brazil",2013 8th International Workshop on Automation of Software Test (AST),,2013,,,93,99,"Refactoring edits, commonly applied during software development, may introduce faults in a previously-stable code. Therefore, regression testing is usually applied to check whether the code maintains its previous behavior. In order to avoid rerunning the whole regression suite, test case prioritization techniques have been developed to order test cases for earlier achievement of a given goal, for instance, improving the rate of fault detection during regression testing execution. However, as current techniques are usually general purpose, they may not be effective for early detection of refactoring faults. In this paper, we propose a refactoring-based approach for selecting and prioritizing regression test cases, which specializes selection/prioritization tasks according to the type of edit made. The approach has been evaluated through a case study that compares it to well-known prioritization techniques by using a real open-source Java system. This case study indicates that the approach can be more suitable for early detection of refactoring faults when comparing to the other prioritization techniques.",,978-1-4673-6161,10.1109/IWAST.2013.6595798,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595798,,Testing;Java;Fault detection;Software;Object oriented modeling;Measurement;Debugging,program testing;software maintenance,refactoring-based approach;test case selection technique;test case prioritization technique;edit refactoring;software development;fault detection;regression testing;open-source Java system,,1,36,,,,,,IEEE,IEEE Conferences
A Study in Prioritization for Higher Strength Combinatorial Testing,X. Qu; M. B. Cohen,NA; NA,"2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops",,2013,,,285,294,"Recent studies have shown that combinatorial interaction testing (CIT) is an effective fault detection technique and that early fault detection can be improved by ordering test suites by interaction based prioritization approaches. Despite research that has shown that higher strength CIT improves fault detection, there have been fewer studies that aim to understand the impact of prioritization based on higher strength criteria. In this paper, we aim to understand how interaction based prioritization techniques perform, in terms of early fault detection when we prioritize based on 3-way interactions. We generalize prior work on prioritizing using 2-way interactions to t-way prioritization, and empirically evaluate this on three open source subjects, across multiple versions of each. We examine techniques that prioritize both existing CIT suites as well as generate new ones in prioritized order. We find that early fault detection can be improved when prioritizing 3-way CIT test suites by interactions that cover more code, and to a lesser degree when generating tests in prioritized order. Our techniques that work only from the specification, appear to work best with 2-way generation.",,978-0-7695-4993-4978-1-4799-1324,10.1109/ICSTW.2013.40,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571645,combinatorial interaction testing;test prioritization;testing strength,Fault detection;Measurement;Software;Flexible printed circuits;Conferences;Software testing,formal specification;program testing;software fault tolerance,specification;test generation;3-way CIT test suite;prioritized order;CIT suite;t-way prioritization;2-way interaction;3-way interaction;strength criteria;interaction based prioritization approach;fault detection;combinatorial interaction testing,,10,22,,,,,,IEEE,IEEE Conferences
A Uniform Representation of Hybrid Criteria for Regression Testing,S. Sampath; R. Bryce; A. M. Memon,"University of Maryland, Baltimore; University of North Texas, Denton; University of Maryland, Baltimore",IEEE Transactions on Software Engineering,,2013,39,10,1326,1344,"Regression testing tasks of test case prioritization, test suite reduction/minimization, and regression test selection are typically centered around criteria that are based on code coverage, test execution costs, and code modifications. Researchers have developed and evaluated new individual criteria; others have combined existing criteria in different ways to form what we--and some others--call hybrid criteria. In this paper, we formalize the notion of combining multiple criteria into a hybrid. Our goal is to create a uniform representation of such combinations so that they can be described unambiguously and shared among researchers. We envision that such sharing will allow researchers to implement, study, extend, and evaluate the hybrids using a common set of techniques and tools. We precisely formulate three hybrid combinations, Rank, Merge, and Choice, and demonstrate their usefulness in two ways. First, we recast, in terms of our formulations, others' previously reported work on hybrid criteria. Second, we use our previous results on test case prioritization to create and evaluate new hybrid criteria. Our findings suggest that hybrid criteria of others can be described using our Merge and Rank formulations, and that the hybrid criteria we developed most often outperformed their constituent individual criteria.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2013.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6484067,Test case prioritization;test criteria;hybrid test criteria;web testing;GUI testing,Testing;Fault detection;Educational institutions;Genetic algorithms;Vectors;Loss measurement;Minimization,program testing;regression analysis,uniform representation;hybrid test criteria;regression testing;rank-merge-and-choice hybrid combination;test case prioritization;merge-and-rank formulations,,13,74,,,,,,IEEE,IEEE Journals & Magazines
Adaptive Test-Case Prioritization Guided by Output Inspection,D. Hao; X. Zhao; L. Zhang,NA; NA; NA,2013 IEEE 37th Annual Computer Software and Applications Conference,,2013,,,169,179,"Test-case prioritization is to schedule the execution order of test cases so as to maximize some objective (e.g., revealing faults early). The existing test-case prioritization approaches separate the process of test-case prioritization and the process of test-case execution by presenting the execution order of all test cases before programmers start running test cases. As the execution information of the modified program is not available for the existing test-case prioritization approaches, these approaches mainly rely on only the execution information of the previous program before modification. To address this problem, we present an adaptive test-case prioritization approach, which determines the execution order of test cases simultaneously during the execution of test cases. In particular, the adaptive approach selects test cases based on their fault-detection capability, which is calculated based on the output of selected test cases. As soon as a test case is selected and runs, the fault-detection capability of each unselected test case is modified according to the output of the latest selected test case. To evaluate the effectiveness of the proposed adaptive approach, we conducted an experimental study on eight C programs and four Java programs. The experimental results show that the adaptive approach is usually significantly better than the total test-case prioritization approach and competitive to the additional test-case prioritization approach. Moreover, the adaptive approach is better than the additional approach on some subjects (e.g, replace and schedule).",0730-3157,978-0-7695-4986,10.1109/COMPSAC.2013.31,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649818,software testing;regression test;test-case prioritization;adaptive approach,Schedules;Software;Java;Software testing;Equations;Adaptation models,program testing;software fault tolerance,adaptive test-case prioritization approach;output inspection;test case execution order;execution information;fault-detection capability;Java programs;C programs;total test-case prioritization approach;additional test-case prioritization approach,,4,39,,,,,,IEEE,IEEE Conferences
Bridging the gap between the total and additional test-case prioritization strategies,L. Zhang; D. Hao; L. Zhang; G. Rothermel; H. Mei,"Key Laboratory of High Confidence Software Technologies (Peking University), MoE, Beijing, 100871, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, Beijing, 100871, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, Beijing, 100871, China; Department of Computer Science and Engineering, University of Nebraska, Lincoln, 68588, USA; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, Beijing, 100871, China",2013 35th International Conference on Software Engineering (ICSE),,2013,,,192,201,"In recent years, researchers have intensively investigated various topics in test-case prioritization, which aims to re-order test cases to increase the rate of fault detection during regression testing. The total and additional prioritization strategies, which prioritize based on total numbers of elements covered per test, and numbers of additional (not-yet-covered) elements covered per test, are two widely-adopted generic strategies used for such prioritization. This paper proposes a basic model and an extended model that unify the total strategy and the additional strategy. Our models yield a spectrum of generic strategies ranging between the total and additional strategies, depending on a parameter referred to as the p value. We also propose four heuristics to obtain differentiated p values for different methods under test. We performed an empirical study on 19 versions of four Java programs to explore our results. Our results demonstrate that wide ranges of strategies in our basic and extended models with uniform p values can significantly outperform both the total and additional strategies. In addition, our results also demonstrate that using differentiated p values for both the basic and extended models with method coverage can even outperform the additional strategy using statement coverage.",0270-5257;1558-1225,978-1-4673-3076-3978-1-4673-3073,10.1109/ICSE.2013.6606565,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606565,,Fault detection;Measurement;Java;Testing;Arrays;Software;Educational institutions,Java;program testing;software fault tolerance,test-case prioritization strategies;fault detection;regression testing;heuristics;Java programs,,32,33,,,,,,IEEE,IEEE Conferences
Bypassing Code Coverage Approximation Limitations via Effective Input-Based Randomized Test Case Prioritization,B. Jiang; W. K. Chan,NA; NA,2013 IEEE 37th Annual Computer Software and Applications Conference,,2013,,,190,199,"Test case prioritization assigns the execution priorities of the test cases in a given test suite with the aim of achieving certain goals. Many existing test case prioritization techniques however assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in many software development projects. This paper proposes a novel family of LBS techniques. They make adaptive tree-based randomized explorations with an adaptive randomized candidate test set strategy to diversify the explorations among the branches of the exploration trees constructed by the test inputs in the test suite. They get rid of the assumption on the historical correlation of code coverage between program versions. Our techniques can be applied to programs with or without any previous versions, and hence are more general than many existing test case prioritization techniques. The empirical study on four popular UNIX utility benchmarks shows that, in terms of APFD, our LBS techniques can be as effective as some of the best code coverage-based greedy prioritization techniques ever proposed. We also show that they are significantly more efficient and scalable than the latter techniques.",0730-3157,978-0-7695-4986,10.1109/COMPSAC.2013.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649820,regression testing;adaptive test case prioritization,Testing;Subspace constraints;Silicon;Software;Approximation methods;History;Equations,approximation theory;greedy algorithms;program testing;software engineering;trees (mathematics),code coverage approximation limitations;effective input-based randomized test case prioritization;full-fledged availability;software development projects;LBS techniques;adaptive tree-based randomized explorations;adaptive randomized candidate test set strategy;exploration trees;historical correlation;UNIX utility benchmarks;code coverage-based greedy prioritization techniques,,4,36,,,,,,IEEE,IEEE Conferences
CDM-Suite: An Attributed Test Selection Tool,P. Luchscheider; T. Herpel; R. German,NA; NA; NA,"2013 IEEE Sixth International Conference on Software Testing, Verification and Validation",,2013,,,398,407,"Many reasons lead to embedded systems getting more complex than ever. The higher integration and interconnection and the additional effort spend for safety-critical functions, require new techniques to support the testing process. Beneath using model-driven test techniques, test engineers need support to take the right decisions on test case selection and prioritization during the whole development process. Available approaches on test selection and prioritization lack in adaptability to different testing techniques. We previously presented our approach of modeling components and dependences of a system, such as black-box systems, using Component-Dependency-Models (CDMs) as shown in earlier publications. In this paper we present our tool CDM-Suite in detail and show the way it can be used by test engineers. We therefore start with the presentation of our approach and discussing related tools. The next section describes the graphical user interface (GUI) and the different ways of interaction for data generation. This point includes the introduction of metrics build upon fuzzy logic and graph analysis. At last we show how to interpret analysis results and derive knowledge for test case selection and prioritization. We sum the paper up by giving a conclusion and a presentation outline.",2159-4848,978-0-7695-4968-2978-1-4673-5961,10.1109/ICST.2013.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569753,Black-Box;Testing;Test case;Selection;Prioritization;System Model;Tool;Path-Search,Graphical user interfaces;Measurement;Vehicles;Analytical models;Computer architecture;Context;XML,embedded systems;fuzzy logic;graph theory;graphical user interfaces;object-oriented programming;program testing;software metrics,model-driven test techniques;black-box systems;graph analysis;fuzzy logic;data generation;GUI;graphical user interface;component-dependency model;test case prioritization;test case selection;testing process;safety-critical functions;embedded systems;attributed test selection tool;CDM-Suite,,1,10,,,,,,IEEE,IEEE Conferences
Closed-loop analysis of soft decisions for serial links,C. A. Lansdowne; G. F. Steele; J. P. Zucha; A. M. Schlesinger,"National Aeronautics and Space Administration, Houston, TX 77058 USA; National Aeronautics and Space Administration, Houston, TX 77058 USA; ITT at the National Aeronautics and Space Administration, Houston, TX 77058 USA; National Aeronautics and Space Administration, Houston, TX 77058 USA",IEEE International Conference on Wireless for Space and Extreme Environments,,2013,,,1,6,"We describe the benefit of using closed-loop measurements for a radio receiver paired with a counterpart transmitter. We show that real-time analysis of the soft decision output of a receiver can provide rich and relevant insight far beyond the traditional hard-decision bit error rate (BER) test statistic. We describe a Soft Decision Analyzer (SDA) implementation for closed-loop measurements on single- or dual-(orthogonal) channel serial data communication links. The analyzer has been used to identify, quantify, and prioritize contributors to implementation loss in live-time during the development of software defined radios. This test technique gains importance as modern receivers are providing soft decision symbol synchronization as radio links are challenged to push more data and more protocol overhead through noisier channels, and software-defined radios (SDRs) use error-correction codes that approach Shannon's theoretical limit of performance.",,978-1-4799-2958,10.1109/WiSEE.2013.6737541,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737541,block codes;channel coding;codes;satellite communication;test equipment,Receivers;Decoding;Clocks;Synchronization;Correlation;Correlators;Histograms,error correction codes;error statistics;radio links;radio receivers;radio transmitters;software radio,closed-loop analysis;serial links;closed-loop measurements;radio receiver;counterpart transmitter;real-time analysis;bit error rate;BER;soft decision analyzer;SDA;channel serial data communication links;software defined radios;noisier channels;error correction codes;SDR;Shannon theoretical limit,,1,9,,,,,,IEEE,IEEE Conferences
Compositional analysis of switched Ethernet topologies,R. Schneider; L. Zhang; D. Goswami; A. Masrur; S. Chakraborty,"Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)",,2013,,,1099,1104,"In this paper we study distributed automotive control applications whose tasks are mapped onto different ECUs communicating via a switched Ethernet network. As traditional automotive communication buses like CAN, FlexRay, LIN and MOST are gradually reaching their performance limits because of the increasing complexity of automotive architectures and applications, Ethernet-based in-vehicle communication systems have attracted a lot of attention in recent times. However, currently there is very little work on systematic timing analysis for Ethernet which is important for its deployment in safety-critical scenarios like in an automotive architecture. In this work, we propose a compositional timing analysis technique that takes various features of switched Ethernet into account like network topology, frame priorities, communication delay, memory requirement on switches, performance, etc. Such an analysis technique is particularly suitable during early design phases of automotive architectures and control software deployment. We demonstrate its use in analyzing mixed-criticality traffic patterns consisting of messages from performance-oriented control loops and timing-sensitive real-time tasks. We further evaluate the tightness of the obtained analytical bounds with an OMNeT++ based network simulation environment, which involves long simulation time and does not provide formal guarantees.",1530-1591;1530-1591;1530-1591,978-3-9815370-0-0978-1-4673-5071,10.7873/DATE.2013.231,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513677,,Delays;Topology;Network topology;Ports (Computers);Automotive engineering;Analytical models;Fabrics,,,,4,14,,,,,,IEEE,IEEE Conferences
Coverage-Based Test Case Prioritisation: An Industrial Case Study,D. Di Nardo; N. Alshahwan; L. Briand; Y. Labiche,NA; NA; NA; NA,"2013 IEEE Sixth International Conference on Software Testing, Verification and Validation",,2013,,,302,311,This paper presents an industrial case study of coverage-based prioritisation techniques on a real world system with real regression faults. The study evaluates four common and different test case prioritisation techniques and examines the effects of using various coverage criteria on the fault detection rates of the prioritised test suites. The results show that prioritisation techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information does not significantly enhance fault detection rates.,2159-4848,978-0-7695-4968-2978-1-4673-5961,10.1109/ICST.2013.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569742,regression testing;industrial case study;test case prioritisation,Testing;Fault detection;Measurement;Software;Computer aided software engineering;Minimization;Data collection,program testing;regression analysis;software fault tolerance,coverage-based test case prioritisation;industrial case study;real world system;regression fault;coverage criteria;fault detection rate,,9,22,,,,,,IEEE,IEEE Conferences
Data Centers as Software Defined Networks: Traffic Redundancy Elimination with Wireless Cards at Routers,Y. Cui; S. Xiao; C. Liao; I. Stojmenovic; M. Li,"Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, P.R.China; Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, P.R.China; Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, P.R.China; SEECS, University of Ottawa, Ottawa, Canada, and School of Software, Tsinghua University, Beijing, 100084, P.R.China; Department of Computer Science, City University of Hong Kong, Hong Kong, China",IEEE Journal on Selected Areas in Communications,,2013,31,12,2658,2672,"We propose a novel architecture of data center networks (DCN), which adds wireless network card to both servers and routers. Existing traffic redundancy elimination (TRE) mechanisms reduce link loads and increase network capacity in several environments by removing strings that have appeared in earlier packets through encoding and decoding them several hops downstream. This article is the first to explore TRE mechanisms in large-scale DCNs and the first to exploit cooperative TRE among servers. Moreover, it also achieves the `logically centralized' control over the physically distributed states in emerging software defined networks (SDN) paradigm, by sharing information among servers and routers in data centers with wireless cards. We first formulate the TREDaCeN (TRE in Data Center Networks) problem and reduce the cycle cover problem to prove that finding an optimal caching task assignment for TREDaCeN problem is NP-hard. We further describe an offline TREDaCeN algorithm which is proved to have good approximation ratio. We then discuss efficient online zero-delay and semi-distributed implementations of TREDaCeN supported by physical proximity of servers and routers, enabling status updates in a single wireless transmission, using an efficient prioritized schedule. We also address online cache replacement and consistency of information in servers and routers with and without delay. Our framework is tested on different parameters and shows superior performance in comparison to other mechanisms (imported directly to this setting). Our results show the robustness and the trade-off between the `logically centralized' implementation and the overhead on handling inconsistency of distributed information in DCN.",0733-8716;1558-0008,,10.1109/JSAC.2013.131207,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678112,Data Center Network;Redundancy Elimination;Wireless Link;Software Defined Networks,Wireless networks;Servers;Decoding;Routing;Database management,approximation theory;cache storage;computational complexity;computer centres;computer networks;network interfaces;telecommunication network routing;telecommunication traffic,data center network architecture;software defined networks;traffic redundancy elimination mechanism;wireless network card;routers;link load reduction;increase network capacity;string removal;hops downstream;TRE mechanisms;logical centralized control;SDN;information sharing;TREDaCeN problem;cycle cover problem reduction;optimal caching task assignment;NP-hard problem;online zero-delay;approximation ratio;semi-distributed implementations;status updates;wireless transmission;online cache replacement;information consistency;inconsistency handling,,22,30,,,,,,IEEE,IEEE Journals & Magazines
Defect Prioritization in the Software Industry: Challenges and Opportunities,N. Kaushik; M. Amoui; L. Tahvildari; W. Liu; S. Li,NA; NA; NA; NA; NA,"2013 IEEE Sixth International Conference on Software Testing, Verification and Validation",,2013,,,70,73,"Defect prioritization is a decision making process wherein stakeholders determine the temporal order of open defects to be fixed. It is critical to the software development lifecycle as the decisions made during this process directly affect release planning, resource management, and maintenance costs. In fact, defect prioritization is complex as many factors need to be taken into consideration and the decisions made can be subjective or incorporate inherent knowledge and intuition of decision makers. We believe that managing the complexities of the decision making process can provide valuable support and help in uncovering any inconsistencies in the interpretation of criteria to prioritize defects. In this paper, we explore the defect triaging process in Research In Motion to gain a better understanding of the shortcomings and challenges of the current practices. Based on our findings, we sketch some research directions to improve industrial software defect prioritization.",2159-4848,978-0-7695-4968-2978-1-4673-5961,10.1109/ICST.2013.40,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569718,software life-cycle;defect prioritization;triage,Software;Testing;Decision making;Software engineering;Economics;Availability;Industries,DP industry;program testing;software development management;software maintenance,software industry;decision making process;software development lifecycle;release planning;resource management;maintenance cost;complexity management;defect triaging process;Research In Motion;industrial software defect prioritization,,2,12,,,,,,IEEE,IEEE Conferences
Developing new Automatic Test Equipments (ATE) using systematic design approaches,H. A. Toku,"Test Engineering Dept., Defence Systems Technologies Div., Aselsan INC., Ankara, Turkey",2013 IEEE AUTOTESTCON,,2013,,,1,7,"Keeping Automatic Test Equipments (ATE) current with technology is one of the major challenges in automatic testing world. Needs and priorities can quickly evolve throughout the life cycle of ATEs and handling obsolescence via performing upgrades on hardware and software can be impossible after several years. While developing Test Program Sets (TPS), if existing ATE systems cannot meet the necessary requirements without inserting extra test devices or decreasing test coverage, then designing a new ATE can be seen inevitable. If a new ATE system is to be designed, it is very crucial that the requirements for the new ATE system should be identified before design process begins. Determining the requirements is a very critical stage in designing ATE because if enough effort is not focused and extended analysis is not carried out on determining the requirements, then the newly formed ATE system will likely fail to cover the test requirements of the DUTs. Setting up the hardware and software architecture is the next stage after the process of determining the requirements of the ATE system. Practical and cost effective solutions should be considered without compromising performance and capabilities of the test devices. The architectures should be suitable for future enhancements to the system. Throughout the design process, the design requirements, critical design descriptions, verification and validation procedures should be clearly documented and reviewed with relevant engineers. In this paper, the design process of a new ATE system by using systematic design approach is discussed. This process is followed during the design of the ATE system which is under use from the beginning of the year 2013. The challenges in the design process, determining the requirements and the formation of hardware and software architecture are explained in detail benefiting from real experiences.",1088-7725;1558-4550,978-1-4673-5683-1978-1-4673-5681,10.1109/AUTEST.2013.6645035,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645035,Automatic Test Equipment;requirements development process;systematic design approach;rack design;PXI;LXI,Instruments;Hardware;Software;Software architecture;Switches;Computers;Cooling,automatic test equipment;automatic test software;automatic testing;design for testability,automatic test equipments;systematic design approaches;test program sets;ATE system requirements;hardware architecture;software architecture;design process,,4,4,,,,,,IEEE,IEEE Conferences
Efficient Automated Program Repair through Fault-Recorded Testing Prioritization,Y. Qi; X. Mao; Y. Lei,NA; NA; NA,2013 IEEE International Conference on Software Maintenance,,2013,,,180,189,"Most techniques for automated program repair use test cases to validate the effectiveness of the produced patches. The validation process can be time-consuming especially when the object programs ship with either lots of test cases or some long-running test cases. To alleviate the cost for testing, we first introduce regression test prioritization insight into the area of automated program repair, and present a novel prioritization technique called FRTP with the goal of reducing the number of test case executions in the repair process. Unlike most existing prioritization techniques frequently requiring additional cost for gathering previous test executions information, FRTP iteratively extracts that information just from the repair process, and thus incurs trivial performance lose. We also built a tool called TrpAutoRepair, which implements our FRTP technique and has the ability of automatically repairing C programs. To evaluate TrpAutoRepair, we compared it with GenProg, a state-of-the-art tool for automated C program repair. The experiment on the 5 subject programs with 16 real-life bugs provides evidence that TrpAutoRepair performs at least as good as GenProg in term of success rate, in most cases (15/16), TrpAutoRepair can significantly improve the repair efficiency by reducing efficiently the test case executions when searching a valid patch in the repair process.",1063-6773,978-0-7695-4981,10.1109/ICSM.2013.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676889,automated program repair;test case prioritization;efficiency;automated debugging,Maintenance engineering;Indexes;Testing;Fault detection;Data mining;Context;Computer bugs,program debugging;program testing;regression analysis;statistical testing,automated debugging;test case prioritization;GenProg;FRTP technique;TrpAutoRepair;regression test prioritization;fault-recorded testing prioritization;automated program repair,,22,29,,,,,,IEEE,IEEE Conferences
Evolutionary Search Algorithms for Test Case Prioritization,S. K. Mohapatra; S. Prasad,NA; NA,2013 International Conference on Machine Intelligence and Research Advancement,,2013,,,115,119,"To improve the effectiveness of certain performance goals, test case prioritization techniques are used. These technique schedule the test cases in particular order for execution so as to increase the efficacy in meeting the performance goals. For every change in the program it is considered inefficient to re-execute each and every test case. Test case prioritization techniques arrange the test cases within a test suite in such a way that the most important test case is executed first. This process enhances the effectiveness of testing. This algorithm during time constraint execution has been shown to have detected maximum number fault while including the sever test cases.",,978-0-7695-5013,10.1109/ICMIRA.2013.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918806,Regression testing;Test case;Genetic algorithm;prioritization,Genetic algorithms;Testing;Software;Sociology;Statistics;Algorithm design and analysis;Fault detection,evolutionary computation;program testing;search problems;software fault tolerance,evolutionary search algorithms;performance goals;test case prioritization techniques;test cases schedule;test suite;testing effectiveness;time constraint execution;fault,,1,19,,,,,,IEEE,IEEE Conferences
Getting more from requirements traceability: Requirements testing progress,C. Ziftci; I. Kr?¬ger,"Department of Computer Science and Engineering, University of California, San Diego, La Jolla, USA; Department of Computer Science and Engineering, University of California, San Diego, La Jolla, USA",2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE),,2013,,,12,18,"Requirements Engineering (RE) and Testing are important steps in many software development processes. It is critical to monitor the progress of the testing phase to allocate resources (person-power, time, computational resources) properly, and to make sure the prioritization of requirements are reflected during testing, i.e. more critical requirements are given higher priority and tested well. In this paper, we propose a new metric to help stakeholders monitor the progress of the testing phase from a requirements perspective, i.e. which requirements are tested adequately, and which ones insufficiently. Unlike existing progress related metrics, such as code coverage and MC/DC (modified condition/decision) coverage, this metric is on the requirements level, not source code level. We propose to automatically reverse engineer this metric from the existing test cases of a system. We also propose a method to evaluate this metric, and report the results of three case studies. On these case studies, our technique obtains results within 75.23% - 91.11% of the baseline on average.",2157-2186;2157-2194,978-1-4799-0495,10.1109/TEFSE.2013.6620148,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620148,Requirements traceability;requirements testing progress;test monitoring;requirements coverage;automated analysis;reverse engineering,Testing;Measurement;Context;Lattices;Software;Vectors;Monitoring,program testing;reverse engineering;software engineering;software metrics,requirements traceability;requirements testing progress;progress related metrics;modified condition-decision coverage;source code level;reverse engineer;software development,,1,28,,,,,,IEEE,IEEE Conferences
History-Based Test Case Prioritization with Software Version Awareness,C. Lin; C. Chen; C. Tsai; G. M. Kapfhammer,NA; NA; NA; NA,2013 18th International Conference on Engineering of Complex Computer Systems,,2013,,,171,172,"Test case prioritization techniques schedule the test cases in an order based on some specific criteria so that the tests with better fault detection capability are executed at an early position in the regression test suite. Many existing test case prioritization approaches are code-based, in which the testing of each software version is considered as an independent process. Actually, the test results of the preceding software versions may be useful for scheduling the test cases of the later software versions. Some researchers have proposed history-based approaches to address this issue, but they assumed that the immediately preceding test result provides the same reference value for prioritizing the test cases of the successive software version across the entire lifetime of the software development process. Thus, this paper describes ongoing research that studies whether the reference value of the immediately preceding test results is version-aware and proposes a test case prioritization approach based on our observations. The experimental results indicate that, in comparison to existing approaches, the presented one can schedule test cases more effectively.",,978-0-7695-5007-7978-0-7695-5007,10.1109/ICECCS.2013.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601820,Regression Testing;Test Case Prioritization,Software;Testing;Fault detection;Schedules;Software engineering;Computers;Educational institutions,fault diagnosis;program testing;regression analysis;scheduling;software reliability,history-based test case prioritization techniques;software version awareness;fault detection;regression test suite;software version testing;scheduling;software development process,,3,7,,,,,,IEEE,IEEE Conferences
"Leveraging the Crowd: How 48,000 Users Helped Improve Lync Performance",R. Musson; J. Richards; D. Fisher; C. Bird; B. Bussone; S. Ganguly,Microsoft; Microsoft; Microsoft Research; Microsoft Research; Microsoft; Microsoft,IEEE Software,,2013,30,4,38,45,"Performance is a critical component of customer satisfaction with network-based applications. Unfortunately, accurately evaluating the performance of collaborative software that operates in extremely heterogeneous environments is difficult with traditional techniques such as modeling workloads or testing in controlled environments. To evaluate performance of an application in the wild during development, the authors deployed early versions of the software, collecting performance data from application users for key usage scenarios. The analysis package they used produces visualizations to help development teams identify and prioritize performance issues by focusing on performance early in the development cycle, evaluating progress, identifying defects, and estimating timelines.",0740-7459;1937-4194,,10.1109/MS.2013.67,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509371,software performance;data collection;data analysis;performance monitoring;software analytics;software data visualization,Software development;Performance evaluation;Customer satisfaction;Software quality;Analytical models,groupware;software engineering,Lync performance;customer satisfaction;collaborative software;software development;data visualization,,13,2,,,,,,IEEE,IEEE Journals & Magazines
Managing technical debt: An industrial case study,Z. Codabux; B. Williams,"Dept. of Computer Science and Engineering Mississippi State University Starkville, MS, USA; Dept. of Computer Science and Engineering Mississippi State University Starkville, MS, USA",2013 4th International Workshop on Managing Technical Debt (MTD),,2013,,,8,15,"Technical debt is the consequence of trade-offs made during software development to ensure speedy releases. The research community lacks rigorously evaluated guidelines to help practitioners characterize, manage and prioritize debt. This paper describes a study conducted with an industrial partner during their implementation of Agile development practices for a large software development division within the company. The report contains our initial findings based on ethnographic observations and semi-structured interviews. The goal is to identify the best practices regarding managing technical debt so that the researchers and the practitioners can further evaluate these practices to extend their knowledge of the technical debt metaphor. We determined that the developers considered their own taxonomy of technical debt based on the type of work they were assigned and their personal understanding of the term. Despite management's high-level categories, the developers mostly considered design debt, testing debt and defect debt. In addition to developers having their own taxonomy, assigning dedicated teams for technical debt reduction and allowing other teams about 20% of time per sprint for debt reduction are good initiatives towards lowering technical debt. While technical debt has become a well-regarded concept in the Agile community, further empirical evaluation is needed to assess how to properly apply the concept for various development organizations.",,978-1-4673-6443,10.1109/MTD.2013.6608672,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608672,technical debt Agile methods industrial case study;Scrum;semi-structure interviews,Software;Interviews;Organizations;Encoding;Taxonomy;Maintenance engineering;Training,software prototyping,managing technical debt;industrial case study;software development;industrial partner;agile development practices;software development division;ethnographic observations;development organizations,,18,13,,,,,,IEEE,IEEE Conferences
On Combining Model-Based Analysis and Testing,M. Saadatmand; M. Sj??din,NA; NA,2013 10th International Conference on Information Technology: New Generations,,2013,,,260,266,"Testing a computer system is a challenging task, both due to the large number of possible test cases and the limited resources allocated for testing activities. This means that only a subset of all possible test cases can be chosen to test a system, and therefore the decision on the selection of test cases becomes important. The result of static analysis of a system can be used to help with this decision, in the context of model-based development of systems, this means that the analysis performed on a system model can be used to prioritize and guide the testing efforts. Furthermore, since models allow expression of non-functional requirements (such as performance, timing and security), model-guided testing can be used to direct testing towards specific parts of the system which have large impact on such requirements. In this paper, we focus on modeling and trade-off analysis of non-functional requirements and how static analysis helps to identify problematic parts of a system and thus guide the selection of test cases to target such parts.",,978-0-7695-4967-5978-0-7695-4967,10.1109/ITNG.2013.42,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614319,Model-based development;static analysis;model-based testing;non-functional requirements;test-case prioritization,Unified modeling language;Testing;Analytical models;Software;Security;Timing;Batteries,program diagnostics;systems analysis,model-based analysis;model-based testing;computer system testing;static analysis;model-based development;nonfunctional requirements;model-guided testing;trade-off analysis,,3,20,,,,,,IEEE,IEEE Conferences
On the Correlation between the Effectiveness of Metamorphic Relations and Dissimilarities of Test Case Executions,Y. Cao; Z. Q. Zhou; T. Y. Chen,NA; NA; NA,2013 13th International Conference on Quality Software,,2013,,,153,162,"Metamorphic testing (MT) is a property-based automated software testing method. It alleviates the oracle problem by testing programs against metamorphic relations (MRs), which are necessary properties among multiple executions of the target program. For a given problem, usually more than one MR can be identified. It is therefore of practical importance for testers to know the nature of good MRs, that is, which MRs are likely to have higher chances of revealing failures. To address this issue we investigate the correlation between the fault-detection effectiveness of MRs and the dissimilarity (distance) of test case execution profiles. Empirical study results reveal that there is a strong and statistically significant positive correlation between the fault-detection effectiveness and the distance. The findings of this research can help to develop automated means of selecting/prioritizing MRs for cost-effective metamorphic testing.",1550-6002;2332-662X,978-0-7695-5039,10.1109/QSIC.2013.43,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605921,Software testing;metamorphic testing;metamorphic relation;fault-detection effectiveness;execution dissimilarity;distance measurement;initial execution;follow-up execution,Testing;Measurement;Educational institutions;Correlation;Vectors;Encyclopedias;Computer science,fault diagnosis;program testing;statistical analysis,metamorphic relations;test case execution profile dissimilarities;MT;property-based automated software testing method;oracle problem;program testing;test case execution profile distance;empirical study;statistical analysis;fault-detection effectiveness;MR selection;metamorphic testing;MR prioritization,,9,22,,,,,,IEEE,IEEE Conferences
On the Gain of Measuring Test Case Prioritization,J. Lv; B. Yin; K. Cai,NA; NA; NA,2013 IEEE 37th Annual Computer Software and Applications Conference,,2013,,,627,632,"Test case prioritization (TCP) techniques aim to schedule the order of regression test suite to maximize some properties, such as early fault detection. In order to measure the abilities of different TCP techniques for early fault detection, a metric named average percentage of faults detected (APFD) is widely adopted. In this paper, we analyze the metric APFD and explore the gain of measuring TCP techniques from a control theory viewpoint. Based on that, we propose a generalized metric for TCP. This new metric focuses on the gain of defining early fault detection and measuring TCP techniques for various needs in different evaluation scenarios. By adopting this new metric, not only flexibility can be guaranteed, but also explicit physical significance for the metric will be provided before evaluation.",0730-3157,978-0-7695-4986,10.1109/COMPSAC.2013.101,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649891,regression testing;test case prioritization;software metric;software cybernetics,Fault detection;Testing;Gain measurement;Software;Weight measurement;Approximation methods,program testing;software metrics,test case prioritization techniques;regression test suite;TCP techniques;average percentage of faults detected;APFD;control theory viewpoint,,,15,,,,,,IEEE,IEEE Conferences
On the Influence of Model Structure and Test Case Profile on the Prioritization of Test Cases in the Context of Model-Based Testing,J. F. S. Ouriques; E. G. Cartaxo; P. D. L. Machado,NA; NA; NA,2013 27th Brazilian Symposium on Software Engineering,,2013,,,119,128,"Test case prioritization techniques aim at defining an ordering of test cases that favor the achievement of a goal during test execution, such as revealing faults as earlier as possible. A number of techniques have already been proposed and investigated in the literature and experimental results have discussed whether a technique is more successful than others. However, in the context of model-based testing, only a few attempts have been made towards either proposing or experimenting test case prioritization techniques. Moreover, a number of factors that may influence on the results obtained still need to be investigated before more general conclusions can be reached. In this paper, we present empirical studies that focus on observing the effects of two factors: the structure of the model and the profile of the test case that fails. Results show that the profile of the test case that fails may have a definite influence on the performance of the techniques investigated.",,978-0-7695-5165,10.1109/SBES.2013.4,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800188,Experimental Software Engineering;Software Testing;Model-Based Testing;Test Case Prioritization,Context;Unified modeling language;Testing;Fault detection;Context modeling;Software;Redundancy,fault diagnosis;program testing;software engineering,test case prioritization technique;model-based testing;test case ordering;test execution;empirical analysis;model structure;test case profile,,2,29,,,,,,IEEE,IEEE Conferences
Optimization of test suite-test case in regression test,A. S. A. Ansari; K. K. Devadkar; P. Gharpure,"Department of Computer Engineering, Sardar Patel Institute of Technology, University Of Mumbai, Mumbai, India; Department of Information Technology, Sardar Patel Institute of Technology, University Of Mumbai, Mumbai, India; Department of Computer Engineering, Sardar Patel Institute of Technology, University Of Mumbai, Mumbai, India",2013 IEEE International Conference on Computational Intelligence and Computing Research,,2013,,,1,4,"Exhaustive product evolution and testing is required to ensure the quality of product. Regression testing is crucial to ensure software excellence. Regression test cases are applied to assure that new or adapted features do not relapse the existing features. As innovative features are included, new test cases are generated to assess the new functionality, and then included in the existing pool of test cases, thus escalating the cost and the time required in performing regression test and this unswervingly impacts the release, laid plan and the quality of the product. Hence there is a need to select minimal test cases that will test all the functionalities of the engineered product and it must rigorously test the functionalities that have high risk exposure. Test Suite-Test Case Refinement Technique will reduce regression test case pool size, reduce regression testing time, cost &amp; effort and also ensure the quality of the engineered product. This technique is a regression test case optimization technique that is a hybrid of Test Case Minimization based on specifications and Test Case Prioritization based on risk exposure. This approach will facilitate achievement of quality product with decreased regression testing time and cost yet uncover same amount of errors as the original test cases.",,978-1-4799-1597-2978-1-4799-1594-1978-1-4799-1595,10.1109/ICCIC.2013.6724206,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724206,Test Case Optimization;Regression Test;Test Suite Prioritization;Test Suite Minimization;Risk Based Prioritization;Specification Based Selection,Minimization;Testing;Software;Conferences;Fault detection;Optimization;Software reliability,program testing;software quality,test suite-test case optimization;regression test;product quality;software excellence;test suite-test case refinement technique;test case minimization;test case prioritization;risk exposure,,1,10,,,,,,IEEE,IEEE Conferences
Prioritizing Variable-Strength Covering Array,R. Huang; J. Chen; T. Zhang; R. Wang; Y. Lu,NA; NA; NA; NA; NA,2013 IEEE 37th Annual Computer Software and Applications Conference,,2013,,,502,511,"Combinatorial interaction testing is a well-studied testing strategy, and has been widely applied in practice. Combinatorial interaction test suite, such as fixed-strength and variable-strength interaction test suite, is widely used for combinatorial interaction testing. Due to constrained testing resources in some applications, for example in combinatorial interaction regression testing, prioritization of combinatorial interaction test suite has been proposed to improve the efficiency of testing. However, nearly all prioritization techniques may only support fixed-strength interaction test suite rather than variable-strength interaction test suite. In this paper, we propose two heuristic methods in order to prioritize variable-strength interaction test suite by taking advantage of its special characteristics. The experimental results show that our methods are more effective for variable-strength interaction test suite by comparing with the technique of prioritizing combinatorial interaction test suites according to test case generation order, the random test prioritization technique, and the fixed-strength interaction test suite prioritization technique. Besides, our methods have additional advantages compared with the prioritization techniques for fixed-strength interaction test suite.",0730-3157,978-0-7695-4986,10.1109/COMPSAC.2013.84,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649874,Software testing;combinatorial interaction testing;test case prioritization;fixed-strength interaction test suite;variable-strength interaction test suite;algorithm,Testing;Arrays;Educational institutions;Fault detection;Heuristic algorithms;Computer science;Software,program testing,variable-strength covering array;combinatorial interaction testing;testing strategy;combinatorial interaction test suite;fixed-strength interaction test suite;variable-strength interaction test suite;constrained testing resources;combinatorial interaction regression testing;testing efficiency;test case generation order;random test prioritization technique;fixed-strength interaction test suite prioritization technique,,6,35,,,,,,IEEE,IEEE Conferences
Randomizing regression tests using game theory,N. Kukreja; W. G. J. Halfond; M. Tambe,"University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA",2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE),,2013,,,616,621,"As software evolves, the number of test-cases in the regression test suites continues to increase, requiring testers to prioritize their execution. Usually only a subset of the test cases is executed due to limited testing resources. This subset is often known to the developers who may try to ƒ??gameƒ? the system by committing insufficiently tested code for parts of the software that will not be tested. In this new ideas paper, we propose a novel approach for randomizing regression test scheduling, based on Stackelberg games for deployment of scarce resources. We apply this approach to randomizing test cases in such a way as to maximize the testers' expected payoff when executing the test cases. Our approach accounts for resource limitations (e.g., number of testers) and provides a probabilistic distribution for scheduling test cases. We provide an example application of our approach showcasing the idea of using Stackelberg games for randomized regression test scheduling.",,978-1-4799-0215,10.1109/ASE.2013.6693122,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693122,,Games;Testing;Security;Equations;Vectors;Game theory;Schedules,game theory;processor scheduling;program testing;regression analysis;statistical distributions,randomizing regression tests;game theory;software evolution;regression test suites;randomizing regression test scheduling;Stackelberg games;probabilistic distribution;scheduling test cases;randomized regression test scheduling,,2,20,,,,,,IEEE,IEEE Conferences
Regression Testing Prioritization Based on Model Checking for Safety-Crucial Embedded Systems,Fuzhen Sun; Yan Li,"Beijing Key Lab. of Intell. Inf. Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Comput. Sci. &amp; Technol., Shandong Univ. of Technol., Zibo, China",2013 Fourth International Conference on Digital Manufacturing & Automation,,2013,,,979,983,"The order in which test-cases are executed has an influence on the rate at which faults can be detected. In this paper we demonstrate how test-case prioritization can be performed with the use of model-checkers. For this, different well known prioritization techniques are adapted for model-based use. New property based prioritization techniques are introduced. In addition it is shown that prioritization can be done at test-case generation time, thus removing the need for test-suite post-processing. Several experiments for safety-crucial embedded systems are used to show the validity of these ideas.",,978-0-7695-5016,10.1109/ICDMA.2013.229,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598153,Test Case Prioritization;Software Testing;Model Checking;Property Testing,Manufacturing;Automation,embedded systems;fault diagnosis;program testing;program verification;regression analysis;safety-critical software,regression testing prioritization;model checking;safety-crucial embedded systems;fault detection;test-case prioritization;model-checkers;model-based use;property based prioritization techniques;test-case generation time,,,10,,,,,,IEEE,IEEE Conferences
Research on optimization scheme of regression testing,S. Sun; X. Hou; C. Gao; L. Sun,"School of Computer Science &amp; Engineering, Changchun University of Technology, Changchun, China; School of Computer Science &amp; Engineering, Changchun University of Technology, Changchun, China; School of Computer Science &amp; Engineering, Changchun University of Technology, Changchun, China; School of Computer Science &amp; Engineering, Changchun University of Technology, Changchun, China",2013 Ninth International Conference on Natural Computation (ICNC),,2013,,,1628,1632,"Regression testing is an important process during software development. In order to reduce costs of regression testing, research on optimization of scheme of regression testing have been done in this paper. For the purpose of reducing the number of test cases and detecting faults of programs early, this paper proposed to combine test case selection with test case prioritization. Regression testing process has been designed and optimization of testing scheme has been implemented. The criterion of test case selection is modify impact of programs, finding programs which are impacted by program modification according to modify information of programs and dependencies between programs. Test cases would be selected during test case selection. The criterion of test case prioritization is coverage ability and troubleshooting capabilities of test case. Test cases which have been selected during test case selection would be ordering in test case prioritization. Finally, the effectiveness of the new method is discussed.",2157-9563;2157-9555,978-1-4673-4714,10.1109/ICNC.2013.6818242,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818242,regression testing;test case selection;test case prioritization,Testing;Software;Optimization;Circuit faults;Convergence;Educational institutions;Computer aided software engineering,cost reduction;optimisation;program testing;regression analysis;software cost estimation;software fault tolerance;software maintenance;statistical testing,software development;regression testing;program fault detection;test case prioritization;testing scheme optimization;test case selection criterion;program modification;troubleshooting capability;coverage ability;software cost reduction;software maintenance,,1,8,,,,,,IEEE,IEEE Conferences
Reusing black box test paths for white box testing of websites,R. Chopra; S. Madan,"Computer Science Engg./ IT, GTBIT, GGSIPU DELHI, India; Computer Science Department, University of Delhi, India",2013 3rd IEEE International Advance Computing Conference (IACC),,2013,,,1345,1350,"As the numbers of web users are increasing exponentially, the software complexity is increasing exponentially and the malwares are increasing exponentially, so exhaustive and extensive testing of websites has become a necessity today. But testing of a website is not 100% exhaustive as the page explosion problem is also very usual. In this paper, we propose to reuse the basis test paths as obtained from the Page-Test-Trees (PTTs) for white box testing of websites. We traverse the same set of paths (obtained above) and test for the source code at these nodes. This saves significant amount of time required to generate test paths and hence test cases as compared to the existing approaches of white box testing. The cost and efforts are also minimized. The proposed technique ensures better website testing coverage as white box testing provides better results than black box testing. Then we validate the proposed reusability testing with two web navigational structures. The results show that doing regression testing can save several billion dollars. These test cases can be further minimized by using prioritization techniques of regression testing.",,978-1-4673-4529-3978-1-4673-4527-9978-1-4673-4528,10.1109/IAdCC.2013.6514424,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514424,Website testing;Website-Under-Test (WUT);Page Flow Diagrams;Navigation;Page test Tree;Path Testing and Regression testing,Testing;Phase frequency detector;Navigation;Complexity theory;Explosions;Conferences;Software,invasive software;program testing;regression analysis;statistical testing;Web sites,black box test path;white box testing;Web site;software complexity;malware;page explosion problem;page-test-trees;PTT;test path generation;Web site testing coverage;reusability testing;prioritization technique;regression testing,,,24,,,,,,IEEE,IEEE Conferences
RisCal -- A Risk Estimation Tool for Software Engineering Purposes,C. Haisjackl; M. Felderer; R. Breu,NA; NA; NA,2013 39th Euromicro Conference on Software Engineering and Advanced Applications,,2013,,,292,299,"Decision making in software engineering requires the consideration of risk information. The reliability of risk information is strongly influenced by the underlying risk estimation process which consists of the steps risk identification, risk analysis and risk prioritization. In this paper we present a novel risk estimation tool for software engineering pruposes called RisCal. RisCal is based on a generic risk model and supports the integration of manually and automatically determined metrics into the risk estimation. This makes the tool applicable for arbitrary software engineering activities like risk-based testing or release planning. We show how RisCal supports risk identification, analysis and prioritizations, provide an estimation example, and discuss its application to risk-based testing and release planning.",1089-6503;2376-9505,978-0-7695-5091,10.1109/SEAA.2013.10,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619524,Risk Estimation;Software Risk Management;Risk-based Testing;Release Planning;Test Management,Estimation;Measurement;Testing;Risk management;Planning;Software engineering,decision making;program testing;risk analysis;software metrics,RisCal;risk estimation tool;risk information;risk estimation process;risk identification;risk analysis;risk prioritization;generic risk model;manually determined metrics;automatically determined metrics;software engineering activities;risk-based testing;release planning;software engineering pruposes;decision making,,2,23,,,,,,IEEE,IEEE Conferences
Selection and Prioritization of Test Cases by Combining White-Box and Black-Box Testing Methods,S. Kukolj; V. Marinkovic; M. Popovic; S. Bogn?­r,NA; NA; NA; NA,2013 3rd Eastern European Regional Conference on the Engineering of Computer Based Systems,,2013,,,153,156,"In this paper, we present a methodology that combines both white-box and black-box testing, in order to improve testing quality for a given class of embedded systems. The goal of this methodology is generation of test cases for the new functional testing campaign based on the test coverage information from the previous testing campaign, in order to maximize the test coverage. Test coverage information is used for selection of proper test cases in order to improve the quality of testing and save available resources for testing. As an output, a set of test cases is produced. Generated test cases are processed by the test Executor application that decides whether results have passed or failed, based on the results of image grabbing, OCR text extraction, and comparison with expected text. The presented methodology is finally validated by means of a case-study targeting an Android device. The results of the case study are affirmative and they indicate that the proposed methodology is applicable for testing embedded systems of this kind.",,978-0-7695-5064,10.1109/ECBS-EERC.2013.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664523,embedded systems;quality assurance;black-box testing;white-box testing;code coverage,Testing;Instruments;Embedded systems;Multimedia communication;Digital TV;Performance evaluation,embedded systems;program testing;software quality,test case selection;test case prioritization;white-box testing methods;black-box testing methods;embedded systems;functional testing campaign;test coverage information;testing quality;Executor application;image grabbing;OCR text extraction;Android device,,3,7,,,,,,IEEE,IEEE Conferences
SMT Malleability in IBM POWER5 and POWER6 Processors,A. Morari; C. Boneti; F. J. Cazorla; R. Gioiosa; C. Cher; A. Buyuktosunoglu; P. Bose; M. Valero,"Barcelona Supercomputing Center, Barcelona and Univesitat Politecnica de Catalunya; Schlumberger BRGC, Rio de Janeiro; Barcelona Supercomputing Center, Barcelona and Spanish National Research Council; Barcelona Supercomputing Center, Barcelona; IBM T.J. Watson Research Center, Yorktown Heights; IBM T.J. Watson Research Center, Yorktown Heights; IBM T.J. Watson Research Center, Yorktown Heights; Barcelona Supercomputing Center, Barcelona and Spanish National Research Council",IEEE Transactions on Computers,,2013,62,4,813,826,"While several hardware mechanisms have been proposed to control the interaction between hardware threads in an SMT processor, few have addressed the issue of software-controllable SMT performance. The IBM POWER5 and POWER6 are the first high-performance processors implementing a software-controllable hardware-thread prioritization mechanism that controls the rate at which each hardware-thread decodes instructions. This paper shows the potential of this basic mechanism to improve several target metrics for various applications on POWER5 and POWER6 processors. Our results show that although the software interface is exactly the same, the software-controlled priority mechanism has a different effect on POWER5 and POWER6. For instance, hardware threads in POWER6 are less sensitive to priorities than in POWER5 due to the in order design. We study the SMT thread malleability to enable user-level optimizations that leverage software-controlled thread priorities. We also show how to achieve various system objectives such as parallel application load balancing, in order to reduce execution time. Finally, we characterize user-level transparent execution on POWER5 and POWER6, and identify the workload mix that best benefits from it.",0018-9340;1557-9956;2326-3814,,10.1109/TC.2012.34,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138854,Malleability;simultaneous multithreading;hardware-thread priorities;IBM POWER5;IBM POWER6,Instruction sets;Hardware;Benchmark testing;Kernel;Linux,microprocessor chips;multi-threading;resource allocation,SMT malleability;IBM POWER5 processor;IBM POWER6 processor;hardware mechanism;hardware thread;software-controllable SMT performance;SMT processor;software-controllable hardware-thread prioritization mechanism;software interface;software-controlled priority mechanism;user-level optimization;parallel application load balancing;execution time;user-level transparent execution;simultaneous multithreading,,2,28,,,,,,IEEE,IEEE Journals & Magazines
Software components prioritization using OCL formal specification for effective testing,A. Jalila; D. J. Mala,"Department of Computer Applications, Thiagarajar College of Engineering, Madurai, India; Department of Computer Applications, Thiagarajar College of Engineering, Madurai, India",2013 International Conference on Recent Trends in Information Technology (ICRTIT),,2013,,,714,720,"In soft real time system development, testing effort minimization is a challenging task. Earlier research has shown that often a small percentage of components are responsible for most of the faults reported at the later stages of software development. Due to the time and other resource constraints, fault-prone components are ignored during testing activity which leads to compromises on software quality. Thus there is a need to identify fault-prone components of the system based on the data collected at the early stages of software development. The major focus of the proposed methodology is to identify and prioritize fault-prone components of the system using its OCL formal specifications. This approach enables testers to distribute more effort on fault-prone components than non fault-prone components of the system. The proposed methodology is illustrated based on three case study applications.",,978-1-4799-1024,10.1109/ICRTIT.2013.6844288,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844288,Critical Components;UML (Unified Modeling Language);Formal Specification;OCL (Object Constraints Language);Design Metrics,Measurement;Complexity theory;Unified modeling language;Context;Software;Object oriented modeling;Indexes,formal specification;program testing;software fault tolerance;software quality,software components prioritization;OCL formal specification;effective testing;soft real time system development;testing effort minimization;software development;fault-prone components;testing activity;software quality,,,23,,,,,,IEEE,IEEE Conferences
Software defect prediction using software metrics - A survey,K. Punitha; S. Chitra,"Bhajarang Engineering College, Tiruvallur, Chennai, TamilNadu, India; Er. Perumal Manimekalai College of Engineering, Hosur, Krishnagiri, Tamil Nadu, India",2013 International Conference on Information Communication and Embedded Systems (ICICES),,2013,,,555,558,"Traditionally software metrics have been used to define the complexity of the program, to estimate programming time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the metric values are to be used in mathematical equations designed to represent a model of the software process, metrics associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully apply. Work on the mechanics of implementing metrics programs. The goal of this research is to help developers identify defects based on existing software metrics using data mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in the development and maintenance phase. This research focuses in identifying defective modules and hence the scope of software that needs to be examined for defects can be prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology helps in identifying modules that require immediate attention and hence the reliability of the software can be improved faster as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the Data mining algorithm. To initiate this process we initially propose to evaluate the existing classification algorithms and based on its weakness we propose a novel Neural network algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy.",,978-1-4673-5788-3978-1-4673-5786-9978-1-4673-5787,10.1109/ICICES.2013.6508369,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508369,Software defect prediction;software defectproneness prediction;machine learning;scheme evaluation,Software;Predictive models;Accuracy;Computer bugs;Measurement;Software reliability;Testing,data mining;fuzzy set theory;neural nets;pattern classification;program diagnostics;software cost estimation;software maintenance;software metrics;software quality;software reliability,software defect prediction;software metrics;program complexity;programming time estimation;mathematical equations;software process;ratio scale data;mathematical operations;metrics programs;data mining techniques;software quality;software development cost;software maintenance phase;software development phase;defective module identification;software reliability;data mining algorithm;neural network algorithm;fuzziness degree,,4,10,,,,,,IEEE,IEEE Conferences
Suitable placements of multiple FACTS devices to improve the transient stability using trajectory sensitivity analysis,A. Nasri; R. Eriksson; M. Ghandhari,"Electric Power System Department, KTH Royal Institute of Technology, Stockholm, Sweden; Electric Power System Department, KTH Royal Institute of Technology, Stockholm, Sweden; Electric Power System Department, KTH Royal Institute of Technology, Stockholm, Sweden",2013 North American Power Symposium (NAPS),,2013,,,1,6,"Trajectory sensitivity analysis (TSA) is used as a tool for suitable placement of multiple series compensators in the power system. The goal is to maximize the benefit of these devices in order to enhance the transient stability of the system. For this purpose, the trajectory sensitivities of the rotor angles of the most critical generators with respect to the reactances of transmission lines are calculated in the presence of the most severe faults. Based on the obtained trajectory sensitivities, a method is proposed to determine how effective the series compensation of each transmission line is for improving the transient stability. This method is applied to the Nordic-32 test system to find the priorities of the transmission lines for installation of several series compensators. Simulation with industrial software shows the validity and efficiency of the proposed method.",,978-1-4799-1255,10.1109/NAPS.2013.6666828,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666828,,Power system stability;Trajectory;Sensitivity;Power transmission lines;Generators;Stability analysis;Power system transients,flexible AC transmission systems;power system transient stability;rotors;sensitivity analysis,Nordic-32 test system;transmission line reactance;most critical generator;rotor angle;multiple series compensator;trajectory sensitivity analysis;transient stability;multiple FACTS device,,,12,,,,,,IEEE,IEEE Conferences
Test Case Prioritization for Continuous Regression Testing: An Industrial Case Study,D. Marijan; A. Gotlieb; S. Sen,NA; NA; NA,2013 IEEE International Conference on Software Maintenance,,2013,,,540,543,"Regression testing in continuous integration environment is bounded by tight time constraints. To satisfy time constraints and achieve testing goals, test cases must be efficiently ordered in execution. Prioritization techniques are commonly used to order test cases to reflect their importance according to one or more criteria. Reduced time to test or high fault detection rate are such important criteria. In this paper, we present a case study of a test prioritization approach ROCKET (Prioritization for Continuous Regression Testing) to improve the efficiency of continuous regression testing of industrial video conferencing software. ROCKET orders test cases based on historical failure data, test execution time and domain-specific heuristics. It uses a weighted function to compute test priority. The weights are higher if tests uncover regression faults in recent iterations of software testing and reduce time to detection of faults. The results of the study show that the test cases prioritized using ROCKET (1) provide faster fault detection, and (2) increase regression fault detection rate, revealing 30% more faults for 20% of the test suite executed, comparing to manually prioritized test cases.",1063-6773,978-0-7695-4981,10.1109/ICSM.2013.91,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676952,software testing;continuous integration;regression testing;test case prioritization;history-based prioritization,Testing;Rockets;Fault detection;Software;Manuals;Time factors;Linear programming,fault diagnosis;program testing;regression analysis;software reliability;statistical testing;teleconferencing,test case prioritization technique;industrial case study;continuous integration environment;time constraints;high fault detection rate;prioritization for continuous regression testing;ROCKET;industrial video conferencing software;historical failure data;test execution time;domain-specific heuristics;weighted function;test priority;regression faults;software testing;fault detection rate,,20,9,,,,,,IEEE,IEEE Conferences
Test Case Prioritization Using Requirements-Based Clustering,M. J. Arafeen; H. Do,NA; NA,"2013 IEEE Sixth International Conference on Software Testing, Verification and Validation",,2013,,,312,321,"The importance of using requirements information in the testing phase has been well recognized by the requirements engineering community, but to date, a vast majority of regression testing techniques have primarily relied on software code information. Incorporating requirements information into the current testing practice could help software engineers identify the source of defects more easily, validate the product against requirements, and maintain software products in a holistic way. In this paper, we investigate whether the requirements-based clustering approach that incorporates traditional code analysis information can improve the effectiveness of test case prioritization techniques. To investigate the effectiveness of our approach, we performed an empirical study using two Java programs with multiple versions and requirements documents. Our results indicate that the use of requirements information during the test case prioritization process can be beneficial.",2159-4848,978-0-7695-4968-2978-1-4673-5961,10.1109/ICST.2013.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569743,regression testing;test case prioritization;requirements-based clustering;empirical study,Testing;Software;Measurement;Complexity theory;Fault detection;Educational institutions;Java,formal specification;Java;pattern clustering;program testing,test case prioritization process;requirements documents;Java programs;test case prioritization techniques;traditional code analysis information;defects source identification;software engineers;regression testing techniques;requirements engineering;requirements information;requirements-based clustering,,37,35,,,,,,IEEE,IEEE Conferences
Test suite prioritisation using trace events technique,K. Rajarathinam; S. Natarajan,"Department of Computer Science and Engineering, Velammal College of Engineering and Technology, Madurai, Tamilnadu, India; Department of Electronics and Communication Engineering, Velammal College of Engineering and Technology, Madurai, Tamilnadu, India",IET Software,,2013,7,2,85,92,"The size of the test suite and the duration of time determines the time taken by the regression testing. Conversely, the testers can prioritise the test cases by the use of a competent prioritisation technique to obtain an increased rate of fault detection in the system, allowing for earlier corrections, and getting higher overall confidence that the software has been tested suitably. A prioritised test suite is more likely to be more effective during that time period than would have been achieved via a random ordering if execution needs to be suspended after some time. An enhanced test case ordering may be probable if the desired implementation time to run the test cases is proven earlier. This research work's main intention is to prioritise the regressiontesting test cases. In order to prioritise the test cases some factors are considered here. These factors are employed in the prioritisation algorithm. The trace events are one of the important factors, used to find the most significant test cases in the projects. The requirement factor value is calculated and subsequently a weightage is calculated and assigned to each test case in the software based on these factors by using a thresholding technique. Later, the test cases are prioritised according to the weightage allocated to them. Executing the test cases based on the prioritisation will greatly decreases the computation cost and time. The proposed technique is efficient in prioritising the regression test cases. The new prioritised subsequences of the given unit test suites are executed on Java programs after the completion of prioritisation. Average of the percentage of faults detected is an evaluation metric used for evaluating the 'superiority' of these orderings.",1751-8806;1751-8814,,10.1049/iet-sen.2011.0203,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519507,,,,,,,33,,,,,,IET,IET Journals & Magazines
Using Dependency Structures for Prioritization of Functional Test Suites,S. Haidry; T. Miller,"University of Melbourne, Parkville; University of Melbourne, Parkville",IEEE Transactions on Software Engineering,,2013,39,2,258,275,"Test case prioritization is the process of ordering the execution of test cases to achieve a certain goal, such as increasing the rate of fault detection. Increasing the rate of fault detection can provide earlier feedback to system developers, improving fault fixing activity and, ultimately, software delivery. Many existing test case prioritization techniques consider that tests can be run in any order. However, due to functional dependencies that may exist between some test cases-that is, one test case must be executed before another-this is often not the case. In this paper, we present a family of test case prioritization techniques that use the dependency information from a test suite to prioritize that test suite. The nature of the techniques preserves the dependencies in the test ordering. The hypothesis of this work is that dependencies between tests are representative of interactions in the system under test, and executing complex interactions earlier is likely to increase the fault detection rate, compared to arbitrary test orderings. Empirical evaluations on six systems built toward industry use demonstrate that these techniques increase the rate of fault detection compared to the rates achieved by the untreated order, random orders, and test suites ordered using existing ""coarse-grainedƒ? techniques based on function coverage.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2012.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189361,Software engineering;testing and debugging;test execution,Fault detection;Digital signal processing;Schedules;Software;Complexity theory;Testing;Weight measurement,program debugging;program testing;software fault tolerance;software process improvement,dependency structures;functional test suite prioritization;test case execution ordering;fault detection rate;system developers;fault fixing improvement;software delivery;functional dependencies;test case prioritization technique;dependency information;system under test;software debugging,,15,31,,,,,,IEEE,IEEE Journals & Magazines
A code coverage-based test suite reduction and prioritization framework,S. U. R. Khan; S. P. Lee; R. M. Parizi; M. Elahi,"Faculty of Computer Science and IT, University of Malaya, Kuala Lumpur, Malaysia; Faculty of Computer Science and IT, University of Malaya, Kuala Lumpur, Malaysia; School of Computing and IT, Taylor's University, Selangor, Malaysia; Dept. of Computer Science, COMSATS Institute of IT, Islamabad, Pakistan",2014 4th World Congress on Information and Communication Technologies (WICT 2014),,2014,,,229,234,"Software testing is extensively used to ensure the development of a quality software system. The test suite size tends to increase by including new test cases due to software evolution. Consequently, the entire test suite cannot be executed considering budget and time limitations. Researchers have examined test suite reduction and prioritization techniques to address the test suite size problem. However, combination of these techniques can be useful for various regression testing situations. In this paper, we present a new code coverage-based test suite reduction and prioritization framework called TestOptimizer. The framework performs a suitable combination of TestFilter and St-Total techniques to determine optimal test cases, keeping in view of time restrictions. The performance of the proposed framework has been assessed using a case study. Results show that TestOptimizer can be beneficial to solve the test suite size problem within time constraints and has a profound impact on the required cost and effort of regression testing.",,978-1-4799-8115-1978-1-4799-8114,10.1109/WICT.2014.7076910,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076910,regression testing;framework;test suite reduction;test suite prioritization,Optimization;Testing;History;Software systems;Time factors;Computer science,program testing;regression analysis;software quality,code coverage-based test suite reduction;code coverage-based test suite prioritization;software testing;quality software system development;software evolution;regression testing situation;TestOptimizer;TestFilter;St-Total techniques;optimal test case,,,15,,,,,,IEEE,IEEE Conferences
A Comparison of Test Case Prioritization Criteria for Software Product Lines,A. B. S?­nchez; S. Segura; A. Ruiz-Cort??s,NA; NA; NA,"2014 IEEE Seventh International Conference on Software Testing, Verification and Validation",,2014,,,41,50,"Software Product Line (SPL) testing is challenging due to the potentially huge number of derivable products. To alleviate this problem, numerous contributions have been proposed to reduce the number of products to be tested while still having a good coverage. However, not much attention has been paid to the order in which the products are tested. Test case prioritization techniques reorder test cases to meet a certain performance goal. For instance, testers may wish to order their test cases in order to detect faults as soon as possible, which would translate in faster feedback and earlier fault correction. In this paper, we explore the applicability of test case prioritization techniques to SPL testing. We propose five different prioritization criteria based on common metrics of feature models and we compare their effectiveness in increasing the rate of early fault detection, i.e. a measure of how quickly faults are detected. The results show that different orderings of the same SPL suite may lead to significant differences in the rate of early fault detection. They also show that our approach may contribute to accelerate the detection of faults of SPL test suites based on combinatorial testing.",2159-4848,978-1-4799-2255,10.1109/ICST.2014.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823864,Software product lines;feature models;automated analysis;test case prioritization,Testing;Fault detection;Complexity theory;Security;Measurement;Analytical models;Feature extraction,fault diagnosis;program testing,test case prioritization criteria comparison;software product line testing;SPL testing;test case prioritization techniques;combinatorial testing;fault detection;SPL test suites,,13,31,,,,,,IEEE,IEEE Conferences
A distributed algorithm for virtual traffic lights with IEEE 802.11p,A. Bazzi; A. Zanella; B. M. Masini; G. Pasolini,"CNR-IEIIT, Bologna, Italy; CNR-IEIIT, Bologna, Italy; CNR-IEIIT, Bologna, Italy; DEI, University of Bologna, Italy",2014 European Conference on Networks and Communications (EuCNC),,2014,,,1,5,"A virtual traffic light (VTL) is a mechanism that allows vehicles to autonomously solve priorities at road junctions in the absence of fixed infrastructures (i.e., conventional traffic lights). To develop an effective VTL system, communication between vehicles is a crucial factor and can be handled either using cellular infrastructure or adopting a vehicle-to-vehicle (V2V) communication paradigm. In this paper we present the design, the implementation, and the field trial of a VTL which exploits V2V communications based on IEEE 802.11p. Specif-ically, we propose a decentralized algorithm, that adopts both broadcast signaling and unicast messages to assign priorities to the vehicles approaching intersections, thus preventing accidents and reducing traffic congestions. The algorithm has been tested both in a controlled laboratory environment and in a field trial with equipped vehicles.",,978-1-4799-5280,10.1109/EuCNC.2014.6882621,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6882621,,Junctions;Vehicles;Roads;Nickel;Unicast;Databases;Software algorithms,cellular radio;road accidents;road safety;road traffic;wireless LAN,distributed algorithm;virtual traffic lights with;VTL system;IEEE 802.11p;road junctions;cellular infrastructure;vehicle-to-vehicle communication paradigm;V2V communication paradigm;broadcast signaling;unicast messages;accident prevention;traffic congestion reduction;road safety,,4,9,,,,,,IEEE,IEEE Conferences
A hierarchical test case prioritization technique for object oriented software,Vedpal; N. Chauhan; H. Kumar,"Dept. of Comp. Engg., YMCAUST, Faridabad India; Dept. of Comp. Engg., YMCAUST, Faridabad, India; Dept. of Comp. Engg., YMCAUST, Faridabad, India",2014 International Conference on Contemporary Computing and Informatics (IC3I),,2014,,,249,254,"Software reuse is the use of existing artifacts to create new software. Inheritance is the foremost technique of reuse. But the inherent complexity due to inheritance hierarchy found in object - oriented paradigm also affect testing. Every time any change occurs in the software, new test cases are added in addition to the existing test suite. So there is need to conduct effective regression testing having less number of test cases to reduce cost and time. In this paper a hierarchical test case prioritization technique is proposed wherein various factors have been considered that affect error propagation in the inheritance. In this paper prioritization of test cases take place at two levels. In the first level the classes are prioritized and in the second level the test cases of prioritized classes are ordered. To show the effectiveness of proposed technique it was applied and analyze on a C++ program.",,978-1-4799-6629,10.1109/IC3I.2014.7019794,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019794,object oriented testing;test case prioritization;regression testing,Testing;Software;Fault detection;Measurement;Unified modeling language;Informatics;Object oriented modeling,C++ language;object-oriented methods;program diagnostics;program testing;regression analysis;software reusability,hierarchical test case prioritization technique;object oriented software;software reuse;regression testing;C++ program,,,11,,,,,,IEEE,IEEE Conferences
A Method to Test the Information Quality of Technical Documentation on Websites,O. Shpak; W. L??we; A. Wingkvist; M. Ericsson,NA; NA; NA; NA,2014 14th International Conference on Quality Software,,2014,,,296,304,"In software engineering, testing is one of the corner-stones of quality assurance. The idea of software testing can be applied to information quality as well. Technical documentation has a set of intended uses that correspond to use cases in a software system. Documentation is, in many cases, presented via software systems, e.g., web servers and browsers, and contains software, e.g., Javascript for user interaction, animation, and customization, etc. This makes it difficult to find a clear-cut distinction between a software system and technical documentation. However, we can assume that each use case of a technical documentation involves retrieval of some sort of information that helps a user answer a specific questions. To assess information testing as a method, we implemented QAnalytics, a tool to assess the information quality of documentation that is provided by a website. The tool is web-based and allows test managers and site owners to define test cases and success criteria, disseminate the test cases to testers, and to analyze the test results. This way, information testing is easily manageable even for non-technical stakeholders. We applied our testing method and tool in a case study. According to common perception, the website of Linnaeus University needs to be re-engineered. Our method and tool helped the stakeholders identify what information is presented well and which parts of the website that need to change. The test results allowed the design and development effort to prioritize actual quality issues and potentially save time and resources.",1550-6002;2332-662X,978-1-4799-7198-5978-1-4799-7197,10.1109/QSIC.2014.48,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958417,information quality;web analytics;web testing,Documentation;Standards;Gold;Educational institutions;Software;Software testing,document handling;program testing;software quality;Web sites,information quality;technical documentation;Web sites;software engineering;software testing;quality assurance;information testing;Linnaeus University,,3,11,,,,,,IEEE,IEEE Conferences
A study of applying severity-weighted greedy algorithm to software test case prioritization during testing,Y. Hsu; K. Peng; C. Huang,"Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu, Taiwan",2014 IEEE International Conference on Industrial Engineering and Engineering Management,,2014,,,1086,1090,"Regression testing is a very useful technique for software testing. Traditionally, there are several techniques for test case prioritization; two of the most used techniques are Greedy and Additional Greedy Algorithm (GA and AGA). However, it can be found that they may not consider the severity while prioritizing test cases. In this paper, an Enhanced Additional Greedy Algorithm (EAGA) is proposed for test case prioritization. Experiments with eight subject programs are performed to investigate the effects of different techniques under different criteria and fault severity. Experimental results show that proposed EAGA perform well than other techniques.",2157-3611;2157-362X,978-1-4799-6410,10.1109/IEEM.2014.7058806,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058806,Test case prioritization;code coverage;search algorithm;APFD;APFDc,Greedy algorithms;Software testing;Fault detection;Software engineering;Software;Schedules,greedy algorithms;program testing,applying severity weighted greedy algorithm;software test case prioritization;software testing;additional greedy algorithm;AGA;enhanced additional greedy algorithm;EAGA,,,32,,,,,,IEEE,IEEE Conferences
An improved genetic approach for test path generation,Preeti; J. Chaudhary,"Computer Engineering, The Technological Institute of Textile &amp; Sciences, Bhiwani, India; Computer Engineering, The Technological Institute of Textile &amp; Sciences, Bhiwani, India",2014 International Conference on Advances in Engineering & Technology Research (ICAETR - 2014),,2014,,,1,5,"Quality of a software system depends on testing approaches adopted to analyze the software product. Testing process itself depends on two main vectors called test sequence generation and test data generation. Test sequence generation is about to identify the order in which the particular test cases will be executed and the test data defines the various checks performed on each test case. In this present work, a fuzzy improved genetic approach is suggested for test case generation. The sequence on these test cases is here dependent on module interaction analysis. Based on this analysis, the test case prioritization will be defined. Once the test cases will be prioritized, the next work is to apply fuzzy improved genetic approach for test path generation. The work is analyzed under different prioritization vectors. Analysis of work is defined in terms of test cost estimation under different prioritization scenarios.",2347-9337,978-1-4799-6393-5978-1-4799-6392,10.1109/ICAETR.2014.7012823,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012823,Genetic Based;Test Data;Test Sequence;Prioritization;Module Integration,Testing;Reliability;Estimation;Algorithm design and analysis,fuzzy set theory;genetic algorithms;program testing;software quality,test path generation;fuzzy improved genetic approach;test case generation;software testing;software quality;module interaction analysis;test case prioritization;prioritization vectors;test cost estimation,,,14,,,,,,IEEE,IEEE Conferences
Applying Parameter Value Weighting to a Practical Application,S. Fujimoto; H. Kojima; H. Nakagawa; T. Tsuchiya,NA; NA; NA; NA,2014 IEEE International Symposium on Software Reliability Engineering Workshops,,2014,,,130,131,"This paper reports a case study where pair-wise testing was applied to a real-world program. In particular we focus on weighting, an added feature which allows the tester to prioritize particular parameter values. In our previous work we proposed a weighting method that can reflect given weights in the resulting test suite more directly than can existing methods. To asses the effects of weighting in a practical testing process, we compare the number of execution times of the program's methods among three pair-wise test suites, including the test suite generated by our weighting method and those generated by an existing test case generation tool with and without the weighting option. The results show that the effects of weighting were most clearly observed when our weighting method was used.",,978-1-4799-7377,10.1109/ISSREW.2014.63,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983821,pair-wise testing;weighting;combinatorial interaction testing,Testing;Local area networks;High definition video;Browsers;Random access memory;Greedy algorithms;Weight measurement,program testing,parameter value weighting;pair-wise testing;program testing;practical testing process;program method execution time,,,3,,,,,,IEEE,IEEE Conferences
Athena: A Visual Tool to Support the Development of Computational Intelligence Systems,P. Oliveira; M. Souza; R. Braga; R. Britto; R. L. Rab?¦lo; P. S. Neto,NA; NA; NA; NA; NA; NA,2014 IEEE 26th International Conference on Tools with Artificial Intelligence,,2014,,,950,959,"Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.",1082-3409;2375-0197,978-1-4799-6572,10.1109/ICTAI.2014.144,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984580,Computational Intelligence;Artificial Intelligence;Visual Programming;Tool;Service,Visualization;Productivity;Remuneration;Computational modeling;Algorithm design and analysis;Computational intelligence;Resource management,artificial intelligence;software product lines;visual programming,Athena;Computational Intelligence Systems;software product line;test case selection;CI techniques;trivial activity;drag-and-drop approach;CI-as-a-Service;CIaaS;visual programming;prioritization;visual tool,,1,30,,,,,,IEEE,IEEE Conferences
Automated Prioritization of Metrics-Based Design Flaws in UML Class Diagrams,M. R. V. Chaudron; B. Katumba; X. Ran,NA; NA; NA,2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications,,2014,,,369,376,"The importance of software architecture in software development prolongs throughout the entire software life cycle. This is because quality of the architectural design defines the structural aspects of the system that are difficult to change, and hence will affect most of the subsequent development and maintenance activities. This paper considers software design flaws (related to the system structure) and not flaws identified at run time (by testing). These design flaws are akin to what is described in the literature as anti-patterns, bad smells or rotting design. Recently, two tools that have been developed for quality assurance of software designs represented in the UML notation: SDMetrics and Metric View. However these tools are not considered practical because they report many design flaws which are not considered by developers (false positives). This paper explores an approach that tries to identify which design flaws should be considered important and which are not. To this end, we propose an approach for automated prioritization of software design flaws (BX approach), to facilitate developers to focus on important design flaws more effectively. We designed and implemented a tool (PoSDef) that implements this approach. The BX approach and the PoSDef tool have been validated using two open source projects and one large industrial system. Our validation consists of comparing our approach and tool with the existing design flaw tools. The evaluation has shown that the proposed approach could facilitate developers to identify and prioritize important design flaws effectively.",1089-6503;2376-9505,978-1-4799-5795,10.1109/SEAA.2014.82,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928837,Model-based Development;UML;Class Diagrams;Software Quality Assurance;Metrics;Software Design Flaws;Software Quality,Measurement;Unified modeling language;Software design;Motion pictures;Fault diagnosis;Educational institutions,program verification;software architecture;software quality;Unified Modeling Language,automated prioritization;metric-based design flaws;UML class diagrams;software architecture;software development prolongs;software life cycle;quality assurance;SDMetrics;metricview;BX approach;PoSDef;software validation,,,23,,,,,,IEEE,IEEE Conferences
Bench level automotive EMC validation test laboratory challenges and preferences,L. Banasky,"Electromagnetic Compatibility, Ford Motor Company, Dearborn, MI, USA",2014 IEEE International Symposium on Electromagnetic Compatibility (EMC),,2014,,,307,315,"Automotive original equipment manufacturers (OEMs) and suppliers of electrical and electronic subsystems are required to perform bench level electrical and electromagnetic compatibility (EMC) validation testing. This is an important process that requires a significant investment in time and money. In an effort to improve the efficiency of testing, a survey was developed to gain an understanding of the challenges faced by test laboratories and also their preferences. This paper summarizes the results of the survey. Given the amount of time and money spent annually for the type of testing considered, the results suggest that pursuing improvements will result in a long term savings for the original equipment manufacturers (OEMs), suppliers, and labs involved. In order to increase test efficiency, the OEMs, suppliers, and laboratories will need to work together to better prepare test plans and test setups. It is suggested that the results of this survey are used to prioritize the improvement activities.",2158-1118;2158-110X,978-1-4799-5545-9978-1-4799-5544-2978-1-4799-5543,10.1109/ISEMC.2014.6898989,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898989,Test;Laboratories;Bench Level;Subsystem;Automotive;Electromagnetic Compatibility (EMC);Validation,Laboratories;Electromagnetic compatibility;Time-frequency analysis;Software;Standards;Automotive engineering,automotive electronics;electromagnetic compatibility,bench level automotive EMC validation test;automotive original equipment manufacturers;OEM;electronic subsystems;electrical subsystems;electromagnetic compatibility;testing;original equipment manufacturers;test plans;test setups,,,11,,,,,,IEEE,IEEE Conferences
Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines,C. Henard; M. Papadakis; G. Perrouin; J. Klein; P. Heymans; Y. Le Traon,"Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg",IEEE Transactions on Software Engineering,,2014,40,7,650,670,"Large Software Product Lines (SPLs) are common in industry, thus introducing the need of practical solutions to test them. To this end, t-wise can help to drastically reduce the number of product configurations to test. Current t-wise approaches for SPLs are restricted to small values of t. In addition, these techniques fail at providing means to finely control the configuration process. In view of this, means for automatically generating and prioritizing product configurations for large SPLs are required. This paper proposes (a) a search-based approach capable of generating product configurations for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of product configurations. Both these techniques employ a similarity heuristic. The ability of the proposed techniques is assessed in an empirical study through a comparison with state of the art tools. The comparison focuses on both the product configuration generation and the prioritization aspects. The results demonstrate that existing t-wise tools and prioritization techniques fail to handle large SPLs. On the contrary, the proposed techniques are both effective and scalable. Additionally, the experiments show that the similarity heuristic can be used as a viable alternative to t-wise.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2014.2327020,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823132,Software product lines;testing;T-wise Interactions;search-based approaches;prioritization;similarity,Testing;Frequency modulation;Context;Scalability;Software;Linux;Arrays,combinatorial mathematics;program testing;software product lines,combinatorial explosion;test configurations;software product lines;SPL;product configurations;configuration process;search based approach;similarity heuristic;product configuration generation,,54,64,,,,,,IEEE,IEEE Journals & Magazines
Competence transfer through enterprise mobile application development,S. VojvodiŽ?; M. ZoviŽ?; V. Re?iŽ?; H. MaraŽiŽ?; M. Kusek,"Ericsson Nikola Tesla, Zagreb, Croatia; Ericsson Nikola Tesla, Zagreb, Croatia; Ericsson Nikola Tesla, Zagreb, Croatia; University of Zagreb / Faculty of Electrical Engineering and Computing, Zagreb, Croatia; University of Zagreb / Faculty of Electrical Engineering and Computing, Zagreb, Croatia","2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",,2014,,,448,452,"Large world corporations need corresponding information technology (IT) support as well as constant improvement of software tools, which should enable further business development and more efficient work to operational organizations. The main interest of IT support organizations is currently more and more connected with mobile IT equipment of the employees. Specific business mobile applications improve the efficiency and result in new operating possibilities. The cooperation between Ericsson Nikola Tesla (ENT) and Faculty of Electrical Engineering and Computing (FER) of the Zagreb University, has enabled a fast development of competencies in ENT, necessary for mobile application development and quality realization of innovative solution for platforms iOS and Android. In this cooperation at the project realization the methodology of application development was defined, the corresponding competencies were developed, the system architecture was designed together with the communication of mobile application and back-end IT systems. In the project the iterative development approach, tools for software code versioning and project control have been used thus enabling continuous insight in project progressing. At any moment it was possible to determine priorities of functional development and solution elements, as well as to realize the necessary additional transfer of knowledge. The project has resulted in an enterprise mobile application for iOS and Android platforms, which had been implemented and tested in several countries. The complete solution enables data management by means of the portal, thus decreasing the frequent changes of users' mobile application and significantly accelerating the process of new functionalities introduction.",,978-953-233-077-9978-953-233-081,10.1109/MIPRO.2014.6859609,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859609,,Smart phones;Mobile communication;Servers;Knowledge transfer;Organizations;Computer crashes,Android (operating system);business data processing;iOS (operating system);mobile computing;organisational aspects;software engineering,competence transfer;enterprise mobile application development;information technology support;business development;operational organizations;IT support organizations;mobile IT equipment;business mobile applications;Ericsson Nikola Tesla;ENT;faculty of electrical engineering and computing;FER;Zagreb university;iOS;Android;back-end IT systems;iterative development approach;software code versioning;project control;project progressing;functional development,,2,22,,,,,,IEEE,IEEE Conferences
Development and design of a platform for arbitration and sharing control applications,J. P??rez; D. Gonz?­lez; F. Nashashibi; G. Dunand; F. Tango; N. Pallaro; A. Rolfsmeier,"INRIA, France; INRIA, France; INRIA, France; Intempora, France; Centro Ricerche FIAT, Italy; Centro Ricerche FIAT, Italy; dSpace, Germany","2014 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS XIV)",,2014,,,322,328,"In this paper, a description of the ADAS development platform in DESERVE project framework is presented. This work is framed within the Sub Project 2 (Development platform) of DESERVE project, and it is divided in 6 different and complementary lines of work. Most of the functions described in the tools and development systems, perception layer and the platform system architecture show the modularity and scalability of our proposal. Moreover, based on vehicle modelling, driver behaviour and intention, a first approach for arbitration and control strategies, which can anticipate the priorities on the control in emergency situations, is described. Furthermore, some simulations will allow the virtual testing for the future implementation in demonstrators. The presented work is the core of DESERVE project, and it is developed in parallel with Driver behaviour and HMI activities (SP3). This work presents some of the achievements in SP2, mainly the application platform integration in one of the demonstrators, along with the arbitration and sharing control, based on intelligent techniques (Fuzzy logic). Simulation shows the feasibility of proposal. This approach will be tested, integrated and validated in a real vehicle in the next stages of the project.",,978-1-4799-3770,10.1109/SAMOS.2014.6893228,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6893228,ADAS;Embedded Systems;Arbitration;Control;Virtual testing;modelling,Vehicles;Computer architecture;Computational modeling;Field programmable gate arrays;Hardware;Software;Fuzzy logic,driver information systems;fuzzy logic;road traffic control,sharing control application;ADAS development platform;DESERVE project framework;perception layer;platform system architecture;vehicle modelling;driver behaviour;emergency situation;virtual testing;HMI activities;intelligent technique;fuzzy logic;advanced driver assistance systems,,2,12,,,,,,IEEE,IEEE Conferences
Development test case prioritization technique in regression testing based on hybrid criteria,M. R. Nejad Dobuneh; D. N. A. Jawawi; M. Ghazali; M. V. Malakooti,"Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia (UTM), Skudai 81310, Johor, Malaysia; Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia (UTM), Skudai 81310, Johor, Malaysia; Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia (UTM), Skudai 81310, Johor, Malaysia; Department of Computer Engineering, Islamic Azad, University UAE Branch, UAE",2014 8th. Malaysian Software Engineering Conference (MySEC),,2014,,,301,305,"Test case prioritization techniques improve the performance of regression testing, and arrange test cases in order to obtain maximum available fault that is going to be detected in a shorter time. In this research the priority is given to test cases that are performed based on multiple criteria and hybrid criteria to enhance the effectiveness of time and cost for proposed technique. This paper shows that our prioritization technique is appropriate for regression testing environment and show that our prioritization approach frequently produces a higher average percentage of fault detection rate value, for web application. The experiments also reveal fundamental tradeoffs in the performance of time aware prioritization. In this technique some fault will be seeded in subject application, then applying the prioritization criteria on test cases to obtain the effective time of average percentage fault detection rate.",,978-1-4799-5439,10.1109/MySec.2014.6986033,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986033,web application;regression testing;prioritization criteria,Fault detection;Software;Software engineering;Schedules;Organizations;Software testing,Internet;program testing;regression analysis;software fault tolerance,development test case prioritization technique;hybrid criteria;regression testing performance;test case arrangement;maximum available fault;regression testing environment;Web application;time aware prioritization;prioritization criteria;fault detection,,1,11,,,,,,IEEE,IEEE Conferences
DITEC (DoD-Centric and Independent Technology Evaluation Capability): A Process for Testing Security,J. Romero-Mariona,NA,"2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops",,2014,,,24,25,"Information Assurance (IA) is one of the Department of Defense's (DoD) top priorities today. IA technologies are constantly evolving to protect critical information from the growing number of cyber threats. Furthermore, DoD spends millions of dollars each year procuring, maintaining, and discontinuing various IA and Cyber technologies. Today, there is no process and/or standardized method for making informed decisions about which IA technologies are better/best. Due to this, efforts for selecting technologies go through very disparate evaluations that are often times non-repeatable and very subjective. DITEC (DoD-centric and Independent Technology Evaluation Capability) is a new capability that streamlines IA technology evaluation. DITEC defines a Process for evaluating whether or not a product meets DoD needs, Security Metrics for measuring how well needs are met, and a Framework for comparing various products that address the same IA technology area. DITEC seeks to reduce the time and cost of creating a test plan and expedite the test and evaluation effort for considering new IA technologies, consequently streamlining the deployment of IA products across DoD and increasing the potential to meet its needs.",,978-1-4799-5790,10.1109/ICSTW.2014.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825634,Security;Information Assurance;Security Metrics;Evaluation;Decision-making Support,Measurement;US Department of Defense;Computer security;Conferences;Usability,data protection;decision making;military computing;security of data,DITEC;DoD-centric and independent technology evaluation capability;security testing process;information assurance;Department of Defense;IA technologies;critical information protection;cyber threats;cyber technologies;IA technologies;informed decision making;security metrics,,2,3,,,,,,IEEE,IEEE Conferences
Dynamic test case prioritization based on multi-objective,X. Wang; H. Zeng,"School of Computer Engineering and Science, Shanghai University Shanghai 200444, China; School of Computer Engineering and Science, Shanghai University Shanghai 200444, China","15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",,2014,,,1,6,"Test case prioritization technology is to sort the test cases before the software testing designed to improve test efficiency. This paper presents a dynamic test case prioritization technique based on multi-objective. It integrates several traditional single-objective technologies so that makes it more flexible. This technology, from five dimensions, calculates prioritization values of test cases separately. Then a weighted sum is made to the values and it sorts the test cases according to the values. The results return to the storage in order to dynamically adjust the sort of test cases. This technology not only meets the high demands of regression testing, but also ensures the high efficiency of the test results.",,978-1-4799-5604,10.1109/SNPD.2014.6888744,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888744,Test case prioritization;Multi-objective;Dynamic;Regression testing,History;Testing;Software;Fault detection;Databases;Measurement;Probability,program testing;regression analysis,dynamic test case prioritization;multiobjective;test case prioritization technology;software testing;test efficiency;single-objective technologies;weighted sum;regression testing,,1,15,,,,,,IEEE,IEEE Conferences
Effective Regression Testing Using Requirements and Risks,C. Hettiarachchi; H. Do; B. Choi,NA; NA; NA,2014 Eighth International Conference on Software Security and Reliability (SERE),,2014,,,157,166,"The use of system requirements and their risks enables software testers to identify more important test cases that can reveal faults associated with risky components. Having identified those test cases, software testers can manage the testing schedule more effectively by running such test cases earlier so that they can fix faults sooner. Some work in this area has been done, but the previous approaches and studies have some limitations, such as an improper use of requirements risks in prioritization and an inadequate evaluation method. To address the limitations, we implemented a new requirements risk-based prioritization technique and evaluated it considering whether the proposed approach can detect faults earlier overall. It can also detect faults associated with risky components earlier. Our results indicate that the proposed approach is effective for detecting faults early and even better for finding faults associated with risky components of the system earlier than the existing techniques.",,978-1-4799-4296,10.1109/SERE.2014.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6895426,regression testing;requirements risks-based testing;test case prioritization;empirical study,Software;Testing;Mathematical model;Complexity theory;Equations;Security;Measurement,program testing;regression analysis;risk management;scheduling,regression testing;system requirements;testing schedule management;requirements risk-based prioritization technique;fault detection;risky components,,4,23,,,,,,IEEE,IEEE Conferences
Enabling Prioritized Cloud I/O Service in Hadoop Distributed File System,T. Yeh; Y. Sun,NA; NA,"2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)",,2014,,,256,259,"Cloud computing has become more and more popular nowadays. Both governments and enterprises provide service through the construction of public and private clouds accordingly. Among the platforms used in cloud computing, Hadoop is considered one of the most practical and stable systems. Nevertheless, as with other regular software, Hadoop still needs to rely on the underlying operating system to communicate with hardware to function appropriately. For modern computer systems, CPUs excessively outrun hard drives (hard disks). The computer hard disk has become a major bottleneck to the overall system performance. Consequently, computer programs can execute faster if their corresponding I/O operation can be completed sooner. This is important in particular when we want to expedite the execution of urgent programs in a busy system. Unfortunately, under the current Hadoop environment, users cannot prioritize operation of disk and memory for programs which they would like them to run faster. With the help of prioritized I/O service we developed earlier, we proposed and implemented a Hadoop environment with the ability of providing prioritized I/O service. Our Hadoop environment could accelerate the execution of programs with high priority assigned by users. We evaluated our design by executing prioritized programs in environments with different busy levels. Experimental results show that programs can improve their performance by up to 33.79% if executed with high priority.",,978-1-4799-6123-8978-1-4799-6122,10.1109/HPCC.2014.45,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056748,cloud computing;Hadoop;HDFS,Cloud computing;Computers;Writing;Operating systems;Benchmark testing;Conferences,cloud computing;data handling;distributed databases;network operating systems;parallel processing,Hadoop distributed file system;cloud computing;CPU;computer hard disk;operating system,,,17,,,,,,IEEE,IEEE Conferences
Exploring Model-Based Repositories for a Broad Range of Industrial Applications and Challenges,T. Yue; S. Ali; A. Descour; Q. Gan; M. Liaaen; G. M. Merkesvik; B. K. Nielsen; J. Nyg?rd; B. O. Olafsen; A. L. Waal,NA; NA; NA; NA; NA; NA; NA; NA; NA; NA,2014 14th International Conference on Quality Software,,2014,,,37,46,"Nowadays, systems are becoming increasingly complex and large and the process of developing such large-scale systems is becoming complicated with high cost and enormous effort required. Such a complicated process has a prominent challenge to ensure the quality of delivered artifacts. Therefore there is clearly a need to facilitate reuse of developed artifacts (e.g., requirements, architecture, tests) and enable automated analyses such as risk analyses, prioritizing test cases, change impact analysis, with the objective to reduce cost, effort and improve quality. Model-based engineering provides a promising mechanism to facilitate reuse and enable automation. The key idea is to use models as the backbone of structuring repositories that contain reusable artifacts (e.g., test cases, requirements). Such a backbone model is subse-quently used to enable various types of automation such as model-based testing and automated rule verification. In this paper, we report 12 industrial projects from five different industry domains that all require the construction of model-based repositories to enable various types of automation. We believe using models as the backbone to structure repositories for the purpose of enabling different types of automation in different contexts is a new and non-conventional model-based development research approach. This exploratory paper will serve the basis for future research to derive a generic model-based repository.",1550-6002;2332-662X,978-1-4799-7198-5978-1-4799-7197,10.1109/QSIC.2014.18,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958385,Model-based Repository;Model-based Repository Engineering;Backbone Model;Automation;Analysis;Optimization;Generation,Unified modeling language;Training;Automation;Analytical models;Cancer;Testing;Optimization,formal verification;program testing;software quality,model-based repository;large-scale system;risk analysis;model-based engineering;model-based testing;automated rule verification,,1,21,,,,,,IEEE,IEEE Conferences
Fair and delay adaptive scheduler for UC and NGN networks,A. M. Elnaka; Q. H. Mahmoud; Xining Li,"School of Computer Science, University of Guelph, ON, Canada N1G 2W1; Dept. of Electrical, Computer and Software Engineering, University of Ontario Institute of Technology, Oshawa, Canada L1H 7K4; School of Computer Science, University of Guelph, ON, Canada N1G 2W1",2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE),,2014,,,1,6,"Fair bandwidth allocation while conforming to stringent end-to-end delay constraints is a major requirement for the successful delivery of next generation QoS demanding traffic. Research has been carried out in the area of processor and scheduler sharing for decades to try to achieve the QoS requirements of network traffic. Fairness and traffic prioritization are two main objectives that many schedulers were originally designed to meet. Another important issue is how schedulers treat the delay sensitive traffic. Although the combination of fairness and prioritization is implemented in several schedulers but, to our knowledge, incorporating adaptive traffic delay treatment in fair and prioritized schedulers has not yet been successfully implemented. In this paper, we introduce a new scheduler that balances between the fairness of bandwidth allocation between flows while implementing prioritization and minimizes the number of end-to-end delay bound breaches. The scheduler combines the virtual clock concept used in well-known fair schedulers together with schedulability testing and evaluation implemented in delay sensitive schedulers. The scheduler is designed to achieve the fairness of bandwidth allocation, such as in fair schedulers, while minimizing the number of possible violation of end-to-end QoS delay of individual flows' packets.",0840-7789,978-1-4799-3101-9978-1-4799-3099,10.1109/CCECE.2014.6900970,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900970,UC;NGN;EDF;WFQ;QoS,Delays;Bandwidth;Quality of service;Next generation networking;Scheduling;Schedules;Scheduling algorithms,adaptive scheduling;bandwidth allocation;delays;next generation networks;quality of service;telecommunication traffic,fair adaptive scheduler;delay adaptive scheduler;UC Network;NGN Network;fair bandwidth allocation;end-to-end delay constraint;next generation QoS demanding traffic prioritization;delay sensitive traffic;virtual clock concept;unified communication network;next generation network;delay sensitive scheduler,,2,11,,,,,,IEEE,IEEE Conferences
Genetic algorithm secure procedures algorithm to manage data integrity of test case prioritization methodology,S. Mahajan; S. D. Joshi; V. Khanaa,"Dept. of Computer Engineering, Bharath University, Chennai, India; Dept. of Computer Engineering, Bharati Vidyapeeth, College of Engineering, Pune, India; Dept. of Computer Engineering, Bharath University, Chennai, India",2014 IEEE Global Conference on Wireless Computing & Networking (GCWCN),,2014,,,208,212,"The focus of present research paper is to manage data integrity and trustworthiness of issues involved in software testing phase of software development life cycle. Many times, it seems that, data integration left behind the software testing phase and it is directly focused at software deployment or delivery time. To avoid issues due to lack of data integration, we developed algorithm which can track integration of test case prioritization along with trustworthiness of test case execution. This paper demonstrates how software testing can be efficient with management of data integrity factor to avoid major security issues. One of the main advantages of our approach is that domain specific semantics can be integrated with the data quality test cases prioritization, thus being able to discover test feed data quality problems beyond conventional quality measures.",,978-1-4799-6298-3978-1-4799-6297,10.1109/GCWCN.2014.7030880,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030880,secure test case;test case prioritization;test case integration issues,Genetic algorithms;Algorithm design and analysis;Software testing;Security;Indexes;Conferences,data integration;data integrity;genetic algorithms;program testing;trusted computing,genetic algorithm secure procedures algorithm;data integrity management;data trustworthiness;software testing phase;software development life cycle;data integration;test case prioritization integration;test case execution trustworthiness,,,16,,,,,,IEEE,IEEE Conferences
How Does the UML Testing Profile Support Risk-Based Testing,S. Ali; T. Yue; A. Hoffmann; M. Wendland; A. Bagnato; E. Brosse; M. Schacher; Z. R. Dai,NA; NA; NA; NA; NA; NA; NA; NA,2014 IEEE International Symposium on Software Reliability Engineering Workshops,,2014,,,311,316,"The increasing complexity of software-intensive systems raises a lot of challenges demanding new techniques for ensuring their overall quality. The risk of not meeting the expected level of quality has negative impact on business, customers, environment and people, especially in the context of safety/security-critical systems. The importance of risk assessment, analysis and management has been well understood both in the literature and practice, which has led to the definition of a number of well-known standards. In the recent years, Risk-Based Testing (RBT) is gaining more attention, especially focusing on test prioritization and selection based on risks. On the other hand, model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software-intensive systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in literature and practice in the last decade. In this paper, we study the feasibility of combining RBT with MBT by using the upcoming version of UML Testing Profile (UTP 2) as the mechanism. We present potential traceability between RBT and UTP 2 concepts.",,978-1-4799-7377,10.1109/ISSREW.2014.13,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983859,Risk-based Testing;Model-based Testing;UML Testing Profile;Risk Assessment,Unified modeling language;Testing;Risk analysis;Security;Dynamic scheduling;Standards;Analytical models,program testing;risk analysis;Unified Modeling Language,UML testing profile;risk-based testing;model-based testing;software-intensive systems;test prioritization;test selection;MBT technique;UTP 2;RBT technique,,2,51,,,,,,IEEE,IEEE Conferences
Identifying usability risk: A survey study,J. T. Sambantha Moorthy; S. bin Ibrahim; M. N. Mahrin,"Advanced Informatics School, Universiti Teknologi of Malaysia (UTM), Kuala Lumpur, Malaysia; Advanced Informatics School, Universiti Teknologi of Malaysia (UTM), Kuala Lumpur, Malaysia; Advanced Informatics School, Universiti Teknologi of Malaysia (UTM), Kuala Lumpur, Malaysia",2014 8th. Malaysian Software Engineering Conference (MySEC),,2014,,,148,153,"As defined in various quality models, usability is recognized as an important attribute of software quality. Failing to address usability requirements in a software product could lead to poor quality and high usability problems in software product. Research is still in progress to introduce the best methods for reducing usability problems and increase the rate of successful usable software products. Studies have shown that problems in software products can also be controlled using Software Risk Management methods, even though these problems cannot be eliminated totally. Using Software Risk Management, problems in software products are dealt before it occurs. This paper presents usability problems as a risk factor and by managing usability risk at earlier phases of the development process, successful and high usability software products can be developed. Unfortunately, currently there is little effort in identifying, analyzing and prioritizing potential usability risks at earlier phases of the development process. This paper focuses on usability risk identification as it is the first stage in usability risk management. This paper presents the results of an industry survey based on the opinion of Malaysian Public Sector involving sample size of 330 software developers and software projects managers regarding potential usability risk that could occur during Software Development Life Cycle (SDLC). Our finding has identified 42 potential usability risks, defined as a list for further risk analysis in future.",,978-1-4799-5439,10.1109/MySec.2014.6986005,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986005,usability;usability risk;risk management;risk identification,Usability;Risk management;Safety;Testing;Documentation;Maintenance engineering,risk management;software quality;software reusability,software product usability risk identification;software quality;software usability requirements;software risk management methods;software development life cycle;SDLC,,,31,,,,,,IEEE,IEEE Conferences
Implementing test case selection and reduction techniques using meta-heuristics,R. Nagar; A. Kumar; S. Kumar; A. S. Baghel,"School of Information &amp; Communication Technology, Gautam Buddha University, Greater Noida, India; Pitney Bowes Software, Noida, India; School of Information &amp; Communication Technology, Gautam Buddha University, Greater Noida, India; School of Information &amp; Communication Technology, Gautam Buddha University, Greater Noida, India",2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence),,2014,,,837,842,"Regression Testing is an inevitable and very costly maintenance activity that is implemented to make sure the validity of modified software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization to select and prioritize a minimum set of test cases, fulfilling some chosen criteria, that is, covering all possible faults in minimum time and other. In this paper a test case reduction hybrid Particle Swarm Optimization (PSO) algorithm has been proposed. This PSO algorithm uses GA mutation operator while processing. PSO is a swarm intelligence algorithm based on particles behavior. GA is an evolutionary algorithm (EA). The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.",,978-1-4799-4236-7978-1-4799-4237,10.1109/CONFLUENCE.2014.6949377,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949377,Particle Swarm Optimization;Genetic Algorithm;Regression Test Selection;Test Case Prioritization,Testing;Genetic algorithms;Software algorithms;Software;Particle swarm optimization;Sociology;Statistics,data reduction;feature selection;genetic algorithms;particle swarm optimisation;program testing;regression analysis;software maintenance,test case selection;test case reduction;test case prioritization;metaheuristics;regression testing;software maintenance;particle swarm optimization;PSO algorithm;GA mutation operator;evolutionary algorithm;EA,,3,17,,,,,,IEEE,IEEE Conferences
Parallelized ACO algorithm for regression testing prioritization in hadoop framework,N. Elanthiraiyan; C. Arumugam,"Department of Computer Science and Engineering, SSN College of Engineering, Chennai, India; Department of Computer Science and Engineering, SSN College of Engineering, Chennai, India","2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies",,2014,,,1568,1571,"Regression testing is an important strategy in the software maintenance phase to produce a high quality software product. This testing ensures that the modified system code does not have an effect on the original software system. Initially, the test suite is generated for the existing software system. After the system undergoes changes the test suite contains both the original test cases and the modified test cases. Regression test prioritization method helps to separate the optimal test cases from the modified test suite. In the existing work, the multi-criteria optimization was applied for generating optimal regression test cases and it was carried out in a non-parallelized environment. The proposed solution is to extend the existing work by generating an optimized test suite using Ant Colony Optimization (ACO) technique on Hadoop Map reduce framework in a parallelized environment.",,978-1-4799-3914-5978-1-4799-3913,10.1109/ICACCCT.2014.7019371,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019371,Regression Testing;Hadoop Mapreduce;parallelization;Ant Colony Optimization;Prioritization,Software;Optimization;TV;Computer aided software engineering,optimisation;parallel processing;program testing;regression analysis;software maintenance;software quality,parallelized ACO algorithm;software maintenance phase;high quality software product;modified system code;test suite;regression test prioritization method;multicriteria optimization;optimal regression test cases;nonparallelized environment;optimized test suite;ant colony optimization technique;ACO;Hadoop Mapreduce framework;parallelized environment,,1,13,,,,,,IEEE,IEEE Conferences
"Planning of Prioritized Test Procedures in Large Integrated Systems: Best Strategy of Defect Discovery and Early Stop of Testing Session, The Selex-ES Experience",G. Ranieri,NA,2014 IEEE International Symposium on Software Reliability Engineering Workshops,,2014,,,112,113,"Large integrated systems verification can be made more efficient if driven by specific strategy, classification and prioritization of test procedures is the Selex ES way to speed up important defects discovery and cost of testing activities.",,978-1-4799-7377,10.1109/ISSREW.2014.106,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983814,testing strategy;priority;integrated;defect;detection,Testing;Software;Safety;Software reliability;Conferences;Planning;Companies,program testing;software fault tolerance,defect discovery strategy;testing session;Selex-ES experience;test procedure classification;test procedure prioritization,,,,,,,,,IEEE,IEEE Conferences
Practical Software Quality Prediction,E. Shihab,NA,2014 IEEE International Conference on Software Maintenance and Evolution,,2014,,,639,644,"Software systems continue to play an increasingly important role in our daily lives, making the quality of software systems an extremely important issue. Therefore, a significant amount of recent research focused on the prioritization of software quality assurance efforts. One line of work that has been receiving an increasing amount of attention is Software Defect Prediction (SDP), where predictions are made to determine where future defects might appear. Our survey showed that in the past decade, more than 100 papers were published on SDP. Nevertheless, the practical adoption of SDP to date is limited. In this paper, we highlight the findings of our thesis, which identifies the challenges that hinder the adoption of SDP in practice. These challenges include the fact that the majority of SDP research rarely considers the impact of defects when performing their predictions, seldom provides guidance on how to use the SDP results, and is too reactive and defect-centric in nature. Therefore, we propose approaches that tackle these challenges. First, we present approaches that predict high-impact defects. Our approaches illustrate how SDP research can be tailored to consider the impact of defects when making their predictions. Second, we present approaches that simplify SDP models so they can be easily understood and illustrates how these simple models can be used to assist practitioners in prioritizing the creation of unit tests in large software systems. These approaches illustrate how SDP research can provide guidance to practitioners using SDP. Then, we argue that organizations are interested in proactive risk management, which covers more than just defects. For example, risky changes may not introduce defects but they could delay the release of projects. Therefore, we present an approach that predicts risky changes, illustrating how SDP can be more encompassing (i.e., by predicting risk, not only defects) and proactive (i.e., by predicting changes before they are incorporated into the code base).",1063-6773,978-1-4799-6146,10.1109/ICSME.2014.114,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976158,Software Defect Predcition;Risky Software Changes;Software Quality,Predictive models;Industries;Software engineering;Communities;Education;Software systems,program testing;quality assurance;risk management;software quality,software quality prediction;software systems;software quality assurance;software defect prediction;SDP;high-impact defect prediction;proactive risk management;unit tests,,5,36,,,,,,IEEE,IEEE Conferences
Prioritization of Unit Testing on non-object oriented using a top-down based approach,A. Kheirkhah; S. Mohd Daud,"Advanced Informatics School, Universiti Teknologi Malaysia, Jalan Semarak, 54100, Kuala Lumpur, Malaysia; Advanced Informatics School, Universiti Teknologi Malaysia, Jalan Semarak, 54100, Kuala Lumpur, Malaysia",2014 8th. Malaysian Software Engineering Conference (MySEC),,2014,,,72,77,"The issue which makes Unit Testing so tough is the ambiguous ways that the software world keeps moving forward. Although sometimes by implementing simple unit testing methods, this task become easy to handle. However to achieve a comprehensive unit testing it is supposed to test all corners of software such as database, devices, communication etc. This paper proposes an orthogonal software testing approach based on Top-Down technique which treats the input parameters of a software unit in an orthogonal partitioning to dynamic level of testing. Describes how test cases are statistically for each trial of software testing steps and makes a dynamic partitioning approach on non-object oriented experiments. The adequacy of the generated test cases can be validated by examining testing coverage metrics. The authors have considered using of different partitioning and mock objects help to make an isolated testing, improve code's structure and automated testing possibility. The results of the test case executions can be analyzed in order to find the ƒ??IFƒ? metrics for partitioning the traceable ways and detecting defects, to generate more effective test cases in future testing, and to help locate and correct defects in the early stage of testing.",,978-1-4799-5439,10.1109/MySec.2014.6985991,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6985991,Top-Down testing;software unit testing;Non-Object Oriented;CFG;P-Nodes,Software;Electronic mail;Databases;Software testing;Measurement;Partitioning algorithms,program testing;software metrics,unit testing prioritization;top-down technique;software testing;software unit input parameters;dynamic partitioning;nonobject oriented experiments;testing coverage metrics,,,20,,,,,,IEEE,IEEE Conferences
Quality of Service (QoS)-Guaranteed Network Resource Allocation via Software Defined Networking (SDN),A. V. Akella; K. Xiong,NA; NA,"2014 IEEE 12th International Conference on Dependable, Autonomic and Secure Computing",,2014,,,7,13,"Quality of Service (QoS) -- based bandwidth allocation plays a key role in real-time computing systems and applications such as voice IP, teleconferencing, and gaming. Likewise, customer services often need to be distinguished according to their service priorities and requirements. In this paper, we consider bandwidth allocation in the networks of a cloud carrier in which cloud users' requests are processed and transferred by a cloud provider subject to QoS requirements. We present a QoS-guaranteed approach for bandwidth allocation that satisfies QoS requirements for all priority cloud users by using Open vSwitch, based on software defined networking (SDN). We implement and test the proposed approach on the Global Environment for Networking Innovations (GENI). Experimental results show the effectiveness of the proposed approach.",,978-1-4799-5079-9978-1-4799-5078,10.1109/DASC.2014.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945296,,Quality of service;Bandwidth;Routing;Switches;Multimedia communication;Cloud computing;Streaming media,bandwidth allocation;cloud computing;computer network performance evaluation;quality of service;real-time systems,quality of service guaranteed network resource allocation;software defined networking;SDN;quality of service based bandwidth allocation;real-time computing systems;voice IP;teleconferencing;gaming;customer services;cloud user requests;cloud carrier networks;QoS-guaranteed approach;QoS requirements;Open vSwitch;global environment for networking innovations;GENI,,25,21,,,,,,IEEE,IEEE Conferences
"RDCC: An effective test case prioritization framework using software requirements, design and source code collaboration",M. S. Siddik; K. Sakib,"Institute of Information Technology, University of Dhaka, Bangladesh; Institute of Information Technology, University of Dhaka, Bangladesh",2014 17th International Conference on Computer and Information Technology (ICCIT),,2014,,,75,80,"Test case prioritization is a technique for selecting those test cases, which are expected to outperform for determining faulty modules earlier. Different phases of software development lifecycle represent the total software from different point of views, where priority module may vary from phase to phase. However, information from different phases of software development lifecycle is rarely introduced and no one integrates that information to prioritize test cases. This paper presents an effective test case prioritization framework, which takes software requirements specification, design diagrams, source codes and test cases as input and provides a prioritized order of test cases using their collaborative information as output. Requirement IDs are split into words or terms excluding stop words to calculate requirements relativity. Design diagrams are extracted as readable XML format to calculate the degree of interconnectivity among the activities. Source codes are parsed as call graphs where vertices and edges represent classes, and calls between two classes respectively. Requirements relativity, design interconnectivity and class dependencies are multiplied by their assigned weight to calculate final weight and select test cases by mapping the customers' requirements and test cases using that weight. The proposed framework is validated with an academic project and the results show that use of collaborative information during prioritization process can be beneficial.",,978-1-4799-6288-4978-1-4799-6287,10.1109/ICCITechn.2014.7073072,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7073072,Software Engineering;Software Testing;Test Case Prioritization;UML and Code Collaboration,Software;Computers;Testing;Collaboration;Information technology;XML;Unified modeling language,formal specification;graph theory;program testing;source code (software);Unified Modeling Language;XML,RDCC;test case prioritization framework;source code collaboration;test case selection;software development lifecycle phases;software priority module;test case prioritization;information integration;software requirements specification;collaborative information;requirement ID;requirements relativity;design diagram extraction;readable XML format;call graphs;graph vertices;graph edges;design interconnectivity degree;class dependencies;weight assignment;final weight calculation;customer requirements mapping;test case mapping,,2,18,,,,,,IEEE,IEEE Conferences
Regression Testing Approach for Large-Scale Systems,P. Kandil; S. Moussa; N. Badr,NA; NA; NA,2014 IEEE International Symposium on Software Reliability Engineering Workshops,,2014,,,132,133,"Regression testing is an important and expensive activity that is undertaken every time a program is modified to ensure that the changes do not introduce new bugs into previously validated code. Instead of re-running all test cases, different approaches were studied to solve regression testing problems. Data mining techniques are introduced to solve regression testing problems with large-scale systems containing huge sets of test cases, as different data mining techniques were studied to group test cases with similar features. Dealing with groups of test cases instead of each test case separately helped to solve regression testing scalability issues. In this paper, we propose a new methodology for regression testing of large-scale systems using data mining techniques to prioritize and select test cases based on their coverage criteria and fault history.",,978-1-4799-7377,10.1109/ISSREW.2014.96,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983822,Regression Testing;Data Mining;Large Scale System;Test Cases Prioritization;Test Cases Selection,Data mining;Large-scale systems;History;Software;Software testing;Conferences,data mining;large-scale systems;program testing,regression testing approach;large-scale systems;validated code;regression testing problem;data mining techniques;group test cases;regression testing scalability;coverage criteria;fault history,,,10,,,,,,IEEE,IEEE Conferences
Reverse Engineering Complex Feature Correlations for Product Line Configuration Improvement,B. Zhang; M. Becker,NA; NA,2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications,,2014,,,320,327,"As a Software Product Line (SPL) evolves with increasing number of variant features and feature values, the feature correlations become extremely intricate. However, these correlations are often incompletely documented (e.g., In feature models) so that most features can only be configured manually. In order to make product configuration processes more efficient, we present an approach to extracting complex feature correlations from existing product configurations using association mining techniques. Then these correlations are pruned and prioritized in order to minimize the effort of correlation validation. Our approach is conducted on an industrial SPL with 100 product configurations across 480 features. While 80 out of the 100 configurations are used as training data to automatically extract 4834 complex feature correlations, the rest 20 configurations are used as test data to evaluate the improvement potential of configuration efficiency. In the end, avg. 25% features in each of the 20 products can be configured automatically.",1089-6503;2376-9505,978-1-4799-5795,10.1109/SEAA.2014.34,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928830,variability reverse engineering;feature correlation mining;product line configuration improvement,Feature extraction;Correlation;Association rules;Itemsets;Training data;Software engineering,data mining;software product lines,reverse engineering;product line configuration improvement;software product line;SPL;product configuration process;association mining techniques;correlation validation;configuration efficiency,,,23,,,,,,IEEE,IEEE Conferences
RSUs placement using overlap based greedy method for urban and rural roads,R. Rizk; R. Daher; A. Makkawi,"Networks Department, Faculty of Information and Engineering Technology, German University in Cairo, Egypt; Networks Department, Faculty of Information and Engineering Technology, German University in Cairo, Egypt; Networks Department, Faculty of Information and Engineering Technology, German University in Cairo, Egypt",2014 7th International Workshop on Communication Technologies for Vehicles (Nets4Cars-Fall),,2014,,,12,18,"The deployment of efficient roadside networks is a necessity for ITS deployment. The main challenge for the roadside deployment is to find a satisfying or best distribution of RSUs on the roads network according to the given conditions in order to meet the requested requirements of the roads operator. Additionally, various factors affect this process such as traffic, infrastructure and topological characteristics of the roads. This paper introduces an Overlap based Greedy Method (OGM) as a basis for RSUs placement and can be applied on urban and rural roads. This method in its current development mainly considers the RSU coverage radius and the overlap rate into the RSUs distribution process. Moreover, a wide range of influencing factors is considered through the prioritization of Sites of Interest (SoIs). The OGM method is developed within the recently started PRONET project and is integrated into its software platform. The tests conducted on selected roads environment in the city of Rostock (Germany) ensure: (1) decrease in number of chosen SoIs for placement compared to the original scanned number of SoIs, and (2) full (continuous) coverage cannot be guaranteed for a roads network using only the available junctions.",,978-1-4799-5270,10.1109/Nets4CarsFall.2014.7000905,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000905,ITS deployment;Roadside Access Network (RAN);RSUs placement/distribution;overlap rate;junction priority;greedyalgorithm;urban and rural roads,Roads;Junctions;Vehicles;Software;Algorithm design and analysis;Greedy algorithms;Planning,greedy algorithms;intelligent transportation systems;mobile radio;road traffic,planning platform for roadside network;Germany;Rostock;software platform;PRONET project;SoI;sites of interest;roadside unit distribution process;overlap rate;RSU coverage radius;OGM method;road topological characteristic;road infrastructure;road traffic;ITS deployment;rural road;urban road;overlap based greedy method;RSU placement,,4,13,,,,,,IEEE,IEEE Conferences
SimLatte: A Framework to Support Testing for Worst-Case Interrupt Latencies in Embedded Software,T. Yu; W. Srisa-an; M. B. Cohen; G. Rothermel,NA; NA; NA; NA,"2014 IEEE Seventh International Conference on Software Testing, Verification and Validation",,2014,,,313,322,"Embedded systems tend to be interrupt-driven, yet the presence of interrupts can affect system dependability because there can be delays in servicing interrupts. Such delays can occur when multiple interrupt service routines and interrupts of different priorities compete for resources on a given CPU. For this reason, researchers have sought approaches by which to estimate worst-case interrupt latencies (WCILs) for systems. Most existing approaches, however, are based on static analysis. In this paper, we present SIMLATTE, a testing-based approach for finding WCILs. SIMLATTE uses a genetic algorithm for test case generation that converges on a set of inputs and interrupt arrival points that are likely to expose WCILs. It also uses an opportunistic interrupt invocation approach to invoke interrupts at a variety of feasible locations. Our evaluation of SIMLATTE on several non-trivial embedded systems reveals that it is considerably more effective and efficient than random testing. We also determine that the combination of the genetic algorithm and opportunistic interrupt invocation allows SIMLATTE to perform better than it can when using either one in isolation.",2159-4848,978-1-4799-2255,10.1109/ICST.2014.44,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823893,Testing;Embedded Software;Interrupt Latencies;Genetic Algorithm,Biological cells;Testing;Delays;Genetic algorithms;Sociology;Statistics;Generators,embedded systems;genetic algorithms;program diagnostics,SimLatte;support testing;worst-case interrupt latencies;embedded software;system dependability;multiple interrupt service routines;CPU;WCIL;static analysis;genetic algorithm;test case generation;opportunistic interrupt invocation;nontrivial embedded systems;random testing,,4,36,,,,,,IEEE,IEEE Conferences
Test Case Prioritization Based on Information Retrieval Concepts,J. Kwon; I. Ko; G. Rothermel; M. Staats,NA; NA; NA; NA,2014 21st Asia-Pacific Software Engineering Conference,,2014,1,,19,26,"In regression testing, running all a system's test cases can require a great deal of time and resources. Test case prioritization (TCP) attempts to schedule test cases to achieve goals such as higher coverage or faster fault detection. While code coverage-based approaches are typical in TCP, recent work has explored the use of additional information to improve effectiveness. In this work, we explore the use of Information Retrieval (IR) techniques to improve the effectiveness of TCP, particularly for testing infrequently tested code. Our approach considers the frequency at which elements have been tested, in additional to traditional coverage information, balancing these factors using linear regression modeling. Our empirical study demonstrates that our approach is generally more effective than both random and traditional code coverage-based approaches, with improvements in rate of fault detection of up to 4.7%.",1530-1362;1530-1362,978-1-4799-7426-9978-1-4799-7425,10.1109/APSEC.2014.12,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091286,,Testing;Fault detection;Training;Java;Mathematical model;Linear regression;Information retrieval,information retrieval;program testing;regression analysis,test case prioritization;information retrieval concepts;regression testing;TCP;fault detection;code coverage;IR techniques;tested code;linear regression modeling,,2,22,,,,,,IEEE,IEEE Conferences
Test case prioritization techniques ƒ??an empirical studyƒ?,N. Sharma; Sujata; G. N. Purohit,"Department of Computer Science and Engineering, ITM University, Gurgaon, India; Department of Computer Science and Engineering, ITM University, Gurgaon, India; Banasthali Vidhypith, India",2014 International Conference on High Performance Computing and Applications (ICHPCA),,2014,,,1,6,"Regression testing is an expensive process. A number of methodologies of regression testing are used to improve its effectiveness. These are retest all, test case selection, test case reduction and test case prioritization. Retest all technique involves re-execution of all available test suites, which are critical moreover cost effective. In order to increase efficiency, test case prioritization is being utilized for rearranging the test cases. A number of algorithms has been stated in the literature survey such as Greedy Algorithms and Metaheuristic search algorithms. A simple greedy algorithm focuses on test case prioritization but results in less efficient manner, due to which researches moved towards the additional greedy and 2-Optimal algorithms. Forthcoming metaheuristic search technique (Hill climbing and Genetic Algorithm) produces a much better solution to the test case prioritization problem. It implements stochastic optimization while dealing with problem concern. The genetic algorithm is an evolutionary algorithm which gives an exact mathematical fitness value for the test cases on which prioritization is done. This paper focuses on the comparison of metaheuristic genetic algorithm with other algorithms and proves the efficiency of genetic algorithm over the remaining ones.",,978-1-4799-5958-7978-1-4799-5957,10.1109/ICHPCA.2014.7045344,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045344,regression testing;test case prioritization;genetic algorithm;greedy algorithm;APFD,Genetics;Computers;Genetic algorithms;Testing,genetic algorithms;greedy algorithms;program testing;regression analysis;search problems;stochastic programming,regression testing;test case selection;test case reduction;test case prioritization;greedy algorith;optimal algorithm;metaheuristic search technique;stochastic optimization;evolutionary algorithm;mathematical fitness value exaction;metaheuristic genetic algorithm;software testing,,,11,,,,,,IEEE,IEEE Conferences
Test case prioritization using multi objective particle swarm optimizer,M. Tyagi; S. Malhotra,"Department of CSE, U.I.E.T., Kurukshetra University, Haryana, India; Department of CSE, U.I.E.T., Kurukshetra University, Haryana, India",2014 International Conference on Signal Propagation and Computer Technology (ICSPCT 2014),,2014,,,390,395,"The goal of regression testing is to validate the modified software. Due to the resource and time constraints, it becomes necessary to develop techniques to minimize existing test suites by eliminating redundant test cases and prioritizing them. This paper proposes a 3-phase approach to solve test case prioritization. In the first phase, we are removing redundant test cases by simple matrix operation. In the second phase, test cases are selected from the test suite such that selected test cases represent the minimal set which covers all faults and also at the minimum execution time. For this phase, we are using multi objective particle swarm optimization (MOPSO) which optimizes fault coverage and execution time. In the third phase, we allocate priority to test cases obtained from the second phase. Priority is obtained by calculating the ratio of fault coverage to the execution time of test cases, higher the value of the ratio higher will be the priority and the test cases which are not selected in phase 2 are added to the test suite in sequential order. We have also performed experimental analysis based on maximum fault coverage and minimum execution time. The proposed MOPSO approach is compared with other prioritization techniques such as No Ordering, Reverse Ordering and Random Ordering by calculating Average Percentage of fault detected (APFD) for each technique and it can be concluded that the proposed approach outperformed all techniques mentioned above.",,978-1-4799-3140-8978-1-4799-3139,10.1109/ICSPCT.2014.6884931,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6884931,Regression Testing;Test case selection;Test case prioritization;Multi objective Particle Swarm Optimization,,particle swarm optimisation;program testing;program verification;regression analysis;software fault tolerance,test case prioritization;multiobjective particle swarm optimizer;regression testing;modified software validation;time constraints;resource constraints;3-phase approach;matrix operation;redundant test case removal;test suite;MOPSO;fault coverage optimization;execution time optimization;average percentage of fault detected calculation;APFD calculation,,2,20,,,,,,IEEE,IEEE Conferences
Test case prioritization with improved genetic algorithm,M. ??. Cingiz; ?. Temei; O. KahpsŽñz,"Bilgisayar M?¬hendisliŽ?i B??l?¬m?¬, YŽñldŽñz Teknik ??niversitesi, Žøstanbul, Esenler; Bilgisayar M?¬hendisliŽ?i B??l?¬m?¬, YŽñldŽñz Teknik ??niversitesi, Žøstanbul, Esenler; Bilgisayar M?¬hendisliŽ?i B??l?¬m?¬, YŽñldŽñz Teknik ??niversitesi, Žøstanbul, Esenler",2014 22nd Signal Processing and Communications Applications Conference (SIU),,2014,,,1223,1226,"In software development, the most time consuming phase is maintenance. Regression testing, which is a part of maintenance, deals with test case prioritization that aims to increase rate of fault detection with less number of tests. In our study, we used 100 tests and 1000 faults; however, faults are detected by tests using genetic algorithm and improved genetic algorithm. After test case prioritization, we may detect all faults with less number of tests so there'll no need to apply all 100 tests (re-test).",2165-0608,978-1-4799-4874,10.1109/SIU.2014.6830456,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6830456,Anahtar Kelimeler;regression test;test case prioritization;genetic algorithms;hybrid algorithms,Software engineering;Signal processing;Conferences;Genetic algorithms;Software;Maintenance engineering;Testing,fault diagnosis;genetic algorithms;program testing;software maintenance,test case prioritization;improved genetic algorithm;software development;software maintenance;regression testing;fault detection rate improvement,,1,8,,,,,,IEEE,IEEE Conferences
Test Suite Prioritization by Switching Cost,H. Wu; C. Nie; F. Kuo,NA; NA; NA,"2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops",,2014,,,133,142,"Test suite generation and prioritization are two main research fields to improve testing efficiency. Combinatorial testing has been proven as an effective method to generate test suite for highly configurable software systems, while test suites are often prioritized by interaction coverage to detect faults as early as possible. However, for some cases, there exists reasonable cost of reconfiguring parameter settings when switching test cases in different orders. Surprisingly, only few studies paid attention to it. In this paper, by proposing greedy algorithms and graph-based algorithms, we aim to prioritize a given test suite to minimize its total switching cost. We also compare two different prioritization strategies by a series of experiments, and discuss the advantages of our prioritization strategy and the selection of prioritization techniques. The results show that prioritization by switching cost can improve testing efficiency and our prioritization strategy can produce a small test suite with a reasonably low switching cost. This prioritization can be used widely and help locate fault causing interactions. The results also suggest that when testing highly configurable software systems and no knowledge of fault detection can be used, prioritization by switching cost is a good choice to detect faults earlier.",,978-1-4799-5790,10.1109/ICSTW.2014.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825648,Combinatorial Testing;Test Suite Prioritization;Switching Cost,Switches;Testing;Greedy algorithms;Arrays;Software;Heuristic algorithms;Fault detection,fault location;graph theory;greedy algorithms;minimisation;program testing;software cost estimation;software fault tolerance,switching cost minimisation;test suite generation;testing efficiency improvement;combinatorial testing;configurable software systems;interaction coverage;reconfiguring parameter settings;greedy algorithms;graph-based algorithms;test suite prioritization strategy;fault location;fault detection,,4,26,,,,,,IEEE,IEEE Conferences
Test Suite Reduction by Combinatorial-Based Coverage of Event Sequences,Q. Mayo; R. Michaels; R. Bryce,NA; NA; NA,"2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops",,2014,,,128,132,"Combinatorial-based criteria are useful in several studies for test suite generation, prioritization, and minimization. In this paper, we extend previous work by using combinatorial-based criteria for test suite reduction. We use criteria that are based on combinatorial coverage of events and consider the order in which events occur. A simple combinatorial-based criterion covers t-way events and does not differentiate between the order of events. The event pair (e<sub>1</sub>, e<sub>2</sub>) is counted the same as if it occurs in the order (e<sub>2</sub>, e<sub>1</sub>). We also use two criteria that consider the order in which events occur since different orderings of events may be valuable during testing. First, the consecutive sequence-based criterion counts all event sequences in different orders, but they must occur adjacent to each other. The sequence-based criterion counts pairs in all orders without the requirement that events must be adjacent. We evaluate the new criteria on three GUI applications. We use 2way inter-window coverage in our studies. All of the 2way combinatorial-based criteria are effective in reducing the test suites and maintaining close to 100% fault finding effectiveness. Our future work examines larger test suites, higher strength coverage, techniques to partition event data, and further empirical studies.",,978-1-4799-5790,10.1109/ICSTW.2014.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825647,Test suite reduction;Combinatorial testing;GUI testing,Software engineering;Paints;Graphical user interfaces;Minimization;Conferences;Software testing,combinatorial mathematics;program testing,test suite reduction;combinatorial-based coverage;event sequences;t-way events;event pair;events ordering;sequence-based criterion;GUI applications;2way interwindow coverage;2way combinatorial-based criteria,,3,19,,,,,,IEEE,IEEE Conferences
Testability of object-oriented systems: An AHP-based approach for prioritization of metrics,P. Khanna,"Siemens Technology and Services, Private Limited, CT DC AA E P-IE BDG3, IFFCO Tower, Plot No. 3, Sector 29, Gurgaon 122001, India",2014 International Conference on Contemporary Computing and Informatics (IC3I),,2014,,,273,281,This paper investigates testability from the perspective of metrics used in an object-oriented system. The idea is to give an overview of object oriented design metrics with the prioritization of same keeping testability as the overall goal. We have used Analytic Hierarchy Process (AHP) method to attain which metric is mostly used and is best for testability.,,978-1-4799-6629,10.1109/IC3I.2014.7019595,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019595,AHP;testability;CK metrics suite;MOOD Metrics Model,Measurement;Testing;Software;Complexity theory;Encapsulation;Couplings;Analytic hierarchy process,analytic hierarchy process;object-oriented programming;program testing;software metrics,testability;metrics prioritization;analytic hierarchy process;AHP-based approach;object-oriented systems,,,17,,,,,,IEEE,IEEE Conferences
The research of the test case prioritization algorithm for black box testing,W. Liu; X. Wu; W. Zhang; Y. Xu,"Beijing Institute of Tracking and Telecommunications Technology, Beijing, China; Beijing Institute of Tracking and Telecommunications Technology, Beijing, China; Beijing Institute of Tracking and Telecommunications Technology, Beijing, China; Beijing Institute of Tracking and Telecommunications Technology, Beijing, China",2014 IEEE 5th International Conference on Software Engineering and Service Science,,2014,,,37,40,"In order to improve the efficiency of software test case execution, this paper analyzed the impact of some factors to test cases prioritization and presented two adjustment algorithms. These factors included software requirement prioritization, software failure severity and software failure probability level. Firstly, gave the definition of software requirement prioritization, the ranking methods of software failure severity and software failure probability level, the description of the relationship between test cases and test requirements. Then, presented an initial test case prioritization method based on the analysis. And then, proposed a dynamic adjustment algorithm using of software requirement prioritization and software failure probability level when software failure occurred. Experimental data show that the two test case prioritization algorithms can improve the efficiency of software testing and are helpful to find more software defects in a short period.",2327-0594;2327-0586,978-1-4799-3279-5978-1-4799-3278-8978-1-4799-3277,10.1109/ICSESS.2014.6933509,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933509,software testing;test case;test case prioritization,Heuristic algorithms;Software algorithms;Algorithm design and analysis;Software systems;Software testing,program testing;software reliability,test case prioritization algorithm;black box testing;software test case execution;dynamic adjustment algorithm;software failure severity;software failure probability level;software requirement prioritization;test requirements;software testing;software defects,,,8,,,,,,IEEE,IEEE Conferences
Toolset and Program Repository for Code Coverage-Based Test Suite Analysis and Manipulation,D. Tengeri; ?. Besz??des; D. Havas; T. Gyim??thy,NA; NA; NA; NA,2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation,,2014,,,47,52,"Code coverage is often used in academic and industrial practice of white-box software testing. Various test optimization methods, e.g. Test selection and prioritization, rely on code coverage information, but other related fields benefit from it as well, such as fault localization. These methods require access to the fine details of coverage information and efficient ways of processing this data. The purpose of the (free) SoDA library and toolset is to provide an efficient set of data structures and algorithms which can be used to prepare, store and analyze in various ways data related to code coverage. The focus of SoDA is not on the calculation of coverage data (such as instrumentation and test execution) but on the analysis and manipulation of test suites based on such information. An important design goal of the library was to be usable on industrial-size programs and test suites. Furthermore, there is no limitation on programming language, analysis granularity and coverage criteria. In this paper, we demonstrate the purpose and benefits of the library, the associated toolset, which also includes a graphical user interface, as well as possible usage scenarios. SoDA also includes a repository of prepared programs, which are from small to large sizes and can be used for experimentation and as a benchmark for code coverage related research.",,978-1-4799-6148,10.1109/SCAM.2014.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975635,Regression testing;test suite analysis;test suite optimization;code coverage;program repository,Measurement;Libraries;Algorithm design and analysis;Data structures;Testing;Instruments;Graphical user interfaces,optimisation;program testing,program repository;toolset repository;code coverage;test suite analysis;test suite manipulation;white-box software testing;optimization methods;fault localization;SoDA library;data structures;data algorithms;industrial size programs;programming language,,6,17,,,,,,IEEE,IEEE Conferences
Using Dual Priority scheduling to improve the resource utilization in the nMPRA microcontrollers,N. C. Gaitan; L. Andries,"Faculty of Electrical Engineering and Computer Science, Stefan cel Mare University of Suceava, Suceava, Romania; Faculty of Electrical Engineering and Computer Science, Stefan cel Mare University of Suceava, Suceava, Romania",2014 International Conference on Development and Application Systems (DAS),,2014,,,73,78,"The current practice in most of the safety-critical areas, including automotive, avionics systems and factory automation, encouraging the use of real-time time-trigger schedulers that does not allow interference to take place between safety-critical components and non-critical. Furthermore, in these systems the lack of interference between safety-critical components and non-critical components is achieved by a strict isolation between components with different degrees of severity. This approach can assure, easily, the certification of the safety-critical functionality, but leads to very low resource utilization. For this purpose it will be presented a solution that when the system enters into a state that is different from the normal running state (test service), allowing relaxation and a change in the activation time of tasks (release) violating the fixed priorities scheduling, but avoiding starvation of the system tasks. The proposed solution modifies a static scheduler in a dynamic scheduler depending on the system status using Dual Priority scheduling. The algorithm has been proposed to be implemented on a nMPRA processor, by multiplying hardware resources (PC, pipeline registers and file registers) and other facilities (events, mutexes, interrupts, IPC communication, timer's, the static scheduler and support for dynamic scheduler) provides a switching and response time for events within 1 to 3 machine cycles.",,978-1-4799-5094-2978-1-4799-5092,10.1109/DAAS.2014.6842431,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842431,real time system;dual priority scheduling;nMPRA microcontroller,Processor scheduling;Real-time systems;Registers;Dynamic scheduling;Operating systems;Heuristic algorithms,microcontrollers;object-oriented programming;processor scheduling;program diagnostics;real-time systems;safety-critical software,dual priority scheduling;resource utilization;nMPRA microcontroller;safety-critical area;automotive;avionics systems;factory automation;real-time time-trigger scheduler;safety-critical component;noncritical component;safety-critical functionality;normal running state;test service;fixed priorities scheduling;system task;static scheduler;dynamic scheduler;system status;nMPRA processor;multiplying hardware resource;pipeline registers;file registers;IPC communication,,2,12,,,,,,IEEE,IEEE Conferences
Using TTCN-3 to Test SPDY Protocol Interaction Property,S. Zhang; H. Li; Y. Xue; X. Wang,NA; NA; NA; NA,2014 IEEE 38th International Computer Software and Applications Conference Workshops,,2014,,,541,546,"SPDY protocol is a new application-layer communication protocol which was proposed by Google in order to overcome the defects of HTTP. On the basis of HTTP protocol, SPDY offered four improvements to shorten the page loading time such as multiplexed requests, prioritized requests, server pushed streams and compressed headers. However, there is no much test work of the SPDY especially treating it as a black box, focusing on its interaction property, or testing it by using TTCN-3. In this paper, the interaction property of SPDY protocol is analyzed according to the SPDY protocol draft specification, and a novel test work designed in view of SPDY interaction property is implemented. During the test work, the interaction granularity of the SPDY peers is divided into three kinds of granularity from different levels, meanwhile the test cases were generated in accordance with the draft specification and encoded by utilizing TTCN-3 language for executing on the TT work bench Professional software. Above all we must ensure the SPDY server-side can support the SPDY protocol and the test host installs TT work bench Professional software to complete TTCN-3 testing. Finally, the interaction property of SPDY protocol was tested and the test results were reported. Moreover, some failed test cases were analyzed in detail.",,978-1-4799-3578,10.1109/COMPSACW.2014.90,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903186,SPDY;Interaction Property;TTCN-3,Protocols;Servers;Testing;Decoding;Loading;Web pages;Google,protocols,SPDY protocol interaction property;application layer communication protocol;Google;HTTP protocol;page loading time;black box;SPDY protocol draft specification;TTCN-3 language;SPDY server-side;professional software;TTCN-3 testing,,1,13,,,,,,IEEE,IEEE Conferences
8th International Workshop on Search-Based Software Testing (SBST 2015),G. Gay; G. Antoniol,NA; NA,2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,,2015,2,,1001,1002,"This paper is a report on the 8th International Workshop on Search-Based Software Testing at the 37th International Conference on Sofrware Engineering (ICSE). Search-Based Software Testing (SBST) is a form of Search-Based Software Engineering (SBSE) that optimizes testing through the use of computational search. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service-orientated architectures, construct test suites for interaction testing, and validate real time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.Three full research papers, three short papers, and threeposition papers will be presented in the two-day workshop. Additionally, six development groups have pitted their test generation tools against a common set of programs and benchmarks, and will present their techniques and results. This report will give the background of the workshop and detail the provisional program.",0270-5257;1558-1225,978-1-4799-1934,10.1109/ICSE.2015.323,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203148,,Conferences;Software engineering;Software;Software testing;Search problems;Measurement,,,,,3,,,,,,IEEE,IEEE Conferences
A Clustering-Bayesian Network Based Approach for Test Case Prioritization,X. Zhao; Z. Wang; X. Fan; Z. Wang,NA; NA; NA; NA,2015 IEEE 39th Annual Computer Software and Applications Conference,,2015,3,,542,547,"Test case prioritization can effectively reduce the cost of regression testing by executing test cases with respect to their contributions to testing goals. Previous research has proved that the Bayesian Networks based technique which uses source code change information, software quality metrics and test coverage data has better performance than those methods merely depending on only one of the items above. Although the former Bayesian Networks based Test Case Prioritization (BNTCP) focusing on assessing the fault detection capability of each test case can utilize all three items above, it still has a deficiency that ignores the similarity between test cases. For mitigating this problem, this paper proposes a hybrid regression test case prioritization technique which aims to achieve better prioritization by incorporating code coverage based clustering approach with BNTCP to depress the impact of those similar test cases having common code coverage. Experiments on two Java projects with mutation faults and one Java project with hand-seeded faults have been conducted to evaluate the fault detection performance of the proposed approach against Additional Greedy approach, Bayesian Networks based approach (BNTCP), Bayesian Networks based approach with feedback (BNA) and code coverage based clustering approach. The experimental results showed that the proposed approach is promising.",0730-3157,978-1-4673-6564-2978-1-4673-6563,10.1109/COMPSAC.2015.154,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273420,Regression testing; Test case prioritization (TCP); Clustering; Bayesian Network (BN),Fault detection;Measurement;Testing;Bayes methods;Software quality;Java;Clustering algorithms,belief networks;fault diagnosis;Java;program testing;regression analysis;software metrics;software quality;source code (software),clustering-Bayesian network based approach;regression testing;source code change information;software quality metrics;test coverage data;Bayesian networks based test case prioritization;BNTCP;fault detection capability;hybrid regression test case prioritization technique;code coverage based clustering approach;Java project;fault detection performance;Bayesian networks based approach;BNA,,1,22,,,,,,IEEE,IEEE Conferences
A coupling effect based test case prioritization technique,H. Kumar; N. Chauhan,"YMCA University of Science &amp; Technology, Faridabad, India; YMCA University of Science &amp; Technology, Faridabad, India",2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom),,2015,,,1341,1345,"Regression testing is a process that executes subset of tests that have already been conducted to ensure that changes have not propagated unintended side effects. Test case prioritization aims at reordering the regression test suit based on certain criteria, so that the test cases with higher priority can be executed first rather than those with lower priority. In this paper, a new approach for test case prioritization has been proposed which is based on a module-coupling effect that considers the module-coupling value for the purpose of prioritizing the modules in the software so that critical modules can be identified which in turn will find the prioritized set of test cases. In this way there will be high percentage of detecting critical errors that have been propagated to other modules due to any change in a module. The proposed approach has been evaluated with the case study of software consisting of ten modules.",,978-9-3805-4416-8978-9-3805-4415-1978-9-3805-4414,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100468,Regression Testing;Test Case Prioritization;Coupling & Cohesion,Couplings;Testing;Software;Fault detection;Symmetric matrices;Computer science;Computer bugs,program testing,coupling effect based test case prioritization technique;regression testing;module-coupling effect;critical error detection,,,12,,,,,,IEEE,IEEE Conferences
A defect dependency based approach to improve software quality in integrated software products,S. A. Karre; Y. R. Reddy,"Software Engineering Research Center, International Institute of Information Technology, Hyderabad, India; Software Engineering Research Center, International Institute of Information Technology, Hyderabad, India",2015 International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE),,2015,,,110,117,"Integrated software products are complex in design. They are prone to defects caused by integrated and non-integrated modules of the entire integrated software suite. In such software products, a small proportion of defects are fixed as soon as they are reported. Rest of the defects are targeted for fixes in future product release cycles. Among such targeted defects, most of them seem to be insignificant and innocuous in the current version but have the potential to become acute in future versions. In this paper, we propose an approach to study defect dependency of the reported defect using a dependency metric. Identifying the dependency of a defect in an integrated product suite can help the product stake-owners to prioritize them and help improve software quality.",,978-989-758-143,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320343,Defect Dependency;Defect Dataset;Dependency Metric;Software Quality;Integrated Software Products;Rule-based Classification,Software;Yttrium;Measurement;Testing;Data mining;Training;Influenza,,,,,20,,,,,,IEEE,IEEE Conferences
A methodology for regression testing reduction and prioritization of agile releases,P. Kandil; S. Moussa; N. Badr,"Department of Information Systems, Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt; Department of Information Systems, Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt; Department of Information Systems, Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt",2015 5th International Conference on Information & Communication Technology and Accessibility (ICTA),,2015,,,1,6,"Regression testing is the type of software testing that seeks to uncover new software bugs in existing areas of a system after changes have been made to them. The significance of regression testing have grown in the past decade with the amplified adoption of agile development methodologies, which requires the execution of regression testing at the end of each release. In this paper, we present an automated agile regression testing approach that reduces the number of test cases to be used at regression phase depending on the similarity of issues exposed from the different test cases, taking into consideration the user story coverage. It then prioritizes the reduced test cases using user-provided weighted agile parameters. The proposed approach achieves enhancement for both the reduction and prioritization of test cases for agile regression testing.",,978-1-4673-8749-1978-1-4673-8748,10.1109/ICTA.2015.7426903,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426903,Regression Testing;Agile Testing;Test Cases Reduction;Test Cases Prioritization;Agile Parameters;Agile Releases,Fault detection;Mathematical model;Software;Software testing;Greedy algorithms;Complexity theory,program debugging;program testing;software prototyping,regression testing reduction;agile release prioritization;software testing;software bugs;agile development methodologies;automated agile regression testing approach;user story coverage;user-provided weighted agile parameters,,,22,,,,,,IEEE,IEEE Conferences
A novel dynamic analysis of test cases to improve testing efficiency in object-oriented systems,Tao Hu; Gangyi Ding,"School of Software, Beijing Institute of Technology, China; School of Software, Beijing Institute of Technology, China",2015 4th International Conference on Computer Science and Network Technology (ICCSNT),,2015,1,,457,461,"In this paper, we present a series of methods to improve testing efficiency especially for regression testing from a novel view, namely dynamic analysis of test cases suitable for class testing in object-oriented systems. We mine static call graphs and dynamic call trees to represent the static features and dynamic tests of the program. By graph analysis, we present a series of methods and testing criteria to evaluate test cases from the view of code coverage. These methods improve testing efficiency for class testing from the following aspects: automation; multi-angle evaluations of test cases; improvement and management of test cases; providing different prioritization criteria and optimization criteria for regression testing to meet different testing requirements etc. What's more, they can be used in large-scale OO systems, and the test results are quantifiable.",,978-1-4673-8173-4978-1-4673-8172,10.1109/ICCSNT.2015.7490789,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490789,software testing;regression test;object-oriented system,Software;Optimization;Software testing;Object oriented modeling;Automation;Encoding,,,,,13,,,,,,IEEE,IEEE Conferences
A Schema Support for Selection of Test Case Prioritization Techniques,Sujata; G. N. Purohit,NA; NA,2015 Fifth International Conference on Advanced Computing & Communication Technologies,,2015,,,547,551,"Regression testing is a vast field of research. It is very costly and time consuming process but on the other hand very important process in software testing. Retest all, Test case Selection, Hybrid and Test Case Prioritization are its various techniques which are used to reduce the efforts in maintenance phase. In technical literature several techniques are present with their different and vast number of goals which can be applied in software projects despite of that they have not proven their true efficiency in the testing process. The major problem in regression testing area is to select the test case prioritization technique/s that is effective in such a way that maximum project characteristics should be cover in a minimum time span. However, consideration of this decision be carefully done so that loss of resources can be avoided in a software project. Based on the above scenario, author proposes a selection schema to support the selection of TCP techniques for a given software project aiming at maximizing the coverage of software project characteristics considering aspect of prioritization of software project characteristics. At the end, preliminary results of an experimental evaluation are presented. The purpose of this research is decision should be based on the objective knowledge of the techniques rather than considering some perception and assumptions.",2327-0659;2327-0632,978-1-4799-8488-6978-1-4799-8487,10.1109/ACCT.2015.91,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079143,regression testing;project characteristics;selection schema;test case prioritization techniques,Software;Maintenance engineering;Software algorithms;Software testing;Reliability;Estimation,program testing;project management;regression analysis;software maintenance;software management;statistical testing,hybrid test case prioritization techniques;software testing;retest all technique;test case selection technique;maintenance phase;regression testing;software project characteristics;TCP techniques,,1,12,,,,,,IEEE,IEEE Conferences
A similarity-based approach for test case prioritization using historical failure data,T. B. Noor; H. Hemmati,"Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada",2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE),,2015,,,58,68,"Test case prioritization is a crucial element in software quality assurance in practice, specially, in the context of regression testing. Typically, test cases are prioritized in a way that they detect the potential faults earlier. The effectiveness of test cases, in terms of fault detection, is estimated using quality metrics, such as code coverage, size, and historical fault detection. Prior studies have shown that previously failing test cases are highly likely to fail again in the next releases, therefore, they are highly ranked, while prioritizing. However, in practice, a failing test case may not be exactly the same as a previously failed test case, but quite similar, e.g., when the new failing test is a slightly modified version of an old failing one to catch an undetected fault. In this paper, we define a class of metrics that estimate the test cases quality using their similarity to the previously failing test cases. We have conducted several experiments with five real world open source software systems, with real faults, to evaluate the effectiveness of these quality metrics. The results of our study show that our proposed similarity-based quality measure is significantly more effective for prioritizing test cases compared to existing test case quality measures.",,978-1-5090-0406-5978-1-5090-0405,10.1109/ISSRE.2015.7381799,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381799,Test case prioritization;Test quality metric;Similarity;Execution trace;Distance function;Historical data;Code coverage;Test size,Measurement;Testing;Fault detection;History;Context;Software quality,fault diagnosis;program testing;public domain software;quality assurance;regression analysis;software metrics;software quality;statistical testing,test case prioritization;historical failure data;software quality assurance;regression testing;software quality metrics;code coverage;code size;historical fault detection;open source software systems;similarity-based quality measure,,6,32,,,,,,IEEE,IEEE Conferences
A simple statically reconfigurable processor architecture,A. Moosa; M. Aneesh,"Dept. of Electronics Engg, School of Engg. and Technology, Pondicherry University, Puducherry, India; Dept. of Electronics Engg, School of Engg. and Technology, Pondicherry University, Puducherry, India","2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)",,2015,,,1,5,"A reconfigurable Processor architecture is presented that has been designed prioritizing modularity, scalability and simplicity. Modular design enables swapping of functional units within the main processing core while maintaining the same programming model. This ensures that the associated software tools chain such as Assembler and Compiler need not be redesigned. Scalable design enables reconfiguring the datapath width to suite application requirements without redesigning the processor architecture or making changes to the software program already written. Applications for such design range from academia where real world performance of many proposed Adder/Multiplier structures may be tested; to data centers where the nature of operation to be performed on massive chunks of data changes regularly requiring ASIC like performance.",,978-1-4799-6085-9978-1-4799-6084-2978-1-4799-6083,10.1109/ICECCT.2015.7226112,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226112,Verilog;Architecture;VLSI;Reconfigurable;CPU,Pipeline processing;Registers;Field programmable gate arrays;Tuning;Table lookup;Embedded systems,adders;application specific integrated circuits;field programmable gate arrays;hardware description languages;reconfigurable architectures;VLSI,FPGA;Verilog;VLSI;ASIC like performance;multiplier structures;adder structures;scalable design;modular design;statically reconfigurable processor architecture,,,5,,,,,,IEEE,IEEE Conferences
A Subsumption Hierarchy of Test Case Prioritization for Composite Services,L. Mei; Y. Cai; C. Jia; B. Jiang; W. K. Chan; Z. Zhang; T. H. Tse,"Department of Solutions Engineering and Operation Excellence, IBM Researchƒ??China, Beijing, China; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong; School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Pokfulam, Hong Kong",IEEE Transactions on Services Computing,,2015,8,5,658,673,"Many composite workflow services utilize non-imperative XML technologies such as WSDL, XPath, XML schema, and XML messages. Regression testing should assure the services against regression faults that appear in both the workflows and these artifacts. In this paper, we propose a refinement-oriented level-exploration strategy and a multilevel coverage model that captures progressively the coverage of different types of artifacts by the test cases. We show that by using them, the test case prioritization techniques initialized on top of existing greedy-based test case prioritization strategy form a subsumption hierarchy such that a technique can produce more test suite permutations than a technique that subsumes it. Our experimental study of a model instance shows that a technique generally achieves a higher fault detection rate than a subsumed technique, which validates that the proposed hierarchy and model have the potential to improve the cost-effectiveness of test case prioritization techniques.",1939-1374;2372-0204,,10.1109/TSC.2014.2331683,National Key Basic Research Program of China; General Research Fund of the Research Grants Council of Hong Kong; National Natural Science Foundation of China; National Science and Technology Major Project of China; CCF-Tencent Open Research Fund; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839018,Test case prioritization;service orientation;XPath;WSDL;XML messages,XML;Fault detection;Semantics;Testing;Business;Educational institutions,program testing;regression analysis;service-oriented architecture;Web Services Business Process Execution Language;XML,subsumption hierarchy;test case prioritization;composite workflow service;XML technology;regression testing;refinement-oriented level-exploration strategy;multilevel coverage model;Web Services Business Process Execution Language;WS-BPEL,,8,52,,,,,,IEEE,IEEE Journals & Magazines
A Test Framework for Communications-Critical Large-Scale Systems,M. A. Nabulsi; R. M. Hierons,Brunel University; Brunel University,IEEE Software,,2015,32,3,86,93,"Today's large-scale systems couldn't function without the reliable availability of a range of network communications capabilities. Software, hardware, and communications technologies have been advancing throughout the past two decades. However, the methods that industry commonly uses to test large-scale systems that incorporate critical communications interfaces haven't kept pace. The need exists for a specifically tailored framework to achieve effective, precise testing of communications-critical large-scale systems. A proposed test framework offers an alternative to the current generic approaches that lead to inefficient, costly testing in industry. A case study illustrates its benefits, which can also be realized with other comparable systems.",0740-7459;1937-4194,,10.1109/MS.2014.53,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6785925,testing;test framework;communications-critical large-scale systems;IT systems;test case prioritization;requirements prioritization;software engineering,Software testing;Large-scale systems;Requirements engineering;Software engineering;Information technology;ISO standards,program testing;safety-critical software,communications-critical large-scale systems;network communication capability;formal software test methodology;communication technology;software technology;hardware technology,,,8,,,,,,IEEE,IEEE Journals & Magazines
AEGIS autonomous targeting for the Curiosity rover's ChemCam instrument,R. Francis; T. Estlin; D. Gaines; B. Bornstein; S. Schaffer; V. Verma; R. Anderson; M. Burl; S. Chu; R. Casta?ño; D. Thompson; D. Blaney; L. de Flores; G. Doran; T. Nelson; R. Wiens,"Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Los Alamos National Laboratory, New Mexico, United States; Los Alamos National Laboratory, New Mexico, United States",2015 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),,2015,,,1,5,"AEGIS (Autonomous Exploration for Gathering Increased Science) is a software suite that will imminently be operational aboard NASA's Curiosity Mars rover, allowing the rover to autonomously detect and prioritize targets in its surroundings, and acquire geochemical spectra using its ChemCam instrument. ChemCam, a Laser-Induced Breakdown Spectrometer (LIBS), is normally used to study targets selected by scientists using images taken by the rover on a previous sol and relayed by Mars orbiters to Earth. During certain mission phases, ground-based target selection entails significant delays and the use of limited communication bandwidth to send the images. AEGIS will allow the science team to define the properties of preferred targets, and obtain geochemical data more quickly, at lower data penalty, without the extra ground-inthe-loop step. The system uses advanced image analysis techniques to find targets in images taken by the rover's stereo navigation cameras (NavCam), and can rank, filter, and select targets based on properties selected by the science team. AEGIS can also be used to analyze images from ChemCam's Remote Micro Imager (RMI) context camera, allowing it to autonomously target very fine-scale features - such as veins in a rock outcrop - which are too small to detect with the range and resolution of NavCam. AEGIS allows science activities to be conducted in a greater range of mission conditions, and saves precious time and command cycles during the rover's surface mission. The system is currently undergoing initial tests and checkouts aboard the rover, and is expected to be operational by late 2015. Other current activities are focused on science team training and the development of target profiles for the environments in which AEGIS is expected to be used on Mars.",2332-5615,978-1-4673-9558,10.1109/AIPR.2015.7444544,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444544,autonomous science;mars;planetary exploration;image interpretation;natural scene interpretation,Mars;Earth;Instruments;Rocks;Cameras;Planning,aerospace computing;aerospace robotics;cameras;computerised instrumentation;control engineering computing;object detection;remotely operated vehicles;stereo image processing,AEGIS autonomous targeting;Autonomous Exploration for Gathering Increased Science;ChemCam instrument;NASA Curiosity Mars rover;National Aeronautics and Space Administration;LIBS;laser-induced breakdown spectrometer;geochemical spectra;ground-based target selection;remote micro imager;RMI context camera;image analysis;stereo navigation cameras,,,5,,,,,,IEEE,IEEE Conferences
An effective test case prioritization method based on fault severity,Y. Wang; X. Zhao; X. Ding,"College of Computer and Information Science, University of Southwest University, Beibei, Chongqing, China; College of Computer and Information Science, University of Southwest University, Beibei, Chongqing, China; College of Computer and Information Science, University of Southwest University, Beibei, Chongqing, China",2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS),,2015,,,737,741,"In regression testing area, test case prioritization is one of the main techniques to improve the test validity and test effectiveness. However, when the test cases have the same maximum coverage rate, the random selection of the additional statement will influence the effect of sorting. For dealing with this problem, a new method is proposed to optimize test case prioritization based on fault severity, referred to as additional-statement-on-fault-severity. Facing those same maximum coverage rate, the new technique main consider a factor, fault severity, to sort test cases, it figures out the value of test case based on the algorithm of the new technique and order the sequence from high to low. Experiment results show that the improved technique of test case prioritizaftion can improve the efficiency of regression testing.",2327-0594;2327-0586,978-1-4799-8353-7978-1-4799-8352-0978-1-4799-8351,10.1109/ICSESS.2015.7339162,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339162,regression testing;random selection;additional statement;test case prioritization;fault severities,Software;Testing;Computers;Fault detection;Sorting;Minimization;Computer aided software engineering,program testing;sorting,test case prioritization method;regression testing area;test validity;test effectiveness;maximum coverage rate;statement random selection;sorting effect;additional-statement-on-fault-severity,,2,14,,,,,,IEEE,IEEE Conferences
An Information Retrieval Approach for Regression Test Prioritization Based on Program Changes,R. K. Saha; L. Zhang; S. Khurshid; D. E. Perry,NA; NA; NA; NA,2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,,2015,1,,268,279,"Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight open-source Java projects shows that REPiR is computationally efficient and performs better than existing (dynamic or static) techniques for the majority of subject systems.",0270-5257;1558-1225,978-1-4799-1934,10.1109/ICSE.2015.47,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194580,Regression Testing;Test Prioritization;Information Retrieval,Software engineering;Information retrieval;Testing;Standards;Open source software;Natural languages,information retrieval;program debugging;program testing;public domain software;system monitoring,information retrieval approach;regression test prioritization;program changes;regression testing;regression suites;bugs;dynamic analysis;REPiR;open-source IR toolkit Indri;open-source Java projects,,12,67,,,,,,IEEE,IEEE Conferences
Applying Ant Colony Optimization in software testing to generate prioritized optimal path and test data,S. Biswas; M. S. Kaiser; S. A. Mamun,"Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh; Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh; Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh",2015 International Conference on Electrical Engineering and Information Communication Technology (ICEEICT),,2015,,,1,6,"Software testing is one of the most important parts of software development lifecycle. Among various types of software testing approaches structural testing is widely used. Structural testing can be improved largely by traversing all possible code paths of the software. Genetic algorithm is the most used search technique to automate path testing and test case generation. Recently, different novel search based optimization techniques such as Ant Colony Optimization (ACO), Artificial Bee Colony (ABC), Artificial Immune System (AIS), Particle Swarm Optimization (PSO) have been applied to generate optimal path to complete software coverage. In this paper, ant colony optimization (ACO) based algorithm has been proposed which will generate set of optimal paths and prioritize the paths. Additionally, the approach generates test data sequence within the domain to use as inputs of the generated paths. Proposed approach guarantees full software coverage with minimum redundancy. This paper also demonstrates the proposed approach applying it in a program module.",,978-1-4673-6676-2978-1-4673-6675,10.1109/ICEEICT.2015.7307500,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307500,Software testing;Ant Colony Optimization (ACO);Path testing;Test data generation;Control Flow Graph (CFG),Testing,ant colony optimisation;artificial immune systems;program testing;software engineering,ant colony optimization;software testing;prioritized optimal path;test data;software development lifecycle;structural testing;genetic algorithm;path testing;test case generation;novel search based optimization techniques;ACO;artificial bee colony;ABC;artificial immune system;AIS;particle swarm optimization;PSO,,4,28,,,,,,IEEE,IEEE Conferences
Approximating Attack Surfaces with Stack Traces,C. Theisen; K. Herzig; P. Morrison; B. Murphy; L. Williams,NA; NA; NA; NA; NA,2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,,2015,2,,199,208,"Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.",0270-5257;1558-1225,978-1-4799-1934,10.1109/ICSE.2015.148,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202964,stack traces;security;vulnerability;models;testing;reliability;attack surface,Computer crashes;Security;Measurement;Approximation methods;Software;Predictive models;Surface treatment,decision making;program diagnostics;project management;software engineering,attack surface approximation;security testing;effort reviewing;software projects;vulnerable code identification;decision-making;software development;stack trace analysis;attack surface measurement;Windows 8,,8,44,,,,,,IEEE,IEEE Conferences
Architecting to Ensure Requirement Relevance: Keynote TwinPeaks Workshop,J. Bosch,NA,2015 IEEE/ACM 5th International Workshop on the Twin Peaks of Requirements and Architecture,,2015,,,1,2,"Research has shown that up to two thirds of features in software systems are hardly ever used or not even used at all. This represents a colossal waste of R&amp;D resources and occurs across the industry. On the other hand, product management and many others work hard at interacting with customers, building business cases and prioritizing requirements. A fundamentally different approach to deciding what to build is required: requirements should be treated as hypothesis throughout the development process and constant feedback from users and systems in the field should be collected to dynamically reprioritize and change requirements. This requires architectural support beyond the current state of practice as continuous deployment, split testing and data collection need to be an integral part of the architecture. In this paper, we present a brief overview of our research and industry collaboration to address this challenge.",,978-1-4673-7100-1978-1-4673-7099,10.1109/TwinPeaks.2015.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184704,requirements engineering;software architecture;data-driven development,Companies;Software;Computer architecture;Industries;Testing,formal specification;formal verification;software architecture;software development management;systems analysis,requirement relevance;software system;product management;softwasre development process;constant user feedback;architectural support;continuous deployment;split testing;data collection;industry collaboration,,,6,,,,,,IEEE,IEEE Conferences
Challenges and Issues of Mining Crash Reports,L. An; F. Khomh,"SWAT, Polytechnique Montreal, Quebec, Canada; SWAT, Polytechnique Montreal, Quebec, Canada",2015 IEEE 1st International Workshop on Software Analytics (SWAN),,2015,,,5,8,"Automatic crash reporting tools built in many software systems allow software practitioners to understand the origin of field crashes and help them prioritise field crashes or bugs, locate erroneous files, and/or predict bugs and crash occurrences in subsequent versions of the software systems. In this paper, after illustrating the structure of crash reports in Mozilla, we discuss some techniques for mining information from crash reports, and highlight the challenges and issues of these techniques. Our aim is to raise the awareness of the research community about issues that may bias research results obtained from crash reports and provide some guidelines to address certain challenges related to mining crash reports.",,978-1-4673-6923,10.1109/SWAN.2015.7070480,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070480,Crash report;bug report;mining softwarerepositories.,Computer bugs;Data mining;Databases;Algorithm design and analysis;Software systems,data mining;program debugging;program testing,crash report mining;crash reporting tool;software systems,,3,13,,,,,,IEEE,IEEE Conferences
Clustering based novel test case prioritization technique,G. Chaurasia; S. Agarwal; S. S. Gautam,"Institute of Information Technology, India; Department of Information Technology, Indian Institute of Information Technology, India; Software Engineering, Indian Institute of Information Technology, India",2015 IEEE Students Conference on Engineering and Systems (SCES),,2015,,,1,5,"Regression testing is an activity during the maintenance phase to validate the changes made to the software and to ensure that these changes would not affect the previously verified code or functionality. Often, regression testing is performed with limited computing resources and time budget. So in this phase, it is infeasible to run the complete test suite Thus, test-case prioritization approaches are applied to ensure the execution of test cases in some prioritized order and to achieve some specific goals like, increasing the rate of bug detection, identifying the most critical bugs as early as possible etc. In this research work, we are going to propose a new and more effective clustering based prioritization technique that uses various metrics and execution time of test cases to reorder them. The results of implementation will prove that the suggested approach is more productive than the existing coverage and clustering based prioritization techniques.",,978-1-4673-8597-8978-1-4673-8598,10.1109/SCES.2015.7506447,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506447,Clustering;Regression testing;test case prioritization;test suite,Measurement;Complexity theory;Fault detection;Testing;Information technology;History;Clustering algorithms,program debugging;program testing;regression analysis;software maintenance,clustering based test-case prioritization;regression testing;software maintenance;bug detection,,,25,,,,,,IEEE,IEEE Conferences
Communication and collaboration of heterogeneous unmanned systems using the joint architecture for Unmanned Systems (JAUS) standards,M. L. Incze; S. R. Sideleau; C. Gagner; C. A. Pippin,"Naval Undersea Warfare Center Division, Newport, Newport, RI 02842, USA; Naval Undersea Warfare Center Division, Newport, Newport, RI 02842, USA; Naval Undersea Warfare Center Division, Newport, Newport, RI 02842, USA; Georgia Tech Research Institute, Atlanta, GA 30332 USA",OCEANS 2015 - Genova,,2015,,,1,6,"The Naval Undersea Warfare Center Division Newport (NUWCDIVNPT) and Georgia Tech Research Institute (GTRI) completed a successful at-sea exercise with autonomous UAS and UUV systems demonstrating cross-domain unmanned system communication and collaboration. The exercise was held at the NUWC Narragansett Bay Shallow Water Test Facility (NBSWTF) range, and it represented for the first time the use of standard protocols and formats that effectively support cross-domain unmanned system operations. Four man-portable Iver2 UUVs operating in coordinated missions autonomously collected environmental data, which was compressed in-stride, re-formatted, and exfiltrated via UAS relay for display and tactical decision making. Two UAS with autonomous flight take-off and mission execution were sequenced to serve as ISR platforms and to support communications as RF relays for the UUVs performing Intelligence Preparation of the Environment missions. Two Command and Control nodes ashore provided unmanned system tasking and re-tasking, and they served to host and display both geo-positional data and status for UAS and UUV vehicles during the operational scenarios run during the exercise. The SAE Joint Architecture for Unmanned Systems (JAUS) standards were used for all message traffic between shore-based C2 nodes, UAS, and UUVs active in the NBSWTF exercise area. Exercise goals focused on CNO priorities expressed in the Undersea Domain Operating Concept of AUG 2013 which emphasized protocols essential to effective command and control of networked unmanned systems with decentralization and flexibility of command structures. Development for this project highlighted both the strengths and shortfalls of JAUS and captured the requirements for moving forward in effective cross-domain communications that support distributed, agile C2 nodes to meet evolving CONOPS for growing unmanned system presence and mission roles. The scenario employed operating parameters for UAS and UUV that have been established in real-world operations and ongoing unmanned system programs. The tactical information from unmanned systems was displayed in real-time on shore-based C2 displays: the tactical FalconView display and the developmental TOPSIDE command and control station. This work represents a critical step in communications for networking of heterogeneous unmanned systems and establishes a solid platform for alignment of development and ongoing programs. The evaluation of JAUS suitability for near-term operational applications provides significant value as Concepts of Operation that rely on netted heterogeneous systems are being targeted. The focus on affordable commercial unmanned systems for this experimentation establishes the value of highly capable, portable systems to provide economical development and test opportunities with low-cost and low-risk alternatives to many planned and fielded systems. The JAUS architecture was introduced to the NUWC and GTRI unmanned systems though an instantiation of the Mission Oriented Operating Suite (MOOS) autonomy framework on secondary CPUs integrated into the Iver2 UUVs and the GTRI UAS. Since the GTRI UASs already had ROS installed, a MOOS-ROS bridge was employed to support use of the developed JAUS messaging capability. Established JAUS services were employed where the required functions could be met. New JAUS services were developed to meet functionality required for the operational scenarios in this exercise but not yet supported in the existing releases of SAE JAUS. Independent C++ header libraries that could be compiled at run time for specific autonomy frameworks, such as MOOS, were employed to support a software-agnostic approach. Immediate targets for broadening the influence of this work to coalition partners include the NATO Recognized Environmental Picture (REP) 2015 and The Technical Cooperation Program (TTCP) 2015 exercises. This project and demonstration was funded under a NUWC Strategic Initiative and GTRI program support.",,978-1-4799-8736-8978-1-4673-7164,10.1109/OCEANS-Genova.2015.7271613,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271613,autonomous vehicles;command and control systems;communication protocols;environmental characterization;data transmission,Standards;Command and control systems;Protocols;Vehicles;Sea surface;Collaboration;Systems operation,autonomous aerial vehicles;autonomous underwater vehicles;command and control systems;military communication,heterogeneous unmanned system;joint architecture for unmanned system;JAUS standard;naval undersea warfare center division Newport;NUWCDIVNPT;Georgia tech research institute;GTRI program support;autonomous UAS;Narragansett bay shallow water test facility;NBSWTF;cross-domain unmanned system communication;man-portable Iver2 UUV;tactical decision making;autonomous flight take-off;ISR platform;RF relay;geopositional data;UUV vehicle;CNO priority;undersea domain operating concept;CONOPS;tactical information;C2 display;tactical FalconView display;developmental TOPSIDE command and control station;netted heterogeneous system;mission oriented operating suite;MOOS-ROS bridge;SAE JAUS;C++ header library;software-agnostic approach;NATO recognized environmental picture;NATO REP;the technical cooperation program;TTCP,,,,,,,,,IEEE,IEEE Conferences
Component based reliability assessment from UML models,V. Chourey; M. Sharma,"Computer Science and Engineering Department, Medi-Caps Institute, Indore, Madhya Pradesh, India; Computer Engineering Department, IET, DAVV, Indore, Madhya Pradesh, India","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)",,2015,,,772,778,"Model based development and testing techniques have ventured diverse research directions to assure quality of the software product. Models developed during architecture and design phases are efficient tools to assess quality at an early development stage. However, testing the extra-functional or non-functional properties of software systems is not frequently practised eg. reliability. The motivation to our work is to model the context of execution which is significant in system reliability analysis. In this paper we visualize the components of complex software systems and their interactions in the form of Functional Flow Diagram (FFD). This notation specifies the dynamic aspect of system behavior as the context of execution. To further asses reliability, the FFD is translated into Reliability Block Diagram (RBD). The relative importance of the components in terms of reliability is evaluated and is associated with prioritization of the component. The model is simple but significant for system maintenance, improvisation and modification. This model supports analysis and testing through better understanding of the interacting components and their reliabilities.",,978-1-4799-8792-4978-1-4799-8790-0978-1-4799-8791,10.1109/ICACCI.2015.7275704,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275704,,Unified modeling language;Testing;Computational modeling;Software reliability;Estimation;Analytical models,flowcharting;object-oriented programming;program testing;program visualisation;software architecture;software maintenance;software quality;software reliability;Unified Modeling Language,component based reliability assessment;UML models;model based development;testing techniques;software product quality;architecture phases;design phases;quality assessment;extra-functional properties;nonfunctional properties;system reliability analysis;components visualization;complex software systems;functional flow diagram;FFD;system behavior;reliability block diagram;RBD;component prioritization;system maintenance;system improvisation;system modification,,1,26,,,,,,IEEE,IEEE Conferences
Component-Based Software System Test Case Prioritization with Genetic Algorithm Decoding Technique Using Java Platform,S. Mahajan; S. D. Joshi; V. Khanaa,NA; NA; NA,2015 International Conference on Computing Communication Control and Automation,,2015,,,847,851,"Test case prioritization includes testing experiments in a request that builds the viability in accomplishing some execution objectives. The importance amongst the most imperative testing objectives is the fast rate of fault recognition. Test case ought to run in a request that extends the likelihood of fault discovery furthermore that detects the most serious issues at the early stage of testing life cycle. In this paper, we develop and prove the necessity of Component-Based Software testing prioritization framework which plans to uncover more extreme bugs at an early stage and to enhance software product deliverable quality utilizing Genetic Algorithm (GA) with java decoding technique. For this, we propose a set of prioritization keys to plan the proposed Component-Based Software java framework. In our proposed method, we allude to these keys as Prioritization Keys (PK). These keys may be project size, scope of the code, information stream, and bug inclination and impact of fault or bug on overall system, which prioritizes the Component-Based Software framework testing. The integrity of these keys was measured with implementation of key assessment metric called KAM that will likewise be ascertained. This paper demonstrates how software testing can be efficient with management of data integrity factor to avoid major security issues. One of the main advantages of our approach is that domain specific semantics can be integrated with the data quality test cases prioritization, thus being able to discover test feed data quality problems beyond conventional quality measures.",,978-1-4799-6892,10.1109/ICCUBEA.2015.169,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155967,based prioritization;Genetic Algorithm;Java Decoding;Alternative prioritization,Testing;Genetic algorithms;Software;Java;Biological cells;Measurement;Sociology,genetic algorithms;Java;object-oriented programming;program testing,component-based software system test case prioritization;genetic algorithm decoding technique;Java platform;testing objectives;fault discovery;testing life cycle;GA;prioritization keys;PK;software testing;data integrity factor;domain specific semantics,,1,24,,,,,,IEEE,IEEE Conferences
Considerations on application of selective hardening based on software fault tolerance techniques,F. Restrepo-Calle; S. Cuenca-Asensi; A. Mart??nez-Alvarez; F. L. Kastensmidt,"Universidad Nacional de Colombia, Bogota, Colombia; University of Alicante, Alicante, Spain; University of Alicante, Alicante, Spain; Universidade Federal do Rio Grande do Sul (UFRGS), Porto Alegre, Brazil",2015 16th Latin-American Test Symposium (LATS),,2015,,,1,6,"This paper analyses the nature of fault tolerance software-based techniques and the influence of their overheads to determine an efficient strategy for applying those techniques in a selective way. Several considerations that have to be taken into account are presented in this work. These include an analysis of fault coverage and overheads when selective hardening is adopted; side effects of selective protection based on software; and the need of new criticality metrics, apart from those used for hardware-based techniques (e.g., AVF), to facilitate and prioritize the selection of resources to be protected.",2373-0862,978-1-4673-6710,10.1109/LATW.2015.7102509,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102509,reliability;software fault-tolerance;selective,Registers;Software;Circuit faults;Hardware;Microprocessors;Reliability;Measurement,electronic engineering computing;radiation hardening (electronics);software fault tolerance,selective hardening;software fault tolerance;fault tolerance software-based techniques;fault coverage analysis;hardware-based techniques,,2,28,,,,,,IEEE,IEEE Conferences
Decision support system prototype on obstetrics ultrasonography for primary service physicians,B. S. Sabarguna; S. K. Wijaya; F. Sakinah; A. Maryati,"Biomedical Engineering Study Program, Universitas Indonesia; Biomedical Engineering Study Program, Universitas Indonesia; Biomedical Engineering Study Program, Universitas Indonesia; Biomedical Engineering Study Program, Universitas Indonesia","2015 4th International Conference on Instrumentation, Communications, Information Technology, and Biomedical Engineering (ICICI-BME)",,2015,,,226,231,"Introduction. The National Social Security System (SJSN) prioritizes primary service as a spearhead to assist Primary Care Physicians to make medical decisions. The purpose of this research is to develop computer software that will assist primary care physicians in the fields of Obstetrics Ultrasonography, related to referral decision-making abilities. Methods. A quasi-experimental post-test only design without a control group. The stages of the research process: Systems Analysis and Design, Prototyping and Testing by Lecture, Students, Programmers and Doctors. Results. From Analysis and Systems design document, has been produced prototype of software, and a test run has been proven successful Decision Support System software helping doctors develop diagnosis and specialty referrals. Conclusion: The Decision Support System software can be used in Obstetrics Ultrasonography by Primary Care Physicians to provide aid in their diagnosis and referrals. Before it is used, it is recommended for trainings and application tests.",,978-1-4673-7800-0978-1-4673-7799,10.1109/ICICI-BME.2015.7401367,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401367,,Software;Medical services;Decision support systems;Prototypes;Ultrasonography;System analysis and design,biomedical ultrasonics;decision support systems;medical image processing;obstetrics;software prototyping;ultrasonic imaging,decision support system prototype;obstetrics ultrasonography;primary service physicians;primary care physicians;computer software;decision-making;quasiexperimental post-testing;system analysis;decision support system software,,,26,,,,,,IEEE,IEEE Conferences
Detecting Display Energy Hotspots in Android Apps,M. Wan; Y. Jin; D. Li; W. G. J. Halfond,NA; NA; NA; NA,"2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)",,2015,,,1,10,"Energy consumption of mobile apps has become an important consideration as the underlying devices are constrained by battery capacity. Display represents a significant portion of an app's energy consumption. However, developers lack techniques to identify the user interfaces in their apps for which energy needs to be improved. In this paper, we present a technique for detecting display energy hotspots - user interfaces of a mobile app whose energy consumption is greater than optimal. Our technique leverages display power modeling and automated display transformation techniques to detect these hotspots and prioritize them for developers. In an evaluation on a set of popular Android apps, our technique was very accurate in both predicting energy consumption and ranking the display energy hotspots. Our approach was also able to detect display energy hotspots in 398 Android market apps, showing its effectiveness and the pervasiveness of the problem. These results indicate that our approach represents a potentially useful technique for helping developers to detect energy related problems and reduce the energy consumption of their mobile apps.",2159-4848,978-1-4799-7125,10.1109/ICST.2015.7102585,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102585,,Color;Energy consumption;User interfaces;Power demand;Mobile communication;Smart phones;Monitoring,Android (operating system);mobile computing;power aware computing;user interfaces,display energy hotspots detection;user interfaces;mobile app;energy consumption;display power modeling;automated display transformation techniques;Android market apps,,14,51,,,,,,IEEE,IEEE Conferences
Detecting hardware Trojans in unspecified functionality using mutation testing,N. Fern; K. Cheng,"University of California, Santa Barbara, ECE Department, USA; University of California, Santa Barbara, ECE Department, USA",2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),,2015,,,560,566,"Existing functional Trojan detection methodologies assume Trojans violate the design specification under carefully crafted rare triggering conditions. We present a new type of Trojan that leaks secret information from the design by only modifying unspecified functionality, meaning the Trojan is no longer restricted to being active only under rare conditions. We provide a method based on mutation testing for detecting this new Trojan type along with mutant ranking heuristics to prioritize analysis of the most dangerous functionality. Applying our method to a UART controller design, we discover unspecified and untested bus functionality with the potential to leak 32 bits of information during hundreds of cycles without being detected! Our method also reveals poorly tested interrupt functionality with information leakage potential. After modifying the specification and test bench to remove the discovered vulnerabilities, we close the verification loop by re-analyzing the design using our methodology and observe the functionality is no longer flagged as dangerous.",,978-1-4673-8388-2978-1-4673-8389,10.1109/ICCAD.2015.7372619,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372619,,Trojan horses;Testing;Hardware;Logic gates;Payloads;Logic functions;Optimization,computer interfaces;invasive software,hardware Trojans;mutation testing;functional Trojan detection methodologies;rare triggering conditions;unspecified functionality;mutant ranking heuristics;UART controller design;bus functionality;information leakage potential;verification loop,,6,19,,,,,,IEEE,IEEE Conferences
Effective test strategy for testing automotive software,S. S. Barhate,"Head Test Laboratory, Hella India Automotive Pvt. Ltd., Pune, India",2015 International Conference on Industrial Instrumentation and Control (ICIC),,2015,,,645,649,"Electronic content is increasing in automobiles day by day. Functionalities like Air Bags, Anti-lock Braking, Driver Assistance Systems, Body Controllers, Passive entry Passive Start, Electronic Power Steering etc. are realized electronically with complex software. These functionalities are related to automobile system safety. Hence, safety is one of the key issues of future automobile development. Risk of system failure is high due to increasing technological complexity and software content. The software shall be tested well to arrest almost all the defects. This paper explains a test case development and execution strategy based on practical implementation. It explains how test case reduction using Taguchi method, prioritization of test execution and automation help to make testing effective. It also demonstrates how maximum defects are discovered in short time.",,978-1-4799-7165-7978-1-4799-7164,10.1109/IIC.2015.7150821,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150821,Test Strategy;Test Prioritization;Taguchi Method;Test Automation,Software;Heating;Testing;Arrays;Automation;Reliability theory,automotive electronics;electronic engineering computing;program testing;road safety;Taguchi methods,automotive software testing;electronic content;automobile system safety;automobile development;system failure risk;test case development;test execution strategy;test case reduction;Taguchi method;test execution prioritization,,1,25,,,,,,IEEE,IEEE Conferences
Effective verification of low-level software with nested interrupts,D. Kroening; L. Liang; T. Melham; P. Schrammel; M. Tautschnig,"University of Oxford, UK; University of Oxford, UK; University of Oxford, UK; University of Oxford, UK; Queen Mary, University of London, UK","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)",,2015,,,229,234,"Interrupt-driven software is difficult to test and debug, especially when interrupts can be nested and subject to priorities. Interrupts can arrive at arbitrary times, leading to an explosion in the number of cases to be considered. We present a new formal approach to verifying interrupt-driven software based on symbolic execution. The approach leverages recent advances in the encoding of the execution traces of interacting, concurrent threads. We assess the performance of our method on benchmarks drawn from embedded systems code and device drivers, and experimentally compare it to conventional formal approaches that use source-to-source transformations. Our experimental results show that our method significantly outperforms conventional techniques. To the best of our knowledge, our technique is the first to demonstrate effective formal verification of low-level embedded software with nested interrupts.",1530-1591;1558-1101,978-3-9815-3705-5978-3-9815-3704,10.7873/DATE.2015.0360,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092387,,Encoding;Benchmark testing;Instruments;Radio frequency;Instruction sets;Semantics,embedded systems;formal verification;interrupts;symbol manipulation,effective low-level software verification;nested interrupts;formal approach;interrupt-driven software;symbolic execution;embedded systems code;device drivers;source-to-source transformations;formal verification;low-level embedded software,,2,15,,,,,,IEEE,IEEE Conferences
Feasibility Analysis of Engine Control Tasks under EDF Scheduling,A. Biondi; G. Buttazzo; S. Simoncelli,NA; NA; NA,2015 27th Euromicro Conference on Real-Time Systems,,2015,,,139,148,"Engine control applications include software tasks that are triggered at predetermined angular values of the crankshaft, thus generating a computational workload that varies with the engine speed. To avoid overloads at high rotation speeds, these tasks are implemented to self adapt and reduce their computational demand by switching mode at given rotation speeds. For this reason, they are referred to as adaptive variable rate (AVR) tasks. Although a few works have been proposed in the literature to model and analyze the schedulability of such a peculiar type of tasks, an exact analysis of engine control applications has been derived only for fixed priority systems, under a set of simplifying assumptions. The major problem of scheduling AVR tasks with fixed priorities, however, is that, due to engine accelerations, the interarrival period of an AVR task is subject to large variations, therefore there will be several speeds at which any fixed priority assignment is far from being optimal, significantly penalizing the schedulability of the system. This paper proposes for the first time an exact feasibility test under the Earliest Deadline First scheduling algorithm for tasks sets including regular periodic tasks and AVR tasks triggered by a common rotation source. In addition, a set of simulation results are reported to evaluate the schedulability gain achieved in this context by EDF over fixed priority scheduling.",2377-5998;1068-3070,978-1-4673-7570,10.1109/ECRTS.2015.20,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176033,,Engines;Acceleration;Scheduling;Search problems;Real-time systems;Processor scheduling;Analytical models,control engineering computing;engines;scheduling;shafts,engine control task feasibility analysis;EDF scheduling;software tasks;crankshaft;adaptive variable rate tasks;AVR task scheduling;engine accelerations;earliest deadline first scheduling algorithm;fixed priority scheduling,,10,20,,,,,,IEEE,IEEE Conferences
Fusion of LIDAR and video cameras to augment medical training and assessment,B. R. VanVoorst; M. Hackett; C. Strayhorn; J. Norfleet; E. Honold; N. Walczak; J. Schewe,"Raytheon BBN Technologies Corp., St. Louis Park, MN 55416 USA; Army Research Laboratory/HRED-STTC, Orlando, FL 32826 USA; Information Visualization and Innovative Research (IVIR) Inc., Sarasota, FL 34240, USA; Army Research Laboratory/HRED-STTC, USA; Information Visualization and Innovative Research (IVIR) Inc., USA; Raytheon BBN Technologies Corp., USA; Raytheon BBN Technologies Corp., USA",2015 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI),,2015,,,345,350,"The Mobile Medical Lane Trainer (MMLT) is a multi-sensor rapidly deployed After-Action Review (AAR) system for Army medical lane training. Current AAR systems have two main drawbacks: 1) video does not provide a complete view of the medical and tactical situation, and 2) the video is not readily available for effective evaluation. The MMLT program is developing a ƒ??smarterƒ? AAR system by using 3D LIDAR (LIght Detection And Ranging), a camera array, People Tracking software and Medical Training Evaluation and Review (MeTER) software. This system can be brought to the field and deployed in less than an hour to provide hands-off data collection for the exercise. MMLT supplements existing evaluation systems deployed at the Medical Simulation Training Centers (MSTCs) by providing a 3-D perspective of the training event for tactical evaluation with synchronized video technology to capture both tactical and clinical skills and instructor scoring. This capability is used in conjunction with the MeTER system's skill assessment checklists for automated performance review. An immediate synchronized playback capability has been developed, ultimately resulting in a rapid AAR for debriefing. This paper will discuss the technical components of the system, including hardware components, data fusion technique, tracking algorithms, and camera prioritization approaches, and will conclude with operational test results and lessons learned.",,978-1-4799-7772-7978-1-5090-0307,10.1109/MFI.2015.7295832,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7295832,,Cameras;Laser radar;Training;Target tracking;Three-dimensional displays;Software;Sensors,biomedical education;computer software;medical computing;sensor fusion;video cameras,3D LIDAR fusion;video camera array;augment medical training and assessment;mobile medical lane trainer;multisensor rapidly deployed after-action review system;army medical lane training;medical-tactical situation;MMLT program;people tracking software;medical training evaluation and review software;hands-off data collection;exercise;tactical evaluation;synchronized video technology;tactical skills;clinical skills;skill assessment checklists;automated performance review;immediate synchronized playback capability;technical components;hardware components;data fusion technique;operational testing,,,16,,,,,,IEEE,IEEE Conferences
GUI Test Case Prioritization by State-Coverage Criterion,Z. He; C. Bai,NA; NA,2015 IEEE/ACM 10th International Workshop on Automation of Software Test,,2015,,,18,22,"Graphical User Interface (GUI) application is a kind of typical event-driven software (EDS) that transforms state according to input events invoked through a user interface. It is time consuming to test a GUI application since there are a large number of possible event sequences generated by the permutations and combinations of user operations. Although some GUI test case prioritization techniques have been proposed to determine ""which test case to execute next"" for early fault detection, most of them use random ordering to break tie cases, which has been proved to be ineffective. Recent research presents the opinion that using hybrid criteria can be an effective way for tie-breaking, but few studies focus on seeking a new criterion cooperating well with other criteria when breaking tie cases. In this paper, we propose a state-distance-based method using state coverage as a new criterion to prioritize GUI test cases. An empirical study on three GUI programs reveals that the state-distance-based method is really suitable for GUI test case prioritization and can cooperate well with the (additional) event length criterion.",,978-1-4673-7022,10.1109/AST.2015.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166260,GUI testing;event-driven software;test case prioritization;GUI state similarity,Graphical user interfaces;Testing;Software;Fault detection;Software engineering;Measurement;Conferences,graphical user interfaces,GUI test case prioritization;state coverage criterion;graphical user interface;event-driven software;EDS;GUI application;fault detection;state distance based method;GUI programs,,2,11,,,,,,IEEE,IEEE Conferences
History-Based Test Case Prioritization for Black Box Testing Using Ant Colony Optimization,T. Noguchi; H. Washizaki; Y. Fukazawa; A. Sato; K. Ota,NA; NA; NA; NA; NA,"2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)",,2015,,,1,2,"Test case prioritization is a technique to improve software testing. Although a lot of work has investigated test case prioritization, they focus on white box testing or regression testing. However, software testing is often outsourced to a software testing company, in which testers are rarely able to access to source code due to a contract. Herein a framework is proposed to prioritize test cases for black box testing on a new product using the test execution history collected from a similar prior product and the Ant Colony Optimization. A simulation using two actual products shows the effectiveness and practicality of our proposed framework.",2159-4848,978-1-4799-7125,10.1109/ICST.2015.7102622,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102622,,Software testing;Software;Ant colony optimization;Companies;History;Fault detection,ant colony optimisation;program testing;regression analysis;source code (software);statistical testing,history-based test case prioritization;black box testing;ant colony optimization;white box testing;regression testing;software testing company;source code;test execution history,,1,6,,,,,,IEEE,IEEE Conferences
Improving Multi-Objective Test Case Selection by Injecting Diversity in Genetic Algorithms,A. Panichella; R. Oliveto; M. D. Penta; A. De Lucia,"Department of Mathematics and Computer Science, University of Salerno, Fisciano, Salerno, Italy; Department of Bioscience and Territory, University of Molise, Pesche, Isernia, Italy; Department of Engineering, University of Sannio, Benevento, Italy; Department of Mathematics and Computer Science, University of Salerno, Fisciano, Salerno, Italy",IEEE Transactions on Software Engineering,,2015,41,4,358,383,"A way to reduce the cost of regression testing consists of selecting or prioritizing subsets of test cases from a test suite according to some criteria. Besides greedy algorithms, cost cognizant additional greedy algorithms, multi-objective optimization algorithms, and multi-objective genetic algorithms (MOGAs), have also been proposed to tackle this problem. However, previous studies have shown that there is no clear winner between greedy and MOGAs, and that their combination does not necessarily produce better results. In this paper we show that the optimality of MOGAs can be significantly improved by diversifying the solutions (sub-sets of the test suite) generated during the search process. Specifically, we introduce a new MOGA, coined as DIversity based Genetic Algorithm (DIV-GA), based on the mechanisms of orthogonal design and orthogonal evolution that increase diversity by injecting new orthogonal individuals during the search process. Results of an empirical study conducted on eleven programs show that DIV-GA outperforms both greedy algorithms and the traditional MOGAs from the optimality point of view. Moreover, the solutions (sub-sets of the test suite) provided by DIV-GA are able to detect more faults than the other algorithms, while keeping the same test execution cost.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2014.2364175,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936894,Test Case Selection;Regression Testing;Orthogonal Design;Singular Value Decomposition;Genetic Algorithms;Empirical Studies;Test case selection;regression testing;orthogonal design;singular value decomposition;genetic algorithms;empirical studies,Optimization;Greedy algorithms;Testing;Linear programming;Genetic algorithms;Genetics;Sociology,genetic algorithms;greedy algorithms;program testing;search problems,multiobjective test case selection improvement;regression testing cost reduction;test case subset prioritization;test case subset selection;greedy algorithms;multiobjective optimization algorithms;multiobjective genetic algorithms;MOGA optimality improvement;test suite subsets;search process;diversity-based genetic algorithm;DIV-GA;orthogonal design mechanism;orthogonal evolution mechanism;empirical analysis,,21,76,,,,,,IEEE,IEEE Journals & Magazines
Improving prioritization of software weaknesses using security models with AVUS,S. Renatus; C. Bartelheimer; J. Eichler,"Fraunhofer Institute for Applied and Integrated Security AISEC, Germany; Fraunhofer Institute for Applied and Integrated Security AISEC, Germany; Fraunhofer Institute for Applied and Integrated Security AISEC, Germany",2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation (SCAM),,2015,,,259,264,"Testing tools for application security have become an integral part of secure development life-cycles. Despite their ability to spot important software weaknesses, the high number of findings require rigorous prioritization. Most testing tools provide generic ratings to support prioritization. Unfortunately, ratings from established tools lack context information especially with regard to the security requirements of respective components or source code. Thus experts often spend a great deal of time re-assessing the prioritization provided by these tools. This paper introduces our lightweight tool AVUS that adjusts context-free ratings of software weaknesses according to a user-defined security model. We also present a first evaluation applying AVUS to a well-known open source project and the findings of a popular, commercially available application security testing tool.",,978-1-4673-7529-0978-1-4673-7528,10.1109/SCAM.2015.7335423,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335423,Secure software development;vulnerability scoring;contextual enrichment;security metrics,Security;Context;Kernel;Testing;Measurement;Complexity theory,program testing;public domain software;safety-critical software;source code (software),software weakness;security model;AVUS;software testing tool;source code;context-free rating;user-defined security model;open source project;application security testing tool,,,21,,,,,,IEEE,IEEE Conferences
Improving reliability using software operational profile and testing profile,M. M. Ali-Shahid; S. Sulaiman,"Faculty of Computing, Universiti Teknologi Malaysia, 81310 UTM, Skudai, Johor, Malaysia; Faculty of Computing, Universiti Teknologi Malaysia, 81310 UTM, Skudai, Johor, Malaysia","2015 International Conference on Computer, Communications, and Control Technology (I4CT)",,2015,,,384,388,"Software testing has ever remained a challenge particularly when testing is done with intention in enhancing the reliability. Conventional testing is increasing the testing in an unpredictable way by reducing the number of faults. There is a need to enhance the reliability by assigning probabilistic priorities to testing mechanism, which is done through software operational profile. This study adopts a case study to generate test cases and test suites with perspective of probabilistic reliability using the proposed framework based on software operational profile and testing profile.",,978-1-4799-7952-3978-1-4799-7951,10.1109/I4CT.2015.7219603,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219603,Software Operational Profile (SOP);Software Reliability Engineering (SRE);Testing Profile (TP);Software Testing,Testing;Software reliability;Probability;Optical character recognition software;Conferences,program testing;software reliability,software operational profile;testing profile;software testing;test case generation;test suites;probabilistic reliability,,1,22,,,,,,IEEE,IEEE Conferences
Introducing Continuous Delivery of Mobile Apps in a Corporate Environment: A Case Study,S. Klepper; S. Krusche; S. Peters; B. Bruegge; L. Alperowitz,NA; NA; NA; NA; NA,2015 IEEE/ACM 2nd International Workshop on Rapid Continuous Software Engineering,,2015,,,5,11,"Software development is conducted in increasingly dynamic business environments. Organizations need the capability to develop, release and learn from software in rapid parallel cycles. The abilities to continuously deliver software, to involve users, and to collect and prioritize their feedback are necessary for software evolution. In 2014, we introduced Rugby, an agile process model with workflows for continuous delivery and feedback management, and evaluated it in university projects together with industrial clients. Based on Rugby's release management workflow we identified the specific needs for project-based organizations developing mobile applications. Varying characteristics and restrictions in projects teams in corporate environments impact both process and infrastructure. We found that applicability and acceptance of continuous delivery in industry depend on its adaptability. To address issues in industrial projects with respect to delivery process, infrastructure, neglected testing and continuity, we extended Rugby's workflow and made it tailor able. Eight projects at Capgemini, a global provider of consulting, technology and outsourcing services, applied a tailored version of the workflow. The evaluation of these projects shows anecdotal evidence that the application of the workflow significantly reduces the time required to build and deliver mobile applications in industrial projects, while at the same time increasing the number of builds and internal deliveries for feedback.",,978-1-4673-7067,10.1109/RCoSE.2015.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167166,Release Management;Configuration Management;Continuous Integration;Continuous Delivery;User Feedback;User Involvement;Agile Methods;Software Evolution,Software;Mobile communication;Servers;Organizations;Testing;Measurement,business data processing;mobile computing;organisational aspects;software prototyping;workflow management software,mobile application continuous delivery process;corporate environment;software development;dynamic business environments;rapid parallel cycles;Rugby agile process model;software evolution;feedback management;Rugby release management workflow;project-based organizations;outsourcing services,,1,31,,,,,,IEEE,IEEE Conferences
Investigating the Correspondence between Mutations and Static Warnings,C. A. D. Ara?§jo; M. E. Delamaro; J. C. Maldonado; A. M. R. Vincenzi,NA; NA; NA; NA,2015 29th Brazilian Symposium on Software Engineering,,2015,,,1,10,"This paper provides evidences on the correspondence between mutations and static warnings. We used mutation operators as a fault model to evaluate the direct correspondence between mutations and static warnings. The main advantage of using mutation operators is that they generate a large number of programs containing faults of different types, which can be used to decide the ones most probable to be detected by static analyzers. Since static analyzers, in general, report a substantial number of false positive warnings, the intention of this study is to define a prioritization approach of static warnings based on the probability they correspond to a true positive and lead to detect software faults. The results obtained for a set of open-source programs indicate that a correspondence exist when considering specific mutation operators such that static warnings may be prioritized based on their correspondence level with mutations.",,978-1-4673-9272,10.1109/SBES.2015.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328004,Software testing;mutants;warnings;static analysis;static analyzer evaluation,Java;Software;XML;Testing;Standards;Databases;Data mining,program diagnostics;public domain software;software fault tolerance,static warnings;static analyzers;false positive warnings;prioritization approach;open-source programs;software faults;specific mutation operators;correspondence level,,,29,,,,,,IEEE,IEEE Conferences
Kvazaar HEVC encoder for efficient intra coding,M. Viitanen; A. Koivula; A. Lemmetti; J. Vanne; T. D. H??m??l??inen,"Department of Pervasive Computing, Tampere University of Technology, Finland; Department of Pervasive Computing, Tampere University of Technology, Finland; Department of Pervasive Computing, Tampere University of Technology, Finland; Department of Pervasive Computing, Tampere University of Technology, Finland; Department of Pervasive Computing, Tampere University of Technology, Finland",2015 IEEE International Symposium on Circuits and Systems (ISCAS),,2015,,,1662,1665,"This paper presents an open-source Kvazaar encoder for HEVC intra coding. This academic software encoder has been developed from the scratch using C as an implementation language by prioritizing modularity, portability, and readability of the source code. Kvazaar implements almost the same intra coding functionality as HEVC reference encoder (HM) but its rewritten source code makes it significantly faster. In all-intra (AI) coding, a single-threaded C implementation of Kvazaar is 2.3 times faster than HM at a cost of 1.7% bit rate increase. The respective values with a high speed preset of Kvazaar are 10.6 and 8.8%. Compared to a single-threaded C++ implementation of x265, Kvazaar improves rate-distortion performance and increases encoding speed in both high-quality and high-speed test cases. Kvazaar has a particular edge in the high-speed test case where it almost halves the BD-rate loss and more than doubles the performance.",0271-4302;2158-1525,978-1-4799-8391,10.1109/ISCAS.2015.7168970,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168970,HEVC;Kvazaar HEVC encoder;intra coding;open-source implementation;rate-distortion-complexity,Encoding;Video coding;Open source software;Transforms;Standards;IP networks;Artificial intelligence,computational complexity;rate distortion theory;source code (software);video coding,Kvazaar HEVC encoder;open-source Kvazaar encoder;HEVC intra coding;single-threaded C implementation;rate-distortion-complexity;high efficiency video coding,,18,18,,,,,,IEEE,IEEE Conferences
Lexical Parsing Expression Recognition Schemata,M. Lumpe,NA,2015 24th Australasian Software Engineering Conference,,2015,,,165,174,"Parsing expression grammars (PEGs) have emerged as a promising substitute for context-free grammars (CFGs) and regular expressions (REs) in programming language specification. The benefits of PEGs are twofold. First, parsing expression grammars replace unordered choice between alternatives by prioritized choice, which naturally solves the ubiquitous ""dangling else"" problem in grammar definitions. Second, PEGs employ ""character-level syntax"" specifications that eliminate the need to separate the lexical and hierarchical components of a language specification. However, there is ""no free lunch"" in PEGs. PEGs capture only syntactic relationships, but many language constructs cannot be parsed without additional semantic information. Moreover, character-level specifications can become unwieldy, as every aspect of the language, including spacing, has to be accounted for. To overcome these issues, we extend the original PEG formalism to incorporate semantic predicates that yield a programmatic means for state-based token recognition control. Furthermore, rather than requiring a single complete specification, we capture lexical components as PEG closures that provide a self-contained token recognition mechanism to reduce the clutter associated with purely character-level PEGs. To test the effectiveness of our approach, we use it for the construction of a Delphi language front-end and practically confirm that Ford's theoretical linear-time result also holds for PEG closures.",1530-0803,978-1-4673-9390,10.1109/ASWEC.2015.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365805,parsing expression grammars;semantic predicates;language composition;language processing,Grammar;Syntactics;Semantics;Java;Character recognition;Automata,grammars;programming languages;specification languages,lexical parsing expression recognition schemata;parsing expression grammars;programming language specification;semantic predicates;state-based token recognition control;lexical components;PEG closures;self-contained token recognition mechanism;Delphi language front-end,,,28,,,,,,IEEE,IEEE Conferences
METEOR: An Enterprise Health Informatics Environment to Support Evidence-Based Medicine,M. Puppala; T. He; S. Chen; R. Ogunti; X. Yu; F. Li; R. Jackson; S. T. C. Wong,"Houston Methodist Hospital Research Institute; Houston Methodist Hospital Research Institute; Houston Methodist Hospital Research Institute; Houston Methodist Hospital Research Institute; Houston Methodist Hospital Research Institute; Houston Methodist Hospital Research Institute; Houston Methodist Hospital; Department of Systems Medicine and Bioengineering, Houston Methodist Hospital Research Institute, Houston, TX, USA",IEEE Transactions on Biomedical Engineering,,2015,62,12,2776,2786,"Goal: The aim of this paper is to propose the design and implementation of next-generation enterprise analytics platform developed at the Houston Methodist Hospital (HMH) system to meet the market and regulatory needs of the healthcare industry. Methods: For this goal, we developed an integrated clinical informatics environment, i.e., Methodist environment for translational enhancement and outcomes research (METEOR). The framework of METEOR consists of two components: the enterprise data warehouse (EDW) and a software intelligence and analytics (SIA) layer for enabling a wide range of clinical decision support systems that can be used directly by outcomes researchers and clinical investigators to facilitate data access for the purposes of hypothesis testing, cohort identification, data mining, risk prediction, and clinical research training. Results: Data and usability analysis were performed on METEOR components as a preliminary evaluation, which successfully demonstrated that METEOR addresses significant niches in the clinical informatics area, and provides a powerful means for data integration and efficient access in supporting clinical and translational research. Conclusion: METEOR EDW and informatics applications improved outcomes, enabled coordinated care, and support health analytics and clinical research at HMH. Significance: The twin pressures of cost containment in the healthcare market and new federal regulations and policies have led to the prioritization of the meaningful use of electronic health records in the United States. EDW and SIA layers on top of EDW are becoming an essential strategic tool to healthcare institutions and integrated delivery networks in order to support evidence-based medicine at the enterprise level.",0018-9294;1558-2531,,10.1109/TBME.2015.2450181,John S Dunn Research Foundation; TT and WF Chao Center for BRAIN; Houston Methodist Research Institute; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7137654,Clinical Data Warehouse;Smartphone Health App;Cohort Identification;Natural Language Processing;Readmission Risk;Outcomes Research;Clinical data warehouse;cohort identification;natural language processing (NLP);outcomes research;readmission risk;smartphone health app,Hospitals;Data mining;Natural language processing;Databases;Informatics;Data warehouses,data analysis;data integration;data mining;data warehouses;decision support systems;electronic health records;health care;information retrieval,health analytics;cost containment;healthcare market;electronic health records;SIA layers;healthcare institutions;integrated delivery networks;enterprise level;informatics applications;METEOR EDW;data integration;clinical informatics area;preliminary evaluation;usability analysis;data analysis;clinical research training;risk prediction;data mining;cohort identification;hypothesis testing;data access;clinical decision support systems;software intelligence and analytics layer;enterprise data warehouse;methodist environment for translational enhancement and outcomes research;integrated clinical informatics environment;healthcare industry;HMH;Houston Methodist Hospital system;enterprise analytics platform;evidence-based medicine;enterprise health informatics environment,"Adult;Aged;Data Mining;Database Management Systems;Decision Support Systems, Clinical;Evidence-Based Medicine;Female;Humans;Male;Middle Aged;Mobile Applications;Natural Language Processing;Risk Assessment;Translational Medical Research",17,27,,,,,,IEEE,IEEE Journals & Magazines
Modification Impact Analysis Based Test Case Prioritization for Regression Testing of Service-Oriented Workflow Applications,H. Wang; J. Xing; Q. Yang; D. Han; X. Zhang,NA; NA; NA; NA; NA,2015 IEEE 39th Annual Computer Software and Applications Conference,,2015,2,,288,297,"Test case prioritization for regression testing is an approach that schedules test cases to improve the efficiency of service-oriented workflow application testing. Most of existing prioritization approaches range test cases according to various metrics (e.g., Statement coverage, path coverage) in different application context. Service-oriented workflow applications orchestrate web services to provide value-added service and typically are long-running and time-consuming processes. Therefore, these applications need more precise prioritization to execute earlier those test cases that may detect failures. Surprisingly, most of current regression test case prioritization researches neglect to use internal structure information of software, which is a significant factor influencing the prioritization of test cases. Considering the internal structure information and fault propagation behavior of modifications respect to modified version for service-oriented workflow applications, we present in this paper a new regression test case prioritization approach. Our prioritization approach schedules test cases based on dependence analysis of internal activities in service-oriented workflow applications. Experimental results show that test case prioritization using our approach is more effective than conventional coverage-based techniques.",0730-3157,978-1-4673-6564-2978-1-4673-6563,10.1109/COMPSAC.2015.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273631,test case prioritization;dependence analysis;service-oriented workflow applications;modification impact,Testing;Correlation;Software;Synchronization;Schedules;Fault detection;Programmable logic arrays,program control structures;program testing;regression analysis;service-oriented architecture;Web services;workflow management software,modification impact analysis;service-oriented workflow applications;test case scheduling;service-oriented workflow application testing;statement coverage;path coverage;Web service orchestration;value-added service;failure detection;software internal structure information;fault propagation behavior;regression test case prioritization approach;dependence analysis,,1,37,,,,,,IEEE,IEEE Conferences
Multi-perspective Regression Test Prioritization for Time-Constrained Environments,D. Marijan,NA,"2015 IEEE International Conference on Software Quality, Reliability and Security",,2015,,,157,162,"Test case prioritization techniques are widely used to enable reaching certain performance goals during regression testing faster. A commonly used goal is high fault detection rate, where test cases are ordered in a way that enables detecting faults faster. However, for optimal regression testing, there is a need to take into account multiple performance indicators, as considered by different project stakeholders. In this paper, we introduce a new optimal multi-perspective approach for regression test case prioritization. The approach is designed to optimize regression testing for faster fault detection integrating three different perspectives: business perspective, performance perspective, and technical perspective. The approach has been validated in regression testing of industrial mobile device systems developed in continuous integration. The results show that our proposed framework efficiently prioritizes test cases for faster and more efficient regression fault detection, maximizing the number of executed test cases with high failure frequency, high failure impact, and cross-functional coverage, compared to manual practice.",,978-1-4673-7989-2978-1-4673-7988,10.1109/QRS.2015.31,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272927,software testing;regression testing;test case prioritization,Testing;Fault detection;Manuals;Software;Business;Time factors;Time-frequency analysis,fault diagnosis;program testing;regression analysis,multiperspective regression test prioritization;time-constrained environment;test case prioritization technique;fault detection rate;optimal regression testing;performance indicator;optimal multiperspective approach;regression test case prioritization;business perspective;performance perspective;technical perspective;industrial mobile device system;continuous integration;regression fault detection;failure frequency;failure impact;cross-functional coverage,,3,18,,,,,,IEEE,IEEE Conferences
Mutation-based test-case prioritization in software evolution,Y. Lou; D. Hao; L. Zhang,"Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China, Institute of Software, School of EECS, Peking University, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China, Institute of Software, School of EECS, Peking University, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China, Institute of Software, School of EECS, Peking University, China",2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE),,2015,,,46,57,"During software evolution, to assure the software quality, test cases for an early version tend to be reused by its latter versions. As a large number of test cases may aggregate during software evolution, it becomes necessary to schedule the execution order of test cases so that the faults in the latter version may be detected as early as possible, which is test-case prioritization in software evolution. In this paper, we proposed a novel test-case prioritization approach for software evolution, which first uses mutation faults on the difference between the early version and the latter version to simulate real faults occurred in software evolution, and then schedules the execution order of test cases based on their fault-detection capability, which is defined based on mutation faults. In particular, we present two models on calculating fault-detection capability, which are statistics-based model and probability-based model. Moreover, we conducted an experimental study and found that our approach with the statistics-based model outperforms our approach with the probability-based model and the total statement coverage-based approach, and slightly outperforms the additional statement-coverage based approach in many cases. Furthermore, compared with the total or additional statement coverage-based approach, our approach with either the statistics-based model or the probability-based model tends to be stably effective when the difference on the source code between the early version and the latter version is non-trivial.",,978-1-5090-0406-5978-1-5090-0405,10.1109/ISSRE.2015.7381798,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381798,,Testing;Schedules;Software quality;Java;Electronic mail;Aggregates,fault diagnosis;probability;program testing;software quality;source code (software),mutation-based test-case prioritization;software evolution;mutation faults;fault-detection capability;statistics-based model;probability-based model;total statement coverage-based approach;source code;software quality,,4,48,,,,,,IEEE,IEEE Conferences
PORA: Proportion-Oriented Randomized Algorithm for Test Case Prioritization,B. Jiang; W. K. Chan; T. H. Tse,NA; NA; NA,"2015 IEEE International Conference on Software Quality, Reliability and Security",,2015,,,131,140,"Effective testing is essential for assuring software quality. While regression testing is time-consuming, the fault detection capability may be compromised if some test cases are discarded. Test case prioritization is a viable solution. To the best of our knowledge, the most effective test case prioritization approach is still the additional greedy algorithm, and existing search-based algorithms have been shown to be visually less effective than the former algorithms in previous empirical studies. This paper proposes a novel Proportion-Oriented Randomized Algorithm (PORA) for test case prioritization. PORA guides test case prioritization by optimizing the distance between the prioritized test suite and a hierarchy of distributions of test input data. Our experiment shows that PORA test case prioritization techniques are as effective as, if not more effective than, the total greedy, additional greedy, and ART techniques, which use code coverage information. Moreover, the experiment shows that PORA techniques are more stable in effectiveness than the others.",,978-1-4673-7989-2978-1-4673-7988,10.1109/QRS.2015.28,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272924,Test case prioritization;randomized algorithm;proportional sampling strategy;multi-objective optimization,Testing;Subspace constraints;Fault detection;Greedy algorithms;Resource management;Clustering algorithms;Genetic algorithms,greedy algorithms;program testing;regression analysis;software fault tolerance;software quality,proportion-oriented randomized algorithm;test case prioritization;software quality;regression testing;fault detection capability;greedy algorithm;search-based algorithms,,1,36,,,,,,IEEE,IEEE Conferences
Practical Combinatorial Interaction Testing: Empirical Findings on Efficiency and Early Fault Detection,J. Petke; M. B. Cohen; M. Harman; S. Yoo,"Computer Science Department, University College London, London, United Kingdom; Computer Science & Engineering Department, University of Nebraska-Lincoln, Lincoln, Nebraska, United States; Computer Science Department, University College London, London, United Kingdom; Computer Science Department, University College London, London, United Kingdom",IEEE Transactions on Software Engineering,,2015,41,9,901,924,"Combinatorial interaction testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. Simulated Annealing (SA) and Greedy Algorithms have been widely used to find CIT test suites. From the literature, there is a widely-held belief that SA is slower, but produces more effective tests suites than Greedy and that SA cannot scale to higher strength coverage. We evaluated both algorithms on seven real-world subjects for the well-studied two-way up to the rarely-studied six-way interaction strengths. Our findings present evidence to challenge this current orthodoxy: real-world constraints allow SA to achieve higher strengths. Furthermore, there was no evidence that Greedy was less effective (in terms of time to fault revelation) compared to SA; the results for the greedy algorithm are actually slightly superior. However, the results are critically dependent on the approach adopted to constraint handling. Moreover, we have also evaluated a genetic algorithm for constrained CIT test suite generation. This is the first time strengths higher than 3 and constraint handling have been used to evaluate GA. Our results show that GA is competitive only for pairwise testing for subjects with a small number of constraints.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2015.2421279,"National Science Foundation; Air Force Office of Scientific Research; UK Engineering and Physical Sciences Research Council (EPSRC); DAASE: Dynamic Adaptive Automated Software Engineering; GISMO: Genetic Improvement of Software for Multiple Objectives; CREST: Centre for Research on Evolution, Search and Testing; DAASE; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081752,Combinatorial Interaction Testing;Prioritisation;Empirical Studies;Software Testing;Combinatorial interaction testing;prioritisation;empirical studies;software testing,Testing;Simulated annealing;Genetic algorithms;Fault detection;Greedy algorithms;Turning;Flexible printed circuits,genetic algorithms;greedy algorithms;program testing;simulated annealing;software fault tolerance,combinatorial interaction testing;early fault detection;software system configuration space;simulated annealing;SA;greedy algorithm;CIT test suite generation;constraint handling;pairwise testing;genetic algorithm,,28,37,,,,,,IEEE,IEEE Journals & Magazines
Preemptive Regression Testingof Workflow-Based Web Services,L. Mei; W. K. Chan; T. H. Tse; B. Jiang; K. Zhai,NA; NA; NA; NA; NA,IEEE Transactions on Services Computing,,2015,8,5,740,754,"An external web service may evolve without prior notification. In the course of the regression testing of a workflow-based web service, existing test case prioritization techniques may only verify the latest service composition using the not-yet-executed test cases, overlooking high-priority test cases that have already been applied to the service composition before the evolution. In this paper, we propose Preemptive Regression Testing (PRT), an adaptive testing approach to addressing this challenge. Whenever a change in the coverage of any service artifact is detected, PRT recursively preempts the current session of regression test and creates a sub-session of the current test session to assure such lately identified changes in coverage by adjusting the execution priority of the test cases in the test suite. Then, the sub-session will resume the execution from the suspended position. PRT terminates only when each test case in the test suite has been executed at least once without any preemption activated in between any test case executions. The experimental result confirms that testing workflow-based web service in the face of such changes is very challenging; and one of the PRT-enriched techniques shows its potential to overcome the challenge.",1939-1374;2372-0204,,10.1109/TSC.2014.2322621,National Natural Science Foundation of China; CCF-Tencent Open Research Fund; National High Technology Research and Development Program of China; National Science and Technology Infrastructure Program; Early Career Scheme and the General Research Fund of the Research Grants Council of Hong Kong; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6812226,Evolving service composition;adaptive regression testing,Testing;Web services;Electronic mail;Educational institutions;Context;Maintenance engineering,program testing;regression analysis;Web services;workflow management software,preemptive regression testing;PRT;workflow-based Web service;adaptive testing approach;test case execution,,5,47,,,,,,IEEE,IEEE Journals & Magazines
Prioritization and ranking of ERP testing components,S. Nagpal; S. K. Khatri; P. K. Kapur,"Amity Institute of Information Technology, Amity University Uttar Pradesh, India; Amity Institute of Information Technology, Amity University Uttar Pradesh, India; Centre for Interdisciplinary Research, Amity University Uttar Pradesh, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)",,2015,,,1,6,"Software Testing is one of the most important activities but more often than not attracts less attention than it deserves in software development and implementation. It often takes twenty to sometimes even more than fifty percent of the total software development time. Enterprise Resource Planning (ERP) Systems provide synergy by integrating all operations of an enterprise. So implementation of ERP systems need even more rigorous testing than that employed in stand-alone software development. Software testing is a well-researched area but software testing as employed on ERP systems albeit is droughted with respect to research. This research paper is an extension of the patent by Kapur et al.[8] that identified the ERP Testing Components to measure ERP testing efficiency. Here, the ERP Testing Components have been accumulated and categorized under five heads. Thereafter, these testing components have been prioritized and ranked with the help of Analytic Hierarchy Process (AHP), as given by Saaty [17].",,978-1-4673-7231-2978-1-4673-7230,10.1109/ICRITO.2015.7359245,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359245,Enterprise Resource Planning (ERP);Critical Success Factors (CSF);ERP Testing Components;Analytic Hierarchy Process (AHP),Testing;Software;Complexity theory;Planning;Analytic hierarchy process;Organizations,analytic hierarchy process;enterprise resource planning;program testing;software engineering,software testing;software development;enterprise resource planning system;ERP system;ERP testing component;analytic hierarchy process;AHP,,1,17,,,,,,IEEE,IEEE Conferences
Prioritization of test scenarios using hybrid genetic algorithm based on UML activity diagram,X. Wang; X. Jiang; H. Shi,"College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China",2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS),,2015,,,854,857,"Software testing is an essential part of the SDLC(Software Development Life Cycle). Test scenarios are used to derive test cases for model based testing. However, with the software rapidly growing in size and complexity, the cost of software will be too high if we want to test all the test cases. So this paper presents an approach using Hybrid Genetic Algorithm(HGA) to prioritize test scenarios, which improves efficiency and reduces cost as well. The algorithm combines Genetic Algorithm(GA) with Particle Swarm Optimization(PSO) algorithm and uses Local Search Strategy to update the local and global best information of the PSO. The proposed algorithm can prioritize test scenarios so as to find a critical scenario. Finally, the proposed method is applied to several typical UML activity diagrams, and compared with the Simple Genetic Algorithm(SGA). The experimental results show that the proposed method not only prioritizes test scenarios, but also improves the efficiency, and further saves effort, time as well as cost.",2327-0594;2327-0586,978-1-4799-8353-7978-1-4799-8352-0978-1-4799-8351,10.1109/ICSESS.2015.7339189,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339189,Software testing;Test scenarios prioritization;Hybrid Genetic Algorithm;UML Activity Diagram,Unified modeling language;Genetic algorithms;Testing;Software;Sociology;Statistics;Biological cells,genetic algorithms;particle swarm optimisation;program testing;search problems;software cost estimation;Unified Modeling Language,test scenarios prioritization;hybrid genetic algorithm;UML activity diagram;software testing;SDLC;software development life cycle;test cases;model based testing;software cost;HGA;particle swarm optimization;PSO algorithm;local search strategy;local best information;global best information;simple genetic algorithm;SGA,,2,8,,,,,,IEEE,IEEE Conferences
Prioritized test-driven reverse engineering process: A case study,P. Sfetsos; L. Angelis; I. Stamelos,"Department of Information Technology, Alexander Technological Education Institute, Thessaloniki, Greece; Department of Informatics, Aristotle University, Thessaloniki, Greece; Department of Informatics, Aristotle University, Thessaloniki, Greece","2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA)",,2015,,,1,6,"In this study we empirically investigate the adaptation of Test-Driven Development (TDD) practice into software Reverse Engineering (RE) process. We call this adaptation as Test-Driven Reverse Engineering (TDRE) process. We propose a two-layer prioritization process, which firstly prioritizes the already-implemented functionalities using the Cumulative Voting (CV) method and three prioritization criteria (importance, complexity and dependency), and secondly prioritizes test-cases for each prioritized functionality, using the same criteria. We conducted a case study in academia with students to empirically evaluate the usability and effectiveness of the prioritization process and the TDD adaptation into RE process. The results have shown that students with a good performance in testing had also good performance in designing UML class-diagrams. Moreover, the implementation of hierarchical test-cases for the already prioritized functionalities, improves code comprehension and redesigning in the RE process in terms of better total grades obtained.",,978-1-4673-9311-9978-1-4673-9310,10.1109/IISA.2015.7388099,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7388099,Test Driven Development;Reverse Engineering;Prioritization;Cumulative Voting;UML class-diagram,Testing;Reverse engineering;Software;Complexity theory;Unified modeling language;Software engineering;Fault detection,program testing;reverse engineering;software maintenance;Unified Modeling Language,prioritized test-driven software reverse engineering process;test-driven development;TDD adaptation;TDRE;two-layer prioritization process;cumulative voting method;importance prioritization criteria;complexity prioritization criteria;dependency prioritization criteria;UML class-diagrams;hierarchical test-cases;code comprehension improvement,,,35,,,,,,IEEE,IEEE Conferences
Prioritizing Manual Test Cases in Traditional and Rapid Release Environments,H. Hemmati; Z. Fang; M. V. Mantyla,NA; NA; NA,"2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)",,2015,,,1,10,"Test case prioritization is one of the most practically useful activities in testing, specially for large scale systems. The goal is ranking the existing test cases in a way that they detect faults as soon as possible, so that any partial execution of the test suite detects maximum number of defects for the given budget. Test prioritization becomes even more important when the test execution is time consuming, e.g., manual system tests vs. automated unit tests. Most existing test case prioritization techniques are based on code coverage, which requires access to source code. However, manual testing is mainly done in a black- box manner (manual testers do not have access to the source code). Therefore, in this paper, we first examine the existing test case prioritization techniques and modify them to be applicable on manual black-box system testing. We specifically study a coverage- based, a diversity-based, and a risk driven approach for test case prioritization. Our empirical study on four older releases of Mozilla Firefox shows that none of the techniques are strongly dominating the others in all releases. However, when we study nine more recent releases of Firefox, where the development has been moved from a traditional to a more agile and rapid release environment, we see a very signifiant difference (on average 65% effectiveness improvement) between the risk-driven approach and its alternatives. Our conclusion, based on one case study of 13 releases of an industrial system, is that test suites in rapid release environments, potentially, can be very effectively prioritized for execution, based on their historical riskiness; whereas the same conclusions do not hold in the traditional software development environments.",2159-4848,978-1-4799-7125,10.1109/ICST.2015.7102602,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102602,,Testing;Manuals;Software;Context;Natural languages;Fault detection;Companies,online front-ends;program testing;software fault tolerance;source code (software),manual test case prioritization;rapid release environments;large scale systems;fault detection;test execution;code coverage;source code;manual black-box system testing;coverage-based approach;diversity-based approach;risk driven approach;Mozilla Firefox;historical riskiness,,7,34,,,,,,IEEE,IEEE Conferences
Priority Integration for Weighted Combinatorial Testing,E. Choi; T. Kitamura; C. Artho; A. Yamada; Y. Oiwa,NA; NA; NA; NA; NA,2015 IEEE 39th Annual Computer Software and Applications Conference,,2015,2,,242,247,"Priorities (weights) for parameter values can improve the effectiveness of combinatorial testing. Previous approaches have employed weights to derive high-priority test cases either earlier or more frequently. Our approach integrates these order-focused and frequency-focused prioritizations. We show that our priority integration realizes a small test suite providing high-priority test cases early and frequently in a good balance. We also propose two algorithms that apply our priority integration to existing combinatorial test generation algorithms. Experimental results using numerous test models show that our approach improves the existing approaches w.r.t. Order-focused and frequency-focused metrics, while overheads in the size and generation time of test suites are small.",0730-3157,978-1-4673-6564-2978-1-4673-6563,10.1109/COMPSAC.2015.113,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273624,Combinatorial testing;Pairwise testing;Prioritized Testing;Priority weight;Weight coverage;KL divergence,Optical wavelength conversion;Measurement;Browsers;Benchmark testing;Focusing;Algorithm design and analysis,program testing,priority integration;weighted combinatorial testing;high-priority test cases;order-focused prioritizations;frequency-focused prioritizations;combinatorial test generation algorithms,,5,10,,,,,,IEEE,IEEE Conferences
Pushing to the top,A. Ivrii; A. Gurfinkel,IBM Research; Software Engineering Institute,2015 Formal Methods in Computer-Aided Design (FMCAD),,2015,,,65,72,"IC3 is undoubtedly one of the most successful and important recent techniques for unbounded model checking. Understanding and improving IC3 has been a subject of a lot of recent research. In this regard, the most fundamental questions are how to choose Counterexamples to Induction (CTIs) and how to generalize them into (blocking) lemmas. Answers to both questions influence performance of the algorithm by directly affecting the quality of the lemmas learned. In this paper, we present a new IC3-based algorithm, called QUIP1, that is designed to more aggressively propagate (or push) learned lemmas to obtain a safe inductive invariant faster. QUIP modifies the recursive blocking procedure of IC3 to prioritize pushing already discovered lemmas over learning of new ones. However, a naive implementation of this strategy floods the algorithm with too many useless lemmas. In QUIP, we solve this by extending IC3 with may-proof-obligations (corresponding to the negations of learned lemmas), and by using an under-approximation of reachable states (i.e., states that witness why a may-proof-obligation is satisfiable) to prune non-inductive lemmas. We have implemented QUIP on top of an industrial-strength implementation of IC3. The experimental evaluation on HWMCC benchmarks shows that the QUIP is a significant improvement (at least 2x in runtime and more properties solved) over IC3. Furthermore, the new reasoning capabilities of QUIP naturally lead to additional optimizations and new techniques that can lead to further improvements in the future.",,978-0-9835-6785-1978-1-5090-4151,10.1109/FMCAD.2015.7542254,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542254,,Optimization;Algorithm design and analysis;Model checking;Cognition;Software engineering;Terminology;Benchmark testing,formal verification;reachability analysis;theorem proving,unbounded model checking;counterexample-to-induction;blocking lemmas;IC3-based algorithm;learned lemma pushing;learned lemma propagation;recursive blocking procedure;may-proof-obligations;reachable state under-approximation;may-proof-obligation;noninductive lemmas;HWMCC benchmarks;QUIP reasoning capabilities,,5,18,,,,,,IEEE,IEEE Conferences
Quantifying security risk by measuring network risk conditions,C. Suh-Lee; J. Jo,"Department of Computer Science, University of Nevada, Las Vegas, USA; Department of Computer Science, University of Nevada, Las Vegas, USA",2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS),,2015,,,9,14,"Software vulnerabilities are the weaknesses in the software that inadvertently allow dangerous operations. If the vulnerability is in a network service, it poses serious security threats because a cyber-attacker can exploit it to gain unauthorized access to the system. Hence, rapid discovery and remediation of network vulnerabilities is critical issues in network security. In today's dynamic IT environment, it is common practice that an organization prioritizes the mitigation of discovered vulnerabilities according to their risk levels. Currently available technologies, however, associate each vulnerability to the static risk level which does not take the unique characteristics of the target network into account. This often leads to inaccurate risk prioritization and less-than-optimal resource allocation. In this research, we introduce a novel way of quantifying the risk of network vulnerability by augmenting the static risk level with conditions specific to the target network. The method calculates the risk value of each vulnerability by measuring the proximity to the untrusted network and risk of the neighboring hosts. The resulting risk value, RCR is a composite index of the individual risk, network location and neighborhood risk conditions. Thus, it can be effectively used for prioritization, comparison and trending. We tested the methodology through the network intrusion simulation. The results shows average 88.9% the correlation between RCR and number of successful attacks on each vulnerability.",,978-1-4799-8679,10.1109/ICIS.2015.7166562,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166562,risk management;network security;vulnerability management;useable security;quantitative risk analysis,Security;Internet;Servers;Workstations;Organizations;Standards organizations;Reliability,computer network security;resource allocation;risk management,security risk quantification;network risk condition measurement;software vulnerability;network service;security threats;cyber-attacker;network vulnerability;network security;dynamic IT environment;risk prioritization;less-than-optimal resource allocation;RCR;network location;network intrusion simulation,,5,22,,,,,,IEEE,IEEE Conferences
Replicating and Re-Evaluating the Theory of Relative Defect-Proneness,M. D. Syer; M. Nagappan; B. Adams; A. E. Hassan,"School of Computing, Queenƒ??s University, Kingston, ON, Canada; School of Computing, Queenƒ??s University, Kingston, ON, Canada; Genie Informatique et Genie Logiciel, Ecole Polytechnique de Montreal, Campus de lƒ??Universite de Montreal; School of Computing, Queenƒ??s University, Kingston, ON, Canada",IEEE Transactions on Software Engineering,,2015,41,2,176,197,"A good understanding of the factors impacting defects in software systems is essential for software practitioners, because it helps them prioritize quality improvement efforts (e.g., testing and code reviews). Defect prediction models are typically built using classification or regression analysis on product and/or process metrics collected at a single point in time (e.g., a release date). However, current defect prediction models only predict if a defect will occur, but not when, which makes the prioritization of software quality improvements efforts difficult. To address this problem, Koru et al. applied survival analysis techniques to a large number of software systems to study how size (i.e., lines of code) influences the probability that a source code module (e.g., class or file) will experience a defect at any given time. Given that 1) the work of Koru et al. has been instrumental to our understanding of the size-defect relationship, 2) the use of survival analysis in the context of defect modelling has not been well studied and 3) replication studies are an important component of balanced scholarly debate, we present a replication study of the work by Koru et al. In particular, we present the details necessary to use survival analysis in the context of defect modelling (such details were missing from the original paper by Koru et al.). We also explore how differences between the traditional domains of survival analysis (i.e., medicine and epidemiology) and defect modelling impact our understanding of the size-defect relationship. Practitioners and researchers considering the use of survival analysis should be aware of the implications of our findings.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2014.2361131,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914599,Survival Analysis;Cox Models;Defect Modelling;Survival analysis;Cox models;defect modelling,Analytical models;Hazards;Software;Measurement;Data models;Mathematical model;Predictive models,program diagnostics;software quality;software reliability,relative defect-proneness theory;survival analysis techniques;source code module;size-defect relationship;defect modelling;software system defects,,3,47,,,,,,IEEE,IEEE Journals & Magazines
Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling,X. Gu; A. Easwaran; K. Phan; I. Shin,NA; NA; NA; NA,2015 27th Euromicro Conference on Real-Time Systems,,2015,,,13,24,"Mixed-criticality real-time scheduling has been developed to improve resource utilization while guaranteeing safe execution of critical applications. These studies use optimistic resource reservation for all the applications to improve utilization, but prioritize critical applications when the reservations become insufficient at runtime. Many of them however share an impractical assumption that all the critical applications will simultaneously demand additional resources. As a consequence, they under-utilize resources by penalizing all the low-criticality applications. In this paper we overcome this shortcoming using a novel mechanism that comprises a parameter to model the expected number of critical applications simultaneously demanding more resources, and an execution strategy based on the parameter to improve resource utilization. Since most mixed criticality systems in practice are component-based, we design our mechanism such that the component boundaries provide the isolation necessary to support the execution of low-criticality applications, and at the same time protect the critical ones. We also develop schedulability tests for the proposed mechanism under both a flat as well as a hierarchical scheduling framework. Finally, through simulations, we compare the performance of the proposed approach with existing studies in terms of schedulability and the capability to support low-criticality applications.",2377-5998;1068-3070,978-1-4673-7570,10.1109/ECRTS.2015.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176022,Mixed-Criticality;Scheduling;Resource,Real-time systems;Resource management;Runtime;Electronic mail;Processor scheduling;Computational modeling;Market research,object-oriented programming;real-time systems;resource allocation;safety-critical software;scheduling,resource efficient isolation mechanism;mixed-criticality real-time scheduling;resource utilization;optimistic resource reservation;component-based software,,13,21,,,,,,IEEE,IEEE Conferences
Strategies for Prioritizing Test Cases Generated Through Model-Based Testing Approaches,J. F. S. Ouriques,NA,2015 IEEE/ACM 37th IEEE International Conference on Software Engineering,,2015,2,,879,882,"Software testing is expensive and time consuming,especially for complex software. In order to deal with the costof testing, researchers develop Model-Based Testing (MBT). InMBT, test cases are generated automatically and a drawback isa huge generated test suite. Our research aims at studying the Test Case Prioritization problem in MBT context. So far, we already evaluated the influence of the model structure and the characteristics of the test cases that fail. Results suggest that the former does not affect significantly the performance of techniques, however, the latter indeed represents a major impact. Therefore, a worthy information in this context might be an expert who knows the crucial parts of the software, thus we propose the first version of a prioritization technique that considers hints from the expert and the distance notion in order to prioritize test cases. Evaluation and tuning of the technique are ongoing, but preliminary evaluation reveals promising results.",0270-5257;1558-1225,978-1-4799-1934,10.1109/ICSE.2015.338,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203104,,Testing;Context;Unified modeling language;Software;Context modeling;Software engineering;Conferences,program testing,test case generation;model-based testing approach;software testing;MBT;test case prioritization problem;prioritization technique,,1,27,,,,,,IEEE,IEEE Conferences
Supporting Continuous Integration by Code-Churn Based Test Selection,E. Knauss; M. Staron; W. Meding; O. S??der; A. Nilsson; M. Castell,NA; NA; NA; NA; NA; NA,2015 IEEE/ACM 2nd International Workshop on Rapid Continuous Software Engineering,,2015,,,19,25,"Continuous integration promises advantages in large-scale software development by enabling software development organizations to deliver new functions faster. However, implementing continuous integration in large software development organizations is challenging because of organizational, social and technical reasons. One of the technical challenges is the ability to rapidly prioritize the test cases which can be executed quickly and trigger the most failures as early as possible. In our research we propose and evaluate a method for selecting a suitable set of functional regression tests on system level. The method is based on analysis of correlations between test-case failures and source code changes and is evaluated by combining semi-structured interviews and workshops with practitioners at Ericsson and Axis Communications in Sweden. The results show that using measures of precision and recall, the test cases can be prioritized. The prioritization leads to finding an optimal test suite to execute before the integration.",,978-1-4673-7067,10.1109/RCoSE.2015.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167168,,Heating;Testing;Software;Companies;Interviews;Data visualization,integrated software;organisational aspects;program testing;software development management;software reliability;source code (software);statistical testing,code-churn based test selection;continuous software integration;large scale software development organization;functional regression test;system level;test case failure;source code;optimal test suite,,4,23,,,,,,IEEE,IEEE Conferences
Test case analytics: Mining test case traces to improve risk-driven testing,T. B. Noor; H. Hemmati,"Department of Computer Science University of Manitoba Winnipeg, Canada; Department of Computer Science University of Manitoba Winnipeg, Canada",2015 IEEE 1st International Workshop on Software Analytics (SWAN),,2015,,,13,16,"In risk-driven testing, test cases are generated and/or prioritized based on different risk measures. For example, the most basic risk measure would analyze the history of the software and assigns higher risk to the test cases that used to detect bugs in the past. However, in practice, a test case may not be exactly the same as a previously failed test, but quite similar. In this study, we define a new risk measure that assigns a risk factor to a test case, if it is similar to a failing test case from history. The similarity is defined based on the execution traces of the test cases, where we define each test case as a sequence of method calls. We have evaluated our new risk measure by comparing it to a traditional risk measure (where the risk measure would be increased only if the very same test case, not a similar one, failed in the past). The results of our study, in the context of test case prioritization, on two open source projects show that our new risk measure is by far more effective in identifying failing test cases compared to the traditional risk measure.",,978-1-4673-6923,10.1109/SWAN.2015.7070482,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070482,Execution trace; Testing; Bug; Risk-driventesting; Similarity; Test case prioritization,Testing;History;Current measurement;Context;Software;Computer bugs;Databases,program diagnostics;program testing;public domain software;risk management;software management,test case analytics;test case trace mining;risk-driven testing;software history;risk factor;open source projects,,7,10,,,,,,IEEE,IEEE Conferences
Test case prioritization for regression testing based on ant colony optimization,D. Gao; X. Guo; L. Zhao,"Beijing Institute of Control Engineering, Beijing 100190, P.R. China; Beijing Sunwise Information Technology Co. Ltd., Beijing 100190, P.R. China; Beijing Sunwise Information Technology Co. Ltd., Beijing 100190, P.R. China",2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS),,2015,,,275,279,"Test case prioritization technique is an efficient method to improve regression testing activities. It orders a regression test suite to execute the test cases with higher priority earlier than those with lower priority, and the problem is how to optimize the test case ordering according to some criterion. In this paper, we have proposed an algorithm which prioritizes the test cases based on ant colony optimization (ACO), considering three factors: number of faults detected, execution time and fault severity, and these three factors are used in ant colony optimization algorithm to help to reveal more severe faults at earlier stage of the regression testing process. The effectiveness of the algorithm is demonstrated using the metric named APFD, and the results of experiment show the algorithm optimizes the test case orderings effectively.",2327-0594;2327-0586,978-1-4799-8353-7978-1-4799-8352-0978-1-4799-8351,10.1109/ICSESS.2015.7339054,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339054,test case prioritization;ant colony optimization;regression testing,Testing;Fault detection;Ant colony optimization;Algorithm design and analysis;Measurement;Software;Optimization,ant colony optimisation;mathematics computing;program testing;regression analysis;statistical testing,test case prioritization technique;regression testing activities;regression test suite;fault detection;execution time;fault severity;ant colony optimization algorithm;APFD;test case ordering,,5,27,,,,,,IEEE,IEEE Conferences
Test case prioritization with textual comparison metrics,R. Tumeng; D. N. A. Jawawi; M. A. Isa,"Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia; Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia; Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia",2015 9th Malaysian Software Engineering Conference (MySEC),,2015,,,7,12,"Regression testing of a large test pool consistently needs a prioritization technique that caters requirements changes. Conventional prioritization techniques cover only the methods to find the ideal ordering of test cases neglecting requirement changes. In this paper, we propose string dissimilarity-based priority assignment to test cases through the combination of classical and non-classical textual comparison metrics and elaborate a prioritization algorithm considering requirement changes. The proposed technique is suitable to be used as a preliminary testing when the information of the entire program is not in possession. We performed evaluation on random permutations and three textual comparison metrics and concluded the findings of the experiment.",,978-1-4673-8227-4978-1-4673-8226,10.1109/MySEC.2015.7475187,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475187,textual comparison;test case prioritization;regression testing,Measurement;Testing;Silicon;Software;Context;Software engineering;Programming,program testing;regression analysis;systems analysis,test case prioritization;textual comparison metrics;regression testing;requirements changes;random permutations,,,9,,,,,,IEEE,IEEE Conferences
Test case prioritization: An approach based on modified ant colony optimization (m-ACO),K. Solanki; Y. Singh; S. Dalal,"M.D. University, Rohtak, India; M.D. University, Rohtak, India; M.D. University, Rohtak, India","2015 International Conference on Computer, Communication and Control (IC4)",,2015,,,1,6,"Intense and widespread usage of software in every field of life has attracted the researchers to focus their attention on developing the methods to improve the efficiency of software testing; which is the most crucial and cost intensive phase of software development. Software testing aims to uncover the potential faults in Application Under Test by running the test cases on software code. Software code keeps on changing as the uncovered faults during testing are fixed by the developers. Regression testing is concerned with verifying the modified software code to ensure that changes in software code does not induce any undesired effect on rest of the code. Test Case Prioritization is a regression testing technique which re-schedule the execution sequence of test cases to improve the fault detection rate and enhance the performance of regression test suite. This paper focuses on proposing a novel method ""m-ACO"" for test case prioritization and the performance evaluation of the proposed method using Average Percentage of faults Detected.",,978-1-4799-8164-9978-1-4799-8163,10.1109/IC4.2015.7375627,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375627,Software Testing;Regression Testing;Test Case Prioritization;APFD,Software;Software testing;Ant colony optimization;Optimization;Algorithm design and analysis;Fault detection,ant colony optimisation;program testing;regression analysis;software fault tolerance,test case prioritization;modified ant colony optimization;m-ACO;software testing;software development;application under test;software code;regression testing technique;performance evaluation;fault detection rate,,2,21,,,,,,IEEE,IEEE Conferences
Test case selection and prioritization using cuckoos search algorithm,R. Nagar; A. Kumar; G. P. Singh; S. Kumar,"Computer Science and Engineering Department, Aligarh College of Engineering and Technology, Aligarh, India; Pitney Bowes Software Noida, India; Computer Science and Engineering Department, Aligarh College of Engineering and Technology, Aligarh, India; Computer Science and Engineering Department, Shivdan Singh Institute of Technology &amp; Management, Aligarh, India",2015 International Conference on Futuristic Trends on Computational Analysis and Knowledge Management (ABLAZE),,2015,,,283,288,"Regression Testing is an inevitable and very costly activity that is implemented to ensure the validity of new version of software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization for proper selection and schedule of test cases in a specific sequence, fulfilling some chosen criteria. Cuckoo search (CS) algorithm is an optimization algorithm proposed by Yang and Deb [13]. It is inspired by the obligate brood parasitism of some cuckoo species by laying their eggs in the nests of other host birds. Cuckoo Search is very easy to implement as it depends on single parameter only unlike other optimization algorithms. In this paper a test case selection and prioritization algorithm has been proposed using Cuckoo Search. This algorithm selects and prioritizes the test cases based on the number of faults covered in minimum time. The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.",,978-1-4799-8433-6978-1-4799-8432,10.1109/ABLAZE.2015.7155012,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155012,Cuckoos Search;Levy Flight;Regression Test Selection;Test Case Prioritization;Artificial Intelligence,Testing;Algorithm design and analysis;Optimization;Software algorithms;Software;Sociology;Statistics,configuration management;program testing;program verification;regression analysis;search problems,test case selection;test case prioritization;Cuckoos search algorithm;regression testing;software version;time constrained environment;resource constrained environment;test case scheduling;CS algorithm;optimization algorithm;obligate brood parasitism;cuckoo species;fault,,4,26,,,,,,IEEE,IEEE Conferences
Test case selection for networked production systems,A. Zeller; M. Weyrich,"Institute of Industrial Automation and Software Engineering, University of Stuttgart, Germany; Institute of Industrial Automation and Software Engineering, University of Stuttgart, Germany",2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA),,2015,,,1,4,"This paper provides a discussion on the coming technological changes in process automation of networked production systems, which will change the testing procedure. In the smart factory of the future there will be no possibility to reach a test coverage of 100%, assuming a flexible automation with continuous reconfiguration and dynamic changes during runtime. Consequently, large amounts of test cases and powerful algorithms for their prioritization are needed in order to certify the correct functionality of the production systems in the network. A concept is presented on how to analyze and prioritize the enormous amount of test cases resulting from the changes during runtime. The proposed approach for test case selection utilizes information of the product, the process and the status of the for the prioritization and selection.",1946-0740;1946-0759,978-1-4673-7929-8978-1-4673-7928,10.1109/ETFA.2015.7301604,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301604,test case selection;functional testing;test prioritizing;quality-assurance;smart factory;networked systems,Production systems;Automation;Testing;Reliability;Accuracy;History,factory automation;management of change,test case selection;networked production systems;technological change;process automation;smart factory of the future;flexible automation;continuous reconfiguration;dynamic change;product information;production system functionality,,,8,,,,,,IEEE,IEEE Conferences
Test cases prioritization for software regression testing using analytic hierarchy process,P. Klindee; N. Prompoon,"Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, Thailand",2015 12th International Joint Conference on Computer Science and Software Engineering (JCSSE),,2015,,,168,173,"Test cases are considered an important asset in the software testing process since they are used to detect defects in the software. In order to produce quality software covering all of the requirements, the test case designer requires much time and effort in designing test cases to cover all requirements and conditions according to the test case structure. This research proposes a method for storing and retrieving of test cases affected by software requirements changes, as well as ranking the retrieved test cases using the AHP method to improve the quality of the ranking. There are to assist system testers in identifying test cases for complete regression testing. An example application of the proposed method will also be presented.",,978-1-4799-1966-6978-1-4799-1965,10.1109/JCSSE.2015.7219790,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219790,Analytical Hierarchy Process;AHP;Information Retrieval;Prioritization Technique;Regression Testing;Test Case Prioritization,Software;Indexes;Complexity theory;Analytic hierarchy process;Software testing;Computer aided software engineering,analytic hierarchy process;program testing;software quality,test cases prioritization;software regression testing process;analytic hierarchy process method;software quality;AHP method,,3,14,,,,,,IEEE,IEEE Conferences
Testing analytics on software variability,H. K. N. Leung; K. M. Lui,"Department of Computing The Hong Kong Polytechnic University Hong Kong, China; Department of Computing The Hong Kong Polytechnic University Hong Kong, China",2015 IEEE 1st International Workshop on Software Analytics (SWAN),,2015,,,17,20,"Software testing is a tool-driven process. However, there are many situations in which different hardware/software components are tightly integrated. Thus system integration testing has to be manually executed to evaluate the system's compliance with its specified requirements and performance. There could be many combinations of changes as different versions of hardware and software components could be upgraded and/or substituted. Occasionally, some software components could even be replaced by clones. The whole system after each component change demands to be re-tested to ensure proper system behavior. For better utilization of resources, there is a need to prioritize the past test cases to test the newly integrated systems. We propose a way to facilitate the use of historical testing records of the previous systems so that a testcase portfolio can be developed, which intends to maximize testing resources for the same integrated product family. As the proposed framework does not consider much of internal software complexity, the implementation costs are relatively low.",,978-1-4673-6923,10.1109/SWAN.2015.7070483,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070483,Software testing;Risk management;Clones;Softwareproject management;Variability testing,Testing;Portfolios;Software;Manuals;Hardware;Risk management;Cloning,program testing;software management,software variability;software testing;tool-driven process;system integration testing;software complexity,,,16,,,,,,IEEE,IEEE Conferences
The Effect of GoF Design Patterns on Stability: A Case Study,A. Ampatzoglou; A. Chatzigeorgiou; S. Charalampidou; P. Avgeriou,"Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands; Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands; Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands",IEEE Transactions on Software Engineering,,2015,41,8,781,802,"Stability refers to a software system's resistance to the ƒ??ripple effectƒ?, i.e., propagation of changes. In this paper, we investigate the stability of classes that participate in instances/occurrences of GoF design patterns. We examine whether the stability of such classes is affected by (a) the pattern type, (b) the role that the class plays in the pattern, (c) the number of pattern occurrences in which the class participates, and (d) the application domain. To this end, we conducted a case study on about 65.000 Java open-source classes, where we performed change impact analysis on classes that participate in zero, one (single pattern), or more than one (coupled) pattern occurrences. The results suggest that, the application of design patterns can provide the expected ƒ??shieldingƒ? of certain pattern-participating classes against changes, depending on their role in the pattern. Moreover, classes that participate in coupled pattern occurrences appear to be the least stable. The results can be used for assessing the benefits and liabilities of the use of patterns and for testing and refactoring prioritization, because less stable classes are expected to require more effort while testing, and urge for refactoring activities that would make them more resistant to change propagation.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2015.2414917,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7066925,"D.2.2 Design Tools and Techniques;D.2.3.a Object-oriented programming,;D.2.8 Metrics/Measurement;Design Tools and Techniques;Object-oriented programming;Metrics/Measurement",Stability analysis;Couplings;Abstracts;Measurement;Production facilities;Open source software,Java;object-oriented programming;public domain software,GoF design pattern;stability;software system resistance;ripple effect;Java open-source class;change impact analysis;coupled pattern occurrence;pattern-participating class;refactoring prioritization;refactoring activity;change propagation,,15,53,,,,,,IEEE,IEEE Journals & Magazines
The evaluation of the results of an eye tracking based usability tests of the so called Instructor's Portal framework (http://tanitlap.ektf.hu/csernaiz),C. K. Prantner,"Department of Human Informatics, Eszterh?­zy K?­roly College, Eger, Hungary",2015 6th IEEE International Conference on Cognitive Infocommunications (CogInfoCom),,2015,,,459,465,"The research discussed in this paper can be positioned at the cross-section of Applied Computer Science, Didactics and Human-Computer Interaction. Accordingly, an Instructor's Portal (IPo) framework system was developed at the Department of Human Informatics of the Eszterh?­zy K?­roly College (EKC) in 2015. The aim of the framework system is to enable instructors working in higher education institutions to establish, customize, and update their own webpages independently without any help from informatics professionals. Said system not only fulfills a gap filling function in the higher education sphere, but performs complex tasks while serving a wide range of users. In order to establish a logically arranged content structure and user interface the respective development process observed several developmental principles, methods, and web-ergonomic rules. This paper introduces the results of usability tests and examinations pertaining to the system. The examination utilized, an eye-tracking hardware device and a mouse movement recording software along with a special software facilitating the evaluation and presentation of the respective results. This essay introduces and highlight how the respective apparatus complements the traditional human observation-based usability tests while identifying the cognitive skills to be ascertained with such devices, one of the main priorities of Cognitive Infocommunications (CogInfoCom).",,978-1-4673-8129-1978-1-4673-8128,10.1109/CogInfoCom.2015.7390637,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390637,eye tracking;usability test;ergonomics;instruction portal;higher education,Usability;Testing;Gaze tracking;Portals;Informatics;Hardware,computer aided instruction;educational institutions;further education;interactive devices;portals;user interfaces,eye tracking based usability tests;instructors portal framework;applied computer science;didactics;human computer interaction;IPo framework system;higher education institutions;Web page;gap filling function;logically arranged content structure;user interface;eye-tracking hardware device;mouse movement recording software;cognitive infocommunications;CogInfoCom,,,20,,,,,,IEEE,IEEE Conferences
Total coverage based regression test case prioritization using genetic algorithm,P. Konsaard; L. Ramingwong,"Department of Computer Engineering, Chiang Mai University, Thailand; Department of Computer Engineering, Chiang Mai University, Thailand","2015 12th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)",,2015,,,1,6,"Regression Testing is a test to ensure that a program that was changed is still working. Changes introduced to a software product often come with defects. Additional test cases are, this could reduce the main challenges of regression testing is test case prioritization. Time, effort and budget needed to retest the software. Former studies in test case prioritization confirm the benefits of prioritization techniques. Most prioritization techniques concern with choosing test cases based on their ability to cover more faults. Other techniques aim to maximize code coverage. Thus, the test cases selected should secure the total coverage to assure the adequacy of software testing. In this paper, we present an algorithm to prioritize test cases based on total coverage using a modified genetic algorithm. Its performance on the average percentage of condition covered and execution time are compared with five other approaches.",,978-1-4799-7961-5978-1-4799-7960,10.1109/ECTICon.2015.7207103,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207103,Test case prioritization;Test suite;Regression testing;Genetic algorith;Code coverage;APCC;Software Testing;Software engineering,Genetic algorithms;Software;Software testing;Sociology;Statistics;Fault detection,genetic algorithms;program testing;regression analysis,total coverage based regression test case prioritization;software product;code coverage;software testing;modified genetic algorithm,,7,21,,,,,,IEEE,IEEE Conferences
Towards a framework for automatic correction of anti-patterns,R. Morales,"SWAT, Polytechnique Montr&#x00E9;al, Canada","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)",,2015,,,603,604,"One of the biggest concerns in software maintenance is design quality; poor design hinders software maintenance and evolution. One way to improve design quality is to detect and correct anti-patterns (i.e., poor solutions to design and implementation problems), for example through refactorings. There are several approaches to detect anti-patterns, that rely on metrics and structural properties. However, finding a specific solution to remove anti-patterns is a challenging task as candidate refactorings can be conflicting and their number very large, making it costly. Hence, development teams often have to prioritize the refactorings to be applied on a system. In addition to this, refactoring is risky, since non-experienced developers can change the behaviour of a system, without a comprehensive test suite. Therefore, there is a need for tools that can automatically remove anti-patterns. We will apply meta-heuristics to propose a technique for automated refactoring that improves design quality.",1534-5351,978-1-4799-8469,10.1109/SANER.2015.7081891,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081891,,Software systems;Software maintenance;Measurement;Space exploration;Software engineering;Correlation,software maintenance;software quality,automatic antipattern correction;software maintenance;software design quality;software evolution;antipattern detection;automatic antipattern removal;metaheuristics;automated refactoring,,1,8,,,,,,IEEE,IEEE Conferences
Towards testing variability intensive systems using user reviews,J. L. Rodas; D. M??ndez-Acu?ña; J. A. Galindo; D. Benavides; J. C?­rdenas,"University of Milagro, Ecuador; INRIA - Rennes, France; INRIA - Rennes, France; University of Seville, Spain; University of Milagro, Ecuador",2015 10th Computing Colombian Conference (10CCC),,2015,,,39,46,"Variability intensive systems are software systems that describe a large set of diverse and different configurations that share some characteristics. This high number of configurations makes testing such systems an expensive and error-prone task. For example, in the Android ecosystem we can find up to 24 different valid configurations, thus, making it impossible to test an application on all of them. To alleviate this problem, previous research suggest the selection of a subset of test cases that maximize the changes of finding errors while maximizing the diversity of configurations. Concretely, the proposals focus on the prioritization and selection of tests, so only relevant configurations are tested according to some criterion. In this paper, we envision the use of user reports to prioritize and select meaningful tests. To do this, we explore the use of recommender systems as a possible improvement to the selection of test cases in intensive variability systems.",,978-1-4673-9464-2978-1-4673-9463,10.1109/ColumbianCC.2015.7333410,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333410,Sistemas de alta variabilidd;Modelos de caracter??sticas;Sistemas de recomendaci??n;Android,Androids;Humanoid robots;Testing;Software;Silicon;Electronic mail;Silicon compounds,program testing;recommender systems,variability intensive systems testing;user reviews;software systems;Android ecosystem;recommender systems;test case selection,,,16,,,,,,IEEE,IEEE Conferences
Tracking down high coverage configuration using clustering and fault detection,S. Dhanalakshmi; S. C. Devi,"Software Engineering, Indian Institute of Information Technology, Srirangam, Tiruchirappalli, Tamilnadu, India; Department of Computer Science Engineering, Bharathidasan Institute of Technology campus, Anna University Tiruchirappalli, Tamilnadu, India",2015 Online International Conference on Green Engineering and Technologies (IC-GET),,2015,,,1,5,"Mostly all software systems are highly configured, it has many benefits but there is a difficulty of software testing because there will be unique errors could be hidden in any of the configurations and undergoing testing for each of the configurations will lead to expensive testing and it is also impractical. The dependable systems will have some mechanism for fault tolerance in software testing. If the rate of the fault detection is calculated then the coverage of the configuration can be easily generated. First load the application for which it is going to be tested by using our test case prioritization approach and loading the dataset for the test case for the given application. After this process, need to assign the individual ids for all the test cases in the test case dataset. Also it is able to add the test cases in the dynamic nature. Then to compute the test case prioritization, first built the dependency structure for the test cases. Through the approach get the height and weight matrix for the test cases after this computation the test cases. The cosine similarity values between the test cases. In the similarity values it will show how it is highly related with the other test cases. Thus the clustering approach is introduced for grouping the test cases. These test cases are analyzed for measuring their relevancy and relationship between the test cases using their constrains and the clustering of the test cases is done for the better result in the rate of fault detect. With the Average percentage fault detection the graph is drawn and it shown the high coverage configurations.",,978-1-4673-9781-0978-1-4673-8625,10.1109/GET.2015.7453839,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453839,dependency structure;simularity measure;clustering;fault detection,Software;Fault detection;Software testing;Software engineering;Decision trees;Classification algorithms,configuration management;fault diagnosis;pattern clustering;program testing,high coverage configuration;clustering;software testing;test case prioritization approach;test cases dependency structure;cosine similarity values;average percentage fault detection,,,13,,,,,,IEEE,IEEE Conferences
UPMOA: An improved search algorithm to support user-preference multi-objective optimization,S. Wang; S. Ali; T. Yue; M. Liaaen,"Certus Software V&V Center, Simula Research Laboratory, Oslo, Norway; Certus Software V&V Center, Simula Research Laboratory, Oslo, Norway; Certus Software V&V Center, Simula Research Laboratory, Oslo, Norway; Cisco Systems, Oslo, Norway",2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE),,2015,,,393,404,"Multi-objective search algorithms (e.g., non-dominated sorting genetic algorithm II (NSGA-II)) have been applied extensively to solve various multi-objective optimization problems in software engineering such as problems in testing. However, existing multi-objective algorithms usually treat all the objectives with equivalent priorities and do not provide a mechanism to reflect various user preferences when guiding search. The need to have such a mechanism was observed in one of our industrial projects on applying search algorithms for test optimization of a product line of Videoconferencing Systems (VCSs) called Saturn, where user preferences must be incorporated into optimization objectives, based on domain knowledge of test engineers for VCS testing. To address this, we propose an extension to the most commonly-used multi-objective search algorithm NSGA-II, which has shown promising results with user preferences. We name the extension as User-Preference Multi-Objective Optimization Algorithm (UPMOA), which includes a user preference indicator p and is based on existing weight assignment strategies. We empirically evaluated UPMOA with two industrial problems focusing on optimizing the test execution system for Saturn in Cisco. To assess the performance and scalability of UPMOA, inspired by the two industrial problems, in total we created 64000 artificial problems with 128 different sets of user preferences. The evaluation includes two aspects: 1) Three weight assignment strategies together with UPMOA were empirically evaluated to identify a best weight assignment strategy for p. Results show that the Uniformly Distributed Weights (UDW) strategy can assist UPMOA in achieving the best performance; 2) UPMOA was compared with three representative multi-objective search algorithms (including NSGA-II) and results show that UPMOA significantly outperformed the others and has the ability to solve problems with a wide range of complexity.",,978-1-5090-0406-5978-1-5090-0405,10.1109/ISSRE.2015.7381833,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381833,User-Preference Multi-objective Optimization;Weight Assignment Strategy;Search Algorithms,Search problems;Optimization;Testing;Software algorithms;Fault detection;Genetic algorithms;Software engineering,optimisation;program testing;search problems,UPMOA;improved search algorithm;user-preference multiobjective optimization algorithm;user preference indicator;weight assignment strategies;test execution system;Saturn;Cisco;uniformly distributed weights;UDW strategy;NSGA-II,,3,,,,,,,IEEE,IEEE Conferences
Using Artificial Bee Colony for Code Coverage Based Test Suite Prioritization,P. Konsaard; L. Ramingwong,NA; NA,2015 2nd International Conference on Information Science and Security (ICISS),,2015,,,1,4,"The goal of test suite prioritization is maximizing fault detection and code coverage rate. Several nature inspired optimization algorithms such as Swarm Intelligence (SI) have been studied for the optimization of such problems. The studies revealed the benefits of Artificial Bee Colony (ABC) over other algorithms. ABC and its variations were implemented in software testing areas, test suite prioritization in particular. However, most SI based approaches focus on fault detection ability which is difficult to predict. In this paper, the standard ABC algorithm is used to prioritize test suites based on code coverage. The results reveal that ABC shows promising results and, hence, is a great candidate for prioritizing test suites. It also suggests that a modification to the standard ABC algorithm or combination of ABC and another SI algorithm should yield an even better result.",,978-1-4673-8611,10.1109/ICISSEC.2015.7371038,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371038,,Software algorithms;Optimization;Software testing;Standards;Software;Genetic algorithms,fault diagnosis;optimisation;program testing,artificial bee colony;code coverage based test suite prioritization;fault detection;code coverage rate;nature inspired optimization algorithm;swarm intelligence;ABC algorithm;software testing,,1,31,,,,,,IEEE,IEEE Conferences
Using Defect Taxonomies for Testing Requirements,M. Felderer; A. Beer,University of Innsbruck; Beer Test Consulting,IEEE Software,,2015,32,3,94,101,"Systematic defect management based on bug-tracking systems such as Bugzilla is well established and has been successfully used in many software organizations. Defect management weights the failures observed during test execution according to their severity and forms the basis for effective defect taxonomies. In practice, most defect taxonomies are used only for the a posteriori allocation of testing resources to prioritize failures for debugging. Thus, these taxonomies' full potential to control and improve all the steps of testing has remained unexploited. This is especially the case for testing a system's user requirements. System-level defect taxonomies can improve the design of requirements-based tests, the tracing of defects to requirements, the quality assessment of requirements, and the control of the relevant defect management. So, we developed requirements-based testing with defect taxonomies (RTDT). This approach is aligned with the standard test process and uses defect taxonomies to support all phases of testing requirements. To illustrate this approach and its benefits, we use an example project (which we call Project A) from a public health insurance institution.",0740-7459;1937-4194,,10.1109/MS.2014.56,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799150,requirements-based testing;defect taxonomy;test management;requirements validation;software quality;software engineering,Taxonomy;Software testing;Graphical user interfaces;Requirements engineering;Software engineering;Syntactics,program testing;software quality,systematic defect management;bug-tracking system;Bugzilla;a posteriori allocation;requirements quality assessment;requirements-based testing;defect taxonomies;RTDT,,3,11,,,,,,IEEE,IEEE Journals & Magazines
Using Fuzzy Logic and Symbolic Execution to Prioritize UML-RT Test Cases,E. J. Rapos; J. Dingel,NA; NA,"2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)",,2015,,,1,10,"The relative ease of test case generation associated with model-based testing can lead to an increased number of test cases being identified for any given system; this is problematic as it is becoming near impossible to run (or even generate) all of the possible tests in available time frames. Test case prioritization is a method of ranking the tests in order of importance, or priority based on criteria specific to a domain or implementation, and selecting some subset of tests to generate and run. Some approaches require the generation of all tests, and simply prioritize the ones to be run, however we propose an approach that would prevent unnecessary generation of tests through the use of symbolic execution trees to determine which tests provide the most benefit to coverage of execution. Our approach makes use of fuzzy logic, specifically fuzzy control systems, to prioritize test cases generated from these execution; the prioritization is based on natural language rules about testing priority. Within this paper we present our motivation, some background research, our methodology and implementation, results, and conclusions.",2159-4848,978-1-4799-7125,10.1109/ICST.2015.7102610,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102610,,Fuzzy logic;Testing;Fuzzy sets;Computational modeling;Unified modeling language;Fuzzy control;Natural languages,fuzzy logic;program testing;real-time systems;trees (mathematics);Unified Modeling Language,fuzzy logic;symbolic execution trees;UML-RT test case generation;model-based testing;test case prioritization;fuzzy control system;natural language rules,,1,18,,,,,,IEEE,IEEE Conferences
Using Partition Information to Prioritize Test Cases for Fault Localization,X. Zhang; D. Towey; T. Y. Chen; Z. Zheng; K. Cai,NA; NA; NA; NA; NA,2015 IEEE 39th Annual Computer Software and Applications Conference,,2015,2,,121,126,"Fault Localization Prioritization (FLP) aims at reordering existing test cases so that the location of detected faulty components can be identified earlier, using certain fault localization techniques. Although some researchers have proposed adaptive prioritization strategies with white-box code coverage information, such information may not always be available. In this paper, we address the FLP problem using black-box information derived from partitioning the input domain. Based on the well-known technique of Spectra-Based Fault Localization (SBFL), three test case prioritization strategies are designed following some basic SBFL heuristics. The implementation of these proposed strategies relies only on the partition information, and does not require any test case execution history. Experiments show that our strategies, when compared with pure random selection, result in a faster localization of faulty statements, reducing the number of test case executions required. Here, we analyze the characteristics and merits of the three proposed strategies.",0730-3157,978-1-4673-6564-2978-1-4673-6563,10.1109/COMPSAC.2015.35,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273609,software fault localization;test case prioritization;adaptive strategies,Computers;Software;Conferences,program testing;software fault tolerance,partition information;fault localization prioritization;FLP;test case reordering;adaptive prioritization;white-box code coverage information;black-box information;spectra-based fault localization;test case prioritization strategies;SBFL heuristics,,5,12,,,,,,IEEE,IEEE Conferences
A hybrid approach for test case prioritization and optimization using meta-heuristics techniques,P. Saraswat; A. Singhal,"Department of CSE, ASET, Amity University Uttar Pradesh, Noida, India; Department of CSE, ASET, Amity University Uttar Pradesh, Noida, India",2016 1st India International Conference on Information Processing (IICIP),,2016,,,1,6,"Software testing is a very crucial and important phase for (SDLC) software development life cycle. Software is being tested on its effectiveness for generating good quality software. Regression testing is done by considering the constraints of resources and in this phase optimization of test suite is very important and crucial. This paper mainly aims to make use of hybrid approach of meta-heuristics, It comprises of two algorithms first is genetic algorithm and second is particle swarm optimization. In addition to algorithm the comparison of proposed algorithm hybrid GA_PSO with other optimization algorithms are been done. To validate the research Average Percentage Fault Detection (APFD) metric is used for comparison and fitness evaluation of the proposed algorithm.",,978-1-4673-6984-8978-1-4673-6985,10.1109/IICIP.2016.7975319,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975319,Software testing;regression testing;test case prioritization;test case optimization;genetic algorithm;partucle swarm optimization,Genetic algorithms;Testing;Sociology;Statistics;Fault detection;Software;Optimization,genetic algorithms;particle swarm optimisation;program testing;software development management;software metrics;software quality,test case prioritization;meta-heuristics techniques;software testing;SDLC;software development life cycle;good quality software;phase optimization;test suite;genetic algorithm;particle swarm optimization;GA_PSO;average percentage fault detection metric;APFD,,,20,,,,,,IEEE,IEEE Conferences
A hybrid approach for test case prioritization and selection,D. Silva; R. Rabelo; M. Campanh?œ; P. S. Neto; P. A. Oliveira; R. Britto,"Universidade Federal do Piau??, Departamento de Computa???œo, Teresina, Piau??, Brasil; Universidade Federal do Piau??, Departamento de Computa???œo, Teresina, Piau??, Brasil; Universidade Federal do Piau??, Departamento de Computa???œo, Teresina, Piau??, Brasil; Universidade Federal do Piau??, Departamento de Computa???œo, Teresina, Piau??, Brasil; Instituto Federal do Maranh?œo, Pedreiras, Maranh?œo, Brazil; Department of Software Engineering, Blekinge Institute of Technology, Karlskrona, Sweden",2016 IEEE Congress on Evolutionary Computation (CEC),,2016,,,4508,4515,"Software testing consists in the dynamic verification of the behavior of a program on a set of test cases. When a program is modified, it must be tested to verify if the changes did not imply undesirable effects on its functionality. The rerunning of all test cases can be impossible, due to cost, time and resource constraints. So, it is required the creation of a test cases subset before the test execution. This is a hard problem and the use of standard Software Engineering techniques could not be suitable. This work presents an approach for test case prioritization and selection, based in relevant inputs obtained from a software development environment. The approach uses Software Quality Function Deployment (SQFD) to deploy the features relevance among the system components, Mamdani fuzzy inference systems to infer the criticality of each class and Ant Colony Optimization to select test cases. An evaluation of the approach is presented, using data from simulations with different number of tests.",,978-1-5090-0623-6978-1-5090-0622-9978-1-5090-0624,10.1109/CEC.2016.7744363,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744363,,Correlation;Testing;Minimization;Measurement;Software quality;Fault detection,ant colony optimisation;formal verification;fuzzy reasoning;program testing;software quality,test case prioritization;software testing;dynamic verification;program behavior;resource constraints;test case selection;software development environment;software quality function deployment;SQFD;features relevance;Mamdani fuzzy inference systems;ant colony optimization,,1,27,,,,,,IEEE,IEEE Conferences
A Multi-Objective Technique to Prioritize Test Cases,A. Marchetto; M. M. Islam; W. Asghar; A. Susi; G. Scanniello,independent researchers; independent researchers; independent researchers; Fondazione Bruno Kessler; DiMIE - University of Basilicata,IEEE Transactions on Software Engineering,,2016,42,10,918,940,"While performing regression testing, an appropriate choice for test case ordering allows the tester to early discover faults in source code. To this end, test case prioritization techniques can be used. Several existing test case prioritization techniques leave out the execution cost of test cases and exploit a single objective function (e.g., code or requirements coverage). In this paper, we present a multi-objective test case prioritization technique that determines the ordering of test cases that maximize the number of discovered faults that are both technical and business critical. In other words, our new technique aims at both early discovering faults and reducing the execution cost of test cases. To this end, we automatically recover links among software artifacts (i.e., requirements specifications, test cases, and source code) and apply a metric-based approach to automatically identify critical and fault-prone portions of software artifacts, thus becoming able to give them more importance during test case prioritization. We experimentally evaluated our technique on 21 Java applications. The obtained results support our hypotheses on efficiency and effectiveness of our new technique and on the use of automatic artifacts analysis and weighting in test case prioritization.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2015.2510633,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7362042,Regression testing;requirements;testing;test case prioritization,Software;Fault diagnosis;Testing;Software engineering;Business;Electronic mail;Optimization,formal specification;formal verification;program testing;regression analysis;software fault tolerance;software metrics;source code (software);systems analysis,multiobjective technique;test case prioritization;regression testing;source code fault;software artifact;requirements specification;metric-based approach,,7,64,,,,,,IEEE,IEEE Journals & Magazines
A novel approach for selecting an effective regression testing technique,Priyanka; H. Kumar; N. Chauhan,"YMCAUST, Faridabad, India; YMCAUST, Faridabad, India; YMCAUST, Faridabad, India",2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom),,2016,,,1122,1125,"All software systems need modifications with time, these modifications involve different types or amounts of code modifications in different versions. To validate these modifications many regression testing sessions are needed. But researchers do not have a single regression testing technique that can be used on every version. The objective of this scrutiny is to evolve a methodology that attempts to determine the re-testing technique that would be effective for every re-testing period accounting testing domain and conditions. This methodology is based on Revised Analytical Hierarchy Process (Revised AHP). There are numerous regression testing techniques. But this investigation is limited to test case prioritization techniques only. The result showed that prioritization techniques selected by proposed technique are more efficacious than those used by the forgoing techniques.",,978-9-3805-4421-2978-9-3805-4420-5978-1-4673-9417,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724438,multiple criteria decision making approach;Regression testing;Revised Analytical Hierarchy process;Test case prioritization techniques,Decision making;Matrices;Software testing;Electronic mail;Software systems,analytic hierarchy process;program testing;regression analysis;software maintenance;source code (software);statistical testing,regression testing;software system modifications;code modifications;retesting technique;revised analytical hierarchy process;revised AHP,,,24,,,,,,IEEE,IEEE Conferences
A random and coverage-based approach for fault localization prioritization,X. Zhang; D. Towey; T. Y. Chen; Z. Zheng; K. Cai,"Department of Automatic Control, Beihang University, Beijing; School of Computer Science, The University of Nottingham Ningbo China; Department of Computer Science and Software Engineering, Swinburne University of Technology, Australia; Department of Automatic Control, Beihang University, Beijing; Department of Automatic Control, Beihang University, Beijing",2016 Chinese Control and Decision Conference (CCDC),,2016,,,3354,3361,"Fault Localization Prioritization (FLP) aims to order the execution sequence of test cases so that faulty statements in a faulty program can be localized faster. FLP is an important part of the automation of testing and fault localization in software engineering. The key issue is to identify which test cases can provide most useful information to help locate the faulty statement. Assuming the well-known technique of Spectra-Based Fault Localization (SBFL) is applied, this paper evaluates the quality of a test case based on the characteristics of its statement coverage information. We propose the COverage-based Random (COR) approach to address the FLP problem. Two statement coverage characteristics, the diversity characteristic and the failure-like characteristic, are analyzed and identified as having significant impacts on the effectiveness of fault localization. When using the COR approach, each test case is examined and the degree of each characteristic is measured, with test cases showing high degrees of the characteristics being assigned higher priority for execution. Because of the power of random strategies to improve the robustness of the approach, some random factors in the selection of test cases are included. Empirical studies show that, compared with existing approaches, the COR approach results in a faster localization of faulty statements, reducing the number of necessary test case executions.",1948-9447,978-1-4673-9714-8978-1-4673-9713-1978-1-4673-9715,10.1109/CCDC.2016.7531562,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7531562,software fault localization;COverage-based approaches;test case prioritization,Testing;Entropy;Debugging;Electronic mail;Software;Computer science;Software engineering,fault diagnosis;program testing;software engineering,fault localization prioritization;spectra-based fault localization;statement coverage information;coverage-based random approach;FLP problem;diversity characteristic;failure-like characteristic;COR approach;random factors;software engineering;testing automation,,1,18,,,,,,IEEE,IEEE Conferences
A study of critical success factors on software quality assurance of cloud networking devices,S. Yang; C. Liao,"Dept. of Computer Science and Information Management, Soochow University, Taipei, Taiwan; Dept. of Computer Science and Information Management, Soochow University, Taipei, Taiwan",2016 3rd International Conference on Systems and Informatics (ICSAI),,2016,,,762,767,"In recent years, many more enterprises and organizations are embracing cloud computing for its commercial benefits and competitive advantages. However, these companies have prioritized hardware specifications when evaluating cloud networking devices, often neglecting the importance of software quality as a result. Most cloud networking devices currently offered on the market include embedded software systems, which mean that software and hardware compatibility can be one of the most essential characteristics to look for. In order to investigate the critical success factors (CSFs) for the software quality assurance (QA) of cloud networking devices, this study employed a Plan-Do-Check-Act (PDCA) research framework. Important Software QA factors from relevant IEEE standards and hardware factors related to software quality were then employed in the 4 facets of the PDCA process to conduct a 2-stage analysis. For the first stage of this study, 5 experts were interviewed and asked to complete a survey form and select CSFs that were then used to generate a questionnaire for the second stage where a total of 15 experts and QA engineers were invited to fill out questionnaires. Completed questionnaires were then subject to an analytic hierarchy process (AHP) to calculate the weight and priority orders for each of the CSFs. The overall results of Stage 2 indicate that Cloud devices security testing, Security of cloud networking devices, and Resources and their allocation are the top three orders of all CSFs when experts take a real concern in software QA of cloud networking devices.",,978-1-5090-5521-0978-1-5090-5520-3978-1-5090-5522,10.1109/ICSAI.2016.7811054,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811054,Cloud Computing;Software QA;PDCA;Analytic Hierarchy Process (AHP),Cloud computing;Software quality;IEEE Standards;Hardware;Computational modeling;Testing,analytic hierarchy process;cloud computing;embedded systems;formal specification;IEEE standards;quality assurance;resource allocation;software quality;software standards,software quality assurance;cloud networking devices;cloud computing;hardware specifications;embedded software systems;critical success factors;CSFs;plan-do-check-act;PDCA;IEEE standards;analytic hierarchy process;AHP;cloud networking device security;resource allocation,,,14,,,,,,IEEE,IEEE Conferences
A tool for constrained pairwise test case generation using statistical user profile based prioritization,S. Nakornburi; T. Suwannasart,"Department of Computer Engineering Chulalongkorn University Bangkok, Thailand; Department of Computer Engineering Chulalongkorn University Bangkok, Thailand",2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE),,2016,,,1,6,"Pairwise testing is a wildly used approach in order to reduce the size of test suite and take steps to combinatorial testing problems because of an extensively large number of possible combinations between input parameters and values. In some cases, there will be invalid combinations between input parameters and values if constraints have not been handled. In this paper, we present a pairwise test generation tool called CPTG, a tool to generate test cases for pairwise testing by applying user profile for guiding and prioritizing in order to select optimal input parameters and values which do not depend on individual tester skills and also providing constraint handling solution between input parameters and values. We performed experiments and comparison with other tools. The experimental results of our tool demonstrated that our tool becomes particularly valuable in guiding testing with a maximized reliability by testing the most frequently used of the system and can generate comparable results of the size of the test case set.",,978-1-5090-2033-1978-1-5090-2034,10.1109/JCSSE.2016.7748881,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748881,pairwise testing;test case generation;test case selection;test case prioritization,Testing;Computer science;Software engineering;Software systems;Cloning;Internet,combinatorial mathematics;program testing;statistical analysis,constrained pairwise test case generation;statistical user profile;prioritization;combinatorial testing problems;CPTG;optimal input parameters;optimal input values,,1,20,,,,,,IEEE,IEEE Conferences
ACO based embedded system testing using UML Activity Diagram,V. Panthi; D. P. Mohapatra,"Dept. of Computer Scinece & Engineering, National Insitute of Technology, Rourkela, Odisha, India- 769008; Dept. of Computer Scinece & Engineering, National Insitute of Technology, Rourkela, Odisha, India- 769008",2016 IEEE Region 10 Conference (TENCON),,2016,,,237,242,"This paper proposed a model-based technique for test scenario generation using Activity Diagram (AD). We transform an AD specification into an intermediate graph called Activity Interaction Graph (AIG) using the proposed parser. After that, we apply combination of BFS and DFS algorithms for generating test scenarios. Then, we apply an algorithm called ACOToTSP (Ant Colony Optimization for Test Scenarios Prioritization) algorithm on the generated test scenarios with respect to some decision and concurrent criteria, for prioritizing the test scenarios. This approach generates test scenarios according to forks, Joins, and merge point's strength in the activity diagram.",2159-3450,978-1-5090-2597-8978-1-5090-2596-1978-1-5090-2598,10.1109/TENCON.2016.7847997,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847997,,Unified modeling language;Ant colony optimization;Software testing;Cellular phones;Embedded systems,ant colony optimisation;embedded systems;graph theory;program testing;Unified Modeling Language,embedded system testing;UML activity diagram;model-based technique;test scenario generation;AD specification;intermediate graph;activity interaction graph;AIG;BFS algorithms;DFS algorithms;ACOToTSP;ant colony optimization for test scenarios prioritization,,,19,,,,,,IEEE,IEEE Conferences
An Adaptive Sequence Approach for OOS Test Case Prioritization,J. Chen; L. Zhu; T. Y. Chen; R. Huang; D. Towey; F. Kuo; Y. Guo,NA; NA; NA; NA; NA; NA; NA,2016 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),,2016,,,205,212,"Test case prioritization (TCP) attempts to improve fault detection effectiveness by scheduling important test cases earlier, where important is determined by some criteria and strategy. Adaptive random sequences (ARSs) may be applied to improve the effectiveness of TCP in black-box testing. In this paper, to improve the effectiveness of TCP for object-oriented software, we present an ARS approach based on clustering techniques. In the proposed approach, test cases are clustered according to the number of objects and methods, using two clustering algorithms - K-means and K-medoids. Our proposed sampling strategy can construct ARSs within the clustering framework, constructing two ARS sequences based on the two clustering algorithms, which results in generated test cases with different execution sequences. We also report on experimental studies to verify the proposed approach, with the results showing that our approach can enhance the probability of earlier fault detection, and deliver higher effectiveness than random prioritization.",,978-1-5090-3601-1978-1-5090-3602,10.1109/ISSREW.2016.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789402,Object-oriented software;Adaptive random sequence;Test cases prioritization;Cluster analysis;Test cases selection,Testing;Clustering algorithms;Algorithm design and analysis;Software;Fault detection;Random sequences;Subspace constraints,object-oriented programming;program testing,adaptive sequence approach;OOS test case prioritization;TCP;fault detection effectiveness;test case prioritization;adaptive random sequences;ARS;black-box testing;object-oriented software;clustering techniques;clustering framework,,,31,,,,,,IEEE,IEEE Conferences
An exploration of various quality of service mechanisms in an OpenFlow and software defined networking environment,O. Chato; W. E. S. Yu,"Department of Information Systems and Computer Science, Ateneo De Manila University, Loyola Heights, Quezon City, Philippines; Department of Information Systems and Computer Science, Ateneo De Manila University, Loyola Heights, Quezon City, Philippines",2016 3rd International Conference on Systems and Informatics (ICSAI),,2016,,,768,776,"Technology prevalent in networks today logically detaches the control from the data plane yet physically deploys them on the same device on all network devices. This has negatively resulted in expensive management of disparate and closed network devices and the inability to implement coherent, synchronized, and wide-ranged network policies. Software Defined Networking (SDN) solves these issues by providing an open and virtually centralized network system through cost-effective and gradual network introduction that minimizes investment in infrastructure. The OpenFlow protocol defines communications from the control plane where the logical center of the network, the controller, resides to the packet-routing devices in the data plane and vice-versa. The Mininet SDN emulator mimics real-world SDN network environments for rapid system testing and proof-of-concept experimentation. Quality Of Service (QoS) represents a ripe and proven technology that allows for the enforcement of priorities and assured performance levels on network links and objects. This study examined the application of QoS-based methods to an SDN OpenFlow environment. QoS parameters were utilized to implement rudimentary Class-Based Queuing (CBQ) scheduling algorithms on a custom OpenFlow controller within a Mininet-emulated SDN network topology. Two CBQ algorithms were used in the experiments contrasted by the classes on which they were based. One algorithm is Basic CBQ which modeled scheduling based on the ƒ??Streamingƒ?, ƒ??Burstyƒ?, and ƒ??Catch-Allƒ? traffic types. The other algorithm, Source CBQ, was based on predefined Source-IP address groupings of client hosts. Each Class Profile was broken down in terms of QoS enforcement points at the Leaf Switches and at the Core Switch. Tests were conducted on the topology with these CBQ-based Class Profiles to gather performance data and observe any issues encountered. Experiment results showed that CBQs at the Leaves yielded better overall average bandwidth than CBQs at the Core with Basic CBQ at the Leaves at 2% higher than Basic CBQ at the Core and Source CBQ at the Leaves at 30% higher than Source CBQ at the Core. This indicated that QoS implemented at the Client Leaf Switches rather than at the Core Switch yielded better performance as they were not prone to the same bottlenecks as the Core Switch. Additionally, experiment results seemingly showed that Basic CBQ, whose classes are based on traffic types, yielded optimum performance at less variability as it managed bandwidth better by segmenting traffic and limiting errors.",,978-1-5090-5521-0978-1-5090-5520-3978-1-5090-5522,10.1109/ICSAI.2016.7811055,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811055,Software Defined Networks;SDN;Quality of Service;QoS;Class-Based Queuing;CBQ,Quality of service;Switches;Streaming media;Servers;Bandwidth;Optimization;Software,client-server systems;quality of service;routing protocols;scheduling;software defined networking;telecommunication traffic,quality of service mechanism;software defined networking environment;data plane;OpenFlow protocol;control plane;logical center;packet-routing devices;Mininet SDN emulator;performance levels;network links;network objects;QoS-based methods;SDN OpenFlow environment;class-based queuing scheduling algorithm;CBQ scheduling algorithms;Mininet-emulated SDN network topology;streaming traffic;bursty-traffic;catch-all traffic;source-IP address;client hosts;class profile;QoS enforcement points;CBQ-based class profiles;performance data;overall average bandwidth;source CBQ;client leaf switches;core switch;optimum performance,,2,20,,,,,,IEEE,IEEE Conferences
Application of Mahalanobis-Taguchi Method and 0-1 Programming Method to Cost-Effective Regression Testing,H. Aman; Y. Tanaka; T. Nakano; H. Ogasawara; M. Kawahara,NA; NA; NA; NA; NA,2016 42th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),,2016,,,240,244,"To enhance the cost effectiveness of regression testing, this paper proposes a method for prioritizing test cases. In general, a test case can be evaluated from various different points of view, therefore whether it is worth it to re-run should be discussed using multi criteria. This paper shows that the Mahalanobis-Taguchi (MT) method is a useful way to successfully integrate different evaluations of a test case. Moreover, this paper proposes to use the 0-1 programming method together with the MT method in order to take into account not only the priority of a test case but also its cost to run. The empirical study with 300 test cases for an industrial software system shows that the combination of the MT method and the 0-1 programming method is more cost-effective than other conventional methods.",2376-9505,978-1-5090-2820-7978-1-5090-2821,10.1109/SEAA.2016.29,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7592803,Mahalanobis-Taguchi method;0-1 programming method;regression testing,Testing;Programming;Measurement;Computer aided software engineering;Software;Focusing;History,mathematical programming;program testing;regression analysis,Mahalanobis-Taguchi method;0-1 programming method;cost-effective regression testing;test case prioritization;MT method,,2,12,,,,,,IEEE,IEEE Conferences
Applying Assemble Clustering Algorithm and Fault Prediction to Test Case Prioritization,L. Xiao; H. Miao; W. Zhuang; S. Chen,NA; NA; NA; NA,"2016 International Conference on Software Analysis, Testing and Evolution (SATE)",,2016,,,108,116,"Cluster application is proposed as an efficient approach to improve test case prioritization, Test case in a same cluster are considered to have similar behaviors. In the process of cluster test case, the selection of test case feature and the clusters number have great influence on the clustering results. but to date almost clustering algorithm to improve test case prioritization are selected random clusters number and clustering result are based on one or a few of the code features, the paper propose a new prioritization techniques that not only consider the best clusters number but also produce the best clustering result based on test case multidimensional feature. After clustering, considering the inter-cluster prioritization and intra-cluster prioritization,in order to improve the effectiveness of our approach, the fault prediction value of code corresponding to the test case is used as one of a prioritization weight. Finally,we implemented an empirical studies using an industrial software to illustrate the effectiveness of the test case prioritization techniques.",,978-1-5090-4517-4978-1-5090-4518,10.1109/SATE.2016.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780203,Assemble clustering algorithm;fault prediction;the best clusters number;test case prioritization,Clustering algorithms;Prediction algorithms;Software;Testing;Feature extraction;Software algorithms;Algorithm design and analysis,fault diagnosis;feature extraction;feature selection;pattern clustering;program testing,assemble clustering algorithm;fault prediction;test case prioritization;cluster application;cluster test case;test case feature selection;test case multidimensional feature;industrial software,,2,32,,,,,,IEEE,IEEE Conferences
Automated comparison of X-ray images for cargo scanning,W. Visser; A. Schwaninger; D. Hardmeier; A. Flisch; M. Costin; C. Vienne; F. Sukowski; U. Hassler; I. Dorion; A. Marciano; G. Koomen; M. Slegt; A. C. Canonica,"Center for Adaptive Security Research and Applications (CASRA), 8050 Z?¬rich, Switzerland; Center for Adaptive Security Research and Applications (CASRA), 8050 Z?¬rich, Switzerland; Center for Adaptive Security Research and Applications (CASRA), 8050 Z?¬rich, Switzerland; Center for X-ray Analytics, Swiss Federal Laboratories for Materials Science and Technology (EMPA), 8600 D?¬bendorf, Switzerland; Department of Imaging & Simulation for Non-Destructive Testing, CEA-LIST, 91191 Gif sur Yvette, France; Department of Imaging & Simulation for Non-Destructive Testing, CEA-LIST, 91191 Gif sur Yvette, France; Development Center for X-ray Technology (EZRT), Fraunhofer Institute for Integrated Circuits IIS, 90768 F?¬rth, Germany; Development Center for X-ray Technology (EZRT), Fraunhofer Institute for Integrated Circuits IIS, 90768 F?¬rth, Germany; Smiths Detection (SH), 94405 Vitry sur Seine, France; Smiths Detection (SH), 94405 Vitry sur Seine, France; Dutch Customs Laboratory, Dutch Tax and Customs Administration (DTCA), 6401 Heerlen, The Netherlands; Dutch Customs Laboratory, Dutch Tax and Customs Administration (DTCA), 6401 Heerlen, The Netherlands; Swiss Federal Customs Administration (FCA), 3003 Bern, Switzerland",2016 IEEE International Carnahan Conference on Security Technology (ICCST),,2016,,,1,8,"Customs administrations are responsible for the enforcement of fiscal integrity and security of movements of goods across land and sea borders. In order to verify whether the transported goods match the transport declaration, X-ray imaging of containers is used at many customs site worldwide. The main objective of the research and development project ƒ??Automated Comparison of X-ray Images for Cargo Scanning (ACXIS)ƒ?, which is funded by the European 7<sup>th</sup>Framework Program, is to improve the efficiency and effectiveness of the inspection procedures of cargo at customs using X-ray technology. The current inspection procedures are reviewed to identify risks, catalogue illegal cargo, and prioritize detection scenarios. Based on these results, we propose an integrated solution that provides automation, information exchange between customs administrations, and computer-based training modules for customs officers. Automated target recognition (ATR) functions analyze the X-ray image after a scan is made to detect certain types of goods such as cigarettes, weapons and drugs in the freight or container. Other helpful information can also be provided, such as the load homogeneity, total or partial weight, or the number of similar items. The ATR functions are provided as an option to the user. The X-ray image is transformed into a manufacturer-independent format through geometrical and spectral corrections and stored into a database along with the user feedback and other related data. This information can be exchanged with similar systems at other sites, thus facilitating information exchange between customs administrations. The database is seeded with over 30'000 examples of legitimate and illegal goods. These examples are used by the ATR functions through machine learning techniques, which are further strengthened by the information exchange. In order to improve X-ray image interpretation competency of human operators (customs officers), a computer-based training software is developed that simulates these new inspection procedures. A study is carried out to validate the effectiveness and efficiency of the computer-based training as well as the implemented procedures. Officers from the Dutch and Swiss Customs administrations partake in the study, covering both land and sea borders.",2153-0742,978-1-5090-1072-1978-1-5090-1073,10.1109/CCST.2016.7815714,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815714,Customs border control;cargo inspection;security screening;X-ray screening;automated target recognition;X-ray image analysis;image standardization;simulation;computer-based training,,computer based training;inspection;learning (artificial intelligence);national security;object recognition;X-ray imaging,X-ray imaging;custom administration;good movement security;good movement integrity;land border;sea border;ACXIS;automated comparison of X-ray image for cargo scanning;European 7thframework program;cargo inspection procedure effectiveness;cargo inspection procedure efficiency improvement;prioritize detection scenario;risk identification;catalogue illegal cargo;information exchange;computer-based training modules;automated target recognition function;ATR function;spectral correction;geometrical correction;machine learning technique;X-ray image interpretation competency improvement;Dutch;Swiss custom administration,,1,37,,,,,,IEEE,IEEE Conferences
Automated testcase generation for software quality assurance,D. Nirmala; T. LathaMaheswari,"Department of Computer Science and Engineering, Sri Krishna College of Engineering and Technology, Coimbatore, India; Department of Computer Science and Engineering, Sri Krishna College Of Engineering and Technology, Coimbatore, India",2016 10th International Conference on Intelligent Systems and Control (ISCO),,2016,,,1,6,"The overall venture of the software engineering is to guarantee the delivery of high quality software to the client. To certify high quality software, it is required to test software. Testing is a decisive constituent of software engineering. In software testing there are number of underlying issues like effective generation of test cases, prioritisation of test cases which need to be tackled. This automated test case generation mainly depends on these four aspects: test strategy, test case generation, test execution and test evaluation. Test strategy is a collection of events that determines the testing approach to be followed by the testing team. The test case generation refers to the generation of testcases based on the certain application. The test execution briefs about the execution of those tests then comparing the expected result with actual result. The test evaluation investigates the test cases and helps us to generate test report and software quality assurance report automatically. The intention of producing this tool is to generate test cases automatically and to decrease the cost of testing in addition to accumulate the time of deriving test cases physically. Hence this system helps to improve overall quality of the software.",,978-1-4673-7807-9978-1-4673-7808,10.1109/ISCO.2016.7727003,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7727003,Software Quality Assurance Report;Software Testing;Test Cases;Test Evaluation;Test Execution;Test Report;Test Strategy,Automation;Software quality;Software testing;Computer bugs;Analytical models,program testing;quality assurance;software quality,software quality assurance;automated test case generation;software engineering;software testing;test execution;test evaluation,,,7,,,,,,IEEE,IEEE Conferences
Automatic Reproducible Crash Detection,Y. Gu; J. Xuan; T. Qian,NA; NA; NA,"2016 International Conference on Software Analysis, Testing and Evolution (SATE)",,2016,,,48,53,"Crash reproduction, which spends much time of developers in reading and understanding source code, is a crucial yet time-consuming task in program debugging. To reduce the time and resource cost, automatic techniques of test generation have been proposed. These techniques aim to automatically generate test cases to reproduce the scenario of a crashed project. Unfortunately, due to the lack of a detailed comprehension of the source code, a generated test case may fail in reproducing an expected crash. In this paper, we propose an automatic approach to reproducible bug detection. This approach predicts whether a crash is difficult to reproduce or not via training a classifier based on historical reproducible crash data. If a crash is difficult to reproduce, it is better to assign the crash to a developer, instead of using an automatic technique of test generation. Our work can help to prioritize crashes and to save the cost of developers. Preliminary experiments show that our approach effectively detects reproducible crashes via evaluating 45 crashes.",,978-1-5090-4517-4978-1-5090-4518,10.1109/SATE.2016.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780193,crash reproduction;machine learning;test generation,Feature extraction;Computer bugs;Software;Java;Training;Manuals,program debugging;program testing;source code (software),automatic reproducible crash detection;crash reproduction;source code;program debugging;automatic test generation techniques;reproducible bug detection,,,17,,,,,,IEEE,IEEE Conferences
Automatically Learning Semantic Features for Defect Prediction,S. Wang; T. Liu; L. Tan,NA; NA; NA,2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),,2016,,,297,308,"Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1.",1558-1225,978-1-4503-3900-1978-1-5090-2071,10.1145/2884781.2884804,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886912,defect prediction;feature generation;deep learning,Semantics;Feature extraction;Training;Predictive models;Buildings;Syntactics;Data models,belief networks;neural nets;program diagnostics;programming language semantics;public domain software;source code (software),semantic feature learning;software defect prediction;representation-learning algorithm;deep learning;programs semantic representation;source code;deep belief network;token vectors;abstract syntax trees;DBN;AST;open source projects;within-project defect prediction;cross-project defect prediction;WPDP;CPDP,,21,70,,,,,,IEEE,IEEE Conferences
Automation Architecture for Bayesian Network Based Test Case Prioritization and Execution,E. Ufuktepe; T. Tuglular,NA; NA,2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC),,2016,2,,52,57,"An automation architecture for Bayesian Network based test case prioritization is designed for software written in Java programming language following the approach proposed by Mirarab and Tahvildari [2]. The architecture is implemented as an integration of a series of tools and called Bayesian Network based test case prioritization and execution platform. The platform is triggered by a change in the source code, then it collects necessary information to be supplied to Bayesian Network and uses Bayesian Network evaluation results to run high priority unit tests.",0730-3157,978-1-4673-8845-0978-1-4673-8846,10.1109/COMPSAC.2016.71,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552177,testing;test prioritization;Bayesian Networks;Unit Testing,Bayes methods;Software;Measurement;Automation;Testing;Unified modeling language;Fault detection,belief networks;Java;program testing,Bayesian network;test case prioritization;test case execution;software written;Java programming language;high priority unit tests,,,14,,,,,,IEEE,IEEE Conferences
AVISAR - a three tier architectural framework for the testing of Object Oriented Programs,P. Vats; A. Gossain,"New Delhi, India; USICT, GGSIPU, New Delhi, India","2016 Second International Innovative Applications of Computational Intelligence on Power, Energy and Controls with their Impact on Humanity (CIPECH)",,2016,,,23,28,"In this research paper, we have proposed a three tier conceptual framework AVISAR for the testing of Object Oriented Programs. We have devised the proposed framework into three levels: Requirements Modeling, Test Case Generation and Effort Estimation. At the first level, we have used the Extend relationship of the Use Case to capture the events generated by the Classes during message passing between them through their objects. These events were captured by an Object Instantiator using Event Templates and further given to a Test case scenario generator, which generates the test cases. At the second level, the test cases will be generated based on the event templates that are used for capturing the events generated by the Extend relationship of the Use cases. Further, we have proposed a Genetic Algorithm (GA) for the effective test case prioritization to ensure maximum code coverage. At the third level, effort estimation, we have used the Cyclometric Complexity and Token count to perform the effort estimation for the Software under Test (SUT).",,978-1-4673-9080-4978-1-4673-9081,10.1109/CIPECH.2016.7918730,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918730,Object oriented;Genetic Algorithm;Fitness function;Complexity,Testing;Software;Genetic algorithms;Generators;Java;Optimization;Object oriented modeling,genetic algorithms;object-oriented programming;program testing,AVISAR;three-tier architectural framework;object-oriented program testing;requirement modeling;test case generation;effort estimation;object instantiator;genetic algorithm;GA;cyclometric complexity;software under test;SUT,,,19,,,,,,IEEE,IEEE Conferences
Comparing White-Box and Black-Box Test Prioritization,C. Henard; M. Papadakis; M. Harman; Y. Jia; Y. Le Traon,NA; NA; NA; NA; NA,2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),,2016,,,523,534,"Although white-box regression test prioritization has been well-studied, the more recently introduced black-box prioritization approaches have neither been compared against each other nor against more well-established white-box techniques. We present a comprehensive experimental comparison of several test prioritization techniques, including well-established white-box strategies and more recently introduced black-box approaches. We found that Combinatorial Interaction Testing and diversity-based techniques (Input Model Diversity and Input Test Set Diameter) perform best among the black-box approaches. Perhaps surprisingly, we found little difference between black-box and white-box performance (at most 4% fault detection rate difference). We also found the overlap between black- and white-box faults to be high: the first 10% of the prioritized test suites already agree on at least 60% of the faults found. These are positive findings for practicing regression testers who may not have source code available, thereby making white-box techniques inapplicable. We also found evidence that both black-box and white-box prioritization remain robust over multiple system releases.",1558-1225,978-1-4503-3900-1978-1-5090-2071,10.1145/2884781.2884791,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886931,Regression Testing;White-box;Black-box,Testing;Fault detection;Robustness;Flexible printed circuits;Software;Servers;Instruments,program testing;regression analysis,black-box test prioritization;white-box regression test prioritization;black-box prioritization approach;combinatorial interaction testing;diversity-based techniques;input model diversity;input test set diameter,,17,69,,,,,,IEEE,IEEE Conferences
Conc-iSE: Incremental symbolic execution of concurrent software,S. Guo; M. Kusano; C. Wang,"Department of ECE, Virginia Tech, Blacksburg, VA, USA; Department of ECE, Virginia Tech, Blacksburg, VA, USA; Department of CS, University of Southern California, Los Angeles, CA, USA",2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE),,2016,,,531,542,"Software updates often introduce new bugs to existing code bases. Prior regression testing tools focus mainly on test case selection and prioritization whereas symbolic execution tools only handle code changes in sequential software. In this paper, we propose the first incremental symbolic execution method for concurrent software to generate new tests by exploring only the executions affected by code changes between two program versions. Specifically, we develop an inter-thread and inter-procedural change-impact analysis to check if a statement is affected by the changes and then leverage the information to choose executions that need to be re-explored. We also check if execution summaries computed in the previous program can be used to avoid redundant explorations in the new program. We have implemented our method in an incremental symbolic execution tool called Conc-iSE and evaluated it on a large set of multithreaded C programs. Our experiments show that the new method can significantly reduce the overall symbolic execution time when compared with state-of-the-art symbolic execution tools such as KLEE.",,978-1-4503-3845-5978-1-5090-5571,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582788,Symbolic execution;Concurrency;Partial order reduction;Weakest precondition,Software;Testing;Concurrent computing;Algorithm design and analysis;Software algorithms;Computer bugs;Programming,C language;concurrency (computers);multi-threading;program debugging;program testing,Conc-iSE;concurrent software incremental symbolic execution;software updates;bugs;code bases;regression testing tools;test case selection;test case prioritization;sequential software;program versions;interthread change-impact analysis;interprocedural change-impact analysis;execution summaries;multithreaded C programs,,,49,,,,,,IEEE,IEEE Conferences
Concolic test generation for PLC programs using coverage metrics,D. Bohlender; H. Simon; N. Friedrich; S. Kowalewski; S. Hauck-Stattelmann,"Informatik 11 - Embedded Software, RWTH Aachen University, Germany; Informatik 11 - Embedded Software, RWTH Aachen University, Germany; Informatik 11 - Embedded Software, RWTH Aachen University, Germany; Informatik 11 - Embedded Software, RWTH Aachen University, Germany; ABB Corporate Research Germany, Ladenburg, Germany",2016 13th International Workshop on Discrete Event Systems (WODES),,2016,,,432,437,"This paper presents a technique for fully automated generation of test cases for PLC programs adhering to the IEC 61131-3 standard. While previous methods strive for completeness and therefore struggle with the state explosion we pursue a symbolic execution based approach, dropping completeness but nevertheless achieving similar or even better results in practice. The core component is a symbolic execution engine which chooses the next state to execute, handles constraints emerging during the execution and derives respective test vectors leading to a state. To make for a high coverage of the generated tests, we adopt techniques from concolic testing, allow for use of heuristics to prioritise promising states but also merge states to alleviate the path explosion. We exploit peculiarities of PLC semantics to determine reasonable merge-points and unlike similar approaches even handle unreachable code. To examine the feasibility of our technique we evaluate it on function blocks used in industry.",,978-1-5090-4190,10.1109/WODES.2016.7497884,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7497884,,Concrete;Measurement;Explosions;Semantics;Merging;Testing;Software,control engineering computing;IEC standards;industrial control;program testing;programmable controllers;software metrics,concolic test generation;PLC programs;coverage metrics;fully automated test case generation;IEC 61131-3 standard;state explosion;symbolic execution based approach;completeness;symbolic execution engine;concolic testing;path explosion;PLC semantics;merge-points;unreachable code handling;function blocks,,6,21,,,,,,IEEE,IEEE Conferences
Customized Regression Testing Using Telemetry Usage Patterns,J. Anderson; H. Do; S. Salem,NA; NA; NA,2016 IEEE International Conference on Software Maintenance and Evolution (ICSME),,2016,,,572,581,"Pervasive telemetry in modern applications is providing new possibilities in the application of regression testing techniques. Similar to how research in bioinformatics is leading to personalized medicine, tailored to individuals, usage telemetry in modern software allows for custom regression testing, tailored to the usage patterns of an installation. By customizing regression testing based on software usage, the effectiveness of regression testing techniques can be greatly improved, leading to reduced testing costs and enhanced detection of defects that are most important to that customer. In this research, we introduce the concept of fingerprinting software usage patterns through telemetry. We provide various algorithms tocompute fingerprints and conduct an empirical study that shows that fingerprints are effective in identifying distinct usage patterns. Further, we discuss how usage fingerprints can be used to improve regression test prioritization run time by over 30 percent compared to traditional prioritization techniques.",,978-1-5090-3806-0978-1-5090-3807,10.1109/ICSME.2016.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816511,regression testing;telemetry,Testing;Telemetry;Computer bugs;Software as a service;Companies;Software algorithms,program testing;regression analysis,regression testing technique;telemetry usage pattern;pervasive telemetry;bioinformatics;software usage;software usage pattern fingerprinting;regression test prioritization,,,17,,,,,,IEEE,IEEE Conferences
Data flow based quality testing approach using ACO for component based software development,N. Prajapati; N. Kumar,"Department of Computer Science and Engineering, Noida Institute of Engineering and Technology, Greater Noida, India; Department of Electrical &amp; Electronics Engineering, Krishna Institute of Engineering and Technology, Ghaziabad, India","2016 International Conference on Computing, Communication and Automation (ICCCA)",,2016,,,807,812,"Component-based Software Engineering (CBSE) has been centered around advancements identified with configuration and implementation of software components and frameworks assembled from software components. Quality Assurance (QA) for CBSE is a new subject in the software development research area. In this paper an enhanced data flow based QA model is presented for CBSE by employing the Ant Colony Optimization (ACO)algorithm to optimize the given code for automatic generation and prioritization of optimal path in decision to decision Control Flow Graph (CFG), which results an enhanced testing phase for QA model with reduced complexity. After that, proposed ACO based approach is also utilized for the generation of test data to satisfy the generated set of paths. This paper additionally exhibits the proposed approach applying it in a program module. Results show that a better testing is achieved by applying proposed ACO based scheme on component based software. Proposed approach ensures full software coverage with least redundancy.",,978-1-5090-1666-2978-1-5090-1667,10.1109/CCAA.2016.7813850,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813850,path testing;optimal path;component based software development;quality assurance;ant colony optimization,Software testing;Software systems;Ant colony optimization;Optimization;Automation,ant colony optimisation;data handling;graph theory;quality assurance;set theory;software engineering,data flow based quality testing;ACO;component based software development;component based software engineering;CBSE;software component configuration;software component implementation;quality assurance;enhanced data flow based QA model;ant colony optimization algorithm;control flow graph;CFG,,,26,,,,,,IEEE,IEEE Conferences
Database design for error searching system based on keyword priority,F. Yang; Z. Dong; Z. Liu,"Academy of Equipment, NO.1 Bayi Road, Beijing, China; Academy of Equipment, NO.1 Bayi Road, Beijing, China; Academy of Equipment, NO.1 Bayi Road, Beijing, China",2016 3rd International Conference on Systems and Informatics (ICSAI),,2016,,,526,530,"Because there are too many types of errors occurred in a multi-software integrated platform, such as Integrated Decision Information System (IDIS). It is urgent to design an error searching system to solve all different problems. However, those errors belong to different stages like setup, configuration, and operation, or those errors may occurred in different services, applications, or IP ports, or may be happened in different system software, different version of software, and those errors are also can be classified into different types. The new requirement on the design of database has been announced, that it has to locate the error, find out the reason of this error, as well as the corresponding solution. Also, it requires to provide the location, phase, etc. of an error. For a multi-software integrated platform, this paper proposed a database design for error searching system based on keyword priority. This DB made the correspondence among error, the reason of the error, and corresponding solution, and put them to different categories in terms of their characteristics, such that it is easy to manage, search, and use. This method treats those characteristics as keywords with higher priority, which are correspondent with errors, reasons, and solutions, respectively. While the keywords extracted from errors, reasons, and solutions are treated as keywords with lower priority. Keywords with different priorities can all be used as index to search errors, reasons, and solutions, while keywords with higher priority can be used to filter the searching results finer, and make the searching results more accurate. On one hand, the database design method based on keyword priority has simplified the logical structure of the database. On the other hand, users do not only search the reason and solution of an error, but also can find out the accurate information of the error, such as in which stage, layer, software, or categories, etc. the error has happened. The data of the database is complete, which has been provided by 500 technicians who had found thousands of errors. Those errors haven been added to the DB after tests to make the DB more complete.",,978-1-5090-5521-0978-1-5090-5520-3978-1-5090-5522,10.1109/ICSAI.2016.7811011,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811011,database design;search engine;keyword priority;error locating,Databases;Search problems;Software;Vehicles;Sorting;Prototypes,database management systems;formal logic;software engineering,database design;error searching system;keyword priority;multisoftware integrated platform;integrated decision information system;IDIS;keyword extraction;logical structure,,2,,,,,,,IEEE,IEEE Conferences
Diversity-Aware Mutation Adequacy Criterion for Improving Fault Detection Capability,D. Shin; S. Yoo; D. Bae,NA; NA; NA,"2016 IEEE Ninth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",,2016,,,122,131,"Many existing testing techniques adopt diversity as an important criterion for the selection and prioritization of tests. However, mutation adequacy has been content with simply maximizing the number of mutants that have been killed. We propose a novel mutation adequacy criterion that considers the diversity in the relationship between tests and mutants, as well as whether mutants are killed. Intuitively, the proposed criterion is based on the notion that mutants can be distinguished by the sets of tests that kill them. A test suite is deemed adequate by our criterion if the test suite distinguishes all mutants in terms of their kill patterns. Our hypothesis is that, simply by using a stronger adequacy criterion, it is possible to improve fault detection capabilities of mutation-adequate test suites. The empirical evaluation selects tests for real world applications using the proposed mutation adequacy criterion to test our hypothesis. The results show that, for real world faults, test suites adequate to our criterion can increase the fault detection success rate by up to 76.8 percentage points compared to test suites adequate to the traditional criterion.",,978-1-5090-3674-5978-1-5090-3675,10.1109/ICSTW.2016.37,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528954,Mutation testing;adequacy criteria;diversity,Fault detection;Conferences;Software testing;Electronic mail;Pathology;Subspace constraints,fault diagnosis;fault tolerant computing;program testing,diversity-aware mutation adequacy criterion;fault detection capability;mutants;kill patterns;mutation-adequate test suites;fault detection success rate,,1,29,,,,,,IEEE,IEEE Conferences
Do We Have a Chance to Fix Bugs When Refactoring Code Smells?,W. Ma; L. Chen; Y. Zhou; B. Xu,NA; NA; NA; NA,"2016 International Conference on Software Analysis, Testing and Evolution (SATE)",,2016,,,24,29,"Code smells are used to describe code structures that may cause detrimental effects on software and should be refactored. Previous studies show that some code smells have significant effect on faults. However, how to refactor code smells to reduce bugs still needs more concern. We investigate the possibility of prioritizing code smell refactoring with the help of fault prediction results. We also investigate the possibility of improving the performance of fault prediction by using code smell detection results. We use Cohen's Kappa statistic to report agreements between results of code smell detections and fault predictions. We use fault prediction result as an indicator to guide code smell refactoring. Our results show that refactoring Blob, Long Parameter List, and Refused Parent Be Request may have a good chance to detect and fix bugs, and some code smells are particularly useful for improving the recall of fault prediction.",,978-1-5090-4517-4978-1-5090-4518,10.1109/SATE.2016.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780189,Code smell;refactoring;refactoring prioritization;fault prediction,Computer bugs;Measurement;Software;Object oriented modeling;Predictive models;Complexity theory;Logistics,program debugging;software fault tolerance;software maintenance;software performance evaluation,bugs;code structures;code smell detection;Kappa statistic;code smell refactoring;Refused Parent Be Request;Long Parameter List;Blob;fault prediction performance improvement,,1,35,,,,,,IEEE,IEEE Conferences
Dynamic Integration Test Selection Based on Test Case Dependencies,S. Tahvili; M. Saadatmand; S. Larsson; W. Afzal; M. Bohlin; D. Sundmark,NA; NA; NA; NA; NA; NA,"2016 IEEE Ninth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",,2016,,,277,286,"Prioritization, selection and minimization of test cases are well-known problems in software testing. Test case prioritization deals with the problem of ordering an existing set of test cases, typically with respect to the estimated likelihood of detecting faults. Test case selection addresses the problem of selecting a subset of an existing set of test cases, typically by discarding test cases that do not add any value in improving the quality of the software under test. Most existing approaches for test case prioritization and selection suffer from one or several drawbacks. For example, they to a large extent utilize static analysis of code for that purpose, making them unfit for higher levels of testing such as integration testing. Moreover, they do not exploit the possibility of dynamically changing the prioritization or selection of test cases based on the execution results of prior test cases. Such dynamic analysis allows for discarding test cases that do not need to be executed and are thus redundant. This paper proposes a generic method for prioritization and selection of test cases in integration testing that addresses the above issues. We also present the results of an industrial case study where initial evidence suggests the potential usefulness of our approach in testing a safety-critical train control management subsystem.",,978-1-5090-3674-5978-1-5090-3675,10.1109/ICSTW.2016.14,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7528974,Software testing;Integration testing;Test selection;Test prioritization;Fuzzy;AHP;Optimization,Logic gates;Conferences;Software testing;Communication channels;Interviews;Documentation,program testing;railway engineering;safety-critical software;software process improvement;software quality;system monitoring,dynamic integration test selection;test case dependencies;test case selection;test case minimization;software testing;test case prioritization;fault detection likelihood estimation;software quality improvement;static code analysis;dynamic analysis;safety-critical train control management subsystem testing,,4,36,,,,,,IEEE,IEEE Conferences
Effect of Time Window on the Performance of Continuous Regression Testing,D. Marijan; M. Liaaen,NA; NA,2016 IEEE International Conference on Software Maintenance and Evolution (ICSME),,2016,,,568,571,"Test prioritization is an effective technique used to reduce the amount of work required to support regression testing in continuous integration development. It aims at finding an optimal order of tests that can detect regressions faster, potentially increasing the frequency of software releases. Prioritization techniques based on test execution history use the results of preceding executions to determine an optimal order of regression tests in the succeeding test executions. In this paper, we investigate how can execution history be optimally used to increase the effectiveness of regression test prioritization. We analyze the effect of history time window on the fault detection effectiveness of prioritized regression tests. We report an experimental study using a data set from Cisco. The results suggest that varying the size of the window can considerably change the performance of regression testing. Our findings will potentially help developers and test teams in adjusting test prioritization techniques for achieving higher cost-effectiveness in continuous regression testing.",,978-1-5090-3806-0978-1-5090-3807,10.1109/ICSME.2016.77,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7816510,test prioritization;regression testing;continuous integration,Testing;History;Fault detection;Context;Manuals;Industries;Software,program testing;software engineering,time window;regression testing;test prioritization;continuous integration development;CI development;test execution history,,3,9,,,,,,IEEE,IEEE Conferences
Effectiveness of prioritization of test cases based on Faults,S. Nayak; C. Kumar; S. Tripathi,"Department of Computer Science and Engineering, Indian School of Mines, Dhanbad, India; Department of Computer Science and Engineering, Indian School of Mines, Dhanbad, India; Department of Computer Science and Engineering, Indian School of Mines, Dhanbad, India",2016 3rd International Conference on Recent Advances in Information Technology (RAIT),,2016,,,657,662,"Regression testing (RT) is an expensive activity. It is applied on a modified program to enhance confidence and reliability by ensuring that the changes are accurately true and have not affected the unmodified portions of the SUT. Due to limited resources, it is not practical to re-run each test cases (TC). To improve the regression testing's effectiveness, the TCs should be arranged according to some objective function or criteria. Test case prioritization (TCP) arranges TCs in an order for execution that enhances their effectiveness by satisfying some testing goals. The highest priority assigned to TCs must execute before the TCs with low priority by virtue of some performance goal. Numerous goals are possible to achieve of which one such goal is rate of fault detection (RFT) in which the faults are surfaced as quickly as possible within the testing process. In this paper, a novel technique is suggested to prioritize the TCs that increase its effectiveness in detecting faults. The effectiveness of the proposed method is compared and matched with other prioritization approaches with the help of Average Percentage of Fault Detection (APFD) metric from which charts have been prepared.",,978-1-4799-8579-1978-1-4799-8580,10.1109/RAIT.2016.7507977,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507977,Regression Testing;TCP;APFD;Severity of Faults,Measurement;Software;Algorithm design and analysis;Information technology;Fault detection;Software testing,program testing;regression analysis,regression testing;test case prioritization;average percentage of fault detection,,,17,,,,,,IEEE,IEEE Conferences
Empirical Evaluation of Cross-Release Effort-Aware Defect Prediction Models,K. E. Bennin; K. Toda; Y. Kamei; J. Keung; A. Monden; N. Ubayashi,NA; NA; NA; NA; NA; NA,"2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)",,2016,,,214,221,"To prioritize quality assurance efforts, various fault prediction models have been proposed. However, the best performing fault prediction model is unknown due to three major drawbacks: (1) comparison of few fault prediction models considering small number of data sets, (2) use of evaluation measures that ignore testing efforts and (3) use of n-fold cross-validation instead of the more practical cross-release validation. To address these concerns, we conducted cross-release evaluation of 11 fault density prediction models using data sets collected from 2 releases of 25 open source software projects with an effort-aware performance measure known as Norm(P<sub>opt</sub>). Our result shows that, whilst M5 and K* had the best performances, they were greatly influenced by the percentage of faulty modules present and size of data set. Using Norm(P<sub>opt</sub>) produced an overall average performance of more than 50% across all the selected models clearly indicating the importance of considering testing efforts in building fault-prone prediction models.",,978-1-5090-4127-5978-1-5090-4128,10.1109/QRS.2016.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589801,fault-density estimation;empirical study;open sourceproject;crossversionprediction;Demsar's significance diagram,Predictive models;Testing;Data models;Measurement;Computational modeling;Software;Computer science,program testing;public domain software;quality assurance;software quality,cross-release effort-aware defect prediction models;quality assurance;n-fold cross-validation;fault density prediction models;open source software projects;effort-aware performance measure;testing efforts;fault-prone prediction models,,6,31,,,,,,IEEE,IEEE Conferences
Enhanced Weighted Method for Test Case Prioritization in Regression Testing Using Unique Priority Value,A. Ammar; S. Baharom; A. A. A. Ghani; J. Din,NA; NA; NA; NA,2016 International Conference on Information Science and Security (ICISS),,2016,,,1,6,"Regression testing is an integral and expensive part in software testing. To reduce its effort, test case prioritization approaches were proposed. The problem with most of the existing approaches is the random ranking of test cases with equal weight. In this paper, an enhanced weighted method to prioritize the full test suite without using random ranking is presented. In addition, a controlled experiment was executed to evaluate the effectiveness of the proposed method. The results show an improved performance in terms of prioritizing test cases and recording higher APFD values over the original weighted method. In future, a larger experiment would be executed to generalize the results.",,978-1-5090-5493-0978-1-5090-5494,10.1109/ICISSEC.2016.7885851,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885851,,Software;Software testing;Minimization;Sorting;Computer science;Information technology,program testing,test case prioritization;regression testing;unique-priority value;software testing;random test case ranking;enhanced weighted method;test suite prioritization;APFD values,,1,21,,,,,,IEEE,IEEE Conferences
Enhancing Test Case Prioritization in an Industrial Setting with Resource Awareness and Multi-objective Search,S. Wang; S. Ali; T. Yue; ??. Bakkeli; M. Liaaen,NA; NA; NA; NA; NA,2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C),,2016,,,182,191,"Test case prioritization is an essential part of test execution systems for large organizations developing software systems in the context that their software versions are released very frequently. They must be tested on a variety of compatible hardware with different configurations to ensure correct functioning of a software version on a compatible hardware. In practice, test case execution must not only execute cost-effective test cases in an optimal order, but also optimally allocate required test resources, in order to deliver high quality software releases. To optimize the current test execution system for testing software releases developed for Videoconferencing Systems (VCSs) at Cisco, Norway, in this paper, we propose a resource- aware multi-objective optimization solution with a fitness function defined based on four cost-effectiveness measures. In this context, a set of software releases must be tested on a set of compatible VCS hardware (test resources) by executing a set of cost-effective test cases in an optimal order within a given test cycle constrained by maximum allowed time budget and maximum available test resources. We empirically evaluated seven search algorithms regarding their performance and scalability by comparing with the current practice (random ordering (RO)). The results show that the proposed solution with the best search algorithm (i.e., Random-Weighted Genetic Algorithm) improved the current practice by reducing on average 40.6% of time for test resource allocation and test case execution, improved test resource usage on average by 37.9% and fault detection on average by 60%.",,978-1-4503-4205-6978-1-5090-2245,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883302,Test Case Prioritization;Multi-objective Optimization;Search,Search problems;Software;Classification algorithms;Hardware;Testing;Fault detection;Time measurement,optimisation;program testing;resource allocation;search problems;software cost estimation;software fault tolerance;software quality;teleconferencing;video communication,test case prioritization;resource awareness;multiobjective search;software system development;software versions;cost-effective test execution;optimal test resource allocation;high quality software releases;videoconferencing systems;Cisco;Norway;resource-aware multiobjective optimization;fitness function;cost-effectiveness measures;VCS hardware;search algorithms;improved test resource usage;fault detection,,1,27,,,,,,IEEE,IEEE Conferences
Experience Report: Automated System Level Regression Test Prioritization Using Multiple Factors,P. E. Strandberg; D. Sundmark; W. Afzal; T. J. Ostrand; E. J. Weyuker,NA; NA; NA; NA; NA,2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE),,2016,,,12,23,"We propose a new method of determining an effective ordering of regression test cases, and describe its implementation as an automated tool called SuiteBuilder developed by Westermo Research and Development AB. The tool generates an efficient order to run the cases in an existing test suite by using expected or observed test duration and combining priorities of multiple factors associated with test cases, including previous fault detection success, interval since last executed, and modifications to the code tested. The method and tool were developed to address problems in the traditional process of regression testing, such as lack of time to run a complete regression suite, failure to detect bugs in time, and tests that are repeatedly omitted. The tool has been integrated into the existing nightly test framework for Westermo software that runs on large-scale data communication systems. In experimental evaluation of the tool, we found significant improvement in regression testing results. The re-ordered test suites finish within the available time, the majority of fault-detecting test cases are located in the first third of the suite, no important test case is omitted, and the necessity for manual work on the suites is greatly reduced.",2332-6549,978-1-4673-9002-6978-1-4673-9003,10.1109/ISSRE.2016.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774503,,Testing;Topology;Ports (Computers);Manuals;Hardware;Software;Research and development,data communication;fault diagnosis;program testing;regression analysis;statistical testing,automated system level regression test prioritization;SuiteBuilder;fault detection success;regression testing process;Westermo software;large-scale data communication systems;fault-detection test;software testing,,4,22,,,,,,IEEE,IEEE Conferences
Experience Report: Understanding Cross-Platform App Issues from User Reviews,Y. Man; C. Gao; M. R. Lyu; J. Jiang,NA; NA; NA; NA,2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE),,2016,,,138,149,"App developers publish apps on different platforms, such as Google Play, App Store, and Windows Store, to maximize the user volumes and potential revenues. Due to the different characteristics of the platforms and the different user preference (e.g., Android is more customized than iOS), app testing cases on these three platforms should also be designed differently. Comprehensive app testing can be time-consuming for developers. Therefore, understanding the differences of the app issues on these platforms can facilitate the testing process. In this paper, we propose a novel framework named CrossMiner to analyze the essential app issues and explore whether the app issues exhibit differently on the three platforms. Based on five million user reviews, the framework automatically captures the distributions of seven app issues, i.e., ""battery"", ""crash"", ""memory"", ""network"", ""privacy"", ""spam"", and ""UI"". We discover that the apps for different platforms indeed generate different issue distributions, which can be employed by app developers to schedule and design the testing cases. The verification based on the official user forums also demonstrates the effectiveness of our framework. Furthermore, we also identify that the issues related to ""crash"" and ""network"" are more concerned by users than the other issues on these three platforms. To assist developers in gaining a deep insight on the user issues, we also prioritize the user reviews corresponding to the issues. Overall, we aim at understanding the differences of issues on different platforms and facilitating the testing process for app developers.",2332-6549,978-1-4673-9002-6978-1-4673-9003,10.1109/ISSRE.2016.27,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774515,user feedback;data mining;user reviews;mobile application,Testing;Google;Computer crashes;Mars;Mobile communication;Batteries;Privacy,mobile computing,cross platform app issues;user reviews;user volumes;Android;iOS;testing process;CrossMiner,,4,37,,,,,,IEEE,IEEE Conferences
Foreword of the Thematic Track Quality Aspects in Agile Methods,E. Miranda; J. M. Fernandes,NA; NA,2016 10th International Conference on the Quality of Information and Communications Technology (QUATIC),,2016,,,100,100,"There is no doubt that agile methods have become mainstream and with their increased use unanswered questions start to appear: How do we address cross-cutting concerns when software is developed vertically? Does value prioritization lead to increases in technical debt by promoting feature development over refactoring? Isnƒ??t the reticence to write initial specifications on the premise of change an invitation to unnecessary change? As agile development matures answers, albeit partial, responses start to appear. The recurring themes in this year presentations are not whether agile is good or bad, better or worse,",,978-1-5090-3581-6978-1-5090-3580-9978-1-5090-3582,10.1109/QUATIC.2016.027,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814524,,Business;Security;Context;Information and communication technology;Software;Planning;Testing,,,,,,,,,,,IEEE,IEEE Conferences
Functional stratification of biomarkers selected from microarray data for understanding oral leukoplakia associated carcinogenesis,S. Banerjee; J. Chatterjee,"School of Medical Science and Technology, Indian Institute of Technology, Kharagpur, India; School of Medical Science and Technology, Indian Institute of Technology, Kharagpur, India",2016 International Conference on Systems in Medicine and Biology (ICSMB),,2016,,,95,98,"In this research paper, we have proposed a three tier conceptual framework AVISAR for the testing of Object Oriented Programs. We have devised the proposed framework into three levels: Requirements Modeling, Test Case Generation and Effort Estimation. At the first level, we have used the Extend relationship of the Use Case to capture the events generated by the Classes during message passing between them through their objects. These events were captured by an Object Instantiator using Event Templates and further given to a Test case scenario generator, which generates the test cases. At the second level, the test cases will be generated based on the event templates that are used for capturing the events generated by the Extend relationship of the Use cases. Further, we have proposed a Genetic Algorithm (GA) for the effective test case prioritization to ensure maximum code coverage. At the third level, effort estimation, we have used the Cyclometric Complexity and Token count to perform the effort estimation for the Software under Test (SUT).",,978-1-4673-7666-2978-1-4673-7665-5978-1-4673-7667,10.1109/ICSMB.2016.7915096,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7915096,Oral leukoplakia;Oral Cancer;miRNA target gene;functional analysis,Testing;Software;Genetic algorithms;Generators;Java;Optimization;Object oriented modeling,genetic algorithms;medical computing;object-oriented programming,AVISAR;Tier architectural framework;object oriented programs;requirements modeling;test case generation;effort estimation;message passing;object instantiator;event templates;test case scenario generator;genetic algorithm;effective test case prioritization;maximum code coverage;cyclometric complexity;token count;software-under-test,,,19,,,,,,IEEE,IEEE Conferences
Guiding Dynamic Symbolic Execution toward Unverified Program Executions,M. Christakis; P. M?¬ller; V. W?¬stholz,NA; NA; NA,2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),,2016,,,144,155,"Most techniques to detect program errors, such as testing, code reviews, and static program analysis, do not fully verify all possible executions of a program. They leave executions unverified when they do not check certain properties, fail to verify properties, or check properties under certain unsound assumptions such as the absence of arithmetic overflow. In this paper, we present a technique to complement partial verification results by automatic test case generation. In contrast to existing work, our technique supports the common case that the verification results are based on unsound assumptions. We annotate programs to reflect which executions have been verified, and under which assumptions. These annotations are then used to guide dynamic symbolic execution toward unverified program executions. Our main technical contribution is a code instrumentation that causes dynamic symbolic execution to abort tests that lead to verified executions, to prune parts of the search space, and to prioritize tests that cover more properties that are not fully verified. We have implemented our technique for the .NET static analyzer Clousot and the dynamic symbolic execution tool Pex. It produces smaller test suites (by up to 19.2%), covers more unverified executions (by up to 7.1%), and reduces testing time (by up to 52.4%) compared to combining Clousot and Pex without our technique.",1558-1225,978-1-4503-3900-1978-1-5090-2071,10.1145/2884781.2884843,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886899,static analysis;program verification;testing;partial verification;dynamic symbolic execution,Testing;Instruments;Redundancy;Aerospace electronics;Conferences;Software engineering;Performance analysis,program testing,dynamic symbolic execution;unverified program execution;program error detection;program execution;arithmetic overflow;automatic test case generation;code instrumentation;Clousot;Pex tool,,3,41,,,,,,IEEE,IEEE Conferences
History-Based Dynamic Test Case Prioritization for Requirement Properties in Regression Testing,X. Wang; H. Zeng,NA; NA,2016 IEEE/ACM International Workshop on Continuous Software Evolution and Delivery (CSED),,2016,,,41,47,"Regression testing is an important but extremely costly and time-consuming process. Because of limited resources in practice, test case prioritization focuses on the improvement of testing efficiency. However, traditional test case prioritization techniques emphasize only one-time testing without considering huge historical data generated in regression testing. This paper proposes an approach to prioritizing test cases based on historical data. Requirements are a significant factor in the testing process, the priorities of test cases are initialized based on requirement priorities in our history-based approach, and then are calculated dynamically according to historical data in regression testing. To evaluate our approach, an empirical study on an industrial system is conducted. Experimental results show an improved performance for our proposed method using measurements of Average Percentage of Faults Detected and Fault Detection Rate.",,978-1-4503-4157-8978-1-5090-2200,10.1109/CSED.2016.016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809434,Test Case Prioritization; Requirement Property; History Data; Regression Testing,Software;History;Fault detection;Complexity theory;Software testing;Time factors,program testing,history-based dynamic test case prioritization;requirement properties;regression testing;testing efficiency;history-based approach;empirical study;average percentage of fault measurement;fault detection rate measurement,,,17,,,,,,IEEE,IEEE Conferences
History-Based Test Case Prioritization for Failure Information,Y. Cho; J. Kim; E. Lee,NA; NA; NA,2016 23rd Asia-Pacific Software Engineering Conference (APSEC),,2016,,,385,388,"From regression tests, developers seek to determine not only the existence of faults, but also failure information such as what test cases failed. Failure information can assist in identifying suspicious modules or functions in order to fix the detected faults. In continuous integration environments, this can also help managers of the source code repository address unexpected situations caused by regression faults. We introduce an approach, referred to as AFSAC, which is a test case prioritization technique based on history data, that can be used to effectively obtain failure information. Our approach is composed of two stages. First, we statistically analyze the failure history for each test case to order the test cases. Next, we reorder the test cases utilizing the correlation data of test cases acquired by previous test results. We performed an empirical study on two open-source Apache software projects (i.e., Tomcat and Camel) to evaluate our approach. The results of the empirical study show that our approach provides failure information to testers and developers more effectively than other prioritization techniques, and each prioritizing method of our approach improves the ability to obtain failure information.",1530-1362,978-1-5090-5575-3978-1-5090-5576,10.1109/APSEC.2016.066,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890618,regression test;test case prioritization;history data;failure information;continuous integration environments,History;Correlation;Statistical analysis;Rockets;Open source software;Optimization,project management;public domain software;regression analysis;software fault tolerance;software management,test case prioritization;failure information;regression test;AFSAC;open-source Apache software project,,,6,,,,,,IEEE,IEEE Conferences
How Does Regression Test Prioritization Perform in Real-World Software Evolution?,Y. Lu; Y. Lou; S. Cheng; L. Zhang; D. Hao; Y. Zhou; L. Zhang,NA; NA; NA; NA; NA; NA; NA,2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),,2016,,,535,546,"In recent years, researchers have intensively investigated various topics in test prioritization, which aims to re-order tests to increase the rate of fault detection during regression testing. While the main research focus in test prioritization is on proposing novel prioritization techniques and evaluating on more and larger subject systems, little effort has been put on investigating the threats to validity in existing work on test prioritization. One main threat to validity is that existing work mainly evaluates prioritization techniques based on simple artificial changes on the source code and tests. For example, the changes in the source code usually include only seeded program faults, whereas the test suite is usually not augmented at all. On the contrary, in real-world software development, software systems usually undergo various changes on the source code and test suite augmentation. Therefore, it is not clear whether the conclusions drawn by existing work in test prioritization from the artificial changes are still valid for real-world software evolution. In this paper, we present the first empirical study to investigate this important threat to validity in test prioritization. We reimplemented 24 variant techniques of both the traditional and time-aware test prioritization, and investigated the impacts of software evolution on those techniques based on the version history of 8 real-world Java programs from GitHub. The results show that for both traditional and time-aware test prioritization, test suite augmentation significantly hampers their effectiveness, whereas source code changes alone do not influence their effectiveness much.",1558-1225,978-1-4503-3900-1978-1-5090-2071,10.1145/2884781.2884874,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886932,,Testing;Software systems;Schedules;Time factors;Instruments;Software engineering,program testing;software engineering,regression test prioritization;real-world software evolution;fault detection;prioritization techniques;software development;test suite augmentation;GitHub,,5,67,,,,,,IEEE,IEEE Conferences
Improving test efficiency through multiple criteria coverage based test case prioritization using Modified heuristic algorithm,A. K. Joseph; G. Radhamani; V. Kallimani,"Dr. GRD College of Science, Coimbatore, India; School of IT &amp; Science, Dr. GRD College of Science, Coimbatore, India; Universiti Teknologi PETRONAS, Perak, Malaysia",2016 3rd International Conference on Computer and Information Sciences (ICCOINS),,2016,,,430,435,"Test case prioritization involves reordering the test cases in an order that helps in attaining certain performance goals. The rate of fault detection is one of the prime goals that we tend to achieve while doing prioritization. Test cases should run in an order to increase the possibility of fault detection and it should be achieved early during the test life cycle. To reduce the cost and time of regression testing, test case prioritization should be done with the intention of periodically modifying the test suite. The humongous set of test cases makes it redundant and cumbersome for the testers who ensure quality for an end application. The fault detection capability of a prioritized test suite is improved up to 15% using Modified PSO which forms the base algorithms for prioritization. The algorithm illustrated detects serious errors at earlier phases of testing process and effectiveness between prioritized and unprioritized test cases.",,978-1-5090-2549-7978-1-5090-5144-1978-1-5090-2550,10.1109/ICCOINS.2016.7783254,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7783254,Test case prioritization;Heuristic Algorithm;Modified Particle Swarm Optimization;Regression testing,Testing;Complexity theory;Software;Fault detection;Electric breakdown;Computers;Software algorithms,fault diagnosis;particle swarm optimisation;program testing;regression analysis;software quality,test efficiency improvement;multiple criteria coverage based test case prioritization;modified heuristic algorithm;fault detection capability;test life cycle;cost reduction;time reduction;regression testing;modified PSO;modified particle swarm optimization,,,18,,,,,,IEEE,IEEE Conferences
Improving Testing in an Enterprise SOA with an Architecture-Based Approach,G. Buchgeher; C. Klammer; W. Heider; M. Sch?¬etz; H. Huber,NA; NA; NA; NA; NA,2016 13th Working IEEE/IFIP Conference on Software Architecture (WICSA),,2016,,,231,240,"High resource demand for system testing is a major obstacle for continuous delivery. This resource demand can be reduced by prioritizing test cases, e.g., by focusing on tests that cover a lot of functionality. For large-scale systems, like an enterprise SOA, defining such test cases can be difficult for the tester because of the lack of relevant knowledge about the system. We propose an approach for test case prioritization and selection that is based on architectural viewpoint that provides software testers with the required architectural information. We outline how architectural information is used for defining and selecting prioritized test cases. The approach has been developed in close cooperation with the provider of an enterprise SOA in the banking domain in Austria following an action research approach. In addition, the approach has been validated in an industrial case study. Validation showed that there is no further need for manual architectural analysis to be able to prioritize and select test cases. We also show the limitations of our approach as it is based on static code analysis.",,978-1-5090-2131-4978-1-5090-2563,10.1109/WICSA.2016.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7516833,Test Management;Testing;Test Case Prioritization;SOA;Architecture Knowledge;Architecture-based Testing,Computer architecture;Testing;Service-oriented architecture;Semiconductor optical amplifiers;Business,program diagnostics;program testing;service-oriented architecture,static code analysis;action research approach;Austria;banking domain;software architectural information;software testers;test case selection;test case prioritization;system testing;enterprise SOA,,2,24,,,,,,IEEE,IEEE Conferences
Improvising the effectiveness of test suites using differential evolution technique,Shilpi; Karambir,"Department of Computer Science and Engineering, UIET, Kurukshetra University, Kurukshetra; Department of Computer Science and Engineering, UIET, Kurukshetra University, Kurukshetra","2016 5th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",,2016,,,52,56,"The process of testing any software system is an atrocious task which indeed consumes a ton of effort, and expensive also. Required effort and time to do adequate as well as effective testing get bigger, as the software gets more complexed that can lead to swarm over the project budget or some test cases left uncovered or delay in completion. A suitably generated test suite does not only locate errors but also aid in reducing cost investment associated with the testing process. This paper implements an optimizing technique called as Differential Evolution to improve the effectiveness of test cases using Average Percentage of Fault Detection (APFD) metric. APFD is taken as the fitness function which is to be optimized. In this work, We have performed comparison of our approach with other existing prioritizing approaches and Experimental computations show that Differential Evolution technique achieve better APFD values than other techniques.",,978-1-5090-1489-7978-1-5090-1490,10.1109/ICRITO.2016.7784924,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7784924,Test suite;Test case Prioritization;APFD;Differential Evolution;Case Study,Optimization;Testing;Measurement;Fault detection;Reliability;Algorithm design and analysis;Software,evolutionary computation;fault diagnosis;program testing,test suites;differential evolution;software system testing;cost investment reduction;average percentage of fault detection;APFD metric;fitness function,,,8,,,,,,IEEE,IEEE Conferences
Integrating risk assessment and threat modeling within SDLC process,V. Maheshwari; M. Prasanna,"School of Information Technology, Vellore Institute of Technology, Vellore, India; School of Information Technology, Vellore Institute of Technology, Vellore, India",2016 International Conference on Inventive Computation Technologies (ICICT),,2016,1,,1,5,"Risk assessment and threat modeling are conducted for different purpose. The integration of risk assessment and threat modeling process limit the risk of software-based system. Incorporating security in all phases of software development life cycle is a tedious task in many organizations. In design phase of SDLC, the 50 % software defects are identified and detected. Most of the security attacks are happen in application layer. This paper explains the combined use of risk assessment and threat model to understand the security risk of an application. We also discuss how the model may be identifying threats and how to frame threat prioritization for threat category. Finally, we recommend understanding of risk of detection and creating a fair environment to reduce the likelihood of committing criminal acts by attackers.",,978-1-5090-1285-5978-1-5090-1283-1978-1-5090-1286,10.1109/INVENTIVE.2016.7823275,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823275,Risk-based testing;application layer;Software development life cycle (SDLC);Threat model,Risk management;Authentication;Web servers;Software systems;Analytical models,risk management;security of data;software development management,risk assessment;SDLC process;threat modeling process;software-based system;software development life cycle;software defects;security attacks;threat model;threat prioritization;threat category;criminal acts,,1,10,,,,,,IEEE,IEEE Conferences
Investigating the Effects of Balanced Training and Testing Datasets on Effort-Aware Fault Prediction Models,K. E. Bennin; J. Keung; A. Monden; Y. Kamei; N. Ubayashi,NA; NA; NA; NA; NA,2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC),,2016,1,,154,163,"To prioritize software quality assurance efforts, fault prediction models have been proposed to distinguish faulty modules from clean modules. The performances of such models are often biased due to the skewness or class imbalance of the datasets considered. To improve the prediction performance of these models, sampling techniques have been employed to rebalance the distribution of fault-prone and non-fault-prone modules. The effect of these techniques have been evaluated in terms of accuracy/geometric mean/F1-measure in previous studies; however, these measures do not consider the effort needed to fix faults. To empirically investigate the effect of sampling techniques on the performance of software fault prediction models in a more realistic setting, this study employs Norm(P<sub>opt</sub>), an effort-aware measure that considers the testing effort. We performed two sets of experiments aimed at (1) assessing the effects of sampling techniques on effort-aware models and finding the appropriate class distribution for training datasets (2) investigating the role of balanced training and testing datasets on performance of predictive models. Of the four sampling techniques applied, the over-sampling techniques outperformed the under-sampling techniques with Random Over-sampling performing best with respect to the Norm(P<sub>opt</sub>) evaluation measure. Also, performance of all the prediction models improved when sampling techniques were applied between the rates of (20-30)% on the training datasets implying that a strictly balanced dataset (50% faulty modules and 50% clean modules) does not result in the best performance for effort-aware models. Our results also indicate that performances of effort-aware models are significantly dependent on the proportions of the two types of the classes in the testing dataset. Models trained on moderately balanced datasets are more likely to withstand fluctuations in performance as the class distribution in the testing data varies.",0730-3157,978-1-4673-8845-0978-1-4673-8846,10.1109/COMPSAC.2016.144,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552003,class imbalance;sampling techniques;software quality;software fault prediction;empirical study,Predictive models;Software;Training;Testing;Computer bugs;Data models;Measurement,program testing;sampling methods;software fault tolerance;software quality,balanced training effect;testing dataset;effort-aware fault prediction models;software quality assurance;sampling techniques;nonfault-prone modules;fault-prone modules;software fault prediction models;norm (P<sub>opt</sub>) evaluation measure,,6,47,,,,,,IEEE,IEEE Conferences
MACKE: Compositional analysis of low-level vulnerabilities with symbolic execution,S. Ognawala; M. Ochoa; A. Pretschner; T. Limmer,"Technical University of Munich, Germany; Singapore University of Technology and Design, Singapore; Technical University of Munich, Germany; Siemens AG, Germany",2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE),,2016,,,780,785,"Concolic (concrete+symbolic) execution has recently gained popularity as an effective means to uncover non-trivial vulnerabilities in software, such as subtle buffer overflows. However, symbolic execution tools that are designed to optimize statement coverage often fail to cover potentially vulnerable code because of complex system interactions and scalability issues of constraint solvers. In this paper, we present a tool (MACKE) that is based on the modular interactions inferred by static code analysis, which is combined with symbolic execution and directed inter-procedural path exploration. This provides an advantage in terms of statement coverage and ability to uncover more vulnerabilities. Our tool includes a novel feature in the form of interactive vulnerability report generation that helps developers prioritize bug fixing based on severity scores. A demo of our tool is available at https://youtu.be/icC3jc3mHEU.",,978-1-4503-3845-5978-1-5090-5571,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582815,Symbolic execution;Compositional analysis,Computer bugs;Engines;Software;Security;Memory management;Complex systems;Scalability,program debugging;program diagnostics;program testing;software tools,MACKE tool;compositional analysis;low-level vulnerabilities;concolic execution;concrete+symbolic execution;nontrivial software vulnerabilities;buffer overflows;complex system interactions;scalability issues;constraint solvers;modular interactions;static code analysis;directed interprocedural path exploration;interactive vulnerability report generation;bug fixing;severity scores,,,34,,,,,,IEEE,IEEE Conferences
Minimum complexity APP prioritization by bandwidth apportioning in smart phones,K. Subramaniam; K. Govindan; S. Jaiswal; S. Das Sunkada Gopinath,"Samsung Research India, Bangalore; Samsung Research India, Bangalore; Samsung Research India, Bangalore; Samsung Research India, Bangalore",2016 IEEE Wireless Communications and Networking Conference,,2016,,,1,6,"The volume of best effort traffic is exploded by rapid adoption of peer-to-peer and content applications. Smart phone consumers are spending more times on applications which include video and music streaming, playing games, video chatting, social media like uploading photos to Facebook, Twitter etc. Many such applications are always running in background and sometimes come in foreground based on user preferences. In this work we propose an approach to improve the user experience by giving more bandwidth to preferred applications. We describe a preliminary model explaining our technique in detail. Further, we validate our proposal using real time test setup with Wireshark traffic analyzer, and results are detailed with respect to (1) Percentage of network share (2) Jitter experience and (3) Time taken for the algorithm to adapt. Proposed algorithm has been tested in two different platforms such as Android and Tizen. Our preliminary observations show that our proposed algorithm allocates more bandwidth to high priority applications while maintaining the low priority APPs are intact with above minimum bandwidth. Our approach gives users better jitter free experience for video streaming (high priority) applications in both Android (KitKat) and Tizen (Z1) platforms.",1558-2612,978-1-4673-9814-5978-1-4673-9815,10.1109/WCNC.2016.7564972,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7564972,,Bandwidth;Smart phones;Receivers;Algorithm design and analysis;Channel allocation;Real-time systems;Software algorithms,Android (operating system);bandwidth allocation;jitter;smart phones;video streaming,minimum complexity APP prioritization;video streaming applications;jitter free experience;high priority applications;Tizen;Android;network share percentage;Wireshark traffic analyzer;real time test setup;user experience;smart phone consumers;content applications;peer-to-peer;best effort traffic,,,16,,,,,,IEEE,IEEE Conferences
Mixed-Criticality Systems as a Service for Non-critical Tasks,M. Hikmet; M. M. Kuo; P. S. Roop; P. Ranjitkar,NA; NA; NA; NA,2016 IEEE 19th International Symposium on Real-Time Distributed Computing (ISORC),,2016,,,221,228,"Mixed-Criticality Systems are capable of accommodating tasks of varying criticality. In this paper, these are [life, mission, and non-critical]. Tasks usually have an overestimated execution time to allow for the Worst Case Execution Time (WCET). When these tasks finish execution prior to their allotted execution time due to pessimistic assumptions present in the static analysis of the system. The surplus time is used to accommodate tasks with tolerance for deadline-misses. Non-critical tasks are often treated in a ƒ?best-effortƒ? capacity where no quality of service is considered. When processor utilisation is not overconstrained, all deadlines will be met. However, in cases where not enough processing resources exist to meet all deadlines for non-critical tasks, the allotted time for the non-critical tasks must be rationed between non-critical tasks. This paper proposes a novel method of prioritising non-critical tasks. By treating task execution as a service, non-critical tasks with unbounded deadline miss tolerance are given a Grade of Service for their met deadlines. This Grade of Service is used for the dynamic scheduling of non-critical tasks. Four different scheduling algorithms were tested with the proposed Highest Penalty First algorithm for distributing the effort of task execution amongst non-critical tasks in a proportionate manner and showing superior fairness of task execution compared to all other tested algorithms.",2375-5261,978-1-4673-9032-3978-1-4673-9033,10.1109/ISORC.2016.38,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515632,mixed criticality;optimisation;optimization;low-criticality;non-critical;measure;fairness;grade of service;quality of service;qos;gos;gini,Schedules;Measurement;Mission critical systems;Sociology;Electronic mail;Scheduling algorithms;Telecommunications,processor scheduling;program diagnostics;safety-critical software,mixed-criticality systems;noncritical tasks;overestimated execution time;worst case execution time;WCET;static analysis;grade-of-service;dynamic scheduling;highest penalty first algorithm,,2,17,,,,,,IEEE,IEEE Conferences
Model-based test case prioritization using ACO: A review,S. Sharma; A. Singh,"Dept. of Computer Science &amp; Engg., DCRUST, Murthal, Sonepat, India; Dept. of Computer Science &amp; Engg., DCRUST, Murthal, Sonepat, India","2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC)",,2016,,,177,181,"Regression testing is very costly and inevitable activity of maintenance that is performed to ensure whether the modified software is valid or not. Running all the test cases of a test suit within given limited time and cost constraints is not possible. So, to cover the maximum number of faults in comparatively less time, it is necessary to prioritize the test cases. To solve the time constraint test case prioritization problems Ant Colony optimization (ACO) is a better approach. This paper presents a review on test case prioritization from a given test suite using ACO.",,978-1-5090-3669-1978-1-5090-3670,10.1109/PDGC.2016.7913140,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7913140,Regression testing;Ant Colony Optimization;Test case Prioritization,Decision support systems;Testing;Time factors;Ant colony optimization;Maintenance engineering;Software,ant colony optimisation;program testing;software maintenance,model-based test case prioritization;ACO;regression testing;software maintenance;test suit;time constraint test case prioritization problem;ant colony optimization,,,16,,,,,,IEEE,IEEE Conferences
Modified ACO to maintain diversity in regression test optimization,S. Kumar; P. Ranjan; R. Rajesh,"Department of computer science, Central University of South Bihar, India; Department of computer science, Central University of South Bihar, India; Department of computer science, Central University of South Bihar, India",2016 3rd International Conference on Recent Advances in Information Technology (RAIT),,2016,,,619,625,"Regression testing is unavoidable maintenance activity that is performed several times in software development life cycle. Optimization of regression test case is required to minimize the test case (which will in-turn reduce the time and cost of testing) and to find the fault in early testing activity. The two widely used regression test case optimization techniques, namely, selection and prioritization are recently found to be integrated with different metaheuristic algorithms for fruitful regression test cases. Among the various meta-heuristic algorithms, Ant colony optimization (ACO) algorithm is most popularly used. ACO will try to find the smallest path out all the test cases and it is not sufficient because it will not cover all the test cases which are needed. In this paper we have proposed a modified ant colony optimization to solve test cases in huge search space. The modified algorithm selects the best test cases that find the maximum fault in minimum time.",,978-1-4799-8579-1978-1-4799-8580,10.1109/RAIT.2016.7507970,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507970,Regresssion testing;Soft computing;Multi-objective optimization;Test case optimization;Nature Based Optimization,Optimization;Software;Ant colony optimization;Software algorithms;Software testing;Maintenance engineering,ant colony optimisation;program testing;regression analysis;search problems;software development management;software maintenance,ACO;maintenance activity;software development life cycle;regression test case optimization techniques;ant colony optimization;search space,,,24,,,,,,IEEE,IEEE Conferences
Multi-objective test report prioritization using image understanding,Y. Feng; J. A. Jones; Z. Chen; C. Fang,"Department of Informatics, University of California, Irvine, USA; Department of Informatics, University of California, Irvine, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE),,2016,,,202,213,"In crowdsourced software testing, inspecting the large number of test reports is an overwhelming but inevitable software maintenance task. In recent years, to alleviate this task, many text-based test-report classification and prioritization techniques have been proposed. However in the mobile testing domain, test reports often consist of more screenshots and shorter descriptive text, and thus text-based techniques may be ineffective or inapplicable. The shortage and ambiguity of natural-language text information and the well defined screenshots of activity views within mobile applications motivate our novel technique based on using image understanding for multi-objective test-report prioritization. In this paper, by taking the similarity of screenshots into consideration, we present a multi-objective optimization-based prioritization technique to assist inspections of crowdsourced test reports. In our technique, we employ the Spatial Pyramid Matching (SPM) technique to measure the similarity of the screenshots, and apply the natural-language processing technique to measure the distance between the text of test reports. Furthermore, to validate our technique, an experiment with more than 600 test reports and 2500 images is conducted. The experimental results show that image-understanding techniques can provide benefit to test-report prioritization for most applications.",,978-1-4503-3845-5978-1-5090-5571,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582758,Crowdsourced Testing;Test Report Prioritization;Image Understanding;Multi-Objective Optimization,Testing;Mobile communication;Computer bugs;Software;Mobile applications;Image color analysis;Inspection,image classification;image matching;mobile computing;natural language processing;program testing;text analysis,multiobjective test report prioritization;image understanding;crowdsourced software testing;software maintenance task;text-based test-report classification;mobile testing domain;screenshots;descriptive text;natural-language text information;mobile applications;spatial pyramid matching;SPM,,,41,,,,,,IEEE,IEEE Conferences
Negative Effects of Bytecode Instrumentation on Java Source Code Coverage,D. Tengeri; F. Horv?­th; ?. Besz??des; T. Gergely; T. Gyim??thy,NA; NA; NA; NA; NA,"2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)",,2016,1,,225,235,"Code coverage measurement is an important element in white-box testing, both in industrial practice and academic research. Other related areas are highly dependent on code coverage as well, including test case generation, test prioritization, fault localization, and others. Inaccuracies of a code coverage tool sometimes do not matter that much but in certain situations they can lead to serious confusion. For Java, the prevalent approach to code coverage measurement is to use bytecode instrumentation due to its various benefits over source code instrumentation. However, if the results are to be mapped back to source code this may lead to inaccuracies due to the differences between the two program representations. In this paper, we systematically investigate the amount of differences in the results of these two Java code coverage approaches, enumerate the possible reasons and discuss the implications on various applications. For this purpose, we relied on two widely used tools to represent the two approaches and a set of benchmark programs from the open source domain.",,978-1-5090-1855,10.1109/SANER.2016.61,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476645,code coverage;white-box testing;Java bytecode instrumentation;source code instrumentation;coverage tools,Testing;Java;Runtime;Probes;Software;Software measurement,Java;program testing;source code (software),bytecode instrumentation;Java source code coverage;white-box testing;test case generation;test prioritization;fault localization;source code instrumentation,,4,36,,,,,,IEEE,IEEE Conferences
Neuro-fuzzy based approach to event driven software testing: A new opportunity,S. Chaudhury; A. Singhal; O. P. Sangwan,"Department of Computer Science &amp; Engineering, AMITY School of Engineering &amp; Technology, AMITY University Uttar Pradesh, Sector 125 NOIDA-201 313 (UP) India; Department of Computer Science &amp; Engineering, AMITY School of Engineering &amp; Technology, AMITY University Uttar Pradesh, Sector 125 NOIDA-201 313 (UP) India; Department of Computer Science &amp; Engineering, AMITY School of Engineering &amp; Technology, AMITY University Uttar Pradesh, Sector 125 NOIDA-201 313 (UP) India",2016 1st India International Conference on Information Processing (IICIP),,2016,,,1,5,"Event Driven Software (EDS) testing is a very challenging task as a large number of events can be invoked by users. So far it is difficult to test all the user inputs invoked, therefore, test case prioritization is essentially required for giving more priority to test cases which reveal higher faults comparatively. We have proposed test case prioritization for EDS: as the Event Type, Interaction of Event, and Coverage of Event. Priority assigned in the proposed model uses these factors in Adaptive Neuro-Fuzzy Inference System (ANFIS) MATLAB Toolbox based on Neuro-Fuzzy logic model. Evaluation and validation will be done using Average Percentage of Fault Detection (APFD). APFD rate for prioritized sequence using the proposed Neuro-Fuzzy logic model exhibited 81% rate, whereas, non-prioritized test sequences showed70% suggesting, thereby, that after prioritization; rate of fault detection has improved considerably. Data shows that proposed Neuro-Fuzzy logic model is apt for Test Case Prioritization of EDS Testing.",,978-1-4673-6984-8978-1-4673-6985,10.1109/IICIP.2016.7975349,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975349,Event Driven Software (EDS);Test Case Prioritization;Event Coverage;Neuro-Fuzzy Mode;Average Percentage of Fault Detection (APFD),Testing;Software;Fault detection;Fuzzy systems;Training data;Adaptation models;Graphical user interfaces,fault diagnosis;fuzzy logic;fuzzy reasoning;mathematics computing;neural nets;program testing,neuro-fuzzy based approach;event driven software testing;EDS testing;test case prioritization;event type;event interaction;event coverage;adaptive neuro-fuzzy inference system;ANFIS MATLAB Toolbox;neuro-fuzzy logic model;average percentage of fault detection;APFD rate;prioritized sequence;nonprioritized test sequences;fault detection rate,,,20,,,,,,IEEE,IEEE Conferences
Opportunistic Competition Overhead Reduction for Expediting Critical Section in NoC Based CMPs,Y. Yao; Z. Lu,NA; NA,2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA),,2016,,,279,290,"With the degree of parallelism increasing, performance of multi-threaded shared variable applications is not only limited by serialized critical section execution, but also by the serialized competition overhead for threads to get access to critical section. As the number of concurrent threads grows, such competition overhead may exceed the time spent in critical section itself, and become the dominating factor limiting the performance of parallel applications. In modern operating systems, queue spinlock, which comprises a low-overhead spinning phase and a high-overhead sleeping phase, is often used to lock critical sections. In the paper, we show that this advanced locking solution may create very high competition overhead for multithreaded applications executing in NoC-based CMPs. Then we propose a software-hardware cooperative mechanism that can opportunistically maximize the chance that a thread wins the critical section access in the low-overhead spinning phase, thereby reducing the competition overhead. At the OS primitives level, we monitor the remaining times of retry (RTR) in a thread's spinning phase, which reflects in how long the thread must enter into the high-overhead sleep mode. At the hardware level, we integrate the RTR information into the packets of locking requests, and let the NoC prioritize locking request packets according to the RTR information. The principle is that the smaller RTR a locking request packet carries, the higher priority it gets and thus quicker delivery. We evaluate our opportunistic competition overhead reduction technique with cycle-accurate full-system simulations in GEM5 using PARSEC (11 programs) and SPEC OMP2012 (14 programs) benchmarks. Compared to the original queue spinlock implementation, experimental results show that our method can effectively increase the opportunity of threads entering the critical section in low-overhead spinning phase, reducing the competition overhead averagely by 39.9% (maximally by 61.8%) and accelerating the execution of the Region-of-Interest averagely by 14.4% (maximally by 24.5%) across all 25 benchmark programs.",1063-6897,978-1-4673-8947-1978-1-4673-8948,10.1109/ISCA.2016.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7551400,Critical Section;CMP;NoC;OS,Nickel;Instruction sets;Spinning;Benchmark testing;Hardware;Operating systems;Queueing analysis,multiprocessing systems;network-on-chip;operating systems (computers),NoC based CMPs;multithreaded shared variable performance;serialized critical section execution;serialized competition overhead;operating systems;queue spinlock;low-overhead spinning phase;high-overhead sleeping phase;software-hardware cooperative mechanism;remaining times of retry;RTR;high-overhead sleep mode;opportunistic competition overhead reduction technique;cycle-accurate full-system simulations;GEM5;PARSEC;SPEC OMP2012 benchmarks;region-of-interest,,2,23,,,,,,IEEE,IEEE Conferences
Optimal test sequence generation using River Formation Dynamics,Y. Khatri; A. Sharma; A. Kumar,"CSE Department, Delhi Technological University, New Delhi, India; CSE Department, Delhi Technological University, New Delhi, India; CSE Department, Delhi Technological University, New Delhi, India",2016 International Conference System Modeling & Advancement in Research Trends (SMART),,2016,,,24,31,"Software testing is a complex and exhaustive process, often limited by the resources. Although many approaches for test sequence generation exist in the literature, but none of it is ideal as far as coverage and redundancy is concerned. This paper aims at improving the efficiency of software testing process by generating the optimal test sequences in the control flow graph (CFG) of the program under test (PUT) by using a novel swarm intelligence method called River Formation Dynamics(RFD). RFD is inspired by a natural phenomenon of how drops transformed into river and river into sea. It provides full path coverage with zero edge/transition redundancy. It also tries to prioritize the paths based on their strength, calculated in terms of their traversal by the drops.",,978-1-5090-3543-4978-1-5090-3544,10.1109/SYSMART.2016.7894483,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7894483,Cyclometic Complexity;Control Flow Graph;River Formation Dynamics;Test Sequence;Program Under Test,Rivers;Software testing;Redundancy;Flow graphs;Genetic algorithms;Heuristic algorithms,evolutionary computation;program testing,optimal test sequence generation;river formation dynamics;software testing;control flow graph;CFG;program under test;PUT;swarm intelligence method;RFD;path coverage,,,21,,,,,,IEEE,IEEE Conferences
PRADA: Prioritizing Android Devices for Apps by Mining Large-Scale Usage Data,X. Lu; X. Liu; H. Li; T. Xie; Q. Mei; D. Hao; G. Huang; F. Feng,NA; NA; NA; NA; NA; NA; NA; NA,2016 IEEE/ACM 38th International Conference on Software Engineering (ICSE),,2016,,,3,13,"Selecting and prioritizing major device models are critical for mobile app developers to select testbeds and optimize resources such as marketing and quality-assurance resources. The heavily fragmented distribution of Android devices makes it challenging to select a few major device models out of thousands of models available on the market. Currently app developers usually rely on some reported or estimated general market share of device models. However, these estimates can be quite inaccurate, and more problematically, can be irrelevant to the particular app under consideration. To address this issue, we propose PRADA, the first approach to prioritizing Android device models for individual apps, based on mining large-scale usage data. PRADA adapts the concept of operational profiling (popularly used in software reliability engineering) for mobile apps - the usage of an app on a specific device model reflects the importance of that device model for the app. PRADA includes a collaborative filtering technique to predict the usage of an app on different device models, even if the app is entirely new (without its actual usage in the market yet), based on the usage data of a large collection of apps. We empirically demonstrate the effectiveness of PRADA over two popular app categories, i.e., Game and Media, covering over 3.86 million users and 14,000 device models collected through a leading Android management app in China.",1558-1225,978-1-4503-3900-1978-1-5090-2071,10.1145/2884781.2884828,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886887,Mobile apps;Android fragmentation;prioritization;usage data,Androids;Humanoid robots;Data models;Games;Testing;Data mining;Mobile communication,collaborative filtering;data mining;smart phones,PRADA;prioritizing android devices for apps;large-scale usage data mining;operational profiling;mobile apps;collaborative filtering technique;Android management app,,6,41,,,,,,IEEE,IEEE Conferences
Prioritization techniques in combinatorial testing: A survey,M. Aggarwal; S. Sabharwal,"Department of Computer Engineering, NSIT, Delhi, India; Department of Computer Engineering, NSIT, Delhi, India",2016 1st India International Conference on Information Processing (IICIP),,2016,,,1,6,"Prioritization techniques have become an indispensable part of the software testing process. They are highly beneficial either due to resource or time constraints or when tester cannot execute the complete test set. Combinatorial test sets are generated with the aim to cover all the possible t-way interactions. In this paper, techniques to prioritize t-way test sets are studied. A comparative study is done of prioritization criteria used by researchers to prioritize t-way test sets. Different evaluation methods used by them are also discussed.",,978-1-4673-6984-8978-1-4673-6985,10.1109/IICIP.2016.7975314,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7975314,Combinatorial testing;t-way testing;prioritization;test set;prioritization criteria,Testing;Measurement;Fault detection;Portable computers;Androids;Humanoid robots;Linux,combinatorial mathematics;program testing,prioritization techniques;combinatorial testing;software testing process;combinatorial test sets;t-way interactions;t-way test sets,,,17,,,,,,IEEE,IEEE Conferences
Prioritizing Interaction Test Suites Using Repeated Base Choice Coverage,R. Huang; W. Zong; J. Chen; D. Towey; Y. Zhou; D. Chen,NA; NA; NA; NA; NA; NA,2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC),,2016,1,,174,184,"Combinatorial interaction testing is a well-studied testing strategy that aims at constructing an effective interaction test suite (ITS) of a specific generation strength to identify interaction faults caused by the interactions among factors. Due to limited testing resources in practice, for example in combinatorial interaction regression testing, interaction test suite prioritization (ITSP) has been proposed to improve the efficiency of testing. An intuitive ITSP strategy that has been widely used in practice is fixed-strength interaction coverage based prioritization (FICBP). FICBP makes use of a property of the ITS: interaction coverage at a fixed prioritization strength. However, a challenge facing FICBP is that, when the ITS is large, the prioritization cost can be very high. In this paper, we propose a new FICBP method that, by repeatedly using base choice coverage (i.e., one-wise coverage) during the prioritization process, improves testing efficiency while maintaining testing effectiveness. The empirical studies show that our method has fault detection capability comparable to current FICBP methods, but obtains more stable results in many cases. Additionally, our method requires considerably less prioritization time than other FICBP methods at different prioritization strengths.",0730-3157,978-1-4673-8845-0978-1-4673-8846,10.1109/COMPSAC.2016.167,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552005,Software testing;combinatorial interaction testing;test case prioritization;fixed-strength interaction coverage based prioritization;base choice coverage,Testing;Fault detection;Time complexity;Fault diagnosis;Algorithm design and analysis;Computer science;Software,program testing;software fault tolerance,repeated base choice coverage;testing strategy;interaction faults;testing resources;combinatorial interaction regression testing;interaction test suite prioritization;ITSP;fixed-strength interaction coverage based prioritization;FICBP;one-wise coverage;software testing,,2,46,,,,,,IEEE,IEEE Conferences
Product feature prioritization using the Hidden Structure method: A practical case at Ericsson,R. Lagerstr??m; M. Addibpour; F. Heiser,"KTH Royal Institute of Technology, Stockholm, Sweden; Ericsson AB, Stockholm, Sweden; Ericsson AB, Stockholm, Sweden",2016 Portland International Conference on Management of Engineering and Technology (PICMET),,2016,,,2308,2315,"In this paper, we present a case were we employ the Hidden Structure method to product feature prioritization at Ericsson. The method extends the more common Design Structure Matrix (DSM) approach that has been used in technology management (e.g. project management and systems engineering) for quite some time in order to model complex systems and processes. The hidden structure method focuses on analyzing a DSM based on coupling and modularity theory, and it has been used in a number of software architecture and software portfolio cases. In previous work by the authors the method was tested on organization transformation at Ericsson, however this is the first time it has been employed in the domain of product feature prioritization. Today, at Ericsson, features are prioritized based on a business case approach where each feature is handled isolated from other features and the main focus is customer or market-based requirements. By employing the hidden structure method we show that features are heavily dependent on each other in a complex network, thus they should not be treated as isolated islands. These dependencies need to be considered when prioritizing features in order to save time and money, as well as increase end customer satisfaction.",,978-1-5090-3595,10.1109/PICMET.2016.7806519,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7806519,,Technology management;Technological innovation,customer services;organisational aspects;product customisation;product design;project management;technology management,end customer satisfaction;customer requirements;market-based requirements;product feature prioritization;organization transformation;software portfolio;software architecture;coupling theory;modularity theory;complex processes;complex systems;systems engineering;project management;technology management;DSM;design structure matrix;Ericsson;hidden structure method;product feature prioritization,,2,23,,,,,,IEEE,IEEE Conferences
Radius aware probabilistic testing of deadlocks with guarantees,Y. Cait; Z. Yang,"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Western Michigan University, Kalamazoo, MI, USA",2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE),,2016,,,356,367,"Concurrency bugs only occur under certain interleaving. Existing randomized techniques are usually ineffective. PCT innovatively generates scheduling, before executing a program, based on priorities and priority change points. Hence, it provides a probabilistic guarantee to trigger concurrency bugs. PCT randomly selects priority change points among all events, which might be effective for non-deadlock concurrency bugs. However, deadlocks usually involve two or more threads and locks, and require more ordering constraints to be triggered. We interestingly observe that, every two events of a deadlock usually occur within a short range. We generally formulate this range as the bug Radius, to denote the max distance of every two events of a concurrency bug. Based on the bug radius, we propose RPro (Radius aware Probabilistic testing) for triggering deadlocks. Unlike PCT, RPro selects priority change points within the radius of the targeted deadlocks but not among all events. Hence, it guarantees larger probabilities to trigger deadlocks. We have implemented RPro and PCT and evaluated them on a set of real-world benchmarks containing 10 unique deadlocks. The experimental results show that RPro triggered all deadlocks with higher probabilities (i.e., &gt;7.7x times larger on average) than that by PCT. We also evaluated RPro with radius varying from 1 to 150 (or 300). The result shows that the radius of a deadlock is much smaller (i.e., from 2 to 114 in our experiment) than the number of all events. This further confirms our observation and makes RPro meaningful in practice.",,978-1-4503-3845-5978-1-5090-5571,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582772,Deadlock;random testing;bug radius;multithreaded program,Computer bugs;System recovery;Concurrent computing;Probabilistic logic;Instruction sets;Benchmark testing,concurrency control;probability;program debugging;program testing;system recovery,radius aware probabilistic testing;RPro;deadlock triggering;concurrency bug;probabilistic guarantee,,,66,,,,,,IEEE,IEEE Conferences
Signature limits: an entire map of clone features and their discovery in nearly linear time,W. Casey; A. Shelmire,"Software Engineering Institute, Carnegie Mellon University; Anomali",2016 11th International Conference on Malicious and Unwanted Software (MALWARE),,2016,,,1,10,"We address an increasingly critical problem of identifying the potential signatures for identifying a given family of malware or unwanted software (i.e., or generally any corpus of artifacts of unknown provenance). We address this with a novel methodology designed to create an entire and complete maps of software code clones (copy features in data). We report on a practical methodology, which employs enhanced suffix data structures and partial orderings of clones to compute a compact representation of most interesting clones features in data. The enumeration of clone features is useful for malware triage and prioritization when human exploration, testing and verification is the most costly factor. We further show that the enhanced arrays may be used for discovery of provenance relations in data and we introduce two distinct Jaccard similarity coefficients to measure code similarity in binary artifacts. We illustrate the use of these tools on real malware data including a retro-diction experiment for measuring and enumerating evidence supporting common provenance in Stuxnet and Duqu. The results indicate the practicality and efficacy of mapping completely the clone features in data.",,978-1-5090-4542-6978-1-5090-4543,10.1109/MALWARE.2016.7888740,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888740,,Cloning;Malware;Entropy;Arrays;Mathematical model,data structures;digital signatures;invasive software,clone features;unwanted software identification;software code clones;suffix data structures;partial orderings;clone feature enumeration;malware triage;malware prioritization;Jaccard similarity coefficients;code similarity;binary artifacts;malware data;retrodiction experiment;Stuxnet;Duqu;malware identification;signature identification;nearly linear time,,,35,,,,,,IEEE,IEEE Conferences
Solving exercise generation problems by diversity oriented meta-heuristics,B. L?­ng; Z. T. Kardkov?­cs,"Corvinus University of Budapest, 8 F?v?­m t??r, Budapest, Hungary, H-1117; Corvinus University of Budapest, 8 F?v?­m t??r, Budapest, Hungary, H-1117","2016 10th International Conference on Software, Knowledge, Information Management & Applications (SKIMA)",,2016,,,49,54,"Evolutionary algorithms used for multi-objective optimization mostly prioritize fitness over diversity to achieve a single optimum fast, or a region in the Pareto-front. In this paper, we argue on that diversity should be a primary objective as well, and we propose a novel approach called EGAL to solve a well-known problem: to generate very different exercises to test students' knowledge in a specific range of topics. We show that focusing on diversity and fitness at the same time result in a better quality of solutions in the resulting population.",,978-1-5090-3298-3978-1-5090-3299,10.1109/SKIMA.2016.7916196,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916196,,Sociology;Statistics;Evolutionary computation;Optimization;Genetics;Software;Information management,educational technology;evolutionary computation;Pareto optimisation,exercise generation problems;diversity oriented meta-heuristics;evolutionary algorithms;multiobjective optimization;Pareto-front;EGAL approach;students knowledge testing,,,11,,,,,,IEEE,IEEE Conferences
Supporting the regression test of multi-variant systems in distributed production scenarios,S. Abele; M. Weyrich,"Institute of Industrial Automation and Software Engineering, University of Stuttgart, Germany; Institute of Industrial Automation and Software Engineering, University of Stuttgart, Germany",2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA),,2016,,,1,4,"Modern manufacturing systems based on cyber-physical systems with a growing amount of software allow frequent updates and reconfigurations to adapt the systems to volatile usage scenarios in the production. A diverse system environment arises even for similar or equal subsystems based on the same platform used at different locations. A major challenge for such systems is the regression test after changes or updates. The resources for the regression test, in a dedicated test environment or deployed to the assembly lines, are limited. To plan the test in the best possible way, a lot of dependencies, relationships and experiences from former tests and tests from other locations have to be considered. This paper describes an assistance system which supports the planning of the regression test in such distributed manufacturing scenarios by combining manual modeling with automated data processing. Therefore the system calculates a cross-location test progress and suggests a prioritized test case sequence.",,978-1-5090-1314-2978-1-5090-1313-5978-1-5090-1315,10.1109/ETFA.2016.7733652,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733652,,Production;Data models;Software;Estimation;Quality assurance;Hardware;Prototypes,assembling;assembly planning;distributed processing;manufacturing systems;production engineering computing;quality assurance;regression analysis,regression test;multivariant systems;distributed production scenarios;cyberphysical systems;assembly lines;planning,,1,15,,,,,,IEEE,IEEE Conferences
System-Level Test Case Prioritization Using Machine Learning,R. Lachmann; S. Schulze; M. Nieke; C. Seidl; I. Schaefer,NA; NA; NA; NA; NA,2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA),,2016,,,361,368,"Regression testing is the common task of retesting software that has been changed or extended (e.g., by new features) during software evolution. As retesting the whole program is not feasible with reasonable time and cost, usually only a subset of all test cases is executed for regression testing, e.g., by executing test cases according to test case prioritization. Although a vast amount of methods for test case prioritization exist, they mostly require access to source code (i.e., white-box). However, in industrial practice, system-level testing is an important task that usually grants no access to source code (i.e., black-box). Hence, for an effective regression testing process, other information has to be employed. In this paper, we introduce a novel technique for test case prioritization for manual system-level regression testing based on supervised machine learning. Our approach considers black-box meta-data, such as test case history, as well as natural language test case descriptions for prioritization. We use the machine learning algorithm SVM Rank to evaluate our approach by means of two subject systems and measure the prioritization quality. Our results imply that our technique improves the failure detection rate significantly compared to a random order. In addition, we are able to outperform a test case order given by a test expert. Moreover, using natural language descriptions improves the failure finding rate.",,978-1-5090-6167-9978-1-5090-6168,10.1109/ICMLA.2016.0065,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7838169,System-Level Testing;Black-Box Testing;Test Case Prioritization;Supervised Machine Learning,Testing;Support vector machines;Software;Training data;Natural languages;Dictionaries;Training,learning (artificial intelligence);natural languages;program testing;regression analysis;source code (software),failure finding rate;natural language descriptions;test expert;random order;failure detection rate;prioritization quality;SVM rank;natural language test case descriptions;test case history;black-box meta-data;supervised machine learning;industrial practice;source code;test case prioritization;software evolution;retesting software;regression testing;system-level test case prioritization,,1,34,,,,,,IEEE,IEEE Conferences
Targeted Scrum: Applying Mission Command to Agile Software Development,D. P. Harvie; A. Agah,"Department of Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS; Department of Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS",IEEE Transactions on Software Engineering,,2016,42,5,476,489,"Software engineering and mission command are two separate but similar fields, as both are instances of complex problem solving in environments with ever changing requirements. Our research hypothesis is that modifications to agile software development based on inspirations from mission command can improve the software engineering process in terms of planning, prioritizing, and communication of software requirements and progress, as well as improving the overall software product. Targeted Scrum is a modification of Traditional Scrum based on three inspirations from Mission Command: End State, Line of Effort, and Targeting. These inspirations have led to the introduction of the Product Design Meeting and modifications of some current Scrum meetings and artifacts. We tested our research hypothesis using a semester-long undergraduate level software engineering class. Students developed two software projects, one using Traditional Scrum and the other using Targeted Scrum. We then assessed how well both methodologies assisted the software development teams in planning and developing the software architecture, prioritizing requirements, and communicating progress. We also evaluated the software product produced by both methodologies. We found that Targeted Scrum did better in assisting the software development teams in the planning and prioritization of the requirements. However, Targeted Scrum had a negligible effect on improving the software development teams external and internal communications. Finally, Targeted Scrum did not have an impact on the product quality by the top performing and worst performing teams. Targeted Scrum did assist the product quality of the teams in the middle of the performance spectrum.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2015.2489654,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7296686,"Scrum, Mission Command;Line of Effort;Product Design Meeting;Agile;Empirical Software Engineering;Scrum;mission command;line of effort;product design meeting;agile;empirical software engineering",Software;Planning;Scrum (Software development);Product design;Software engineering;Force,software architecture;software prototyping,targeted Scrum;mission command;agile software development;software engineering;software requirements;software product improvement;end state;line of effort;targeting;product design meeting;software projects;traditional Scrum;software architecture;product quality;performance spectrum,,4,34,,,,,,IEEE,IEEE Journals & Magazines
Test case design based technique for the improvement of test case selection in software maintenance,A. Lawanna,"Department of Information Technology, Assumption University, Samut Prakarn, Thailand",2016 55th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE),,2016,,,345,350,"Test case selection is the maintenance technique regarding the concept of choosing the appropriate representative of the modified program that has to be changed depending upon the new requirements added for the next modification. The main problem of updating software is the amounts of the test suite that is generated for the tests increases, which can drop the performance of using the new code that added to the previous program. According to this, execution and testing time will increase, including the complexity of integrating the codes also increase. Therefore, the test suite minimization approaches are offered to solve these problems. The objective of proposing the design based technique is to refine the competency of using test case selection to control the large programs by applying four algorithms, which are testing test case, classification, deletion, and selection. The result of determining the size reduction is greater than random selection, general regression technique, and test case prioritization as about 28.00%, 17.00%, and 11.00% approximately.",,978-4-907764-50-0978-1-5090-0937,10.1109/SICE.2016.7749202,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7749202,Test Case;Test Suite;Selection;Prioritization;Minimization,Testing;Computer bugs;Classification algorithms;Algorithm design and analysis;Software algorithms;Software maintenance,program testing;software maintenance,test case design based technique;test case selection;software maintenance;test suite minimization approaches,,,11,,,,,,IEEE,IEEE Conferences
Test Case Prioritization Approach to Improving the Effectiveness of Fault Localization,W. Fu; H. Yu; G. Fan; X. Ji,NA; NA; NA; NA,"2016 International Conference on Software Analysis, Testing and Evolution (SATE)",,2016,,,60,65,"Fault localization aims to use testing information from executed test cases to help locate the fault position. However, obtaining testing information (including test results and coverage information) is expensive because it needs much manual effort. How to orderly choose and execute a small number of test cases, and in the meantime achieve a good effectiveness of fault localization in the case of unknowing testing information is still a challenge for us. In this paper, we propose a new test case prioritization algorithm for fault localization, which is based on the rank changes of suspicious values of program elements. Test case which can maximize the improvements of suspicious ranks of program elements may assign the highest priority for execution. A set of empirical studies have been designed and conducted on Siemens programs and four medium-sized programs. The results show that our algorithm can help reduce the debugging effort in terms of the percentage of statements needed to be inspected to locate faults in both single-fault and multi-fault programs.",,978-1-5090-4517-4978-1-5090-4518,10.1109/SATE.2016.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780195,fault localization;test case prioritization;rank change,Software;Debugging;Algorithm design and analysis;Software testing;Linear programming;Software algorithms,program debugging;program testing;software fault tolerance,test case prioritization;fault localization;fault position location;information testing;program elements;Siemens programs;medium-sized programs;multifault programs;single-fault programs,,,16,,,,,,IEEE,IEEE Conferences
Test case prioritization based on requirement correlations,T. Ma; H. Zeng; X. Wang,"School of Computer Engineering and Science, Shanghai University, 200444, China; School of Computer Engineering and Science, Shanghai University, 200444, China; School of Computer Engineering and Science, Shanghai University, 200444, China","2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",,2016,,,419,424,"Test case prioritization technique aims to improve test efficiency rate by sorting test cases according to some specific criteria. Requirements play an important role throughout software testing. This paper proposes a test case prioritization method based on requirement correlations. Prioritization of requirements is defined by the users and the developers. This technique focuses on requirements with detected faults after the last regression testing. By readjusting prioritization of fault-related requirements, it can optimize the order of test cases. Experimental results show that this technique exactly contributes to achieving high testing efficiency.",,978-1-5090-2239-7978-1-5090-0804,10.1109/SNPD.2016.7515934,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515934,Test case prioritization;Requirement correlations;Fault detection rate;Regression Testing,Software;Correlation;Software testing;Heuristic algorithms;Complexity theory;Cognition,formal specification;program testing;regression analysis;software fault tolerance,fault-related requirements;regression testing;fault detection;requirements prioritization;software testing;test cases sorting;test efficiency rate;requirement correlations;test case prioritization,,1,17,,,,,,IEEE,IEEE Conferences
Test Case Prioritization for Compilers: A Text-Vector Based Approach,J. Chen; Y. Bai; D. Hao; Y. Xiong; H. Zhang; L. Zhang; B. Xie,NA; NA; NA; NA; NA; NA; NA,"2016 IEEE International Conference on Software Testing, Verification and Validation (ICST)",,2016,,,266,277,"Test case prioritization aims to schedule the execution order of test cases so as to detect bugs as early as possible. For compiler testing, the demand for both effectiveness and efficiency imposes challenge to test case prioritization. In the literature, most existing approaches prioritize test cases by using some coverage information (e.g., statement coverage or branch coverage), which is collected with considerable extra effort. Although input-based test case prioritization relies only on test inputs, it can hardly be applied when test inputs are programs. In this paper we propose a novel text-vector based test case prioritization approach, which prioritizes test cases for C compilers without coverage information. Our approach first transforms each test case into a text-vector by extracting its tokens which reflect fault-relevant characteristics and then prioritizes test cases based on these text-vectors. In particular, in our approach we present three prioritization strategies: greedy strategy, adaptive random strategy, and search strategy. To investigate the efficiency and effectiveness of our approach, we conduct an experiment on two C compilers (i.e., GCC and LLVM), and find that our approach is much more efficient than the existing approaches and is effective in prioritizing test cases.",,978-1-5090-1827-7978-1-5090-1828,10.1109/ICST.2016.19,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515478,,Testing;Search problems;Computer bugs;Principal component analysis;Program processors;Transforms;Optimization,feature extraction;greedy algorithms;program compilers;program debugging;program testing;search problems;software fault tolerance;vectors,search strategy;adaptive random strategy;greedy strategy;fault-relevant characteristic;token extraction;bug detection;text-vector based approach;C compiler testing;test case prioritization,,5,54,,,,,,IEEE,IEEE Conferences
Test case prioritization technique based on early fault detection using fuzzy logic,D. K. Yadav; S. Dutta,"Department of Computer Science and Engineering, National Institute of Technology, Jamshedpur, India; Department of Computer Science and Engineering, Birla Institute of Technology, Mesra, Ranchi, India",2016 3rd International Conference on Computing for Sustainable Global Development (INDIACom),,2016,,,1033,1036,"Regression testing is time consuming and expensive activity in software testing. In Regression testing when any changes made to already tested program it should not affect to other part of program. Regression testing is crucial activities in software testing and maintenance phases. If some part of code is altered then it is mandatory to validate the modified code. Throughout regression testing test case from test suite will be re-executed and re-execution of all the test case will be very expensive. In this paper we present regression test case prioritization for object oriented program. The most important research is how to select efficient and suitable test cases during regression testing from the test suite. To minimize the regression testing cost we have applied prioritization technique. In this paper prioritization is done based on fault detection rate of program, execution time and requirement coverage using fuzzy logic.",,978-9-3805-4421-2978-9-3805-4420-5978-1-4673-9417,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7724418,Regression testing;Test case prioritization;APFD metric;Fuzzy inference system (FIS),Testing;Measurement;Fault detection;Software;Conferences;Software engineering;Fuzzy logic,fuzzy logic;fuzzy set theory;object-oriented programming;program testing;software fault tolerance;software maintenance,test case prioritization technique;fault detection;fuzzy logic;regression testing;software testing;software maintenance;object oriented program;prioritization technique,,,19,,,,,,IEEE,IEEE Conferences
Test case prioritization techniques for software product line: A survey,S. Kumar; Rajkumar,"Department of Computer Science, Gurukula Kangri Vishwavidyalaya, Haridwar, India; Department of Computer Science, Gurukula Kangri Vishwavidyalaya, Haridwar, India","2016 International Conference on Computing, Communication and Automation (ICCCA)",,2016,,,884,889,"Software product line (SPL) testing is a tougher work than testing of single systems. Still testing of each individual SPL product would be perfect but it is too costly in practice. In fact, when the number of features increases then the number of possible products also increases exponentially usually derived from a feature model. Number of features is leading to thousands of different products. Due to cost and time constraints, it is infeasible or large number of effort to run all the test cases in an existing test suite. To decrease the cost of testing, various techniques have been proposed. One of them is test case prioritization (TCP) techniques. Here we presented a survey for TCP techniques for software SPL.",,978-1-5090-1666-2978-1-5090-1667,10.1109/CCAA.2016.7813841,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7813841,Software product lines;Test Case Prioritization;Variability;Commonality;Feature Model,Software;Software product lines;Testing;Frequency modulation;Fault detection;Automation;Libraries,program testing;software product lines,software product line;SPL product;feature model;test suite;test case prioritization;TCP,,2,30,,,,,,IEEE,IEEE Conferences
Test Case Prioritization Using Adaptive Random Sequence with Category-Partition-Based Distance,X. Zhang; X. Xie; T. Y. Chen,NA; NA; NA,"2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)",,2016,,,374,385,"Test case prioritization schedules test cases in a certain order aiming to improve the effectiveness of regression testing. Random sequence is a basic and simple prioritization technique, while Adaptive Random Sequence (ARS) makes use of extra information to improve the diversity of random sequence. Some researchers have proposed prioritization techniques using ARS with white-box information, such as code coverage information, or with black-box information, such as string distances of the input data. In this paper, we propose new black-box test case prioritization techniques using ARS, and the diversity of test cases is assessed by category-partition-based distance. Our experimental studies show that these new techniques deliver higher fault-detection effectiveness than random prioritization, especially in the case of smaller ratio of failed test cases. In addition, in the comparison of different distance metrics, techniques with category-partition-based distance generally deliver better fault-detection effectiveness and efficiency, meanwhile in the comparison of different ordering algorithms, our ARS-based ordering algorithms usually have comparable fault-detection effectiveness but much lower computation overhead, and thus are much more cost-effective.",,978-1-5090-4127-5978-1-5090-4128,10.1109/QRS.2016.49,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589817,test case prioritization;adaptive random sequence;random sequence;catergory partition;string distance,Random sequences;Measurement;Testing;Subspace constraints;Fault detection;Algorithm design and analysis;Semantics,fault diagnosis;program testing;scheduling,adaptive random sequence;category-partition-based distance;test case scheduling;regression testing;white-box information;black-box information;black-box test case prioritization techniques;test case diversity assessment;distance metrics;fault-detection effectiveness;ARS-based ordering algorithms,,3,31,,,,,,IEEE,IEEE Conferences
Test Case Prioritization Using Lexicographical Ordering,S. Eghbali; L. Tahvildari,"Department of Electrical and Computer Engineering, 200 University Ave West, University of Waterloo, Waterloo, Ontario; Department of Electrical and Computer Engineering, 200 University Ave West, University of Waterloo, Waterloo, Ontario",IEEE Transactions on Software Engineering,,2016,42,12,1178,1195,"Test case prioritization aims at ordering test cases to increase the rate of fault detection, which quantifies how fast faults are detected during the testing phase. A common approach for test case prioritization is to use the information of previously executed test cases, such as coverage information, resulting in an iterative (greedy) prioritization algorithm. Current research in this area validates the fact that using coverage information can improve the rate of fault detection in prioritization algorithms. The performance of such iterative prioritization schemes degrade as the number of ties encountered in prioritization steps increases. In this paper, using the notion of lexicographical ordering, we propose a new heuristic for breaking ties in coverage based techniques. Performance of the proposed technique in terms of the rate of fault detection is empirically evaluated using a wide range of programs. Results indicate that the proposed technique can resolve ties and in turn noticeably increases the rate of fault detection.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2016.2550441,Natural Sciences and Engineering Research Council of Canada; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456343,Regression testing;test case prioritization;lexicographical ordering,Software testing;Fault detection;Feature extraction;Regression analysis;Fault diagnosis,fault diagnosis;greedy algorithms;iterative methods;program testing;regression analysis,regression testing;coverage information;iterative greedy prioritization algorithm;fault detection;lexicographical ordering;test case prioritization,,6,60,,,,,,IEEE,IEEE Journals & Magazines
Test Effectiveness Evaluation of Prioritized Combinatorial Testing: A Case Study,E. Choi; S. Kawabata; O. Mizuno; C. Artho; T. Kitamura,NA; NA; NA; NA; NA,"2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)",,2016,,,61,68,"Combinatorial testing is a widely-used technique to detect system interaction failures. To improve test effectiveness with given priority weights of parameter values in a system under test, prioritized combinatorial testing constructs test suites where highly weighted parameter values appear earlier or more frequently. Such order-focused and frequency-focused combinatorial test generation algorithms have been evaluated using metrics called weight coverage and KL divergence but not sufficiently with fault detection effectiveness so far. We evaluate the fault detection effectiveness on a collection of open source utilities, applying prioritized combinatorial test generation and investigating its correlation with weight coverage and KL divergence.",,978-1-5090-4127-5978-1-5090-4128,10.1109/QRS.2016.17,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589785,Prioritized combinatorial testing;Pairwise testing;Fault detection;Weight coverage;KL divergence,Fault detection;Testing;Correlation;Frequency measurement;Electronic mail;Optical wavelength conversion,program testing;software fault tolerance,test effectiveness evaluation;prioritized combinatorial testing;system interaction failure detection;parameter value;weighted parameter value;order-focused combinatorial test generation algorithm;frequency-focused combinatorial test generation algorithm;weight coverage metric;KL divergence metric,,3,20,,,,,,IEEE,IEEE Conferences
Testing and Debugging in Continuous Integration with Budget Quotas on Test Executions,B. Jiang; W. K. Chan,NA; NA,"2016 IEEE International Conference on Software Quality, Reliability and Security (QRS)",,2016,,,439,447,"In Continuous Integration, a software application is developed through a series of development sessions, each with limited time allocated to testing and debugging on each of its modules. Test Case Prioritization can help execute test cases with higher failure estimate earlier in each session. When the testing time is limited, executing such prioritized test cases may only produce partial and prioritized execution coverage data. To identify faulty code, existing Spectrum-Based Fault Localization techniques often use execution coverage data but without the assumption of execution coverage priority. Is it possible to decompose these two steps for optimization within individual steps? In this paper, we study to what extent the selection of test case prioritization techniques may reduce its influence on the effectiveness of spectrum-based fault localization, thereby showing the possibility to decompose the process of continuous integration for optimization in workflow steps. We present a controlled experiment using the Siemens suite as subjects, nine test case prioritization techniques and four spectrum-based fault localization techniques. The findings showed that the studied test cases prioritization and spectrum-based fault localization can be customized separately, and, interestingly, prioritization over a smaller test suite can enable spectrum-based fault localization to achieve higher accuracy by assigning faulty statements with higher ranks.",,978-1-5090-4127-5978-1-5090-4128,10.1109/QRS.2016.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7589824,continuous integration;fault localization;test case prioritization;regression testing;debugging;standardization,Measurement;Testing;Clustering algorithms;Debugging;Computer bugs;Fault diagnosis;Optimization,program debugging;program testing;software fault tolerance,software testing;software debugging;continuous integration;budget quotas;test executions;software application;software development sessions;test case prioritization;failure estimate;execution coverage data;faulty code identification;spectrum-based fault localization;execution coverage priority;faulty statements,,,28,,,,,,IEEE,IEEE Conferences
Test-Suite Prioritisation by Application Navigation Tree Mining,M. Muzammal,NA,2016 International Conference on Frontiers of Information Technology (FIT),,2016,,,205,210,"Software tend to evolve over time and so does the test-suite. Regression testing is aimed at assessing that the software evolution did not compromise the working of the existing software components. However, as the software and consequently the test-suite grow in size, the execution of the entire test-suite for each new build becomes infeasible. Techniques like test-suite selection, test-suite minimisation and test-suite prioritisation have been proposed in literature for regression testing. Whilst all of these techniques are essentially an attempt to reduce the testing effort, test-suite selection and minimisation reduce the test-suite size whereas test-suite prioritisation provides a priority order of the test cases without changing the test-suite size. In this work, we focus on test-suite prioritisation. Recently, techniques from data mining have been used for test-suite prioritisation which consider the frequent pairs of interaction among the application interaction patterns. We propose test-Suite prioritisation by Application Navigation Tree mining (t-SANT). First, we construct an application navigation tree by way of extracting both tester and user interaction patterns. Next, we extract frequent sequences of interaction using a sequence mining algorithm inspired from sequential pattern mining. The most frequent longest sequences are assumed to model complex and most frequently used work-flows and hence a prioritisation algorithm is proposed that prioritises the test cases based on the most frequent and longest sequences. We show the usefulness of the proposed scheme with the help of two case studies, an online book store and calculator.",,978-1-5090-5300-1978-1-5090-5301,10.1109/FIT.2016.045,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7866754,Software testing;Test-suite prioritisation;Interaction pattern mining,Data mining;Testing;Software;Navigation;Calculators;Fault diagnosis;Databases,data mining;program testing;regression analysis;software maintenance;trees (mathematics),test-suite prioritisation;application navigation tree mining;regression testing;software evolution assessment;software components;test-suite selection;test-suite minimisation;data mining;application interaction pattern;t-SANT;tester interaction pattern extraction;user interaction pattern extraction;sequence mining algorithm;sequential pattern mining;prioritisation algorithm;test case prioritisation;online book store;calculator,,,26,,,,,,IEEE,IEEE Conferences
The drawbacks of statement code coverage test case prioritization related to domain testing,O. Banias,"ƒ??Politehnicaƒ? University of Timisoara, Romania",2016 IEEE 11th International Symposium on Applied Computational Intelligence and Informatics (SACI),,2016,,,221,224,"In this paper we study the weaknesses of the test case prioritization algorithms based on statement code coverage and applied to the domain test cases. We present the inconsistency between the principles of domain testing and the selection and prioritization practices over domain test cases on criteria unrelated to the scope of the domain testing. We continue the study by discussing the impact of 100% statement code coverage over the suites of domain test cases, studying why this type of code coverage should not produce effects over the domain test cases. Statement code coverage prioritization techniques related to unit testing, integration testing and regression testing phases are discussed, emphasizing the incompatibility between statement code coverage, domain testing and test case prioritization all at once.",,978-1-5090-2380-6978-1-5090-2379-0978-1-5090-2381,10.1109/SACI.2016.7507373,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7507373,,Software testing;Software;Computational intelligence;Informatics;Fault detection;Computer aided software engineering,program testing;regression analysis,statement code coverage;test case prioritization;domain testing;unit testing;integration testing;regression testing,,,12,,,,,,IEEE,IEEE Conferences
The Perception of Technical Debt in the Embedded Systems Domain: An Industrial Case Study,A. Ampatzoglou; A. Ampatzoglou; A. Chatzigeorgiou; P. Avgeriou; P. Abrahamsson; A. Martini; U. Zdun; K. Systa,NA; NA; NA; NA; NA; NA; NA; NA,2016 IEEE 8th International Workshop on Managing Technical Debt (MTD),,2016,,,9,16,"Technical Debt Management (TDM) has drawn the attention of software industries during the last years, including embedded systems. However, we currently lack an overview of how practitioners from this application domain perceive technical debt. To this end, we conducted a multiple case study in the embedded systems industry, to investigate: (a) the expected life-time of components that have TD, (b) the most frequently occurring types of TD in them, and (c) the significance of TD against run-time quality attributes. The case study was performed on seven embedded systems industries (telecommunications, printing, smart manufacturing, sensors, etc.) from five countries (Greece, Netherlands, Sweden, Austria, and Finland). The results of the case study suggest that: (a) maintainability is more seriously considered when the expected lifetime of components is larger than ten years, (b) the most frequent types of debt are test, architectural, and code debt, and (c) in embedded systems the run-time qualities are prioritized compared to design-time qualities that are usually associated with TD. The obtained results can be useful for both researchers and practitioners: the former can focus their research on the most industrially-relevant aspects of TD, whereas the latter can be informed about the most common types of TD and how to focus their TDM processes.",,978-1-5090-3854-1978-1-5090-3855,10.1109/MTD.2016.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776447,technical debt;embedded systems;industry;case study,Embedded systems;Industries;Companies;Embedded software;Time division multiplexing;Context,embedded systems;software architecture;software development management;software quality,architectural debt;code debt;run-time quality attributes;expected component lifetime;software industries;TDM process;technical debt management;embedded systems,,3,24,,,,,,IEEE,IEEE Conferences
To Be Optimal or Not in Test-Case Prioritization,D. Hao; L. Zhang; L. Zang; Y. Wang; X. Wu; T. Xie,"Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, Peking University, Beijing, P. R. China; Department of Computer Science, University of Illinois at Urbana-Champaign",IEEE Transactions on Software Engineering,,2016,42,5,490,505,"Software testing aims to assure the quality of software under test. To improve the efficiency of software testing, especially regression testing, test-case prioritization is proposed to schedule the execution order of test cases in software testing. Among various test-case prioritization techniques, the simple additional coverage-based technique, which is a greedy strategy, achieves surprisingly competitive empirical results. To investigate how much difference there is between the order produced by the additional technique and the optimal order in terms of coverage, we conduct a study on various empirical properties of optimal coverage-based test-case prioritization. To enable us to achieve the optimal order in acceptable time for our object programs, we formulate optimal coverage-based test-case prioritization as an integer linear programming (ILP) problem. Then we conduct an empirical study for comparing the optimal technique with the simple additional coverage-based technique. From this empirical study, the optimal technique can only slightly outperform the additional coverage-based technique with no statistically significant difference in terms of coverage, and the latter significantly outperforms the former in terms of either fault detection or execution time. As the optimal technique schedules the execution order of test cases based on their structural coverage rather than detected faults, we further implement the ideal optimal test-case prioritization technique, which schedules the execution order of test cases based on their detected faults. Taking this ideal technique as the upper bound of test-case prioritization, we conduct another empirical study for comparing the optimal technique and the simple additional technique with this ideal technique. From this empirical study, both the optimal technique and the additional technique significantly outperform the ideal technique in terms of coverage, but the latter significantly outperforms the former two techniques in terms of fault detection. Our findings indicate that researchers may need take cautions in pursuing the optimal techniques in test-case prioritization with intermediate goals.",0098-5589;1939-3520;2326-3881,,10.1109/TSE.2015.2496939,National 973 Program of China; National Natural Science Foundation of China; National Science Foundation; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314957,Test-Case Prioritization;Integer Linear Programming;Greedy Algorithm;Empirical Study;Test-case prioritization;integer linear programming;greedy algorithm;empirical study,Software;Measurement;Schedules;Fault detection;Integer linear programming;Software testing,integer programming;linear programming;program testing,test-case prioritization techniques;software testing;software quality;regression testing;simple additional coverage-based technique;optimal coverage-based test-case prioritization;integer linear programming;ILP problem,,14,47,,,,,,IEEE,IEEE Journals & Magazines
Tolerance to complexity: Measuring capacity of development teams to handle source code complexity,M. A. Barbosa; F. B. de Lima Neto; T. Marwala,"Faculty of Engineering and the Built Environment, University of Johannesburg, South Africa; Polytechnic school of Pernambuco, University of Pernambuco, Brazil; Faculty of Engineering and the Built Environment, University of Johannesburg, South Africa","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",,2016,,,2954,2959,"A well defined testing strategy is essential for any software development project. Testing efforts need to be carefully planed and executed in order to ensure effectiveness. Programming failures can represent a high risk for business. In order to mitigate such risk, companies have been increasingly investing more resources on software testing. In despite of massive investments on software testing and extensive collection of static analysis techniques and tools, there are still few conclusive explanations for what causes human programming failures on software. The hypothesis investigated in this paper is that a metric based on development teams characteristics can be more effective to predict defective source code than metrics purely focused on information about source code, alone. Aiming to assist software engineers during testing initiatives, this article presents a new approach to systematically measure capacity of development teams to handle source code complexity. The proposed metric can be effective for raising information and comparing multiple development teams, planning training initiatives and prioritising testing efforts. Experiments were carried out with the entire source code base of device drivers for Linux Operating System. Our approach was able to predict, with 80% of accuracy rate, which development teams introduced more issues from 2010 to 2014.",,978-1-5090-1897-0978-1-5090-1819-2978-1-5090-1898,10.1109/SMC.2016.7844689,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844689,,Complexity theory;Software;Measurement;Linux;Mathematical model;Testing;Conferences,Linux;program diagnostics;program testing;software development management;software metrics;source code (software);team working,complexity tolerance;development team capacity measurement;source code complexity handling;software development project;software testing strategy;programming failures;risk mitigation;static analysis tools;human programming failures;training initiative planning;device drivers;Linux Operating System,,,15,,,,,,IEEE,IEEE Conferences
UML-based reconfigurable middleware for design-level timing verification in model-based approach,R. Mzid; M. Abid,"CES Laboratory, National school of engineers of Sfax, Tunisia; CES Laboratory, National school of engineers of Sfax, Tunisia",2016 11th International Design & Test Symposium (IDT),,2016,,,181,186,"Model-based approaches for the development of software intensive real-time embedded systems allow early verification of timing properties at the design phase. In order to perform such verification, some aspects of the target software platform (i.e. the Real-Time Operating System (RTOS)) need to be considered such as priorities, scheduling policies, etc. However, one of the basic principles of model-based approaches, is to keep RTOS-independence of the design model. Hence, some assumptions on the software platform are implicitly made to achieve timing verification. This approach may lead to a mismatch between the design model and the RTOS-specific model describing the real-time application and thus, at the implementation level, timing properties may be affected. To tackle this issue, we define in this paper a reconfigurable middleware called RT-Mw. This middleware aims to explicitly describe the software assumptions at the design level for timing verification. Such approach allows early verification of these assumptions before the effective deployment which may prevents the mismatch between the design and the RTOS-Specific models. RT-Mw is described using UML modeling language together with the MARTE Standard.",2162-061X,978-1-5090-4900-4978-1-5090-4899-1978-1-5090-4901,10.1109/IDT.2016.7843037,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7843037,,Unified modeling language;Timing;Real-time systems;Middleware;Concrete;Standards,embedded systems;formal verification;middleware;operating systems (computers);Unified Modeling Language,UML modeling language;reconfigurable middleware;RT-Mw;design-level timing verification;model-based approach;embedded system development;RTOS;real-time operating system,,,13,,,,,,IEEE,IEEE Conferences
User-Centric Network Provisioning in Software Defined Data Center Environment,T. Bakhshi; B. Ghita,NA; NA,2016 IEEE 41st Conference on Local Computer Networks (LCN),,2016,,,289,297,"Present data center (DC) network provisioning schemes primarily utilize conventional load-balancing technologies, offering individual application performance improvement. Diversity in application usage however, makes isolated application prioritization a performance caveat for users with varying application trends. The present paper proposes a user profiling approach to capture application trends based on generic flow measurements (NetFlow) and employs the extracted profiles to create DC traffic forwarding policies. The scheme allows operators to define a global profile and application hierarchy based on extracted profiles to prioritize traffic for individual user classes. The proposed design was tested by extracting user profiles from a realistic enterprise network, and further simulated to dynamically manage DC traffic using the software defined networking paradigm. Compared to conventional traffic management schemes, the frame delivery ratio and effective throughput of our design was significantly higher for high priority north-south user traffic as well as the inter-server east-west application traffic.",,978-1-5090-2054-6978-1-5090-2055,10.1109/LCN.2016.57,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796801,Data center networking;software defined networking;user traffic profiling,Servers;Bandwidth;Real-time systems;Market research;Protocols;Algorithm design and analysis;Resource management,computer centres;software defined networking,user-centric network provisioning;software defined data center environment;user profiling approach;generic flow measurements;NetFlow;DC traffic forwarding policies;software defined networking paradigm;frame delivery ratio,,1,22,,,,,,IEEE,IEEE Conferences
Using memetic algorithms for test case prioritization in model based software testing,F. M. Nejad; R. Akbari; M. M. Dejam,"Software Engineering Lab, Department of Computer Engineering and IT, Shiraz University of Technology, Shiraz, Iran; Software Engineering Lab, Department of Computer Engineering and IT, Shiraz University of Technology, Shiraz, Iran; Shahabi Software Engineering Lab, Department of Computer Engineering and IT, Shiraz University of Technology, Shiraz, Iran",2016 1st Conference on Swarm Intelligence and Evolutionary Computation (CSIEC),,2016,,,142,147,"Building high quality software is one of the main goals in software industry. Software testing is a critical step in confirming the quality of software. Testing is an expensive activity because it consumes about 30% to 50% of all software developing cost. Today much research has been done in generating and prioritizing tests. First, tester should find the most important and critical path in software. They can reduce cost by finding errors and preventing to propagate it in design step. In this paper, a model based testing method is introduced. This method can prioritize tests using activity diagram, control flow graph, genetic and memetic algorithm. Different version of memetic algorithm has been made by stochastic local search, randomize iterative improvement, hill climbing and simulated annealing algorithms. The results show that the using local search methods with genetic algorithm (GA) provide efficiency and produce competitive results in comparison with GA.",,978-1-4673-8737,10.1109/CSIEC.2016.7482129,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482129,software testing;prioritization;Genetic;Memetic;Stochastic local search;Randomize iterative improvement;Hill climbing;Simulated annealing,Genetic algorithms;Software;Testing;Unified modeling language;Object oriented modeling;Memetics;Algorithm design and analysis,DP industry;genetic algorithms;program testing;search problems;software cost estimation;software quality,memetic algorithms;test case prioritization;model based software testing;high quality software;software industry;software developing cost;software critical path;cost reduction;model based testing method;activity diagram;control flow graph;genetic algorithm;stochastic local search;randomize iterative improvement;hill climbing;simulated annealing algorithms,,2,22,,,,,,IEEE,IEEE Conferences
Using Software Metrics Thresholds to Predict Fault-Prone Classes in Object-Oriented Software,A. Boucher; M. Badri,NA; NA,"2016 4th Intl Conf on Applied Computing and Information Technology/3rd Intl Conf on Computational Science/Intelligence and Applied Informatics/1st Intl Conf on Big Data, Cloud Computing, Data Science & Engineering (ACIT-CSII-BCD)",,2016,,,169,176,"Most code-based quality measurement approaches are based, at least partially, on values of multiple source code metrics. A class will often be classified as being of poor quality if the values of its metrics are above given thresholds, which are different from one metric to another. The metrics thresholds are calculated using various techniques. In this paper, we investigated two specific techniques: ROC curves and Alves rankings. These techniques are supposed to give metrics thresholds which are practical for code quality measurements or even for fault-proneness prediction. However, Alves Rankings technique has not been validated as being a good choice for fault-proneness prediction, and ROC curves only partially on few datasets. Fault-proneness prediction is an important field of software engineering, as it can be used by developers and testers as a test effort indication to prioritize tests. This will allow a better allocation of resources, reducing therefore testing time and costs, and an improvement of the effectiveness of testing by testing more intensively the components that are likely more fault-prone. In this paper, we wanted to compare empirically the selected threshold calculation methods used as part of fault-proneness prediction techniques. We also used a machine learning technique (Bayes Network) as a baseline for comparison. Thresholds have been calculated for different object-oriented metrics using four different datasets obtained from the PROMISE Repository and another one based on the Eclipse project.",,978-1-5090-4871-7978-1-5090-4872,10.1109/ACIT-CSII-BCD.2016.042,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7916977,Metrics Thresholds;Class-Level Metrics;Object-Oriented Metrics;Faults;Fault-Proneness Prediction;Code Quality;Object-Oriented Programming,Measurement;Prediction algorithms;Testing;Context;Object oriented modeling;Java;Software,learning (artificial intelligence);object-oriented methods;program testing;software metrics;software quality;source code (software),code-based quality measurement;source code metrics;ROC curves;Alves rankings;fault-proneness prediction;software engineering;test effort indication;machine learning;Bayes network;object-oriented metrics;PROMISE repository;Eclipse project,,2,28,,,,,,IEEE,IEEE Conferences
Visualization of combinatorial models and test plans,R. Tzoref-Brill; P. Wojciak; S. Maoz,"School of Computer Science, Tel Aviv University and IBM Research, Israel; IBM Systems, USA; School of Computer Science, Tel Aviv University, Israel",2016 31st IEEE/ACM International Conference on Automated Software Engineering (ASE),,2016,,,144,154,"Combinatorial test design (CTD) is an effective and widely used test design technique. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. One challenge for successful application of CTD in practice relates to this manual model definition and maintenance process. Another challenge relates to the comprehension and use of the test plan generated by CTD for prioritization purposes. In this work we introduce the use of visualizations as a means to address these challenges. We apply three different forms of visualization, matrices, graphs, and treemaps, to visualize the relationships between the different elements of the model, and to visualize the strength of each test in the test plan and the relationships between the different tests in terms of combinatorial coverage. We evaluate our visualizations via a user survey with 19 CTD practitioners, as well as via two industrial projects in which our visualization was used and allowed test designers to get vital insight into their models and into the coverage provided through CTD generated test plans.",,978-1-4503-3845-5978-1-5090-5571,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582753,Combinatorial Testing;Software Visualization,Visualization;Data visualization;Atmospheric modeling;Manuals;Computational modeling;Testing;Image color analysis,program testing;program visualisation,combinatorial models visualization;test plans visualization;combinatorial test design;automatic test plan generation;matrices;graphs;treemaps;combinatorial coverage;software visualization;CTD generated test plans,,,25,,,,,,IEEE,IEEE Conferences
10th International Workshop on Search-Based Software Testing (SBST 2017),J. P. Galeotti; J. Petke,NA; NA,2017 IEEE/ACM 10th International Workshop on Search-Based Software Testing (SBST),,2017,,,1,1,"Summary form only given, as follows. SBST 2017 Workshop Summary. Search-Based Software Testing (SBST) is the application of optimizing search techniques (for example, Genetic Algorithms) to solve problems in software testing. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service- orientated architectures, construct test suites for interaction testing, and validate real-time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.",,978-1-5386-2789-1978-1-5386-2790,10.1109/SBST.2017.16,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967910,,Search problems;Software testing;Software engineering;Genetic algorithms;Software,,,,,,,,,,,IEEE,IEEE Conferences
A Fault Based Approach to Test Case Prioritization,F. Farooq; A. Nadeem,NA; NA,2017 International Conference on Frontiers of Information Technology (FIT),,2017,,,52,57,"Regression testing is performed to ensure that the no new faults have been introduced in the software after modification and the software continues to work correctly. Regression testing is an expensive process because the test suite might be too large to execute in full. Thus to reduce the cost of such testing, regression testing techniques are used. One such technique is test case prioritization. Software testers assign priority to each test case to make sure that the test cases with higher priorities are executed first, in case of not having enough resources to execute the whole test suite. Test case prioritization is mainly used to increase fault detection rate of test suite which is the measure of how early faults are detected. In this paper, we propose an approach which exploits mutation testing in order to assign priorities to test cases. Using mutation testing, we introduce different faults in original program thus creating a number of mutated copies of the program and test case that exposes maximum number of these faults is given the highest priority. We report the outcomes of our experiments in which we applied our technique to test suites and calculated the fault detection rates produced by the prioritized test suites, comparing those rates of fault detection to the rates achieved by existing prioritization technique. The resulting data shows that prioritization technique proposed improved the fault detection rate of test suites.",,978-1-5386-3567-4978-1-5386-3568,10.1109/FIT.2017.00017,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8261011,Regression testing;Test case prioritization;Mutation testing,Testing;Fault detection;Software;Minimization;Computer science;History;Information technology,fault diagnosis;program testing;regression analysis,test case prioritization;regression testing techniques;fault detection rate;mutation testing;prioritization technique;test suite prioritization;software faults,,,17,,,,,,IEEE,IEEE Conferences
A Framework for Combining and Ranking Static Analysis Tool Findings Based on Tool Performance Statistics,A. Xypolytos; H. Xu; B. Vieira; A. M. T. Ali-Eldin,NA; NA; NA; NA,"2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)",,2017,,,595,596,"This paper proposes a conceptual, performance-based ranking framework that prioritises the output of multiple Static Analysis Tools, to improve the tool effectiveness and usefulness. The framework weights the performance of Static Analysis Tools per defect type and cross-validates the findings between different Static Analysis Tools' reports. An initial validation shows the potential benefits of the proposed framework.",,978-1-5386-2072-4978-1-5386-2073,10.1109/QRS-C.2017.110,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004389,Static Analysis;Benchmarking;Ranking,Tools;Benchmark testing;Software;Security;Ranking (statistics);Manuals;Computer architecture,program diagnostics;statistics,static analysis tool finding combination;static analysis tool finding ranking;tool performance statistics;conceptual-based ranking framework;performance-based ranking framework;defect type,,,8,,,,,,IEEE,IEEE Conferences
A General Framework for Dynamic Stub Injection,M. Christakis; P. Emmisberger; P. Godefroid; P. M?¬ller,NA; NA; NA; NA,2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE),,2017,,,586,596,"Stub testing is a standard technique to simulate the behavior of dependencies of an application under test such as the file system. Even though existing frameworks automate the actual stub injection, testers typically have to implement manually where and when to inject stubs, in addition to the stub behavior. This paper presents a novel framework that reduces this effort. The framework provides a domain specific language to describe stub injection strategies and stub behaviors via declarative rules, as well as a tool that automatically injects stubs dynamically into binary code according to these rules. Both the domain specific language and the injection are language independent, which enables the reuse of stubs and injection strategies across applications. We implemented this framework for both unmanaged (assembly) and managed (.NET) code and used it to perform fault injection for twelve large applications, which revealed numerous crashes and bugs in error handling code. We also show how to prioritize the analysis of test failures based on a comparison of the effectiveness of stub injection rules across applications.",1558-1225,978-1-5386-3868-2978-1-5386-3869,10.1109/ICSE.2017.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985696,,Instruments;DSL;Computer bugs;Testing;Runtime;Debugging,binary codes;program diagnostics;program testing;specification languages,stub testing;fault injection;binary code;declarative rules;stub injection strategies;domain specific language,,,26,,,,,,IEEE,IEEE Conferences
A Greedy-Based Method for Modified Condition/Decision Coverage Testing Criterion,B. Li; C. Huang,NA; NA,2017 IEEE 36th Symposium on Reliable Distributed Systems (SRDS),,2017,,,244,246,"During software regression testing, the code coverage of target program is a crucial factor while we perform test case reduction and prioritization. Modified Condition/ Decision Coverage (MC/DC) is one of the most strict and high-accuracy criterion in code coverage and it is usually considered necessary for adequate testing of critical software. In the past, Hayhurst et al proposed a method to implement the MC/DC criterion that complies with regulatory guidance for DO-178B level A software. Hayhurst's MC/DC approach was to find some test cases which are satisfied by MC/DC criterion for each operator (and, or, not, or xor) in the Boolean expression. However, there could be some problems when using Hayhurst's MC/DC approach to select test cases. In this paper, we discuss how to improve and/or enhance Hayhurst's MC/DC approach by using a greedy-based method. Some experiments are performed based on real programs to evaluate as well as compare the performance of our proposed and Hayhurst's approaches.",,978-1-5386-1679-6978-1-5386-1680,10.1109/SRDS.2017.33,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8069087,Test suite reduction;Modified Condition/ Decision Coverage;Software testing;Fault detection effectivenes,Software;Fault detection;Flexible printed circuits;Software testing;Greedy algorithms;Software algorithms,program testing;safety-critical software,DO-178B level A software;greedy-based method;software regression testing;code coverage;test case reduction;high-accuracy criterion;critical software;Hayhurst MC-DC approach;modified condition-decision coverage;Boolean expression,,,15,,,,,,IEEE,IEEE Conferences
A novel approach to multiple criteria based test case prioritization,R. Abid; A. Nadeem,"Department of Computer Science, Capital University of Science and Technology, Islamabad, Pakistan; Department of Computer Science, Capital University of Science and Technology, Islamabad, Pakistan",2017 13th International Conference on Emerging Technologies (ICET),,2017,,,1,6,"When software is modified, it is retested to ensure that no new faults have been introduced in the previously tested code and it still works correctly. Such testing is known as regression testing. The cost of regression testing is high because the original program has large number of test cases. It is not feasible to execute all test cases for regression testing. Test suite minimization, test case selection and test case prioritization are cost commonly used techniques in regression testing to reduce the cost of regression testing. While test suite minimization and test case selection techniques select a subset of test cases, test case prioritization does not eliminate any test case, it only orders the test cases with the objective of increasing the fault detection rate. Prioritization is usually preferred over other two approaches because it does not involve the risk of losing useful test cases. Prioritization techniques assign priority to each test case on the basis of some coverage criteria. A number of different single criterion and multiple criteria based prioritization techniques have been proposed in the literature. Multiple criteria based prioritization techniques perform better than single criterion based prioritization techniques. The existing multiple criteria based prioritization techniques combine the criteria in such a way that ƒ??Additionalƒ? strategy cannot be applied on them. In this paper, we propose a new multiple criteria based test case prioritization algorithm that considers two criteria to prioritize test cases using ƒ??Additionalƒ? strategy. One criterion is considered as primary and other is considered as secondary. Primary criterion is used to prioritize the test cases whereas secondary criterion is used to break the tie among test cases when two or more test cases provide equal coverage of entities of first criterion. Our proposed multiple criteria based prioritization algorithm performs better than the existing prioritization techniques.",,978-1-5386-2260-5978-1-5386-2259-9978-1-5386-2261,10.1109/ICET.2017.8281742,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8281742,,Software;Minimization;Fault detection;Software algorithms;History;Software testing,program testing,prioritization techniques;test case prioritization algorithm;regression testing;test suite minimization;test case selection techniques;multiple criteria based test case prioritization;software modification;additional strategy,,,20,,,,,,IEEE,IEEE Conferences
A QoS Guaranteed Technique for Cloud Applications Based on Software Defined Networking,F. Li; J. Cao; X. Wang; Y. Sun,"School of Computer Science and Engineering, Northeastern University, Shenyang, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China",IEEE Access,,2017,5,,21229,21241,"Due to the centralized control, network-wide monitoring and flow-level scheduling of software-defined-networking (SDN), it can be utilized to achieve quality of service (QoS) for cloud applications and services, such as voice over IP, video conference, and online games. However, most existing approaches stay at the QoS framework design and test level, while few works focus on studying the basic QoS techniques supported by SDN. In this paper, we enable SDN with QoS guaranteed abilities, which could provide end-to-end QoS routing for each cloud user service. First of all, we implement an application identification technique on SDN controller to determine required QoS levels for each application type. Then, we implement a queue scheduling technique on SDN switch. It queues the application flows into different queues and schedules the flows out of the queues with different priorities. At last, we evaluate the effectiveness of the proposed SDN-based QoS technique through both theoretical and experimental analysis. Theoretical analysis shows that our methods can provide differentiated services for the application flows mapped to different QoS levels. Experiment results show that when the output interface has sufficiently available bandwidth, the delay can be reduced by 28% on average. In addition, for the application flow with the highest priority, our methods can reduce 99.99% delay and increase 90.17% throughput on average when the output interface utilization approaches to the maximum bandwidth limitation.",2169-3536,,10.1109/ACCESS.2017.2755768,RGC General Research Fund; National Natural Science Foundation of China; China Postdoctoral Science Foundation; Fundamental Research Funds for the Central Universities Project; CERNET Innovation Project; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048495,Software defined networking;cloud computing;QoS;applications,Quality of service;Switches;Cloud computing;Routing;Delays;Computational modeling,cloud computing;quality of service;scheduling;software defined networking,centralized control;network-wide monitoring;cloud applications;application identification technique;SDN controller;queue scheduling technique;SDN switch;QoS guaranteed technique;software defined networking;quality of service;end-to-end QoS routing,,1,37,,OAPA,,,,IEEE,IEEE Journals & Magazines
A Regression Test Case Prioritization Algorithm Based on Program Changes and Method Invocation Relationship,W. Fu; H. Yu; G. Fan; X. Ji; X. Pei,NA; NA; NA; NA; NA,2017 24th Asia-Pacific Software Engineering Conference (APSEC),,2017,,,169,178,"Regression testing is essential for assuring the quality of a software product. Because rerunning all test cases in regression testing may be impractical under limited resources, test case prioritization is a feasible solution to optimize regression testing by reordering test cases for the current testing version. In this paper, we propose a new test case prioritization algorithm based on program changes and method (function) invocation relationship. Combining the estimated risk value of each program method (function) and the method (function) coverage information, the fault detection capability of each test case can be calculated. The algorithm reduces the prioritization problem to an integer linear programming (ILP) problem, and finally prioritizes test cases according to their fault detection capabilities. Experiments are conducted on 11 programs to validate the effectiveness of our proposed algorithm. Experimental results show that our approach is more effective than some well studied test case prioritization techniques in terms of average percentage of fault detected (APFD) values.",,978-1-5386-3681-7978-1-5386-3682,10.1109/APSEC.2017.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305939,regression testing;test case prioritization;program changes;method invocation,Fault detection;Testing;Greedy algorithms;Software;Software algorithms;Complexity theory;Heuristic algorithms,integer programming;linear programming;program testing;regression analysis;software fault tolerance;software quality,program changes;method invocation relationship;regression testing;program method;method coverage information;fault detection capability;prioritization problem;integer linear programming problem;prioritizes test cases;regression test case prioritization algorithm;software product quality,,,36,,,,,,IEEE,IEEE Conferences
A survey on prioritization regression testing test case,D. Suleiman; M. Alian; A. Hudaib,"Computer Science Department, King Hussein Faculty of Computing Sciences, Princess Sumaya University for Technology, Teacher at the University of Jordan, Amman, Jordan; Basic sciences Department, Faculty of science, Hashemite University, Zarqa, Jordan; Department of Computer Information Systems, King Abdullah II for Information Technology the University of Jordan, Amman, Jordan",2017 8th International Conference on Information Technology (ICIT),,2017,,,854,862,"Regression testing is a process used to measure the validity of the system during software maintenance. Regression testing process is very expensive and must be introduced each time a modification occurs in software to ensure that the system still work and that the new modification doesn't cause any bugs, this process depends on selecting test cases from a test suite. Selection of test cases is very critical since it affect the regression testing time and effort, so that many algorithms exist to enhance regression testing process. One of the methods used to make enhancements is to select test cases using prioritization testing techniques. Prioritization techniques find the bugs early to improve regression testing efficiency by prioritizing the test cases. In this paper many regression testing prioritization techniques were reviewed and analyzed.",,978-1-5090-6332-1978-1-5090-6333,10.1109/ICITECH.2017.8079958,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8079958,component;Prioritization;Requirements;Regression;Test Cases;Test Suite;Coverage;Model;GA,Testing;Software;Genetic algorithms;Information technology;Time factors;Fault detection;Software algorithms,program testing;software maintenance,prioritization regression;test case;regression testing process;test suite;regression testing time;prioritization testing techniques;software maintenance,,,43,,,,,,IEEE,IEEE Conferences
"A Test Case Recommendation Method Based on Morphological Analysis, Clustering and the Mahalanobis-Taguchi Method",H. Aman; T. Nakano; H. Ogasawara; M. Kawahara,NA; NA; NA; NA,"2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",,2017,,,29,35,"This paper focuses on the content of test cases, and categorizes test cases into clusters using the similarity between test cases, their degree of similarity is obtained through a morphological analysis. If there are two similar test cases, they would test the same or similar functionalities in similar but different conditions. Thus, when one of them is run for a regression testing, the remaining one should be run as well, in order to reduce a risk of overlooking regressions. Once a test engineer decides to run a set of test cases, the method proposed in this paper can recommend adding similar test cases to their candidate set. The proposed method also considers the priorities of recommended test cases by using the Mahalanobis-Taguchi method. This paper reports on an empirical study with an industrial software product. The results show that the proposed method is useful to prevent overlooking regressions.",,978-1-5090-6676-6978-1-5090-6677,10.1109/ICSTW.2017.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899026,regression testing;test case recommendation;morphological analysis;clustering;Mahalanobis-Taguchi method,Testing;Software;History;Indexes;Proposals;Focusing;Feature extraction,pattern clustering;program testing;recommender systems;regression analysis,test case recommendation method;morphological analysis;Mahalanobis-Taguchi method;test case categorization;regression testing;industrial software product,,1,15,,,,,,IEEE,IEEE Conferences
Adapting code maintainability to bat-inspired test case prioritization,M. M. ??zt?¬rk,"Department of Computer Engineering, Engineering Faculty, Isparta, TURKEY",2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA),,2017,,,67,72,"Time and budget constraints in developing a software create an adverse effect in terms of the adequacy of maintenance and test processes. This case can be considered as a burden for persons who account for test processes. In order to alleviate this burden, test case prioritization is one of the solutions. A nature-inspired method namely BITCP, which was developed based on bat algorithm, produced promising results. However, this method does not involve test case elements with respect to the code maintainability. In this work, the correlation between some code maintainability indicators including WMC, LCOM, and Coupling and cyclomatic complexity is investigated. IMPBITCP appears after adapting the results of the investigation to BITCP. The method is then compared with well known alternatives such as greedy-search, particle swarm optimization, and BITCP. The experiment involving six open source project showed that IMPCBITCP outperformed the others with respect to the APFD. The findings of the work indicates that if the factors affecting code maintenance are considered while developing test case prioritization techniques, APFD results becomes high and stable.",,978-1-5090-5795-5978-1-5090-5796,10.1109/INISTA.2017.8001134,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8001134,bat algorithm;test case prioritization;nature inspired methods,Complexity theory;Software;Couplings;Time-frequency analysis;Libraries;Genetic algorithms,optimisation;program testing,code maintainability;bat-inspired test case prioritization;BITCP;WMC;LCOM;cyclomatic complexity;APFD,,,27,,,,,,IEEE,IEEE Conferences
ALOJA: A Framework for Benchmarking and Predictive Analytics in Hadoop Deployments,J. L. Berral; N. Poggi; D. Carrera; A. Call; R. Reinauer; D. Green,"Barcelona Supercomputing Center, Universitat Polit??cnica de Catalunya, Barcelona, Spain; Barcelona Supercomputing Center, Universitat Polit??cnica de Catalunya, Barcelona, Spain; Barcelona Supercomputing Center, Universitat Polit??cnica de Catalunya, Barcelona, Spain; Barcelona Supercomputing Center, Universitat Polit??cnica de Catalunya, Barcelona, Spain; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA",IEEE Transactions on Emerging Topics in Computing,,2017,5,4,480,493,"This article presents the ALOJA project and its analytics tools, which leverages machine learning to interpret big data benchmark performance data and tuning. ALOJA is part of a long-term collaboration between Barcelona Supercomputing Center and Microsoft to automate the characterization of cost-effectiveness on big data deployments, currently focusing on Hadoop. Hadoop presents a complex run-time environment, where costs and performance depend on a large number of configuration choices. The ALOJA project has created an open, vendor-neutral repository, featuring over 40000 Hadoop job executions and their performance details. The repository is accompanied by a test bed and tools to deploy and evaluate the cost-effectiveness of different hardware configurations, parameters, and cloud services. Despite early success within ALOJA, a comprehensive study requires automation of modeling procedures to allow an analysis of large and resource-constrained search spaces. The predictive analytics extension, ALOJA-ML, provides an automated system allowing knowledge discovery by modeling environments from observed executions. The resulting models can forecast execution behaviors, predicting execution times for new configurations and hardware choices. That also enables model-based anomaly detection or efficient benchmark guidance by prioritizing executions. In addition, the community can benefit from ALOJA data sets and framework to improve the design and deployment of big data applications.",2168-6750;2376-4562,,10.1109/TETC.2015.2496504,"European Research Council through the European Unionƒ??s Horizon 2020 Research and Innovation Programme; Ministerio de Econom??a y Competitividad, Spain; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7312954,Data-center management;Hadoop;benchmarks;modeling and prediction;machine learning;execution experiences,Benchmark testing;Hardware;Predictive models;Software;Analytical models;Licenses;Big data,Big Data;cloud computing;data analysis;data mining;learning (artificial intelligence);parallel processing,ALOJA data sets;ALOJA project;big data benchmark performance data;long-term collaboration;Barcelona Supercomputing Center;big data deployments;complex run-time environment;open vendor-neutral repository;different hardware configurations;resource-constrained search spaces;predictive analytics extension;ALOJA-ML;automated system;Big data applications;Hadoop job executions,,,25,,,,,,IEEE,IEEE Journals & Magazines
An Empirical Comparison of Similarity Measures for Abstract Test Case Prioritization,R. Huang; Y. Zhou; W. Zong; D. Towey; J. Chen,NA; NA; NA; NA; NA,2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC),,2017,1,,3,12,"Test case prioritization (TCP) attempts to order test cases such that those which are more important, according to some criterion or measurement, are executed earlier. TCP has been applied in many testing situations, including, for example, regression testing. An abstract test case (also called a model input) is an important type of test case, and has been widely used in practice, such as in configurable systems and software product lines. Similarity-based test case prioritization (STCP) has been proven to be cost-effective for abstract test cases (ATCs), but because there are many similarity measures which could be used to evaluate ATCs and to support STCP, we face the following question: How can we choose the similarity measure(s) for prioritizing ATCs that will deliver the most effective results? To address this, we studied fourteen measures and two popular STCP algorithms - local STCP (LSTCP), and global STCP (GSTCP). We also conducted an empirical study of five realworld programs, and investigated the efficacy of each similarity measure, according to the interaction coverage rate and fault detection rate. The results of these studies show that GSTCP outperforms LSTCP - in 61% to 84% of the cases, in terms of interaction coverage rates; and in 76% to 78% of the cases with respect to fault detection rates. Our studies also show that Overlap, the simplest similarity measure examined in this study, could obtain the overall best performance for LSTCP; and that Goodall3 has the best performance for GSTCP.",0730-3157,978-1-5386-0367-3978-1-5386-0368,10.1109/COMPSAC.2017.271,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8029584,Software testing;test case prioritization;abstract test case;similarity,Testing;Fault detection;Software;Computer science;Software product lines;Fault diagnosis;Algorithm design and analysis,program testing,global STCP;local STCP;ATC;abstract test case prioritization;similarity measures,,2,25,,,,,,IEEE,IEEE Conferences
An Empirical Examination of Abstract Test Case Prioritization Techniques,R. Huang; W. Zong; D. Towey; Y. Zhou; J. Chen,NA; NA; NA; NA; NA,2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C),,2017,,,141,143,"Abstract test case prioritization (ATCP) aims at ordering abstract test case in order to increase the speed at which faults are detected, potentially increasing the fault detection rate. This paper empirically examines possible ATCP techniques, according to the following four categories: non-information-guided prioritization (NIGP), interaction coverage based prioritization (ICBP), input-model mutation based prioritization (IMBP), and similarity based prioritization (SBP). We found that the ICBP category has better testing effectiveness than others, according to fault detection rates. Surprisingly, we found that NIGP can achieve similar performance to IMBP, and that SBP can sometimes achieve even better rates of fault detection than some ICBP techniques.",,978-1-5386-1589-8978-1-5386-1590,10.1109/ICSE-C.2017.105,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965282,Software testing;abstract test case prioritization;empirical examination,Testing;Fault detection;Software;Conferences;Software product lines;Computer science,fault diagnosis;program testing,abstract test case prioritization techniques;ATCP techniques;fault detection;noninformation-guided prioritization category;NIGP category;interaction coverage based prioritization category;ICBP category;input-model mutation based prioritization category;IMBP category;similarity based prioritization category;SBP category;testing effectiveness;empirical examination;software testing,,2,24,,,,,,IEEE,IEEE Conferences
An empirical study on clustering approach combining fault prediction for test case prioritization,L. Xiao; H. Miao; W. Zhuang; S. Chen,"School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; Meitu corporation, Xiamen, China",2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS),,2017,,,815,820,"Using Clustering algorithm to improve the effectiveness of test case prioritization has been well recognized by many researchers. Software fault prediction has been one of the active parts of software engineering, but to date, there are few test cases prioritization technique using fault prediction. We conjecture that if the code has a fault-proneness, the test cases covering the code will find fault with higher probability. In addition, most of the existing test cases prioritization techniques using clustering algorithm don't consider the number of clusters. Thus, in this paper, we design a test case prioritization based on clustering approach combining fault prediction. We consider the method to obtain the best number of clusters and the clustering prioritization based on the results of fault prediction. To investigate the effectiveness of our approach, we perform an empirical study using an object which contains test cases and faults. The experiment results indicate that our techniques can improve the effectiveness of test case prioritization.",,978-1-5090-5507-4978-1-5090-5508,10.1109/ICIS.2017.7960104,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960104,clustering algorithm;fault prediction;test case prioritization;empirical study,Clustering algorithms;Prediction algorithms;Feature extraction;Software;Testing;Complexity theory;Algorithm design and analysis,fault diagnosis;pattern clustering;program testing;software maintenance,clustering prioritization;software engineering;test case prioritization effectiveness improvement;clustering algorithm;software fault prediction,,,22,,,,,,IEEE,IEEE Conferences
An empirical study on clustering approach combining fault prediction for test case prioritization,L. Xiao; H. Miao; W. Zhuang; S. Chen,"School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer Engineering and Science, Shanghai University, Shanghai, China; School of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; Meitu corporation, Xiamen, China",2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS),,2017,,,815,820,"Using Clustering algorithm to improve the effectiveness of test case prioritization has been well recognized by many researchers. Software fault prediction has been one of the active parts of software engineering, but to date, there are few test cases prioritization technique using fault prediction. We conjecture that if the code has a fault-proneness, the test cases covering the code will findfault with higher probability. In addition, most of the existing test cases prioritization techniques using clustering algorithm don't consider the number of clusters. Thus, in this paper, we design a test case prioritization based on clustering approach combining fault prediction. We consider the method to obtain the best number of clusters and the clustering prioritization based on the results of fault prediction. To investigate the effectiveness of our approach, we perform an empirical study using an object which contains test cases and faults. The experiment results indicate that our techniques can improve the effectiveness of test case prioritization.",,978-1-5090-5507-4978-1-5090-5508,10.1109/ICIS.2017.7960105,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7960105,clustering algorithm;fault prediction;test case prioritization;empirical study,Clustering algorithms;Prediction algorithms;Feature extraction;Software;Testing;Complexity theory;Algorithm design and analysis,,,,,22,,,,,,IEEE,IEEE Conferences
An Empirical Study on the Effect of Testing on Code Quality Using Topic Models: A Case Study on Software Development Systems,T. Chen; S. W. Thomas; H. Hemmati; M. Nagappan; A. E. Hassan,"Department of Computer Science and Software Engineering, Concordia University, Montreal, QC, Canada; Software Analysis and Intelligence Lab (SAIL), Queen's University, Kingston, ON, Canada; University of Calgary, Calgary, AB, Canada; University of Waterloo, Waterloo, ON, Canada; Software Analysis and Intelligence Lab (SAIL), Queen's University, Kingston, ON, Canada",IEEE Transactions on Reliability,,2017,66,3,806,824,"Previous research in defect prediction has proposed approaches to determine which files require additional testing resources. However, practitioners typically create tests at a higher level of abstraction, which may span across many files. In this paper, we study software testing, especially test resource prioritization, from a different perspective. We use topic models to generate topics that provide a high-level view of a system, allowing developers to look at the test case coverage from a different angle. We propose measures of how well tested and defect prone a topic is, allowing us to discover which topics are well tested and which are defect prone. We conduct case studies on the histories of Mylyn, Eclipse, and NetBeans. We find that 34-78% of topics are shared between source code and test files, indicating that we can use topic models to study testing; well-tested topics are usually less defect prone, defect-prone topics are usually undertested; we can predict which topics are defect prone but not well tested with an average precision and recall of 75% and 77%, respectively; our approach complements traditional prediction-based approaches by saving testing and code inspection effort; and our approach is not particularly sensitive to the parameters that we use.",0018-9529;1558-1721,,10.1109/TR.2017.2699938,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949108,Code quality;empirical study;testing;topic models,Microwave integrated circuits;Software;Measurement;Correlation;Predictive models;Software testing,program testing;software quality;source code (software),empirical analysis;code quality testing;topic model;software development systems;defect prediction;software testing;test resource prioritization;test case coverage;Mylyn;Eclipse;NetBeans;source code;test files;defect-prone topics;precision value;recall value,,2,69,,Traditional,,,,IEEE,IEEE Journals & Magazines
An extended adaptive process model for agile software development methodology,S. Sadaf; S. Iqbal; A. Saba; M. KamarMohsin,"Department of Computer Science and Engineering, Al-Falah University, Dhauj, Faridabad, Haryana, India; Department of Computer Science and Engineering, Al-Falah University, Dhauj, Faridabad, Haryana, India; Department of Computer Science and Engineering, Al-Falah University, Dhauj, Faridabad, Haryana, India; Department of Computer Science and Engineering, Al-Falah University, Dhauj, Faridabad, Haryana, India","2017 International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)",,2017,,,1373,1378,"Agile methodologies focus on the agility for the development of software. Among various agile methods, eXtreme Programming (XP) is the most adopted agile method. XP has been used for the development of other agile methods, for example, ƒ??adaptive software development process modelƒ? (ASDPM), which is the modified approach of XP. ASDPM was proposed to support the following activities: ƒ??(a) communication and planning, (b) analysis, (c) design and development, and (d) testingƒ?. Based on our literature review of ASDPM, we identify that ASDPM does not support the following: (i) how to identify the different types of agile team members who will participate during the communication and planning phase?; and (ii) how to deliver the most important requirements of the software during analysis phase?. Therefore, in order to address this issue and to strengthen the analysis phase of agile process models, in this paper we propose an extended adaptive process model (APM) for agile software development methodology. This method includes the following steps: (1) identification of agile team members, (2) communication and planning, (3) analysis includes the computation of function point of each requirement; and the selection and prioritization of the requirements, (4) design and development, (5) testing. Finally, the utilization of the proposed method is demonstrated with the help of Institute Examination System, as a case study.",,978-1-5090-6106-8978-1-5090-6107,10.1109/ICICICT1.2017.8342770,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8342770,Agile methods;XP;adaptive software development;function point,Software;Adaptation models;Agile software development;Planning;Testing;Analytical models;Variable speed drives,project management;software development management;software prototyping;software quality,XP;ASDPM;agile team members;planning phase;analysis phase;agile process models;agile software development methodology;communication phase;adaptive process model;eXtreme programming;adaptive software development process model;testing;design and development;Institute examination system,,,25,,,,,,IEEE,IEEE Conferences
An Ilities-Driven Methodology for the Analysis of Gaps of Stakeholders Needs in Space Systems Conceptual Design,S. Corpino; F. Nichele,"Department of Mechanical and Aerospace Engineering, Politecnico di Torino, Torino, Italy; Department of Mechanical and Aerospace Engineering, Politecnico di Torino, Torino, Italy",IEEE Systems Journal,,2017,11,4,2182,2191,"The new generation of space-based services includes large-scale, integrated, and distributed informational systems for which traditional system engineering approaches show some limits in delivering the ƒ??big picture.ƒ? Missing the view of the full range of design options, or prematurely translating the perceived stakeholders needs into design requirements, is often a consequence of insufficient regard to the end-users priorities. Objective of the present research is to bring to light the gaps extant between what system architects prioritize, and the preferences of potential system users. To this purpose, the proposed method aims at incorporating lifecycle properties (-ilities) in the concept design phase, by submitting attributes of these properties for the evaluation of two stakeholders representative groups. The case study refers to the integration of environmental measurements, coming from a global-navigation-satellite-systems-based remote sensing satellite constellation, as complementary data to the traditional weather-forecasting service, resulting in a new system of systems. The method runs through an interview-based quality function deployment process and collaborative sessions of teams of stakeholders. The strength of the formulation relies on the ability to treat a quantitative measure of the gaps extant between system desired capabilities as perceived by architects, and real end-user needs. The method can be potentially tested in a concurrent design environment as a complementary tool for eliciting requirements and suggesting the areas where investments and resources should be preferably allocated. Results can be used by researchers as pieces of knowledge to be further investigated, and by practitioners in development projects, taking into account that they are preliminary findings.",1932-8184;1937-9234;2373-7816,,10.1109/JSYST.2016.2572089,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7498637,Design methods techniques and tools;distributed space systems;lifecycle properties;satellite constellations;stakeholders expectations in systems design,Stakeholders;Quality function deployment;Systems engineering and theory;Software engineering;Satellites;Atmospheric measurements,aerospace computing;groupware;information systems;Internet;quality function deployment;remote sensing;systems analysis;systems engineering,stakeholders needs;space systems conceptual design;informational systems;design requirements;end-users priorities;lifecycle properties;concept design phase;environmental measurements;global-navigation-satellite-systems;remote sensing satellite constellation;quality function deployment process;concurrent design environment;system engineering;system architecture;ilities-driven methodology;weather-forecasting service,,1,32,,Traditional,,,,IEEE,IEEE Journals & Magazines
An Industrial Study of Natural Language Processing Based Test Case Prioritization,Y. Yang; X. Huang; X. Hao; Z. Liu; Z. Chen,NA; NA; NA; NA; NA,"2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)",,2017,,,548,549,"In mobile application development, the frequentsoftware release limits the testing time resource. In order todetect bugs in early phases, researchers proposed various testcase prioritization (TCP) techniques in past decades. In practice, considering that some test case is described or contains text, theresearchers also employed Natural Language Processing (NLP)to assist the TCP techniques. This paper conducted an extensiveempirical study to analyze the performance of three NLP basedTCP technologies, which is based on 15059 test cases from 30industrial projects. The result shows that all of these threestrategies can help to improve the efficiency of software testing, and the Risk strategy achieved the best performance across thesubject programs.",,978-1-5090-6031-3978-1-5090-6032,10.1109/ICST.2017.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7928016,Software Testing;Test case prioritization;NLP,Natural language processing;Mobile applications;Software;Software testing;Training;Dictionaries,mobile computing;natural language processing;program testing,software testing;industrial projects;NLP-based TCP technologies;mobile application development;natural language processing based test case prioritization,,,4,,,,,,IEEE,IEEE Conferences
Automated System-Level Regression Test Prioritization in a Nutshell,P. Erik Strandberg; W. Afzal; T. J. Ostrand; E. J. Weyuker; D. Sundmark,Westermo Research and Development; M&#x00E4;lardalen University; M&#x00E4;lardalen University; M&#x00E4;lardalen University; M&#x00E4;lardalen University,IEEE Software,,2017,34,4,30,37,"Westermo Research and Development has developed SuiteBuilder, an automated tool to determine an effective ordering of regression test cases. The ordering is based on factors such as fault detection success, the interval since the last execution, and code modifications. SuiteBuilder has enabled Westermo to overcome numerous regression-testing problems, including lack of time to run a complete regression suite, failure to detect bugs in a timely manner, and repeatedly omitted tests. In the tool's first two years of use, reordered test suites finished in the available time, most fault-detecting test cases were located in the first third of suites, no important test case was omitted, and the necessity for manual work on the suites decreased greatly.",0740-7459;1937-4194,,10.1109/MS.2017.92,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7974685,regression testing;software testing;testing strategies;test execution;testing tools;test levels;validation;SuiteBuilder;Westermo;software engineering;software development,Software testing;Software development;Databases;Reliability engineering;Computer bugs;Regression analysis,program testing,automated system-level regression test prioritization;SuiteBuilder;complete regression suite;fault-detecting test cases,,,9,,,,,,IEEE,IEEE Journals & Magazines
Autonomous observation of multiple USVs from UAV while prioritizing camera tilt and yaw over UAV motion,C. G. L. Krishna; M. Cao; R. R. Murphy,"Department of Computer Science and Engineering, Texas A&amp;M University, College Station, Texas 77843; Department of Computer Science and Engineering, Texas A&amp;M University, College Station, Texas 77843; Department of Computer Science and Engineering, Texas A&amp;M University, College Station, Texas 77843","2017 IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR)",,2017,,,141,146,"This paper proposes a scheme for observing cooperative Unmanned Surface Vehicles (USV), using a rotorcraft Unmanned Aerial Vehicle (UAV) with camera movements (tilt and yaw) prioritized over UAV movements. Most of the current researches consider a fixed-wing type UAV for surveillance of multiple moving targets (MMT), whose functionality is limited to just UAV movements. Experiments in simulation are conducted and verified that, prioritizing camera movements increased the number of times each USV is visited (on an average by 5.68 times more), decreased the percentage of the duration that the UAV is not observing any USV (on an average by 19.8%) and increased the efficiency by decreasing the distance traveled by the UAV (on an average by 747 pixels) for the six test cases. Autonomous repositioning of the UAV at regular intervals to observe USVs during a disaster scenario will provide the operator with better situational awareness. Using a rotorcraft over a fixed-wing type UAV provides the operator with a flexibility of observing the target for the required duration by hovering and freedom of unrestricted movements, which help improve the efficiency of target observation.",2475-8426,978-1-5386-3923-8978-1-5386-3922-1978-1-5386-3924,10.1109/SSRR.2017.8088154,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8088154,,Cameras;Unmanned aerial vehicles;Measurement;Manuals;Surveillance;Software algorithms;Robots,aerospace components;autonomous aerial vehicles;cameras;helicopters;mobile robots;robot vision,autonomous observation;multiple USVs;UAV motion;USV;rotorcraft Unmanned Aerial Vehicle;UAV movements;camera yaw;camera tilt;rotorcraft;situational awareness;camera movements;cooperative unmanned surface vehicles;multiple moving targets;fixed-wing type UAV,,,10,,,,,,IEEE,IEEE Conferences
Big RF Data Assisted Cognitive Radio Network Coexistence in 3.5GHz Band,O. Omotere; L. Qian; R. Jantti; M. Pan; Z. Han,NA; NA; NA; NA; NA,2017 26th International Conference on Computer Communication and Networks (ICCCN),,2017,,,1,8,"In this paper, big Radio Frequency (RF) data assisted optimization is considered for future wireless networks employing cognitive radio technology with machine learning capability. A cognitive radio network (CRN) with multiple Secondary Users (SUs) may coexist with other wireless systems such as Small Cells (SC) and Radar systems, both Primary Users (PUs) with different level of priorities. Traditional spectrum sensing typically only gives information about the presence or absence of a PU. However, when multiple heterogeneous systems coexist, it becomes imperative to acquire the knowledge of the systems operating in a specific band at a particular time so as to choose an appropriate transmission strategy. In this work, we take advantage of the learning capability of a Neural Network Predictor (NNP) to obtain the statistics of the coexisted wireless systems from the RF traces collected in our Universal Software Radio Peripheral (USRP) based test bed. The NNP is able to learn the features of the RF traces and make accurate prediction of the signals prevalent in the wireless environment. Because of the augmented information learned from the RF traces, a novel optimization problem incorporating the outputs from the NNP is formulated to maximize the throughput of the CRN. The solution is derived using Karush- Kuhn-Tucker (KKT) and extensive simulations using the real RF traces are carried out. It is demonstrated that the NNP can detect the type and number of coexisted users reliably and the proposed scheme will improve the performance of the coexisted CRN.",,978-1-5090-2991-4978-1-5090-2990-7978-1-5090-2992,10.1109/ICCCN.2017.8038357,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038357,,Radio frequency;Artificial neural networks;Sensors;Cognitive radio;Interference,cognitive radio;learning (artificial intelligence);optimisation;radio spectrum management;signal detection;software radio,big Radio Frequency data assisted optimization;multiple Secondary Users;Primary Users;multiple heterogeneous systems;Neural Network Predictor;NNP;coexisted wireless systems;wireless environment;machine learning;spectrum sensing;universal software radio peripheral;big RF data assisted cognitive radio network;augmented information;Karush-Kuhn-Tucker simulations;frequency 3.5 GHz,,,14,,,,,,IEEE,IEEE Conferences
CBGA-ES: A Cluster-Based Genetic Algorithm with Elitist Selection for Supporting Multi-Objective Test Optimization,D. Pradhan; S. Wang; S. Ali; T. Yue; M. Liaaen,NA; NA; NA; NA; NA,"2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)",,2017,,,367,378,"Multi-objective search algorithms (e.g., non-dominated sorting genetic algorithm II (NSGA-II)) have been frequently applied to address various testing problems requiring multi-objective optimization such as test case selection. However, existing multi-objective search algorithms have certain randomness when selecting parent solutions for producing offspring solutions. In the worse case, suboptimal parent solutions may result in offspring solutions with bad quality, and thus affect the overall quality of the next generation. To address such a challenge, we propose a cluster-based genetic algorithm with elitist selection (CBGA-ES) with the aim to reduce such randomness for supporting multi-objective test optimization. We empirically compared CBGA-ES with random search, greedy (as baselines) and four commonly used multi-objective search algorithms (e.g., NSGA-II) using two industrial and one real world test optimization problem, i.e., test suite minimization, test case prioritization, and test case selection. The results showed that CBGA-ES significantly outperformed the baseline algorithms (e.g., greedy), and the four selected search algorithms for all the three test optimization problems. CBGA-ES managed to outperform more than 75% of the objectives for all the four algorithms in each test optimization problem. Moreover, CBGA-ES was able to improve the quality of the solutions for an average of 32.5% for each objective as compared to the four algorithms for the three test optimization problems.",,978-1-5090-6031-3978-1-5090-6032,10.1109/ICST.2017.40,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927990,Multi-objective test optimization;Search;Multi-objective genetic algorithm;Cluster,Conferences;Software testing,genetic algorithms;program testing,CBGA-ES;cluster-based genetic algorithm;elitist selection;multiobjective test optimization;multiobjective search algorithms;test case selection,,3,70,,,,,,IEEE,IEEE Conferences
Classification model for test case prioritization techniques,Sujata; G. N. Purohit,"Dept. of Computer Science, SOE GD Goenka University, Gurugram, Haryana, India; Dept. of AIM & ACT, Banasthali Vidyapeeth Banasthali, Rajasthan, India","2017 International Conference on Computing, Communication and Automation (ICCCA)",,2017,,,919,924,"Regression Testing is mainly done in software maintenance aiming to assure that the changes made in the software have correctly been implemented and also to achieve the confidence that the modifications have not affected the other parts of the software. It is very costly and expensive technique. There are number of techniques present in literature that focus on achieving various testing objectives early in the process and hence reduces its cost. Despite of that, testers usually prefer only few already known techniques for test case prioritization. The main reason behind is the absence of guidelines for the selection of TCP techniques. Hence, this piece of research introduces a novel approach for classification of TCP techniques using fuzzy logic to support the efficient selection of test case prioritization techniques. This work is an extension of already proposed selection schema for test case prioritization techniques. To perform the validation of proposed approach results are compared with other classification techniques using Weka tool. The analysis clearly shows the effectiveness of proposed approach as compared to others in terms of its accuracy.",,978-1-5090-6471-7978-1-5090-6472,10.1109/CCAA.2017.8229925,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8229925,regression testing;test case prioritization;classification;fuzzy logic,Testing;Complexity theory;Fuzzy logic;Software;Fault detection;Automation;Optimization,fuzzy logic;pattern classification;program testing;regression analysis;software maintenance,TCP techniques;test case prioritization techniques;classification techniques;Regression Testing;testing objectives;classification model;fuzzy logic;software,,,21,,,,,,IEEE,IEEE Conferences
Cloud-based parallel concolic execution,T. Chen; Y. Feng; X. Luo; X. Lin; X. Zhang,"Center for Cybersecurity, University of Electronic Science and Technology of China, China; Center for Cybersecurity, University of Electronic Science and Technology of China, China; Department of Computing, The Hong Kong Polytechnic University, China; Faculty of Business and Information Technology, University of Ontario Institute of Technology, Canada; Center for Cybersecurity, University of Electronic Science and Technology of China, China","2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)",,2017,,,437,441,"Path explosion is one of the biggest challenges hindering the wide application of concolic execution. Although several parallel approaches have been proposed to accelerate concolic execution, they neither scale well nor properly handle resource fluctuations and node failures, which often happen in practice. In this paper, we propose a novel approach, named PACCI, which parallelizes concolic execution and adapts to the drastic changes of computing resources by leveraging cloud infrastructures. PACCI tailors concolic execution to the MapReduce programming model and takes into account the features of cloud infrastructures. In particular, we tackle several challenging issues, such as making the exploration of different program paths independently and constructing an extensible path exploration module to support the prioritization of test inputs from a global perspective. Preliminary experimental results show that PACCI is scalable (e.g., gaining about 20?? speedup using 24 nodes) and its efficiency declines slightly about 5% and 6.1% under resource fluctuations and node failures, respectively.",,978-1-5090-5501-2978-1-5090-5502,10.1109/SANER.2017.7884649,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884649,,Algorithm design and analysis;Software algorithms;Sparks;Programming;Explosions;Computational modeling;Adaptation models,cloud computing;data handling;programming,cloud-based parallel concolic execution;path explosion;resource fluctuations;node failures;computing resources;cloud infrastructures;PACCI tailors concolic execution;MapReduce programming model;path exploration module,,,12,,,,,,IEEE,IEEE Conferences
Comparison analysis of two test case prioritization approaches with the core idea of adaptive,J. Ding; X. Zhang,"AVIC CHENG DU AIRCRAFT INDUSTRIAL (GROUP) CO., LTD, Chengdu; Department of Automatic Control, Beihang University, Beijing",2017 29th Chinese Control And Decision Conference (CCDC),,2017,,,1723,1730,"Test case prioritization problem (TCP) has been widely discussed. It aims to controlling the test case execution sequence to improve the effectiveness of software testing. The key issue of TCP is to identify which test cases can provide useful information for failure detection and fault localization. So far, many TCP approaches have been proposed. Among them, Adaptive Random Testing (ART) and Dynamic Random Testing (DRT) are two of the most popular approaches to solve TCP with a basic idea borrowed from Cybernetics: adaptive. Both ART and DRT has been widely explored and observed with good performances in experimental studies. Nevertheless, although they are proposed by two related research groups, they are developed independently and in parallel. In fact, their mechanisms have many similarities and differences and, for the completeness of the domains of Adaptive Testing and Software Cybernetics, many issues concerning the comparison between these two approaches should be further explored. In this paper, we specifically explores the relationship between these two adaptive TCP approaches. Their mechanisms are described respectively with explorations of their distinctions, similarities, and respective characteristics. Moreover, based on these explorations, we analyse their advantages from the aspects of failure detection and fault understanding. During the analysis, a symbolic-graphic combination method is applied. Finally simulation based on real-life programs is conducted to observe our analysis. Our comparison analysis can support the selection of a proper testing approach according to various practical environments with different targets. Furthermore, the clarification of the two easily confused concepts is also a complement for the framework of Adaptive Testing and Software Cybernetics.",1948-9447,978-1-5090-4657-7978-1-5090-4656-0978-1-5090-4658,10.1109/CCDC.2017.7978795,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7978795,Software testing;Adaptive testing approaches;Dynamic;Software Cybernetics,Subspace constraints;Software;Software testing;Cybernetics;Debugging;Power capacitors,program testing;software fault tolerance,test case prioritization;TCP problem;test case execution sequence;software testing effectiveness improvement;failure detection;fault localization;adaptive random testing;dynamic random testing;ART;DRT;software cybernetics;fault understanding;symbolic-graphic combination method,,,21,,,,,,IEEE,IEEE Conferences
Cost aware test suite reduction algorithm for regression testing,C. P. Indumathi; S. Madhumathi,"Department of IT, Anna University - BIT Campus, Trichy; M.E - Computer Science and Engineering, Anna University- BIT Campus, Trichy",2017 International Conference on Trends in Electronics and Informatics (ICEI),,2017,,,869,874,"Regression testing is the process that a recent code change has not adversely affect the existing features. The re-running of all the test cases during regression testing is very expensive as it requires huge time and resources. Test case prioritization techniques are to schedule the test cases in accordance with some criteria such that important test cases are executed with that given period. This study presents test case prioritization using genetic algorithm and their effectiveness is measured using APFD. Then the prioritized test cases are reduced. Test suite reduction techniques aim at identifying and eliminating redundant test cases from test suites in order to reduce the total number of test cases to execute, thereby improving the efficiency of the software testing activity. Our aim is to reduce the cost by reducing the number of test suite after prioritization. MFTS algorithm is used to reduce the given test suite with maximum coverage and it improves the rate of fault detection effectiveness.",,978-1-5090-4257-9978-1-5090-4258,10.1109/ICOEI.2017.8300829,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300829,Regression testing;Genetic Algorithm;APFD;MFTS algorithm,Testing;Genetic algorithms;Fault detection;Software;Market research;Informatics;Heuristic algorithms,genetic algorithms;program testing;regression analysis,regression testing;software testing activity;cost aware test suite reduction algorithm;redundant test case elimination identification;APFD;MFTS algorithm;genetic algorithm,,,20,,,,,,IEEE,IEEE Conferences
Cost-Effective Regression Testing Using Bloom Filters in Continuous Integration Development Environments,J. Kwon; I. Ko,NA; NA,2017 24th Asia-Pacific Software Engineering Conference (APSEC),,2017,,,160,168,"Regression testing in continuous integration development environments must be cost-effective and should provide fast feedback on test suite failures to the developers. In order to provide faster feedback on failures to developers while using computing resources efficiently, two types of regression testing techniques have been developed: Regression Testing Selection (RTS) and Test Case Prioritization (TCP). One of the factors that reduces the effectiveness of the RTS and TCP techniques is the inclusion of test suites that fail only once over a period. We propose an approach based on Bloom filtering to exclude such test suites during the RTS process, and to assign such test suites with a lower priority during the TCP process. We experimentally evaluate our approach using a Google dataset, and demonstrate that cost-effectiveness of the proposed RTS and TCP techniques outperforms the state-of-the-art techniques.",,978-1-5386-3681-7978-1-5386-3682,10.1109/APSEC.2017.22,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305938,,Testing;Microsoft Windows;Google;Instruments;History;Fault detection,program testing;regression analysis;statistical testing,cost-effective Regression;Bloom filters;continuous integration development environments;fast feedback;test suite failures;regression testing techniques;RTS;Test Case Prioritization;TCP techniques;test suites;cost-effectiveness,,,28,,,,,,IEEE,IEEE Conferences
Coverage-Based Reduction of Test Execution Time: Lessons from a Very Large Industrial Project,T. Bach; A. Andrzejak; R. Pannemans,NA; NA; NA,"2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",,2017,,,3,12,"There exist several coverage-based approaches to reduce time and resource costs of test execution. While these methods are well-investigated and evaluated for smaller to medium-size projects, we faced several challenges in applying them in the context of a very large industrial software project, namely SAP HANA. These issues include: varying effectiveness of algorithms for test case selection/prioritization, large amounts of shared (non-specific) coverage between different tests, high redundancy of coverage data, and randomness of test results (i.e. flaky tests), as well as of the coverage data (e.g. due to concurrency issues). We address these issues by several approaches. First, our study shows that compared to standard algorithms, so-called overlap-aware solvers can achieve up to 50% higher code coverage in a fixed time budget, significantly increasing the effectiveness of test case prioritization and selection. We also detected in our project high redundancy of line coverage data (up to 97%), providing opportunities for data size reduction. Finally, we show that removal of coverage shared by tests can significantly increase test specificity. Our analysis and approaches can help to narrow the gap between research and practice in context of coverage-based testing approaches, especially in case of very large software projects.",,978-1-5090-6676-6978-1-5090-6677,10.1109/ICSTW.2017.6,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899023,coverage;test case selection;test case prioritization;test specificty;large real world project,Testing;Software;Redundancy;Runtime;Context;Optimization;Heuristic algorithms,data reduction;program testing;project management;redundancy;software cost estimation;software development management,coverage-based testing;data size reduction;line coverage data;fixed time budget;overlap-aware solvers;coverage data redundancy;test case prioritization;test case selection;SAP HANA;very large industrial software project;resource cost reduction;time cost reduction;test execution time reduction;coverage-based reduction,,3,21,,,,,,IEEE,IEEE Conferences
d(mu)Reg: A Path-Aware Mutation Analysis Guided Approach to Regression Testing,C. Sun; C. Fan; Z. Wang; H. Liu,NA; NA; NA; NA,2017 IEEE/ACM 12th International Workshop on Automation of Software Testing (AST),,2017,,,59,64,"Regression testing re-runs some previously executed test cases, with the purpose of checking whether previously fixed faults have re-emerged and ensuring that the changes do not negatively affect the existing behaviors of the software under development. Today's software is rapidly developed and evolved, and thus it is critical to implement regression testing quickly and effectively. In this paper, we propose a novel technique for regression testing, based on a family of mutant selection strategies. The preliminary results show that the proposed technique can significantly improve the efficiency of different regression testing activities, including test case reduction and prioritization. Our work also makes it possible to develop a unified framework that effectively implements various activities in regression testing.",,978-1-5386-1548-5978-1-5386-1549,10.1109/AST.2017.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962333,regression testing;test case reduction;test case prioritization;mutation analysis;path depth,Software testing;Schedules;History;Sun;Software systems,program testing;software engineering,d?¬Reg;path-aware mutation analysis guided approach;software development;regression testing;mutant selection strategies;test case reduction;test case prioritization,,,20,,,,,,IEEE,IEEE Conferences
Delta-Oriented Product Prioritization for Similarity-Based Product-Line Testing,M. Al-Hajjaji; S. Lity; R. Lachmann; T. Th?¬m; I. Schaefer; G. Saake,NA; NA; NA; NA; NA; NA,2017 IEEE/ACM 2nd International Workshop on Variability and Complexity in Software Design (VACE),,2017,,,34,40,"Testing every product of a software product line (SPL) is often not feasible due to the exponential number of products in the number of features. Thus, the order in which products are tested matters, because it can increase the early rate of fault detection. Several approaches have been proposed to prioritize products based on configuration similarity. However, current approaches are oblivious to solution-space differences among products, because they consider only problem-space information. With delta modeling, we incorporate solution-space information in product prioritization to improve the effectiveness of SPL testing. Deltas capture the differences between products facilitating the reasoning about product similarity. As a result, we select the most dissimilar product to the previously tested ones, in terms of deltas, to be tested next. We evaluate the effectiveness of our approach using an SPL from the automotive domain showing an improvement in the effectiveness of SPL testing.",,978-1-5386-2803-4978-1-5386-2804,10.1109/VACE.2017.8,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968061,Software product line;Model-based testing;Product prioritization;Delta-oriented,Testing;Computer architecture;Fault detection;Software systems;Computational modeling,program testing;software product lines,automotive domain;SPL testing;solution-space information;delta modeling;configuration similarity;fault detection rate;similarity-based software product-line testing;delta-oriented product prioritization,,,28,,,,,,IEEE,IEEE Conferences
Deriving high-priority acceptance test cases using utility trees: A case study,P. Cruz; H. Astudillo,"Departamento de Inform?­tica, Universidad T??cnica Federico Santa Mar??a, Avenida Espa?ña 1680, Valpara??so, Chile; Departamento de Inform?­tica, Universidad T??cnica Federico Santa Mar??a, Avenida Espa?ña 1680, Valpara??so, Chile",2017 36th International Conference of the Chilean Computer Science Society (SCCC),,2017,,,1,8,"Even though software testing is considered a mature field in software engineering, deriving test cases is still an important issue and even more when related to quality requirements. Utility Trees are used to evaluate software architectures, organizing requirements as scenarios associated to quality attributes and decorating them with stakeholder-given priority and developer- given difficulty. In this article, we propose an approach to use Utility Trees to derive prioritized acceptance test cases allowing to focus in high-value tests. The technique has been tried in two medium-sized projects for a Chilean public agency, with positive results. This innovative use of Utility Trees offers a simple, collaborative way to focus testing resources.",,978-1-5386-3483-7978-1-5386-3484,10.1109/SCCC.2017.8405142,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8405142,,Software;Proposals;Software testing;Password;Software engineering;Computer architecture,program testing;software architecture;software development management;software quality;trees (mathematics),high-priority acceptance test cases;utility trees;case study;software testing;mature field;software engineering;quality requirements;software architectures;quality attributes;stakeholder-given priority;developer- given difficulty;prioritized acceptance test cases;high-value tests;Chilean public agency;testing resources,,,27,,,,,,IEEE,IEEE Conferences
Design and Implementation of Combinatorial Testing Tools,Y. Yao; Y. Yan; Z. Wang; C. Liu,NA; NA; NA; NA,"2017 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)",,2017,,,320,325,"As an effective software testing technique, combinatorial testing has been gradually applied in various types of test practice. In this case, it is necessary to provide useful combinatorial testing tools to support the application of combinatorial testing technique on industrial scenarios, as well as the academic research for combinatorial testing technique. To this end, on the basis of the research results of this group, a suite of combinatorial testing tools has been developed, whose functions include test case generation, test case optimization, and etc. For the requirements from both industrial and academic scenarios, the tools should be configurable, scalable, modular, and etc. This paper gives a brief introduction to the design and implementation of these tools. Keywords-combinatorial testing, combinatorial testing tools, test generation, test prioritization.",,978-1-5386-2072-4978-1-5386-2073,10.1109/QRS-C.2017.61,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8004338,combinatorial testing;combinatorial testing tools;test generation;test prioritization,Testing;Tools;Optimization;Algorithm design and analysis;Standards;Random access memory;Scalability,program testing,combinatorial testing tool design;software testing technique;test practice;industrial scenarios;combinatorial testing technique;academic scenarios,,,14,,,,,,IEEE,IEEE Conferences
Digital learning as a tool to overcome school failure in minority groups,D. Pal'ov?­; N. M. Novak; V. Weidinger,"Department of Applied Mathematics and Business Informatics, Faculty of Economics, Technical University of Ko­ice, Slovakia; Institute of Software Technology and Interactive Systems, Vienna University of Technology, Austria; Verein Offenes Lernen, Vienna, Austria","2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",,2017,,,767,772,"In the European Union development strategy formulated in the Europe 2020 document (European Commission, 2015) it was indicated, that the smart growth of the EU as a whole should be reached through the realization of three priorities: the increase in employment, the increase of productiveness and the social cohesion and specialized agendas: Digital Agenda, Education and Learning, E-skills and Employment. The main documents identify main weaknesses and risk areas. One of the most significant was described as the early school leaving of Roma minority members. Roma constitute Europe's largest transnational ethnic minority with an estimate of ten million people. Learning outcomes of this minority are significantly lower than outcomes of the majority. As one of the reasons for early school leaving of Roma, insufficient understanding of learning materials is identified. The result is that most of the Roma community members drop out of education before attending a secondary school and continue their lives as unemployed or enter the labor market as unskilled workers. Within the paper will be presented the CloudLearning project that represents an alternative and innovative educational method: the way of the SOLE method implemented in their education. This paper will include partial results from the pilot tests realized these days.",,978-953-233-090-8978-953-233-092-2978-1-5090-4969,10.23919/MIPRO.2017.7973525,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7973525,,Education;Computers;Europe;Employment;Cloud computing;Economics;Tools,cloud computing;computer aided instruction;employment,digital learning;school failure;European Union development strategy;Europe 2020 document;Digital Agenda;e-skills;employment;Roma minority members;transnational ethnic minority;CloudLearning project;SOLE method,,,23,,,,,,IEEE,IEEE Conferences
Educational prototype demonstrating frequency spectrum sharing through channel borrowing and priority assignment,B. Keneni; B. Austin; C. Elkin; V. Devabhaktuni,"EECS Department, The University of Toledo, Toledo, OH, USA; EECS Department, The University of Toledo, Toledo, OH, USA; EECS Department, The University of Toledo, Toledo, OH, USA; EECS Department, The University of Toledo, Toledo, OH, USA",2017 IEEE International Conference on Electro Information Technology (EIT),,2017,,,540,544,"The purpose of this research is to build an educational prototype for attracting high school seniors and college students to pursue university degrees. The prototype entails demonstrable hardware and software comprising of a set of communication nodes with call priorities, which are used to help educate students on future and practical implications of spectrum sharing. Two objectives are achieved by building this hands-on prototype: (1) Students learn firsthand basics of communication systems and (2) Students are taught the concept and feasibility of ƒ??priorityƒ? in RF device communication. In crafting this easy-to-use prototype, integrated Arduinos, RF modules, and open-source libraries are utilized. Eight simplex devices (or nodes) that can communicate over just three channels in the 2.4GHz ISM band are built to articulate the spectrum scarcity challenge. When placing a call, a device requests a channel from the base station, which notifies the device about available channels, taking into account both device priorities (e.g. some devices have higher priorities) and channels that are in use by other calls. The base station has the authority to remove devices from a channel or transfer them to another channel when necessary. The paradigm of ƒ??priority assignmentƒ? is implemented keeping in mind futuristic trends in communication systems aimed at optimal use of the transmission spectrum. The prototype is validated via many test cases.",2154-0373,978-1-5090-4767-3978-1-5090-4766-6978-1-5090-4768,10.1109/EIT.2017.8053422,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053422,,Prototypes;Channel allocation;Computer architecture;Mobile handsets;Microprocessors;Base stations;Radio frequency,educational institutions;radio networks;radio spectrum management,RF device communication;easy-to-use prototype;RF modules;open-source libraries;simplex devices;spectrum scarcity challenge;device requests;base station;available channels;priority assignment;communication systems;transmission spectrum;channel borrowing;high school seniors;college students;university degrees;demonstrable hardware;communication nodes;call priorities;spectrum sharing;firsthand basics;ISM band;educational prototype;frequency spectrum;demonstrable software;frequency 2.4 GHz,,,5,,,,,,IEEE,IEEE Conferences
Efficient Product-Line Testing Using Cluster-Based Product Prioritization,M. Al-Hajjaji; J. Kr?¬ger; S. Schulze; T. Leich; G. Saake,NA; NA; NA; NA; NA,2017 IEEE/ACM 12th International Workshop on Automation of Software Testing (AST),,2017,,,16,22,"A software product-line comprises a set of products that share a common set of features. These features can be reused to customize a product to satisfy specific needs of certain customers or markets. As the number of possible products increases exponentially for new features, testing all products is infeasible. Existing testing approaches reduce their effort by restricting the number of products (sampling) and improve their effectiveness by considering the order of tests (prioritization). In this paper, we propose a cluster-based prioritization technique to sample similar products with respect to the feature selection. We evaluate our approach using feature models of different sizes and show that cluster-based prioritization can enhance the effectiveness of product-line testing.",,978-1-5386-1548-5978-1-5386-1549,10.1109/AST.2017.7,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7962326,Software product line testing;Highly configurable system;Product prioritization;Cluster-based prioritization,Testing;Clustering algorithms;Media;Global Positioning System;Digital audio players;Linux;Kernel,feature selection;program testing;software product lines,cluster-based product prioritization;software product-line testing;product customize;similar-product sampling;feature selection;SPLE approach;software product-line engineering,,,28,,,,,,IEEE,IEEE Conferences
Enabling Software Defined Networking with QoS Guarantee for Cloud Applications,F. Li; J. Cao; X. Wang; Y. Sun; Y. Sahni,NA; NA; NA; NA; NA,2017 IEEE 10th International Conference on Cloud Computing (CLOUD),,2017,,,130,137,"Due to the centralized control, network-wide monitoring and flow-level scheduling of Software-Defined-Networking (SDN), it can be utilized to achieve Quality of Service (QoS) for cloud applications and services, such as voice over IP, video conference and online games, etc. However, most existing approaches stay at the QoS framework design and test level, while few works focus on studying the basic QoS techniques supported by SDN. In this paper, we enable SDN with QoS guaranteed abilities, which could provide end-to-end QoS routing for each cloud user service. First of all, we implement an application identification technique on SDN controller to determine required QoS levels for each application type. Then, we implement a queue scheduling technique on SDN switch. It queues the application flows into different queues and schedules the flows out of the queues with different priorities. At last, we evaluate the effectiveness of the proposed SDN-based QoS technique through an experimental analysis. Results show that when the output interface has sufficiently available bandwidth, the delay can be reduced by 28% on average. In addition, for the application flow with the highest priority, our methods can reduce 99.99% delay and increase 90.17% throughput on average when the output interface utilization approaches to the maximum bandwidth limitation.",2159-6190,978-1-5386-1993-3978-1-5386-1992-6978-1-5386-1994,10.1109/CLOUD.2017.25,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030581,,Quality of service;Switches;Cloud computing;Processor scheduling;Routing;Ports (Computers),cloud computing;quality of service;software defined networking;telecommunication scheduling,application identification technique;SDN controller;QoS levels;queue scheduling technique;SDN switch;QoS technique;application flow;QoS guarantee;cloud applications;centralized control;network-wide monitoring;video conference;online games;basic QoS techniques;cloud user service;software defined networking;Quality of Service;end-to-end QoS routing,,2,36,,,,,,IEEE,IEEE Conferences
Epistasis Based ACO for Regression Test Case Prioritization,Y. Bian; Z. Li; R. Zhao; D. Gong,"College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, China; School of Information Science and Technology, Qingdao University of Science and Technology, Qingdao, China",IEEE Transactions on Emerging Topics in Computational Intelligence,,2017,1,3,213,223,"Metaheuristics that are inspired by natural systems have been widely applied into search-based software engineering. It has been shown that combining knowledge of the application domain with a biological theory for metaheuristics can narrow down the search space and speed up the convergence for metaheuristics based algorithms. This paper introduces Epistatic Test case Segment (ETS) for multiobjective search-based regression Test Case Prioritization (MoTCP), based on epistasis theory that reflects the correlation between genes in evolution process. An ETS-based pheromone update strategy for ant colony optimization (ACO) algorithm is proposed. The experiments with three benchmarks and a real industrial program V8 illustrate that proposed pheromone update strategy guided by the epistasis theory can significant improve the performance of ACO in terms of effectiveness and efficiency for search-based MoTCP.",2471-285X,,10.1109/TETCI.2017.2699228,National Natural Science Foundation of China; Program for New Century Excellent Talents in University; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7935487,Ant colony optimization;epistasis;multi-objective optimization;test case prioritization,Silicon;Optimization;Software algorithms;Testing;Search problems;Genetic algorithms;Software engineering,ant colony optimisation;program testing;regression analysis,epistasis based ACO;regression test case prioritization;search-based software engineering;biological theory;metaheuristics;epistatic test case segment;ETS;MoTCP;epistasis theory;ETS-based pheromone update strategy;ant colony optimization;ACO algorithm;search-based MoTCP,,1,45,,,,,,IEEE,IEEE Journals & Magazines
Evaluation of AV systems against modern malware,A. Zarghoon; I. Awan; J. P. Disso; R. Dennis,"Department of informatics, University of Bradford, Bradford, BD7 1DP; Department of informatics, University of Bradford, Bradford, BD7 1DP; Research and Innovation, Nettitude Ltd, Leamington spa, CV31 3RZ; Research and Innovation, Nettitude Ltd, Leamington spa, CV31 3RZ",2017 12th International Conference for Internet Technology and Secured Transactions (ICITST),,2017,,,269,273,"Countering the proliferation of malware has been for recent years one of the top priorities for governments, businesses, critical infrastructure, and end users. Despite the apparent evolvement of anti-virus (AV) systems, malicious authors have managed to create a sense of insecurity amongst computer users. Security controls do not appear to be sufficiently strong to stop malware proliferating. There seems to be a disconnect between public reports on AV tests and what people are experiencing on the daily basis. In this research, we are testing the efficiency of AV products and their ability to detect malicious files commonly known as malware. We manually generated payloads from five malware frameworks freely available to download and use. We use two modes of tests during our experiments. We manually installed a selection of AV systems in one first instance. We also use an online framework for testing malicious files. The findings in this study show that many antivirus systems were not able to achieve a higher score than 80% detection rate. Certain attack frameworks were much more successful in generating payloads that were not detectable by AV systems. We conclude that AV systems have their roles to play as they are the most common first line of defense, but more work is needed to successfully detect most malware the first day of their release.",,978-1-908320-93-3978-1-5386-0598,10.23919/ICITST.2017.8356397,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356397,component;Malware;AV bypass;Antivirus Systems;Detection Techniques;Payloads;Antivirus Evaluation,Malware;Payloads;Tools;Security;Microsoft Windows;Testing;Internet,computer viruses;invasive software;security of data,AV products;malicious files;malware frameworks;AV systems;antivirus systems;anti-virus systems;malware proliferating;AV tests,,,15,,,,,,IEEE,IEEE Conferences
Exniffer: Learning to Prioritize Crashes by Assessing the Exploitability from Memory Dump,S. Tripathi; G. Grieco; S. Rawat,NA; NA; NA,2017 24th Asia-Pacific Software Engineering Conference (APSEC),,2017,,,239,248,"An important component of software reliability is the assurance of certain security guarantees, such as absence of low-level bugs that may result in code exploitation, for example. A program crash is an early indicator of possible errors in the program like memory corruption, access violation or division by zero. In particular, a crash may indicate the presence of safety or security critical errors. A safety-error crash does not result in any exploitable condition, whereas a security-error crash allows an attacker to exploit a vulnerability. However, distinguishing one from the other is a non-trivial task. This exacerbates the problem in cases where we get hundreds of crashes and programmers have to make choices which crash to patch first! In this work, we present a technique to identify security critical crashes by applying machine learning on a set of features derived from core-dump files and runtime information obtained from hardware assisted monitoring such as the last branch record (LBR) register. We implement the proposed technique in a prototype called Exniffer. Our empirical results, obtained by experimenting Exniffer on several crashes on real-world applications show that proposed technique is able to classify a given crash as exploitable or not-exploitable with high accuracy.",,978-1-5386-3681-7978-1-5386-3682,10.1109/APSEC.2017.30,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305946,crash analysis;core-dump;hardware branch tracing;machine learning;information security;operating systems security;software security engineering;vulnerability management,Feature extraction;Computer bugs;Registers;Runtime;Security;Hardware,learning (artificial intelligence);program debugging;program diagnostics;program testing;security of data;software reliability,safety-error crash;exploitable condition;security-error crash;security critical crashes;Exniffer;low-level bugs;code exploitation;program crash;machine learning,,,33,,,,,,IEEE,IEEE Conferences
Extravehicular activity operations concepts under communication latency and bandwidth constraints,K. H. Beaton; S. P. Chappell; A. F. J. Abercromby; M. J. Miller; S. K. Nawotniak; S. S. Hughes; A. Brady; D. S. S. Lim,"KBRwyle, KBRwyle/HAC/37C, 2400 NASA Parkway, Houston, TX 77058; KBRwyle, KBRwyle/HAC/37C, 2400 NASA Parkway, Houston, TX 77058; NASA Johnson Space Center, 2101 NASA Parkway, Houston, TX 77058; Georgia Institute of Technology, 270 Ferst Drive, Room 416 Atlanta, GA 30332; Idaho State University, Department of Geosciences, 921 S. 8 Ave, Mail Stop 8072, Pocatello, ID 83209; Idaho State University, Department of Geosciences, 921 S. 8 Ave, Mail Stop 8072, Pocatello, ID 83209; McMaster University, School of Geography & Earth Sciences, General Science Building Room, 206 Hamilton, Ontario L8S 4K1 Canada; NASA Ames Research Center, Mail Stop 245-3, Moffett Field, CA 94035",2017 IEEE Aerospace Conference,,2017,,,1,20,"The Biologic Analog Science Associated with Lava Terrains (BASALT) project is a multi-year program dedicated to iteratively develop, implement, and evaluate concepts of operations (ConOps) and supporting capabilities intended to enable and enhance human scientific exploration of Mars. This paper describes the planning, execution, and initial results from the first field deployment, referred to as BASALT-1, which consisted of a series of ten simulated extravehicular activities on volcanic flows in Idaho's Craters of the Moon National Monument and Preserve. The ConOps and capabilities deployed and tested during BASALT-1 were based on previous NASA trade studies and analog testing. Our primary research question was whether those ConOps and capabilities work acceptably when performing real (non-simulated) biological and geological scientific exploration under four different Mars-to-Earth communication conditions: 5 and 15 min one-way light time communication latencies and low (0.512 Mb/s uplink, 1.54 Mb/s downlink) and high (5.0 Mb/s uplink, 10.0 Mb/s downlink) bandwidth conditions, which represent two alternative technical communication capabilities currently proposed for future human exploration missions. The synthesized results, based on objective and subjective measures, from BASALT-1 established preliminary findings that the baseline ConOp, software systems, and communication protocols were scientifically and operationally acceptable with minor improvements desired by the ƒ??Marsƒ? extravehicular and intravehicular crewmembers. However, unacceptable components of the ConOps and required improvements were identified by the ƒ??Earthƒ? Mission Support Center. These data provide a basis for guiding and prioritizing capability development for future BASALT deployments and, ultimately, future human exploration missions.",,978-1-5090-1613-6978-1-5090-1614,10.1109/AERO.2017.7943570,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7943570,,Earth;NASA;Geology;Bandwidth;Tools;Lead;Biology,Mars;protocols;space communication links,extravehicular activity operation concepts;bandwidth constraints;Biologic Analog Science Associated with Lava Terrains project;BASALT-1 project;concepts of operation evaluation;ConOps;human scientific exploration enhancement;Mars;Idaho craters;Moon national monument;analog testing;geological scientific exploration;Mars-to-Earth communication conditions;one-way light time communication latencies;software systems;communication protocols;Earth Mission Support Center;NASA trade studies,,2,29,,,,,,IEEE,IEEE Conferences
Flow Reconnaissance via Timing Attacks on SDN Switches,S. Liu; M. K. Reiter; V. Sekar,NA; NA; NA,2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS),,2017,,,196,206,"When encountering a packet for which it has no matching forwarding rule, a software-defined networking (SDN) switch requests an appropriate rule from its controller; this request delays the routing of the flow until the controller responds. We show that this delay gives rise to a timing side channel in which an attacker can test for the recent occurrence of a target flow by judiciously probing the switch with forged flows and using the delays they encounter to discern whether covering rules were previously installed in the switch. We develop a Markov model of an SDN switch to permit the attacker to select the best probe (or probes) to infer whether a target flow has recently occurred. Our model captures practical challenges related to rule evictions to make room for other rules; rule timeouts due to inactivity; the presence of multiple rules that apply to overlapping sets of flows; and rule priorities. We show that our model enables detection of target flows with considerable accuracy in many cases.",1063-6927,978-1-5386-1792-2978-1-5386-1793,10.1109/ICDCS.2017.281,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7979967,,Probes;Delays;Control systems;Reconnaissance;Markov processes,computer network security;Markov processes;software defined networking,flow reconnaissance;timing attacks;SDN switches;software-defined networking;timing side channel;target flow;Markov model;matching forwarding rule,,1,23,,,,,,IEEE,IEEE Conferences
Formal Methods for Validation and Test Point Prioritization in Railway Signaling Logic,S. Ghosh; A. Das; N. Basak; P. Dasgupta; A. Katiyar,"Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, India; Research Design and Standards Organization, Indian Railways, Lucknow, India",IEEE Transactions on Intelligent Transportation Systems,,2017,18,3,678,689,"The EN50128 Railway Safety Standard recommends the use of formal methods for proving the correctness of the yard-specific logic, which was developed for electronic signaling and interlocking systems. We present a tool flow, which consists of three components. The core component uses a novel method for automatically generating the relevant safety properties for a yard from its control table. The second component proves the validity of the properties on the application logic by using a new theory of invariant checking. The third component leverages the suite of formal properties to prioritize site acceptance test points. Experimental results are presented on real application data for the yards in India that are demonstrating the performance of the proposed methods.",1524-9050;1558-0016,,10.1109/TITS.2016.2586512,"Indian Railways; Center for Railways Research, Indian Institute of Technology Kharagpur; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529152,Railway safety;formal verification,Safety;Rail transportation;Relays;Communication system signaling;Companies;Layout;Software,formal verification;rail traffic control;railway safety;signalling,test point prioritization;railway signaling logic;EN50128 Railway Safety Standard;correctness proving;yard-specific logic;electronic signaling system;interlocking system;tool flow;safety property;control table;application logic;theory of invariant checking;site acceptance test point,,1,18,,,,,,IEEE,IEEE Journals & Magazines
Game Theoretic Study on Channel-Based Authentication in MIMO Systems,L. Xiao; T. Chen; G. Han; W. Zhuang; L. Sun,"Department of Communication Engineering, Xiamen University, Xiamen, China; Department of Communication Engineering, Xiamen University, Xiamen, China; Department of Communication Engineering, Xiamen University, Xiamen, China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; Beijing Key Laboratory of IOT Information Security Technology, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China",IEEE Transactions on Vehicular Technology,,2017,66,8,7474,7484,"In this paper, we investigate the authentication based on radio channel information in multiple-input multiple-output (MIMO) systems and formulate the interactions between a receiver with multiple antennas and a spoofing node as a zero-sum physical (PHY)-layer authentication game. In this game, the receiver chooses the test threshold of the hypothesis test to maximize its Bayesian risk-based utility in the spoofing detection, while the adversary chooses its attack rate, i.e., how often a spoofing signal is sent. We derive the Nash equilibrium (NE) of the static PHY-layer authentication game and present the condition that the NE exists, showing that both the spoofing detection error rates and the spoofing rate decrease with the number of transmit and receive antennas. We propose a PHY-layer spoofing detection algorithm for MIMO systems based on Q-learning, in which the receiver applies the reinforcement learning technique to achieve the optimal test threshold via trials in a dynamic game without knowing the system parameters, such as the channel time variation and spoofing cost. We also use Dyna architecture and prioritized sweeping (Dyna-PS) to improve the spoofing detection in time-variant radio environments. The proposed authentication algorithms are implemented over universal software radio peripherals and evaluated via experiments in an indoor environment. Experimental results show that the Dyna-PS-based spoofing detection algorithm further reduces the spoofing detection error rates and increases the utility of the receiver compared with the Q-learning-based algorithm, and both performances improve with more number of transmit or receive antennas.",0018-9545;1939-9359,,10.1109/TVT.2017.2652484,National Natural Science Foundation of China; 863 High Technology Plan; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815442,Game theory;MIMO;PHY-layer authentication;reinforcement learning;spoofing detection,Games;Authentication;MIMO;Receiving antennas;Game theory;Channel estimation,Bayes methods;cryptography;game theory;indoor radio;learning (artificial intelligence);MIMO communication;radio receivers;receiving antennas;transmitting antennas;wireless channels,game theory;radio channel information authentication;MIMO system;multiple-input multiple-output system;multiple antennas;receiver;spoofing node;zero-sum PHY-layer authentication game;zero-sum physical-layer authentication game;Bayesian risk-based utility maximization;spoofing detection;Nash equilibrium;PHY-layer spoofing detection algorithm;Q-learning;reinforcement learning technique;time-variant radio environment;indoor environment;transmit antennas;receive antennas,,7,20,,,,,,IEEE,IEEE Journals & Magazines
Gamifying Collaborative Prioritization: Does Pointsification Work?,F. M. Kifetew; D. Munante; A. Perini; A. Susi; A. Siena; P. Busetta; D. Valerio,NA; NA; NA; NA; NA; NA; NA,2017 IEEE 25th International Requirements Engineering Conference (RE),,2017,,,322,331,"Gamification has been applied in software engineering contexts, and more recently in requirements engineering with the purpose of improving the motivation and engagement of people performing specific engineering tasks. But often an objective evaluation that the resulting gamified tasks successfully meet the intended goal is missing. On the other hand, current practices in designing gamified processes seem to rest on a try, test and learn approach, rather than on first principles design methods. Thus empirical evaluation should play an even more important role.We combined gamification and automated reasoning techniques to support collaborative requirements prioritization in software evolution. A first prototype has been evaluated in the context of three industrial use cases. To further investigate the impact of specific game elements, namely point-based elements, we performed a quasi-experiment comparing two versions of the tool, with and without pointsification. We present the results from these two empirical evaluations, and discuss lessons learned.",2332-6441,978-1-5386-3191-1978-1-5386-3192,10.1109/RE.2017.66,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8049138,gamification;collaborative requirements prioritization;quasi-experiment,Games;Collaboration;Software;Cognition;Decision making;Stakeholders,computer games;formal specification;groupware;human factors;software maintenance,collaborative prioritization;software engineering;automated reasoning;pointsification;people engagement;people motivation;software evolution;collaborative requirements prioritization;requirements engineering;gamification,,2,22,,,,,,IEEE,IEEE Conferences
Impact of Static and Dynamic Coverage on Test-Case Prioritization: An Empirical Study,J. Zhou; D. Hao,NA; NA,"2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",,2017,,,392,394,"Most of existing research in Test-Case Prioritization uses coverage information as the input during the process of prioritization and these coverage can be classified into two categories: static coverage and dynamic coverage. As these coverage information are collected in different ways, they have different influence on test-case prioritization. In this work, we present the first empirical study comparing the impact of static coverage and dynamic coverage with five typical techniques at different test-case granularities (e.g., test-method and test-class level) and different coverage criteria (e.g., method and statement coverage). This study is performed on 15 real-world Java projects (using 163 versions) and we find that the dynamic coverage performs better than static coverage in terms of the results of test-case prioritization.",,978-1-5090-6676-6978-1-5090-6677,10.1109/ICSTW.2017.74,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899091,Test Case Prioritization;Empirical Study;Coverage Information;Regression Testing,Software;Software engineering;Conferences;Software testing;Java;Performance analysis,Java;program diagnostics;program testing,static coverage;dynamic coverage;test-case prioritization;coverage information;test-case granularities;coverage criteria;Java Projects,,,17,,,,,,IEEE,IEEE Conferences
Improving test case prioritization based on practical priority factors,M. H. Mahmood; M. S. Hosain,"Department of Electrical and Computer Engineering, North South University, Dhaka-1229, Bangladesh; Department of Electrical and Computer Engineering, North South University, Dhaka-1229, Bangladesh",2017 8th IEEE International Conference on Software Engineering and Service Science (ICSESS),,2017,,,899,902,"Test case prioritization involves prioritized the test cases for regression testing which improve the effectiveness of the testing process. By improving test case scheduling we can optimize time and cost as well as can produce better tested products. There are a number of methods to do prioritized test cases but not that effective or practical for the real-life large commercial systems. Most of the technique deals with finding defects or covering more test cases. In this paper, we will extend the previous work to incorporate real life practical aspects to schedule test cases. This will cover most of the businesses functionally based on the practical aspects. This approach covers more business area and ensure more defects. By prioritized test cases with this technique we will cover most important business functionally with less number of test cases.",2327-0594,978-1-5386-0497-7978-1-5386-0496-0978-1-5386-0498,10.1109/ICSESS.2017.8343055,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8343055,Test case;Test case prioritization;Test case prioritization methods;Regression Testing,Complexity theory;Testing;Computer bugs;Business;Fault detection;Software systems,program testing;regression analysis;scheduling,test case prioritization;regression testing;testing process;test case scheduling;practical priority factors,,,9,,,,,,IEEE,IEEE Conferences
Improving the Cooperation of Fuzzing and Symbolic Execution by Test-cases Prioritizing,J. Ye; B. Zhang; Z. Ye; C. Feng; C. Tang,NA; NA; NA; NA; NA,2017 13th International Conference on Computational Intelligence and Security (CIS),,2017,,,543,547,"Nowadays much attention is paid to the threat of vulnerabilities on the software security. Fuzzing and symbolic execution, complementary to each other, are two effective techniques in software testing. In this paper, we develop the prototype called FAS(Fuzzing and Symbolic) for software testing under both fuzzing and symbolic execution. In our method, the test cases are prioritized through deep-oriented strategy and large-distance-first strategy, in order to get a higher path-coverage with the condition of limited resource.",,978-1-5386-4822-3978-1-5386-4823,10.1109/CIS.2017.00126,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288548,Fuzzing;Symbolic execution;Strategy;Effectiveness,Fuzzing;Engines;Tools;Software;Computer crashes,program testing,software testing;symbolic execution;Fuzzing and Symbolic execution;FAS execution;software security;test cases,,,15,,,,,,IEEE,IEEE Conferences
Knowledge Transfer for Global Roles in GSE,R. K. Gupta; T. Anand,NA; NA,2017 IEEE 12th International Conference on Global Software Engineering (ICGSE),,2017,,,81,85,"This practice paper presents how a software engineering organization spread across three countries successfully transferred the knowledge of a few identified roles for a large mission-critical software system that had to conform to regulatory requirements. Multiple releases of the system have been delivered to customers over the 15 years it has been in the market. Each release of the product had a focus area. The competence availability for these focus areas was distributed. As a natural evolution of the globally distributed team, greater responsibility is devolved to a particular location, based on the availability of the competence at that location. Moving the increased responsibility to a location, created a global role, which did not exist earlier. Building the new role required a new skill, what is unique about a global role. Equipping the team members in the new skill was necessary to take up the roles effectively and quickly. The first step was the identification of the competence for a function/role, training for which may be imparted to another person, who will take over the function/role. This is followed by a process of knowledge transfer, which ensured that a person can take up a new global role from another location. Prioritization based on ease of knowledge transfer for different areas of work, that was found to be effective is described. This helped reduce possible problems that could occur due to incorrect or incomplete transfer of knowledge. The advantages by such knowledge transfer that resulted in new persons taking up global roles have outweighed its disadvantages. The practices described are generic and can be applied to any organization of similar size and complexity.",,978-1-5386-1587-4978-1-5386-1588,10.1109/ICGSE.2017.2,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976692,global role;knowledge transfer;product management;test management;test strategy;stake holder involvement;agile;global software engineering;regulatory requirements,Knowledge transfer;Training;Tools;Organizations;Configuration management;Software engineering;Collaboration,software engineering,GSE;software engineering organization;critical software system;globally distributed team;knowledge transfer process,,1,6,,,,,,IEEE,IEEE Conferences
Learning to Predict Severity of Software Vulnerability Using Only Vulnerability Description,Z. Han; X. Li; Z. Xing; H. Liu; Z. Feng,NA; NA; NA; NA; NA,2017 IEEE International Conference on Software Maintenance and Evolution (ICSME),,2017,,,125,136,"Software vulnerabilities pose significant security risks to the host computing system. Faced with continuous disclosure of software vulnerabilities, system administrators must prioritize their efforts, triaging the most critical vulnerabilities to address first. Many vulnerability scoring systems have been proposed, but they all require expert knowledge to determine intricate vulnerability metrics. In this paper, we propose a deep learning approach to predict multi-class severity level of software vulnerability using only vulnerability description. Compared with intricate vulnerability metrics, vulnerability description is the ""surface level"" information about how a vulnerability works. To exploit vulnerability description for predicting vulnerability severity, discriminative features of vulnerability description have to be defined. This is a challenging task due to the diversity of software vulnerabilities and the richness of vulnerability descriptions. Instead of relying on manual feature engineering, our approach uses word embeddings and a one-layer shallow Convolutional Neural Network (CNN) to automatically capture discriminative word and sentence features of vulnerability descriptions for predicting vulnerability severity. We exploit large amounts of vulnerability data from the Common Vulnerabilities and Exposures (CVE) database to train and test our approach.",,978-1-5386-0992-7978-1-5386-0993,10.1109/ICSME.2017.52,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094415,vulnerability severity prediction;multi-class classification;deep learning;mining software repositories,Software;Security;Databases;Feature extraction;Measurement;Vocabulary,learning (artificial intelligence);neural nets;risk management;security of data,vulnerability description;intricate vulnerability metrics;software vulnerability severity prediction;deep learning approach;security risks;convolutional neural network;CNN,,1,57,,,,,,IEEE,IEEE Conferences
Learning to Prioritize Test Programs for Compiler Testing,J. Chen; Y. Bai; D. Hao; Y. Xiong; H. Zhang; B. Xie,NA; NA; NA; NA; NA; NA,2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE),,2017,,,700,711,"Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases.",1558-1225,978-1-5386-3868-2978-1-5386-3869,10.1109/ICSE.2017.70,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985706,,Computer bugs;Testing;Program processors;Life estimation;Feature extraction;Training;Predictive models,learning (artificial intelligence);program compilers;program testing;scheduling;software reliability,compiler reliability;software systems;automated compiler testing;test-generation tools;compiler bugs;bug-revealing test programs;LET;learning process;scheduling process;time model;bug-revealing probabilities,,1,79,,,,,,IEEE,IEEE Conferences
On a Pursuit for Perfecting an Undergraduate Requirements Engineering Course,C. R. Rupakheti; M. Hays; S. Mohan; S. Chenoweth; A. Stouder,NA; NA; NA; NA; NA,2017 IEEE 30th Conference on Software Engineering Education and Training (CSEE&T),,2017,,,97,106,"Requirements Engineering (RE) is an essential component of any software development cycle. Understanding and satisfying stakeholder needs and wants is the difference between the success and failure of a product. However, RE is often perceived as a ""soft"" skill by our students and is often ignored by students who prioritize the learning of coding, testing, and algorithmic thinking. This view contrasts with the industry, where ""soft"" skills are instead valued equal to any other engineering ability. A key challenge in teaching RE is that students who are accustomed to technical work have a hard time relating to something that is non-technical. Furthermore, students are rarely afforded the opportunity to practice requirements elicitation and management skills in a meaningful way while learning the RE concepts as an adjunct to other content. At Rose-Hulman, we have experimented with several project-based approaches to teaching RE, which have evolved over time. In this paper, we document the progress of our teaching methodologies, capture the pros and cons of these varied approaches, and reflect on what worked and what did not in teaching RE to undergraduate engineering students.",2377-570X,978-1-5386-2536-1978-1-5386-2537,10.1109/CSEET.2017.24,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8166688,Requirements Engineering;Project-Based Learning;Course Evolution,Education;Software;Requirements engineering;Industries;Stakeholders;Interviews;Software engineering,educational courses;engineering education;formal specification;software engineering;teaching,software development cycle;satisfying stakeholder needs;soft skill;management skills;teaching methodologies;engineering students;undergraduate requirement engineering course;RE,,,20,,,,,,IEEE,IEEE Conferences
Performance analysis of OSPF and hybrid networks,A. A. Khan; M. Zafrullah; M. Hussain; A. Ahmad,"Department of Electrical Engineering, University of Lahore, Lahore, Pakistan; Department of Electrical Engineering, University of Lahore, Lahore, Pakistan; Department of Computer Science, COMSATSInstitute of Information Technology, Sahiwal, Pakistan; Department of Electrical Engineering, University of Lahore, Lahore Pakistan",2017 International Symposium on Wireless Systems and Networks (ISWSN),,2017,,,1,4,"Software Defined Network (SDN) for large-scale IP provider network is an open issue and different solutions were proposed. However, the hybrid IP networks in which both distributed and centralized approach provide centralization of SDN and reliability of distributed networks. The common approach in which SDN controls the prioritized traffic and OSPF (Open Shortest Path First) guarantees the operation of traffic. In this research, we propose the SDN segregation, which maintain central management over dispersed routing control. A given topology is split in some fields with OpenFlow enabled switches as in between nodes. OSPF enabled router triggered updates to other routers in other field via SDN switches. The centralized controller defines how two OSPF routers observe each other. There will be a tradeoff between central control of SDN and fault tolerance capability of OSPF. As we increase SDN nodes control will increase and fault tolerance capacity of overall network decreases. The novelty of research work for balanced topology segregation also offers the models for network management. To show the enhancement provided by hybrid network over routing protocol deployment we have deployed separate test beds for routing protocol and proposed hybrid network.",,978-1-5386-1556-0978-1-5386-1555-3978-1-5386-1557,10.1109/ISWSN.2017.8250022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8250022,SDN (Software Defined Network);OSPF (Open Shortest Path First);ABR (Area Border Router),Software defined networking;Routing;Convergence;Network topology;Topology;Time factors;Routing protocols,computer network management;Internet;IP networks;quality of service;routing protocols;software defined networking;telecommunication network management;telecommunication network topology;telecommunication traffic,hybrid network;large-scale IP provider network;hybrid IP networks;distributed approach;centralized approach;distributed networks;Open Shortest Path First;SDN segregation;central management;dispersed routing control;SDN switches;centralized controller;OSPF routers;central control;SDN nodes control;network management;software defined network;Internet;OSPF;IP,,,24,,,,,,IEEE,IEEE Conferences
Petri net based software testing scheduling and selecting,Senke Ding; Peifu Xu; W. Wu; Yi Yang; Zichao Xing; Feihua Lu; Cheng Li,"Institute of Cyber-systems and Control, Zhejiang University, Hangzhou, China; China Tobacco, Zhejiang Industrial Co., Ltd, Hangzhou, China; Institute of Cyber-systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-systems and Control, Zhejiang University, Hangzhou, China; Institute of Cyber-systems and Control, Zhejiang University, Hangzhou, China","2017 IEEE 14th International Conference on Networking, Sensing and Control (ICNSC)",,2017,,,168,173,Computer software system has a profound impact on human society. It increasingly highlights the importance of software testing. Reducing the cost and improving the efficiency of software testing has an important practical significance and economic value. This paper investigates on software testing workflow from the perspective of discrete event dynamic systems and presents a method to improve the efficiency of software testing by optimizing task scheduling and execution priorities. We developed a simulation program of task scheduling based on Petri net to compare the performance of each scheduling option in different situations and made the analysis of their differences.,,978-1-5090-4429-0978-1-5090-4430,10.1109/ICNSC.2017.8000086,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000086,,Software;Analytical models;Phase locked loops,discrete event systems;Petri nets;program testing;scheduling,Petri net based software testing scheduling;computer software system;discrete event dynamic systems;task scheduling;execution priorities,,,8,,,,,,IEEE,IEEE Conferences
Predicting Fault-Prone Classes in Object-Oriented Software: An Adaptation of an Unsupervised Hybrid SOM Algorithm,A. Boucher; M. Badri,NA; NA,"2017 IEEE International Conference on Software Quality, Reliability and Security (QRS)",,2017,,,306,317,"Many fault-proneness prediction models have been proposed in literature to identify fault-prone code in software systems. Most of the approaches use fault data history and supervised learning algorithms to build these models. However, since fault data history is not always available, some approaches also suggest using semi-supervised or unsupervised fault-proneness prediction models. The HySOM model, proposed in literature, uses function-level source code metrics to predict fault-prone functions in software systems, without using any fault data. In this paper, we adapt the HySOM approach for object-oriented software systems to predict fault-prone code at class-level granularity using object-oriented source code metrics. This adaptation makes it easier to prioritize the efforts of the testing team as unit tests are often written for classes in object-oriented software systems, and not for methods. Our adaptation also generalizes one main element of the HySOM model, which is the calculation of the source code metrics threshold values. We conducted an empirical study using 12 public datasets. Results show that the adaptation of the HySOM model for class-level fault-proneness prediction improves the consistency and the performance of the model. We additionally compared the performance of the adapted model to supervised approaches based on the Naive Bayes Network, ANN and Random Forest algorithms.",,978-1-5386-0592-9978-1-5386-0591-2978-1-5386-0593,10.1109/QRS.2017.41,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8009935,Unsupervised Fault-Proneness Prediction;Self-Organizing Map;Multilayer Perceptron;Naive Bayes Network;Object-Oriented Metrics Threshold Values;Object-Oriented Software Systems,Predictive models;Object oriented modeling;Measurement;Software systems;Adaptation models;Data models;Prediction algorithms,object-oriented programming;self-organising feature maps;software metrics;software quality;source code (software);unsupervised learning,fault-prone classes prediction model;unsupervised hybrid SOM algorithm;fault-prone code identification;supervised learning algorithms;fault data history;unsupervised fault-proneness prediction models;semisupervised fault-proneness prediction models;HySOM model;function-level source code metrics;object-oriented software systems;class-level granularity,,,33,,,,,,IEEE,IEEE Conferences
PV-OWL ƒ?? Pharmacovigilance surveillance through semantic web-based platform for continuous and integrated monitoring of drug-related adverse effects in open data sources and social media,C. Piccinni; E. Poluzzi; M. Orsini; S. Bergamaschi,"University of Bologna Pharmacology Unit, Department of Medical and Surgical Sciences, Bologna, Italy; University of Bologna Pharmacology Unit, Department of Medical and Surgical Sciences, Bologna, Italy; Data River srl, Modena, Italy; University of Modena and Reggio Emilia, Modena, Italy",2017 IEEE 3rd International Forum on Research and Technologies for Society and Industry (RTSI),,2017,,,1,5,"The recent EU regulation on Pharmacovigilance [Regulation (EU) 1235/2010, Directive 2010/84/EU] imposes both to Pharmaceutical companies and Public health agencies to maintain updated safety information of drugs, monitoring all available data sources. Here, we present our project aiming to develop a web platform for continuous monitoring of adverse effects of medicines (pharmacovigilance), by integrating information from public databases, scientific literature and social media. The project will start by scanning all available data sources concerning drug adverse events, both open (e.g., FAERS - FDA Adverse Event Reporting Systems, medical literature, social media, etc.) and proprietary data (e.g., discharge hospital records, drug prescription archives, electronic health records), that require agreement with respective data owners. Subsequent, pharmacovigilance experts will perform a semi-automatic mapping of codes identifying drugs and adverse events, to build the thesaurus of the web based platform. After these preliminary activities, signal generation and prioritization will be the core of the project. This task will result in risk confidence scores for each included data source and a comprehensive global score, indicating the possible association between a specific drug and an adverse event. The software framework MOMIS, an open source data integration system, will allow semi-automatic virtual integration of heterogeneous and distributed data sources. A web platform, based on MOMIS, able to merge many heterogeneous data sets concerning adverse events will be developed. The platform will be tested by external specialized subjects (clinical researchers, public or private employees in pharmacovigilance field). The project will provide a) an innovative way to link, for the first time in Italy, different databases to obtain novel safety indicators; b) a web platform for a fast and easy integration of all available data, useful to verify and validate hypothesis generated in signal detection. Finally, the development of the unified safety indicator (global risk score) will result in a compelling, easy-to-understand, visual format for a broad range of professional and not professional users like patients, regulatory authorities, clinicians, lawyers, human scientists.",,978-1-5386-3906-1978-1-5386-3907,10.1109/RTSI.2017.8065931,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8065931,PharmacoVigilance;semantic web-based platform;data-integration;open data,Drugs;Monitoring;Social network services;Safety;Semantics;Databases,data integration;drugs;electronic health records;medicine;patient monitoring;pharmaceutical industry;semantic Web;social networking (online),PV-OWL;pharmacovigilance surveillance;semantic Web;drug-related adverse effects monitoring;social media;EU regulation 1235/2010;Directive 2010/84/EU;pharmaceutical companies;public health agencies;safety information;medicines;drug prescription archives;electronic health records;semiautomatic mapping;MOMIS software framework;open source data integration system;semiautomatic virtual integration;heterogeneous data sources;distributed data sources,,,24,,,,,,IEEE,IEEE Conferences
QoS-based routing over software defined networks,A. Kucminski; A. Al-Jawad; P. Shah; R. Trestian,"School of Science and Technology, Middlesex University, London, UK; School of Science and Technology, Middlesex University, London, UK; School of Science and Technology, Middlesex University, London, UK; School of Science and Technology, Middlesex University, London, UK",2017 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB),,2017,,,1,6,"Quality of Service (QoS) relies on the shaping of preferential delivery services for applications in favour of ensuring sufficient bandwidth, controlling latency and reducing packet loss. QoS can be achieved by prioritizing important broadband data traffic over the less important one. Thus, depending on the users' needs, video, voice or data traffic take different priority based on the prevalent importance within a particular context. This prioritization might require changes in the configuration of each network entity which can be difficult in traditional network architecture. To this extent, this paper investigates the use of a QoS-based routing scheme over a Software Defined Network (SDN). A real SDN test-bed is constructed using Raspberry Pi computers as virtual SDN switches managed by a centralized controller. It is shown that a QoS-based routing approach over SDN generates enormous control possibilities and enables automation.",2155-5052,978-1-5090-4937-0978-1-5090-4938,10.1109/BMSB.2017.7986239,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986239,Quality of Service;Software Defined Networks;Prioritized Routing;Networking,Quality of service;Routing;Control systems;Ports (Computers);Network topology;Topology;Software defined networking,broadband networks;centralised control;quality of service;software defined networking;telecommunication network routing;telecommunication traffic,software defined network;quality of service;packet loss reduction;latency control;broadband data traffic;QoS-based routing scheme;real SDN test-bed;Raspberry Pi computers;virtual SDN switch;centralized controller,,2,15,,,,,,IEEE,IEEE Conferences
Ranking Modules for Integrate Testing Based on PageRank Algorithm,Q. Sun; K. R. Moniz; Y. Yuan,"Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China; Beijing University of Posts and Telecommunications, China",Chinese Journal of Electronics,,2017,26,5,993,998,"The testing industry need to prioritize the limited resources and focus on testing modules whose failure is mostly likely to cause faults. This paper discusses a method that can rank modules in a software package for integrate testing using the PageRank algorithm. In this algorithm, a sequences of random walks iteratively can find a high likelihood of encountering a node, which is interpreted as it being an important performance resource. An experiment result prove that the proposed method actually can be used to prioritize testing of specific modules when testing resource are scarce.",1022-4653;2075-5597,,10.1049/cje.2017.03.015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8055343,,,graph theory;program testing;random processes;software packages,modules ranking;integrate testing;PageRank algorithm;software testing;software package;random walks sequences,,,,,,,,,IET,IET Journals & Magazines
Regression Testing Goals - View of Practitioners and Researchers,N. M. Minhas; K. Petersen; N. B. Ali; K. Wnuk,NA; NA; NA; NA,2017 24th Asia-Pacific Software Engineering Conference Workshops (APSECW),,2017,,,25,31,"Context: Regression testing is a well-researched area. However, the majority regression testing techniques proposed by the researchers are not getting the attention of the practitioners. Communication gaps between industry and academia, and disparity in the regression testing goals are the main reasons. Close collaboration can help in bridging the communication gaps and resolving the disparities. Objective: The study aims at exploring the views of academics and practitioners about the goals of regression testing. The purpose is to investigate the commonalities and differences in their viewpoints and defining some common goals for the success of regression testing. Method: We conducted a focus group study, with 7 testing experts from industry and academia. 4 testing practitioners from 2 companies and 3 researchers from 2 universities participated in the study. We followed GQM approach, to elicit the regression testing goals, information needs, and measures. Results: 43 regression testing goals were identified by the participants, which were reduced to 10 on the basis of similarity among the identified goals. Later during the priority assignment process, 5 goals were discarded, because the priority assigned to these goals was very low. Participants identified 47 information needs/questions required to evaluate the success of regression testing with reference to goal G5 (confidence). Which were then reduced to 10 on the basis of similarity. Finally, we identified measures to gauge those information needs/questions, which were corresponding to the goal (G5). Conclusions: We observed that participation level of practitioners and researchers during the elicitation of goals and questions was same. We found a certain level of agreement between the participants regarding the regression testing definitions and goals. But there was some level of disagreement regarding the priorities of the goals. We also identified the need to implement a regression testing evaluation framework in the participating companies.",,978-1-5386-2649-8978-1-5386-2650,10.1109/APSECW.2017.23,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8312521,Regression testing;Regression testing goals;GQM;Focus group,Testing;Industries;Collaboration;Companies;Software engineering;Atmospheric measurements;Particle measurements,program testing;regression analysis;software metrics;software quality,majority regression;communication gaps;regression testing definitions;regression testing evaluation framework;regression testing goals,,,26,,,,,,IEEE,IEEE Conferences
Regression Testing of Database Applications Under an Incremental Software Development Setting,R. H. Rosero; O. S. G??mez; G. Rodr??guez,"Universidad Nacional Mayor de San Marcos, Lima, Peru; Escuela Superior Polit??cnica de Chimborazo, Riobamba, Ecuador; Universidad Nacional Mayor de San Marcos, Lima, Peru",IEEE Access,,2017,5,,18419,18428,"Software regression testing verifies previous features on a software product when it is modified or new features are added to it. Because of the nature of regression testing it is a costly process. Different approaches have been proposed to reduce the costs of this activity, among which are: minimization, prioritization, and selection of test cases. Recently, soft computing techniques, such as data mining, machine learning, and others have been used to make regression testing more efficient and effective. Currently, in different contexts, to a greater or lesser extent, software products have access to databases (DBs). Given this situation, it is necessary to consider regression testing also for software products such as information systems that are usually integrated with or connected to DBs. In this paper, we present a selection regression testing approach that utilizes a combination of unsupervised clustering with random values, unit tests, and the DB schema to determine the test cases related to modifications or new features added to software products connected to DBs. Our proposed approach is empirically evaluated with two database software applications in a production context. Effectiveness metrics, such as test suite reduction, fault detection capability, recall, precision, and the F-measure are examined. Our results suggest that the proposed approach is enough effective with the resulting clusters of test cases.",2169-3536,,10.1109/ACCESS.2017.2749502,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8027014,Software regression testing;fault detection capability;test suite reduction;software verification;software engineering,Software;Testing;Databases;Minimization;Data mining;Fault detection;Optimization,data mining;database management systems;learning (artificial intelligence);program testing;regression analysis;software development management;software metrics,DB schema;effectiveness metrics;machine learning;data mining;soft computing techniques;test suite reduction;database software applications;unit tests;selection regression;test cases;software product;software regression testing;incremental software development setting,,,56,,OAPA,,,,IEEE,IEEE Journals & Magazines
Requirement dependencies-based formal approach for test case prioritization in regression testing,A. Vescan; C. ?erban; C. ChisŽŸliœŽŸ-Cretu; L. Dio?an,"Department of Computer Science, Faculty of Mathematics and Computer Science, Babes-Bolyai University, M. Kogalniceanu 1, 400084, Cluj-Napoca, Romania; Department of Computer Science, Faculty of Mathematics and Computer Science, Babes-Bolyai University, M. Kogalniceanu 1, 400084, Cluj-Napoca, Romania; Department of Computer Science, Faculty of Mathematics and Computer Science, Babes-Bolyai University, M. Kogalniceanu 1, 400084, Cluj-Napoca, Romania; Department of Computer Science, Faculty of Mathematics and Computer Science, Babes-Bolyai University, M. Kogalniceanu 1, 400084, Cluj-Napoca, Romania",2017 13th IEEE International Conference on Intelligent Computer Communication and Processing (ICCP),,2017,,,181,188,"Regression testing is the testing activity performed after changes occurred on software. Its aim is to increase confidence that achieved software adjustments have no negative impact on the already functional parts of the software. Test case prioritization is one technique that could be applied in regression testing with the aim to find faults early, resulting in reduced cost and shorten time of testing activities. Thus, prioritizing in the context of regression testing means to re-order test cases such that high priority ones are run first. The current paper addresses the test case prioritization as a consistent part of a larger approach on regression testing, which combines both test case prioritization and test case selection in order to overcome the limitations of each of them. A comprehensive formalization of test case prioritization is provided, incorporating beside the well known ingredients (test case, test requirement, fault, cost) also elements relating to the functional requirements and dependencies between requirements. An evolutionary algorithm is used to construct the re-ordering of test cases, considering as optimization objectives fault detection and cost. A synthetic case study was used to empirically prove our perspective for test case prioritization approach.",,978-1-5386-3368-7978-1-5386-3367-0978-1-5386-3369,10.1109/ICCP.2017.8117002,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8117002,,Testing;Software;Evolutionary computation;Fault detection;Optimization;Measurement;Search problems,evolutionary computation;fault diagnosis;program testing;regression analysis;software fault tolerance,re-order test cases;regression testing;test case selection;test requirement;test case prioritization approach;testing activity;requirement dependencies-based formal approach;evolutionary algorithm;software adjustments,,,31,,,,,,IEEE,IEEE Conferences
Requirement paramerisation of flap actuation system: Product life-cycle management processes &amp; tools,D. Taylor; I. Panella,"Systems Engineering, Actuation Systems, UTC Aerospace Systems, Wolverhampton, United Kingdom; Systems Engineering, Actuation Systems, UTC Aerospace Systems, Wolverhampton, United Kingdom",2017 IEEE International Systems Engineering Symposium (ISSE),,2017,,,1,6,"Flap actuation systems (FAS) have numerous operational states, modes and environmental constraints, which translate to thousands of requirements associated with them. FAS must be analyzed and tested to determine compliance with requirements in any operating conditions. In this paper, examples of parametrized requirements for centrally driven FAS are presented and the advantages brought by a requirement parametrization process in the systems engineering life- cycle discussed, from requirements elicitation to requirements validation and verification. The challenges of robust re-use and customization of components within legacy systems to drive cost reduction is significant as often legacy products were not developed within a model base framework. The requirements prioritization process provides a low cost, high impact example of alternative to ensure integrity and full traceability of requirements without the need to implement complex simulation platforms.",,978-1-5386-3403-5978-1-5386-3404,10.1109/SysEng.2017.8088317,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8088317,Systems Engineering;Requirement Management;Dynamic Requirement Allocation;DOORS.,Actuators;Sensors;Safety;Aerospace control;Aircraft;Brakes;Robustness,actuators;aerospace computing;aircraft control;control engineering computing;cost reduction;formal specification;formal verification;product life cycle management;software maintenance;systems analysis;systems engineering,flap actuation system;environmental constraints;centrally driven FAS;requirement parametrization process;requirements elicitation;requirements validation;legacy systems;legacy products;requirements prioritization process;systems engineering lifecycle;requirements verification;complex simulation platforms;product life-cycle management processes;cost reduction,,,3,,,,,,IEEE,IEEE Conferences
Risk-based attack surface approximation: how much data is enough?,C. Theisen; K. Herzig; B. Murphy; L. Williams,"Comput. Sci., NC State Univ., Raleigh, NC, USA; Microsoft Res., Cambridge, UK; Microsoft Res., Cambridge, UK; Comput. Sci., NC State Univ., Raleigh, NC, USA",2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP),,2017,,,273,282,"Proactive security reviews and test efforts are a necessary component of the software development lifecycle. Resource limitations often preclude reviewing the entire code base. Making informed decisions on what code to review can improve a team's ability to find and remove vulnerabilities. Risk-based attack surface approximation (RASA) is a technique that uses crash dump stack traces to predict what code may contain exploitable vulnerabilities. The goal of this research is to help software development teams prioritize security efforts by the efficient development of a risk-based attack surface approximation. We explore the use of RASA using Mozilla Firefox and Microsoft Windows stack traces from crash dumps. We create RASA at the file level for Firefox, in which the 15.8% of the files that were part of the approximation contained 73.6% of the vulnerabilities seen for the product. We also explore the effect of random sampling of crashes on the approximation, as it may be impractical for organizations to store and process every crash received. We find that 10-fold random sampling of crashes at a rate of 10% resulted in 3% less vulnerabilities identified than using the entire set of stack traces for Mozilla Firefox. Sampling crashes in Windows 8.1 at a rate of 40% resulted in insignificant differences in vulnerability and file coverage as compared to a rate of 100%.",,978-1-5386-2717-4978-1-5386-2718,10.1109/ICSE-SEIP.2017.9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7965451,stack traces;attack surface;prediction models,Security;Software systems;Computer bugs;Measurement,security of data;software engineering,risk-based attack surface approximation;RASA;Mozilla Firefox;Microsoft Windows;random sampling;proactive security reviews;test efforts;software development lifecycle,,2,42,,,,,,IEEE,IEEE Conferences
Studies on open source real time operating systems: For vehicle suspension control,M. Balasubramanian; N. S. Usha,"Department of Computer Science and Engineering S.A. Engineering College, Chennai-77; Department of Computer Science and Engineering S.A. Engineering College, Chennai-77",2017 International Conference on Information Communication and Embedded Systems (ICICES),,2017,,,1,3,"Studying the TIMELINESS of PERIODIC tasks of real time operating systems available in open source, to determine the reliability and efficiency of the systems for implementing them in suspension control of ground vehicles. We take several operating systems which are open source and are available to the public through GNU license (ex: Linux). The Kernels, which are the building blocks of the operating systems which connect hardware and software, are patched using selective real time patches which makes the operating system real time. This is done in order to unlock real time capabilities such as unbounded latencies and real time priorities. These operating systems are tested for their efficiency and timeliness. The results are compared with the test logs of non-real time operating system. These kernels are then cross compiled and built for ARM architecture. This kernel is then applied to an embedded system which is then tested using the same afore mentioned tests. The results are logged and analyzed.",,978-1-5090-6135-8978-1-5090-6136,10.1109/ICICES.2017.8070787,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070787,operating systems;embedded systems;real-time operating systems;xenomai,Real-time systems;Kernel;Linux;Suspensions;Embedded systems,Linux;operating system kernels;real-time systems;suspensions (mechanical components);vehicle dynamics,operating systems;Kernels;selective real time patches;embedded system;vehicle suspension control;open source real time operating systems,,,4,,,,,,IEEE,IEEE Conferences
Table of contents,,,2017 IEEE/ACM 2nd International Workshop on Variability and Complexity in Software Design (VACE),,2017,,,v,vi,The following topics are dealt with: software design; mutation testing; collective online testing; collective offline testing; usability tests; Web services; delta-oriented programming; delta-oriented product prioritization and feature models.,,978-1-5386-2803-4978-1-5386-2804,10.1109/VACE.2017.15,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968031,,,mobile computing;program testing;software engineering;Web services,feature models;delta-oriented product prioritization;delta-oriented programming;Web services;usability tests;collective offline testing;collective online testing;mutation testing;software design,,,,,,,,,IEEE,IEEE Conferences
Test Case Generation and Prioritization: A Process-Mining Approach,A. Janes,NA,"2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",,2017,,,38,39,"Test cases are an essential tool in software quality assurance: they ensure that code behaves as specified in the requirement. However, writing test cases does not have only benefits, it comes with a cost: the programmer has to formulate the test cases and maintain them when the tested source code changes. Particularly for start-ups or small enterprises such costs become prohibitive, which often prefer to invest their time into the development of new functionalities instead of testing. This paper explores the use of process-mining as an approach to create a model of how users interact with a system to a) generate test cases and b) prioritize them. Using process-mining, it is possible to mine from the user behaviour which parts of the system are the most used, in which order they are executed, generate test cases repeating user input, and prioritizing test cases.",,978-1-5090-6676-6978-1-5090-6677,10.1109/ICSTW.2017.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899028,Process mining;Test case generation;Test case prioritization,Software;Data mining;Testing;Conferences;Writing;Computer aided software engineering;Programming,data mining;program testing;software quality;source code (software),test case generation;prioritization;process-mining approach;software quality assurance;source code,,,6,,,,,,IEEE,IEEE Conferences
Test Optimization from Release Insights: An Analytical Hierarchy Approach,K. Singi; V. Kaulgud; V. S. Sharma; N. Dubash; S. Podder,NA; NA; NA; NA; NA,2017 IEEE/ACM 3rd International Workshop on Rapid Continuous Software Engineering (RCoSE),,2017,,,16,19,"Software Testing is an essential aspect to ensure software quality, reliability and consistent user experience. Digital applications such as mobile app usually follow rapid software delivery which consists of various releases. It typically uses insights from the development data such as defects, test logs for test execution optimization. Once the application is released and deployed, there is rich availability of untapped heterogeneous data which can also be effectively utilized for the next release test execution optimization. The data from the release includes direct customer feedback, application monitoring data such as user behavioral traces, device usages, release logs. In this position paper, we discuss about the various data sources and the multiple insights which can be derived from them. We also propose a framework which uses Analytical Hierarchy Process to prioritize the tests based on these insights available from the release data. The framework also recommends the prioritized and missed device configurations for next release test planning.",,978-1-5386-0428-1978-1-5386-0429,10.1109/RCoSE.2017.2,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7967947,Test Prioritization;Device Configurations;Release data;Analytical Hierarchy Process,Computer crashes;Testing;Monitoring;Optimization;Tools;Mobile communication;Production,analytic hierarchy process;program testing;software quality;software reliability,software testing;software quality;software reliability;analytical hierarchy approach;test execution optimization;direct customer feedback,,,22,,,,,,IEEE,IEEE Conferences
Test Prioritization with Optimally Balanced Configuration Coverage,D. Marijan; M. Liaaen,NA; NA,2017 IEEE 18th International Symposium on High Assurance Systems Engineering (HASE),,2017,,,100,103,"Testing configurable software for high assurancesystems developed in continuous integration requires effectivetechniques for selecting failure-inducing test cases, thoroughlycovering entire configuration space, while providing rapid feedbackon failures. This involves satisfying multiple objectives:maximizing test fault detection, maximizing test coverage ofthe configuration space, and minimizing test execution time, which often leads to compromises in practice. In this paper, weaddress this problem with a practical test optimization approachthat uses historical test data to determine an optimal order oftests ensuring high progressively uniform configuration coverage, early fault detection, and rapid test feedback. We extensivelyvalidate the approach in a set of experiments using industry testsuites, and report experimental results showing the improvementin efficiency compared to industry practice. In particular, theapproach showed to increase the uniformity of configurationcoverage by 39% on average, which increases fault detectionup to 15%, while just slightly delaying test feedback.",1530-2059,978-1-5090-4636-2978-1-5090-4637,10.1109/HASE.2017.26,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911878,,Testing;Fault detection;Optimization;Software;Industries;Bars;History,,,,1,10,,,,,,IEEE,IEEE Conferences
Test Suite Prioritization for Efficient Regression Testing of Model-Based Automotive Software,A. Morozov; K. Ding; T. Chen; K. Janschek,NA; NA; NA; NA,"2017 International Conference on Software Analysis, Testing and Evolution (SATE)",,2017,,,20,29,"Up to 80% of the automotive software can be generated from models. MATLAB Simulink is a common tool for creation of complex combinations of block diagrams and state machines, automated generation of executable code, and its deployment on a target ECU. The automotive safety standards require extensive testing of the developed models. Regression testing should be undertaken every time a model is updated to ensure that the modifications do not introduce new faults into the previously validated model. A common, time-consuming way is to rerun an entire test suite after even minor changes. This paper introduces a new method for automatic prioritization of test cases. The method is based on two principles: (i) A test case should stimulate an error in an updated block and (ii) the stimulated error should propagate to the place where it can be detected. The proposed method includes the evaluation of input vectors that are provided to updated blocks by each test case and a Markov-based stochastic error propagation analysis of the model. The application of the method is demonstrated with a Simulink model of a gearbox and a test-suite, automatically generated with the Reactis Tester.",,978-1-5386-3687-9978-1-5386-3688,10.1109/SATE.2017.11,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8118517,Model-based software;Automotive software;Simulink;Software testing;Regression testing;Test suite prioritization;Model-based testing;Error propagation analysis;Control flow;Data flow;Dual-graph error propagation model;Fault activation analysis;M,Testing;Software packages;Analytical models;Automotive engineering;Data models;Tools,automotive electronics;automotive engineering;Markov processes;program testing,automatic prioritization;updated block;Simulink model;test suite prioritization;automotive software;MATLAB Simulink;block diagrams;state machines;automated generation;automotive safety standards;regression testing;validated model;model-based automotive software;Markov-based stochastic error propagation analysis;gearbox;Reactis Tester,,,43,,,,,,IEEE,IEEE Conferences
The organization of arrangements set to ensure enterprise IPV6 network secure work by modern switching equipment tools (using the example of a network attack on a default gateway),A. M. Shabalin; E. A. Kaliberda,"Omsk State Technical University, Omsk, Russia; Omsk State Technical University, Omsk, Russia","2017 Dynamics of Systems, Mechanisms and Machines (Dynamics)",,2017,,,1,8,"The article issue is the enterprise information protection within the internet of things concept. The aim of research is to develop arrangements set to ensure secure enterprise IPv6 network operating. The object of research is the enterprise IPv6 network. The subject of research is modern switching equipment as a tool to ensure network protection. The research task is to prioritize functioning of switches in production and corporation enterprise networks, to develop a network host protection algorithm, to test the developed algorithm on the Cisco Packet Tracer 7 software emulator. The result of research is the proposed approach to IPv6-network security based on analysis of modern switches functionality, developed and tested enterprise network host protection algorithm under IPv6-protocol with an automated network SLAAC-configuration control, a set of arrangements for resisting default enterprise gateway attacks, using ACL, VLAN, SEND, RA Guard security technology, which allows creating sufficiently high level of networks security.",,978-1-5386-1820-2978-1-5386-1821,10.1109/Dynamics.2017.8239505,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8239505,default gateway;host;network security;attacks;protocols;network;IPv6-protocol;switches;routers;IP network;host protection algorithm,Security;Logic gates;Protocols;Computers;Organizations,business communication;computer network security;Internet;Internet of Things;internetworking;IP networks;protocols,enterprise IPV6 network security;internet of things;network switches;switching equipment;IPv6 network;Cisco Packet Tracer 7 software emulator;IPv6-network security;corporation enterprise networks;network protection;secure enterprise;enterprise information protection;network attack;networks security;default enterprise gateway attacks;automated network SLAAC-configuration control;IPv6-protocol;enterprise network host protection algorithm,,,14,,,,,,IEEE,IEEE Conferences
The Significant Effects of Data Sampling Approaches on Software Defect Prioritization and Classification,K. E. Bennin; J. Keung; A. Monden; P. Phannachitta; S. Mensah,NA; NA; NA; NA; NA,2017 ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM),,2017,,,364,373,"Context: Recent studies have shown that performance of defect prediction models can be affected when data sampling approaches are applied to imbalanced training data for building defect prediction models. However, the magnitude (degree and power) of the effect of these sampling methods on the classification and prioritization performances of defect prediction models is still unknown. Goal: To investigate the statistical and practical significance of using resampled data for constructing defect prediction models. Method: We examine the practical effects of six data sampling methods on performances of five defect prediction models. The prediction performances of the models trained on default datasets (no sampling method) are compared with that of the models trained on resampled datasets (application of sampling methods). To decide whether the performance changes are significant or not, robust statistical tests are performed and effect sizes computed. Twenty releases of ten open source projects extracted from the PROMISE repository are considered and evaluated using the AUC, pd, pf and G-mean performance measures. Results: There are statistical significant differences and practical effects on the classification performance (pd, pf and G-mean) between models trained on resampled datasets and those trained on the default datasets. However, sampling methods have no statistical and practical effects on defect prioritization performance (AUC) with small or no effect values obtained from the models trained on the resampled datasets. Conclusions: Existing sampling methods can properly set the threshold between buggy and clean samples, while they cannot improve the prediction of defect-proneness itself. Sampling methods are highly recommended for defect classification purposes when all faulty modules are to be considered for testing.",,978-1-5090-4039-1978-1-5090-4040,10.1109/ESEM.2017.50,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170123,Imbalanced data;Defect prediction;Sampling methods;Statistical significance;Empirical software engineering,Predictive models;Sampling methods;Data models;Software;Computational modeling;Robustness;Software engineering,learning (artificial intelligence);sampling methods;software fault tolerance;software metrics;software quality;statistical analysis,data sampling approaches;software defect prioritization;imbalanced training data;building defect prediction models;prioritization performances;statistical significance;practical significance;data sampling methods;prediction performances;no sampling method;resampled datasets;pf;statistical effects;defect prioritization performance;defect classification purposes,,,51,,,,,,IEEE,IEEE Conferences
Thermo-mechanical reliability analysis of flip-chip bonded silicon carbide Schottky diodes,S. Seal; A. K. Wallace; J. E. Zumbro; H. A. Mantooth,"Department of Electrical Engineering, University of Arkansas, Fayetteville, AR; Department of Electrical Engineering, University of Arkansas, Fayetteville, AR; Department of Electrical Engineering, University of Arkansas, Fayetteville, AR; Department of Electrical Engineering, University of Arkansas, Fayetteville, AR",2017 IEEE International Workshop On Integrated Power Packaging (IWIPP),,2017,,,1,5,"This paper presents the thermo-mechanical reliability analysis of a novel chip-scale wire bondless packaging technique for a SiC Schottky diode that leads to lower parasitics, higher reliability, lower costs, and lower losses. The proposed approach uses a flip-chip solder ball array to make connections to the anode. A copper connector was used to make contact with the bottom cathode, thus reconfiguring the bare die into a chip-scale, flip-chip capable device. Thermo-mechanical analysis in a finite element software showed that the proposed approach could better manage Coefficient of Thermal Expansion (CTE) mismatch stresses arising at the critical module interfaces as compared with a conventional wire bonded module. A detailed analysis of the flip-chip structure is presented and contrasted with a state-of-the-art wire bonded module. Different design parameters were explored for the drain connector to be able to make an optimized decision. However, keeping production costs low was prioritized without compromising significant performance. The fabrication process for manufacturing a flip-chip schottky diode module was also demonstrated along with preliminary test results to demonstrate functionality.",,978-1-5090-4278-4978-1-5090-4279,10.1109/IWIPP.2017.7936756,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7936756,Silicon carbide;wire bondless;flip-chip bonding;electronics packaging;chip scale packaging;low parasitics,Stress;Wires;Silicon carbide;Flip-chip devices;Schottky diodes;Thermomechanical processes;Connectors,chip scale packaging;electronics packaging;flip-chip devices;Schottky diodes;semiconductor device reliability;silicon compounds;solders;thermal expansion;wide band gap semiconductors,thermo-mechanical reliability analysis;flip-chip bonded silicon carbide Schottky diodes;chip-scale wire bondless packaging technique;flip-chip solder ball array;copper connector;coefficient of thermal expansion mismatch stresses;SiC,,1,9,,,,,,IEEE,IEEE Conferences
TITAN: Test Suite Optimization for Highly Configurable Software,D. Marijan; M. Liaaen; A. Gotlieb; S. Sen; C. Ieva,NA; NA; NA; NA; NA,"2017 IEEE International Conference on Software Testing, Verification and Validation (ICST)",,2017,,,524,531,"Exhaustive testing of highly configurable software developed in continuous integration is rarely feasible in practice due to the configuration space of exponential size on the one hand, and strict time constraints on the other. This entails using selective testing techniques to determine the most failure-inducing test cases, conforming to highly-constrained time budget. These challenges have been well recognized by researchers, such that many different techniques have been proposed. In practice, however, there is a lack of efficient tools able to reduce high testing effort, without compromising software quality. In this paper we propose a test suite optimization technology TITAN, which increases the time-and cost-efficiency of testing highly configurable software developed in continuous integration. The technology implements practical test prioritization and minimization techniques, and provides test traceability and visualization for improving the quality of testing. We present the TITAN tool and discuss a set of methodological and technological challenges we have faced during TITAN development. We evaluate TITAN in testing of Cisco's highly configurable software with frequent high quality releases, and demonstrate the benefit of the approach in such a complex industry domain.",,978-1-5090-6031-3978-1-5090-6032,10.1109/ICST.2017.60,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7928010,Test optimization;test prioritization;highly-configurable software;continuous integration testing,Testing;Optimization;Software;Minimization;Tools;Fault detection;Data models,minimisation;program diagnostics;program testing;program visualisation;software quality,TITAN;test suite optimization;Cisco highly configurable software testing;selective testing;failure-inducing test cases;software quality;test prioritization techniques;test minimization techniques;test traceability;visualization,,,28,,,,,,IEEE,IEEE Conferences
Towards Execution Time Prediction for Manual Test Cases from Test Specification,S. Tahvili; M. Saadatmand; M. Bohlin; W. Afzal; S. H. Ameerjan,NA; NA; NA; NA; NA,2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA),,2017,,,421,425,"Knowing the execution time of test cases is important to perform test scheduling, prioritization and progress monitoring. This work in progress paper presents a novel approach for predicting the execution time of test cases based on test specifications and available historical data on previously executed test cases. Our approach works by extracting timing information (measured and maximum execution time)for various steps in manual test cases. This information is then used to estimate the maximum time for test steps that have not previously been executed, but for which textual specifications exist. As part of our approach, natural language parsing of the specifications is performed to identify word combinations to check whether existing timing information on various test activities is already available or not. Finally, linear regression is used to predict the actual execution time for test cases. A proof-of-concept use case at Bombardier Transportation serves to evaluate the proposed approach.",,978-1-5386-2141-7978-1-5386-2140-0978-1-5386-2142,10.1109/SEAA.2017.10,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8051381,Software Testing;Execution time;Linear Regression;NLP;Optimization;Test Specification;Estimation,Testing;Estimation;Manuals;Prediction algorithms;Time measurement;Natural languages;Linear regression,natural language processing;program testing;regression analysis;system monitoring,execution time prediction;manual test cases;test specification;test scheduling;progress monitoring;historical data;maximum time estimation;natural language parsing;linear regression;Bombardier Transportation,,2,8,,,,,,IEEE,IEEE Conferences
Towards the design of a secure and compliant framework for OpenEMR,S. Acharya; Y. Yin; A. Mak,"Department of Computer and Information Sciences, Towson University, Towson, MD, 21252; Department of Computer and Information Sciences, Towson University, Towson, MD, 21252; Department of Computer Science, Johns Hopkins University, Baltimore, MD, 21218",2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),,2017,,,992,999,"The purpose of this research is to explore and identify the vulnerabilities in OpenEMR 5.0.0, which is a free and open source medical practice management application. We are to provide recommendations/suggestions to OpenEMR developers on identifying the vulnerabilities. We chose to use vulnerabilities scanning tools to manually explore the demo site of OpenEMR 5.0.0. The targeted vulnerabilities belong to the following three types, namely, SQL Injection, Cross-Site Scripting (XSS) including persistent XSS and reflected XSS and Arbitrary File Upload. We have inducted a qualitative based risk assessment to determine the risk levels for the vulnerabilities identified. The results of risk assessment include two kinds of risk levels, which are high risk and medium risk, and two kinds of priorities, which are priority 1 (high) and priority 2 (medium). In addition, we provided recommendations and best practices about how to prevent the identified vulnerabilities. Furthermore, the research also presents an exploit automation program written in Python to test and exploit the vulnerabilities including SQL Injection and reflected XSS on the demo server of OpenEMR.",,978-1-5090-3050-7978-1-5090-3051,10.1109/BIBM.2017.8217792,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8217792,,Tools;Servers;Risk management;Databases,Internet;medical information systems;public domain software;risk management;security of data;software tools;SQL;Web sites,persistent XSS;qualitative based risk assessment;identified vulnerabilities;SQL Injection;compliant framework;OpenEMR 5;open source medical practice management application;OpenEMR developers;demo site;targeted vulnerabilities;Cross-Site Scripting;secure framework design;reflected XSS;Arbitrary File Upload;vulnerabilities scanning tools,,,16,,,,,,IEEE,IEEE Conferences
Trends on empty exception handlers for Java open source libraries,A. F. Nogueira; J. C. B. Ribeiro; M. A. Zenha-Rela,"CISUC, University of Coimbra, Portugal; Polytechnic Institute of Leiria, Portugal; CISUC, University of Coimbra, Portugal","2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)",,2017,,,412,416,"Exception-handling structures provide a means to recover from unexpected or undesired flows that occur during software execution, allowing the developer to put the program in a valid state. Still, the application of proper exception-handling strategies is at the bottom of priorities for a great number of developers. Studies have already discussed this subject pinpointing that, frequently, the implementation of exception-handling mechanisms is enforced by compilers. As a consequence, several anti-patterns about Exception-handling are already identified in literature. In this study, we have picked several releases from different Java programs and we investigated one of the most well-known anti-patterns: the empty catch handlers. We have analysed how the empty handlers evolved through several releases of a software product. We have observed some common approaches in terms of empty catches' evolution. For instance, often an empty catch is transformed into a empty catch with a comment. Moreover, for the majority of the programs, the percentage of empty handlers has decreased when comparing the first and last releases. Future work includes the automation of the analysis allowing the inclusion of data collected from other software artefacts: test suites and data from issue tracking systems.",,978-1-5090-5501-2978-1-5090-5502,10.1109/SANER.2017.7884644,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884644,Exception-Handling;Open-Source;Software Evolution,Software;Java;Measurement;Computer bugs;Libraries;XML;Market research,exception handling;Java;public domain software;software libraries,empty-exception handlers;Java open source libraries;software execution;compilers;Java programs;antipatterns;empty catch handlers;software product releases;software artefacts;test suites,,,11,,,,,,IEEE,IEEE Conferences
Value-Based Decision-Making Using a Web-Based Tool: A Multiple Case Study,V. Freitas; M. Perkusich; E. Mendes; P. Rodr??guez; M. Oivo,NA; NA; NA; NA; NA,2017 24th Asia-Pacific Software Engineering Conference (APSEC),,2017,,,279,288,"[Context]: To remain competitive, innovative and to grow, companies should use a value-based decision-making where decisions are the best for that company's overall value creation. However, without tool support, the use of explicit value propositions and aggregation of different key stakeholders' decisions during decision-making may be a challenge for many companies. [Goal]: The goal of this paper is to investigate the extent to which a Web-based tool for value-based decision-making can successfully support stakeholders' decision-making process. [Method]: We conducted three case studies across four software projects, during six weeks, in the contexts of feature selection, test cases execution prioritization and user interfaces design selection. Prior to using the tool, stakeholders' value propositions were elicited via focus-group meetings; later, during a post-mortem phase, data was gathered via observation, semi-structured interviews and structured questionnaires. [Results]: Participants reported an improvement of their decision-making process and quality of decisions; further, they also felt confident about using the tool, and that it can be useful to their work. [Conclusions]: Results suggested that the use of tool support by the stakeholders in the investigated company for value-based decision-making improved their decision-making process and the quality of decisions.",,978-1-5386-3681-7978-1-5386-3682,10.1109/APSEC.2017.34,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8305950,Value-based decision-making;Value-based software engineering;tool support,Tools;Decision making;Stakeholders;Companies;Software;Biological system modeling;Software engineering,decision making;feature selection;Internet;project management;user interfaces,value-based decision-making;value creation;explicit value propositions;user interfaces;web-based tool;feature selection,,,20,,,,,,IEEE,IEEE Conferences
Weighting for Combinatorial Testing by Bayesian Inference,E. Choi; T. Fujiwara; O. Mizuno,NA; NA; NA,"2017 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",,2017,,,389,391,"Combinatorial testing (CT) is a widely-used technique to detect system interaction failures. To improve the test effectiveness of CT, prioritized combinatorial testing inputs priority weights of parameter-values, and generates combinatorial test suites based on the weights. This paper proposes a method to automatically determine the weights of parameter-values by Bayesian inference using previous testing results. Using two open source projects, we evaluate the fault detection effectiveness of the proposed weighting based prioritized combinatorial testing.",,978-1-5090-6676-6978-1-5090-6677,10.1109/ICSTW.2017.73,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899090,Combinatorial testing;Pairwise testing;Priority weights;Bayesian inference;Fault detection effectiveness,Bayes methods;Fault detection;Conferences;Software testing;Software,Bayes methods;inference mechanisms;program testing,fault detection effectiveness;open source projects;combinatorial test suites;prioritized combinatorial testing;CT;Bayesian inference;combinatorial testing,,,11,,,,,,IEEE,IEEE Conferences
A Model-Based Test Case Management Approach for Integrated Sets of Domain-Specific Models,R. Pr??ll; B. Bauer,NA; NA,"2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",,2018,,,175,184,"Due to rapid improvements in the area of embedded processing hardware, the complexity of developed systems constantly increases. In order to ensure a high quality level of such systems, related quality assurance concepts have to evolve. The introduction of Model-Based Testing (MBT) approaches has shown promising results by automating and abstracting multiple activities of the software testing life cycle. Nevertheless, there is a strong need for approaches supporting scoped test models, i.e. subsets of test cases, reflecting specific test purposes driven by risk-oriented development strategies. Therefore, we developed an integrated and model-based approach supporting test case management, which incorporates the beneficial aspects of abstract development methodologies with predominant research for test case management in non-model-based scenarios. Based on a new model artifact, the integration model, tasks like cross-domain information mapping and the integration of domain-specific KPIs derived by analyses favor the subsequently applied constraint-based mechanism for test case management. Further, a prototypical implementation of these concepts within the Architecture And Analysis Framework (A3F) is elaborated and further evaluated based on representative application scenarios. A comparative view on related work leads to a conclusive statement regarding our future work.",,978-1-5386-6352-3978-1-5386-6353,10.1109/ICSTW.2018.00048,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411751,Model-Based Testing;Test Case Management;Test Selection;Test Prioritization;Test Suite Reduction;Test Model Scoping,Analytical models;Data models;Context modeling;Software;Software testing,program testing;quality assurance;software architecture;software quality,test cases;specific test purposes;risk-oriented development strategies;integrated model-based;abstract development methodologies;nonmodel-based scenarios;model artifact;integration model;cross-domain information mapping;domain-specific KPIs;subsequently applied constraint-based mechanism;Model-Based test case management approach;integrated sets;domain-specific models;embedded processing hardware;developed systems;high quality level;related quality assurance concepts;automating activities;abstracting multiple activities;software testing life cycle;scoped test models;MBT,,,18,,,,,,IEEE,IEEE Conferences
Aggregation process for implementation of application security management based on risk assessment,P. N. Anatoliy; F. K. Yuri; D. G. Vagiz; V. K. Yana; V. S. Aleksandr,"Chair of Comprehensive Information Security, Admiral Makarov State University of Maritime and Inland Shipping, St. Petersburg, Russia; Chair of Comprehensive Information Security, Admiral Makarov State University of Maritime and Inland Shipping, St. Petersburg, Russia; Chair of Comprehensive Information Security, Admiral Makarov State University of Maritime and Inland Shipping, St. Petersburg, Russia; Chair of Comprehensive Information Security, Admiral Makarov State University of Maritime and Inland Shipping, St. Petersburg, Russia; Chair of Comprehensive Information Security, Admiral Makarov State University of Maritime and Inland Shipping, St. Petersburg, Russia",2018 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus),,2018,,,98,101,This article is devoted to the review and analysis of existing methods of ensuring information security based on risk models. The strengths and weaknesses of the model are investigated on the basis of reliability theory. The article discusses potential obstacle to managing application security effectively and describes five steps for managing security. Create inventory of application and their attributes and evaluating their role in business impact (Create a profile for each application and conduction analysis of date processed in the application). Software vulnerability search (Static Analysis (ƒ??white-boxƒ?); Dynamic Analysis (ƒ??black-boxƒ?); Interactive Analysis (ƒ??glass-boxƒ?); Mobile Application Analysis); Risk assessment and prioritization of vulnerabilities (Setting priorities for applications; Setting priorities for types of vulnerabilities; Setting priorities for the development team; Changing vulnerability priorities and reassessing risks). Elimination of vulnerabilities and minimization of risks (security manager sets priorities and firmed tasks for the development team.,,978-1-5386-4340-2978-1-5386-4339-6978-1-5386-4341,10.1109/EIConRus.2018.8317039,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8317039,application security;security management;risk rating;information security,Testing;Risk management;Security;Software;Static analysis;Indexes,program diagnostics;risk management;security of data;software reliability,static analysis;dynamic analysis;interactive analysis;mobile application analysis;vulnerability priorities;software vulnerability search;business impact;reliability theory;risk models;information security;risk assessment;application security management;aggregation process,,,8,,,,,,IEEE,IEEE Conferences
An Empirical Comparison of Fixed-Strength and Mixed-Strength for Interaction Coverage Based Prioritization,R. Huang; Q. Zhang; T. Y. Chen; J. Hamlyn-Harris; D. Towey; J. Chen,"School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, China; School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, China; Department of Computer Science and Software Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; Department of Computer Science and Software Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Computer Science, University of Nottingham Ningbo China, Ningbo, China; School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, China",IEEE Access,,2018,6,,68350,68372,"Test case prioritization (TCP) plays an important role in identifying, characterizing, diagnosing, and correcting faults quickly. The TCP has been widely used to order test cases of different types, including model inputs (also called<italic>abstract test cases</italic>). Model inputs are constructed by modeling the program according to its input parameters, values, and constraints, and has been used in different testing methods, such as combinatorial interaction testing and software product line testing. The<italic>Interaction coverage-based TCP</italic>(ICTCP) uses interaction coverage information derived from the model input to order inputs. Previous studies have focused generally on the<italic>fixed-strength</italic>ICTCP, which adopts a fixed strength (<italic>i.e.</italic>, the level of parameter interactions) to support the ICTCP process. It is generally accepted that using more strengths for ICTCP,<italic>i.e.</italic>,<italic>mixed-strength</italic>ICTCP, may give better ordering than fixed-strength. To confirm whether mixed-strength is better than fixed-strength, in this paper, we report on an extensive empirical study using five real-world programs (written in C), each of which has six versions. The results of the empirical studies show that mixed-strength has better rates of interaction coverage overall than fixed-strength, but they have very similar rates of fault detection. Our results also show that fixed-strength should be used instead of the mixed-strength at the later stage of software testing. Finally, we offer some practical guidelines for testers when using interaction coverage information to prioritize model inputs, under different testing scenarios and resources.",2169-3536,,10.1109/ACCESS.2018.2879638,National Natural Science Foundation of China; Ningbo Municipal Bureau of Science and Technology; Jiangsu University; Young Backbone Teacher Cultivation Project of Jiangsu University; Jiangsu Overseas Visiting Scholar Program for the University Prominent Young & Middle-Aged Teachers and Presidents; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8523673,Test case prioritization;model input;interaction coverage;mixed-strength;fixed-strength,Testing;Fault detection;Guidelines;Computer science;Fats;File systems;Fault diagnosis,,,,,145,,,,,,IEEE,IEEE Journals & Magazines
Assessing Technical Debt in Automated Tests with CodeScene,A. Tornhill,NA,"2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",,2018,,,122,125,"Test automation promises several advantages such as shorter lead times, higher code quality, and an executable documentation of the system's behavior. However, test automation won't deliver on those promises unless the quality of the automated test code itself is maintained, and to manually inspect the evolution of thousands of tests that change on a daily basis is impractical at best. This paper investigates how CodeScene - a tool for predictive analyses and visualizations - could be used to identify technical debt in automated test code. CodeScene combines repository mining, static code analysis, and machine learning to prioritize potential code improvements based on the most likely return on investment.",,978-1-5386-6352-3978-1-5386-6353,10.1109/ICSTW.2018.00039,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411742,technical debt;test automation;code quality;repository mining;vendor tools,Conferences;Software testing,data mining;inspection;learning (artificial intelligence);program diagnostics;program testing;software quality;source code (software),CodeScene;automated test code;static code analysis;test automation;code improvements;technical debt assessment;repository mining;machine learning;predictive analyses,,,17,,,,,,IEEE,IEEE Conferences
Assessing Test Case Prioritization on Real Faults and Mutants,Q. Luo; K. Moran; D. Poshyvanyk; M. Di Penta,NA; NA; NA; NA,2018 IEEE International Conference on Software Maintenance and Evolution (ICSME),,2018,,,240,251,"Test Case Prioritization (TCP) is an important component of regression testing, allowing for earlier detection of faults or helping to reduce testing time and cost. While several TCP approaches exist in the research literature, a growing number of studies have evaluated them against synthetic software defects, called mutants. Hence, it is currently unclear to what extent TCP performance on mutants would be representative of the performance achieved on real faults. To answer this fundamental question, we conduct the first empirical study comparing the performance of TCP techniques applied to both real-world and mutation faults. The context of our study includes eight well-studied TCP approaches, 35k+ mutation faults, and 357 real-world faults from five Java systems in the Defects4J dataset. Our results indicate that the relative performance of the studied TCP techniques on mutants may not strongly correlate with performance on real faults, depending upon attributes of the subject programs. This suggests that, in certain contexts, the best performing technique on a set of mutants may not be the best technique in practice when applied to real faults. We also illustrate that these correlations vary for mutants generated by different operators depending on whether chosen operators reflect typical faults of a subject program. This highlights the importance, particularly for TCP, of developing mutation operators tailored for specific program domains.",2576-3148;1063-6773,978-1-5386-7870-1978-1-5386-7871,10.1109/ICSME.2018.00033,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530033,Test Case Prioritization;Empirical Study;TCP;Mutation Analysis;Mutation Testing;Mutants,Testing;Software;Fault detection;Genetic algorithms;Correlation;Measurement;Data mining,Java;program testing;regression analysis;software fault tolerance,studied TCP techniques;typical faults;test case prioritization;regression testing;testing time;synthetic software defects;well-studied TCP approaches;real-world faults;Defects4J dataset;TCP performance;mutation faults,,,80,,,,,,IEEE,IEEE Conferences
BP: Profiling Vulnerabilities on the Attack Surface,C. Theisen; H. Sohn; D. Tripp; L. Williams,NA; NA; NA; NA,2018 IEEE Cybersecurity Development (SecDev),,2018,,,110,119,"Security practitioners use the attack surface of software systems to prioritize areas of systems to test and analyze. To date, approaches for predicting which code artifacts are vulnerable have utilized a binary classification of code as vulnerable or not vulnerable. To better understand the strengths and weaknesses of vulnerability prediction approaches, vulnerability datasets with classification and severity data are needed. The goal of this paper is to help researchers and practitioners make security effort prioritization decisions by evaluating which classifications and severities of vulnerabilities are on an attack surface approximated using crash dump stack traces. In this work, we use crash dump stack traces to approximate the attack surface of Mozilla Firefox. We then generate a dataset of 271 vulnerable files in Firefox, classified using the Common Weakness Enumeration (CWE) system. We use these files as an oracle for the evaluation of the attack surface generated using crash data. In the Firefox vulnerability dataset, 14 different classifications of vulnerabilities appeared at least once. In our study, 85.3% of vulnerable files were on the attack surface generated using crash data. We found no difference between the severity of vulnerabilities found on the attack surface generated using crash data and vulnerabilities not occurring on the attack surface. Additionally, we discuss lessons learned during the development of this vulnerability dataset.",,978-1-5386-7662-2978-1-5386-7661-5978-1-5386-7663,10.1109/SecDev.2018.00022,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543394,security;vulnerabilities;classification;severity,Software systems;Security;Surface treatment;Predictive models;Computer bugs;Operating systems,online front-ends;pattern classification;program diagnostics;security of data,Firefox vulnerability dataset;attack surface;crash data;vulnerability prediction;vulnerable files;vulnerability profiling;software systems;code artifacts;code binary classification;severity data;security effort prioritization decisions;crash dump stack traces;Mozilla Firefox;Common Weakness Enumeration system;vulnerability classification;vulnerability severity,,,39,,,,,,IEEE,IEEE Conferences
Characterizing Defective Configuration Scripts Used for Continuous Deployment,A. Rahman; L. Williams,NA; NA,"2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)",,2018,,,34,45,"In software engineering, validation and verification (V&V) resources are limited and characterization of defective software source files can help in efficiently allocating V&V resources. Similar to software source files, defects occur in the scripts used to automatically manage configurations and software deployment infrastructure, often known as infrastructure as code (IaC) scripts. Defects in IaC scripts can have dire consequences, for example, creating large-scale system outages. Identifying the characteristics of defective IaC scripts can help in mitigating these defects by allocating V&V efforts efficiently based upon these characteristics. The objective of this paper is to help software practitioners to prioritize validation and verification efforts for infrastructure as code (IaC) scripts by identifying the characteristics of defective IaC scripts. Researchers have previously extracted text features to characterize defective software source files written in general purpose programming languages. We investigate if text features can be used to identify properties that characterize defective IaC scripts. We use two text mining techniques to extract text features from IaC scripts: the bag-of-words technique, and the term frequency-inverse document frequency (TF-IDF) technique. Using the extracted features and applying grounded theory, we characterize defective IaC scripts. We also use the text features to build defect prediction models with tuned statistical learners. We mine open source repositories from Mozilla, Openstack, and Wikimedia Commons, to construct three case studies and evaluate our methodology. We identify three properties that characterize defective IaC scripts: filesystem operations, infrastructure provisioning, and managing user accounts. Using the bag-of-word technique, we observe a median F-Measure of 0.74, 0.71, and 0.73, respectively, for Mozilla, Openstack, and Wikimedia Commons. Using the TF-IDF technique, we observe a median F-Measure of 0.72, 0.74, and 0.70, respectively, for Mozilla, Openstack, and Wikimedia Commons.",,978-1-5386-5012-7978-1-5386-5013,10.1109/ICST.2018.00014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367034,configuration as code;continuous deployment;defect;devops;infrastructure as code;puppet,Feature extraction;Software;Predictive models;Text mining;Organizations;Measurement;DSL,data mining;program verification;resource allocation;software engineering;software fault tolerance;text analysis,defective IaC scripts;defective software source files;code scripts;user account management;infrastructure provisioning;filesystem operations;term frequency-inverse document frequency;bag-of-words technique;text mining;V&V resource allocation;validation and verification resources;defective configuration scripts;text feature extraction,,,67,,,,,,IEEE,IEEE Conferences
Clustering Based Prioritization of Test Cases,P. Ramya; V. Sindhura; P. Vidya Sagar,"Department of Information Technology, VR Siddhartha Engineering College Kanuru, Vijayawada, Andhra Pradesh, India; Department of Information Technology, VR Siddhartha Engineering College Kanuru, Vijayawada, Andhra Pradesh, India; Department of Information Technology, VR Siddhartha Engineering College Kanuru, Vijayawada, Andhra Pradesh, India",2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT),,2018,,,1181,1185,"Regression testing is the procedure of retesting the product and checking whether additional faults or errors have been created in the existing one. It is vital for keeping up programming quality. But it is a costly process. By., utilizing prioritization technique cost can be diminished. Prioritization increases productiveness of regression testing and its main criteria is to build the rate of error detection. Merging requirements information into current testing practice helps the engineers to recognize the source of faults easily. In this paper a research is done on whether the requirements-based grouping methodology can enhance the viability of prioritization techniques. So., here a grouping approach is performed on given requirements and prioritization techniques based on code scope metric.",,978-1-5386-1974-2978-1-5386-1973-5978-1-5386-1975,10.1109/ICICCT.2018.8473253,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8473253,Regression testing;Test case prioritization-necessities based grouping;Code scope metric,Testing;Programming;Clustering algorithms;Conferences;Software;Information technology;Tools,pattern clustering;program testing;regression analysis,prioritization techniques;test cases;regression testing;additional faults;programming quality;prioritization technique cost;error detection;requirements-based grouping methodology,,,13,,,,,,IEEE,IEEE Conferences
Component Selection in Software Engineering - Which Attributes are the Most Important in the Decision Process?,P. Chatzipetrou; E. Al??groth; E. Papatheocharous; M. Borg; T. Gorschek; K. Wnuk,NA; NA; NA; NA; NA; NA,2018 44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),,2018,,,198,205,"Component-based software engineering is a common approach to develop and evolve contemporary software systems where different component sourcing options are available: 1)Software developed internally (in-house), 2)Software developed outsourced, 3)Commercial of the shelf software, and 4) Open Source Software. However, there is little available research on what attributes of a component are the most important ones when selecting new components. The object of the present study is to investigate what matters the most to industry practitioners during component selection. We conducted a cross-domain anonymous survey with industry practitioners involved in component selection. First, the practitioners selected the most important attributes from a list. Next, they prioritized their selection using the Hundred-Dollar ($100) test. We analyzed the results using Compositional Data Analysis. The descriptive results showed that Cost was clearly considered the most important attribute during the component selection. Other important attributes for the practitioners were: Support of the component, Longevity prediction, and Level of off-the-shelf fit to product. Next, an exploratory analysis was conducted based on the practitioners' inherent characteristics. Nonparametric tests and biplots were used. It seems that smaller organizations and more immature products focus on different attributes than bigger organizations and mature products which focus more on Cost.",,978-1-5386-7383-6978-1-5386-7384,10.1109/SEAA.2018.00039,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8498206,Component-based software engineering;Decision making;Compositional Data Analysis;Cumulative voting,Software;Companies;Software engineering;Stakeholders;Data analysis;Decision making;Statistical analysis,data analysis;object-oriented programming;outsourcing;public domain software;software development management;software engineering,component selection;software engineering;shelf software;software systems;component-based software engineering;component sourcing options;compositional data analysis;component support;longevity prediction;open source software;outsourced software;in-house software;product off-the-shelf fit level,,,31,,,,,,IEEE,IEEE Conferences
Context-Aware Patch Generation for Better Automated Program Repair,M. Wen; J. Chen; R. Wu; D. Hao; S. Cheung,NA; NA; NA; NA; NA,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),,2018,,,1,11,"The effectiveness of search-based automated program repair is limited in the number of correct patches that can be successfully generated. There are two causes of such limitation. First, the search space does not contain the correct patch. Second, the search space is huge and therefore the correct patch cannot be generated (ie correct patches are either generated after incorrect plausible ones or not generated within the time budget). To increase the likelihood of including the correct patches in the search space, we propose to work at a fine granularity in terms of AST nodes. This, however, will further enlarge the search space, increasing the challenge to find the correct patches. We address the challenge by devising a strategy to prioritize the candidate patches based on their likelihood of being correct. Specifically, we study the use of AST nodes' context information to estimate the likelihood. In this paper, we propose CapGen, a context-aware patch generation technique. The novelty which allows CapGen to produce more correct patches lies in three aspects: (1) The fine-granularity design enables it to find more correct fixing ingredients; (2) The context-aware prioritization of mutation operators enables it to constrain the search space; (3) Three context-aware models enable it to rank correct patches at high positions before incorrect plausible ones. We evaluate CapGen on Defects4J and compare it with the state-of-the-art program repair techniques. Our evaluation shows that CapGen outperforms and complements existing techniques. CapGen achieves a high precision of 84.00% and can prioritize the correct patches before 98.78% of the incorrect plausible ones.",1558-1225,978-1-4503-5638-1978-1-5386-5293,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453055,Context-Aware;Automated Program Repair;Patch Prioritization,Computer bugs;Maintenance engineering;Search problems;Explosions;Context modeling;Software;Benchmark testing,,,,,58,,,,,,IEEE,IEEE Conferences
DevOps Improvements for Reduced Cycle Times with Integrated Test Optimizations for Continuous Integration,D. Marijan; M. Liaaen; S. Sen,NA; NA; NA,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),,2018,1,,22,27,"DevOps, as a growing development practice that aims to enable faster development and efficient deployment of applications without compromising on quality, is often hampered by long cycle times. One contributing factor to long cycle times in DevOps is long build time. Automated testing in continuous integration is one of the build stages that is highly prone to long run-time due to software complexity and evolution, and inefficient due to unoptimized testing approaches. To be cost-effective, testing in continuous integration needs to use only a fast-running set of comprehensive tests that are able to ensure the level of quality needed for deployment to production. Known approaches use time-aware test selection methods to improve time-efficiency of continuous integration testing by providing optimized combinations and order of tests with respect to decreased run-time. However, focusing on time-efficiency as the sole criterion in DevOps often jeopardizes the quality of software deliveries. This paper proposes a technique that integrates fault-based and risk-based test selection and prioritization optimized for low run-time, to improve time-effectiveness of continuous integration testing, and thus reduce long cycle times in DevOps, without compromising on quality. The technique has been evaluated in testing of a large-scale configurable software in continuous integration, and has shown considerable improvement over industry practice with respect to time-efficiency.",0730-3157,978-1-5386-2667,10.1109/COMPSAC.2018.00012,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377636,"DevOps, Continuous integration testing, Test prioritization, History-based test prioritization, Risk-based test optimization, Fault-based test prioritization",Testing;Software;Optimization;Production;Fault detection;Industries;Indexes,program testing,unoptimized testing approaches;comprehensive tests;time-aware test selection methods;continuous integration testing;risk-based test selection;time-effectiveness;DevOps improvements;integrated test optimizations;automated testing,,,20,,,,,,IEEE,IEEE Conferences
Dynamic programming optimization algorithm applied in test case selection,O. Banias,"Automation and Applied Informatics Department, Politehnica University of Timisoara, Timisoara, Romania",2018 International Symposium on Electronics and Telecommunications (ISETC),,2018,,,1,4,"In this paper we propose a quadratic dynamic programming algorithm applied in software testing domain, more specific in the test case selection decision making. We addressed a specific problem in software testing: running a subset of test cases from the whole set of available test cases in a limited time frame with the goal of maximizing the chances of finding potential defects. We employed both objective methods as the dynamic programming algorithm and subjective and empiric human decision as defining the selection and prioritization criteria. The proposed solution is suited for medium to large projects where in the worst-case scenarios the memory space complexity of the proposed algorithm does not exceed the order of GBytes. The proposed optimization algorithm is presented in pseudocode along with the dynamic programming recurrence formula and potential selection criteria as currently used in the industry.",2475-7861;2475-787X,978-1-5386-5925-0978-1-5386-5923-6978-1-5386-5926,10.1109/ISETC.2018.8583984,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8583984,dynamic programming;algorithms;test case prioritization;test case selection;software testing,,,,,,14,,,,,,IEEE,IEEE Conferences
Dynamic Random Testing Strategy for Test Case Optimization in Cloud Environment,H. Pei; B. Yin; M. Xie,NA; NA; NA,2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),,2018,,,148,149,"Dynamic Random Testing (DRT) strategy employs feedback mechanism to guide the selection of test cases, which has shown to be effective in fault detection process. Cloud testing is the combination of cloud computing and software testing, in which the parallel mechanism is introduced to handle multiple test tasks simultaneously. The efficiency of DRT can be improved by combining it into cloud environment. However, it faces challenges in cloud testing as its test cases are selected sequentially, which does not consist with the characteristic of parallelism underlying cloud testing. In this paper, we present a cloud-based DRT strategy to adapt DRT in cloud testing, in which both the test case prioritization and resource allocation are considered. The results of the experiments show that the cloud-based DRT can improve the efficiency of original DRT and provide stable fault detection performance enhancement over other testing strategies.",,978-1-5386-9443-5978-1-5386-9444,10.1109/ISSREW.2018.000-9,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539185,dynamic random testing;cloud testing;test case selection;resource allocation,Cloud computing;Software algorithms;Fault detection;Software testing;Partitioning algorithms;Servers,cloud computing;fault diagnosis;parallel processing;program testing;resource allocation,cloud testing;cloud computing;software testing;cloud environment;cloud-based DRT strategy;test case prioritization;resource allocation;test case optimization;dynamic random testing;parallel mechanism;fault detection performance enhancement,,,3,,,,,,IEEE,IEEE Conferences
Embedded Platform for Gas Applications Using Hardware/Software Co-Design and RFID,A. Ait Si Ali; A. Farhat; S. Mohamad; A. Amira; F. Bensaali; M. Benammar; A. Bermak,"Faculty of Engineering and Environment, University of Northumbria, Newcastle upon Tyne, U.K.; Department of Electrical Engineering, Qatar University, Doha, Qatar; Department of Electrical and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Computer Science and Engineering, Qatar University, Doha, Qatar; Department of Electrical Engineering, Qatar University, Doha, Qatar; Department of Electrical Engineering, Qatar University, Doha, Qatar; Department of Electrical and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong",IEEE Sensors Journal,,2018,18,11,4633,4642,"This paper presents the development of a wireless low power reconfigurable self-calibrated multi-sensing platform for gas sensing applications. The proposed electronic nose (EN) system monitors gas temperatures, concentrations, and mixtures wirelessly using the radio-frequency identification (RFID) technology. The EN takes the form of a set of gas and temperature sensors and multiple pattern recognition algorithms implemented on the Zynq system on chip (SoC) platform. The gas and temperature sensors are integrated on a semi-passive RFID tag to reduce the consumed power. Various gas sensors are tested, including an in-house fabricated 4??4 SnO<sub>2</sub>based sensor and seven commercial Figaro sensors. The data is transmitted to the Zynq based processing unit using a RFID reader, where it is processed using multiple pattern recognition algorithms for dimensionality reduction and classification. Multiple algorithms are explored for optimum performance, including principal component analysis (PCA) and linear discriminant analysis (LDA) for dimensionality reduction while decision tree (DT) and k-nearest neighbors (KNN) are assessed for classification purpose. Different gases are targeted at diverse concentration, including carbon monoxide (CO), ethanol (C<sub>2</sub>H<sub>6</sub>O), carbon dioxide (CO<sub>2</sub>), propane (C<sub>3</sub>H<sub>8</sub>), ammonia (NH<sub>3</sub>), and hydrogen (H<sub>2</sub>). An accuracy of 100% is achieved in many cases with an overall accuracy above 90% in most scenarios. Finally, the hardware/software heterogeneous solution to implementation PCA, LDA, DT, and KNN on the Zynq SoC shows promising results in terms of resources usage, power consumption, and processing time.",1530-437X;1558-1748;2379-9153,,10.1109/JSEN.2018.2822711,National Priorities Research Program; Qatar National Research Fund (a member of Qatar Foundation); ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330010,RFID tag;Zynq SoC;E-Nose;real-time processing;gas sensing;temperature sensing,Gas detectors;Temperature sensors;Principal component analysis;RFID tags;Dimensionality reduction,calibration;decision trees;electronic noses;organic compounds;pattern recognition;principal component analysis;radiofrequency identification;sensor fusion;system-on-chip;temperature measurement;temperature sensors;wireless sensor networks,gas sensing applications;temperature sensors;multiple pattern recognition algorithms;electronic nose system;radiofrequency identification technology;hardware-software heterogeneous solution;gas temperature monitoring;Zynq SoC system;commercial Figaro sensors;semipassive RFID tag reader;hardware-software codesign;wireless low power reconfigurable self-calibrated multisensing platform;EN system;system on chip;Zynq based processing unit;principal component analysis;PCA;linear discriminant analysis;LDA;decision tree;DT;k-nearest neighbor;KNN,,,38,,,,,,IEEE,IEEE Journals & Magazines
FAST Approaches to Scalable Similarity-Based Test Case Prioritization,B. Miranda; E. Cruciani; R. Verdecchia; A. Bertolino,NA; NA; NA; NA,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),,2018,,,222,232,"Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in efficiency with no significant impact on effectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.",1558-1225,978-1-4503-5638-1978-1-5386-5293,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453081,locality sensitive hashing;minhashing;scalability;similarity;software testing;test case prioritization,Scalability;History;Fault detection;Big Data;Software testing;Data mining,,,,,34,,,,,,IEEE,IEEE Conferences
"Feature-Based Testing by Using Model Synthesis, Test Generation and Parameterizable Test Prioritization",M. Reider; S. Magnus; J. Krause,NA; NA; NA,"2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)",,2018,,,130,137,"An approach for feature-based testing in efficient test processes, especially for use in agile development, is presented. Methods of model synthesis, model-based test generation, as well as coverage-based and requirement-based test prioritization are linked together in order to systematically and efficiently obtain prioritized test cases. The result is a reordered test suite promising quick feedback for the test engineer during test execution. The process is highly parameterizable in regard to the selection of features to be tested and the optimization criteria for the test prioritization. Using an example from industrial automation, the results of the work are demonstrated.",,978-1-5386-6352-3978-1-5386-6353,10.1109/ICSTW.2018.00041,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411744,feature-based testing;test prioritization;model-based test generation;model synthesis,Unified modeling language;Test pattern generators;Adaptation models;Complexity theory;Tools;Conferences,program testing;software prototyping,model synthesis;parameterizable test prioritization;efficient test processes;model-based test generation;requirement-based test prioritization;test engineer;test execution;agile development;feature-based testing;coverage-based test prioritization,,,21,,,,,,IEEE,IEEE Conferences
Functional Dependency Detection for Integration Test Cases,S. Tahvili; M. Ahlberg; E. Fornander; W. Afzal; M. Saadatmand; M. Bohlin; M. Sarabi,NA; NA; NA; NA; NA; NA; NA,"2018 IEEE International Conference on Software Quality, Reliability and Security Companion (QRS-C)",,2018,,,207,214,"This paper presents a natural language processing (NLP) based approach that, given software requirements specification, allows the functional dependency detection between integration test cases. We analyze a set of internal signals to the implemented modules for detecting dependencies between requirements and thereby identifying dependencies between test cases such that: module 2 depends on module 1 if an output internal signal from module 1 enters as an input internal signal to the module 2. Consequently, all requirements (and thereby test cases) for module 2 are dependent on all the designed requirements (and test cases) for module 1. The dependency information between requirements (and thus corresponding test cases) can be utilized for test case prioritization and scheduling. We have implemented our approach as a tool and the feasibility is evaluated through an industrial use case in the railway domain at Bombardier Transportation (BT), Sweden.",,978-1-5386-7839-8978-1-5386-7840,10.1109/QRS-C.2018.00047,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8431975,Software Testing;Dependency;Software Requirement;Internal Signals;NLP;Optimization,Testing;Software;Hardware;Natural language processing;Transportation;Job shop scheduling;Tools,formal specification;natural language processing;program testing;railway engineering;railways,functional dependency detection;integration test cases;natural language processing based approach;output internal signal;input internal signal;dependency information;test case prioritization;scheduling;industrial use case;software requirements specification;railway domain;Bombardier Transportation,,1,26,,,,,,IEEE,IEEE Conferences
Graphite: A Greedy Graph-Based Technique for Regression Test Case Prioritization,M. Azizi; H. Do,NA; NA,2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),,2018,,,245,251,"To date, various test prioritization techniques have been developed, but the majority of these techniques consider a single objective that could limit the applicability of prioritization techniques by ignoring practical constraints imposed on regression testing. Multi-objective prioritization techniques try to reorder test cases so that they can optimize multiple goals that testers want to achieve. In this paper, we introduced a novel graph-based framework that maps the prioritization task to a graph traversal algorithm. To evaluate our approach, we performed an empirical study using 20 versions of four open source applications. Our results indicate that the use of the graph-based technique can improve the effectiveness and efficiency of test case prioritization technique.",,978-1-5386-9443-5978-1-5386-9444,10.1109/ISSREW.2018.00014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539203,Regression Testing,Graphite;Testing;Measurement;Feature extraction;Task analysis;Fault detection;Genetic algorithms,graph theory;greedy algorithms;program testing;regression analysis,regression testing;graph traversal algorithm;open source applications;test case prioritization technique;greedy graph-based technique;regression test case prioritization;multiobjective prioritization;Graphite,,,39,,,,,,IEEE,IEEE Conferences
How Do Software Metrics Affect Test Case Prioritization?,M. Ozawa; T. Dohi; H. Okamura,NA; NA; NA,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),,2018,1,,245,250,"In this paper we consider a statistical method to prioritize software test cases with operational profile, where the system behavior is described by a Markov reward model. Especially, we introduce software code metrics as reward parameters and apply the resulting Markov reward model to the test case prioritization problem, where our research question is set as how software code metrics affect the test case prioritization. In a numerical example with a real application software, we embed some seeding faults in advance and carry out 1,000 random test experiments. It is shown that our metrics-based test case prioritization can reduce a large amount of testing effort efficiently.",0730-3157,978-1-5386-2667,10.1109/COMPSAC.2018.00038,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377662,"test case prioritization, operational profile, software reliability, software code metrics, Markov reward model",Software;Software reliability;Unified modeling language;Markov processes;Software measurement,Markov processes;program testing;software metrics;statistical analysis,statistical method;software test cases;operational profile;software code metrics;reward parameters;test case prioritization problem;software metrics;Markov reward model;random test experiments;metric-based test case prioritization,,,25,,,,,,IEEE,IEEE Conferences
Identifying Technical Debt in Database Normalization Using Association Rule Mining,M. Albarak; M. Alrazgan; R. Bahsoon,NA; NA; NA,2018 44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA),,2018,,,437,441,"In previous work, we explored a new context of technical debt that relates to database normalization design decisions. We claimed that database normalization debts are likely to be incurred for tables below the fourth normal form. We proposed a method to prioritize the tables that should be normalized based on their impact on data quality and performance. In this study, we propose a framework to identify normalization debt items (i.e. tables below the fourth normal form) by mining the data stored in each table. Our framework makes use of association rule mining to discover functional dependencies between attributes in a table, which will help determine the current normal form of that table and reveal debt tables. To illustrate our method, we use a case study from Microsoft, AdventureWorks database. The results revealed the applicability of our framework to identify debt tables.",,978-1-5386-7383-6978-1-5386-7384,10.1109/SEAA.2018.00077,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8498244,Technical Debt;Database Normalization;Data mining,Databases;Data mining;Remuneration;Data integrity;Ice;Testing;Electronic mail,data mining;Microsoft Windows (operating systems);project management;relational databases,association rule mining;database normalization design decisions;database normalization debts;fourth normal form;data quality;debt items;reveal debt tables;AdventureWorks database;technical debt identification;Microsoft;functional dependencies,,,,,,,,,IEEE,IEEE Conferences
Industrially Applicable System Regression Test Prioritization in Production Automation,S. Ulewicz; B. Vogel-Heuser,"Institute of Automation and Information Systems, Technical University of Munich, Garching, Germany; Institute of Automation and Information Systems, Technical University of Munich, Garching, Germany",IEEE Transactions on Automation Science and Engineering,,2018,15,4,1839,1851,"When changes are performed on an automated production system (aPS), new faults can be accidentally introduced into the system, which are called regressions. A common method for finding these faults is regression testing. In most cases, this regression testing process is performed under high time pressure and onsite in a very uncomfortable environment. Until now, there has been no automated support for finding and prioritizing system test cases regarding the fully integrated aPS that are suitable for finding regressions. Thus, the testing technician has to rely on personal intuition and experience, possibly choosing an inappropriate order of test cases, finding regressions at a very late stage of the test run. Using a suitable prioritization, this iterative process of finding and fixing regressions can be streamlined and a lot of time can be saved by executing test cases likely to identify new regressions earlier. Thus, an approach is presented in this paper that uses previously acquired runtime data from past test executions and performs a change identification and impact analysis to prioritize test cases that have a high probability to unveil regressions caused by side effects of a system change. The approach was developed in cooperation with reputable industrial partners active in the field of aPS engineering, ensuring a development in line with industrial requirements. An industrial case study and an expert evaluation were performed, showing promising results.",1545-5955;1558-3783,,10.1109/TASE.2018.2810280,"Bavarian Ministry of Economic Affairs and Media, Energy, and Technology through the research program ƒ??Information Technology and Communication Technology,ƒ? Bavaria, Germany; ",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320514,Change detection algorithms;IEC 61131-3;manufacturing automation;software quality;system testing,Change detection algorithms;Software quality;IEC Standards;Manufacturing automation;System testing;Production systems,factory automation;production engineering computing;program testing;regression analysis,production automation;automated production system;regression testing process;high time pressure;testing technician;test run;suitable prioritization;test executions;system change;industrial case study;system test cases;industrially applicable system regression test prioritization;aPS,,,35,,,,,,IEEE,IEEE Journals & Magazines
Influencers of Quality Assurance in an Open Source Community,A. Alami; Y. Dittrich; A. Wasowski,NA; NA; NA,2018 IEEE/ACM 11th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE),,2018,,,61,68,"ROS (Robot Operating System) is an open source community in robotics that is developing standard robotics operating system facilities such as hardware abstraction, low-level device control, communication middleware, and a wide range of software components for robotics functionality. This paper studies the quality assurance practices of the ROS community. We use qualitative methods to understand how ideology, priorities of the community, culture, sustainability, complexity, and adaptability of the community affect the implementation of quality assurance practices. Our analysis suggests that software engineering practices require social and cultural alignment and adaptation to the community particularities to achieve seamless implementation in open source environments. This alignment should be incorporated into the design and implementation of quality assurance practices in open source communities.",2574-1837,978-1-4503-5725-8978-1-5386-6176,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445538,Open Source Software;Quality Assurance;OSS Community,Software;Testing;Software engineering;Quality assurance;Computer bugs;Standards;Sustainable development,,,,,48,,,,,,IEEE,IEEE Conferences
Integrating Weight Assignment Strategies With NSGA-II for Supporting User Preference Multiobjective Optimization,S. Wang; S. Ali; T. Yue; M. Liaaen,"Software Engineering Department, Simula Research Laboratory, Fornebu, Norway; Software Engineering Department, Simula Research Laboratory, Fornebu, Norway; Software Engineering Department, Simula Research Laboratory, Fornebu, Norway; Software Quality Assurance Group, Cisco Systems, Lysaker, Norway",IEEE Transactions on Evolutionary Computation,,2018,22,3,378,393,"Driven by the needs of several industrial projects on the applications of multiobjective search algorithms, we observed that user preferences must be properly incorporated into optimization objectives. However, existing algorithms usually treat all the objectives with equal priorities and do not provide a mechanism to reflect user preferences. To address this, we propose an extension-user-preference multiobjective optimization algorithm (UPMOA), to the most commonly applied, nondominated sorting genetic algorithm II by introducing a user preference indicator ??, based on existing weight assignment strategies [e.g., uniformly distributed weights (UDW)]. We empirically evaluated UPMOA using four industrial problems from three diverse domains (i.e., communication, maritime, and subsea oil and gas). We also performed a sensitivity analysis for UPMOA with 625 algorithm parameter settings. To further assess the performance and scalability, 103 500 artificial problems were created and evaluated representing 207 sets of user preferences. Results show that the UDW strategy with UPMOA achieves the best performance and UPMOA significantly outperformed other three multiobjective search algorithms, and has the ability to solve problems with a wide range of complexity. We also observed that different parameter settings led to the varied performance of UPMOA, thus suggesting that configuring proper parameters is highly problem-specific.",1089-778X;1089-778X;1941-0026,,10.1109/TEVC.2017.2778560,Research Council of Norway (RCN) through Certus SFI; RFF Hovedstaden through MBE-CR Project; EU COST Action through CA15140 ImAppNIO; RCN through Zen-Configurator Project; EU Horizon 2020 Project through U-Test; RFF Hovedstaden through MBE-CR Project; RCN through MBT4CPS Project; ,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8123878,Multiobjective optimization;search algorithms;user preference;weight assignment strategies,Optimization;Search problems;Fault detection;Testing;Stakeholders;Software engineering;Saturn,genetic algorithms;Pareto optimisation;search problems;sensitivity analysis,NSGA-II;multiobjective search algorithms;extension-user-preference multiobjective optimization algorithm;UPMOA;commonly applied algorithm II;nondominated sorting genetic algorithm II;user preference indicator ??;distributed weights];UDW strategy;parameter settings,,,53,,,,,,IEEE,IEEE Journals & Magazines
Investigating NLP-Based Approaches for Predicting Manual Test Case Failure,H. Hemmati; F. Sharifi,NA; NA,"2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)",,2018,,,309,319,"System-level manual acceptance testing is one of the most expensive testing activities. In manual testing, typically, a human tester is given an instruction to follow on the software. The results as ""passed"" or ""failed"" will be recorded by the tester, according to the instructions. Since this is a labourintensive task, any attempt in reducing the amount of this type of expensive testing is essential, in practice. Unfortunately, most of the existing heuristics for reducing test executions (e.g., test selection, prioritization, and reduction) are either based on source code or specification of the software under test, which are typically not being accessed during manual acceptance testing. In this paper, we propose a test case failure prediction approach for manual testing that can be used as a noncode/ specifcation-based heuristic for test selection, prioritization, and reduction. The approach uses basic Information Retrieval (IR) methods on the test case descriptions, written in natural language. The IR-based measure is based on the frequency of terms in the manual test scripts. We show that a simple linear regression model using the extracted natural language/IR-based feature together with a typical history-based feature (previous test execution results) can accurately predict the test cases' failure in new releases. We have conducted an extensive empirical study on manual test suites of 41 releases of Mozilla Firefox over three projects (Mobile, Tablet, Desktop). Our comparison of several proposed approaches for predicting failure shows that a) we can accurately predict the test case failure and b) the NLP-based feature can improve the prediction models.",,978-1-5386-5012-7978-1-5386-5013,10.1109/ICST.2018.00038,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367058,NLP;POS;TF/IDF;Neural Network;Linear Regression;Test Failure Prediction;Manual acceptance testing,Manuals;Testing;Feature extraction;Predictive models;Natural language processing;Measurement;Software,formal specification;information retrieval;natural language processing;program testing;regression analysis;software fault tolerance,system-level manual acceptance testing;test executions;test selection;test case descriptions;manual test scripts;test cases;manual test suites;test case failure prediction;NLP-based approaches;software specification;test prioritization;test reduction;Information Retrieval;natural language;linear regression model,,,31,,,,,,IEEE,IEEE Conferences
Learning to Accelerate Compiler Testing,J. Chen,NA,2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion),,2018,,,472,475,"Compilers are one of the most important software infrastructures. Compiler testing is an effective and widely-used way to assure the quality of compilers. While many compiler testing techniques have been proposed to detect compiler bugs, these techniques still suffer from the serious efficiency problem. This is because these techniques need to run a large number of randomly generated test programs on the fly through automated test-generation tools (e.g., Csmith). To accelerate compiler testing, it is desirable to schedule the execution order of the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. Since different test programs tend to trigger the same compiler bug, the ideal goal of accelerating compiler testing is to execute the test programs triggering different compiler bugs in the beginning. However, such perfect goal is hard to achieve, and thus in this work, we design four steps to approach the ideal goal through learning, in order to largely accelerate compiler testing.",2574-1934,978-1-4503-5663-3978-1-5386-6479,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449630,Compiler Testing;Test Prioritization;Machine Learning,Testing;Program processors;Computer bugs;Life estimation;Acceleration;Electromagnetic interference,,,,,,,,,,,IEEE,IEEE Conferences
MDroid+: A Mutation Testing Framework for Android,K. Moran; M. Tufano; C. Bernal-C?­rdenas; M. Linares-V?­squez; G. Bavota; C. Vendome; M. Di Penta; D. Poshyvanyk,NA; NA; NA; NA; NA; NA; NA; NA,2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion),,2018,,,33,36,"Mutation testing has shown great promise in assessing the effectiveness of test suites while exhibiting additional applications to test-case generation, selection, and prioritization. Traditional mutation testing typically utilizes a set of simple language specific source code transformations, called operators, to introduce faults. However, empirical studies have shown that for mutation testing to be most effective, these simple operators must be augmented with operators specific to the domain of the software under test. One challenging software domain for the application of mutation testing is that of mobile apps. While mobile devices and accompanying apps have become a mainstay of modern computing, the frameworks and patterns utilized in their development make testing and verification particularly difficult. As a step toward helping to measure and ensure the effectiveness of mobile testing practices, we introduce MDroid+, an automated framework for mutation testing of Android apps. MDroid+ includes 38 mutation operators from ten empirically derived types of Android faults and has been applied to generate over 8,000 mutants for more than 50 apps.",2574-1934,978-1-4503-5663-3978-1-5386-6479,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449438,Android;Mutation;MDroid+;Empirical;Apps,Androids;Humanoid robots;Testing;Software;Tools;Computer bugs;Java,,,,,24,,,,,,IEEE,IEEE Conferences
Methods and Tools for Focusing and Prioritizing the Testing Effort,D. Di Nucci,NA,2018 IEEE International Conference on Software Maintenance and Evolution (ICSME),,2018,,,722,726,"Software testing is essential for any software development process, representing an extremely expensive activity. Despite its importance recent studies showed that developers rarely test their application and most programming sessions end without any test execution. Indeed, new methods and tools able to better allocating the developers effort are needed to increment the system reliability and to reduce the testing costs. In this work we focus on three activities able to optimize testing activities, specifically, bug prediction, test case prioritization, and energy leaks detection. Indeed, despite the effort devoted in the last decades by the research community led to interesting results, we highlight some aspects that might be improved and propose empirical investigations and novel approaches. Finally, we provide a set of open issues that should be addressed by the research community in the future.",2576-3148;1063-6773,978-1-5386-7870-1978-1-5386-7871,10.1109/ICSME.2018.00089,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530092,Testing;Bug Prediction;Test Case Prioritization;Energy Efficiency,Computer bugs;Predictive models;Measurement;Testing;Software;Tools;Genetic algorithms,program debugging;program testing;software engineering,software testing;software development process;test execution;system reliability;bug prediction;test case prioritization;research community;expensive activity;programming sessions,,1,46,,,,,,IEEE,IEEE Conferences
MS-guided many-objective evolutionary optimisation for test suite minimisation,W. Zheng; X. Wu; S. Cao; J. Lin,"Northwestern Polytechnical University, People's Republic of China; Northwestern Polytechnical University, People's Republic of China; Northwestern Polytechnical University, People's Republic of China; Northwestern Polytechnical University, People's Republic of China",IET Software,,2018,12,6,547,554,"Test suite minimisation is a process that seeks to identify and then eliminate the obsolete or redundant test cases from the test suite. It is a trade-off between cost and other value criteria and is appropriate to be described as a many-objective optimisation problem. This study introduces a mutation score (MS)-guided many-objective optimisation approach, which prioritises the fault detection ability of test cases and takes MS, cost and three standard code coverage criteria as objectives for the test suite minimisation process. They use six classical evolutionary many-objective optimisation algorithms to identify efficient test suite, and select three small programs from the Software-Artefact Infrastructure Repository (SIR) and two larger program space and gzip for experimental evaluation as well as statistical analysis. The experiment results of the three small programs show non-dominated sorting genetic algorithm II (NSGA-II) with tuning was the most effective approach. However, MOEA/D-PBI and MOEA/D-WS outperform NSGA-II in the cases of two large programs. On the other hand, the test cost of the optimal test suite obtained by their proposed MS-guided many-objective optimisation approach is much lower than the one without it in most situation for both small programs and large programs.",1751-8806;1751-8814,,10.1049/iet-sen.2018.5133,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8572623,,,statistical analysis;Pareto optimisation;minimisation;genetic algorithms;program testing,statistical analysis;nondominated sorting genetic algorithm II;program space;software-artefact infrastructure repository;MS-guided many-objective optimisation approach;optimal test suite;test cost;NSGA-II;test suite minimisation process;standard code coverage criteria;mutation score-guided many-objective optimisation approach;many-objective optimisation problem;redundant test cases;obsolete test cases;MS-guided many-objective evolutionary optimisation,,,28,,,,,,IET,IET Journals & Magazines
On the Selection of Strength for Fixed-Strength Interaction Coverage Based Prioritization,R. Huang; W. Zong; T. Y. Chen; D. Towey; J. Chen; Y. Zhou; W. Sun,NA; NA; NA; NA; NA; NA; NA,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),,2018,1,,310,315,"Abstract test cases are derived by modeling the system under test, and have been widely applied in practice, such as for software product line testing and combinatorial testing. Abstract test case prioritization (ATCP) is used to prioritize abstract test cases and aims at achieving higher rates of fault detection. Many ATCP algorithms have been proposed, using different prioritization criteria and information. One ATCP approach makes use of fixed-strength level-combinations information covered by abstract test cases, and is called fixed-strength interaction coverage based prioritization (FICBP). Before using FICBP, the prioritization strength ?¯ needs to be decided. Previous studies have generally focused on ?¯ values ranging between 1 and 6. However, no study has investigated the appropriateness of such a range, nor how to assign the prioritization strength for FICBP. To answer these questions, this paper reports on an empirical study involving four real-life programs (each of which with six versions). The experimental results indicate that ?¯ should be set approximately equal to a value corresponding to half of the number of parameters, when testing resources are sufficient. Our results also show that when testing resources are limited or insufficient, either small or large ?¯ values are suggested for FICBP.",0730-3157,978-1-5386-2667,10.1109/COMPSAC.2018.00049,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377673,Software testing;abstract test case prioritization;FICBP;prioritization strength;empirical study,Testing;Fault detection;Flexible printed circuits;Computer science;Market research;Software;Software product lines,fault diagnosis;program testing,FICBP;testing resources;software product line testing;combinatorial testing;abstract test case prioritization;fixed-strength level-combinations information;prioritization strength;fixed-strength interaction coverage based prioritization;fault detection,,,23,,,,,,IEEE,IEEE Conferences
On the Suitability of a Portfolio-Based Design Improvement Approach,J. Br??uer; R. Pl??sch; C. K??rner; M. Saft,NA; NA; NA; NA,"2018 IEEE International Conference on Software Quality, Reliability and Security (QRS)",,2018,,,249,256,"The design debt metaphor tries to illustrate quality deficits in the design of a software and the impact thereof to the business value of the system. To pay off the debt, the literature offers various approaches for identifying and prioritizing these design flaws, but without proper support in aligning strategic improvement actions to the identified issues. This work addresses this challenge and examines the suitability of our proposed portfolio-based design assessment approach. Therefore, this investigation is conducted based on three case studies where the product source code was analyzed and assessed using our portfolio-based approach. As a result, the approach has proven to be able to recommend concrete and valuable design improvement actions that can be adapted to project constraints.",,978-1-5386-7757-5978-1-5386-7758,10.1109/QRS.2018.00038,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424976,software quality;technical debt;design debt;design improvement,Best practices;Indexes;Benchmark testing;Portfolios;Investment;Measurement,project management;software maintenance,portfolio-based design improvement approach;design debt metaphor;quality deficits;business value;design flaws;portfolio-based design assessment approach;concrete design improvement actions;valuable design improvement actions;strategic improvement actions;project constraints,,,27,,,,,,IEEE,IEEE Conferences
Poster: An Experimental Analysis of Fault Detection Capabilities of Covering Array Constructors,R. Huang; Y. Zhou; T. Y. Chen; D. Towey; J. Chen,NA; NA; NA; NA; NA,2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion),,2018,,,246,247,"Combinatorial Interaction Testing (CIT) aims at constructing an effective test suite, such as a Covering Array (CA), that can detect faults that are caused by the interaction of parameters. In this paper, we report on some empirical studies conducted to examine the fault detection capabilities of five popular CA constructors: ACTS, Jenny, PICT, CASA, and TCA. The experimental results indicate that Jenny has the best performance, because it achieves better fault detection than the other four constructors in many cases. Our results also indicate that CAs generated using ACTS, PICT, or CASA should be prioritized before testing.",2574-1934,978-1-4503-5663-3978-1-5386-6479,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449522,Combinatorial Interaction Testing;Covering Array;Constructor;Empirical Study;Software Testing,Fault detection;Software engineering;Tools;Software testing;Software;Generators,,,,,7,,,,,,IEEE,IEEE Conferences
Poster: Identification of Methods with Low Fault Risk,R. Niedermayr; T. R??hm; S. Wagner,NA; NA; NA,2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion),,2018,,,390,391,"Test resources are usually limited and therefore it is often not possible to completely test an application before a release. Therefore, testers need to focus their activities on the relevant code regions. In this paper, we introduce an inverse defect prediction approach to identify methods that contain hardly any faults. We applied our approach to six Java open-source projects and show that on average 31.6% of the methods of a project have a low fault risk; they contain in total, on average, only 5.8% of all faults. Furthermore, the results suggest that, unlike defect prediction, our approach can also be applied in cross-project prediction scenarios. Therefore, inverse defect prediction can help prioritize untested code areas and guide testers to increase the fault detection probability.",2574-1934,978-1-4503-5663-3978-1-5386-6479,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449591,testing;inverse defect prediction;fault risk;low fault risk methods;defect prediction,Measurement;Fault diagnosis;Testing;Data mining;Software engineering;Java;Complexity theory,,,,,,,,,,,IEEE,IEEE Conferences
Practical Test Dependency Detection,A. Gambi; J. Bell; A. Zeller,NA; NA; NA,"2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)",,2018,,,1,11,"Regression tests should consistently produce the same outcome when executed against the same version of the system under test. Recent studies, however, show a different picture: in many cases simply changing the order in which tests execute is enough to produce different test outcomes. These studies also identify the presence of dependencies between tests as one likely cause of this behavior. Test dependencies affect the quality of tests and of the correlated development activities, like regression test selection, prioritization, and parallelization, which assume that tests are independent. Therefore, developers must promptly identify and resolve problematic test dependencies. This paper presents PRADET, a novel approach for detecting problematic dependencies that is both effective and efficient. PRADET uses a systematic, data-driven process to detect problematic test dependencies significantly faster and more precisely than prior work. PRADET scales to analyze large projects with thousands of tests that existing tools cannot analyze in reasonable amount of time, and found 27 previously unknown dependencies.",,978-1-5386-5012-7978-1-5386-5013,10.1109/ICST.2018.00011,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367031,Test dependence;detection algorithm;empirical study;flaky tests;data-flow,Tools;Testing;Pollution;Computer bugs;Out of order;Minimization,program testing;regression analysis,practical test dependency detection;regression tests;regression test selection;problematic test dependencies;PRADET,,,33,,,,,,IEEE,IEEE Conferences
Prioritization of Metamorphic Relations Based on Test Case Execution Properties,M. Srinivasan,NA,2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW),,2018,,,162,165,"A test oracle is essential for software testing. In certain complex systems, it is hard to distinguish between correct and incorrect behavior. Metamorphic testing is one of the solution to solve the test oracle problem. In metamorphic testing, metamorphic relations (MRs) are derived based on the properties exhibited by the program under test (PUT). These MRs play a major role in the generation of test data for conducting MT. The effectiveness of MRs can be determined based on the ability to detect considerable faults for the given PUT. Many metamorphic relations with different fault finding capability can be used to test the PUT and it is important to identify and prioritize the MRs based on its fault finding effectiveness. In order to answer this challenge, we propose to prioritize the MRs based on the diversity in the execution path of the source and follow-up test cases of the MRs. We propose four metrics to capture different levels of diversity in the execution behavior of the test cases for each of the derived MRs. The total weight calculated for each of the MRs using the metrics is used to prioritize the MRs.",,978-1-5386-9443-5978-1-5386-9444,10.1109/ISSREW.2018.000-5,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539189,"Metamorphic Testing, Metamorphic Relations, Test case diversity, MR Prioritization",Measurement;Complexity theory;Indexes;Correlation;Tools;Software testing,program testing;software fault tolerance,metamorphic relations;test case execution properties;software testing;metamorphic testing;test oracle problem;test data;fault finding effectiveness;fault finding capability;program under test;fault detection,,,13,,,,,,IEEE,IEEE Conferences
"Prioritizing Alerts from Multiple Static Analysis Tools, Using Classification Models",L. Flynn; W. Snavely; D. Svoboda; N. VanHoudnos; R. Qin; J. Burns; D. Zubrow; R. Stoddard; G. Marce-Santurio,NA; NA; NA; NA; NA; NA; NA; NA; NA,2018 IEEE/ACM 1st International Workshop on Software Qualities and their Dependencies (SQUADE),,2018,,,13,20,"Static analysis (SA) tools examine code for flaws without executing the code, and produce warnings (""alerts"") about possible flaws. A human auditor then evaluates the validity of the purported code flaws. The effort required to manually audit all alerts and repair all confirmed code flaws is often too much for a project's budget and schedule. An alert triaging tool enables strategically prioritizing alerts for examination, and could use classifier confidence. We developed and tested classification models that predict if static analysis alerts are true or false positives, using a novel combination of multiple static analysis tools, features from the alerts, alert fusion, code base metrics, and archived audit determinations. We developed classifiers using a partition of the data, then evaluated the performance of the classifier using standard measurements, including specificity, sensitivity, and accuracy. Test results and overall data analysis show accurate classifiers were developed, and specifically using multiple SA tools increased classifier accuracy, but labeled data for many types of flaws were inadequately represented (if at all) in the archive data, resulting in poor predictive accuracy for many of those flaws.",,978-1-4503-5737-1978-1-5386-6186,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8445036,static analysis;alert;classification;rapid;accurate,Tools;Static analysis;Analytical models;Measurement;Software;Predictive models;Encoding,,,,,23,,,,,,IEEE,IEEE Conferences
Prioritizing Browser Environments for Web Application Test Execution,J. Kwon; I. Ko; G. Rothermel,NA; NA; NA,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),,2018,,,468,479,"When testing client-side web applications, it is important to consider different web-browser environments. Different properties of these environments such as web-browser types and underlying platforms may cause a web application to exhibit different types of failures. As web applications evolve, they must be regression tested across these different environments. Because there are many environments to consider this process can be expensive, resulting in delayed feedback about failures in applications. In this work, we propose six techniques for providing a developer with faster feedback on failures when regression testing web applications across different web-browser environments. Our techniques draw on methods used in test case prioritization; however, in our case we prioritize web-browser environments, based on information on recent and frequent failures. We evaluated our approach using four non-trivial and popular open-source web applications. Our results show that our techniques outperform two baseline methods, namely, no ordering and random ordering, in terms of the cost-effectiveness. The improvement rates ranged from -12.24% to 39.05% for no ordering, and from -0.04% to 45.85% for random ordering.",1558-1225,978-1-4503-5638-1978-1-5386-5293,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453107,Web application testing;Regression testing;Browser environments,Browsers;Testing;History;Schedules;Optimal scheduling;Operating systems;Production,,,,,,,,,,,IEEE,IEEE Conferences
Redefining Prioritization: Continuous Prioritization for Continuous Integration,J. Liang; S. Elbaum; G. Rothermel,NA; NA; NA,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),,2018,,,688,698,"Continuous integration (CI) development environments allow soft-ware engineers to frequently integrate and test their code. While CI environments provide advantages, they also utilize non-trivial amounts of time and resources. To address this issue, researchers have adapted techniques for test case prioritization (TCP) to CI environments. To date, however, the techniques considered have operated on test suites, and have not achieved substantial improvements. Moreover, they can be inappropriate to apply when system build costs are high. In this work we explore an alternative: prioritization of commits. We use a lightweight approach based on test suite failure and execution history that is highly efficient; our approach ""continuously"" prioritizes commits that are waiting for execution in response to the arrival of each new commit and the completion of each previously scheduled commit. We have evaluated our approach on three non-trivial CI data sets. Our results show that our approach can be more effective than prior techniques.",1558-1225,978-1-4503-5638-1978-1-5386-5293,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453137,continuous integration;regression testing;large scale testing,Software engineering,program testing;software engineering,continuous prioritization;continuous integration development environments;CI environments;nontrivial amounts;test case prioritization;test suites;test suite failure;nontrivial CI data sets;software engineers,,,,,,,,,IEEE,IEEE Conferences
Region Priority Based Adaptive 360-Degree Video Streaming Using DASH,F. Yang; J. Luo; J. Yang; Z. Xu,"Chongqing University of Posts and Telecommunications, Electronic Information and Networking Institute, Chongqing, China; Chongqing University of Posts and Telecommunications, Electronic Information and Networking Institute, Chongqing, China; Chongqing University of Posts and Telecommunications, Electronic Information and Networking Institute, Chongqing, China; Chongqing University of Posts and Telecommunications, Electronic Information and Networking Institute, Chongqing, China","2018 International Conference on Audio, Language and Image Processing (ICALIP)",,2018,,,398,405,"With the continuous improvement of Virtual Reality (VR) hardware and software facilities and development of VR streaming platform, the VR industry ushered in a period of rapid development. VR makes use of 360-degree panoramic or omnidirectional video with high resolution and high frame rate in order to create the immersive experience to the user. However those characteristics cause the big volume of 360-degree video and bandwidth intensive during transmission. Due to the limitation of the human eye vision, the user can only watch the part of the 360-degree video in a head-mounted display (HMD) at one time. Hence, streaming the VR video by the traditional video transmission method causes bandwidth waste. In view of this, this paper proposed a 360-degree video adaptive transmission method based on user viewport. Firstly, a 360-degree video region prioritization scheme is proposed based on user vision characteristics and head motion features. Different priority regions transmit 360-degree video content with different quality levels as the format of tiles. Then, based on the bandwidth estimation, buffer status and user viewport prediction, a 360-degree video adaptive transmission decision strategy is given. According to the predicted available bandwidth and viewport, the quality combination of the 360-degree video tiles to be transmitted in the near future is determined to implement adaptive transmission. The test results of the 360-degree video transmission experiment system based on the DASH standard show that the proposed 360-degree video adaptive transmission strategy can effectively reduce the bandwidth consumption on the basis of guaranteeing the user Quality of Experience(QoE).",,978-1-5386-5195-7978-1-5386-5193-3978-1-5386-5196,10.1109/ICALIP.2018.8455396,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8455396,360-degree video;adaptive delivery;regional priority;viewport;DASH,Streaming media;Bandwidth;Mathematical model;Estimation;Bit rate;Quality assessment;Kalman filters,,,,,28,,,,,,IEEE,IEEE Conferences
REMAP: Using Rule Mining and Multi-objective Search for Dynamic Test Case Prioritization,D. Pradhan; S. Wang; S. Ali; T. Yue; M. Liaaen,NA; NA; NA; NA; NA,"2018 IEEE 11th International Conference on Software Testing, Verification and Validation (ICST)",,2018,,,46,57,"Test case prioritization (TP) prioritizes test cases into an optimal order for achieving specific criteria (e.g., higher fault detection capability) as early as possible. However, the existing TP techniques usually only produce a static test case order before the execution without taking runtime test case execution results into account. In this paper, we propose an approach for black-box dynamic TP using rule mining and multi-objective search (named as REMAP). REMAP has three key components: 1) Rule Miner, which mines execution relations among test cases from historical execution data; 2) Static Prioritizer, which defines two objectives (i.e., fault detection capability (FDC) and test case reliance score (TRS)) and applies multi-objective search to prioritize test cases statically; and 3) Dynamic Executor and Prioritizer, which executes statically-prioritized test cases and dynamically updates the test case order based on the runtime test case execution results. We empirically evaluated REMAP with random search, greedy based on FDC, greedy based on FDC and TRS, static search-based prioritization, and rule-based prioritization using two industrial and three open source case studies. Results showed that REMAP significantly outperformed the other approaches for 96% of the case studies and managed to achieve on average 18% higher Average Percentage of Faults Detected (APFD).",,978-1-5386-5012-7978-1-5386-5013,10.1109/ICST.2018.00015,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367035,dynamic test case prioritization;black-box regression testing;multi-objective optimization;rule-mining;search,Testing;Fault detection;Runtime;Data mining;Search problems;Software;Conferences,data mining;fault diagnosis;program diagnostics;program testing,REMAP;rule mining;multiobjective search;dynamic test case prioritization;higher fault detection capability;static test case order;runtime test case execution results;black-box dynamic TP;test case reliance score;static search;test case prioritization;TP techniques;execution relation mining;Static Prioritizer;open source case;APFD;Average Percentage of Faults Detected;test case order,,,74,,,,,,IEEE,IEEE Conferences
Requirements Analysis Skills: How to Train Practitioners?,I. Morales-Ramirez; L. H. Alva-Martinez,NA; NA,2018 IEEE 8th International Workshop on Requirements Engineering Education and Training (REET),,2018,,,24,29,"One of the goals of any software development organization (SDO) is the assurance of a high quality software. To achieve this, it is important to perform all the software related activities, especially those of the requirements engineering (RE) phase, in the right way and ideally by experts. However, the current practice reveals that this crucial phase is commonly performed by people with limited experience in RE, indeed, some of them ignoring the basic activities. We present a training plan in order to improve practitioners' RE analysis skills. The training plan was applied to 44 practitioners working at a Mexican SDO. We developed such a plan based on the idea of considering six main dimensions that include theory, tests and mentoring sessions. The so called dimensions are: understanding the organization's domain, basic concepts of RE, requirements elicitation, requirements expression, requirements prioritization and requirements analysis. In this paper, we present what are the topics discussed in each dimension, the feedback received by the practitioners after the training and how we envision the evolution of the training plan.",2164-0297,978-1-5386-8408-5978-1-5386-8409,10.1109/REET.2018.00009,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8501281,Requirements engineering;RE analysis skills;training plan,Training;Software;Organizations;Requirements engineering;Knowledge engineering;Industries;Problem-solving,formal specification;formal verification;personnel;software development management;software engineering;software quality;systems analysis;training,software quality;Mexican SDO;requirements engineering phase;software development organization;requirements analysis skills;training plan;requirements prioritization;requirements expression;requirements elicitation,,,23,,,,,,IEEE,IEEE Conferences
SDN-Based Architecture for Providing QoS to High Performance Distributed Applications,A. T. Oliveira; B. J. C. A. Martins; M. F. Moreno; A. B. Vieira; A. T. A. Gomes; A. Ziviani,"Universidade Federal de Juiz de Fora (UFJF), Brazil; Universidade Federal de Juiz de Fora (UFJF), Brazil; Universidade Federal de Juiz de Fora (UFJF), Brazil; Universidade Federal de Juiz de Fora (UFJF), Brazil; National Laboratory for Scientific Computing (LNCC), Brazil; National Laboratory for Scientific Computing (LNCC), Brazil",2018 IEEE Symposium on Computers and Communications (ISCC),,2018,,,602,607,"The specification of quality of service (QoS) requirements in traditional networks is limited by the high administrative cost of these environments. Nevertheless, newer network paradigms, as software-defined networks (SDNs), simplify and relaxes the management of networks. In this sense, SDN can provide a simple/effective way to develop QoS provisioning. In this paper, we propose a QoS provision architecture exploiting the capabilities of SDN. Our approach allows the specification of classes of service and also negotiates the QoS requirements between applications and the SDN network controller. The SDN controller, in turn, monitors the network and adjusts its performance through resource reservation and traffic prioritization. We developed a proof-of-concept of our proposal and, our experimental results show that the additional routines present low overhead, whereas -for a given test application- we observe a reduction of up to 47% in transfer times.",1530-1346,978-1-5386-6950-1978-1-5386-6949-5978-1-5386-6951,10.1109/ISCC.2018.8538694,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8538694,"QoS, SDN, network management",Quality of service;Computer architecture;Proposals;Protocols;Streaming media;Delays;Throughput,parallel processing;quality of service;software defined networking;telecommunication traffic,SDN-based architecture;service requirements;high administrative cost;software-defined networks;QoS provisioning;QoS provision architecture;QoS requirements;SDN network controller;resource reservation;traffic prioritization;network paradigms;test application,,,16,,,,,,IEEE,IEEE Conferences
Search-Based Optimization for the Testing Resource Allocation Problem: Research Trends and Opportunities,R. Pietrantuono; S. Russo,NA; NA,2018 IEEE/ACM 11th International Workshop on Search-Based Software Testing (SBST),,2018,,,6,12,"This paper explores the usage of search-based techniques for the Testing Resource Allocation Problem (TRAP). We focus on the analysis of the literature, surveying the research proposals where search-based techniques are exploited for different formulations of the TRAP. Three dimensions are considered: the model formulation, solution, and validation. The analysis allows to derive several observations, and finally outline some new research directions towards better (namely, closer to real-world settings) modelling and solutions, highlighting the most promising areas of investigation.",,978-1-4503-5741-8978-1-5386-6267,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452803,Testing Resource Allocation;Reliability Allocation;Search-based Software Testing;Test Prioritization,Testing;Resource management;Search problems;Software reliability;Software;Optimization,optimisation;program testing;resource allocation;search problems,research trends;search-based techniques;testing resource allocation problem;TRAP;model formulation;search-based optimization,,,34,,,,,,IEEE,IEEE Conferences
Test Case Prioritization Based on Method Call Sequences,J. Chi; Y. Qu; Q. Zheng; Z. Yang; W. Jin; D. Cui; T. Liu,NA; NA; NA; NA; NA; NA; NA,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),,2018,1,,251,256,"Test case prioritization is widely used in testing with the purpose of detecting faults as early as possible. Most existing techniques exploit coverage to prioritize test cases based on the hypothesis that a test case with higher coverage is more likely to catch bugs. Statement coverage and function coverage are the two widely used coverage granularity. The former typically achieves better test case prioritization in terms of fault detection capability, while the latter is more efficient because it incurs less overhead. In this paper we argue that static information such as statement and function coverage may not be the best criteria for guiding dynamic executions. Executions that cover the same set of statements /functions can may exhibit very different behavior. Therefore, the abstraction that reduces program behavior to statement/function coverage can be too simplistic to predicate fault detection capability. We propose a new approach that exploits function call sequences to prioritize test cases. This is based on the observation that the function call sequences rather than the set of executed functions is a better indicator of program behavior. Test cases that reveal unique function call sequences may have better chance to encounter faults. We choose function instead of statement sequences due to the consideration of efficiency. We have developed and implemented a new prioritization strategy AGC (Additional Greedy method Call sequence), that exploit function call sequences. We compare AGC against existing test case prioritization techniques on eight real-world open source Java projects. Our experiments show that our approach outperforms existing techniques on large programs (but not on small programs) in terms of bug detection capability. The performance shows a growth trend when the size of program increases.",0730-3157,978-1-5386-2667,10.1109/COMPSAC.2018.00039,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377663,"software testing, test case prioritization, call behavior graph",Testing;Image edge detection;Fault detection;Software;Computer bugs;Subspace constraints;Instruments,fault diagnosis;Java;program debugging;program testing,test case prioritization techniques;statement coverage;fault detection capability;program behavior;executed functions;unique function;statement sequences;prioritization strategy AGC;function coverage;method call sequences;static information;dynamic executions;function call sequences;real-world open source Java projects;bug detection capability,,,34,,,,,,IEEE,IEEE Conferences
Test Case Prioritization for GUI Regression Testing Based on Centrality Measures,Y. Ren; B. Yin; B. Wang,NA; NA; NA,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),,2018,2,,454,459,"Regression testing has been widely used in GUI software testing. For the reason of economy, the prioritization of test cases is particularly important. However, few studies discussed test case prioritization (TCP) for GUI software. Based on GUI software features, a two-layer model is proposed to assist the test case prioritization in this paper, in which, the outer layer is an event handler tree (EHT), and the inner layer is a function call graph (FCG). Compared with the conventional methods, more source code information is used based on the two-layer model for prioritization. What is more, from a global perspective, centrality measure, a complex network viewpoint is used to highlight the importance of modified functions for specific version TCP. The experiment proved the effectiveness of this model and this method.",0730-3157,978-1-5386-2667,10.1109/COMPSAC.2018.10275,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377903,"GUI Testing, Regression Testing, Test Case Prioritization, Event Handler Tree, Complex Network",Graphical user interfaces;Software;Object oriented modeling;Complex networks;Software testing;Regression tree analysis,graphical user interfaces;program testing,GUI regression testing;event handler tree;function call graph;source code information;complex network viewpoint;TCP;FCG;EHT;two-layer model;GUI software features;test case prioritization;GUI software testing;centrality measure,,,25,,,,,,IEEE,IEEE Conferences
Test Re-Prioritization in Continuous Testing Environments,Y. Zhu; E. Shihab; P. C. Rigby,NA; NA; NA,2018 IEEE International Conference on Software Maintenance and Evolution (ICSME),,2018,,,69,79,"New changes are constantly and concurrently being made to large software systems. In modern continuous integration and deployment environments, each change requires a set of tests to be run. This volume of tests leads to multiple test requests being made simultaneously, which warrant prioritization of such requests. Previous work on test prioritization schedules queued tests at set time intervals. However, after a test has been scheduled it will never be reprioritized even if new higher risk tests arrive. Furthermore, as each test finishes, new information is available which could be used to reprioritize tests. In this work, we use the conditional failure probability among tests to reprioritize tests after each test run. This means that tests can be reprioritized hundreds of times as they wait to be run. Our approach is scalable because we do not depend on static analysis or coverage measures and simply prioritize tests based on their co-failure probability distributions. We named this approach CODYNAQ and in particular, we propose three prioritization variants called CODYNAQSINGLE, CODYNAQDOUBLE and CODYNAQFLEXI. We evaluate our approach on two data sets, CHROME and Google testing data. We find that our co-failure dynamic re-prioritization approach, CODYNAQ, outperforms the default order, FIFOBASELINE, finding the first failure and all failures for a change request by 31% and 62% faster, respectively. CODYNAQ also outperforms GOOGLETCP by finding the first failure 27% faster and all failures 62% faster.",2576-3148;1063-6773,978-1-5386-7870-1978-1-5386-7871,10.1109/ICSME.2018.00016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8530018,Regression testing;Test minimization;Dynamic test prioritization;Test dependency;Continuous testing;Continuous integration,Testing;Google;Software engineering;Computer science;Schedules;Software;Companies,program testing;scheduling,change request;continuous testing environments;software systems;multiple test requests;test prioritization schedules;set time intervals;conditional failure probability;static analysis;coverage measures;Google testing data;cofailure dynamic reprioritization approach;CODYNAQ approach;cofailure probability distributions,,,22,,,,,,IEEE,IEEE Conferences
Using Controlled Numbers of Real Faults and Mutants to Empirically Evaluate Coverage-Based Test Case Prioritization,D. Paterson; G. Kapfhammer; G. Fraser; P. McMinn,NA; NA; NA; NA,2018 IEEE/ACM 13th International Workshop on Automation of Software Test (AST),,2018,,,57,63,"Used to establish confidence in the correctness of evolving software, regression testing is an important, yet costly, task. Test case prioritization enables the rapid detection of faults during regression testing by reordering the test suite so that effective tests are run as early as is possible. However, a distinct lack of information about the regression faults found in complex real-world software forced prior experimental studies of these methods to use artificial faults called mutants. Using the Defects4J database of real faults, this paper presents the results of experiments evaluating the effectiveness of four representative test prioritization techniques. Since this paper's results show that prioritization is susceptible to high amounts of variance when only one fault is present, our experiments also control the number of real faults and mutants in the program subject to regression testing. Our overall findings are that, in comparison to mutants, real faults are harder for reordered test suites to quickly detect, suggesting that mutants are not a surrogate for real faults.",,978-1-4503-5743-2978-1-5386-6255,,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8536351,test case prioritization;regression testing;real faults;defects4j,Software;Tools;Testing;Fault detection;Measurement;Conferences;Automation,feature extraction;program testing;regression analysis;software fault tolerance,controlled numbers;mutants;regression testing;regression faults;artificial faults;coverage-based test case prioritization;representative test prioritization;software correctness;fault detection;Defects4J database,,,33,,,,,,IEEE,IEEE Conferences
Using Mutant Stubbornness to Create Minimal and Prioritized Test Sets,L. Gonzalez-Hernandez; B. Lindstr??m; J. Offutt; S. F. Andler; P. Potena; M. Bohlin,NA; NA; NA; NA; NA; NA,"2018 IEEE International Conference on Software Quality, Reliability and Security (QRS)",,2018,,,446,457,"In testing, engineers want to run the most useful tests early (prioritization). When tests are run hundreds or thousands of times, minimizing a test set can result in significant savings (minimization). This paper proposes a new analysis technique to address both the minimal test set and the test case prioritization problems. This paper precisely defines the concept of mutant stubbornness, which is the basis for our analysis technique. We empirically compare our technique with other test case minimization and prioritization techniques in terms of the size of the minimized test sets and how quickly mutants are killed. We used seven C language subjects from the Siemens Repository, specifically the test sets and the killing matrices from a previous study. We used 30 different orders for each set and ran every technique 100 times over each set. Results show that our analysis technique performed significantly better than prior techniques for creating minimal test sets and was able to establish new bounds for all cases. Also, our analysis technique killed mutants as fast or faster than prior techniques. These results indicate that our mutant stubbornness technique constructs test sets that are both minimal in size, and prioritized effectively, as well or better than other techniques.",,978-1-5386-7757-5978-1-5386-7758,10.1109/QRS.2018.00058,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8424996,Test Case Minimization;Minimal Sets;Test Case Prioritization;Mutant Stubbornness,Testing;Minimization;Redundancy;Fault detection;Software;Analytical models;Conferences,program testing,minimal test sets;prioritized test sets;test case prioritization problems;test case minimization;analysis technique;mutant stubbornness technique;C language;Siemens Repository,,,34,,,,,,IEEE,IEEE Conferences
Varying defect prediction approaches during project evolution: A preliminary investigation,S. Geremia; D. A. Tamburri,"University of Molise, Pesche (IS), Italy; TU/e - JADS, s'Hertogenbosch, The Netherlands",2018 IEEE Workshop on Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE),,2018,,,1,6,"Defect prediction approaches use various features of software product or process to prioritize testing, analysis and general quality assurance activities. Such approaches require the availability of project's historical data, making them inapplicable in early phase. To cope with this problem, researchers have proposed cross-project and even cross-company prediction models, which use training material from other projects to build the model. Despite such advances, there is limited knowledge of how, as the project evolves, it would be convenient to still keep using data from other projects, and when, instead, it might become convenient to switch towards a local prediction model. This paper empirically investigates, using historical data from four open source projects, on how the performance of various kinds of defect prediction approaches - within-project prediction, local and global cross-project prediction, and mixed (injected local cross) prediction - varies over time. Results of the study are part of a long-term investigation towards supporting the customization of defect prediction models over projects' history.",,978-1-5386-5920-5978-1-5386-5921,10.1109/MALTESQUE.2018.8368451,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368451,Defect prediction;empirical study;global prediction;local prediction,Predictive models;Measurement;Data models;Training;Switches;History;Software,program testing;quality assurance;software metrics;software quality;software reliability,defect prediction approaches;project evolution;general quality assurance activities;cross-company prediction models;local prediction model;open source projects;within-project prediction;local cross-project prediction;global cross-project prediction;mixed prediction;defect prediction models;software product,,,33,,,,,,IEEE,IEEE Conferences
Virtual Test Method for Complex and Variant-Rich Automotive Systems,A. Lauber; H. Guissouma; E. Sax,"Karlsruher Institute of Technology (KIT), Institute for Information Processing Technologies (ITIV), Karlsruhe, 76131, Germany; Karlsruher Institute of Technology (KIT), Institute for Information Processing Technologies (ITIV), Karlsruhe, 76131, Germany; Karlsruher Institute of Technology (KIT), Institute for Information Processing Technologies (ITIV), Karlsruhe, 76131, Germany",2018 IEEE International Conference on Vehicular Electronics and Safety (ICVES),,2018,,,1,7,"The fast development of embedded automotive systems in form of connected Electronic Control Units (ECUs) has led to complex development processes. Especially for safetycritical functions, the testing activities are essential to check if the designed system complies with the requirements. Nowadays, the continuous development of mobile electronic devices through software updates is performed almost on a daily basis. This trend is now starting to be observed in cyber-physical systems with higher safety priorities. In the automotive field, the rising software portion in the vehicles and the shortening technology life-cycles are accentuating the need for Software Over The Air (SOTA) updates. Despite the opportunities offered by SOTA updates, the current test processes and methods must be adapted to manage the resulting complexity throughout the life-cycle of the vehicles. Especially the typical variants abundance in automotive product lines is considered as an important challenge, which cannot be solved only by ƒ?classicalƒ? testing methods such as Hardware-In-the-Loop. In this paper, we present a testing method for variantrich systems, which can be applied for automotive software updates. It uses virtual platforms for automated delta testing to handle the abundance of system configurations. Virtual testing is introduced as a powerful tool to reduce the amount of real tests and allow efficient variants verification. As a proof of concept, an Adaptive Cruise Control (ACC) composed of two ECUs has been implemented both in real hardware and using a virtual platform. With this approach, virtual delta tests, i. e. specific test-benches targeting the differences to a basic variant, can be rapidly executed for various system configurations. To prove the feasibility of the presented test method in more complex systems, a scalability study has been conducted.",,978-1-5386-3543-8978-1-5386-3542-1978-1-5386-3544,10.1109/ICVES.2018.8519599,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8519599,,Hardware;Testing;Automotive engineering;Computational modeling;Operating systems;Adaptation models,automotive electronics;automotive engineering;electronic engineering computing;embedded systems;program testing,cyber-physical systems;automotive field;SOTA updates;life-cycle;automotive product lines;automotive software updates;virtual platform;automated delta testing;system configurations;virtual testing;ECUs;virtual delta tests;specific test-benches;virtual test method;variant-rich automotive systems;embedded automotive systems;testing activities;mobile electronic devices;adaptive cruise control;electronic control units;safety-critical functions;software over the air updates,,,25,,,,,,IEEE,IEEE Conferences
