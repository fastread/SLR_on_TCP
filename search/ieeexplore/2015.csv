"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication_Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,"Reference Count","Copyright Year","License","Online Date",Issue Date,"Meeting Date","Publisher",Document Identifier
"The research of the test case prioritization algorithm for black box testing","W. Liu; X. Wu; W. Zhang; Y. Xu","Beijing Institute of Tracking and Telecommunications Technology, Beijing, China; Beijing Institute of Tracking and Telecommunications Technology, Beijing, China; Beijing Institute of Tracking and Telecommunications Technology, Beijing, China; Beijing Institute of Tracking and Telecommunications Technology, Beijing, China","2014 IEEE 5th International Conference on Software Engineering and Service Science","","2014","","","37","40","In order to improve the efficiency of software test case execution, this paper analyzed the impact of some factors to test cases prioritization and presented two adjustment algorithms. These factors included software requirement prioritization, software failure severity and software failure probability level. Firstly, gave the definition of software requirement prioritization, the ranking methods of software failure severity and software failure probability level, the description of the relationship between test cases and test requirements. Then, presented an initial test case prioritization method based on the analysis. And then, proposed a dynamic adjustment algorithm using of software requirement prioritization and software failure probability level when software failure occurred. Experimental data show that the two test case prioritization algorithms can improve the efficiency of software testing and are helpful to find more software defects in a short period.","2327-0594;2327-0586","978-1-4799-3279-5978-1-4799-3278-8978-1-4799-3277","10.1109/ICSESS.2014.6933509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933509","software testing;test case;test case prioritization","Heuristic algorithms;Software algorithms;Algorithm design and analysis;Software systems;Software testing","program testing;software reliability","test case prioritization algorithm;black box testing;software test case execution;dynamic adjustment algorithm;software failure severity;software failure probability level;software requirement prioritization;test requirements;software testing;software defects","","","8","","","","","","IEEE","IEEE Conferences"
"History-Based Test Case Prioritization for Black Box Testing Using Ant Colony Optimization","T. Noguchi; H. Washizaki; Y. Fukazawa; A. Sato; K. Ota","NA; NA; NA; NA; NA","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","","2015","","","1","2","Test case prioritization is a technique to improve software testing. Although a lot of work has investigated test case prioritization, they focus on white box testing or regression testing. However, software testing is often outsourced to a software testing company, in which testers are rarely able to access to source code due to a contract. Herein a framework is proposed to prioritize test cases for black box testing on a new product using the test execution history collected from a similar prior product and the Ant Colony Optimization. A simulation using two actual products shows the effectiveness and practicality of our proposed framework.","2159-4848","978-1-4799-7125","10.1109/ICST.2015.7102622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102622","","Software testing;Software;Ant colony optimization;Companies;History;Fault detection","ant colony optimisation;program testing;regression analysis;source code (software);statistical testing","history-based test case prioritization;black box testing;ant colony optimization;white box testing;regression testing;software testing company;source code;test execution history","","1","6","","","","","","IEEE","IEEE Conferences"
"Prioritization of test scenarios using hybrid genetic algorithm based on UML activity diagram","X. Wang; X. Jiang; H. Shi","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","","2015","","","854","857","Software testing is an essential part of the SDLC(Software Development Life Cycle). Test scenarios are used to derive test cases for model based testing. However, with the software rapidly growing in size and complexity, the cost of software will be too high if we want to test all the test cases. So this paper presents an approach using Hybrid Genetic Algorithm(HGA) to prioritize test scenarios, which improves efficiency and reduces cost as well. The algorithm combines Genetic Algorithm(GA) with Particle Swarm Optimization(PSO) algorithm and uses Local Search Strategy to update the local and global best information of the PSO. The proposed algorithm can prioritize test scenarios so as to find a critical scenario. Finally, the proposed method is applied to several typical UML activity diagrams, and compared with the Simple Genetic Algorithm(SGA). The experimental results show that the proposed method not only prioritizes test scenarios, but also improves the efficiency, and further saves effort, time as well as cost.","2327-0594;2327-0586","978-1-4799-8353-7978-1-4799-8352-0978-1-4799-8351","10.1109/ICSESS.2015.7339189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339189","Software testing;Test scenarios prioritization;Hybrid Genetic Algorithm;UML Activity Diagram","Unified modeling language;Genetic algorithms;Testing;Software;Sociology;Statistics;Biological cells","genetic algorithms;particle swarm optimisation;program testing;search problems;software cost estimation;Unified Modeling Language","test scenarios prioritization;hybrid genetic algorithm;UML activity diagram;software testing;SDLC;software development life cycle;test cases;model based testing;software cost;HGA;particle swarm optimization;PSO algorithm;local search strategy;local best information;global best information;simple genetic algorithm;SGA","","2","8","","","","","","IEEE","IEEE Conferences"
"Optimization of test suite-test case in regression test","A. S. A. Ansari; K. K. Devadkar; P. Gharpure","Department of Computer Engineering, Sardar Patel Institute of Technology, University Of Mumbai, Mumbai, India; Department of Information Technology, Sardar Patel Institute of Technology, University Of Mumbai, Mumbai, India; Department of Computer Engineering, Sardar Patel Institute of Technology, University Of Mumbai, Mumbai, India","2013 IEEE International Conference on Computational Intelligence and Computing Research","","2013","","","1","4","Exhaustive product evolution and testing is required to ensure the quality of product. Regression testing is crucial to ensure software excellence. Regression test cases are applied to assure that new or adapted features do not relapse the existing features. As innovative features are included, new test cases are generated to assess the new functionality, and then included in the existing pool of test cases, thus escalating the cost and the time required in performing regression test and this unswervingly impacts the release, laid plan and the quality of the product. Hence there is a need to select minimal test cases that will test all the functionalities of the engineered product and it must rigorously test the functionalities that have high risk exposure. Test Suite-Test Case Refinement Technique will reduce regression test case pool size, reduce regression testing time, cost &amp; effort and also ensure the quality of the engineered product. This technique is a regression test case optimization technique that is a hybrid of Test Case Minimization based on specifications and Test Case Prioritization based on risk exposure. This approach will facilitate achievement of quality product with decreased regression testing time and cost yet uncover same amount of errors as the original test cases.","","978-1-4799-1597-2978-1-4799-1594-1978-1-4799-1595","10.1109/ICCIC.2013.6724206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724206","Test Case Optimization;Regression Test;Test Suite Prioritization;Test Suite Minimization;Risk Based Prioritization;Specification Based Selection","Minimization;Testing;Software;Conferences;Fault detection;Optimization;Software reliability","program testing;software quality","test suite-test case optimization;regression test;product quality;software excellence;test suite-test case refinement technique;test case minimization;test case prioritization;risk exposure","","1","10","","","","","","IEEE","IEEE Conferences"
"Research on optimization scheme of regression testing","S. Sun; X. Hou; C. Gao; L. Sun","School of Computer Science &amp; Engineering, Changchun University of Technology, Changchun, China; School of Computer Science &amp; Engineering, Changchun University of Technology, Changchun, China; School of Computer Science &amp; Engineering, Changchun University of Technology, Changchun, China; School of Computer Science &amp; Engineering, Changchun University of Technology, Changchun, China","2013 Ninth International Conference on Natural Computation (ICNC)","","2013","","","1628","1632","Regression testing is an important process during software development. In order to reduce costs of regression testing, research on optimization of scheme of regression testing have been done in this paper. For the purpose of reducing the number of test cases and detecting faults of programs early, this paper proposed to combine test case selection with test case prioritization. Regression testing process has been designed and optimization of testing scheme has been implemented. The criterion of test case selection is modify impact of programs, finding programs which are impacted by program modification according to modify information of programs and dependencies between programs. Test cases would be selected during test case selection. The criterion of test case prioritization is coverage ability and troubleshooting capabilities of test case. Test cases which have been selected during test case selection would be ordering in test case prioritization. Finally, the effectiveness of the new method is discussed.","2157-9563;2157-9555","978-1-4673-4714","10.1109/ICNC.2013.6818242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6818242","regression testing;test case selection;test case prioritization","Testing;Software;Optimization;Circuit faults;Convergence;Educational institutions;Computer aided software engineering","cost reduction;optimisation;program testing;regression analysis;software cost estimation;software fault tolerance;software maintenance;statistical testing","software development;regression testing;program fault detection;test case prioritization;testing scheme optimization;test case selection criterion;program modification;troubleshooting capability;coverage ability;software cost reduction;software maintenance","","1","8","","","","","","IEEE","IEEE Conferences"
"Development test case prioritization technique in regression testing based on hybrid criteria","M. R. Nejad Dobuneh; D. N. A. Jawawi; M. Ghazali; M. V. Malakooti","Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia (UTM), Skudai 81310, Johor, Malaysia; Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia (UTM), Skudai 81310, Johor, Malaysia; Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia (UTM), Skudai 81310, Johor, Malaysia; Department of Computer Engineering, Islamic Azad, University UAE Branch, UAE","2014 8th. Malaysian Software Engineering Conference (MySEC)","","2014","","","301","305","Test case prioritization techniques improve the performance of regression testing, and arrange test cases in order to obtain maximum available fault that is going to be detected in a shorter time. In this research the priority is given to test cases that are performed based on multiple criteria and hybrid criteria to enhance the effectiveness of time and cost for proposed technique. This paper shows that our prioritization technique is appropriate for regression testing environment and show that our prioritization approach frequently produces a higher average percentage of fault detection rate value, for web application. The experiments also reveal fundamental tradeoffs in the performance of time aware prioritization. In this technique some fault will be seeded in subject application, then applying the prioritization criteria on test cases to obtain the effective time of average percentage fault detection rate.","","978-1-4799-5439","10.1109/MySec.2014.6986033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986033","web application;regression testing;prioritization criteria","Fault detection;Software;Software engineering;Schedules;Organizations;Software testing","Internet;program testing;regression analysis;software fault tolerance","development test case prioritization technique;hybrid criteria;regression testing performance;test case arrangement;maximum available fault;regression testing environment;Web application;time aware prioritization;prioritization criteria;fault detection","","1","11","","","","","","IEEE","IEEE Conferences"
"Parallelized ACO algorithm for regression testing prioritization in hadoop framework","N. Elanthiraiyan; C. Arumugam","Department of Computer Science and Engineering, SSN College of Engineering, Chennai, India; Department of Computer Science and Engineering, SSN College of Engineering, Chennai, India","2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies","","2014","","","1568","1571","Regression testing is an important strategy in the software maintenance phase to produce a high quality software product. This testing ensures that the modified system code does not have an effect on the original software system. Initially, the test suite is generated for the existing software system. After the system undergoes changes the test suite contains both the original test cases and the modified test cases. Regression test prioritization method helps to separate the optimal test cases from the modified test suite. In the existing work, the multi-criteria optimization was applied for generating optimal regression test cases and it was carried out in a non-parallelized environment. The proposed solution is to extend the existing work by generating an optimized test suite using Ant Colony Optimization (ACO) technique on Hadoop Map reduce framework in a parallelized environment.","","978-1-4799-3914-5978-1-4799-3913","10.1109/ICACCCT.2014.7019371","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019371","Regression Testing;Hadoop Mapreduce;parallelization;Ant Colony Optimization;Prioritization","Software;Optimization;TV;Computer aided software engineering","optimisation;parallel processing;program testing;regression analysis;software maintenance;software quality","parallelized ACO algorithm;software maintenance phase;high quality software product;modified system code;test suite;regression test prioritization method;multicriteria optimization;optimal regression test cases;nonparallelized environment;optimized test suite;ant colony optimization technique;ACO;Hadoop Mapreduce framework;parallelized environment","","1","13","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization for Continuous Regression Testing: An Industrial Case Study","D. Marijan; A. Gotlieb; S. Sen","NA; NA; NA","2013 IEEE International Conference on Software Maintenance","","2013","","","540","543","Regression testing in continuous integration environment is bounded by tight time constraints. To satisfy time constraints and achieve testing goals, test cases must be efficiently ordered in execution. Prioritization techniques are commonly used to order test cases to reflect their importance according to one or more criteria. Reduced time to test or high fault detection rate are such important criteria. In this paper, we present a case study of a test prioritization approach ROCKET (Prioritization for Continuous Regression Testing) to improve the efficiency of continuous regression testing of industrial video conferencing software. ROCKET orders test cases based on historical failure data, test execution time and domain-specific heuristics. It uses a weighted function to compute test priority. The weights are higher if tests uncover regression faults in recent iterations of software testing and reduce time to detection of faults. The results of the study show that the test cases prioritized using ROCKET (1) provide faster fault detection, and (2) increase regression fault detection rate, revealing 30% more faults for 20% of the test suite executed, comparing to manually prioritized test cases.","1063-6773","978-0-7695-4981","10.1109/ICSM.2013.91","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676952","software testing;continuous integration;regression testing;test case prioritization;history-based prioritization","Testing;Rockets;Fault detection;Software;Manuals;Time factors;Linear programming","fault diagnosis;program testing;regression analysis;software reliability;statistical testing;teleconferencing","test case prioritization technique;industrial case study;continuous integration environment;time constraints;high fault detection rate;prioritization for continuous regression testing;ROCKET;industrial video conferencing software;historical failure data;test execution time;domain-specific heuristics;weighted function;test priority;regression faults;software testing;fault detection rate","","20","9","","","","","","IEEE","IEEE Conferences"
"History-Based Test Case Prioritization with Software Version Awareness","C. Lin; C. Chen; C. Tsai; G. M. Kapfhammer","NA; NA; NA; NA","2013 18th International Conference on Engineering of Complex Computer Systems","","2013","","","171","172","Test case prioritization techniques schedule the test cases in an order based on some specific criteria so that the tests with better fault detection capability are executed at an early position in the regression test suite. Many existing test case prioritization approaches are code-based, in which the testing of each software version is considered as an independent process. Actually, the test results of the preceding software versions may be useful for scheduling the test cases of the later software versions. Some researchers have proposed history-based approaches to address this issue, but they assumed that the immediately preceding test result provides the same reference value for prioritizing the test cases of the successive software version across the entire lifetime of the software development process. Thus, this paper describes ongoing research that studies whether the reference value of the immediately preceding test results is version-aware and proposes a test case prioritization approach based on our observations. The experimental results indicate that, in comparison to existing approaches, the presented one can schedule test cases more effectively.","","978-0-7695-5007-7978-0-7695-5007","10.1109/ICECCS.2013.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601820","Regression Testing;Test Case Prioritization","Software;Testing;Fault detection;Schedules;Software engineering;Computers;Educational institutions","fault diagnosis;program testing;regression analysis;scheduling;software reliability","history-based test case prioritization techniques;software version awareness;fault detection;regression test suite;software version testing;scheduling;software development process","","3","7","","","","","","IEEE","IEEE Conferences"
"Applying Ant Colony Optimization in software testing to generate prioritized optimal path and test data","S. Biswas; M. S. Kaiser; S. A. Mamun","Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh; Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh; Institute of Information Technology, Jahangirnagar University, Dhaka, Bangladesh","2015 International Conference on Electrical Engineering and Information Communication Technology (ICEEICT)","","2015","","","1","6","Software testing is one of the most important parts of software development lifecycle. Among various types of software testing approaches structural testing is widely used. Structural testing can be improved largely by traversing all possible code paths of the software. Genetic algorithm is the most used search technique to automate path testing and test case generation. Recently, different novel search based optimization techniques such as Ant Colony Optimization (ACO), Artificial Bee Colony (ABC), Artificial Immune System (AIS), Particle Swarm Optimization (PSO) have been applied to generate optimal path to complete software coverage. In this paper, ant colony optimization (ACO) based algorithm has been proposed which will generate set of optimal paths and prioritize the paths. Additionally, the approach generates test data sequence within the domain to use as inputs of the generated paths. Proposed approach guarantees full software coverage with minimum redundancy. This paper also demonstrates the proposed approach applying it in a program module.","","978-1-4673-6676-2978-1-4673-6675","10.1109/ICEEICT.2015.7307500","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7307500","Software testing;Ant Colony Optimization (ACO);Path testing;Test data generation;Control Flow Graph (CFG)","Testing","ant colony optimisation;artificial immune systems;program testing;software engineering","ant colony optimization;software testing;prioritized optimal path;test data;software development lifecycle;structural testing;genetic algorithm;path testing;test case generation;novel search based optimization techniques;ACO;artificial bee colony;ABC;artificial immune system;AIS;particle swarm optimization;PSO","","4","28","","","","","","IEEE","IEEE Conferences"
"Automated software testing for application maintenance by using bee colony optimization algorithms (BCO)","K. Karnavel; J. Santhoshkumar","Department of Computer science Engineering, Anand Institute of Higher Technology, India; Department of Computer Science Engineering, Anand Institute of Higher Technology, India","2013 International Conference on Information Communication and Embedded Systems (ICICES)","","2013","","","327","330","The discipline of software engineering encompasses knowledge, tools, and methods for defining software requirements, and performing software design, software construction, software testing, and software maintenance tasks. In software development practice, testing accounts for as much as 50% of total development efforts. Testing can be manual, automated, or a combination of both. Manual testing is the process of executing the application and manually interacting with the application, specifying inputs and observing outputs The objective of the project is to test the software application by suitable algorithm. The proposed system has to reduce the application testing moment, easily can find out bug and solve the bug by Regression Testing. Here by we are going to using a safe efficient regression bee colony optimization algorithm(BCO) for construct control flow graphs for a procedure or program and its modified version and use these graphs to select tests that execute changed code from the original test suite. Traceability relation of completeness checking for agent errors. In traceability relation to identifying missing elements in entire application and classifying defect in the testing moment.","","978-1-4673-5788-3978-1-4673-5786-9978-1-4673-5787","10.1109/ICICES.2013.6508211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508211","Bee colony optimization algorithm;Traceability","Software;Algorithm design and analysis;Optimization;Software testing;Computer bugs;Software algorithms","automatic testing;flow graphs;optimisation;program debugging;program diagnostics;program testing;program verification;regression analysis;software maintenance","software engineering;software requirements;software design;software construction;software maintenance tasks;software development practice;software application;regression testing;regression bee colony optimization algorithm;control flow graphs;changed code execution;test suite;completeness checking;agent errors;traceability relation completeness;missing element identification;defect classification;BCO;automated software testing","","1","3","","","","","","IEEE","IEEE Conferences"
"A hierarchical test case prioritization technique for object oriented software","Vedpal; N. Chauhan; H. Kumar","Dept. of Comp. Engg., YMCAUST, Faridabad India; Dept. of Comp. Engg., YMCAUST, Faridabad, India; Dept. of Comp. Engg., YMCAUST, Faridabad, India","2014 International Conference on Contemporary Computing and Informatics (IC3I)","","2014","","","249","254","Software reuse is the use of existing artifacts to create new software. Inheritance is the foremost technique of reuse. But the inherent complexity due to inheritance hierarchy found in object - oriented paradigm also affect testing. Every time any change occurs in the software, new test cases are added in addition to the existing test suite. So there is need to conduct effective regression testing having less number of test cases to reduce cost and time. In this paper a hierarchical test case prioritization technique is proposed wherein various factors have been considered that affect error propagation in the inheritance. In this paper prioritization of test cases take place at two levels. In the first level the classes are prioritized and in the second level the test cases of prioritized classes are ordered. To show the effectiveness of proposed technique it was applied and analyze on a C++ program.","","978-1-4799-6629","10.1109/IC3I.2014.7019794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019794","object oriented testing;test case prioritization;regression testing","Testing;Software;Fault detection;Measurement;Unified modeling language;Informatics;Object oriented modeling","C++ language;object-oriented methods;program diagnostics;program testing;regression analysis;software reusability","hierarchical test case prioritization technique;object oriented software;software reuse;regression testing;C++ program","","","11","","","","","","IEEE","IEEE Conferences"
"Test case prioritization: An approach based on modified ant colony optimization (m-ACO)","K. Solanki; Y. Singh; S. Dalal","M.D. University, Rohtak, India; M.D. University, Rohtak, India; M.D. University, Rohtak, India","2015 International Conference on Computer, Communication and Control (IC4)","","2015","","","1","6","Intense and widespread usage of software in every field of life has attracted the researchers to focus their attention on developing the methods to improve the efficiency of software testing; which is the most crucial and cost intensive phase of software development. Software testing aims to uncover the potential faults in Application Under Test by running the test cases on software code. Software code keeps on changing as the uncovered faults during testing are fixed by the developers. Regression testing is concerned with verifying the modified software code to ensure that changes in software code does not induce any undesired effect on rest of the code. Test Case Prioritization is a regression testing technique which re-schedule the execution sequence of test cases to improve the fault detection rate and enhance the performance of regression test suite. This paper focuses on proposing a novel method ""m-ACO"" for test case prioritization and the performance evaluation of the proposed method using Average Percentage of faults Detected.","","978-1-4799-8164-9978-1-4799-8163","10.1109/IC4.2015.7375627","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375627","Software Testing;Regression Testing;Test Case Prioritization;APFD","Software;Software testing;Ant colony optimization;Optimization;Algorithm design and analysis;Fault detection","ant colony optimisation;program testing;regression analysis;software fault tolerance","test case prioritization;modified ant colony optimization;m-ACO;software testing;software development;application under test;software code;regression testing technique;performance evaluation;fault detection rate","","2","21","","","","","","IEEE","IEEE Conferences"
"Test case prioritization for regression testing based on ant colony optimization","D. Gao; X. Guo; L. Zhao","Beijing Institute of Control Engineering, Beijing 100190, P.R. China; Beijing Sunwise Information Technology Co. Ltd., Beijing 100190, P.R. China; Beijing Sunwise Information Technology Co. Ltd., Beijing 100190, P.R. China","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","","2015","","","275","279","Test case prioritization technique is an efficient method to improve regression testing activities. It orders a regression test suite to execute the test cases with higher priority earlier than those with lower priority, and the problem is how to optimize the test case ordering according to some criterion. In this paper, we have proposed an algorithm which prioritizes the test cases based on ant colony optimization (ACO), considering three factors: number of faults detected, execution time and fault severity, and these three factors are used in ant colony optimization algorithm to help to reveal more severe faults at earlier stage of the regression testing process. The effectiveness of the algorithm is demonstrated using the metric named APFD, and the results of experiment show the algorithm optimizes the test case orderings effectively.","2327-0594;2327-0586","978-1-4799-8353-7978-1-4799-8352-0978-1-4799-8351","10.1109/ICSESS.2015.7339054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339054","test case prioritization;ant colony optimization;regression testing","Testing;Fault detection;Ant colony optimization;Algorithm design and analysis;Measurement;Software;Optimization","ant colony optimisation;mathematics computing;program testing;regression analysis;statistical testing","test case prioritization technique;regression testing activities;regression test suite;fault detection;execution time;fault severity;ant colony optimization algorithm;APFD;test case ordering","","5","27","","","","","","IEEE","IEEE Conferences"
"A Schema Support for Selection of Test Case Prioritization Techniques","Sujata; G. N. Purohit","NA; NA","2015 Fifth International Conference on Advanced Computing & Communication Technologies","","2015","","","547","551","Regression testing is a vast field of research. It is very costly and time consuming process but on the other hand very important process in software testing. Retest all, Test case Selection, Hybrid and Test Case Prioritization are its various techniques which are used to reduce the efforts in maintenance phase. In technical literature several techniques are present with their different and vast number of goals which can be applied in software projects despite of that they have not proven their true efficiency in the testing process. The major problem in regression testing area is to select the test case prioritization technique/s that is effective in such a way that maximum project characteristics should be cover in a minimum time span. However, consideration of this decision be carefully done so that loss of resources can be avoided in a software project. Based on the above scenario, author proposes a selection schema to support the selection of TCP techniques for a given software project aiming at maximizing the coverage of software project characteristics considering aspect of prioritization of software project characteristics. At the end, preliminary results of an experimental evaluation are presented. The purpose of this research is decision should be based on the objective knowledge of the techniques rather than considering some perception and assumptions.","2327-0659;2327-0632","978-1-4799-8488-6978-1-4799-8487","10.1109/ACCT.2015.91","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079143","regression testing;project characteristics;selection schema;test case prioritization techniques","Software;Maintenance engineering;Software algorithms;Software testing;Reliability;Estimation","program testing;project management;regression analysis;software maintenance;software management;statistical testing","hybrid test case prioritization techniques;software testing;retest all technique;test case selection technique;maintenance phase;regression testing;software project characteristics;TCP techniques","","1","12","","","","","","IEEE","IEEE Conferences"
"Process optimization for testing of domain specific languages in industrial automation","R. Kumar; V. Kumar","Siemens Technologies and Services limited, Bangalore India; Siemens Technologies and Services limited, Bangalore India","2015 World Congress on Information Technology and Computer Applications (WCITCA)","","2015","","","1","4","Software testing is essential part of software development. The goal of the testing process is not only to enhance the quality and robustness of the software but also verify the correctness and non functional requirements of the software under all working conditions. Large software has their large testing suites to verify the stability of legacy features. Testing processes have huge challenges to maintain effectiveness and efficiency of the legacy test cases. There are many different processes and techniques available all technique or processes have their advantages and limitations. A tailored testing process has been tried to utilize all technique together to improvise the benefits and efficiency of testing in the industrial automation domain. This paper tries to explain a customized approach of utilizing the available testing techniques in such a way that it enhances the effectiveness and efficiency of regression testing, thus improving the time to market of large product-line Industrial automation software.","","978-1-4673-6636-6978-1-4673-6635","10.1109/WCITCA.2015.7367051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367051","Regression Testing;Test Effectiveness;Test suite optimization;Software Testing process;Test Automation;Industial automation","Automation;Software testing;Fault detection;Software engineering;Software maintenance","factory automation;production engineering computing;program testing;program verification;software maintenance;software quality","process optimization;domain specific language testing;software testing;software development;software quality;software verification;legacy test cases;regression testing;product-line industrial automation software","","","16","","","","","","IEEE","IEEE Conferences"
"Test cases prioritization for software regression testing using analytic hierarchy process","P. Klindee; N. Prompoon","Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, Thailand; Department of Computer Engineering, Faculty of Engineering, Chulalongkorn University, Bangkok, Thailand","2015 12th International Joint Conference on Computer Science and Software Engineering (JCSSE)","","2015","","","168","173","Test cases are considered an important asset in the software testing process since they are used to detect defects in the software. In order to produce quality software covering all of the requirements, the test case designer requires much time and effort in designing test cases to cover all requirements and conditions according to the test case structure. This research proposes a method for storing and retrieving of test cases affected by software requirements changes, as well as ranking the retrieved test cases using the AHP method to improve the quality of the ranking. There are to assist system testers in identifying test cases for complete regression testing. An example application of the proposed method will also be presented.","","978-1-4799-1966-6978-1-4799-1965","10.1109/JCSSE.2015.7219790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219790","Analytical Hierarchy Process;AHP;Information Retrieval;Prioritization Technique;Regression Testing;Test Case Prioritization","Software;Indexes;Complexity theory;Analytic hierarchy process;Software testing;Computer aided software engineering","analytic hierarchy process;program testing;software quality","test cases prioritization;software regression testing process;analytic hierarchy process method;software quality;AHP method","","3","14","","","","","","IEEE","IEEE Conferences"
"A methodology for regression testing reduction and prioritization of agile releases","P. Kandil; S. Moussa; N. Badr","Department of Information Systems, Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt; Department of Information Systems, Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt; Department of Information Systems, Faculty of Computer and Information Sciences, Ain Shams University, Cairo, Egypt","2015 5th International Conference on Information & Communication Technology and Accessibility (ICTA)","","2015","","","1","6","Regression testing is the type of software testing that seeks to uncover new software bugs in existing areas of a system after changes have been made to them. The significance of regression testing have grown in the past decade with the amplified adoption of agile development methodologies, which requires the execution of regression testing at the end of each release. In this paper, we present an automated agile regression testing approach that reduces the number of test cases to be used at regression phase depending on the similarity of issues exposed from the different test cases, taking into consideration the user story coverage. It then prioritizes the reduced test cases using user-provided weighted agile parameters. The proposed approach achieves enhancement for both the reduction and prioritization of test cases for agile regression testing.","","978-1-4673-8749-1978-1-4673-8748","10.1109/ICTA.2015.7426903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426903","Regression Testing;Agile Testing;Test Cases Reduction;Test Cases Prioritization;Agile Parameters;Agile Releases","Fault detection;Mathematical model;Software;Software testing;Greedy algorithms;Complexity theory","program debugging;program testing;software prototyping","regression testing reduction;agile release prioritization;software testing;software bugs;agile development methodologies;automated agile regression testing approach;user story coverage;user-provided weighted agile parameters","","","22","","","","","","IEEE","IEEE Conferences"
"Software test cases generation based on improved particle swarm optimization","M. Huang; C. Zhang; X. Liang","Software Technology Institute, Dalian Jiaotong University, Dalian, Liaoning, 116028, China; Software Technology Institute, Dalian Jiaotong University, Dalian, Liaoning, 116028, China; Software Technology Institute, Dalian Jiaotong University, Dalian, Liaoning, 116028, China","Proceedings of 2nd International Conference on Information Technology and Electronic Commerce","","2014","","","52","55","The analysis of test case generation based on particle swarm algorithm introduced the group self-activity feedback (SAF) operator and Gauss mutation (G) changing inertia weight to improve the performance of particle swarm optimization (PSO). Using the improved algorithm in software test case, experiments show that the introduction of a single path fitness function structure and multi-path fitness calculation of parallel thinking are superior to the iteration time in single path test than standard PSO, and more efficient in multi-path test case generation.","","978-1-4799-5299-1978-1-4799-5298","10.1109/ICITEC.2014.7105570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105570","Self-active Feedback;Gauss Mutation;Particle Swarm optimization;Test Case","Particle swarm optimization;Algorithm design and analysis;Software algorithms;Software;Genetic algorithms;Sociology;Statistics","automatic test pattern generation;particle swarm optimisation;program testing","self-activity feedback;group SAF operator;Gauss mutation changing inertia weight;particle swarm optimization;improved PSO algorithm;software test case generation;single path fitness function structure;multipath fitness calculation","","3","12","","","","","","IEEE","IEEE Conferences"
"RDCC: An effective test case prioritization framework using software requirements, design and source code collaboration","M. S. Siddik; K. Sakib","Institute of Information Technology, University of Dhaka, Bangladesh; Institute of Information Technology, University of Dhaka, Bangladesh","2014 17th International Conference on Computer and Information Technology (ICCIT)","","2014","","","75","80","Test case prioritization is a technique for selecting those test cases, which are expected to outperform for determining faulty modules earlier. Different phases of software development lifecycle represent the total software from different point of views, where priority module may vary from phase to phase. However, information from different phases of software development lifecycle is rarely introduced and no one integrates that information to prioritize test cases. This paper presents an effective test case prioritization framework, which takes software requirements specification, design diagrams, source codes and test cases as input and provides a prioritized order of test cases using their collaborative information as output. Requirement IDs are split into words or terms excluding stop words to calculate requirements relativity. Design diagrams are extracted as readable XML format to calculate the degree of interconnectivity among the activities. Source codes are parsed as call graphs where vertices and edges represent classes, and calls between two classes respectively. Requirements relativity, design interconnectivity and class dependencies are multiplied by their assigned weight to calculate final weight and select test cases by mapping the customers' requirements and test cases using that weight. The proposed framework is validated with an academic project and the results show that use of collaborative information during prioritization process can be beneficial.","","978-1-4799-6288-4978-1-4799-6287","10.1109/ICCITechn.2014.7073072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7073072","Software Engineering;Software Testing;Test Case Prioritization;UML and Code Collaboration","Software;Computers;Testing;Collaboration;Information technology;XML;Unified modeling language","formal specification;graph theory;program testing;source code (software);Unified Modeling Language;XML","RDCC;test case prioritization framework;source code collaboration;test case selection;software development lifecycle phases;software priority module;test case prioritization;information integration;software requirements specification;collaborative information;requirement ID;requirements relativity;design diagram extraction;readable XML format;call graphs;graph vertices;graph edges;design interconnectivity degree;class dependencies;weight assignment;final weight calculation;customer requirements mapping;test case mapping","","2","18","","","","","","IEEE","IEEE Conferences"
"Effective test strategy for testing automotive software","S. S. Barhate","Head Test Laboratory, Hella India Automotive Pvt. Ltd., Pune, India","2015 International Conference on Industrial Instrumentation and Control (ICIC)","","2015","","","645","649","Electronic content is increasing in automobiles day by day. Functionalities like Air Bags, Anti-lock Braking, Driver Assistance Systems, Body Controllers, Passive entry Passive Start, Electronic Power Steering etc. are realized electronically with complex software. These functionalities are related to automobile system safety. Hence, safety is one of the key issues of future automobile development. Risk of system failure is high due to increasing technological complexity and software content. The software shall be tested well to arrest almost all the defects. This paper explains a test case development and execution strategy based on practical implementation. It explains how test case reduction using Taguchi method, prioritization of test execution and automation help to make testing effective. It also demonstrates how maximum defects are discovered in short time.","","978-1-4799-7165-7978-1-4799-7164","10.1109/IIC.2015.7150821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150821","Test Strategy;Test Prioritization;Taguchi Method;Test Automation","Software;Heating;Testing;Arrays;Automation;Reliability theory","automotive electronics;electronic engineering computing;program testing;road safety;Taguchi methods","automotive software testing;electronic content;automobile system safety;automobile development;system failure risk;test case development;test execution strategy;test case reduction;Taguchi method;test execution prioritization","","1","25","","","","","","IEEE","IEEE Conferences"
"Cascade: A Test Generation Tool for Combinatorial Testing","Y. Zhao; Z. Zhang; J. Yan; J. Zhang","NA; NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops","","2013","","","267","270","Combinatorial Testing (CT) is a black-box testing technique, which is used to test parameterized systems. Real applications often have some special testing requirements, especially parameter constraints. Our experimental results show that existing CT test generators cannot deal with constraints very well as the number of constraints increases. This is due to their constraint solving mechanisms, where constraint solving and parameter assignment are separated. In this paper, we propose a new CT test generation method based on constrained optimization, which combines these two together to mitigate the effect brought by the growing complexity of constraints. Our approach is implemented in a tool called Cascade, which can deal with common CT usage scenarios.","","978-0-7695-4993-4978-1-4799-1324","10.1109/ICSTW.2013.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571642","combinatorial testing;test generation;constrained optimization;parameter constraints","Optimization;Graphics;Software testing;Software;Software engineering;Generators","combinatorial mathematics;optimisation;program testing","Cascade;test generation tool;combinatorial testing;black-box testing technique;constraint solving mechanism;parameter assignment;CT test generation method;constrained optimization","","5","16","","","","","","IEEE","IEEE Conferences"
"On the Influence of Model Structure and Test Case Profile on the Prioritization of Test Cases in the Context of Model-Based Testing","J. F. S. Ouriques; E. G. Cartaxo; P. D. L. Machado","NA; NA; NA","2013 27th Brazilian Symposium on Software Engineering","","2013","","","119","128","Test case prioritization techniques aim at defining an ordering of test cases that favor the achievement of a goal during test execution, such as revealing faults as earlier as possible. A number of techniques have already been proposed and investigated in the literature and experimental results have discussed whether a technique is more successful than others. However, in the context of model-based testing, only a few attempts have been made towards either proposing or experimenting test case prioritization techniques. Moreover, a number of factors that may influence on the results obtained still need to be investigated before more general conclusions can be reached. In this paper, we present empirical studies that focus on observing the effects of two factors: the structure of the model and the profile of the test case that fails. Results show that the profile of the test case that fails may have a definite influence on the performance of the techniques investigated.","","978-0-7695-5165","10.1109/SBES.2013.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800188","Experimental Software Engineering;Software Testing;Model-Based Testing;Test Case Prioritization","Context;Unified modeling language;Testing;Fault detection;Context modeling;Software;Redundancy","fault diagnosis;program testing;software engineering","test case prioritization technique;model-based testing;test case ordering;test execution;empirical analysis;model structure;test case profile","","2","29","","","","","","IEEE","IEEE Conferences"
"A similarity-based approach for test case prioritization using historical failure data","T. B. Noor; H. Hemmati","Department of Computer Science, University of Manitoba, Winnipeg, Canada; Department of Computer Science, University of Manitoba, Winnipeg, Canada","2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)","","2015","","","58","68","Test case prioritization is a crucial element in software quality assurance in practice, specially, in the context of regression testing. Typically, test cases are prioritized in a way that they detect the potential faults earlier. The effectiveness of test cases, in terms of fault detection, is estimated using quality metrics, such as code coverage, size, and historical fault detection. Prior studies have shown that previously failing test cases are highly likely to fail again in the next releases, therefore, they are highly ranked, while prioritizing. However, in practice, a failing test case may not be exactly the same as a previously failed test case, but quite similar, e.g., when the new failing test is a slightly modified version of an old failing one to catch an undetected fault. In this paper, we define a class of metrics that estimate the test cases quality using their similarity to the previously failing test cases. We have conducted several experiments with five real world open source software systems, with real faults, to evaluate the effectiveness of these quality metrics. The results of our study show that our proposed similarity-based quality measure is significantly more effective for prioritizing test cases compared to existing test case quality measures.","","978-1-5090-0406-5978-1-5090-0405","10.1109/ISSRE.2015.7381799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381799","Test case prioritization;Test quality metric;Similarity;Execution trace;Distance function;Historical data;Code coverage;Test size","Measurement;Testing;Fault detection;History;Context;Software quality","fault diagnosis;program testing;public domain software;quality assurance;regression analysis;software metrics;software quality;statistical testing","test case prioritization;historical failure data;software quality assurance;regression testing;software quality metrics;code coverage;code size;historical fault detection;open source software systems;similarity-based quality measure","","6","32","","","","","","IEEE","IEEE Conferences"
"An effective test case prioritization method based on fault severity","Y. Wang; X. Zhao; X. Ding","College of Computer and Information Science, University of Southwest University, Beibei, Chongqing, China; College of Computer and Information Science, University of Southwest University, Beibei, Chongqing, China; College of Computer and Information Science, University of Southwest University, Beibei, Chongqing, China","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","","2015","","","737","741","In regression testing area, test case prioritization is one of the main techniques to improve the test validity and test effectiveness. However, when the test cases have the same maximum coverage rate, the random selection of the additional statement will influence the effect of sorting. For dealing with this problem, a new method is proposed to optimize test case prioritization based on fault severity, referred to as additional-statement-on-fault-severity. Facing those same maximum coverage rate, the new technique main consider a factor, fault severity, to sort test cases, it figures out the value of test case based on the algorithm of the new technique and order the sequence from high to low. Experiment results show that the improved technique of test case prioritizaftion can improve the efficiency of regression testing.","2327-0594;2327-0586","978-1-4799-8353-7978-1-4799-8352-0978-1-4799-8351","10.1109/ICSESS.2015.7339162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339162","regression testing;random selection;additional statement;test case prioritization;fault severities","Software;Testing;Computers;Fault detection;Sorting;Minimization;Computer aided software engineering","program testing;sorting","test case prioritization method;regression testing area;test validity;test effectiveness;maximum coverage rate;statement random selection;sorting effect;additional-statement-on-fault-severity","","2","14","","","","","","IEEE","IEEE Conferences"
"Test case prioritization with improved genetic algorithm","M. Ö. Cingiz; Ş. Temei; O. Kahpsız","Bilgisayar Mühendisliği Bölümü, Yıldız Teknik Üniversitesi, İstanbul, Esenler; Bilgisayar Mühendisliği Bölümü, Yıldız Teknik Üniversitesi, İstanbul, Esenler; Bilgisayar Mühendisliği Bölümü, Yıldız Teknik Üniversitesi, İstanbul, Esenler","2014 22nd Signal Processing and Communications Applications Conference (SIU)","","2014","","","1223","1226","In software development, the most time consuming phase is maintenance. Regression testing, which is a part of maintenance, deals with test case prioritization that aims to increase rate of fault detection with less number of tests. In our study, we used 100 tests and 1000 faults; however, faults are detected by tests using genetic algorithm and improved genetic algorithm. After test case prioritization, we may detect all faults with less number of tests so there'll no need to apply all 100 tests (re-test).","2165-0608","978-1-4799-4874","10.1109/SIU.2014.6830456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6830456","Anahtar Kelimeler;regression test;test case prioritization;genetic algorithms;hybrid algorithms","Software engineering;Signal processing;Conferences;Genetic algorithms;Software;Maintenance engineering;Testing","fault diagnosis;genetic algorithms;program testing;software maintenance","test case prioritization;improved genetic algorithm;software development;software maintenance;regression testing;fault detection rate improvement","","1","8","","","","","","IEEE","IEEE Conferences"
"Planning of Prioritized Test Procedures in Large Integrated Systems: Best Strategy of Defect Discovery and Early Stop of Testing Session, The Selex-ES Experience","G. Ranieri","NA","2014 IEEE International Symposium on Software Reliability Engineering Workshops","","2014","","","112","113","Large integrated systems verification can be made more efficient if driven by specific strategy, classification and prioritization of test procedures is the Selex ES way to speed up important defects discovery and cost of testing activities.","","978-1-4799-7377","10.1109/ISSREW.2014.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983814","testing strategy;priority;integrated;defect;detection","Testing;Software;Safety;Software reliability;Conferences;Planning;Companies","program testing;software fault tolerance","defect discovery strategy;testing session;Selex-ES experience;test procedure classification;test procedure prioritization","","","","","","","","","IEEE","IEEE Conferences"
"Adaptive Test-Case Prioritization Guided by Output Inspection","D. Hao; X. Zhao; L. Zhang","NA; NA; NA","2013 IEEE 37th Annual Computer Software and Applications Conference","","2013","","","169","179","Test-case prioritization is to schedule the execution order of test cases so as to maximize some objective (e.g., revealing faults early). The existing test-case prioritization approaches separate the process of test-case prioritization and the process of test-case execution by presenting the execution order of all test cases before programmers start running test cases. As the execution information of the modified program is not available for the existing test-case prioritization approaches, these approaches mainly rely on only the execution information of the previous program before modification. To address this problem, we present an adaptive test-case prioritization approach, which determines the execution order of test cases simultaneously during the execution of test cases. In particular, the adaptive approach selects test cases based on their fault-detection capability, which is calculated based on the output of selected test cases. As soon as a test case is selected and runs, the fault-detection capability of each unselected test case is modified according to the output of the latest selected test case. To evaluate the effectiveness of the proposed adaptive approach, we conducted an experimental study on eight C programs and four Java programs. The experimental results show that the adaptive approach is usually significantly better than the total test-case prioritization approach and competitive to the additional test-case prioritization approach. Moreover, the adaptive approach is better than the additional approach on some subjects (e.g, replace and schedule).","0730-3157","978-0-7695-4986","10.1109/COMPSAC.2013.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649818","software testing;regression test;test-case prioritization;adaptive approach","Schedules;Software;Java;Software testing;Equations;Adaptation models","program testing;software fault tolerance","adaptive test-case prioritization approach;output inspection;test case execution order;execution information;fault-detection capability;Java programs;C programs;total test-case prioritization approach;additional test-case prioritization approach","","4","39","","","","","","IEEE","IEEE Conferences"
"A Study in Prioritization for Higher Strength Combinatorial Testing","X. Qu; M. B. Cohen","NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops","","2013","","","285","294","Recent studies have shown that combinatorial interaction testing (CIT) is an effective fault detection technique and that early fault detection can be improved by ordering test suites by interaction based prioritization approaches. Despite research that has shown that higher strength CIT improves fault detection, there have been fewer studies that aim to understand the impact of prioritization based on higher strength criteria. In this paper, we aim to understand how interaction based prioritization techniques perform, in terms of early fault detection when we prioritize based on 3-way interactions. We generalize prior work on prioritizing using 2-way interactions to t-way prioritization, and empirically evaluate this on three open source subjects, across multiple versions of each. We examine techniques that prioritize both existing CIT suites as well as generate new ones in prioritized order. We find that early fault detection can be improved when prioritizing 3-way CIT test suites by interactions that cover more code, and to a lesser degree when generating tests in prioritized order. Our techniques that work only from the specification, appear to work best with 2-way generation.","","978-0-7695-4993-4978-1-4799-1324","10.1109/ICSTW.2013.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571645","combinatorial interaction testing;test prioritization;testing strength","Fault detection;Measurement;Software;Flexible printed circuits;Conferences;Software testing","formal specification;program testing;software fault tolerance","specification;test generation;3-way CIT test suite;prioritized order;CIT suite;t-way prioritization;2-way interaction;3-way interaction;strength criteria;interaction based prioritization approach;fault detection;combinatorial interaction testing","","10","22","","","","","","IEEE","IEEE Conferences"
"Using Dependency Structures for Prioritization of Functional Test Suites","S. Haidry; T. Miller","University of Melbourne, Parkville; University of Melbourne, Parkville","IEEE Transactions on Software Engineering","","2013","39","2","258","275","Test case prioritization is the process of ordering the execution of test cases to achieve a certain goal, such as increasing the rate of fault detection. Increasing the rate of fault detection can provide earlier feedback to system developers, improving fault fixing activity and, ultimately, software delivery. Many existing test case prioritization techniques consider that tests can be run in any order. However, due to functional dependencies that may exist between some test cases-that is, one test case must be executed before another-this is often not the case. In this paper, we present a family of test case prioritization techniques that use the dependency information from a test suite to prioritize that test suite. The nature of the techniques preserves the dependencies in the test ordering. The hypothesis of this work is that dependencies between tests are representative of interactions in the system under test, and executing complex interactions earlier is likely to increase the fault detection rate, compared to arbitrary test orderings. Empirical evaluations on six systems built toward industry use demonstrate that these techniques increase the rate of fault detection compared to the rates achieved by the untreated order, random orders, and test suites ordered using existing ""coarse-grained” techniques based on function coverage.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6189361","Software engineering;testing and debugging;test execution","Fault detection;Digital signal processing;Schedules;Software;Complexity theory;Testing;Weight measurement","program debugging;program testing;software fault tolerance;software process improvement","dependency structures;functional test suite prioritization;test case execution ordering;fault detection rate;system developers;fault fixing improvement;software delivery;functional dependencies;test case prioritization technique;dependency information;system under test;software debugging","","15","31","","","","","","IEEE","IEEE Journals & Magazines"
"Applications of Combinatorial Testing Methods for Breakthrough Results in Software Testing","M. Mehta; R. Philip","NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops","","2013","","","348","351","In last three decades, lot of research work has been done by statisticians and academicians (D. Richard Kuhn, Raghu N. Kacker, Vu Lei, Ljubomir Lazic, Sloane, Neil J. A., Madhav Phadke, D. M. Cohen, James Bach and others) in the field of Combinatorial testing (CT) and its use in software testing. However, its tremendous potential in terms of test optimization and fault detection is yet to be exploited to its fullest. This paper showcases some of the real-world examples from iGATE Corporation, where CT has been used successfully to achieve breakthrough results in software testing. The paper, through three case studies taken from different domains describes the challenges faced by testing teams in terms of test scenario complexities, schedule and budget constraints; and how CT helped resolve those challenges successfully. Our experience shows 60 to 70 percent benefits in terms of optimization of test efforts while enhancing test case coverage. In the end, the paper highlights some learnings from our experiences in implementing CT approach which may be of help to fellow practitioners.","","978-0-7695-4993-4978-1-4799-1324","10.1109/ICSTW.2013.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571652","","Software testing;NIST;Complexity theory;Schedules;Insurance","combinatorial mathematics;program testing;software fault tolerance","combinatorial testing method;software testing;test optimization;fault detection;iGATE Corporation;test scenario complexity;test schedule;budget constraint;test case coverage;CT approach","","3","9","","","","","","IEEE","IEEE Conferences"
"Test Suite Prioritization by Switching Cost","H. Wu; C. Nie; F. Kuo","NA; NA; NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops","","2014","","","133","142","Test suite generation and prioritization are two main research fields to improve testing efficiency. Combinatorial testing has been proven as an effective method to generate test suite for highly configurable software systems, while test suites are often prioritized by interaction coverage to detect faults as early as possible. However, for some cases, there exists reasonable cost of reconfiguring parameter settings when switching test cases in different orders. Surprisingly, only few studies paid attention to it. In this paper, by proposing greedy algorithms and graph-based algorithms, we aim to prioritize a given test suite to minimize its total switching cost. We also compare two different prioritization strategies by a series of experiments, and discuss the advantages of our prioritization strategy and the selection of prioritization techniques. The results show that prioritization by switching cost can improve testing efficiency and our prioritization strategy can produce a small test suite with a reasonably low switching cost. This prioritization can be used widely and help locate fault causing interactions. The results also suggest that when testing highly configurable software systems and no knowledge of fault detection can be used, prioritization by switching cost is a good choice to detect faults earlier.","","978-1-4799-5790","10.1109/ICSTW.2014.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825648","Combinatorial Testing;Test Suite Prioritization;Switching Cost","Switches;Testing;Greedy algorithms;Arrays;Software;Heuristic algorithms;Fault detection","fault location;graph theory;greedy algorithms;minimisation;program testing;software cost estimation;software fault tolerance","switching cost minimisation;test suite generation;testing efficiency improvement;combinatorial testing;configurable software systems;interaction coverage;reconfiguring parameter settings;greedy algorithms;graph-based algorithms;test suite prioritization strategy;fault location;fault detection","","4","26","","","","","","IEEE","IEEE Conferences"
"Regression Testing Prioritization Based on Model Checking for Safety-Crucial Embedded Systems","Fuzhen Sun; Yan Li","Beijing Key Lab. of Intell. Inf. Technol., Beijing Inst. of Technol., Beijing, China; Sch. of Comput. Sci. &amp; Technol., Shandong Univ. of Technol., Zibo, China","2013 Fourth International Conference on Digital Manufacturing & Automation","","2013","","","979","983","The order in which test-cases are executed has an influence on the rate at which faults can be detected. In this paper we demonstrate how test-case prioritization can be performed with the use of model-checkers. For this, different well known prioritization techniques are adapted for model-based use. New property based prioritization techniques are introduced. In addition it is shown that prioritization can be done at test-case generation time, thus removing the need for test-suite post-processing. Several experiments for safety-crucial embedded systems are used to show the validity of these ideas.","","978-0-7695-5016","10.1109/ICDMA.2013.229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598153","Test Case Prioritization;Software Testing;Model Checking;Property Testing","Manufacturing;Automation","embedded systems;fault diagnosis;program testing;program verification;regression analysis;safety-critical software","regression testing prioritization;model checking;safety-crucial embedded systems;fault detection;test-case prioritization;model-checkers;model-based use;property based prioritization techniques;test-case generation time","","","10","","","","","","IEEE","IEEE Conferences"
"A Test Data Generation Approach for Automotive Software","J. Zhou; Z. Zhang; P. Xie; J. Wang","NA; NA; NA; NA","2015 IEEE International Conference on Software Quality, Reliability and Security - Companion","","2015","","","216","220","Since automotive software contains many control flows, symbolic execution is an effective approach to generate test data for it. However, symbolic execution is cost expensive, so it is difficult to apply it directly. Moreover, parameters in automotive software are usually closely related to implement the same function, thus the constraints are dependent on other constraints in the entire path constraint set, which results in traditional optimization techniques, such as constraint independence optimization, could not be used for symbolic execution of automotive software. In this paper, we present a new test data generation approach for automotive software. In our approach, we combine symbolic execution and minimum cut to generate test data for automotive software. We firstly use minimum cut technique to divide the entire path constraint set into two constraint subsets. Then we solve the smaller subset and reuse the solution when solving the entire path constraint set. We believe this approach can not only be faster than solving the entire constraint set directly, but also increase the probability of hitting the cache.","","978-1-4673-9598","10.1109/QRS-C.2015.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322150","automotive software;test generation;symbolic execution;minimum cut","Automotive engineering;Optimization;Testing;Concrete;Software quality;Conferences","automotive engineering;optimisation;program testing","test data generation approach;automotive software;control flows;symbolic execution;path constraint set;optimization techniques;constraint independence optimization;minimum cut technique;constraint subsets","","","14","","","","","","IEEE","IEEE Conferences"
"A novel approach for test case prioritization","R. U. Maheswari; D. JeyaMala","Department of computer Applications, K.L.N. College of Engineering, Sivagangai, India; Department of computer Applications, Thiagarajar College of Engineering, Madurai, India","2013 IEEE International Conference on Computational Intelligence and Computing Research","","2013","","","1","5","The process of verifying the modified software in the maintenance phase is called Regression Testing. The size of the regression test suite and its selection process is a complex task for regression testers because of time and budget constraints. In this research paper, new Prioritization technique based on hamming distance has been proposed. It is illustrated using an example and found that it produces good results. Average Percentage of Fault Detection (APFD) metrics and charts has been used to show the effectiveness of proposed algorithm.","","978-1-4799-1597-2978-1-4799-1594-1978-1-4799-1595","10.1109/ICCIC.2013.6724209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724209","APFD;Fault based Test Suite prioritization;Hamming Distance","Fault detection;Testing;Hamming distance;Bismuth;Conferences;Measurement;Software engineering","program testing;software maintenance","test case prioritization;modified software;maintenance phase;regression testing;regression test suite;time constraint;budget constraint;prioritization technique;hamming distance;average percentage of fault detection metrics;APFD metrics","","3","12","","","","","","IEEE","IEEE Conferences"
"Test case prioritization using multi objective particle swarm optimizer","M. Tyagi; S. Malhotra","Department of CSE, U.I.E.T., Kurukshetra University, Haryana, India; Department of CSE, U.I.E.T., Kurukshetra University, Haryana, India","2014 International Conference on Signal Propagation and Computer Technology (ICSPCT 2014)","","2014","","","390","395","The goal of regression testing is to validate the modified software. Due to the resource and time constraints, it becomes necessary to develop techniques to minimize existing test suites by eliminating redundant test cases and prioritizing them. This paper proposes a 3-phase approach to solve test case prioritization. In the first phase, we are removing redundant test cases by simple matrix operation. In the second phase, test cases are selected from the test suite such that selected test cases represent the minimal set which covers all faults and also at the minimum execution time. For this phase, we are using multi objective particle swarm optimization (MOPSO) which optimizes fault coverage and execution time. In the third phase, we allocate priority to test cases obtained from the second phase. Priority is obtained by calculating the ratio of fault coverage to the execution time of test cases, higher the value of the ratio higher will be the priority and the test cases which are not selected in phase 2 are added to the test suite in sequential order. We have also performed experimental analysis based on maximum fault coverage and minimum execution time. The proposed MOPSO approach is compared with other prioritization techniques such as No Ordering, Reverse Ordering and Random Ordering by calculating Average Percentage of fault detected (APFD) for each technique and it can be concluded that the proposed approach outperformed all techniques mentioned above.","","978-1-4799-3140-8978-1-4799-3139","10.1109/ICSPCT.2014.6884931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6884931","Regression Testing;Test case selection;Test case prioritization;Multi objective Particle Swarm Optimization","","particle swarm optimisation;program testing;program verification;regression analysis;software fault tolerance","test case prioritization;multiobjective particle swarm optimizer;regression testing;modified software validation;time constraints;resource constraints;3-phase approach;matrix operation;redundant test case removal;test suite;MOPSO;fault coverage optimization;execution time optimization;average percentage of fault detected calculation;APFD calculation","","2","20","","","","","","IEEE","IEEE Conferences"
"Automatic test data generation approach using Swine Influenza Models Based Optimization (SIMBO) with dominance concepts","S. Singla; R. Kumar; D. Kumar","UIET, MDU Rohatk, India; UIET, MDU Rohatk, India; GJUS&amp;T, Hisar, India","2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)","","2015","","","2159","2164","This paper is based on Swine Influenza Models Based Optimization (SIMBO) natural computing technique which is used to automatic software test data generation. The concept of dominance relation between nodes has been used as fitness function in this paper. Finally, the results in the paper show the effectiveness of SIMBO techniques as compare to Particle Swarm Optimization (PSO) and Genetic Algorithm (GA).","","978-9-3805-4416-8978-9-3805-4415-1978-9-3805-4414","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100620","Data flow testing;Particle Swarm Optimization (PSO);Genetic algorithm (GA);Dominance Concept;Swine Influenza Models Based Optimization (SIMBO)","Genetic algorithms;Optimization;Testing;Software;Sociology;Statistics;Biological cells","automatic test software;optimisation;program testing","dominance concepts;swine influenza models based optimization;natural computing technique;automatic software test data generation;nodes dominance relation;fitness function;SIMBO techniques;particle swarm optimization;PSO;genetic algorithm;GA","","","33","","","","","","IEEE","IEEE Conferences"
"Test case prioritization techniques “an empirical study”","N. Sharma; Sujata; G. N. Purohit","Department of Computer Science and Engineering, ITM University, Gurgaon, India; Department of Computer Science and Engineering, ITM University, Gurgaon, India; Banasthali Vidhypith, India","2014 International Conference on High Performance Computing and Applications (ICHPCA)","","2014","","","1","6","Regression testing is an expensive process. A number of methodologies of regression testing are used to improve its effectiveness. These are retest all, test case selection, test case reduction and test case prioritization. Retest all technique involves re-execution of all available test suites, which are critical moreover cost effective. In order to increase efficiency, test case prioritization is being utilized for rearranging the test cases. A number of algorithms has been stated in the literature survey such as Greedy Algorithms and Metaheuristic search algorithms. A simple greedy algorithm focuses on test case prioritization but results in less efficient manner, due to which researches moved towards the additional greedy and 2-Optimal algorithms. Forthcoming metaheuristic search technique (Hill climbing and Genetic Algorithm) produces a much better solution to the test case prioritization problem. It implements stochastic optimization while dealing with problem concern. The genetic algorithm is an evolutionary algorithm which gives an exact mathematical fitness value for the test cases on which prioritization is done. This paper focuses on the comparison of metaheuristic genetic algorithm with other algorithms and proves the efficiency of genetic algorithm over the remaining ones.","","978-1-4799-5958-7978-1-4799-5957","10.1109/ICHPCA.2014.7045344","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045344","regression testing;test case prioritization;genetic algorithm;greedy algorithm;APFD","Genetics;Computers;Genetic algorithms;Testing","genetic algorithms;greedy algorithms;program testing;regression analysis;search problems;stochastic programming","regression testing;test case selection;test case reduction;test case prioritization;greedy algorith;optimal algorithm;metaheuristic search technique;stochastic optimization;evolutionary algorithm;mathematical fitness value exaction;metaheuristic genetic algorithm;software testing","","","11","","","","","","IEEE","IEEE Conferences"
"Practical Combinatorial Interaction Testing: Empirical Findings on Efficiency and Early Fault Detection","J. Petke; M. B. Cohen; M. Harman; S. Yoo","Computer Science Department, University College London, London, United Kingdom; Computer Science & Engineering Department, University of Nebraska-Lincoln, Lincoln, Nebraska, United States; Computer Science Department, University College London, London, United Kingdom; Computer Science Department, University College London, London, United Kingdom","IEEE Transactions on Software Engineering","","2015","41","9","901","924","Combinatorial interaction testing (CIT) is important because it tests the interactions between the many features and parameters that make up the configuration space of software systems. Simulated Annealing (SA) and Greedy Algorithms have been widely used to find CIT test suites. From the literature, there is a widely-held belief that SA is slower, but produces more effective tests suites than Greedy and that SA cannot scale to higher strength coverage. We evaluated both algorithms on seven real-world subjects for the well-studied two-way up to the rarely-studied six-way interaction strengths. Our findings present evidence to challenge this current orthodoxy: real-world constraints allow SA to achieve higher strengths. Furthermore, there was no evidence that Greedy was less effective (in terms of time to fault revelation) compared to SA; the results for the greedy algorithm are actually slightly superior. However, the results are critically dependent on the approach adopted to constraint handling. Moreover, we have also evaluated a genetic algorithm for constrained CIT test suite generation. This is the first time strengths higher than 3 and constraint handling have been used to evaluate GA. Our results show that GA is competitive only for pairwise testing for subjects with a small number of constraints.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2421279","National Science Foundation; Air Force Office of Scientific Research; UK Engineering and Physical Sciences Research Council (EPSRC); DAASE: Dynamic Adaptive Automated Software Engineering; GISMO: Genetic Improvement of Software for Multiple Objectives; CREST: Centre for Research on Evolution, Search and Testing; DAASE; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081752","Combinatorial Interaction Testing;Prioritisation;Empirical Studies;Software Testing;Combinatorial interaction testing;prioritisation;empirical studies;software testing","Testing;Simulated annealing;Genetic algorithms;Fault detection;Greedy algorithms;Turning;Flexible printed circuits","genetic algorithms;greedy algorithms;program testing;simulated annealing;software fault tolerance","combinatorial interaction testing;early fault detection;software system configuration space;simulated annealing;SA;greedy algorithm;CIT test suite generation;constraint handling;pairwise testing;genetic algorithm","","28","37","","","","","","IEEE","IEEE Journals & Magazines"
"Approach for Test Profile Optimization in Dynamic Random Testing","Y. Li; B. Yin; J. Lv; K. Cai","NA; NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","3","","466","471","Dynamic Random Testing (DRT) is a feedback-based software testing strategy, which has been proved to be more effective than the traditional Random Testing (RT) and Random-Partition Testing (RPT) strategies. The major advantage of DRT is that the test profile is dynamically adjusted based on the previous test data. Since the frequency and range of the profile adjustment are fixed during the testing process, DRT might not react in time to the changes of the defect detection rates. In order to overcome these shortcomings, an approach for the test profile optimization in DRT, denoted as O-DRT, is proposed in this paper. In O-DRT, the test profile adjustment contains two parts. In addition to the original adjustment in DRT, O-DRT will change the test profile to a theoretically optimal one when the pre-defined criterion is satisfied. The theoretically optimal test profile is calculated based on an optimization goal of both maximizing the overall defect detection rate and minimizing its variance. Experiments on five real-life software subjects are conducted to validate the effectiveness of O-DRT. Experimental results demonstrate that O-DRT outperforms RPT and DRT in terms of the number of test cases required to detect and remove a given number of defects.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273405","software cybernetics;random-partation testing;dynamic random testing;test profile optimization","Testing;Linear programming;Optimization;Estimation;Software;Bayes methods;History","dynamic testing;optimisation;program testing","test profile optimization;dynamic random testing;feedback-based software testing;random-partition testing;RPT;O-DRT","","2","20","","","","","","IEEE","IEEE Conferences"
"A Clustering-Bayesian Network Based Approach for Test Case Prioritization","X. Zhao; Z. Wang; X. Fan; Z. Wang","NA; NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","3","","542","547","Test case prioritization can effectively reduce the cost of regression testing by executing test cases with respect to their contributions to testing goals. Previous research has proved that the Bayesian Networks based technique which uses source code change information, software quality metrics and test coverage data has better performance than those methods merely depending on only one of the items above. Although the former Bayesian Networks based Test Case Prioritization (BNTCP) focusing on assessing the fault detection capability of each test case can utilize all three items above, it still has a deficiency that ignores the similarity between test cases. For mitigating this problem, this paper proposes a hybrid regression test case prioritization technique which aims to achieve better prioritization by incorporating code coverage based clustering approach with BNTCP to depress the impact of those similar test cases having common code coverage. Experiments on two Java projects with mutation faults and one Java project with hand-seeded faults have been conducted to evaluate the fault detection performance of the proposed approach against Additional Greedy approach, Bayesian Networks based approach (BNTCP), Bayesian Networks based approach with feedback (BNA) and code coverage based clustering approach. The experimental results showed that the proposed approach is promising.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273420","Regression testing; Test case prioritization (TCP); Clustering; Bayesian Network (BN)","Fault detection;Measurement;Testing;Bayes methods;Software quality;Java;Clustering algorithms","belief networks;fault diagnosis;Java;program testing;regression analysis;software metrics;software quality;source code (software)","clustering-Bayesian network based approach;regression testing;source code change information;software quality metrics;test coverage data;Bayesian networks based test case prioritization;BNTCP;fault detection capability;hybrid regression test case prioritization technique;code coverage based clustering approach;Java project;fault detection performance;Bayesian networks based approach;BNA","","1","22","","","","","","IEEE","IEEE Conferences"
"On the Fly Test Suite Optimization with FuzzyOptimizer","A. A. Haider; A. Nadeem; S. Rafiq","NA; NA; NA","2013 11th International Conference on Frontiers of Information Technology","","2013","","","101","106","There occur frequent requirement changes in software systems even after the software has been developed. Regression testing is continuously performed to identify the undesired affects of these requirement changes on already tested system. Test suites grow enormously with these changes due to addition of new test cases for enhanced functionality. Optimization of test suite to perform regression testing within the budgetary and time restrictions is ultimate choice for a tester because ""Retest all"" test suite is un-economical and is not suitable choice. Test suite optimization can be either static or on the fly. With on the fly optimization, optimal suite keeps on changing with the requirement changes. On the fly optimization of test suite is preferable option for regression testing. Presently, static test suite optimization approaches exist. We have proposed an application specific, on the fly optimization approach for test suite optimization problem. We have implemented our approach on an academic testing problem. We use fuzzy logic to optimize the test suite with multiple optimization objectives. Our approach has been successful to generate on the fly optimized test suites for changing requirements. In future, we will implement this approach on considerably large sized testing problems.","","978-1-4799-2503-2978-1-4799-2293","10.1109/FIT.2013.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717234","regression testing;regression suite;test suite optimization;fuzzy logic;multi-objective optimization;computational intelligence","Optimization;Testing;Fuzzy logic;Equations;Particle swarm optimization;Databases;Mathematical model","fuzzy logic;optimisation;program testing;regression analysis","on the fly test suite optimization;FuzzyOptimizer;software system;regression testing;static test suite optimization;academic testing problem;fuzzy logic","","3","30","","","","","","IEEE","IEEE Conferences"
"Test case analytics: Mining test case traces to improve risk-driven testing","T. B. Noor; H. Hemmati","Department of Computer Science University of Manitoba Winnipeg, Canada; Department of Computer Science University of Manitoba Winnipeg, Canada","2015 IEEE 1st International Workshop on Software Analytics (SWAN)","","2015","","","13","16","In risk-driven testing, test cases are generated and/or prioritized based on different risk measures. For example, the most basic risk measure would analyze the history of the software and assigns higher risk to the test cases that used to detect bugs in the past. However, in practice, a test case may not be exactly the same as a previously failed test, but quite similar. In this study, we define a new risk measure that assigns a risk factor to a test case, if it is similar to a failing test case from history. The similarity is defined based on the execution traces of the test cases, where we define each test case as a sequence of method calls. We have evaluated our new risk measure by comparing it to a traditional risk measure (where the risk measure would be increased only if the very same test case, not a similar one, failed in the past). The results of our study, in the context of test case prioritization, on two open source projects show that our new risk measure is by far more effective in identifying failing test cases compared to the traditional risk measure.","","978-1-4673-6923","10.1109/SWAN.2015.7070482","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070482","Execution trace; Testing; Bug; Risk-driventesting; Similarity; Test case prioritization","Testing;History;Current measurement;Context;Software;Computer bugs;Databases","program diagnostics;program testing;public domain software;risk management;software management","test case analytics;test case trace mining;risk-driven testing;software history;risk factor;open source projects","","7","10","","","","","","IEEE","IEEE Conferences"
"A Comparison of Test Case Prioritization Criteria for Software Product Lines","A. B. Sánchez; S. Segura; A. Ruiz-Cortés","NA; NA; NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation","","2014","","","41","50","Software Product Line (SPL) testing is challenging due to the potentially huge number of derivable products. To alleviate this problem, numerous contributions have been proposed to reduce the number of products to be tested while still having a good coverage. However, not much attention has been paid to the order in which the products are tested. Test case prioritization techniques reorder test cases to meet a certain performance goal. For instance, testers may wish to order their test cases in order to detect faults as soon as possible, which would translate in faster feedback and earlier fault correction. In this paper, we explore the applicability of test case prioritization techniques to SPL testing. We propose five different prioritization criteria based on common metrics of feature models and we compare their effectiveness in increasing the rate of early fault detection, i.e. a measure of how quickly faults are detected. The results show that different orderings of the same SPL suite may lead to significant differences in the rate of early fault detection. They also show that our approach may contribute to accelerate the detection of faults of SPL test suites based on combinatorial testing.","2159-4848","978-1-4799-2255","10.1109/ICST.2014.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823864","Software product lines;feature models;automated analysis;test case prioritization","Testing;Fault detection;Complexity theory;Security;Measurement;Analytical models;Feature extraction","fault diagnosis;program testing","test case prioritization criteria comparison;software product line testing;SPL testing;test case prioritization techniques;combinatorial testing;fault detection;SPL test suites","","13","31","","","","","","IEEE","IEEE Conferences"
"Testing analytics on software variability","H. K. N. Leung; K. M. Lui","Department of Computing The Hong Kong Polytechnic University Hong Kong, China; Department of Computing The Hong Kong Polytechnic University Hong Kong, China","2015 IEEE 1st International Workshop on Software Analytics (SWAN)","","2015","","","17","20","Software testing is a tool-driven process. However, there are many situations in which different hardware/software components are tightly integrated. Thus system integration testing has to be manually executed to evaluate the system's compliance with its specified requirements and performance. There could be many combinations of changes as different versions of hardware and software components could be upgraded and/or substituted. Occasionally, some software components could even be replaced by clones. The whole system after each component change demands to be re-tested to ensure proper system behavior. For better utilization of resources, there is a need to prioritize the past test cases to test the newly integrated systems. We propose a way to facilitate the use of historical testing records of the previous systems so that a testcase portfolio can be developed, which intends to maximize testing resources for the same integrated product family. As the proposed framework does not consider much of internal software complexity, the implementation costs are relatively low.","","978-1-4673-6923","10.1109/SWAN.2015.7070483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070483","Software testing;Risk management;Clones;Softwareproject management;Variability testing","Testing;Portfolios;Software;Manuals;Hardware;Risk management;Cloning","program testing;software management","software variability;software testing;tool-driven process;system integration testing;software complexity","","","16","","","","","","IEEE","IEEE Conferences"
"Automatic test case generation for unit software testing using genetic algorithm and mutation analysis","R. Khan; M. Amjad","Department of Computer Engineering, Faculty of Engineering & Technology, Jamia Millia Islamia, Jamia Nagar, New Delhi, India; Department of Computer Engineering, Faculty of Engineering & Technology, Jamia Millia Islamia, Jamia Nagar, New Delhi, India","2015 IEEE UP Section Conference on Electrical Computer and Electronics (UPCON)","","2015","","","1","5","Software Engineers waste a lot of their time in the process of software testing. It also has been seen in the industries that a lot of money is depleting on the software process. In software testing process we apply test cases as input and check for final output. So our first concern is to choose the appropriate test cases for the software testing process. To give the correct output, it is very difficult to choose test cases. So generation of the test cases is a NP problem. To generate automatic test cases many nature inspired optimization algorithms have been used. These nature inspired optimization algorithms help to generate appropriate test cases. In this paper for generating automatic test cases genetic algorithm and mutation analysis had been used.","","978-1-4673-8507-7978-1-4673-8506","10.1109/UPCON.2015.7456734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456734","Genetic Algorithms (GA);Mutation Analysis;Software Testing;Automatic Test Cases","Genetic algorithms;Software;Software testing;Optimization;Algorithm design and analysis;Biological cells","automatic test pattern generation;genetic algorithms;program testing","automatic test case generation;unit software testing process;genetic algorithm;mutation analysis;software industries;NP problem;nature inspired optimization algorithms","","1","25","","","","","","IEEE","IEEE Conferences"
"An efficient particle swarm intelligence based strategy to generate optimum test data in t-way testing","K. Rabbi; Q. Mamun; M. R. Islam","School of Computing and Mathematics, Charles Sturt University, Australia; School of Computing and Mathematics, Charles Sturt University, Australia; School of Computing and Mathematics, Charles Sturt University, Australia","2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA)","","2015","","","123","128","Limited resources and tight deadline factor inhibits exhaustive testing. Thus, generation of optimal test data in an acceptable number is very important to accelerate the overall software engineering process. Search based optimization technique has been used in software test data generation since 1992 with recently increasing interest and activity within this area. Brief literature shows that, a change to the parameter interaction (t-way interaction) can significantly reduce the number of test data. Based on this principle, many t-way test data generation strategies have been developed over the past decade. Recent finding state that, implementation of artificial intelligence based searching for test data generation can obtain near optimum solution. However, producing the optimum test data appear to be NP-hard problem (Non-deterministic polynomial). As such, it is almost impossible for a strategy to produce the optimal set of test data. With the analysis of recent studies of the valid different search based optimization approach, this paper represents a swarm intelligent based searching strategy to generate near optimum test data. The performances are analyzed and compared to other well-known strategies. Empirical result shows that the proposed strategy is highly acceptable in terms of the test data size.","","978-1-4799-8389-6978-1-4673-7317","10.1109/ICIEA.2015.7334096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334096","Combinatorial interaction testing;Software testing;T-way testing;Test case generation;Interaction testing;Swarm intelligence","Software testing;Software;Generators;Aerospace electronics;Particle swarm optimization;Optimization","artificial intelligence;particle swarm optimisation;program testing;search problems;software engineering","particle swarm intelligence;optimum test data;t-way testing;deadline factor;software engineering process;search based optimization technique;software test data generation;parameter interaction;t-way interaction;artificial intelligence;optimum solution;NP-hard problem;nondeterministic polynomial;optimization approach;swarm intelligent based searching strategy","","","32","","","","","","IEEE","IEEE Conferences"
"Using Artificial Bee Colony for Code Coverage Based Test Suite Prioritization","P. Konsaard; L. Ramingwong","NA; NA","2015 2nd International Conference on Information Science and Security (ICISS)","","2015","","","1","4","The goal of test suite prioritization is maximizing fault detection and code coverage rate. Several nature inspired optimization algorithms such as Swarm Intelligence (SI) have been studied for the optimization of such problems. The studies revealed the benefits of Artificial Bee Colony (ABC) over other algorithms. ABC and its variations were implemented in software testing areas, test suite prioritization in particular. However, most SI based approaches focus on fault detection ability which is difficult to predict. In this paper, the standard ABC algorithm is used to prioritize test suites based on code coverage. The results reveal that ABC shows promising results and, hence, is a great candidate for prioritizing test suites. It also suggests that a modification to the standard ABC algorithm or combination of ABC and another SI algorithm should yield an even better result.","","978-1-4673-8611","10.1109/ICISSEC.2015.7371038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371038","","Software algorithms;Optimization;Software testing;Standards;Software;Genetic algorithms","fault diagnosis;optimisation;program testing","artificial bee colony;code coverage based test suite prioritization;fault detection;code coverage rate;nature inspired optimization algorithm;swarm intelligence;ABC algorithm;software testing","","1","31","","","","","","IEEE","IEEE Conferences"
"Mutation-based test-case prioritization in software evolution","Y. Lou; D. Hao; L. Zhang","Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China, Institute of Software, School of EECS, Peking University, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China, Institute of Software, School of EECS, Peking University, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China, Institute of Software, School of EECS, Peking University, China","2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)","","2015","","","46","57","During software evolution, to assure the software quality, test cases for an early version tend to be reused by its latter versions. As a large number of test cases may aggregate during software evolution, it becomes necessary to schedule the execution order of test cases so that the faults in the latter version may be detected as early as possible, which is test-case prioritization in software evolution. In this paper, we proposed a novel test-case prioritization approach for software evolution, which first uses mutation faults on the difference between the early version and the latter version to simulate real faults occurred in software evolution, and then schedules the execution order of test cases based on their fault-detection capability, which is defined based on mutation faults. In particular, we present two models on calculating fault-detection capability, which are statistics-based model and probability-based model. Moreover, we conducted an experimental study and found that our approach with the statistics-based model outperforms our approach with the probability-based model and the total statement coverage-based approach, and slightly outperforms the additional statement-coverage based approach in many cases. Furthermore, compared with the total or additional statement coverage-based approach, our approach with either the statistics-based model or the probability-based model tends to be stably effective when the difference on the source code between the early version and the latter version is non-trivial.","","978-1-5090-0406-5978-1-5090-0405","10.1109/ISSRE.2015.7381798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381798","","Testing;Schedules;Software quality;Java;Electronic mail;Aggregates","fault diagnosis;probability;program testing;software quality;source code (software)","mutation-based test-case prioritization;software evolution;mutation faults;fault-detection capability;statistics-based model;probability-based model;total statement coverage-based approach;source code;software quality","","4","48","","","","","","IEEE","IEEE Conferences"
"UPMOA: An improved search algorithm to support user-preference multi-objective optimization","S. Wang; S. Ali; T. Yue; M. Liaaen","Certus Software V&V Center, Simula Research Laboratory, Oslo, Norway; Certus Software V&V Center, Simula Research Laboratory, Oslo, Norway; Certus Software V&V Center, Simula Research Laboratory, Oslo, Norway; Cisco Systems, Oslo, Norway","2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)","","2015","","","393","404","Multi-objective search algorithms (e.g., non-dominated sorting genetic algorithm II (NSGA-II)) have been applied extensively to solve various multi-objective optimization problems in software engineering such as problems in testing. However, existing multi-objective algorithms usually treat all the objectives with equivalent priorities and do not provide a mechanism to reflect various user preferences when guiding search. The need to have such a mechanism was observed in one of our industrial projects on applying search algorithms for test optimization of a product line of Videoconferencing Systems (VCSs) called Saturn, where user preferences must be incorporated into optimization objectives, based on domain knowledge of test engineers for VCS testing. To address this, we propose an extension to the most commonly-used multi-objective search algorithm NSGA-II, which has shown promising results with user preferences. We name the extension as User-Preference Multi-Objective Optimization Algorithm (UPMOA), which includes a user preference indicator p and is based on existing weight assignment strategies. We empirically evaluated UPMOA with two industrial problems focusing on optimizing the test execution system for Saturn in Cisco. To assess the performance and scalability of UPMOA, inspired by the two industrial problems, in total we created 64000 artificial problems with 128 different sets of user preferences. The evaluation includes two aspects: 1) Three weight assignment strategies together with UPMOA were empirically evaluated to identify a best weight assignment strategy for p. Results show that the Uniformly Distributed Weights (UDW) strategy can assist UPMOA in achieving the best performance; 2) UPMOA was compared with three representative multi-objective search algorithms (including NSGA-II) and results show that UPMOA significantly outperformed the others and has the ability to solve problems with a wide range of complexity.","","978-1-5090-0406-5978-1-5090-0405","10.1109/ISSRE.2015.7381833","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381833","User-Preference Multi-objective Optimization;Weight Assignment Strategy;Search Algorithms","Search problems;Optimization;Testing;Software algorithms;Fault detection;Genetic algorithms;Software engineering","optimisation;program testing;search problems","UPMOA;improved search algorithm;user-preference multiobjective optimization algorithm;user preference indicator;weight assignment strategies;test execution system;Saturn;Cisco;uniformly distributed weights;UDW strategy;NSGA-II","","3","","","","","","","IEEE","IEEE Conferences"
"A methodology for test suite optimization based on testing requirement","Shiping Xu; Ronghua Guo; Yongqiang Bai; Yuhan Zhang; Yaowei Li","63880 Unit of PLA, Luoyang, Henan Province, China; 63880 Unit of PLA, Luoyang, Henan Province, China; 63880 Unit of PLA, Luoyang, Henan Province, China; 63880 Unit of PLA, Luoyang, Henan Province, China; 63880 Unit of PLA, Luoyang, Henan Province, China","2013 IEEE 4th International Conference on Software Engineering and Service Science","","2013","","","216","219","In order to reduce costs of software testing, and make the process more efficient, a methodology for test suite optimization is presented in this paper. An example is given to demonstrate the efficiency of this methodology.","2327-0594;2327-0586","978-1-4673-5000-6978-1-4673-4997-0978-1-4673-4998","10.1109/ICSESS.2013.6615291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615291","software testing;testing requirement;test suite","Software;Computers","optimisation;program testing","test suite optimization;testing requirement;software testing","","","11","","","","","","IEEE","IEEE Conferences"
"Comparison study of optimized test suite generation using Genetic and Memetic algorithm","A. A. Mundade; T. M. Pattewar","Department of Computer Engineering, NMU, SES's R. C. Patel Institute of Technology, Shirpur, MS, India; Department of Information Technology, NMU, SES's R. C. Patel Institute of Technology, Shirpur, MS, India","2015 International Conference on Pervasive Computing (ICPC)","","2015","","","1","5","Testing is one of the important phase of software engineering field, which checks the correctness of software. A common part in software testing is that test data are generated. For this test data, tester manually adds different test cases. But adding test cases manually is very difficult task. So we generate set of test cases called as test suite. Test case is nothing but a condition which we want to check. Code coverage is important factor in test suite. While generating a test suite, consider a inheritance tree a of class and generate test cases by considering all branches of that tree. Test case may have more than one goal, check feasibility for this particular goal. For generating optimized test suite, apply Genetic and Memetic algorithm. The aim of this test suite generation is covering all branches for maximum code coverage while keeping the minimum size. Applying both algorithms for code coverage. Code coverage of Memetic algorithm is maximum than code coverage of Genetic algorithm. We have implemented this system and for checking result use open source project such as Roops and net.","","978-1-4799-6272","10.1109/PERVASIVE.2015.7087175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7087175","Software testing;test suite;Genetic algorithm;Memetic algorithm;code coverage","Memetics;Software;Software algorithms;Software testing;Genetic algorithms;Genetics","genetic algorithms;program testing;public domain software","optimized test suite generation;software engineering field;software correctness;software testing;inheritance tree;genetic algorithm;memetic algorithm;test data generation;code coverage;open source project","","1","18","","","","","","IEEE","IEEE Conferences"
"Strategies for Prioritizing Test Cases Generated Through Model-Based Testing Approaches","J. F. S. Ouriques","NA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","2","","879","882","Software testing is expensive and time consuming,especially for complex software. In order to deal with the costof testing, researchers develop Model-Based Testing (MBT). InMBT, test cases are generated automatically and a drawback isa huge generated test suite. Our research aims at studying the Test Case Prioritization problem in MBT context. So far, we already evaluated the influence of the model structure and the characteristics of the test cases that fail. Results suggest that the former does not affect significantly the performance of techniques, however, the latter indeed represents a major impact. Therefore, a worthy information in this context might be an expert who knows the crucial parts of the software, thus we propose the first version of a prioritization technique that considers hints from the expert and the distance notion in order to prioritize test cases. Evaluation and tuning of the technique are ongoing, but preliminary evaluation reveals promising results.","0270-5257;1558-1225","978-1-4799-1934","10.1109/ICSE.2015.338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203104","","Testing;Context;Unified modeling language;Software;Context modeling;Software engineering;Conferences","program testing","test case generation;model-based testing approach;software testing;MBT;test case prioritization problem;prioritization technique","","1","27","","","","","","IEEE","IEEE Conferences"
"Results for EvoSuite -- MOSA at the Third Unit Testing Tool Competition","A. Panichella; F. M. Kifetew; P. Tonella","NA; NA; NA","2015 IEEE/ACM 8th International Workshop on Search-Based Software Testing","","2015","","","28","31","Evo Suite-MOSA is a unit test data generation tool that employs a novel many-objective optimization algorithm suitably developed for branch coverage. It was implemented by extending the Evo Suite test data generation tool. In this paper we present the results achieved by Evo Suite-MOSA in the third Unit Testing Tool Competition at SBST'15. Among six participants, Evo Suite-MOSA stood third with an overall score of 189.22.","","978-1-4673-7079","10.1109/SBST.2015.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7173587","search-based testing;many-objective optimization","Java;Benchmark testing;Aggregates;Computer crashes;Conferences;Software testing","data handling;high level languages;optimisation;program testing","third unit testing tool competition;test data generation tool;optimization algorithm;branch coverage;unit testing tool competition;Evo Suite-MOSA;Java classes;software testing","","1","4","","","","","","IEEE","IEEE Conferences"
"Total coverage based regression test case prioritization using genetic algorithm","P. Konsaard; L. Ramingwong","Department of Computer Engineering, Chiang Mai University, Thailand; Department of Computer Engineering, Chiang Mai University, Thailand","2015 12th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)","","2015","","","1","6","Regression Testing is a test to ensure that a program that was changed is still working. Changes introduced to a software product often come with defects. Additional test cases are, this could reduce the main challenges of regression testing is test case prioritization. Time, effort and budget needed to retest the software. Former studies in test case prioritization confirm the benefits of prioritization techniques. Most prioritization techniques concern with choosing test cases based on their ability to cover more faults. Other techniques aim to maximize code coverage. Thus, the test cases selected should secure the total coverage to assure the adequacy of software testing. In this paper, we present an algorithm to prioritize test cases based on total coverage using a modified genetic algorithm. Its performance on the average percentage of condition covered and execution time are compared with five other approaches.","","978-1-4799-7961-5978-1-4799-7960","10.1109/ECTICon.2015.7207103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207103","Test case prioritization;Test suite;Regression testing;Genetic algorith;Code coverage;APCC;Software Testing;Software engineering","Genetic algorithms;Software;Software testing;Sociology;Statistics;Fault detection","genetic algorithms;program testing;regression analysis","total coverage based regression test case prioritization;software product;code coverage;software testing;modified genetic algorithm","","7","21","","","","","","IEEE","IEEE Conferences"
"A Multi-Objective optimization algorithm for uniformly distributed generation of test cases","K. Choudhary; G. N. Purohit","Department of CSE &amp; IT, ITM University, Gurgaon, India; AIM &amp; ACT Department, Banasthali Vidyapith, India","2014 International Conference on Computing for Sustainable Global Development (INDIACom)","","2014","","","455","457","Multi-objective optimization deals with conflicting objectives. A multi-objective problem is determined to find acceptable solution for all conflicting objectives based on the concept of Pareto-Optimality. This paper focuses automatic test data generation on the aspect of Multi - Objective. One objective will be uniformly distribution and another is to maximize code. Multi-objective optimization incorporates decision making. This paper also covers non-dominance property to maintain sub-population of best fitness value. Generally, multi-objective optimization has two approaches, one is to decompose multi-objective into various single objective components, and another is to evolve Pareto-optimal set of solutions. Software Testing is a tedious, critical and expensive phase of software development, thus there is a need for automatic generation of uniformly distributed test cases over the provided range with maximum code coverage. Software system failure incur huge loss, to overcome this problem an efficient testing approach is required.","","978-93-80544-12-0978-93-80544-10-6978-93-80544-11","10.1109/IndiaCom.2014.6828179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6828179","Multi-objective Optimization;Pareto Optimal;Code Coverage;Non-dominance;Scalability;Automatic Test Case Generation;etc.","Optimization;Software testing;Distributed databases;Software;Evolutionary computation;Genetic algorithms","Pareto optimisation;program testing","multiobjective optimization algorithm;uniformly distributed generation;test case generation;conflicting objectives;Pareto optimality concept;automatic test data generation;decision making;fitness value;nondominance property;software testing;software development;code coverage","","1","9","","","","","","IEEE","IEEE Conferences"
"Prioritizing Variable-Strength Covering Array","R. Huang; J. Chen; T. Zhang; R. Wang; Y. Lu","NA; NA; NA; NA; NA","2013 IEEE 37th Annual Computer Software and Applications Conference","","2013","","","502","511","Combinatorial interaction testing is a well-studied testing strategy, and has been widely applied in practice. Combinatorial interaction test suite, such as fixed-strength and variable-strength interaction test suite, is widely used for combinatorial interaction testing. Due to constrained testing resources in some applications, for example in combinatorial interaction regression testing, prioritization of combinatorial interaction test suite has been proposed to improve the efficiency of testing. However, nearly all prioritization techniques may only support fixed-strength interaction test suite rather than variable-strength interaction test suite. In this paper, we propose two heuristic methods in order to prioritize variable-strength interaction test suite by taking advantage of its special characteristics. The experimental results show that our methods are more effective for variable-strength interaction test suite by comparing with the technique of prioritizing combinatorial interaction test suites according to test case generation order, the random test prioritization technique, and the fixed-strength interaction test suite prioritization technique. Besides, our methods have additional advantages compared with the prioritization techniques for fixed-strength interaction test suite.","0730-3157","978-0-7695-4986","10.1109/COMPSAC.2013.84","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649874","Software testing;combinatorial interaction testing;test case prioritization;fixed-strength interaction test suite;variable-strength interaction test suite;algorithm","Testing;Arrays;Educational institutions;Fault detection;Heuristic algorithms;Computer science;Software","program testing","variable-strength covering array;combinatorial interaction testing;testing strategy;combinatorial interaction test suite;fixed-strength interaction test suite;variable-strength interaction test suite;constrained testing resources;combinatorial interaction regression testing;testing efficiency;test case generation order;random test prioritization technique;fixed-strength interaction test suite prioritization technique","","6","35","","","","","","IEEE","IEEE Conferences"
"Automated generation of software testing path based on ant colony","F. Sayyari; S. Emadi","Department of Computer Engineering, Yazd Science and Research Branch, Islamic azad university, yazd, Iran, yazd, Iran; Department of Computer Engineering, Yazd Branch, Islamic azad university, yazd, Iran, yazd, Iran","2015 International Congress on Technology, Communication and Knowledge (ICTCK)","","2015","","","435","440","Software testing is one of the expensive and time consuming processes and many studies have been conducted to facilitate and perform it automatically. One of the most important topics in software testing is developing the test path to generated test data and coverage of the generated path. Optimization methods can be used to solve the problem of path testing. Heuristic search methods especially evolutionary algorithms are cost savings and can be effective in the automated generation of test paths. One of the most important challenges in the development of path tests is lack of full coverage of defined nodes and ignoring the important parameters of user. In this study, a solution is proposed based on ant colony optimization algorithm and model-based testing to develop test paths faster with maximum coverage and minimum time and cost. The model in this study is based on Markov chain. The results obtained from Markov chain are good choices for studying the viability of the testing process while developing them. Evaluation of the proposed algorithm has shown better performance compared to existing methods in terms of cost, coverage, time and parameters of user.","","978-1-4673-9762-9978-1-4673-9763","10.1109/ICTCK.2015.7582709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582709","","Ant colony optimization;Software algorithms;Software;Markov processes;Software testing;Mathematical model","ant colony optimisation;Markov processes;program testing;search problems;software quality","automated software testing path generation;test path;test data generation;path testing problem;heuristic search method;evolutionary algorithms;user parameters;ant colony optimization algorithm;model-based testing;Markov chain;cost factor;coverage factor;time factor;parameter factor","","","12","","","","","","IEEE","IEEE Conferences"
"A test automation framework in POCT system using TDD techniques","R. Chaipraserth; A. Leelasantitham; S. Kiattisin","Technology Information System Management Program, Faculty of Engineering, Mahidol University, 25/25 Puttamonthon, Nakorn Pathom 73170, Thailand; Technology Information System Management Program, Faculty of Engineering, Mahidol University, 25/25 Puttamonthon, Nakorn Pathom 73170, Thailand; Technology Information System Management Program, Faculty of Engineering, Mahidol University, 25/25 Puttamonthon, Nakorn Pathom 73170, Thailand","2013 13th International Symposium on Communications and Information Technologies (ISCIT)","","2013","","","600","604","Nowadays, the growth of IT business are highly competitive both of development process and functionality of software to meet mostly customer requirements. Normally, software performance should develop to increase the high quality whilst it should reduce both of the cost and the time. The IT business is to start the attention with satisfaction of the customer for expanding opportunities of grown business. The problem of former software development is an operation under the pressure conditions affecting to see more bugs of the software in a period of testing system and delivering the program to the required customer. This paper presents an implementation of approach which is an agile development (If what we do that able to modify the code to meet the requirement in the future, that's call agile.) using Test Driven Development (TDD) in automation testing framework for software development process. It helps optimization of software testing as it can found early bug, flexible for requirement changes in the future, either to increase or change requirement without any effect on coding behavior. It also helps to reduce cost of software development as a human resource, time and rapid feedback of test result. We apply this approach with prototype in software development processes using the Point of care testing (POCT) system which is as an example to create test case with black-box and white-box testing techniques based on the requirements specification to improve more product quality.","","978-1-4673-5580-3978-1-4673-5578","10.1109/ISCIT.2013.6645931","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645931","Testing process;Software Test Process;Black-box testing;White-box testing;Test Driven Development;Test cases generation","Software;Computer bugs;Automation;Monitoring;Software testing;Encyclopedias","automatic testing;customer satisfaction;formal specification;program debugging;program testing;software prototyping;software quality","IT business;software functionality;software development process;customer requirements;software performance;customer satisfaction;software bugs;agile development;test driven development;automation testing framework;software testing system;coding behavior;cost reduction;human resource;black-box testing techniques;white-box testing techniques;requirements specification;product quality","","","11","","","","","","IEEE","IEEE Conferences"
"Implementing test case selection and reduction techniques using meta-heuristics","R. Nagar; A. Kumar; S. Kumar; A. S. Baghel","School of Information &amp; Communication Technology, Gautam Buddha University, Greater Noida, India; Pitney Bowes Software, Noida, India; School of Information &amp; Communication Technology, Gautam Buddha University, Greater Noida, India; School of Information &amp; Communication Technology, Gautam Buddha University, Greater Noida, India","2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence)","","2014","","","837","842","Regression Testing is an inevitable and very costly maintenance activity that is implemented to make sure the validity of modified software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization to select and prioritize a minimum set of test cases, fulfilling some chosen criteria, that is, covering all possible faults in minimum time and other. In this paper a test case reduction hybrid Particle Swarm Optimization (PSO) algorithm has been proposed. This PSO algorithm uses GA mutation operator while processing. PSO is a swarm intelligence algorithm based on particles behavior. GA is an evolutionary algorithm (EA). The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.","","978-1-4799-4236-7978-1-4799-4237","10.1109/CONFLUENCE.2014.6949377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949377","Particle Swarm Optimization;Genetic Algorithm;Regression Test Selection;Test Case Prioritization","Testing;Genetic algorithms;Software algorithms;Software;Particle swarm optimization;Sociology;Statistics","data reduction;feature selection;genetic algorithms;particle swarm optimisation;program testing;regression analysis;software maintenance","test case selection;test case reduction;test case prioritization;metaheuristics;regression testing;software maintenance;particle swarm optimization;PSO algorithm;GA mutation operator;evolutionary algorithm;EA","","3","17","","","","","","IEEE","IEEE Conferences"
"A code coverage-based test suite reduction and prioritization framework","S. U. R. Khan; S. P. Lee; R. M. Parizi; M. Elahi","Faculty of Computer Science and IT, University of Malaya, Kuala Lumpur, Malaysia; Faculty of Computer Science and IT, University of Malaya, Kuala Lumpur, Malaysia; School of Computing and IT, Taylor's University, Selangor, Malaysia; Dept. of Computer Science, COMSATS Institute of IT, Islamabad, Pakistan","2014 4th World Congress on Information and Communication Technologies (WICT 2014)","","2014","","","229","234","Software testing is extensively used to ensure the development of a quality software system. The test suite size tends to increase by including new test cases due to software evolution. Consequently, the entire test suite cannot be executed considering budget and time limitations. Researchers have examined test suite reduction and prioritization techniques to address the test suite size problem. However, combination of these techniques can be useful for various regression testing situations. In this paper, we present a new code coverage-based test suite reduction and prioritization framework called TestOptimizer. The framework performs a suitable combination of TestFilter and St-Total techniques to determine optimal test cases, keeping in view of time restrictions. The performance of the proposed framework has been assessed using a case study. Results show that TestOptimizer can be beneficial to solve the test suite size problem within time constraints and has a profound impact on the required cost and effort of regression testing.","","978-1-4799-8115-1978-1-4799-8114","10.1109/WICT.2014.7076910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7076910","regression testing;framework;test suite reduction;test suite prioritization","Optimization;Testing;History;Software systems;Time factors;Computer science","program testing;regression analysis;software quality","code coverage-based test suite reduction;code coverage-based test suite prioritization;software testing;quality software system development;software evolution;regression testing situation;TestOptimizer;TestFilter;St-Total techniques;optimal test case","","","15","","","","","","IEEE","IEEE Conferences"
"Multi-perspective Regression Test Prioritization for Time-Constrained Environments","D. Marijan","NA","2015 IEEE International Conference on Software Quality, Reliability and Security","","2015","","","157","162","Test case prioritization techniques are widely used to enable reaching certain performance goals during regression testing faster. A commonly used goal is high fault detection rate, where test cases are ordered in a way that enables detecting faults faster. However, for optimal regression testing, there is a need to take into account multiple performance indicators, as considered by different project stakeholders. In this paper, we introduce a new optimal multi-perspective approach for regression test case prioritization. The approach is designed to optimize regression testing for faster fault detection integrating three different perspectives: business perspective, performance perspective, and technical perspective. The approach has been validated in regression testing of industrial mobile device systems developed in continuous integration. The results show that our proposed framework efficiently prioritizes test cases for faster and more efficient regression fault detection, maximizing the number of executed test cases with high failure frequency, high failure impact, and cross-functional coverage, compared to manual practice.","","978-1-4673-7989-2978-1-4673-7988","10.1109/QRS.2015.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272927","software testing;regression testing;test case prioritization","Testing;Fault detection;Manuals;Software;Business;Time factors;Time-frequency analysis","fault diagnosis;program testing;regression analysis","multiperspective regression test prioritization;time-constrained environment;test case prioritization technique;fault detection rate;optimal regression testing;performance indicator;optimal multiperspective approach;regression test case prioritization;business perspective;performance perspective;technical perspective;industrial mobile device system;continuous integration;regression fault detection;failure frequency;failure impact;cross-functional coverage","","3","18","","","","","","IEEE","IEEE Conferences"
"JTExpert at the Third Unit Testing Tool Competition","A. Sakti; G. Pesant; Y. Guéhéneuc","NA; NA; NA","2015 IEEE/ACM 8th International Workshop on Search-Based Software Testing","","2015","","","52","55","JTExpert is a software testing tool that automatically generates a whole test suite to satisfy the branch-coverage criterion on a given Java source code. It takes as inputs a Java source code and its dependencies and automatically produces a test-case suite in JUnit format. In this paper, we summarize our results for the Unit Testing Tool Competition held at the third SBST Contest, where JT Expert receives 159.16 points and was ranked sixth of seven participating tools. We discuss the internal and external reasons that were behind the relatively poor score and ranking.","","978-1-4673-7079","10.1109/SBST.2015.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7173593","test-case generation;classes testing;unit testing;random testing;static analysis","Libraries;Java;Generators;Software testing;Benchmark testing;Conferences","Java;program testing;source code (software)","JTExpert;third unit testing tool competition;software testing tool;branch-coverage criterion;Java source code;JUnit format;SBST Contest","","","4","","","","","","IEEE","IEEE Conferences"
"Multi-objective Optimal Test Suite Computation for Software Product Line Pairwise Testing","R. E. Lopez-Herrejon; F. Chicano; J. Ferrer; A. Egyed; E. Alba","NA; NA; NA; NA; NA","2013 IEEE International Conference on Software Maintenance","","2013","","","404","407","Software Product Lines (SPLs) are families of related software products, which usually provide a large number of feature combinations, a fact that poses a unique set of challenges for software testing. Recently, many SPL testing approaches have been proposed, among them pair wise combinatorial techniques that aim at selecting products to test based on the pairs of feature combinations such products provide. These approaches regard SPL testing as an optimization problem where either coverage (maximize) or test suite size (minimize) are considered as the main optimization objective. Instead, we take a multi-objective view where the two objectives are equally important. In this exploratory paper we propose a zero-one mathematical linear program for solving the multi-objective problem and present an algorithm to compute the true Pareto front, hence an optimal solution, from the feature model of a SPL. The evaluation with 118 feature models revealed an interesting trade-off between reducing the number of constraints in the linear program and the runtime which opens up several venues for future research.","1063-6773","978-0-7695-4981","10.1109/ICSM.2013.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676918","software product lines;pairwise testing;multi-objective optimization","Testing;Software;Computational modeling;Mathematical model;Optimization;Runtime;Correlation","combinatorial mathematics;linear programming;Pareto optimisation;product development;program testing;software reusability","multiobjective optimal test suite computation;software product line pairwise testing;SPL testing approach;pairwise combinatorial techniques;optimization problem;test suite size;zero-one mathematical linear programming;true Pareto front","","10","18","","","","","","IEEE","IEEE Conferences"
"A Search-Based Approach for Cost-Effective Software Test Automation Decision Support and an Industrial Case Study","Y. Amannejad; V. Garousi; R. Irving; Z. Sahaf","NA; NA; NA; NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops","","2014","","","302","311","Test automation is a widely-used approach to reduce the cost of manual software testing. However, if it is not planned or conducted properly, automated testing would not necessarily be more cost effective than manual testing. Deciding what parts of a given System Under Test (SUT) should be tested in an automated fashion and what parts should remain manual is a frequently-asked and challenging question for practitioner testers. In this study, we propose a search-based approach for deciding what parts of a given SUT should be tested automatically to gain the highest Return On Investment (ROI). This work is the first systematic approach for this problem, and significance of our approach is that it considers automation in the entire testing process (i.e., from test-case design, to test scripting, to test execution, and test-result evaluation). The proposed approach has been applied in an industrial setting in the context of a software product used in the oil and gas industry in Canada. Among the results of the case study is that, when planned and conducted properly using our decision-support approach, test automation provides the highest ROI. In this study, we show that if automation decision is taken effectively, test-case design, test execution, and test evaluation can result in about 307%, 675%, and 41% ROI in 10 rounds of using automated test suites.","","978-1-4799-5790","10.1109/ICSTW.2014.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825677","action research;cost-benefit analysis;industrial case study;search-based software engineering;software test automation","Automation;Manuals;Software;Software testing;Context;Optimization","cost reduction;investment;program testing;software product lines","test evaluation;test execution;test-case design;oil industry;gas industry;software product;ROI;return on investment;SUT;system under test;cost reduction;industrial case study;cost-effective software test automation decision support approach;search-based approach","","7","38","","","","","","IEEE","IEEE Conferences"
"Component-Based Software System Test Case Prioritization with Genetic Algorithm Decoding Technique Using Java Platform","S. Mahajan; S. D. Joshi; V. Khanaa","NA; NA; NA","2015 International Conference on Computing Communication Control and Automation","","2015","","","847","851","Test case prioritization includes testing experiments in a request that builds the viability in accomplishing some execution objectives. The importance amongst the most imperative testing objectives is the fast rate of fault recognition. Test case ought to run in a request that extends the likelihood of fault discovery furthermore that detects the most serious issues at the early stage of testing life cycle. In this paper, we develop and prove the necessity of Component-Based Software testing prioritization framework which plans to uncover more extreme bugs at an early stage and to enhance software product deliverable quality utilizing Genetic Algorithm (GA) with java decoding technique. For this, we propose a set of prioritization keys to plan the proposed Component-Based Software java framework. In our proposed method, we allude to these keys as Prioritization Keys (PK). These keys may be project size, scope of the code, information stream, and bug inclination and impact of fault or bug on overall system, which prioritizes the Component-Based Software framework testing. The integrity of these keys was measured with implementation of key assessment metric called KAM that will likewise be ascertained. This paper demonstrates how software testing can be efficient with management of data integrity factor to avoid major security issues. One of the main advantages of our approach is that domain specific semantics can be integrated with the data quality test cases prioritization, thus being able to discover test feed data quality problems beyond conventional quality measures.","","978-1-4799-6892","10.1109/ICCUBEA.2015.169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155967","based prioritization;Genetic Algorithm;Java Decoding;Alternative prioritization","Testing;Genetic algorithms;Software;Java;Biological cells;Measurement;Sociology","genetic algorithms;Java;object-oriented programming;program testing","component-based software system test case prioritization;genetic algorithm decoding technique;Java platform;testing objectives;fault discovery;testing life cycle;GA;prioritization keys;PK;software testing;data integrity factor;domain specific semantics","","1","24","","","","","","IEEE","IEEE Conferences"
"On the Asymptotic Behavior of Adaptive Testing Strategy for Software Reliability Assessment","J. Lv; B. Yin; K. Cai","School of Automation Science and Electrical Engineering, Bejing, China; School of Automation Science and Electrical Engineering, Bejing, China; School of Automation Science and Electrical Engineering, Bejing, China","IEEE Transactions on Software Engineering","","2014","40","4","396","412","In software reliability assessment, one problem of interest is how to minimize the variance of reliability estimator, which is often considered as an optimization goal. The basic idea is that an estimator with lower variance makes the estimates more predictable and accurate. Adaptive Testing (AT) is an online testing strategy, which can be adopted to minimize the variance of software reliability estimator. In order to reduce the computational overhead of decision-making, the implemented AT strategy in practice deviates from its theoretical design that guarantees AT's local optimality. This work aims to investigate the asymptotic behavior of AT to improve its global performance without losing the local optimality. To this end, a new AT strategy named Adaptive Testing with Gradient Descent method (AT-GD) is proposed. Theoretical analysis indicates that AT-GD, a locally optimal testing strategy, converges to the globally optimal solution as the assessment process proceeds. Simulation and experiments are set up to validate AT-GD's effectiveness and efficiency. Besides, sensitivity analysis of AT-GD is also conducted in this study.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2310194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6762895","Adaptive testing;operational profile;software reliability;testing strategy","Testing;Software reliability;Software;Global Positioning System;Aircraft;Reliability theory","decision making;gradient methods;optimisation;program testing;sensitivity analysis;software reliability","asymptotic behavior;adaptive testing strategy;software reliability assessment;optimization goal;online testing strategy;software reliability estimator;computational overhead reduction;decision-making;global performance improvement;adaptive testing with gradient descent method;AT-GD;locally optimal testing strategy;sensitivity analysis","","15","35","","","","","","IEEE","IEEE Journals & Magazines"
"A coupling effect based test case prioritization technique","H. Kumar; N. Chauhan","YMCA University of Science &amp; Technology, Faridabad, India; YMCA University of Science &amp; Technology, Faridabad, India","2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)","","2015","","","1341","1345","Regression testing is a process that executes subset of tests that have already been conducted to ensure that changes have not propagated unintended side effects. Test case prioritization aims at reordering the regression test suit based on certain criteria, so that the test cases with higher priority can be executed first rather than those with lower priority. In this paper, a new approach for test case prioritization has been proposed which is based on a module-coupling effect that considers the module-coupling value for the purpose of prioritizing the modules in the software so that critical modules can be identified which in turn will find the prioritized set of test cases. In this way there will be high percentage of detecting critical errors that have been propagated to other modules due to any change in a module. The proposed approach has been evaluated with the case study of software consisting of ten modules.","","978-9-3805-4416-8978-9-3805-4415-1978-9-3805-4414","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100468","Regression Testing;Test Case Prioritization;Coupling & Cohesion","Couplings;Testing;Software;Fault detection;Symmetric matrices;Computer science;Computer bugs","program testing","coupling effect based test case prioritization technique;regression testing;module-coupling effect;critical error detection","","","12","","","","","","IEEE","IEEE Conferences"
"Test Case Prioritization Using Requirements-Based Clustering","M. J. Arafeen; H. Do","NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation","","2013","","","312","321","The importance of using requirements information in the testing phase has been well recognized by the requirements engineering community, but to date, a vast majority of regression testing techniques have primarily relied on software code information. Incorporating requirements information into the current testing practice could help software engineers identify the source of defects more easily, validate the product against requirements, and maintain software products in a holistic way. In this paper, we investigate whether the requirements-based clustering approach that incorporates traditional code analysis information can improve the effectiveness of test case prioritization techniques. To investigate the effectiveness of our approach, we performed an empirical study using two Java programs with multiple versions and requirements documents. Our results indicate that the use of requirements information during the test case prioritization process can be beneficial.","2159-4848","978-0-7695-4968-2978-1-4673-5961","10.1109/ICST.2013.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569743","regression testing;test case prioritization;requirements-based clustering;empirical study","Testing;Software;Measurement;Complexity theory;Fault detection;Educational institutions;Java","formal specification;Java;pattern clustering;program testing","test case prioritization process;requirements documents;Java programs;test case prioritization techniques;traditional code analysis information;defects source identification;software engineers;regression testing techniques;requirements engineering;requirements information;requirements-based clustering","","37","35","","","","","","IEEE","IEEE Conferences"
"Test case selection and prioritization using cuckoos search algorithm","R. Nagar; A. Kumar; G. P. Singh; S. Kumar","Computer Science and Engineering Department, Aligarh College of Engineering and Technology, Aligarh, India; Pitney Bowes Software Noida, India; Computer Science and Engineering Department, Aligarh College of Engineering and Technology, Aligarh, India; Computer Science and Engineering Department, Shivdan Singh Institute of Technology &amp; Management, Aligarh, India","2015 International Conference on Futuristic Trends on Computational Analysis and Knowledge Management (ABLAZE)","","2015","","","283","288","Regression Testing is an inevitable and very costly activity that is implemented to ensure the validity of new version of software in a time and resource constrained environment. Execution of entire test suite is not possible so it is necessary to apply techniques like Test Case Selection and Test Case Prioritization for proper selection and schedule of test cases in a specific sequence, fulfilling some chosen criteria. Cuckoo search (CS) algorithm is an optimization algorithm proposed by Yang and Deb [13]. It is inspired by the obligate brood parasitism of some cuckoo species by laying their eggs in the nests of other host birds. Cuckoo Search is very easy to implement as it depends on single parameter only unlike other optimization algorithms. In this paper a test case selection and prioritization algorithm has been proposed using Cuckoo Search. This algorithm selects and prioritizes the test cases based on the number of faults covered in minimum time. The proposed algorithm is an optimistic approach which provides optimum best results in minimum time.","","978-1-4799-8433-6978-1-4799-8432","10.1109/ABLAZE.2015.7155012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155012","Cuckoos Search;Levy Flight;Regression Test Selection;Test Case Prioritization;Artificial Intelligence","Testing;Algorithm design and analysis;Optimization;Software algorithms;Software;Sociology;Statistics","configuration management;program testing;program verification;regression analysis;search problems","test case selection;test case prioritization;Cuckoos search algorithm;regression testing;software version;time constrained environment;resource constrained environment;test case scheduling;CS algorithm;optimization algorithm;obligate brood parasitism;cuckoo species;fault","","4","26","","","","","","IEEE","IEEE Conferences"
"Defect Prioritization in the Software Industry: Challenges and Opportunities","N. Kaushik; M. Amoui; L. Tahvildari; W. Liu; S. Li","NA; NA; NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation","","2013","","","70","73","Defect prioritization is a decision making process wherein stakeholders determine the temporal order of open defects to be fixed. It is critical to the software development lifecycle as the decisions made during this process directly affect release planning, resource management, and maintenance costs. In fact, defect prioritization is complex as many factors need to be taken into consideration and the decisions made can be subjective or incorporate inherent knowledge and intuition of decision makers. We believe that managing the complexities of the decision making process can provide valuable support and help in uncovering any inconsistencies in the interpretation of criteria to prioritize defects. In this paper, we explore the defect triaging process in Research In Motion to gain a better understanding of the shortcomings and challenges of the current practices. Based on our findings, we sketch some research directions to improve industrial software defect prioritization.","2159-4848","978-0-7695-4968-2978-1-4673-5961","10.1109/ICST.2013.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569718","software life-cycle;defect prioritization;triage","Software;Testing;Decision making;Software engineering;Economics;Availability;Industries","DP industry;program testing;software development management;software maintenance","software industry;decision making process;software development lifecycle;release planning;resource management;maintenance cost;complexity management;defect triaging process;Research In Motion;industrial software defect prioritization","","2","12","","","","","","IEEE","IEEE Conferences"
"Measuring software testing efficiency using two-way assessment technique","P. K. Kapur; G. Singh; N. Sachdeva; A. Tickoo","Amity University, Noida, UP, India; Amity University, Noida, UP, India; Department of Operational Research, University of Delhi, India; Amity University, Noida, UP, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","6","Software has become a driving force. Its impact on our society continues to be profound. Customers have come to rely on the functionality and accuracy of software. A bug free software is an illusion and infinite testing is simply not a viable option.. But with scientific software development technology, quality control and systematic testing, number of bugs could be minimized. Software testing is an important activity to improve software quality. However, it is well known that it is costly (Yang et al, 2008; Bertolino, 2008). Thus, there has always been a need to increase the efficiency of testing while, in parallel, making it more effective in terms of finding &amp; removing defects. Measuring critical attributes of software testing process enables software developers with further insight in to software testing process. Thereafter, optimal utilization of their resources for testing purpose. In this paper, we propose a two way assessment framework for software developers to find out overall utility from the testing process that can serve as an important indicator of the efficiency and effectiveness of software testing process. It illustrates a two way assessment technique, which can be adopted to inculcate a culture of continuous improvement in the overall software testing lifecycle. The framework takes into consideration 1). Managements/Developers perspective to highlight the importance of various attributes of testing; &amp; 2). Testers/Team Members perspective, to assess functioning and working of each of the attribute.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014679","Two-Way Assessment;Analytical Hierarchical Process;Utility Value;Software Testing","Software;Software testing;Software measurement;Organizations;Software reliability","program testing;software quality","software testing efficiency;two-way assessment technique;software functionality;software accuracy;bug free software;software development technology;quality control;systematic testing;software quality;software testing process;resource optimization;continuous improvement;software testing lifecycle;managements-developers perspective;testers-team members perspective","","","29","","","","","","IEEE","IEEE Conferences"
"Multi-objective Test Suite Optimization for Incremental Product Family Testing","H. Baller; S. Lity; M. Lochau; I. Schaefer","NA; NA; NA; NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation","","2014","","","303","312","The design of an adequate test suite is usually guided by identifying test requirements which should be satisfied by the selected set of test cases. To reduce testing costs, test suite minimization heuristics aim at eliminating redundancy from existing test suites. However, recent test suite minimization approaches lack (1) to handle test suites commonly derived for families of similar software variants under test, and (2) to incorporate fine-grained information concerning cost/profit goals for test case selection. In this paper, we propose a formal framework to optimize test suites designed for sets of software variants under test w.r.t. multiple conflicting cost/profit objectives. The problem representation is independent of the concrete testing methodology. We apply integer linear programming (ILP) to approximate optimal solutions. We further develop an efficient incremental heuristic for deriving a sequence of representative software variants to be tested for approaching optimal profits under reduced costs. We evaluated the algorithm by comparing its outcome to the optimal solution.","2159-4848","978-1-4799-2255","10.1109/ICST.2014.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823892","Testing Strategies;Test Coverage of Specifications;Constrained Optimization;Linear Programming","Optimization;Testing;Minimization;Linear programming;Software systems;Partial transmit sequences","cost reduction;formal specification;integer programming;linear programming;profitability;program testing;program verification","multiobjective test suite optimization;incremental product family testing;test requirement identification;testing cost reduction;test suite minimization heuristics;redundancy elimination;software variants;cost-profit goals;test case selection;formal framework;cost-profit objectives;integer linear programming;ILP","","11","30","","","","","","IEEE","IEEE Conferences"
"Software-Based Self Test Methodology for On-Line Testing of L1 Caches in Multithreaded Multicore Architectures","G. Theodorou; N. Kranitis; A. Paschalis; D. Gizopoulos","Department of Informatics and Telecommunications, University of Athens, Athens, Greece; Department of Informatics and Telecommunications, University of Athens, Athens, Greece; Department of Informatics and Telecommunications, University of Athens, Athens, Greece; Department of Informatics and Telecommunications, University of Athens, Athens, Greece","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2013","21","4","786","790","The flexibility that allows the application of different March tests is a critical requirement for on-line testing of memory arrays. In a previous study, we have introduced a low-cost software-based self test (SBST) program development methodology for on-line periodic testing of L1 caches that utilizes direct cache access (DCA) instructions and exploits the native monitoring hardware available in modern architectures. In this brief, we discuss a multithreaded optimization of this SBST methodology that exploits the thread level parallelism of multithreaded multicore architectures in order to speed up March test execution by elaborating the low level multiple sub-bank cache organization. The effectiveness of the methodology and its multithreaded optimization is demonstrated on the L1 caches of OpenSPARC T1 processor. Our results showed a speedup of more than 1.7 when the multithreaded optimization is applied and an acceptable performance overhead (less than 11%), even in intensive periodic test scenarios.","1063-8210;1557-9999","","10.1109/TVLSI.2012.2191000","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6180021","Cache memories;March test;microprocessor test;on-line test;software-based self test (SBST);thread level parallelism","Instruction sets;Testing;Arrays;Optimization;Random access memory;Organizations","automatic test software;built-in self test;cache storage;circuit optimisation;electronic engineering computing;memory architecture;microprocessor chips;multiprocessing systems;multi-threading;parallel architectures;performance evaluation","L1 caches;performance overhead evaluation;OpenSPARC T1 processor;low level multiple sub-bank cache organization;March test execution;thread level parallelism;multithreaded optimization;monitoring hardware;DCA instructions;direct cache access instructions;online periodic testing;low-cost SBST program development methodology;low-cost software-based self test program development methodology;memory arrays;multithreaded multicore architectures","","12","9","","","","","","IEEE","IEEE Journals & Magazines"
"A refactoring-based approach for test case selection and prioritization","E. L. G. Alves; P. D. L. Machado; T. Massoni; S. T. C. Santos","SPLab - Software Practices Laboratory, Federal University of Campina Grande, UFCG, Campina Grande, Brazil; SPLab - Software Practices Laboratory, Federal University of Campina Grande, UFCG, Campina Grande, Brazil; SPLab - Software Practices Laboratory, Federal University of Campina Grande, UFCG, Campina Grande, Brazil; SPLab - Software Practices Laboratory, Federal University of Campina Grande, UFCG, Campina Grande, Brazil","2013 8th International Workshop on Automation of Software Test (AST)","","2013","","","93","99","Refactoring edits, commonly applied during software development, may introduce faults in a previously-stable code. Therefore, regression testing is usually applied to check whether the code maintains its previous behavior. In order to avoid rerunning the whole regression suite, test case prioritization techniques have been developed to order test cases for earlier achievement of a given goal, for instance, improving the rate of fault detection during regression testing execution. However, as current techniques are usually general purpose, they may not be effective for early detection of refactoring faults. In this paper, we propose a refactoring-based approach for selecting and prioritizing regression test cases, which specializes selection/prioritization tasks according to the type of edit made. The approach has been evaluated through a case study that compares it to well-known prioritization techniques by using a real open-source Java system. This case study indicates that the approach can be more suitable for early detection of refactoring faults when comparing to the other prioritization techniques.","","978-1-4673-6161","10.1109/IWAST.2013.6595798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595798","","Testing;Java;Fault detection;Software;Object oriented modeling;Measurement;Debugging","program testing;software maintenance","refactoring-based approach;test case selection technique;test case prioritization technique;edit refactoring;software development;fault detection;regression testing;open-source Java system","","1","36","","","","","","IEEE","IEEE Conferences"
"Prioritized test-driven reverse engineering process: A case study","P. Sfetsos; L. Angelis; I. Stamelos","Department of Information Technology, Alexander Technological Education Institute, Thessaloniki, Greece; Department of Informatics, Aristotle University, Thessaloniki, Greece; Department of Informatics, Aristotle University, Thessaloniki, Greece","2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA)","","2015","","","1","6","In this study we empirically investigate the adaptation of Test-Driven Development (TDD) practice into software Reverse Engineering (RE) process. We call this adaptation as Test-Driven Reverse Engineering (TDRE) process. We propose a two-layer prioritization process, which firstly prioritizes the already-implemented functionalities using the Cumulative Voting (CV) method and three prioritization criteria (importance, complexity and dependency), and secondly prioritizes test-cases for each prioritized functionality, using the same criteria. We conducted a case study in academia with students to empirically evaluate the usability and effectiveness of the prioritization process and the TDD adaptation into RE process. The results have shown that students with a good performance in testing had also good performance in designing UML class-diagrams. Moreover, the implementation of hierarchical test-cases for the already prioritized functionalities, improves code comprehension and redesigning in the RE process in terms of better total grades obtained.","","978-1-4673-9311-9978-1-4673-9310","10.1109/IISA.2015.7388099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7388099","Test Driven Development;Reverse Engineering;Prioritization;Cumulative Voting;UML class-diagram","Testing;Reverse engineering;Software;Complexity theory;Unified modeling language;Software engineering;Fault detection","program testing;reverse engineering;software maintenance;Unified Modeling Language","prioritized test-driven software reverse engineering process;test-driven development;TDD adaptation;TDRE;two-layer prioritization process;cumulative voting method;importance prioritization criteria;complexity prioritization criteria;dependency prioritization criteria;UML class-diagrams;hierarchical test-cases;code comprehension improvement","","","35","","","","","","IEEE","IEEE Conferences"
"An improved genetic approach for test path generation","Preeti; J. Chaudhary","Computer Engineering, The Technological Institute of Textile &amp; Sciences, Bhiwani, India; Computer Engineering, The Technological Institute of Textile &amp; Sciences, Bhiwani, India","2014 International Conference on Advances in Engineering & Technology Research (ICAETR - 2014)","","2014","","","1","5","Quality of a software system depends on testing approaches adopted to analyze the software product. Testing process itself depends on two main vectors called test sequence generation and test data generation. Test sequence generation is about to identify the order in which the particular test cases will be executed and the test data defines the various checks performed on each test case. In this present work, a fuzzy improved genetic approach is suggested for test case generation. The sequence on these test cases is here dependent on module interaction analysis. Based on this analysis, the test case prioritization will be defined. Once the test cases will be prioritized, the next work is to apply fuzzy improved genetic approach for test path generation. The work is analyzed under different prioritization vectors. Analysis of work is defined in terms of test cost estimation under different prioritization scenarios.","2347-9337","978-1-4799-6393-5978-1-4799-6392","10.1109/ICAETR.2014.7012823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7012823","Genetic Based;Test Data;Test Sequence;Prioritization;Module Integration","Testing;Reliability;Estimation;Algorithm design and analysis","fuzzy set theory;genetic algorithms;program testing;software quality","test path generation;fuzzy improved genetic approach;test case generation;software testing;software quality;module interaction analysis;test case prioritization;prioritization vectors;test cost estimation","","","14","","","","","","IEEE","IEEE Conferences"
"The Maintenance and Evolution of Field-Representative Performance Tests","M. D. Syer","NA","2014 IEEE International Conference on Software Maintenance and Evolution","","2014","","","665","665","The rise of large-scale software systems poses new challenges for the software performance engineering field. Failures in these systems are typically associated with performance issues, rather than with feature bugs. Therefore, performance testing has become essential to ensuring the problem-free operation of these systems. However, the performance testing process is faced with a major challenge: evolving field workloads and deployment configurations, often lead to ""outdated"" tests that are not reflective of the field. Hence performance analysts must maintain their field-representative tests by evolving these tests. Therefore, we propose new approaches for maintaining and evolving field-representative performance tests. We believe that field-representative tests can be autonomically maintained and evolved, i.e., the configurable parameters of a performance test can be optimized, such that the resulting test exercises the system in a similar manner as the field. Such approaches should help performance analysts to ensure the ""realism"" of their performance testing efforts, resulting in higher quality systems.","1063-6773","978-1-4799-6146","10.1109/ICSME.2014.120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976164","Performance Engineering;Performance Testing;Maintenance and Evolution","Testing;Software systems;Software performance;Software engineering;Maintenance engineering;Educational institutions","program testing;software maintenance;software performance evaluation","feature bugs;software quality systems;software performance engineering field;large-scale software systems;field-representative performance test maintenance;field-representative performance test evolution","","","10","","","","","","IEEE","IEEE Conferences"
"A study of applying severity-weighted greedy algorithm to software test case prioritization during testing","Y. Hsu; K. Peng; C. Huang","Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Institute of Information Systems and Applications, National Tsing Hua University, Hsinchu, Taiwan","2014 IEEE International Conference on Industrial Engineering and Engineering Management","","2014","","","1086","1090","Regression testing is a very useful technique for software testing. Traditionally, there are several techniques for test case prioritization; two of the most used techniques are Greedy and Additional Greedy Algorithm (GA and AGA). However, it can be found that they may not consider the severity while prioritizing test cases. In this paper, an Enhanced Additional Greedy Algorithm (EAGA) is proposed for test case prioritization. Experiments with eight subject programs are performed to investigate the effects of different techniques under different criteria and fault severity. Experimental results show that proposed EAGA perform well than other techniques.","2157-3611;2157-362X","978-1-4799-6410","10.1109/IEEM.2014.7058806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058806","Test case prioritization;code coverage;search algorithm;APFD;APFDc","Greedy algorithms;Software testing;Fault detection;Software engineering;Software;Schedules","greedy algorithms;program testing","applying severity weighted greedy algorithm;software test case prioritization;software testing;additional greedy algorithm;AGA;enhanced additional greedy algorithm;EAGA","","","32","","","","","","IEEE","IEEE Conferences"
"Test case prioritization with textual comparison metrics","R. Tumeng; D. N. A. Jawawi; M. A. Isa","Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia; Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia; Department of Software Engineering, Faculty of Computing, Universiti Teknologi Malaysia, Skudai, Johor, Malaysia","2015 9th Malaysian Software Engineering Conference (MySEC)","","2015","","","7","12","Regression testing of a large test pool consistently needs a prioritization technique that caters requirements changes. Conventional prioritization techniques cover only the methods to find the ideal ordering of test cases neglecting requirement changes. In this paper, we propose string dissimilarity-based priority assignment to test cases through the combination of classical and non-classical textual comparison metrics and elaborate a prioritization algorithm considering requirement changes. The proposed technique is suitable to be used as a preliminary testing when the information of the entire program is not in possession. We performed evaluation on random permutations and three textual comparison metrics and concluded the findings of the experiment.","","978-1-4673-8227-4978-1-4673-8226","10.1109/MySEC.2015.7475187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475187","textual comparison;test case prioritization;regression testing","Measurement;Testing;Silicon;Software;Context;Software engineering;Programming","program testing;regression analysis;systems analysis","test case prioritization;textual comparison metrics;regression testing;requirements changes;random permutations","","","9","","","","","","IEEE","IEEE Conferences"
"Modification Impact Analysis Based Test Case Prioritization for Regression Testing of Service-Oriented Workflow Applications","H. Wang; J. Xing; Q. Yang; D. Han; X. Zhang","NA; NA; NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","2","","288","297","Test case prioritization for regression testing is an approach that schedules test cases to improve the efficiency of service-oriented workflow application testing. Most of existing prioritization approaches range test cases according to various metrics (e.g., Statement coverage, path coverage) in different application context. Service-oriented workflow applications orchestrate web services to provide value-added service and typically are long-running and time-consuming processes. Therefore, these applications need more precise prioritization to execute earlier those test cases that may detect failures. Surprisingly, most of current regression test case prioritization researches neglect to use internal structure information of software, which is a significant factor influencing the prioritization of test cases. Considering the internal structure information and fault propagation behavior of modifications respect to modified version for service-oriented workflow applications, we present in this paper a new regression test case prioritization approach. Our prioritization approach schedules test cases based on dependence analysis of internal activities in service-oriented workflow applications. Experimental results show that test case prioritization using our approach is more effective than conventional coverage-based techniques.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273631","test case prioritization;dependence analysis;service-oriented workflow applications;modification impact","Testing;Correlation;Software;Synchronization;Schedules;Fault detection;Programmable logic arrays","program control structures;program testing;regression analysis;service-oriented architecture;Web services;workflow management software","modification impact analysis;service-oriented workflow applications;test case scheduling;service-oriented workflow application testing;statement coverage;path coverage;Web service orchestration;value-added service;failure detection;software internal structure information;fault propagation behavior;regression test case prioritization approach;dependence analysis","","1","37","","","","","","IEEE","IEEE Conferences"
"Bypassing Code Coverage Approximation Limitations via Effective Input-Based Randomized Test Case Prioritization","B. Jiang; W. K. Chan","NA; NA","2013 IEEE 37th Annual Computer Software and Applications Conference","","2013","","","190","199","Test case prioritization assigns the execution priorities of the test cases in a given test suite with the aim of achieving certain goals. Many existing test case prioritization techniques however assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in many software development projects. This paper proposes a novel family of LBS techniques. They make adaptive tree-based randomized explorations with an adaptive randomized candidate test set strategy to diversify the explorations among the branches of the exploration trees constructed by the test inputs in the test suite. They get rid of the assumption on the historical correlation of code coverage between program versions. Our techniques can be applied to programs with or without any previous versions, and hence are more general than many existing test case prioritization techniques. The empirical study on four popular UNIX utility benchmarks shows that, in terms of APFD, our LBS techniques can be as effective as some of the best code coverage-based greedy prioritization techniques ever proposed. We also show that they are significantly more efficient and scalable than the latter techniques.","0730-3157","978-0-7695-4986","10.1109/COMPSAC.2013.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649820","regression testing;adaptive test case prioritization","Testing;Subspace constraints;Silicon;Software;Approximation methods;History;Equations","approximation theory;greedy algorithms;program testing;software engineering;trees (mathematics)","code coverage approximation limitations;effective input-based randomized test case prioritization;full-fledged availability;software development projects;LBS techniques;adaptive tree-based randomized explorations;adaptive randomized candidate test set strategy;exploration trees;historical correlation;UNIX utility benchmarks;code coverage-based greedy prioritization techniques","","4","36","","","","","","IEEE","IEEE Conferences"
"EvoSuite at the SBST 2015 Tool Competition","G. Fraser; A. Arcuri","NA; NA","2015 IEEE/ACM 8th International Workshop on Search-Based Software Testing","","2015","","","25","27","EvoSuite is a mature research prototype that automatically generates unit tests for Java code. This paper summarizes the results and experiences of Evo Suite's participation at the third unit testing competition at SBST 2015. An unfortunate issue of conflicting dependency versions in two out of the nine benchmark projects reduced Evo Suite's overall score to 190.6, leading to the overall second rank.","","978-1-4673-7079","10.1109/SBST.2015.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7173586","test case generation;search-based testing;testing classes;search-based software engineering","Java;Software;Software testing;Benchmark testing;Conferences;Software engineering","benchmark testing;Java;program testing","EvoSuite;SBST-2015 tool competition;automatic unit test generation;Java code;third-unit testing competition;benchmark projects;search-based software testing","","","14","","","","","","IEEE","IEEE Conferences"
"Static analysis intermediate file analysis optimization strategy","Q. Deng; D. Jin","Institute of Network Technology, Beijing University of Posts and Telecommunications, Beijing, China; Institute of Network Technology, Beijing University of Posts and Telecommunications, Beijing, China","2015 8th International Conference on Biomedical Engineering and Informatics (BMEI)","","2015","","","705","709","With the rapid development of the software industry, the continuous increasing of software size and complexity, how to guarantee and improve the reliability and the safety of the software has become the focus in the research of computer science. Software testing has become an indispensable part in software's life circle and people are paying more attention to the software. The static analysis is a code analysis technology, which is opposite to the dynamic analysis. The former can detect the latent defects and the security vulnerabilities inside of software and is developing rapidly during the last years. This thesis is based on the C language as the research object, and focuses on generating intermediate files with compiler front-end. This thesis first introduces the principle of C preprocessor and the way of getting the redundant type of the intermediate files based on the file structure after the preprocessing. With the rational analysis, the redundant code in the intermediate files will be removed, and the simplification of the intermediate files are designed and realized, in order to increase the static analysis efficiency. Finally, we based on the software, defect detecting DTS (Defect Testing System) 1. We implement the file simplification module. DTS is used to validate some open source projects in order to prove the feasibility and effectiveness of our method.","","978-1-5090-0022-7978-1-5090-0021","10.1109/BMEI.2015.7401594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401594","Software security;Static detection;Intermediate file;Preprocessor","Software;Redundancy;Optimization;Software testing;Syntactics;Grammar","C language;computational complexity;optimisation;program compilers;program diagnostics;program testing;public domain software;software development management","open source projects;file simplification module;defect testing system;DTS;rational analysis;file structure;C preprocessor;compiler front-end;C language;security vulnerabilities;dynamic analysis;code analysis technology;software life circle;software testing;computer science;software complexity;software size;software industry;static analysis intermediate file analysis optimization strategy","","","12","","","","","","IEEE","IEEE Conferences"
"A Learning-to-Rank Approach to Software Defect Prediction","X. Yang; K. Tang; X. Yao","USTC-Birmingham Joint Research Institute in Intelligent Computation and Its Applications (UBRI), School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; USTC-Birmingham Joint Research Institute in Intelligent Computation and Its Applications (UBRI), School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; USTC-Birmingham Joint Research Institute in Intelligent Computation and Its Applications (UBRI), School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China","IEEE Transactions on Reliability","","2015","64","1","234","246","Software defect prediction can help to allocate testing resources efficiently through ranking software modules according to their defects. Existing software defect prediction models that are optimized to predict explicitly the number of defects in a software module might fail to give an accurate order because it is very difficult to predict the exact number of defects in a software module due to noisy data. This paper introduces a learning-to-rank approach to construct software defect prediction models by directly optimizing the ranking performance. In this paper, we build on our previous work, and further study whether the idea of directly optimizing the model performance measure can benefit software defect prediction model construction. The work includes two aspects: one is a novel application of the learning-to-rank approach to real-world data sets for software defect prediction, and the other is a comprehensive evaluation and comparison of the learning-to-rank method against other algorithms that have been used for predicting the order of software modules according to the predicted number of defects. Our empirical studies demonstrate the effectiveness of directly optimizing the model performance measure for the learning-to-rank approach to construct defect prediction models for the ranking task.","0018-9529;1558-1721","","10.1109/TR.2014.2370891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996020","Software defect prediction;learning-to-rank;software metrics;count models;metric selection","Software;Predictive models;Data models;Software metrics;Testing;Radio frequency","learning (artificial intelligence);optimisation;program testing;resource allocation;software fault tolerance","learning-to-rank approach;software defect prediction;SDP;testing resource allocation;model performance optimization;software module","","23","30","","","","","","IEEE","IEEE Journals & Magazines"
"Prioritization of Unit Testing on non-object oriented using a top-down based approach","A. Kheirkhah; S. Mohd Daud","Advanced Informatics School, Universiti Teknologi Malaysia, Jalan Semarak, 54100, Kuala Lumpur, Malaysia; Advanced Informatics School, Universiti Teknologi Malaysia, Jalan Semarak, 54100, Kuala Lumpur, Malaysia","2014 8th. Malaysian Software Engineering Conference (MySEC)","","2014","","","72","77","The issue which makes Unit Testing so tough is the ambiguous ways that the software world keeps moving forward. Although sometimes by implementing simple unit testing methods, this task become easy to handle. However to achieve a comprehensive unit testing it is supposed to test all corners of software such as database, devices, communication etc. This paper proposes an orthogonal software testing approach based on Top-Down technique which treats the input parameters of a software unit in an orthogonal partitioning to dynamic level of testing. Describes how test cases are statistically for each trial of software testing steps and makes a dynamic partitioning approach on non-object oriented experiments. The adequacy of the generated test cases can be validated by examining testing coverage metrics. The authors have considered using of different partitioning and mock objects help to make an isolated testing, improve code's structure and automated testing possibility. The results of the test case executions can be analyzed in order to find the “IF” metrics for partitioning the traceable ways and detecting defects, to generate more effective test cases in future testing, and to help locate and correct defects in the early stage of testing.","","978-1-4799-5439","10.1109/MySec.2014.6985991","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6985991","Top-Down testing;software unit testing;Non-Object Oriented;CFG;P-Nodes","Software;Electronic mail;Databases;Software testing;Measurement;Partitioning algorithms","program testing;software metrics","unit testing prioritization;top-down technique;software testing;software unit input parameters;dynamic partitioning;nonobject oriented experiments;testing coverage metrics","","","20","","","","","","IEEE","IEEE Conferences"
"Supporting the Transition to an Agile Test Matrix","R. Korosec; R. Pfarrhofer","NA; NA","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","","2015","","","1","2","The transition of AVL's software development unit towards lean and agile practices on team and enterprise level (implementing the scaled agile framework SAFe) requires a change in testing role, set and practices. We describe the changes of the testing strategy in reference to the agile test matrix - moving the focus of testing from system acceptance tests towards functional and unit tests. Furthermore, a supporting automated testing procedure was adopted to enable the splitting of tasks between different, globally distributed teams. With a test distribution tool, we are optimizing test execution time and test resource usage to meet the needs of the short agile cadence. The lessons learned so far during this ongoing project of tool implementation are shared. We conclude with an outlook on a research project that examines ways of systematic testing of nonfunctional requirements.","2159-4848","978-1-4799-7125","10.1109/ICST.2015.7102632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102632","","Testing;Software;Automation;Mechanical power transmission;Organizations;Debugging;Runtime","program testing;software prototyping;team working","agile test matrix;AVL;software development unit;lean practices;agile practices;team level;enterprise level;scaled agile framework;SAFe;testing strategy;system acceptance tests;functional tests;unit tests;automated testing procedure;globally distributed teams;test distribution tool;test execution time optimization;test resource usage;agile cadence;nonfunctional requirements","","1","10","","","","","","IEEE","IEEE Conferences"
"Bypassing the Combinatorial Explosion: Using Similarity to Generate and Prioritize T-Wise Test Configurations for Software Product Lines","C. Henard; M. Papadakis; G. Perrouin; J. Klein; P. Heymans; Y. Le Traon","Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg","IEEE Transactions on Software Engineering","","2014","40","7","650","670","Large Software Product Lines (SPLs) are common in industry, thus introducing the need of practical solutions to test them. To this end, t-wise can help to drastically reduce the number of product configurations to test. Current t-wise approaches for SPLs are restricted to small values of t. In addition, these techniques fail at providing means to finely control the configuration process. In view of this, means for automatically generating and prioritizing product configurations for large SPLs are required. This paper proposes (a) a search-based approach capable of generating product configurations for large SPLs, forming a scalable and flexible alternative to current techniques and (b) prioritization algorithms for any set of product configurations. Both these techniques employ a similarity heuristic. The ability of the proposed techniques is assessed in an empirical study through a comparison with state of the art tools. The comparison focuses on both the product configuration generation and the prioritization aspects. The results demonstrate that existing t-wise tools and prioritization techniques fail to handle large SPLs. On the contrary, the proposed techniques are both effective and scalable. Additionally, the experiments show that the similarity heuristic can be used as a viable alternative to t-wise.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2327020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823132","Software product lines;testing;T-wise Interactions;search-based approaches;prioritization;similarity","Testing;Frequency modulation;Context;Scalability;Software;Linux;Arrays","combinatorial mathematics;program testing;software product lines","combinatorial explosion;test configurations;software product lines;SPL;product configurations;configuration process;search based approach;similarity heuristic;product configuration generation","","53","64","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluation and Application of Bounded Generalized Pareto Analysis to Fault Distributions in Open Source Software","C. Huang; C. Kuo; S. Luan","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","IEEE Transactions on Reliability","","2014","63","1","309","319","In general, one of the most important aspects of software development and project management is how to make predictions and assessments of quality and reliability for developed products. Project data usually will be systematically collected and analyzed during the process of software development. Practically, it would be helpful if developers could identify the most error-prone modules early so that they can optimize testing-resource allocation and increase fault detection effectiveness accordingly. In the past, many research studies revealed the applicability of the Pareto principle to software systems, and some of them reported that the Pareto distribution (PD) model can be used to predict the fault distribution of software. In this paper, a special form of the Generalized PD model, named the Bounded Generalized Pareto distribution (BGPD) model, is further proposed to investigate the fault distributions of Open Source Software (OSS). It can be seen that the BGPD model eliminates the issue which occurred in the classical PD model. Three methods of parameter estimation will be presented, and related experiments are performed based on real OSS failure data. Experimental results show that the BGPD model presents high fitness to the actual failure data of OSS. Finally, the possibility of using early limited fault data to predict the later software fault distribution is also studied. Numerical results indicate that the BGPD model can be trusted to consistently produce accurate estimates of fault predictions during the early stages of development. The findings can provide an effective foundation for managing the necessary activities of software development and testing.","0018-9529;1558-1721","","10.1109/TR.2013.2285056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6631477","Fault distribution;open source software;Pareto principle;software quality;software reliability;software testing;Weibull model","Predictive models;Mathematical model;Data models;Open source software;Cascading style sheets;Software reliability","Pareto distribution;Pareto optimisation;program testing;project management;public domain software;resource allocation;software fault tolerance;software management;software quality","bounded generalized Pareto analysis application;bounded generalized Pareto analysis evaluation;open source software;software project management;software product reliability assessment;software product quality assessment;project data collection;project data analysis;software development process;error-prone module identification;testing-resource allocation optimization;fault detection;software systems;generalized PD model;bounded generalized Pareto distribution model;BGPD model;parameter estimation;OSS failure data;software fault distribution;software testing","","4","41","","","","","","IEEE","IEEE Journals & Magazines"
"Dynamic test case prioritization based on multi-objective","X. Wang; H. Zeng","School of Computer Engineering and Science, Shanghai University Shanghai 200444, China; School of Computer Engineering and Science, Shanghai University Shanghai 200444, China","15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","","2014","","","1","6","Test case prioritization technology is to sort the test cases before the software testing designed to improve test efficiency. This paper presents a dynamic test case prioritization technique based on multi-objective. It integrates several traditional single-objective technologies so that makes it more flexible. This technology, from five dimensions, calculates prioritization values of test cases separately. Then a weighted sum is made to the values and it sorts the test cases according to the values. The results return to the storage in order to dynamically adjust the sort of test cases. This technology not only meets the high demands of regression testing, but also ensures the high efficiency of the test results.","","978-1-4799-5604","10.1109/SNPD.2014.6888744","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888744","Test case prioritization;Multi-objective;Dynamic;Regression testing","History;Testing;Software;Fault detection;Databases;Measurement;Probability","program testing;regression analysis","dynamic test case prioritization;multiobjective;test case prioritization technology;software testing;test efficiency;single-objective technologies;weighted sum;regression testing","","1","15","","","","","","IEEE","IEEE Conferences"
"An assimilated approach of concept analysis and particle swarm optimization algorithm for effective test suite minimization","S. Selvakumar; T. Manikumar; A. J. S. Kumar; L. Latha","Department of Computer Science &amp; Engineering, GKM College of Engineering &amp; Technology, Chennai, India; Department of Computer Applications, RVS College of Engineering &amp; Technology, Dindigul, India; Department of Computer Application, Thiagarajar College of Engineering, Madurai - 625015, India; Department of Computer Science, Research and Development Centre, Bharathiar University, Coimbatore, India","2013 IEEE International Conference on Computational Intelligence and Computing Research","","2013","","","1","4","In most of the test suite minimization techniques, either the size minimization is more or the fault detection is more. But a combination of both would yield better qualified reduced test suite. This paper presents a technique where the size minimization is obtained through the optimization algorithm, Particle Swarm Optimization and the Fault Detection Effectiveness is obtained through Concept Analysis. In spite of our algorithm producing results similar to Genetic Algorithm, the computation time of our algorithm is simple and improves the fault detection capacity. The experimental results indicate that PSO outperforms GAs for most code elements to be covered in terms of effectiveness and efficiency.","","978-1-4799-1597-2978-1-4799-1594-1978-1-4799-1595","10.1109/ICCIC.2013.6724231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724231","Test suite minimization;concept analysis;Particle Swarm Optimization Algorithm;Empirical analysis","Minimization;Fault detection;Genetic algorithms;Software;Algorithm design and analysis;Particle swarm optimization;Testing","minimisation;particle swarm optimisation;program testing","particle swarm optimization algorithm;test suite minimization techniques;size minimization;fault detection effectiveness;concept analysis;fault detection capacity;PSO;software testing","","1","11","","","","","","IEEE","IEEE Conferences"
"Supporting software product line testing by optimizing code configuration coverage","L. Vidács; F. Horváth; J. Mihalicza; B. Vancsics; À. Beszédes","MTA-SZTE Research Group on Artificial Intelligence, Szeged, Hungary; University of Szeged, Szeged, Hungary; NNG LLC, Budapest, Hungary; University of Szeged, Szeged, Hungary; University of Szeged, Szeged, Hungary","2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","2015","","","1","7","Software product lines achieve much shorter time to market by system level reuse and code variability. A possible way to achieve this flexibility is to use generic components, including the core system, in different products in alternative configurations. The focus of testing efforts for such complex and highly variable systems often shifts from testing specific products to assessing the overall quality of the core system or potential new configurations. As a complementary approach to feature models and related combinatorial testing methods optimizing for feature coverage, we apply a source code oriented analysis of variability. We present two algorithms that optimize for high coverage of the common code base in terms of C++ preprocessor-based configurations with a limited set of actual configurations selected for testing. The methods have been evaluated on iGO Navigation, a large industrial system with typical configuration support for product lines, hence we believe the approach can be generalized to other systems as well.","","978-1-4799-1885","10.1109/ICSTW.2015.7107478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7107478","Variability;Preprocessor;Configurations;Software Product Line;White box testing","Testing;Navigation;Algorithm design and analysis;Software;Frequency modulation;Electronic mail;Analytical models","C++ language;program testing;software product lines;source code (software)","iGO navigation;C++ preprocessor-based configuration;source code oriented analysis;combinatorial testing method;core system quality;code variability;code configuration coverage optimization;software product line testing","","1","33","","","","","","IEEE","IEEE Conferences"
"Coverage-Based Test Case Prioritisation: An Industrial Case Study","D. Di Nardo; N. Alshahwan; L. Briand; Y. Labiche","NA; NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation","","2013","","","302","311","This paper presents an industrial case study of coverage-based prioritisation techniques on a real world system with real regression faults. The study evaluates four common and different test case prioritisation techniques and examines the effects of using various coverage criteria on the fault detection rates of the prioritised test suites. The results show that prioritisation techniques that are based on additional coverage with finer grained coverage criteria perform significantly better in fault detection rates. The study also reveals that using modification information does not significantly enhance fault detection rates.","2159-4848","978-0-7695-4968-2978-1-4673-5961","10.1109/ICST.2013.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569742","regression testing;industrial case study;test case prioritisation","Testing;Fault detection;Measurement;Software;Computer aided software engineering;Minimization;Data collection","program testing;regression analysis;software fault tolerance","coverage-based test case prioritisation;industrial case study;real world system;regression fault;coverage criteria;fault detection rate","","9","22","","","","","","IEEE","IEEE Conferences"
"PORA: Proportion-Oriented Randomized Algorithm for Test Case Prioritization","B. Jiang; W. K. Chan; T. H. Tse","NA; NA; NA","2015 IEEE International Conference on Software Quality, Reliability and Security","","2015","","","131","140","Effective testing is essential for assuring software quality. While regression testing is time-consuming, the fault detection capability may be compromised if some test cases are discarded. Test case prioritization is a viable solution. To the best of our knowledge, the most effective test case prioritization approach is still the additional greedy algorithm, and existing search-based algorithms have been shown to be visually less effective than the former algorithms in previous empirical studies. This paper proposes a novel Proportion-Oriented Randomized Algorithm (PORA) for test case prioritization. PORA guides test case prioritization by optimizing the distance between the prioritized test suite and a hierarchy of distributions of test input data. Our experiment shows that PORA test case prioritization techniques are as effective as, if not more effective than, the total greedy, additional greedy, and ART techniques, which use code coverage information. Moreover, the experiment shows that PORA techniques are more stable in effectiveness than the others.","","978-1-4673-7989-2978-1-4673-7988","10.1109/QRS.2015.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272924","Test case prioritization;randomized algorithm;proportional sampling strategy;multi-objective optimization","Testing;Subspace constraints;Fault detection;Greedy algorithms;Resource management;Clustering algorithms;Genetic algorithms","greedy algorithms;program testing;regression analysis;software fault tolerance;software quality","proportion-oriented randomized algorithm;test case prioritization;software quality;regression testing;fault detection capability;greedy algorithm;search-based algorithms","","1","36","","","","","","IEEE","IEEE Conferences"
"GUI Test Case Prioritization by State-Coverage Criterion","Z. He; C. Bai","NA; NA","2015 IEEE/ACM 10th International Workshop on Automation of Software Test","","2015","","","18","22","Graphical User Interface (GUI) application is a kind of typical event-driven software (EDS) that transforms state according to input events invoked through a user interface. It is time consuming to test a GUI application since there are a large number of possible event sequences generated by the permutations and combinations of user operations. Although some GUI test case prioritization techniques have been proposed to determine ""which test case to execute next"" for early fault detection, most of them use random ordering to break tie cases, which has been proved to be ineffective. Recent research presents the opinion that using hybrid criteria can be an effective way for tie-breaking, but few studies focus on seeking a new criterion cooperating well with other criteria when breaking tie cases. In this paper, we propose a state-distance-based method using state coverage as a new criterion to prioritize GUI test cases. An empirical study on three GUI programs reveals that the state-distance-based method is really suitable for GUI test case prioritization and can cooperate well with the (additional) event length criterion.","","978-1-4673-7022","10.1109/AST.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166260","GUI testing;event-driven software;test case prioritization;GUI state similarity","Graphical user interfaces;Testing;Software;Fault detection;Software engineering;Measurement;Conferences","graphical user interfaces","GUI test case prioritization;state coverage criterion;graphical user interface;event-driven software;EDS;GUI application;fault detection;state distance based method;GUI programs","","2","11","","","","","","IEEE","IEEE Conferences"
"On the Gain of Measuring Test Case Prioritization","J. Lv; B. Yin; K. Cai","NA; NA; NA","2013 IEEE 37th Annual Computer Software and Applications Conference","","2013","","","627","632","Test case prioritization (TCP) techniques aim to schedule the order of regression test suite to maximize some properties, such as early fault detection. In order to measure the abilities of different TCP techniques for early fault detection, a metric named average percentage of faults detected (APFD) is widely adopted. In this paper, we analyze the metric APFD and explore the gain of measuring TCP techniques from a control theory viewpoint. Based on that, we propose a generalized metric for TCP. This new metric focuses on the gain of defining early fault detection and measuring TCP techniques for various needs in different evaluation scenarios. By adopting this new metric, not only flexibility can be guaranteed, but also explicit physical significance for the metric will be provided before evaluation.","0730-3157","978-0-7695-4986","10.1109/COMPSAC.2013.101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649891","regression testing;test case prioritization;software metric;software cybernetics","Fault detection;Testing;Gain measurement;Software;Weight measurement;Approximation methods","program testing;software metrics","test case prioritization techniques;regression test suite;TCP techniques;average percentage of faults detected;APFD;control theory viewpoint","","","15","","","","","","IEEE","IEEE Conferences"
"Regression Testing Approach for Large-Scale Systems","P. Kandil; S. Moussa; N. Badr","NA; NA; NA","2014 IEEE International Symposium on Software Reliability Engineering Workshops","","2014","","","132","133","Regression testing is an important and expensive activity that is undertaken every time a program is modified to ensure that the changes do not introduce new bugs into previously validated code. Instead of re-running all test cases, different approaches were studied to solve regression testing problems. Data mining techniques are introduced to solve regression testing problems with large-scale systems containing huge sets of test cases, as different data mining techniques were studied to group test cases with similar features. Dealing with groups of test cases instead of each test case separately helped to solve regression testing scalability issues. In this paper, we propose a new methodology for regression testing of large-scale systems using data mining techniques to prioritize and select test cases based on their coverage criteria and fault history.","","978-1-4799-7377","10.1109/ISSREW.2014.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983822","Regression Testing;Data Mining;Large Scale System;Test Cases Prioritization;Test Cases Selection","Data mining;Large-scale systems;History;Software;Software testing;Conferences","data mining;large-scale systems;program testing","regression testing approach;large-scale systems;validated code;regression testing problem;data mining techniques;group test cases;regression testing scalability;coverage criteria;fault history","","","10","","","","","","IEEE","IEEE Conferences"
"Automated test data generation using computational intelligence","S. Dixit; P. Tomar","Department of Computer Science, School of ICT, Gautam Buddha University, Greater Noida, Uttar Pradesh, India; Department of Computer Science, School of ICT, Gautam Buddha University, Greater Noida, Uttar Pradesh, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","4","This paper evaluates performance of Genetic Algorithm (GA) & Particle Swarm Optimization (PSO) and henceforth derive an algorithm Genetic Particle Swarm Hybrid Algorithm (GPSHA) which is combination of these techniques. The comparison of the algorithms' performance is done, to prove the efficiency of the GPSHA over GA and PSO, based on the number of generations and uniqueness of the test cases that are generated.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359319","computational intelligence;genetic algorithm;particle swarm optimization;software testing;GPSHA","Genetic algorithms;Sociology;Statistics;Software;Biological cells;Particle swarm optimization;Algorithm design and analysis","genetic algorithms;particle swarm optimisation;program testing","efficiency analysis;GPSHA algorithm;genetic particle swarm hybrid algorithm;PSO;particle swarm optimization;GA;genetic algorithm;performance evaluation;computational intelligence;automated test data generation","","2","7","","","","","","IEEE","IEEE Conferences"
"AFIT's laboratory test equipment to optimise the integrated communication systems for polish military helicopters","A. Pazur; A. Szelmanowski; S. Michalak","Air Force Institute of Technology, Warsaw, Poland; Air Force Institute of Technology, Warsaw, Poland; Air Force Institute of Technology, Warsaw, Poland","2014 IEEE Metrology for Aerospace (MetroAeroSpace)","","2014","","","358","361","The paper has been intended to present a research/testing tool used in the Air Force Institute of Technology (AFIT) to build, actuate, test, and optimise integrated communication systems as far as both a set of devices the system is composed of and the applied software are concerned. Particular attention has been paid to the so-called integration station (built under the Mi-8, Mi-17, Mi-24 upgrade project), i.e. the laboratory equipment to optimise and unify communication systems integrated on the basis of digital data buses (following the MIL-1553B standard, among other ones). Such equipment has allowed AFIT to integrate new communication devices/systems while upgrading the W-3PL helicopter. Some selected tasks performed with this equipment engaged have been discussed. Also, problems arising while actuating and testing the developed software to integrate communication devices/systems (including digitally controlled radio stations of the RRC, HARRIS, and MR6000 types, communication control panels of the PSŁ-1 type and multifunction displays of the MW-1 type) have been given consideration in the scope of the software functionality and reliability. Presented are also additional monitoring and measuring systems used to test this software, just to mention the M230 rugged laptop computer used to diagnose the system and prepare plans of the radio communication.","","978-1-4799-2069","10.1109/MetroAeroSpace.2014.6865949","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6865949","communication systems;measuring equipment;optimization of functionality","Helicopters;Military aircraft;Servers;Software;Military communication;Testing","aircraft communication;aircraft testing;helicopters;military aircraft;military communication;military computing;program testing;software reliability;telecommunication computing;test equipment","AFIT laboratory test equipment;integrated communication systems;Polish military helicopters;Air Force Institute of Technology;research-testing tool;applied software;integration station;digital data buses;W-3PL helicopter;software functionality;software reliability;measuring systems;monitoring systems;software testing;M230 rugged laptop computer;radio communication","","","6","","","","","","IEEE","IEEE Conferences"
"A Software Cost Model Considering the Difference between Testing Environment and Operating Environment","F. Gao; C. Liu; B. Meng; S. Chen","NA; NA; NA; NA","2014 Seventh International Joint Conference on Computational Sciences and Optimization","","2014","","","45","50","A difference exists between the software testing environment and operating environment, which has a crucial impact on the software reliability assessment, testing costs and optimal release time. The software cost model is an effective tool used to help software developers control software costs and determine the release time. However, the current studies have not taken the difference of the environment into account, and developers cannot predict the costs and release time accurately. The impact of the environment difference on costs is discussed firstly, and a cost model is established considering the difference between the testing and operating environment. Then we investigate the influence on the costs of software testing when environmental changes. Experimental results show that the cost model proposed here can predict more accurately and be accordant with the practical situation of the software testing.","","978-1-4799-5372-1978-1-4799-5371","10.1109/CSO.2014.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923633","software testing;the difference of environment;cost model;optimal release time","Software;Software testing;Environmental factors;Fault detection;Software reliability;Predictive models","program testing;software cost estimation;software reliability","software cost model;testing environment;operating environment;software reliability assessment;testing costs;software testing","","","10","","","","","","IEEE","IEEE Conferences"
"Computational intelligence and safe reduction of test suite","A. A. Haider; A. Nadeem; S. Rafiq","Center for Software Dependability, Mohammad Ali Jinnah University (MAJU), Islamabad, Pakistan; Center for Software Dependability, Mohammad Ali Jinnah University (MAJU), Islamabad, Pakistan; Center for Software Dependability, Mohammad Ali Jinnah University (MAJU), Islamabad, Pakistan","2013 IEEE 9th International Conference on Emerging Technologies (ICET)","","2013","","","1","6","Systems are frequently regression tested during the maintenance phase due to corrective, preventive, adaptive or perfective actions. Regression testing is used to prevent the undesirable effects of these changes on the previously tested version. Due to these changes, new test cases become part of the test suite making it huge and inefficient for `retest all' strategy. The ultimate solution of this problem is optimization or reduction of the test suite. Computational Intelligence (CI) based approaches like evolutionary computation, fuzzy logic, neural networks and swarm optimization have been used for test suite reduction. Optimization approaches reduce the test suite by compromising its safety. Ideally optimization of test suite must guarantee safe reduction. In this work, we have optimized the test suite using some CI based approaches and then analysed the test suite for `safe reduction'. Safe reduction can be gauged using control flow graphs. Test cases of optimal solutions were traversed on these graphs. We found that these solutions partially cover control flow graph. This showed that optimal solutions returned by CI based approaches except fuzzy logic are not safe and will be inadequate for regression testing.","","978-1-4799-3457-7978-1-4799-3456","10.1109/ICET.2013.6743502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6743502","test suite optimization;regression testing;multi objective optimization;safe reduction;fuzzy logic;evolutionary algorithms;swarm optimization;computational intelligence","Optimization;Testing;Genetic algorithms;Particle swarm optimization;Fuzzy logic;Neural networks","fuzzy logic;neural nets;particle swarm optimisation;program testing;regression analysis;software maintenance","test suite reduction;swarm optimization;neural networks;fuzzy logic;evolutionary computation;regression testing;maintenance phase;safe reduction;computational intelligence","","2","40","","","","","","IEEE","IEEE Conferences"
"Constraints: The Future of Combinatorial Interaction Testing","J. Petke","NA","2015 IEEE/ACM 8th International Workshop on Search-Based Software Testing","","2015","","","17","18","Combinatorial Interaction Testing (CIT) has gained a lot of attention in the area of software engineering in the last few years. CIT problems have their roots in combinatorics. Mathematicians have been concerned with the NP-complete problem of finding minimal covering arrays (in other words, minimal CIT test suites) since early nineties. With the adoption of these techniques into the area of software testing, an important gap has been identified - namely consideration of real-world constraints. We show that indeed finding an efficient way of handling constraints during search is the key factor in wider applicability of CIT techniques.","","978-1-4673-7079","10.1109/SBST.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7173584","","Software;Software testing;Explosions;Browsers;Software product lines","combinatorial mathematics;optimisation;program testing","combinatorial interaction testing;software engineering;NP-complete problem;minimal covering arrays;minimal CIT test suites;software testing;real-world constraints;constraint handling","","3","20","","","","","","IEEE","IEEE Conferences"
"Prioritization and ranking of ERP testing components","S. Nagpal; S. K. Khatri; P. K. Kapur","Amity Institute of Information Technology, Amity University Uttar Pradesh, India; Amity Institute of Information Technology, Amity University Uttar Pradesh, India; Centre for Interdisciplinary Research, Amity University Uttar Pradesh, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","Software Testing is one of the most important activities but more often than not attracts less attention than it deserves in software development and implementation. It often takes twenty to sometimes even more than fifty percent of the total software development time. Enterprise Resource Planning (ERP) Systems provide synergy by integrating all operations of an enterprise. So implementation of ERP systems need even more rigorous testing than that employed in stand-alone software development. Software testing is a well-researched area but software testing as employed on ERP systems albeit is droughted with respect to research. This research paper is an extension of the patent by Kapur et al.[8] that identified the ERP Testing Components to measure ERP testing efficiency. Here, the ERP Testing Components have been accumulated and categorized under five heads. Thereafter, these testing components have been prioritized and ranked with the help of Analytic Hierarchy Process (AHP), as given by Saaty [17].","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359245","Enterprise Resource Planning (ERP);Critical Success Factors (CSF);ERP Testing Components;Analytic Hierarchy Process (AHP)","Testing;Software;Complexity theory;Planning;Analytic hierarchy process;Organizations","analytic hierarchy process;enterprise resource planning;program testing;software engineering","software testing;software development;enterprise resource planning system;ERP system;ERP testing component;analytic hierarchy process;AHP","","1","17","","","","","","IEEE","IEEE Conferences"
"An Experimental Protocol for Analyzing the Accuracy of Software Error Impact Analysis","V. Musco; M. Monperrus; P. Preux","NA; NA; NA","2015 IEEE/ACM 10th International Workshop on Automation of Software Test","","2015","","","60","64","In software engineering, error impact analysis consists in predicting the software elements (e.g. Modules, classes, methods) potentially impacted by a change. Impact analysis is required to optimize the testing effort. In this paper we present a new protocol to analyze the accuracy of impact analysis. This protocol uses mutation testing to simulate changes that introduce errors. To this end, we introduce a variant of call graphs we name the ""use graph"" of a software which may be computed efficiently. We apply this protocol to two open-source projects and correctly predict the impact of 30% to 49% of changes.","","978-1-4673-7022","10.1109/AST.2015.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166269","","Protocols;Accuracy;Java;Software packages;Testing;Open source software","graph theory;program testing;public domain software;software fault tolerance","experimental protocol;software error impact analysis;software engineering;software elements;mutation testing;call graphs;software use graph;open-source projects","","","5","","","","","","IEEE","IEEE Conferences"
"Software release time problem with learning function under fuzzy environment","S. G. Sharma; D. Kumar; P. K. Kapur","Amity University, Noida, India; Amity University, Noida, India; Amity University, Noida, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","5","Cost, quality and target are the three challenges faced during the software development process. All the Software organizations aim at timely release of the software. The optimal release time is dependent on several factors such as level of reliability, quality, manpower, cost etc. These factors don't have precise or crisp values and thus require fuzzy techniques to optimize the release time. This paper focuses on finding the optimum release time of the software in fuzzy environment. The SRGM with learning function is used to optimize the release time. Results are illustrated numerically and based on fuzzy optimization model we can easily address the problem of timely release of software and give the optimal time of release.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014680","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014680","Software Reliability Growth model(SRGM);fuzzy optimization;Release time;Learning function;Cost;Membership Function","Software;Software reliability;Testing;Mathematical programming;Debugging","fuzzy set theory;software quality;software reliability","software release time problem;learning function;software development process;software organizations;fuzzy techniques;SRGM;fuzzy optimization model;software reliability;software quality","","","12","","","","","","IEEE","IEEE Conferences"
"Reusing black box test paths for white box testing of websites","R. Chopra; S. Madan","Computer Science Engg./ IT, GTBIT, GGSIPU DELHI, India; Computer Science Department, University of Delhi, India","2013 3rd IEEE International Advance Computing Conference (IACC)","","2013","","","1345","1350","As the numbers of web users are increasing exponentially, the software complexity is increasing exponentially and the malwares are increasing exponentially, so exhaustive and extensive testing of websites has become a necessity today. But testing of a website is not 100% exhaustive as the page explosion problem is also very usual. In this paper, we propose to reuse the basis test paths as obtained from the Page-Test-Trees (PTTs) for white box testing of websites. We traverse the same set of paths (obtained above) and test for the source code at these nodes. This saves significant amount of time required to generate test paths and hence test cases as compared to the existing approaches of white box testing. The cost and efforts are also minimized. The proposed technique ensures better website testing coverage as white box testing provides better results than black box testing. Then we validate the proposed reusability testing with two web navigational structures. The results show that doing regression testing can save several billion dollars. These test cases can be further minimized by using prioritization techniques of regression testing.","","978-1-4673-4529-3978-1-4673-4527-9978-1-4673-4528","10.1109/IAdCC.2013.6514424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6514424","Website testing;Website-Under-Test (WUT);Page Flow Diagrams;Navigation;Page test Tree;Path Testing and Regression testing","Testing;Phase frequency detector;Navigation;Complexity theory;Explosions;Conferences;Software","invasive software;program testing;regression analysis;statistical testing;Web sites","black box test path;white box testing;Web site;software complexity;malware;page explosion problem;page-test-trees;PTT;test path generation;Web site testing coverage;reusability testing;prioritization technique;regression testing","","","24","","","","","","IEEE","IEEE Conferences"
"Using Partition Information to Prioritize Test Cases for Fault Localization","X. Zhang; D. Towey; T. Y. Chen; Z. Zheng; K. Cai","NA; NA; NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","2","","121","126","Fault Localization Prioritization (FLP) aims at reordering existing test cases so that the location of detected faulty components can be identified earlier, using certain fault localization techniques. Although some researchers have proposed adaptive prioritization strategies with white-box code coverage information, such information may not always be available. In this paper, we address the FLP problem using black-box information derived from partitioning the input domain. Based on the well-known technique of Spectra-Based Fault Localization (SBFL), three test case prioritization strategies are designed following some basic SBFL heuristics. The implementation of these proposed strategies relies only on the partition information, and does not require any test case execution history. Experiments show that our strategies, when compared with pure random selection, result in a faster localization of faulty statements, reducing the number of test case executions required. Here, we analyze the characteristics and merits of the three proposed strategies.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273609","software fault localization;test case prioritization;adaptive strategies","Computers;Software;Conferences","program testing;software fault tolerance","partition information;fault localization prioritization;FLP;test case reordering;adaptive prioritization;white-box code coverage information;black-box information;spectra-based fault localization;test case prioritization strategies;SBFL heuristics","","5","12","","","","","","IEEE","IEEE Conferences"
"BEN: A combinatorial testing-based fault localization tool","L. S. Ghandehari; J. Chandrasekaran; Y. Lei; R. Kacker; D. R. Kuhn","Dept. of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, USA; Dept. of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, USA; Dept. of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, USA; Information Technology Lab, National Institute of Standards and Technology, Gaithersburg, MD, USA; Information Technology Lab, National Institute of Standards and Technology, Gaithersburg, MD, USA","2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","2015","","","1","4","We present a combinatorial testing-based fault localization tool called BEN. BEN takes as input three types of information, including the subject program, the source code, an input parameter model, and a combinatorial test set created based on the input parameter model. It is assumed that the combinatorial test set has already been executed, and thus the execution status of each test is known. The output of BEN is a ranking of statements in terms of their likelihood to be faulty. In the fault localization process, a small number of additional tests are generated by BEN and need to be executed by the user. In this paper, we present the major user scenarios and the highlevel design of BEN. BEN is implemented in Java and provides a graphical user interface that provides friendly access to the tool.","","978-1-4799-1885","10.1109/ICSTW.2015.7107446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7107446","BEN;Fault Localization;Combinatorial Testing","Conferences;NIST;Software testing;Fault diagnosis;Software engineering;Graphical user interfaces","combinatorial mathematics;program testing;software engineering;source code (software)","BEN;combinatorial testing-based fault localization tool;subject program;source code;input parameter model;combinatorial test set;Java","","5","14","","","","","","IEEE","IEEE Conferences"
"An Information Retrieval Approach for Regression Test Prioritization Based on Program Changes","R. K. Saha; L. Zhang; S. Khurshid; D. E. Perry","NA; NA; NA; NA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","1","","268","279","Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight open-source Java projects shows that REPiR is computationally efficient and performs better than existing (dynamic or static) techniques for the majority of subject systems.","0270-5257;1558-1225","978-1-4799-1934","10.1109/ICSE.2015.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194580","Regression Testing;Test Prioritization;Information Retrieval","Software engineering;Information retrieval;Testing;Standards;Open source software;Natural languages","information retrieval;program debugging;program testing;public domain software;system monitoring","information retrieval approach;regression test prioritization;program changes;regression testing;regression suites;bugs;dynamic analysis;REPiR;open-source IR toolkit Indri;open-source Java projects","","12","67","","","","","","IEEE","IEEE Conferences"
"Improving reliability using software operational profile and testing profile","M. M. Ali-Shahid; S. Sulaiman","Faculty of Computing, Universiti Teknologi Malaysia, 81310 UTM, Skudai, Johor, Malaysia; Faculty of Computing, Universiti Teknologi Malaysia, 81310 UTM, Skudai, Johor, Malaysia","2015 International Conference on Computer, Communications, and Control Technology (I4CT)","","2015","","","384","388","Software testing has ever remained a challenge particularly when testing is done with intention in enhancing the reliability. Conventional testing is increasing the testing in an unpredictable way by reducing the number of faults. There is a need to enhance the reliability by assigning probabilistic priorities to testing mechanism, which is done through software operational profile. This study adopts a case study to generate test cases and test suites with perspective of probabilistic reliability using the proposed framework based on software operational profile and testing profile.","","978-1-4799-7952-3978-1-4799-7951","10.1109/I4CT.2015.7219603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219603","Software Operational Profile (SOP);Software Reliability Engineering (SRE);Testing Profile (TP);Software Testing","Testing;Software reliability;Probability;Optical character recognition software;Conferences","program testing;software reliability","software operational profile;testing profile;software testing;test case generation;test suites;probabilistic reliability","","1","22","","","","","","IEEE","IEEE Conferences"
"Kansei Engineering system for test generation using multi-objective optimization","K. Choudhary; Shilpa","Dept. of CSE &amp; IT, ITM University, Gurgaon, INDIA; ITM University, Gurgaon, INDIA","2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)","","2015","","","125","129","Software Quality plays a critical role in software development. This paper emphasize on involvement of Kansei Engineering in the field of test case generation. Multi-objective genetic algorithm under multi-objective optimization methodology is used to generate test cases covering Boolean specific conditions.","","978-9-3805-4416-8978-9-3805-4415-1978-9-3805-4414","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100234","Kansei engineering;Multi-objective optimization;Multi-objective genetic algorithm;Pareto-optimal;Boolean-specific conditions etc","Optimization;Testing;Genetic algorithms;Software;Evolutionary computation;Linear programming;Conferences","genetic algorithms;program testing;software quality","Kansei engineering system;software quality;software development;test case generation;multi objective genetic algorithm;multi objective optimization methodology;Boolean specific conditions","","","13","","","","","","IEEE","IEEE Conferences"
"Clustering based novel test case prioritization technique","G. Chaurasia; S. Agarwal; S. S. Gautam","Institute of Information Technology, India; Department of Information Technology, Indian Institute of Information Technology, India; Software Engineering, Indian Institute of Information Technology, India","2015 IEEE Students Conference on Engineering and Systems (SCES)","","2015","","","1","5","Regression testing is an activity during the maintenance phase to validate the changes made to the software and to ensure that these changes would not affect the previously verified code or functionality. Often, regression testing is performed with limited computing resources and time budget. So in this phase, it is infeasible to run the complete test suite Thus, test-case prioritization approaches are applied to ensure the execution of test cases in some prioritized order and to achieve some specific goals like, increasing the rate of bug detection, identifying the most critical bugs as early as possible etc. In this research work, we are going to propose a new and more effective clustering based prioritization technique that uses various metrics and execution time of test cases to reorder them. The results of implementation will prove that the suggested approach is more productive than the existing coverage and clustering based prioritization techniques.","","978-1-4673-8597-8978-1-4673-8598","10.1109/SCES.2015.7506447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506447","Clustering;Regression testing;test case prioritization;test suite","Measurement;Complexity theory;Fault detection;Testing;Information technology;History;Clustering algorithms","program debugging;program testing;regression analysis;software maintenance","clustering based test-case prioritization;regression testing;software maintenance;bug detection","","","25","","","","","","IEEE","IEEE Conferences"
"Optimal Testing Resource Allocation for modular software systems based-on multi-objective evolutionary algorithms with effective local search strategy","Yu Shuaishuai; Fei Dong; Bin Li","Dep. of Electronic Science and Technology, USTC, Hefei, China; Dep. of Electronic Science and Technology, USTC, Hefei, China; Dep. of Electronic Science and Technology, USTC, Hefei, China","2013 IEEE Workshop on Memetic Computing (MC)","","2013","","","1","8","Software testing is a very important part in software projects. As a key issue in software testing, Optimal Testing Resource Allocation Problems (OTRAPs) have drawn more and more attention recently. Along with the rapid increasing of the scale and complexity of software systems, the problems become more and more difficult to solve. Although some single objective optimization approaches had been used to solve such problems, quite a number of flaws were observed with these approaches, such as trapping into local optima, high computational complexity and few available optimal solutions. In this paper, to solve the problem of few available optimal solutions, an effective local search (ELS) is introduced into two effective multi-objective evolutionary algorithms: Non-dominated Sorting Genetic Algorithm II (NSGA-II) and Harmonic Distance Based Multi-objective Evolutionary Algorithm (HaD-MOEA), advantages of this strategy over pure multi-objective approaches are testified on two OTRAPs with parallel-series modular software systems. To deal with the problem of high computational complexity, the proposed ELS is also embedded into another effective multi-objective algorithm, Multi-objective Evolutionary Algorithm based on Decomposition (MOEA/D) to solve OTRAPs. Comprehensive experimental studies show the better performance over the state-of-the-art multi-objective approaches for OTRAPs.","","978-1-4673-5891","10.1109/MC.2013.6608200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608200","Multi-objective evolutionary algorithm;parallel-series modular software system;effective local search (ELS);NSGA-II;MOEA/D;software testing reliability;testing cost","Software systems;Testing;Software reliability;Resource management;Vectors;Evolutionary computation","evolutionary computation;genetic algorithms;program testing;resource allocation;search problems;software metrics","optimal testing resource allocation;modular software system based-multiobjective evolutionary algorithms;effective local search strategy;software testing;software resource;software projects;OTRAP;software system complexity;software system scale;optimal solutions;ELS;effective multiobjective evolutionary algorithms;nondominated sorting genetic algorithm ii;NSGA-II;harmonic distance based multiobjective evolutionary algorithm;HaD-MOEA;parallel-series modular software systems;computational complexity;multiobjective evolutionary algorithm based-on decomposition;MOEA/D","","","29","","","","","","IEEE","IEEE Conferences"
"Test effort estimation in regression testing","Abhilasha; A. Sharma","Department of CEA, GLA University, Mathura (U. P.), India; Department of CEA, GLA University, Mathura (U. P.), India","2013 IEEE International Conference in MOOC, Innovation and Technology in Education (MITE)","","2013","","","343","348","Software test effort estimation has always been a challenge for the software practitioners, because it consumes approximately half of the overall development costs of any software project. In order to provide effective software maintenance it is necessary to carry out the regression testing of the software. Hence, this research work aims to propose a measure for the estimation of the software test effort in regression testing. Since, the effort required developing or test software shall depend on various major contributing factors like, therefore, the proposed measure first estimates the change type of any software, make test cases for any software, then calculate execution complexity of any software and tester rank. In general, the regression testing takes more time and cost to perform it. Therefore, the effort estimation in regression testing is utmost required in order to compute man-hour for any software. In order to analyze the validity of the proposed test effort estimation measure, the measure is compared for various ranges of problem from small, mid and large size program to real life software projects. The result obtained shows that, the proposed test measure is a comprehensive one and compares well with other prevalent measures proposed in the past.","","978-1-4799-1626-9978-1-4799-1625","10.1109/MITE.2013.6756364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6756364","Regression testing;test effort;test case","Software;Estimation;Complexity theory;Software testing;Productivity;Software measurement","program testing;software maintenance;software metrics","regression testing;software test effort estimation;software project;software maintenance;execution complexity;test effort estimation measure","","","17","","","","","","IEEE","IEEE Conferences"
"A Test Framework for Communications-Critical Large-Scale Systems","M. A. Nabulsi; R. M. Hierons","Brunel University; Brunel University","IEEE Software","","2015","32","3","86","93","Today's large-scale systems couldn't function without the reliable availability of a range of network communications capabilities. Software, hardware, and communications technologies have been advancing throughout the past two decades. However, the methods that industry commonly uses to test large-scale systems that incorporate critical communications interfaces haven't kept pace. The need exists for a specifically tailored framework to achieve effective, precise testing of communications-critical large-scale systems. A proposed test framework offers an alternative to the current generic approaches that lead to inefficient, costly testing in industry. A case study illustrates its benefits, which can also be realized with other comparable systems.","0740-7459;1937-4194","","10.1109/MS.2014.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6785925","testing;test framework;communications-critical large-scale systems;IT systems;test case prioritization;requirements prioritization;software engineering","Software testing;Large-scale systems;Requirements engineering;Software engineering;Information technology;ISO standards","program testing;safety-critical software","communications-critical large-scale systems;network communication capability;formal software test methodology;communication technology;software technology;hardware technology","","","8","","","","","","IEEE","IEEE Journals & Magazines"
"Prioritizing Manual Test Cases in Traditional and Rapid Release Environments","H. Hemmati; Z. Fang; M. V. Mantyla","NA; NA; NA","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","","2015","","","1","10","Test case prioritization is one of the most practically useful activities in testing, specially for large scale systems. The goal is ranking the existing test cases in a way that they detect faults as soon as possible, so that any partial execution of the test suite detects maximum number of defects for the given budget. Test prioritization becomes even more important when the test execution is time consuming, e.g., manual system tests vs. automated unit tests. Most existing test case prioritization techniques are based on code coverage, which requires access to source code. However, manual testing is mainly done in a black- box manner (manual testers do not have access to the source code). Therefore, in this paper, we first examine the existing test case prioritization techniques and modify them to be applicable on manual black-box system testing. We specifically study a coverage- based, a diversity-based, and a risk driven approach for test case prioritization. Our empirical study on four older releases of Mozilla Firefox shows that none of the techniques are strongly dominating the others in all releases. However, when we study nine more recent releases of Firefox, where the development has been moved from a traditional to a more agile and rapid release environment, we see a very signifiant difference (on average 65% effectiveness improvement) between the risk-driven approach and its alternatives. Our conclusion, based on one case study of 13 releases of an industrial system, is that test suites in rapid release environments, potentially, can be very effectively prioritized for execution, based on their historical riskiness; whereas the same conclusions do not hold in the traditional software development environments.","2159-4848","978-1-4799-7125","10.1109/ICST.2015.7102602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102602","","Testing;Manuals;Software;Context;Natural languages;Fault detection;Companies","online front-ends;program testing;software fault tolerance;source code (software)","manual test case prioritization;rapid release environments;large scale systems;fault detection;test execution;code coverage;source code;manual black-box system testing;coverage-based approach;diversity-based approach;risk driven approach;Mozilla Firefox;historical riskiness","","7","34","","","","","","IEEE","IEEE Conferences"
"A Uniform Representation of Hybrid Criteria for Regression Testing","S. Sampath; R. Bryce; A. M. Memon","University of Maryland, Baltimore; University of North Texas, Denton; University of Maryland, Baltimore","IEEE Transactions on Software Engineering","","2013","39","10","1326","1344","Regression testing tasks of test case prioritization, test suite reduction/minimization, and regression test selection are typically centered around criteria that are based on code coverage, test execution costs, and code modifications. Researchers have developed and evaluated new individual criteria; others have combined existing criteria in different ways to form what we--and some others--call hybrid criteria. In this paper, we formalize the notion of combining multiple criteria into a hybrid. Our goal is to create a uniform representation of such combinations so that they can be described unambiguously and shared among researchers. We envision that such sharing will allow researchers to implement, study, extend, and evaluate the hybrids using a common set of techniques and tools. We precisely formulate three hybrid combinations, Rank, Merge, and Choice, and demonstrate their usefulness in two ways. First, we recast, in terms of our formulations, others' previously reported work on hybrid criteria. Second, we use our previous results on test case prioritization to create and evaluate new hybrid criteria. Our findings suggest that hybrid criteria of others can be described using our Merge and Rank formulations, and that the hybrid criteria we developed most often outperformed their constituent individual criteria.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2013.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6484067","Test case prioritization;test criteria;hybrid test criteria;web testing;GUI testing","Testing;Fault detection;Educational institutions;Genetic algorithms;Vectors;Loss measurement;Minimization","program testing;regression analysis","uniform representation;hybrid test criteria;regression testing;rank-merge-and-choice hybrid combination;test case prioritization;merge-and-rank formulations","","13","74","","","","","","IEEE","IEEE Journals & Magazines"
"Achievements, Open Problems and Challenges for Search Based Software Testing","M. Harman; Y. Jia; Y. Zhang","NA; NA; NA","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","","2015","","","1","12","Search Based Software Testing (SBST) formulates testing as an optimisation problem, which can be attacked using computational search techniques from the field of Search Based Software Engineering (SBSE). We present an analysis of the SBST research agenda, focusing on the open problems and challenges of testing non-functional properties, in particular a topic we call 'Search Based Energy Testing' (SBET), Multi-objective SBST and SBST for Test Strategy Identification. We conclude with a vision of FIFIVERIFY tools, which would automatically find faults, fix them and verify the fixes. We explain why we think such FIFIVERIFY tools constitute an exciting challenge for the SBSE community that already could be within its reach.","2159-4848","978-1-4799-7125","10.1109/ICST.2015.7102580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102580","","Software testing;Search problems;Optimization;Software engineering;Energy consumption;Software","program testing;search problems;software engineering","search based software testing;SBST;optimisation problem;computational search technique;search based software engineering;SBSE;nonfunctional property testing;search based energy testing;SBET;test strategy identification;FIFIVERIFY tool","","28","129","","","","","","IEEE","IEEE Conferences"
"Genetic algorithm secure procedures algorithm to manage data integrity of test case prioritization methodology","S. Mahajan; S. D. Joshi; V. Khanaa","Dept. of Computer Engineering, Bharath University, Chennai, India; Dept. of Computer Engineering, Bharati Vidyapeeth, College of Engineering, Pune, India; Dept. of Computer Engineering, Bharath University, Chennai, India","2014 IEEE Global Conference on Wireless Computing & Networking (GCWCN)","","2014","","","208","212","The focus of present research paper is to manage data integrity and trustworthiness of issues involved in software testing phase of software development life cycle. Many times, it seems that, data integration left behind the software testing phase and it is directly focused at software deployment or delivery time. To avoid issues due to lack of data integration, we developed algorithm which can track integration of test case prioritization along with trustworthiness of test case execution. This paper demonstrates how software testing can be efficient with management of data integrity factor to avoid major security issues. One of the main advantages of our approach is that domain specific semantics can be integrated with the data quality test cases prioritization, thus being able to discover test feed data quality problems beyond conventional quality measures.","","978-1-4799-6298-3978-1-4799-6297","10.1109/GCWCN.2014.7030880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7030880","secure test case;test case prioritization;test case integration issues","Genetic algorithms;Algorithm design and analysis;Software testing;Security;Indexes;Conferences","data integration;data integrity;genetic algorithms;program testing;trusted computing","genetic algorithm secure procedures algorithm;data integrity management;data trustworthiness;software testing phase;software development life cycle;data integration;test case prioritization integration;test case execution trustworthiness","","","16","","","","","","IEEE","IEEE Conferences"
"Software defect prediction model based on LLE and SVM","Chun Shan; Boyang Chen; Changzhen Hu; Jingfeng Xue; Ning Li","School of software, Beijing Institute of Technology, 100081, China; School of software, Beijing Institute of Technology, 100081, China; School of software, Beijing Institute of Technology, 100081, China; School of software, Beijing Institute of Technology, 100081, China; 15th Research Institute of China Electronics Technology Group Corporation, Beijing 100083, China","2014 Communications Security Conference (CSC 2014)","","2014","","","1","5","Software defect prediction strives to improve software security by helping testers locate the software defects accurately. The data redundancy caused by the overmuch attributes in defects data set will make the prediction accuracy decrease. A model based on locally linear embedding and support vector machine (LLE-SVM) is proposed to solve this problem in this paper. The SVM is used as the basic classifier in the model. And the LLE algorithm is used to solve data redundancy due to its ability of maintaining local geometry. The parameters in SVM are optimized by the method of ten-fold cross validation and grid search. The comparison between LLE-SVM model and SVM model was experimentally verified on the same NASA defect data set. The results indicate that the proposal LLE-SVM model performs better than SVM model, and it is available to avoid the accuracy decrease caused by the data redundancy.","","978-1-84919-844","10.1049/cp.2014.0749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6992242","Software Security;Software Defect Prediction;Support Vector Machine;Locally Linear Embedding","","pattern classification;program testing;search problems;security of data;software reliability;support vector machines","software defect prediction model;data redundancy;locally linear embedding-support vector machine;LLE algorithm;local geometry;ten-fold cross validation method;grid search;LLE-SVM model;NASA defect data set;software security;software testing","","","","","","","","","IET","IET Conferences"
"Operational Profile Modeling as a Risk Assessment Tool for Software Quality Techniques","O. AlShathry","NA","2014 International Conference on Computational Science and Computational Intelligence","","2014","2","","181","184","A major share of software project investment is assigned to activities concerning the detection and removal of defects. Software project managers tend to apply the most efficient QA techniques to assure low defect density within their software project. However, the criteria of selecting a QA technique based on its efficiency is not always safe and cost effective. Software defects vary in their severity in terms of the magnitude of their negative impact on both the testing process and the whole project. Some defects would make intense implications if it passed to the operational use as it belongs to significant functional components of the software. The consideration of potential risk with some defects types should be taken into account when selecting a QA technique to avoid future failure. In this paper, we build on previous work of software quality optimization by proposing a model whereby QA decisions are normalized by the risk associated with their defects detection and removal activities.","","978-1-4799-3010","10.1109/CSCI.2014.115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822326","Software Quality;Software Defects;FMEA;DRE;SDLC","Software quality;System testing;Conferences;Investment;Software engineering","object-oriented programming;program testing;project management;risk management;software fault tolerance;software quality","removal activities;QA decisions;software quality optimization;defects types;software functional components;testing process;software defects;low defect density;QA techniques;software project managers;defect removal;defect detection;software project investment;software quality techniques;risk assessment tool;operational profile modeling","","","20","","","","","","IEEE","IEEE Conferences"
"Efficient Automated Program Repair through Fault-Recorded Testing Prioritization","Y. Qi; X. Mao; Y. Lei","NA; NA; NA","2013 IEEE International Conference on Software Maintenance","","2013","","","180","189","Most techniques for automated program repair use test cases to validate the effectiveness of the produced patches. The validation process can be time-consuming especially when the object programs ship with either lots of test cases or some long-running test cases. To alleviate the cost for testing, we first introduce regression test prioritization insight into the area of automated program repair, and present a novel prioritization technique called FRTP with the goal of reducing the number of test case executions in the repair process. Unlike most existing prioritization techniques frequently requiring additional cost for gathering previous test executions information, FRTP iteratively extracts that information just from the repair process, and thus incurs trivial performance lose. We also built a tool called TrpAutoRepair, which implements our FRTP technique and has the ability of automatically repairing C programs. To evaluate TrpAutoRepair, we compared it with GenProg, a state-of-the-art tool for automated C program repair. The experiment on the 5 subject programs with 16 real-life bugs provides evidence that TrpAutoRepair performs at least as good as GenProg in term of success rate, in most cases (15/16), TrpAutoRepair can significantly improve the repair efficiency by reducing efficiently the test case executions when searching a valid patch in the repair process.","1063-6773","978-0-7695-4981","10.1109/ICSM.2013.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676889","automated program repair;test case prioritization;efficiency;automated debugging","Maintenance engineering;Indexes;Testing;Fault detection;Data mining;Context;Computer bugs","program debugging;program testing;regression analysis;statistical testing","automated debugging;test case prioritization;GenProg;FRTP technique;TrpAutoRepair;regression test prioritization;fault-recorded testing prioritization;automated program repair","","22","29","","","","","","IEEE","IEEE Conferences"
"VertexRank: Importance Rank for Software Network Vertices","H. Luo; Y. Dong; Y. Ng; S. Wang","NA; NA; NA; NA","2014 IEEE 38th Annual Computer Software and Applications Conference","","2014","","","251","260","Finding the critical components of software system is very important for software comprehension, operation and maintenance. We propose Vertex Rank, a method to rank the importance of vertices of software network. The complexity of the algorithm to compute Vertex Rank is proportional to the sum of the number of edges and vertices. And, it converges quickly at an ideal deviation within limited iterations. Vertex Rank captures the essence of software network by utilizing software entry point during calculation. We compare Vertex Rank with widely used ranking measures, degree, betweenness centrality, Page Rank and Component Rank, result shows that it is a more appropriate measure for software network because it better differentiates the various roles that vertices play in network as it utilizes the characteristic of software entry points. It not only finds the most important vertices more accurately, but also makes clear distinction between less important ones and completely excludes the unused ones. We test Vertex Rank on real-world open source software systems and find that the functionality of vertex greatly affect its Vertex Rank value, there is a strong positive correlation between in degree and Vertex Rank, and Vertex Rank ranking highly coincides with reuse.","0730-3157","978-1-4799-3575","10.1109/COMPSAC.2014.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899224","VertexRank;ranking;Extended Call Graph;software engineering;entry point","Software systems;Androids;Humanoid robots;Libraries;Software measurement;Complexity theory","graph theory;public domain software;software maintenance","Vertex Rank;software network vertices;software comprehension;software operation;software maintenance;software entry points;open source software systems;extended call graph","","","40","","","","","","IEEE","IEEE Conferences"
"Particle swarm optimization algorithm for test case automatic generation based on clustering thought","D. Y. Ming; W. Y. Ting; W. Ding Hui","School of IoT Engineering, Jiangnan University, Wuxi, China; School of IoT Engineering, Jiangnan University, Wuxi, China; School of IoT Engineering, Jiangnan University, Wuxi, China","2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER)","","2015","","","1479","1485","In order to improve the efficiency and quality of software test case automatic generation, a kind of particle swarm optimization was proposed. It had adaptive optimization based on the clustering thought. The algorithm divided the population into two types which were main particle and secondary particle when the algorithm was executed. They used different search strategies so that the algorithm expanded the search scope of particles to speed up the algorithm running. The experimental result shows that the proposed algorithm has more advantages and is more effective than the other contrastive algorithms in the software test case automatic generation.","","978-1-4799-8730-6978-1-4799-8728-3978-1-4799-8727-6978-1-4799-8729","10.1109/CYBER.2015.7288163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7288163","software testing;test case generation;particle swarm optimization(PSO);clustering thought;neighborhood search","Clustering algorithms;Sociology;Statistics;Algorithm design and analysis;Particle swarm optimization;Search problems;Optimization","particle swarm optimisation;pattern clustering;program testing;search problems;software quality","particle swarm optimization algorithm;software test case automatic generation;software quality;software efficiency;adaptive optimization;clustering thought;search strategy","","","16","","","","","","IEEE","IEEE Conferences"
"The optimization method of Markov chain usage model based on three-parameter interval number","Yang Zhao","Jiangsu Automation Research Institute, Lianyungang, China","2015 IEEE International Conference on Grey Systems and Intelligent Services (GSIS)","","2015","","","528","530","In software reliability testing based on Markov chain usage model, how to assign the migrating probability between states of the Markov chain usage model is still a problem yet to be solved. In order to deal with the problems such as the sample data for estimating the migrating probability being small and uncertain, a migrating probability assignment method based on a three-parameter interval number is proposed to build the Markov chain usage model. The method for generating test cases is also given. An example shows in this paper, the software usage model based on the three-parameter interval number is closer to the actual situation, and the software reliability testing efficiency is improved.","2166-9430;2166-9449","978-1-4799-8375-9978-1-4799-8374-2978-1-4799-8373","10.1109/GSIS.2015.7301913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301913","Markov Chain;software reliability testing;usage model;three-parameter interval number","Markov processes;Testing;Indexes","Markov processes;number theory;optimisation;probability;program testing;software reliability","Markov chain usage model;optimization method;software reliability testing;migrating probability assignment method;three-parameter interval number;software usage model;software reliability testing efficiency","","","9","","","","","","IEEE","IEEE Conferences"
"Software defect prediction using software metrics - A survey","K. Punitha; S. Chitra","Bhajarang Engineering College, Tiruvallur, Chennai, TamilNadu, India; Er. Perumal Manimekalai College of Engineering, Hosur, Krishnagiri, Tamil Nadu, India","2013 International Conference on Information Communication and Embedded Systems (ICICES)","","2013","","","555","558","Traditionally software metrics have been used to define the complexity of the program, to estimate programming time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the metric values are to be used in mathematical equations designed to represent a model of the software process, metrics associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully apply. Work on the mechanics of implementing metrics programs. The goal of this research is to help developers identify defects based on existing software metrics using data mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in the development and maintenance phase. This research focuses in identifying defective modules and hence the scope of software that needs to be examined for defects can be prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology helps in identifying modules that require immediate attention and hence the reliability of the software can be improved faster as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the Data mining algorithm. To initiate this process we initially propose to evaluate the existing classification algorithms and based on its weakness we propose a novel Neural network algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy.","","978-1-4673-5788-3978-1-4673-5786-9978-1-4673-5787","10.1109/ICICES.2013.6508369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6508369","Software defect prediction;software defectproneness prediction;machine learning;scheme evaluation","Software;Predictive models;Accuracy;Computer bugs;Measurement;Software reliability;Testing","data mining;fuzzy set theory;neural nets;pattern classification;program diagnostics;software cost estimation;software maintenance;software metrics;software quality;software reliability","software defect prediction;software metrics;program complexity;programming time estimation;mathematical equations;software process;ratio scale data;mathematical operations;metrics programs;data mining techniques;software quality;software development cost;software maintenance phase;software development phase;defective module identification;software reliability;data mining algorithm;neural network algorithm;fuzziness degree","","4","10","","","","","","IEEE","IEEE Conferences"
"Optimizing testing efforts based on change proneness through machine learning techniques","A. K. Tripathi; K. Sharma","Department of computer Eng, Delhi Technological University, Delhi, India; Department of computer Eng, Delhi Technological University, Delhi, India","2014 6th IEEE Power India International Conference (PIICON)","","2014","","","1","4","For any software organization, understanding the software quality is desirable in order to increase user experience of the software. When we talk about security software this factor becomes even more important. This paper aims to develop models for predicting the change proneness for object oriented system. The developed models may be used to predict the change prone classes at early phase of software development. Rigorous testing and allocation of some extra resources to those change prone classes may lead to better quality and it may also reduce our work at the maintenance phase. We apply one statistical and 10 machine learning techniques to predict the models. The results are analyzed from Receiver Operating Characteristics (ROC) analysis using Area under the Curve (AUC) obtained from ROC. Adaboost and Random forest method have shown the best result and hence, based on these results we can claim that quality models have a good relevance with Object Oriented systems.","","978-1-4799-6042-2978-1-4799-6041-5978-1-4799-6040","10.1109/POWERI.2014.7117742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7117742","Empirical Validation;Object Oriented;Receiver Operating Characteristics;Statistical Methods;Machine Learning;change Prediction","Object oriented modeling;Software;Measurement;Predictive models;Unified modeling language;Security;Maintenance engineering","learning (artificial intelligence);object-oriented methods;optimisation;program testing;security of data;sensitivity analysis;software maintenance;software quality;statistical analysis;user interfaces","optimizing testing efforts;change proneness;machine learning techniques;software organization;software quality;user experience;security software;object oriented system;software development;rigorous testing;maintenance phase;statistical techniques;receiver operating characteristics;ROC analysis;area under the curve;AUC;Adaboost;random forest method","","","17","","","","","","IEEE","IEEE Conferences"
"Automated Regression Test Suite Optimization Based on Heuristics","D. S. U. M. Prasad; S. Chacko; S. S. P. Kanakadandi; G. K. Durbhaka","NA; NA; NA; NA","2014 4th International Conference on Artificial Intelligence with Applications in Engineering and Technology","","2014","","","48","53","In the Software Development Life Cycle, Testing is an integral and important phase. It is estimated that close to 45% of project cost is marked for testing. Defect removal efficiency is directly proportional to the rigor of the testing and number of test cycles. Given this prelude, important optimization dual is to reduce the testing time and cost without compromising on the quality and coverage. We revisit this popular research and industry sought problem, in the historical data perspective. Proposed model has two steps. N Test cases based on multiple heuristics are recommended as part of first step. These heuristics can be derived based on test manager, test lead and/or test director requirements as inputs. The N test cases that are to be recommended will be derived upon executing evolutionary randomized algorithms such as Random Forest / Genetic Algorithm. These algorithms fed with historically derived inputs such as test case execution frequency, test case failure pattern, change feature pattern and bug fixes &amp; associations. The recommended test suite is further optimized based on a 2 dimensional approach during second step. Test case specific vertical constraints such as distribution of environments, distribution of features as well as test suite composition parameters such as golden test cases, sanity test cases, that serves as horizontal constraints.","","978-1-4799-7910-3978-1-4799-7909","10.1109/ICAIET.2014.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351812","Test coverage; Regression Test Suite; Random Forest; Test failure prediction; Optimisation; Greedy Algorithm","Radio frequency;Testing;Genetic algorithms;Predictive models;Sociology;Statistics;Vegetation","genetic algorithms;heuristic programming;program testing;randomised algorithms;software development management;statistical testing","automated regression test suite optimization;software development life cycle;testing time;test director requirements;test lead requirements;test manager requirements;evolutionary randomized algorithms;random forest;genetic algorithm;test case execution frequency;test case failure pattern;change feature pattern;bug fixes & associations;feature distribution;test suite composition parameters;golden test cases;sanity test cases","","","17","","","","","","IEEE","IEEE Conferences"
"Bridging the gap between the total and additional test-case prioritization strategies","L. Zhang; D. Hao; L. Zhang; G. Rothermel; H. Mei","Key Laboratory of High Confidence Software Technologies (Peking University), MoE, Beijing, 100871, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, Beijing, 100871, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, Beijing, 100871, China; Department of Computer Science and Engineering, University of Nebraska, Lincoln, 68588, USA; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, Beijing, 100871, China","2013 35th International Conference on Software Engineering (ICSE)","","2013","","","192","201","In recent years, researchers have intensively investigated various topics in test-case prioritization, which aims to re-order test cases to increase the rate of fault detection during regression testing. The total and additional prioritization strategies, which prioritize based on total numbers of elements covered per test, and numbers of additional (not-yet-covered) elements covered per test, are two widely-adopted generic strategies used for such prioritization. This paper proposes a basic model and an extended model that unify the total strategy and the additional strategy. Our models yield a spectrum of generic strategies ranging between the total and additional strategies, depending on a parameter referred to as the p value. We also propose four heuristics to obtain differentiated p values for different methods under test. We performed an empirical study on 19 versions of four Java programs to explore our results. Our results demonstrate that wide ranges of strategies in our basic and extended models with uniform p values can significantly outperform both the total and additional strategies. In addition, our results also demonstrate that using differentiated p values for both the basic and extended models with method coverage can even outperform the additional strategy using statement coverage.","0270-5257;1558-1225","978-1-4673-3076-3978-1-4673-3073","10.1109/ICSE.2013.6606565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606565","","Fault detection;Measurement;Java;Testing;Arrays;Software;Educational institutions","Java;program testing;software fault tolerance","test-case prioritization strategies;fault detection;regression testing;heuristics;Java programs","","32","33","","","","","","IEEE","IEEE Conferences"
"Portability testing of scientific computing software systems","R. Nori; N. Karodiya; H. Reza","University of North Dakota School of Aerospace Sciences Grand Forks, ND 58201 USA; University of North Dakota School of Aerospace Sciences Grand Forks, ND 58201 USA; University of North Dakota School of Aerospace Sciences Grand Forks, ND 58201 USA","IEEE International Conference on Electro-Information Technology , EIT 2013","","2013","","","1","8","Software testing is considered to be the ultimate obstacle before software can be released. Traditional approaches to testing the software revolves around capabilities of a system using various types of testing (e.g., black-box) at various level of software development process. Testing the software for system-wide qualities such as portability is very rare. The research work in this area is very limited. In this work, we discuss the importance of portability testing and attempt to explore a testing approach to test the portability of Scientific Computing (SC) software using different platforms. The main incentive for testing of SC software is that this class of software is challenging because of the elements such as computing environment, computational mathematics, languages, optimization; these elements play significant roles in success or failure of these systems.","2154-0373;2154-0357","978-1-4673-5208-6978-1-4673-5207","10.1109/EIT.2013.6632686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6632686","Sceinfic computing;software testing;portability testing;software engineering","Software;Testing;Uncertainty;Mathematical model;Numerical models;Computational modeling;Scientific computing","natural sciences computing;program testing;software engineering","portability testing;scientific computing software systems;software testing;software development process;computing environment;computational mathematics;languages;optimization","","1","38","","","","","","IEEE","IEEE Conferences"
"Mutation Testing","P. Reales; M. Polo; J. L. Fernández-Alemán; A. Toval; M. Piattini","University of Castilla-La Mancha; University of Castilla-La Mancha; University of Murcia; University of Murcia; University of Castilla-La Mancha","IEEE Software","","2014","31","3","30","35","This article gives a short overview of the main characteristics of mutation tools. If a test suite finds all the artificial errors inserted in the mutants and finds no fault in the original, it's likely that the program under test is free of them. Obviously, the validity of this affirmation depends on the nature of the artificial fault: some of them are better than others. This testing technique has been used in the research arena to check the effectiveness of new proposed testing techniques, but it hasn't been used until recently in industry due to its costs and the lack of knowledge and industrial tools.","0740-7459;1937-4194","","10.1109/MS.2014.68","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6802989","software testing;mutation testing;debugging;test suite","Java;Testing;Industries;Program processors;Optimization;Fault diagnosis","program testing","testing technique;artificial fault;program under test;mutation tools;software testing;mutation testing","","","6","","","","","","IEEE","IEEE Journals & Magazines"
"Sequence and sequence-less T-way test suite generation strategy based on flower pollination algorithm","A. B. Nasser; F. Hujainah; A. A. Alsewari; K. Z. Zamli","Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, 26300, Kuantan, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, 26300, Kuantan, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, 26300, Kuantan, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, 26300, Kuantan, Malaysia","2015 IEEE Student Conference on Research and Development (SCOReD)","","2015","","","676","680","In an attempt to ensure good-quality software, there is need to test all possible inputs. Owing to the fact that the exhaustive testing is hardly feasible, many software testing approaches has been proposed. Combinatorial Interaction Testing (CIT) is very promising technique to minimize the number of test cases. Although useful, most of exiting CIT strategies and tools focus on data inputs and assume “sequence-less” interactions between input parameters. However, reactive systems show sequence related behaviors and their faults may not expose if the sequence of inputs are not considered. In this paper, we propose a new t-way strategy (i.e. t refers to the degree of the combination) strategy, called Flower Strategy (FS), that addresses both sequence and sequence-less test generation. Experimental results show that FS produces test size.","","978-1-4673-9572-4978-1-4673-9571","10.1109/SCORED.2015.7449424","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7449424","Sequence-based T-way Testing;Flower Pollination Algorithm;Sequence Test Suite Generation;Optimization Problems","Testing;Algorithm design and analysis;Memory management;Switches;Sociology;Statistics;Conferences","optimisation;program testing;sequences;software quality","sequence-less T-way test suite generation strategy;sequence T-way test suite generation strategy;flower pollination algorithm;good-quality software;software testing approach;input parameters;reactive systems;sequence related behaviors;flower strategy;FS;test size","","3","31","","","","","","IEEE","IEEE Conferences"
"Predicting Fault-Prone Software Modules with Rank Sum Classification","J. Cahill; J. M. Hogan; R. Thomas","NA; NA; NA","2013 22nd Australian Software Engineering Conference","","2013","","","211","219","The detection and correction of defects remains among the most time consuming and expensive aspects of software development. Extensive automated testing and code inspections may mitigate their effect, but some code fragments are necessarily more likely to be faulty than others, and automated identification of fault prone modules helps to focus testing and inspections, thus limiting wasted effort and potentially improving detection rates. However, software metrics data is often extremely noisy, with enormous imbalances in the size of the positive and negative classes. In this work, we present a new approach to predictive modelling of fault proneness in software modules, introducing a new feature representation to overcome some of these issues. This rank sum representation offers improved or at worst comparable performance to earlier approaches for standard data sets, and readily allows the user to choose an appropriate trade-off between precision and recall to optimise inspection effort to suit different testing environments. The method is evaluated using the NASA Metrics Data Program (MDP) data sets, and performance is compared with existing studies based on the Support Vector Machine (SVM) and Naïve Bayes (NB) Classifiers, and with our own comprehensive evaluation of these methods.","1530-0803;2377-5408","978-0-7695-4995","10.1109/ASWEC.2013.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601309","metrics;fault proneness;machine learning","Support vector machines;Measurement;Niobium;Software;Testing;Inspection;NASA","program testing;software fault tolerance;software metrics","fault-prone software modules prediction;rank sum classification;defect detection;defect correction;software development;automated testing;code inspections;code fragments;automated fault identification;fault prone modules;detection rates;software metrics data;predictive modelling;fault proneness;rank sum representation;testing environments;NASA metrics data program;MDP data sets;support vector machine;SVM;naïve Bayes classifier;NB classifiers","","3","32","","","","","","IEEE","IEEE Conferences"
"Software reliability modeling with different type of faults incorporating both imperfect debugging and change point","S. Chatterjee; A. Shukla","Department of Applied Mathematics, Indian School of Mines, Dhanbad, India; Department of Applied Mathematics, Indian School of Mines, Dhanbad, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","5","In past four decades, many nonhomogeneous Poisson process (NHPP) based software reliability growth models (SRGMs) have been proposed to measure and assess the reliability growth of software. During the testing process, the faults which causes failure are detected and removed. One common assumption of many traditional SRGMs is that the fault removal rate is constant. In practical, the fault removal rate increases with time as learning and maturity of software engineer increases. Hence, time variant fault removal rate has been considered in this study. A complex software system may contain different category of faults. Some of faults can be easily detected and removed and some of faults required more effort to be detected and removed. Therefore, in this article, a NHPP based SRGM has been proposed which incorporates mainly two type of faults, major and minor. The concepts of imperfect debugging and change point have also been incorporated in the proposed SRGM. The parameters of the proposed SRGM is estimated using Statistical Package for Social Sciences (SPSS) software and validation of the proposed SRGM has been done using real life data set.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359361","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359361","Non-homogeneous Poisson process;Software reliability;Software reliability growth model;Imperfect debugging;Change point","Testing;Debugging;Software reliability;Parameter estimation;Software systems","learning (artificial intelligence);program debugging;program verification;software packages;software reliability;stochastic processes","software reliability modeling;program debugging;nonhomogeneous Poisson process;software reliability growth model;software testing process;fault removal rate;NHPP-based SRGM;statistical package-for-social science;software validation","","","25","","","","","","IEEE","IEEE Conferences"
"Automatic verification of test oracles in functional testing","T. R. Monisha; A. Chamundeswari","SSN College of Engineering; SSN College of Engineering","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","","2013","","","1","4","Functional testing of the applications becomes essential to detect the faults. Nowadays machine learning techniques have been implemented in the software engineering particularly in software testing field. But machine learning algorithms are difficult to detect the faults in certain applications because it is difficult to find the test oracle which is used to verify computed outputs. Here in this paper, a novel functional testing approach to verify the test oracles is attempted. The expected execution result for a given application is generated and verified with the oracle whether the application under test has or has not behaved correctly and issues a pass/fail verdict.","","978-1-4799-3926-8978-1-4799-3925","10.1109/ICCCNT.2013.6726656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726656","Oracle;Verification;Testing","Software;Machine learning algorithms;Economic indicators;Software testing;Optimization;Classification algorithms","functional programming;program testing;program verification;software fault tolerance","automatic verification;test oracles;functional testing;faults detection;software engineering;software testing","","2","12","","","","","","IEEE","IEEE Conferences"
"Optimizing reliability testing with limited resources","A. M. Gillespie","ASQ CRE, SAIC, 1710 SAIC Drive, McLean, VA 22102, USA","2015 Annual Reliability and Maintainability Symposium (RAMS)","","2015","","","1","6","In the current limited budget environment, more government entities are using commercial and commercial off-the shelf (COTS) products, which may not have the resources or documentation to ensure high quality and high reliability components. It is then up to the program to determine the quality and reliability properties (estimated failure rate, consumer's risk, environmental limitations, etc). This type of life testing can be costly and time consuming. In order to minimize the cost, the number of components required for reliability testing and the number of hours required to test must be optimized using statistical and reliability testing techniques. A step-by-step methodology using reliability testing parameters and statistical standards to optimize and define the number of components and test time required to achieve a desired confidence in reliability will be presented. This paper will discuss all aspects of a reliability and quality test plan and present the way to inform the management of those risks.","0149-144X","978-1-4799-6703-2978-1-4799-6702","10.1109/RAMS.2015.7105112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105112","Reliability Testing;Sampling Plans;Life Testing;Quality","Testing;Reliability engineering;Stress;Inspection;Software;Standards","life testing;program testing;risk management;software quality;software reliability;statistical testing","optimizing reliability testing;limited resources;limited budget environment;government entity;commercial off-the shelf product;COTS product;documentation;reliability property;life testing;statistical testing;reliability testing technique;step-by-step methodology;reliability testing parameter;statistical standard;quality test plan;risk management","","1","8","","","","","","IEEE","IEEE Conferences"
"Multi-objective regression test suite optimization with Fuzzy logic","Z. Anwar; A. Ahsan","Center for Advanced Studies in Engineering (CASE), Islamabad, Pakistan; Center for Advanced Studies in Engineering (CASE), Islamabad, Pakistan","INMIC","","2013","","","95","100","Regression Testing is performed on already tested programs to ensure that modifications have not revealed defects into the unmodified portions of programs. Regression Test Suites are always growing due to addition of Test Cases. Many broken and redundant test cases are also part of regression test suites. Running the all regression test suite is always not feasible; therefore optimization of regression test suites is required to meet the constraints. In this work we proposed multi-objective optimization of regression test suites with Fuzzy Logic (Sugeno Model) for Black Box based testing methods. Proposed approach was implemented on two published case studies. Results indicates that optimization can be achieved by using Fuzzy Logic that is a safe technique for optimization and we can reduce the execution time and size of regression test suites up to 50%.","","978-1-4799-3043","10.1109/INMIC.2013.6731331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6731331","Regression Testing;Regression Test Suite Optimization;Fuzzy Logic;Computaional Intelligence;Multi-Objective Optimization","Optimization;Fuzzy logic;Testing;Mathematical model;Fault detection;Linear programming;Software","fuzzy logic;optimisation;program testing;regression analysis","multiobjective regression test suite optimization;fuzzy logic;regression testing;redundant test cases;regression test suites;Sugeno model;Black Box based testing methods","","","28","","","","","","IEEE","IEEE Conferences"
"Evolutionary Search Algorithms for Test Case Prioritization","S. K. Mohapatra; S. Prasad","NA; NA","2013 International Conference on Machine Intelligence and Research Advancement","","2013","","","115","119","To improve the effectiveness of certain performance goals, test case prioritization techniques are used. These technique schedule the test cases in particular order for execution so as to increase the efficacy in meeting the performance goals. For every change in the program it is considered inefficient to re-execute each and every test case. Test case prioritization techniques arrange the test cases within a test suite in such a way that the most important test case is executed first. This process enhances the effectiveness of testing. This algorithm during time constraint execution has been shown to have detected maximum number fault while including the sever test cases.","","978-0-7695-5013","10.1109/ICMIRA.2013.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918806","Regression testing;Test case;Genetic algorithm;prioritization","Genetic algorithms;Testing;Software;Sociology;Statistics;Algorithm design and analysis;Fault detection","evolutionary computation;program testing;search problems;software fault tolerance","evolutionary search algorithms;performance goals;test case prioritization techniques;test cases schedule;test suite;testing effectiveness;time constraint execution;fault","","1","19","","","","","","IEEE","IEEE Conferences"
"Identifying a Subset of TMMi Practices to Establish a Streamlined Software Testing Process","K. G. Camargo; F. C. Ferrari; S. C. P. F. Fabbri","NA; NA; NA","2013 27th Brazilian Symposium on Software Engineering","","2013","","","137","146","Context: Testing is one of the most important phases of software development. However, in industry this phase is usually compromised by the lack of planning and resources. Due to it, the adoption of a streamlined testing process can lead to the construction of software products with desirable quality levels. Objective: Presenting the results of a survey conducted to identify a set of key practices to support the definition of a generic, streamlined software testing process, based on the practices described in the TMMi (Test Maturity Model integration). Method: Based on the TMMi, we have performed a survey among software testing professionals who work in both academia and industry. Their responses were analyzed quantitatively and qualitatively in order to identify priority practices to build the intended generic process. Results: The analysis enabled us to identify practices that were ranked as mandatory, those that are essential and should be implemented in all cases. This set of practices (33 in total) represents only 40% of the TMMis full set of practices, which sums up to 81 items related to the steps of a testing process. Conclusion: The results show that there is a consensus on a subset of practices that can guide the definition of a lean testing process when compared to a process that includes all TMMi practices. It is expected that such a process encourages a wider adoption of testing activities within the software industry.","","978-0-7695-5165","10.1109/SBES.2013.6","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800190","software testing process;TMMi practices","Companies;Software testing;Software;Planning;Industries;Context","program testing;software engineering","TMMi practices;streamlined software testing process;test maturity model integration;lean testing process;software development","","1","15","","","","","","IEEE","IEEE Conferences"
"A Mutation and Multi-objective Test Data Generation Approach for Feature Testing of Software Product Lines","R. A. M. Filho; S. R. Vergilio","NA; NA","2015 29th Brazilian Symposium on Software Engineering","","2015","","","21","30","Mutation approaches have been recently applied for feature testing of Software Product Lines (SPLs). The idea is to select products, associated to mutation operators that describe possible faults in the Feature Model (FM). In this way, the operators and mutation score can be used to evaluate and generate a test set, that is a set of SPL products to be tested. However, the generation of test sets to kill all the mutants with a reduced, possible minimum, number of products is a complex task. To solve such problem, this paper introduces a multiobjective approach that includes a representation to the problem, search operators and two objectives related to the number of test cases and dead mutants. The approach was implemented with three representative multi-objective and evolutionary algorithms: NSGA-II, SPEA2 and IBEA. The conducted evaluation analyses the solutions obtained and compares the algorithms. An advantage of this approach is to offer a set of good solutions to the tester with a reduced number of products and high mutation score values, that is, with high probability of revealing faults described by the mutation testing.","","978-1-4673-9272","10.1109/SBES.2015.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328006","software produtct line;multiobjective optimization","Frequency modulation;Testing;Sociology;Statistics;Optimization;Search problems;Context","genetic algorithms;mathematical operators;program testing;search problems;software product lines","multiobjective test data generation approach;mutation approach;feature testing;software product lines;mutation operators;feature model;FM;mutation score;SPL products;test set generation;complex task;search operators;test cases;dead mutants;evolutionary algorithm;NSGA-II;SPEA2;IBEA;probability;mutation testing","","1","36","","","","","","IEEE","IEEE Conferences"
"A Method and Tool for Test Optimization for Automotive Controllers","A. Petrenko; A. Dury; S. Ramesh; S. Mohalik","NA; NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops","","2013","","","198","207","Completely automatic generation of tests from formal executable test models of industrial size still looks like a “holy grail”, in spite of significant progress in model-based testing research and tool development. Realizing this, we follow a more down-to-earth approach by assuming that, even if a test model is available, the test expert manually derives powerful test fragments and what remains to be automated is chaining them into an optimal test. Focusing on this task, we develop a test optimization framework using an FSM extended with input variables and clocks, which reflects important features of Simulink/Stateflow statecharts. The test optimization is expressed as the Asymmetric Travelling Salesman Problem (ATSP). We show how this approach can be used for solving some testing problems specific to automotive controllers. We describe a proof-of-concept prototype, implementing the proposed approach, which we tested on a case study of a particular controller available along with some tests. Experiments with the prototype indicate that the approach scales well for hundreds of tests.","","978-0-7695-4993-4978-1-4799-1324","10.1109/ICSTW.2013.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571630","model-based testing;test optimization;test generation;Extended FSM;clock automata;asynchronous machines;automotive controllers","Clocks;Bridges;Cost accounting;Optimization;Input variables;Unified modeling language;Testing","automobile industry;automotive components;controllers;testing;travelling salesman problems","test optimization;automotive controllers;automatic generation;holy grail;model-based testing;FSM;Simulink statecharts;Stateflow statecharts;asymmetric travelling salesman problem;ATSP;proof-of-concept prototype","","11","27","","","","","","IEEE","IEEE Conferences"
"Software integration in global software development: Success factors for GSD vendors","M. Ilyas; S. U. Khan","Software Engineering Research Group (SERG_UOM), Deptartment of Computer Science & IT, University of Malakand, KPK, Pakistan; Software Engineering Research Group (SERG_UOM), Deptartment of Computer Science & IT, University of Malakand, KPK, Pakistan","2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","","2015","","","1","6","Global software development (GSD) is in a booming stage from the last decade with the advances in information and communication technologies (ICTs). Despite of the benefits gained from GSD, the developer organizations still face a lot of difficulties in integrating the software components, developed by various GSD teams, into a final product. The objective of the study presented in this paper is to find out critical success factors (CSFs) for vendors that can play a positive role at any stage of the software integration process. To achieve the purpose we conducted a systematic literature review (SLR) for the identifications of the factors that can be adapted to assist the integration process at various stages i.e. before integration, during integration and after the integration. We have found a total of fourteen success factors among which nine factors are ranked as CSFs. Some of the top ranked CSFs are “Consistency in Requirements and Architecture Design”, “Intra and inter team Communication and Coordination” and “Component/Unit Testing prior to integration”.","","978-1-4799-8676","10.1109/SNPD.2015.7176187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176187","Global Software Development;Systematic Literature Review;Software Product Integration;Success Factors","Software;Computer architecture;Testing;Documentation;Software engineering;Systematics;Planning","software engineering","global software development;software integration;GSD vendors;information and communication technologies;ICT;software components;critical success factors;CSF;software integration process;systematic literature review;SLR;integration process","","3","47","","","","","","IEEE","IEEE Conferences"
"Automatic and Incremental Product Optimization for Software Product Lines","A. Demuth; R. E. Lopez-Herrejon; A. Egyed","NA; NA; NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation","","2014","","","31","40","Software Product Lines (SPLs) have gained popularity in industry as they foster the reuse of artifacts, such as code, and reduce product development effort. Although some SPLs ensure that only valid products are configurable, those products are not necessarily optimal. For instance, they may include code that is not necessary for providing the desired functionality -- often because of erroneous traceability between features and code. Such unnecessary code may be disallowed in safety critical domains, it may lead to losses in runtime performance, or it may lead to errors during later SPL evolution. In this paper, we present an approach for automatic and incremental product optimization. Our approach leverages product functionality tests to ensure that configured products do not include unnecessary artifacts -- an automatic re-optimization of products after SPL evolution is performed incrementally. The evaluation results show that such a re-optimization takes only milliseconds.","2159-4848","978-1-4799-2255","10.1109/ICST.2014.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823863","Software product lines;optimization;evolution","Optimization;Unified modeling language;Testing;Media;Databases;Software;Libraries","optimisation;software product lines","software product lines;incremental product optimization;safety critical domains;product functionality tests;products automatic reoptimization;SPL evolution","","1","26","","","","","","IEEE","IEEE Conferences"
"Selection and Prioritization of Test Cases by Combining White-Box and Black-Box Testing Methods","S. Kukolj; V. Marinkovic; M. Popovic; S. Bognár","NA; NA; NA; NA","2013 3rd Eastern European Regional Conference on the Engineering of Computer Based Systems","","2013","","","153","156","In this paper, we present a methodology that combines both white-box and black-box testing, in order to improve testing quality for a given class of embedded systems. The goal of this methodology is generation of test cases for the new functional testing campaign based on the test coverage information from the previous testing campaign, in order to maximize the test coverage. Test coverage information is used for selection of proper test cases in order to improve the quality of testing and save available resources for testing. As an output, a set of test cases is produced. Generated test cases are processed by the test Executor application that decides whether results have passed or failed, based on the results of image grabbing, OCR text extraction, and comparison with expected text. The presented methodology is finally validated by means of a case-study targeting an Android device. The results of the case study are affirmative and they indicate that the proposed methodology is applicable for testing embedded systems of this kind.","","978-0-7695-5064","10.1109/ECBS-EERC.2013.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664523","embedded systems;quality assurance;black-box testing;white-box testing;code coverage","Testing;Instruments;Embedded systems;Multimedia communication;Digital TV;Performance evaluation","embedded systems;program testing;software quality","test case selection;test case prioritization;white-box testing methods;black-box testing methods;embedded systems;functional testing campaign;test coverage information;testing quality;Executor application;image grabbing;OCR text extraction;Android device","","3","7","","","","","","IEEE","IEEE Conferences"
"Ranking of software reliability growth models using Bacterial Foraging Optimization Algorithm","B. Khalid; K. Sharma","Department of Computer Engineering, Delhi Technological University, INDIA; Department of Computer Engineering, Delhi Technological University, INDIA","2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)","","2015","","","1643","1648","Developing new software is an easy task but is really very difficult when it comes to change existing software, because the new changes can affect the old functionalities of the software, hence making the software unreliable. It doesn't matter how much testing is done, but the correct result is known once the product is delivered to the customer. Hence, we need to know whether our products are reliable or not before delivering them to customers. This can be achieved by Software Reliability Models. In this paper, Bacterial Foraging Optimization Algorithm (BFOA) technique is operated on Tandem Computer data set to estimate parameters of thirteen software reliability growth models based on Least Square Estimation fitting technique. These models were then compared against eleven comparison criteria like BIAS, Variance etc. Then Distance Based Approach (DBA) is applied to rank all these models based on the comparison criteria. Among the thirteen models under study inflection s-shaped model and goel-okumoto model were ranked one and two respectively.","","978-9-3805-4416-8978-9-3805-4415-1978-9-3805-4414","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100526","Software reliability growth models;Bacterial Foraging Optimization Algorithm;Least Square Estimation;Distance Based Approach;rank","Software reliability;Object oriented modeling;Software;Predictive models;Data models;Mathematical model","estimation theory;least squares approximations;optimisation;parameter estimation;software reliability","software reliability growth models;bacterial foraging optimization algorithm;BFOA;Tandem computer data set;parameter estimation;least square estimation fitting technique;distance based approach;DBA;inflection s-shaped model;goel-okumoto model","","","28","","","","","","IEEE","IEEE Conferences"
"On the applicability of evolutionary computation for software defect prediction","R. Malhotra; N. Pritam; Y. Singh","Department of Software Engineering, Delhi Technological University, India; Department of Software Engineering, Delhi Technological University, India; Maharaja Sayajirao University of Baroda, Gujrat, India","2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","","2014","","","2249","2257","Removal of defects is the key in ensuring long-term error free operation of a software system. Although improvements in the software testing process has resulted in better coverage, it is evident that some parts of a software system tend to be more defect prone than the other parts and identification of these parts can greatly benefit the software practitioners in order to deliver high quality maintainable software products. A defect prediction model is built by training a learner using the software metrics. These models can later be used to predict defective classes in a software system. Many studies have been conducted in the past for predicting defective classes in the early phases of the software development. However, the evolutionary computation techniques have not yet been explored for predicting defective classes. The nature of evolutionary computation techniques makes them better suited to the software engineering problems. In this study we explore the predictive ability of the evolutionary computation and hybridized evolutionary computation techniques for defect prediction. This work contributes to the literature by examining the effectiveness of the 15 evolutionary computation and hybridized evolutionary computation techniques to 5 datasets obtained from the Apache Software Foundation using the Defect Collection and Reporting System. The results are evaluated in terms of the values of accuracy. We further compare the evolutionary computation techniques using the Friedman ranking. The results suggest that the defect prediction models built using the evolutionary computation techniques perform well over all the datasets in terms of prediction accuracy.","","978-1-4799-3080-7978-1-4799-3078","10.1109/ICACCI.2014.6968592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968592","Defect Prediction;Search Based Software Engineering;Evolutionary Computation;Software Metrics","Measurement;Evolutionary computation;Predictive models;Accuracy;Software systems;Computational modeling","evolutionary computation;program testing;software fault tolerance;software metrics","software defect prediction;defect removal;long-term error free operation;software testing process;high quality maintainable software products;software practitioners;defect prediction model;software metrics;software engineering problems;hybridized evolutionary computation techniques;Apache Software Foundation;Defect Collection and Reporting System;Friedman ranking","","4","43","","","","","","IEEE","IEEE Conferences"
"Optimizing COTS Selection Process Using Prototype Framework Approach: A Theoritical Concept","D. Sarkar","NA","2015 Fifth International Conference on Advanced Computing & Communication Technologies","","2015","","","521","523","Software engineering is a field which is an essential part of all the organizations and business. In broad sense it affects the global economy. It helps in optimizing resources and minimizing the cost by incorporating reusability property. One good example of using reusability is Commercial off-the-shelf (COTS). COTS is a common name in software industry now-a-days. COTS based systems can be developed with less time period and less budget. There are existing development processes for COTS where once the requirement analysis is done, the COTS are selected from different vendors based on those requirements. Then the different COTS are integrated using glue code and interfaces. Finally the product is tested and ready for installation. The main drawback in this existing system is that after acceptance testing if there is a discrepancy between the customer requirement and system function then the system should get modified. This modification may include exclusion of some COTS already used in the product, inclusion of some new COTS and change in the glue code. So finally it affects the performance of the system in terms of effort and cost of quality. This paper will describe a framework for COTS development to remove this drawback where we will not integrate different COTS until and unless the prototype of the system does not get accepted by the customer.","2327-0659;2327-0632","978-1-4799-8488-6978-1-4799-8487","10.1109/ACCT.2015.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079138","Commercial off-the-shelf;COTS;COTS based system;Reusability property;Requirement engineering","Software;Organizations;Prototypes;Testing;Software reliability","formal specification;program testing;software houses;software quality;software reusability","COTS selection process optimization;prototype framework approach;software engineering field;global economy;resource optimization;cost minimization;reusability property;commercial off-the-shelf;software industry;COTS development processes;requirement analysis;gluecode;software interfaces;acceptance testing;customer requirement;system function;software quality","","2","6","","","","","","IEEE","IEEE Conferences"
"Software components prioritization using OCL formal specification for effective testing","A. Jalila; D. J. Mala","Department of Computer Applications, Thiagarajar College of Engineering, Madurai, India; Department of Computer Applications, Thiagarajar College of Engineering, Madurai, India","2013 International Conference on Recent Trends in Information Technology (ICRTIT)","","2013","","","714","720","In soft real time system development, testing effort minimization is a challenging task. Earlier research has shown that often a small percentage of components are responsible for most of the faults reported at the later stages of software development. Due to the time and other resource constraints, fault-prone components are ignored during testing activity which leads to compromises on software quality. Thus there is a need to identify fault-prone components of the system based on the data collected at the early stages of software development. The major focus of the proposed methodology is to identify and prioritize fault-prone components of the system using its OCL formal specifications. This approach enables testers to distribute more effort on fault-prone components than non fault-prone components of the system. The proposed methodology is illustrated based on three case study applications.","","978-1-4799-1024","10.1109/ICRTIT.2013.6844288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844288","Critical Components;UML (Unified Modeling Language);Formal Specification;OCL (Object Constraints Language);Design Metrics","Measurement;Complexity theory;Unified modeling language;Context;Software;Object oriented modeling;Indexes","formal specification;program testing;software fault tolerance;software quality","software components prioritization;OCL formal specification;effective testing;soft real time system development;testing effort minimization;software development;fault-prone components;testing activity;software quality","","","23","","","","","","IEEE","IEEE Conferences"
"A comparison of three black-box optimization approaches for model-based testing","T. Kanstrén; M. Chechik","VTT, Oulu, Finland; University of Toronto, Canada","2014 Federated Conference on Computer Science and Information Systems","","2014","","","1591","1598","Model-based testing is a technique for generating test cases from a test model. Various notations and techniques have been used to express the test model and generate test cases from those models. Many use customized modelling languages and in-depth white-box static analysis for test generation. This allows for optimizing generated tests to specific paths in the model. Others use general-purpose programming languages and light-weight black-box dynamic analysis. While this light-weight approach allows for quick prototyping and easier integration with existing tools and user skills, optimizing the resulting test suite becomes more challenging since less information about the possible paths is available. In this paper, we present and compare three approaches to such black-box optimization.","","978-83-60810-58","10.15439/2014F152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933208","model based testing;test automation;evaluation;test generation;optimization","Generators;Optimization;Testing;Algorithm design and analysis;Radiation detectors;Greedy algorithms;Analytical models","dynamic testing;formal specification;program testing;software prototyping","three black-box optimization approaches;model-based testing;test case generation;test model;customized modelling languages;in-depth white-box static analysis;test generation;general-purpose programming languages;light-weight black-box dynamic analysis;light-weight approach;prototyping","","","25","","","","","","IEEE","IEEE Conferences"
"On the Automatic Generation of Optimized Software-Based Self-Test Programs for VLIW Processors","D. Sabena; M. S. Reorda; L. Sterpone","Dipartimento di Automatica e Informatica, Politecnico di Torino, Torino, Italy; Dipartimento di Automatica e Informatica, Politecnico di Torino, Torino, Italy; Dipartimento di Automatica e Informatica, Politecnico di Torino, Torino, Italy","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2014","22","4","813","823","Very long instruction word (VLIW) processors are increasingly employed in a large range of embedded signal processing applications, mainly due to their ability to provide high performances with reduced clock rate and power consumption. At the same time, there is an increasing request for efficient and optimal test techniques able to detect permanent faults in VLIW processors. Software-based self-test (SBST) methods are a consolidated and effective solution to detect faults in a processor both at the end of the production phase or during the operational life; however, when traditional SBST techniques are applied to VLIW processors, they may prove to be ineffective (especially in terms of size and duration), due to their inability to exploit the parallelism intrinsic in these architectures. In this paper, we present a new method for the automatic generation of efficient test programs specifically oriented to VLIW processors. The method starts from existing test programs based on generic SBST algorithms and automatically generates effective test programs able to reach the same fault coverage, while minimizing the test duration and the test code size. The method consists of four parametric phases and can deal with different VLIW processor models. The main goal of the paper is to show that in the case of VLIW processors, it is possible to automatically generate an effective test program able to achieve high fault coverage with minimal test time and required resources. Experimental data gathered on a case study demonstrate the effectiveness of the proposed approach; results show that this method is able to exploit the intrinsic parallelism of the VLIW processor, taming the growth in size, and duration of the test program when the processor size grows.","1063-8210;1557-9999","","10.1109/TVLSI.2013.2252636","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497676","Software-based self-test (SBST);test program generation;very long instruction word (VLIW) processors","","automatic testing;electronic engineering computing;embedded systems;integrated circuit testing;microprocessor chips","test program automatic generation;optimized software based self-test programs;VLIW processor;very long instruction word processor;embedded signal processing applications;optimal test techniques;permanent fault detection;software based self-test methods;processor faults;test code","","5","21","","","","","","IEEE","IEEE Journals & Magazines"
"Test Suite generation using Memetic algorithm on adaptive local search","A. A. Mundade; T. M. Pattewar","Department of Computer Engineering, RCPIT, Shirpur, North Maharashtra University, India; Department of Information Technology, RCPIT, Shirpur, North Maharashtra University, India","2015 International Conference on Communications and Signal Processing (ICCSP)","","2015","","","0630","0633","Testing is the process of evaluating quality and correctness of software. In testing, we always check different functions of application. Test cases are the measure component, which is required in testing the functions and conditions. Test case is nothing but a condition which we want to check. It is very difficult to add test cases manually, so that we generate set of test cases called Test Suite. To generate the test suite with high code coverage we apply Memetic algorithm. Code coverage is not only important factor in test suite but also measuring part of test suite. Code coverage measure used to describe the degree of source code program which is tested. Test case may have more than one goal so that we can check feasibility of particular goal. We have implement Memetic algorithm to find code coverage. Here we also applied Memetic algorithm on adaptive local search. In the evaluation it is found that code coverage of Memetic algorithm on adaptive local search is maximum than Memetic algorithm.","","978-1-4799-8081-9978-1-4799-8080","10.1109/ICCSP.2015.7322564","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322564","Code coverage;Memtic Algorithm;Software testing;Test suite","Memetics;Optimization;Software","program testing;search problems;software quality","test suite generation;memetic algorithm;adaptive local search;software correctness;test cases;measure component;high code coverage;code coverage;code coverage measure;source code program;check feasibility","","","15","","","","","","IEEE","IEEE Conferences"
"An efficient algorithm for pairwise test case generation in presence of constraints","S. Gao; B. Du; Y. Jiang; J. Lv; S. Ma","State Key Laboratory of Software Development Environment, Beihang University, Beijing, China 100191; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China 100191; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China 100191; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China 100191; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China 100191","The 2014 2nd International Conference on Systems and Informatics (ICSAI 2014)","","2014","","","406","410","Constraints handling problem in combinational testing is an intensive computation process. In this paper, we present an effective algorithm, called IPO SAT (In-Parameter-Order-Satisfiability), for pairwise test case generation in presence of constraints. In our strategy, constraints are denoted as forbidden tuples, which are converted to conjunctive normal form. Then, the combination test cases which meet the constraints are found out by calling Boolean satisfiability(SAT) solvers. Besides, an optimization upon the process is given, in order to improve the performance of IPO SAT by reducing the number of times of calling SAT solver and avoiding checking irrelevant constraints. Finally, experimental results show that the proposed IPO SAT algorithm is efficient and the optimization has obvious improvements on reducing time cost.","","978-1-4799-5458-2978-1-4799-5457","10.1109/ICSAI.2014.7009323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009323","Combinational testing;IPO SAT;Constraints handling;Forbidden tuples","Optimization;Software;Arrays;Software testing;Software algorithms;Software engineering","Boolean functions;computability;constraint handling;optimisation;program testing","pairwise test case generation;constraint handling problem;combinational testing;IPO SAT;in-parameter-order-satisfiability;forbidden tuples;Boolean satisfiability solvers;Boolean SAT solver;optimization","","1","14","","","","","","IEEE","IEEE Conferences"
"Object tracking based on hardware/software co-design of particle filter and particle swarm optimization","C. Hsu; W. Kao; Y. Chu; S. Li; W. Lin","Department of Applied Electronics Technology, National Taiwan Normal University, Taipei, Taiwan; Department of Applied Electronics Technology, National Taiwan Normal University, Taipei, Taiwan; Department of Applied Electronics Technology, National Taiwan Normal University, Taipei, Taiwan; Department of Electrical Engineering, Tamkang University, Taipei, Taiwan; Department of Electrical Engineering, Tamkang University, Taipei, Taiwan","2014 IEEE Fourth International Conference on Consumer Electronics Berlin (ICCE-Berlin)","","2014","","","225","227","This paper presents a hardware/software co-design method for a hybrid object tracking algorithm incorporating particle filter (PF) and particle swarm optimization (PSO) based on System On Program Chip (SOPC) technique. Considering both the execution speed and design flexibility, we use a embedded processor to calculate weight for each particle and a hybrid accelerator implemented by hardware to update particles. As a result, execution efficiency of the proposed hardware/software co-design method is significantly improved while maintaining design flexibility for various embedded applications. As soon as prototype testing for a specific problem is completed by using the software weight assignment, full hardware implementation of the weight calculating module can be used to speed up the execution speed.","2166-6814;2166-6822","978-1-4799-6165","10.1109/ICCE-Berlin.2014.7034287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034287","HW/SW Co-design;Particle swarm optimization (PSO);Particle filter;Object tracking;System on a programmable chip (SOPC);Field Programmable Gate Array (FPGA)","Object tracking;Hardware;Software;Particle swarm optimization;Particle filters;Software algorithms;Search problems","hardware-software codesign;object tracking;particle filtering (numerical methods);particle swarm optimisation","object tracking;hardware/software codesign;particle filter;particle swarm optimization;PSO;system on program chip;SOPC technique;hybrid accelerator","","","6","","","","","","IEEE","IEEE Conferences"
"Model-based testing of automotive software: Some challenges and solutions","A. Petrenko; O. N. Timo; S. Ramesh","Computer Research Institute of Montreal, CRIM, Canada; Computer Research Institute of Montreal, CRIM, Canada; GM Global R&D, Warren, MI, USA","2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)","","2015","","","1","6","Automotive software has been growing in size, criticality and complexity with each new generation of vehicles. Testing at the model and code level is an important step in validating the software against various types of defects that may be introduced in the development process. Model based testing (MBT) methodology, paves a road towards automation of testing activities. Test generation is a computationally complex task, which requires efficient constraint solving techniques and some guidance from the test engineer when this task cannot be solved by a tool. At the same time, automatic tools can hardly substitute domain testing experts which can develop more effective tests or at least test fragments than any tool. This is why we believe that future test generation tools should support “tester-in-the-loop” MBT approaches. In this paper, we provide a brief report on our results in this direction.","0738-100X","978-1-4799-8052","10.1145/2744769.2747935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167303","Model-based testing;test generation;test chaining;test optimization;tester-in-the-loop;test models;statecharts;finite state machines;extended state machines","Testing;Context;Computational modeling;Bridges;Input variables;Software;Prototypes","automotive engineering;constraint handling;program testing;program verification;traffic engineering computing;vehicles","model-based testing;automotive software;code level testing;software validation;software defects;development process;MBT methodology;constraint solving techniques;automatic tools;domain testing;test generation tools;tester-in-the-loop;vehicles","","3","20","","","","","","IEEE","IEEE Conferences"
"CDM-Suite: An Attributed Test Selection Tool","P. Luchscheider; T. Herpel; R. German","NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation","","2013","","","398","407","Many reasons lead to embedded systems getting more complex than ever. The higher integration and interconnection and the additional effort spend for safety-critical functions, require new techniques to support the testing process. Beneath using model-driven test techniques, test engineers need support to take the right decisions on test case selection and prioritization during the whole development process. Available approaches on test selection and prioritization lack in adaptability to different testing techniques. We previously presented our approach of modeling components and dependences of a system, such as black-box systems, using Component-Dependency-Models (CDMs) as shown in earlier publications. In this paper we present our tool CDM-Suite in detail and show the way it can be used by test engineers. We therefore start with the presentation of our approach and discussing related tools. The next section describes the graphical user interface (GUI) and the different ways of interaction for data generation. This point includes the introduction of metrics build upon fuzzy logic and graph analysis. At last we show how to interpret analysis results and derive knowledge for test case selection and prioritization. We sum the paper up by giving a conclusion and a presentation outline.","2159-4848","978-0-7695-4968-2978-1-4673-5961","10.1109/ICST.2013.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569753","Black-Box;Testing;Test case;Selection;Prioritization;System Model;Tool;Path-Search","Graphical user interfaces;Measurement;Vehicles;Analytical models;Computer architecture;Context;XML","embedded systems;fuzzy logic;graph theory;graphical user interfaces;object-oriented programming;program testing;software metrics","model-driven test techniques;black-box systems;graph analysis;fuzzy logic;data generation;GUI;graphical user interface;component-dependency model;test case prioritization;test case selection;testing process;safety-critical functions;embedded systems;attributed test selection tool;CDM-Suite","","1","10","","","","","","IEEE","IEEE Conferences"
"Survey on parameter estimation in software reliability","I. Altaf; F. Rashid; J. A. Dar; M. Rafiq","Department of Computer Science and Engineering, Amity University Noida, India; Department of Computer Science and Engineering, Islamic University Of Science & technology, Kashmir, India; Department of Computer Science and Engineering, Islamic University Of Science & technology, Kashmir, India; Department of Computer Science and Engineering, Islamic University of Science & technology, Kashmir, India","2015 International Conference on Soft Computing Techniques and Implementations (ICSCTI)","","2015","","","22","27","Throughout the process of software engineering, software reliability analysis is accomplished at numerous phases as an endeavor for the estimation of whether the software necessities have been encountered or not. During the initial phases of software development, software reliability prediction techniques were very challenging. A number of software reliability models have been put forward during the ancient few years in order to measure the software reliability. Reliability of every software artifact is a measurable characteristic which is highly indispensable for foreseeing the degree of trustworthiness of any software so that it can work precisely for a definite interval of time. Moreover there must not be presence of any type of failure. In this paper, I have briefly discussed about some fundamental parametric estimation techniques in software reliability models. This paper highly focuses on how different types of techniques can be utilized for estimating the various parameters.","","978-1-4673-6792-9978-1-4673-6790","10.1109/ICSCTI.2015.7489632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489632","Software reliability;Software reliability growth models;parameter estimation;maximum likelihood estimation;least square estimation;particle swarm optimization;ant colony optimization;cuckoo search","Software reliability;Software;Estimation;Parameter estimation;Testing;Predictive models","ant colony optimisation;maximum likelihood estimation;particle swarm optimisation;search problems;software reliability;trusted computing","parameter estimation;software reliability analysis;software development;software reliability prediction;software trustworthiness;particle swarm optimisation;ant colony optimisation;maximum likelihood estimation;cuckoo search","","","31","","","","","","IEEE","IEEE Conferences"
"Study of software development method of battery test platform based on Simulink and C#","Gao Ke; B. Sun; Pan Tian; Niu Jun-long; Wen Jia-peng; Feng Tao; Liu Jun","National Active Distribution Network Technology Research Center (NANTEC), Beijing Jiaotong University, China; National Active Distribution Network Technology Research Center (NANTEC), Beijing Jiaotong University, China; UTEK New Energy Technology Co., Ltd, Tongzhou District, Majuqiao U test area in East Building 56, China; National Active Distribution Network Technology Research Center (NANTEC), Beijing Jiaotong University, China; UTEK New Energy Technology Co., Ltd, Tongzhou District, Majuqiao U test area in East Building 56, China; UTEK New Energy Technology Co., Ltd, Tongzhou District, Majuqiao U test area in East Building 56, China; UTEK New Energy Technology Co., Ltd, Tongzhou District, Majuqiao U test area in East Building 56, China","2014 IEEE Conference and Expo Transportation Electrification Asia-Pacific (ITEC Asia-Pacific)","","2014","","","1","5","In this paper, we have studied the software control system of battery test platform, focusing on Simulink and C # hybrid programming. At first, we build a simulation model in accordance with the needs of battery test platform. Then, we select RTW tool to generate standard C code after test and verify. Next, we use C# tool and regular expression to write program on the basis of the code to realize the .c files automatically read and .DLL files automatically generation, ultimately realize the strategy and host computer cohesion call. At last the SOC estimation model is purposed to validate the method, the output is fully consistent with the simulation result. This method is suitable for complex strategy on battery test software platform.","","978-1-4799-4239-8978-1-4799-4240","10.1109/ITEC-AP.2014.6940940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6940940","software control system;Simulink;DLL;SOC estimation model","Computational modeling;Analytical models;System-on-chip;Hybrid power systems;Optimization;Lead;Batteries","battery testers;C language;power engineering computing","software development method;software control system;battery test software platform;SOC estimation model;Simulink;C # hybrid programming;RTW tool","","","10","","","","","","IEEE","IEEE Conferences"
"The Issues of Solving Staffing and Scheduling Problems in Software Development Projects","D. C. C. Peixoto; G. R. Mateus; R. F. Resende","NA; NA; NA","2014 IEEE 38th Annual Computer Software and Applications Conference","","2014","","","1","10","Search-Based Software Engineering (SBSE) applies search-based optimization techniques in order to solve complex Software Engineering problems. In the recent years there has been a dramatic increase in the number of SBSE applications in areas such as Software Test, Requirements Engineering, and Project Planning. Our focus is on the analysis of the literature in Project Planning, specifically the researches conducted in software project scheduling and resource allocation. SBSE project scheduling and resource allocation solutions basically use optimization algorithms. Considering the results of a previous Systematic Literature Review, in this work, we analyze the issues of adopting these optimization algorithms in what is considered typical settings found in software development organizations. We found few evidence signaling that the expectations of software development organizations are being attended.","0730-3157","978-1-4799-3575","10.1109/COMPSAC.2014.96","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899194","Search-Based Software Engineering;Resource allocation;Staffing;Scheduling;Project management;Literature review","Software;Optimization;Project management;Scheduling;Software engineering;Resource management;Organizations","optimisation;organisational aspects;personnel;program testing;resource allocation;scheduling;software development management","staffing problems;scheduling problems;software development projects;search-based software engineering;SBSE;search-based optimization techniques;software test;requirements engineering;project planning;software project scheduling;resource allocation;systematic literature review;optimization algorithms;software development organizations","","3","65","","","","","","IEEE","IEEE Conferences"
"Supporting Continuous Integration by Code-Churn Based Test Selection","E. Knauss; M. Staron; W. Meding; O. Söder; A. Nilsson; M. Castell","NA; NA; NA; NA; NA; NA","2015 IEEE/ACM 2nd International Workshop on Rapid Continuous Software Engineering","","2015","","","19","25","Continuous integration promises advantages in large-scale software development by enabling software development organizations to deliver new functions faster. However, implementing continuous integration in large software development organizations is challenging because of organizational, social and technical reasons. One of the technical challenges is the ability to rapidly prioritize the test cases which can be executed quickly and trigger the most failures as early as possible. In our research we propose and evaluate a method for selecting a suitable set of functional regression tests on system level. The method is based on analysis of correlations between test-case failures and source code changes and is evaluated by combining semi-structured interviews and workshops with practitioners at Ericsson and Axis Communications in Sweden. The results show that using measures of precision and recall, the test cases can be prioritized. The prioritization leads to finding an optimal test suite to execute before the integration.","","978-1-4673-7067","10.1109/RCoSE.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167168","","Heating;Testing;Software;Companies;Interviews;Data visualization","integrated software;organisational aspects;program testing;software development management;software reliability;source code (software);statistical testing","code-churn based test selection;continuous software integration;large scale software development organization;functional regression test;system level;test case failure;source code;optimal test suite","","4","23","","","","","","IEEE","IEEE Conferences"
"A comparative study between iterative waterfall and incremental software development life cycle model for optimizing the resources using computer simulation","P. Trivedi; A. Sharma","Govt. Engineering College Ajmer, Rajasthan India; Govt. Engineering College Ajmer, Rajasthan India","2013 2nd International Conference on Information Management in the Knowledge Economy","","2013","","","188","194","To achieve the maximum productivity using minimum resource is the aim of any software industry. Software engineering provides an abstraction process to develop software product. It has introduced various methodologies, principles and concepts. Most of them are the software process models which are also known as software life cycle models. In the software industry different-different types of projects (small, medium, large, complex) arrive in random inter arrival time for development of software products. Before implementing the products, project managers decide an appropriated software process model in documentation that is used in production of products. An empirical study conducted in 2012 that presents various effecting factors for selecting any software life cycle model. One of the important factors is team size. Software industry has a development team which works in co-ordination and depends on each other. Sometimes project managers are not intelligently assigned resources to particular phases of software life cycle model. Therefore to overcome these issues we are simulating iterative waterfall and incremental model to determine optimal resources for every phase of software life cycle model. This study shall enable the project manager to determine optimal resource without implementing software product. As future work, we shall be simulating spiral and iterative incremental model to determine which SDLC model is more appropriate for software industry.","","978-81-920249-7","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915096","Science;Software engineering;Software process model;Iterative waterfall model;Increment model;Simphony model;Simphony .NET","Software;Industries;Maintenance engineering;Testing;Computational modeling;Business;Analytical models","DP industry;iterative methods;software product lines","iterative waterfall simulation;incremental software development life cycle model;computer simulation;resource optimization;minimum resource;software industry;software engineering;software product development;software process models;iterative incremental model;SDLC model;random inter arrival time;optimal resource","","","17","","","","","","IEEE","IEEE Conferences"
"Using Defect Taxonomies for Testing Requirements","M. Felderer; A. Beer","University of Innsbruck; Beer Test Consulting","IEEE Software","","2015","32","3","94","101","Systematic defect management based on bug-tracking systems such as Bugzilla is well established and has been successfully used in many software organizations. Defect management weights the failures observed during test execution according to their severity and forms the basis for effective defect taxonomies. In practice, most defect taxonomies are used only for the a posteriori allocation of testing resources to prioritize failures for debugging. Thus, these taxonomies' full potential to control and improve all the steps of testing has remained unexploited. This is especially the case for testing a system's user requirements. System-level defect taxonomies can improve the design of requirements-based tests, the tracing of defects to requirements, the quality assessment of requirements, and the control of the relevant defect management. So, we developed requirements-based testing with defect taxonomies (RTDT). This approach is aligned with the standard test process and uses defect taxonomies to support all phases of testing requirements. To illustrate this approach and its benefits, we use an example project (which we call Project A) from a public health insurance institution.","0740-7459;1937-4194","","10.1109/MS.2014.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799150","requirements-based testing;defect taxonomy;test management;requirements validation;software quality;software engineering","Taxonomy;Software testing;Graphical user interfaces;Requirements engineering;Software engineering;Syntactics","program testing;software quality","systematic defect management;bug-tracking system;Bugzilla;a posteriori allocation;requirements quality assessment;requirements-based testing;defect taxonomies;RTDT","","3","11","","","","","","IEEE","IEEE Journals & Magazines"
"Broadening the Search in Search-Based Software Testing: It Need Not Be Evolutionary","R. Feldt; S. Poulding","NA; NA","2015 IEEE/ACM 8th International Workshop on Search-Based Software Testing","","2015","","","1","7","Search-based software testing (SBST) can potentially help software practitioners create better test suites using less time and resources by employing powerful methods for search and optimization. However, research on SBST has typically focused on only a few search approaches and basic techniques. A majority of publications in recent years use some form of evolutionary search, typically a genetic algorithm, or, alternatively, some other optimization algorithm inspired from nature. This paper argues that SBST researchers and practitioners should not restrict themselves to a limited choice of search algorithms or approaches to optimization. To support our argument we empirically investigate three alternatives and compare them to the de facto SBST standards in regards to performance, resource efficiency and robustness on different test data generation problems: classic algorithms from the optimization literature, bayesian optimization with gaussian processes from machine learning, and nested monte carlo search from game playing / reinforcement learning. In all cases we show comparable and sometimes better performance than the current state-of-the-SBST-art. We conclude that SBST researchers should consider a more general set of solution approaches, more consider combinations and hybrid solutions and look to other areas for how to develop the field.","","978-1-4673-7079","10.1109/SBST.2015.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7173581","Search-based software testing;Machine learning;Reinforcement learning;Operations research","Optimization;Generators;Search problems;Testing;Monte Carlo methods;Libraries;Machine learning algorithms","Bayes methods;evolutionary computation;Gaussian processes;genetic algorithms;learning (artificial intelligence);Monte Carlo methods;program testing","search-based software testing;SBST;optimization;genetic algorithm;evolutionary search;search algorithms;data generation problem;bayesian optimization;gaussian process;machine learning;nested Monte Carlo search;reinforcement learning","","5","25","","","","","","IEEE","IEEE Conferences"
"Test Case Generation for Modified Code Using a Variant of Particle Swarm Optimization (PSO) Algorithm","S. Tiwari; K. K. Mishra; A. K. Misra","NA; NA; NA","2013 10th International Conference on Information Technology: New Generations","","2013","","","363","368","In this paper, a variant of particle swarm optimization (PSO) algorithm using modified time varying acceleration coefficients (PSO-TVAC) has been proposed and applied in creation of new test cases for modified code in regression testing. The performance of the proposed algorithm is compared with other existing PSO algorithms on five well known benchmark test functions. The experiments prove that the proposed algorithm has better performance. The test cases generated by the proposed PSO-TVAC algorithm have greater code coverage capability over the initial test cases.","","978-0-7695-4967-5978-0-7695-4967","10.1109/ITNG.2013.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614335","regression testing;particle swarm optimization algorithm;test case generation","Algorithm design and analysis;Acceleration;Benchmark testing;Particle swarm optimization;Equations;Software algorithms","benchmark testing;particle swarm optimisation;program testing","test case generation;particle swarm optimization algorithm;time-varying acceleration coefficients;PSO-TVAC algorithm;regression testing;benchmark test functions;code coverage capability;code modification","","4","26","","","","","","IEEE","IEEE Conferences"
"Power-aware optimization of software-based self-test for L1 caches in microprocessors","G. Theodorou; N. Kranitis; A. Paschalis; D. Gizopoulos","Department of Informatics & Telecommunications, University of Athens, Greece; Department of Informatics & Telecommunications, University of Athens, Greece; Department of Informatics & Telecommunications, University of Athens, Greece; Department of Informatics & Telecommunications, University of Athens, Greece","2014 IEEE 20th International On-Line Testing Symposium (IOLTS)","","2014","","","154","159","In the era of terascale integration, the “reliability wall” and the “power wall” arise as barriers imposing significant challenges to the microprocessor industry. Nowadays, on-line testing is essential for modern microprocessors to detect latent defects that either escaped manufacturing testing or appear during system operation. Moreover, many-core scaling is now facing the “power wall”. More cores can now be placed on a chip than can be concurrently operating due to energy/power limitations. Software-Based Self-Test (SBST) is a flexible and low-cost solution for on-line March test application and defect detection in small memories, such as L1 caches, that lack Memory Built-In Self-Test (MBIST) hardware. In this paper, a power-aware optimization of the SBST methodology introduced in [10] is presented targeting Ll caches by analyzing the unique characteristics of March SBST routines and possible power optimizations without sacrificing March test quality. Experimental results using an architectural-level-power evaluation framework based on GEM5 and McPAT show that the exploitation of Direct Cache Access (DCA) instructions introduced in [10] along with the application of compiler-based power optimizations can achieve significant power savings up to 82.27% and energy savings up to 84,89% for the UltraSPARC Tl processor core. To the best of our knowledge, this is the first paper that addresses power-aware SBST for L1 caches in microprocessors.","1942-9398;1942-9401","978-1-4799-5324","10.1109/IOLTS.2014.6873688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6873688","","Decision support systems;Testing","automatic testing;cache storage;integrated circuit testing;microprocessor chips","power-aware optimization;software-based self-test;L1 caches;reliability wall;power wall;microprocessor industry;latent defect detection;manufacturing testing;system operation;many-core scaling;energy-power limitations;online March test application;memory built-in self-test hardware;MBIST hardware;SBST methodology;March SBST routine;March test quality;architectural-level-power evaluation framework;GEM5;McPAT;direct cache access instruction;DCA instruction;compiler-based power optimization;UltraSPARC Tl processor core;power-aware SBST","","2","23","","","","","","IEEE","IEEE Conferences"
"Weighted Particle Swarm Optimization algorithm for Randomized unit testing","K. D. R. Dhivya; V. S. Meenakshi","Department of CA and SS, Sri Krishna Arts and Science College, Coimbatore, India; Department of Computer Applications, Dr. Mahalingam College of Engineering &amp; Technology, Pollachi, India","2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)","","2015","","","1","7","Randomized testing is an effective method for testing software units. Thoroughness of randomized unit testing is according to the settings of optimal parameters. Randomized testing uses randomization for some aspects of test input data. Designing Genetic algorithm (GA) is somewhat of a black art. The feature subset selection (FSS) tool is used with GA to assess and to reduce the size and the content of the test case. FSS can be used to find and remove unnecessary parts of the search control automatically. The existing system does not cover all test data in test cases for the reason that it can quickly generate many test cases and does not consider the target method. Thus GA for Randomized unit testing has not achieves high coverage and does not produce better optimal test data. In the proposed method, Particle Swarm Optimization (PSO) algorithm is used for randomized unit testing. PSO algorithm is used to evaluate the target method solutions for test coverage in test data. The main goal is to generate the optimal test parameter, to reduce the size of test case generation and to achieve high coverage of the units under test. PSO achieves high coverage and produce optimal value. PSO algorithm is enhanced weighted value. Weighted Particle Swarm Optimization (WPSO) algorithm uses weight value in calculating the mean best position for each particle. It improves the efficiency of the system and achieves high coverage of the units under test within 5% of the time with better accuracy.","","978-1-4799-6085-9978-1-4799-6084-2978-1-4799-6083","10.1109/ICECCT.2015.7226068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226068","Randomized unit testing;Genetic algorithm;Feature Sub Set Selection;Particle Swarm Optimization algorithm;Weighted Particle Swarm Optimization algorithm","Genetic algorithms;Software;Software algorithms;Heuristic algorithms;Accuracy;Optimization;Generators","feature selection;genetic algorithms;particle swarm optimisation;program testing","weighted particle swarm optimization algorithm;WPSO algorithm;randomized unit testing;software unit testing;genetic algorithm;GA;feature subset selection;FSS","","1","20","","","","","","IEEE","IEEE Conferences"
"Migrating Birds Optimization based strategies for Pairwise testing","H. L. Zakaria; K. Z. Zamli","School of Computer and Communication Engineering, Universiti Malaysia Perlis(uniMAP), 02600 Arau, MALAYSIA; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang(UMP), Lebuhraya Tun Razak, 26300 Kuantan, MALAYSIA","2015 9th Malaysian Software Engineering Conference (MySEC)","","2015","","","19","24","Exhaustive testing of all possible combinations of input parameter values of a large system is impossible. Here, pairwise testing technique is often chosen owing to its effectiveness for bug detection. For pairwise testing, test cases are designed to cover all possible pair combinations of input parameter values at least once. In this paper, we investigate the adoption of Migrating Birds Optimization (MBO) algorithm as a strategy to find an optimal solution for pairwise test data reduction. Two strategies have been proposed; the first strategy implements the basic MBO algorithm, called Pairwise MBO Strategy (PMBOS) and the second strategy implements an improved Pairwise MBO strategy, called iPMBOS. The iPMBOS enhances the PMBOS with multiple neighborhood structures and elitism. Based on the published benchmarking results, these two strategies offers competitive results with most existing strategies in terms of the generated test size. We also noted that iPMBOS outperforms PMBOS in several parameter configurations, especially when the test size generated is relatively small.","","978-1-4673-8227-4978-1-4673-8226","10.1109/MySEC.2015.7475189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475189","pairwise testing;MBO Algorithm;PMBOS;iPMBOS","Birds;Testing;Optimization;Algorithm design and analysis;Software;Software engineering;Computers","optimisation;program testing","pairwise testing technique;test cases;input parameter values;migrating birds optimization algorithm;improved pairwise MBO strategy;iPMBOS;multiple neighborhood structures;parameter configurations","","","27","","","","","","IEEE","IEEE Conferences"
"Modern DevOps: Optimizing software development through effective system interactions","C. A. Cois; J. Yankel; A. Connell","Carnegie Mellon University, Software Engineering Institute; Carnegie Mellon University, Software Engineering Institute; Carnegie Mellon University, Software Engineering Institute","2014 IEEE International Professional Communication Conference (IPCC)","","2014","","","1","7","Software development processes are fundamentally based on efficient and effective communication. Communication between engineers, between engineers and managers, and between teams and clients are all essential components of a successful project. Requirements must be effectively transferred from client to engineer, specifications must be transitioned from architect to engineer, and constant communication between project team members, managers, and clients throughout the project life cycle is critical to the success of projects of any complexity. To succeed in a world where technologies, requirements, ideas, tools, and timelines are constantly changing, information must be accurate, readily available, easily found, and ideally delivered constantly, in real-time, to all team members. To meet these challenges, modern software development has evolved to encompass key concepts of adaptability to change and data-driven project management. A recent movement dubbed DevOps has attempted to use automated systems to bridge the information gap between project team entities and to enforce rigorous processes to ensure real-time communications. In this paper, the authors frame this challenge as a communications problem that can be addressed by the introduction of specifically designed autonomous system actors and processes. Successful implementation of such a methodology will enable efficient, effective, and immediate data collection, synthesis, and transfer of information between all requisite entities within the software project. A generalized model of DevOps will be presented and analyzed, offering a formalization of the communications and actors requisite to any effective software development process. These concepts will be further developed to illustrate the information flow between human and system actors, and explore how this model can be used to optimize the processes of a software development team to maximize productivity and quality of work products.","2158-091X;2158-1002","978-1-4799-3749","10.1109/IPCC.2014.7020388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7020388","Software engineering;Software development;DevOps;Project management;Process improvement","Software;Productivity;Software engineering;Project management;Real-time systems;Testing;Context","productivity;project management;software development management;team working","software development optimization;system interactions;software development processes;project life cycle;project success;data-driven project management;automated systems;information gap;project team entities;real-time communications;communications problem;data collection;data synthesis;information transfer;software project;DevOps generalized model;human actors;system actors;software development team;productivity;work products quality","","8","21","","","","","","IEEE","IEEE Conferences"
"Improving Multi-Objective Test Case Selection by Injecting Diversity in Genetic Algorithms","A. Panichella; R. Oliveto; M. D. Penta; A. De Lucia","Department of Mathematics and Computer Science, University of Salerno, Fisciano, Salerno, Italy; Department of Bioscience and Territory, University of Molise, Pesche, Isernia, Italy; Department of Engineering, University of Sannio, Benevento, Italy; Department of Mathematics and Computer Science, University of Salerno, Fisciano, Salerno, Italy","IEEE Transactions on Software Engineering","","2015","41","4","358","383","A way to reduce the cost of regression testing consists of selecting or prioritizing subsets of test cases from a test suite according to some criteria. Besides greedy algorithms, cost cognizant additional greedy algorithms, multi-objective optimization algorithms, and multi-objective genetic algorithms (MOGAs), have also been proposed to tackle this problem. However, previous studies have shown that there is no clear winner between greedy and MOGAs, and that their combination does not necessarily produce better results. In this paper we show that the optimality of MOGAs can be significantly improved by diversifying the solutions (sub-sets of the test suite) generated during the search process. Specifically, we introduce a new MOGA, coined as DIversity based Genetic Algorithm (DIV-GA), based on the mechanisms of orthogonal design and orthogonal evolution that increase diversity by injecting new orthogonal individuals during the search process. Results of an empirical study conducted on eleven programs show that DIV-GA outperforms both greedy algorithms and the traditional MOGAs from the optimality point of view. Moreover, the solutions (sub-sets of the test suite) provided by DIV-GA are able to detect more faults than the other algorithms, while keeping the same test execution cost.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2364175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6936894","Test Case Selection;Regression Testing;Orthogonal Design;Singular Value Decomposition;Genetic Algorithms;Empirical Studies;Test case selection;regression testing;orthogonal design;singular value decomposition;genetic algorithms;empirical studies","Optimization;Greedy algorithms;Testing;Linear programming;Genetic algorithms;Genetics;Sociology","genetic algorithms;greedy algorithms;program testing;search problems","multiobjective test case selection improvement;regression testing cost reduction;test case subset prioritization;test case subset selection;greedy algorithms;multiobjective optimization algorithms;multiobjective genetic algorithms;MOGA optimality improvement;test suite subsets;search process;diversity-based genetic algorithm;DIV-GA;orthogonal design mechanism;orthogonal evolution mechanism;empirical analysis","","21","76","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic software fault localization based on artificial bee colony","L. Huang; J. Ai","School of Reliability and Systems Engineering, Beihang University, Beijing 100191, China; School of Reliability and Systems Engineering, Beihang University, Beijing 100191, China","Journal of Systems Engineering and Electronics","","2015","26","6","1325","1332","Software debugging accounts for a vast majority of the financial and time costs in software developing and maintenance. Thus, approaches of software fault localization that can help automate the debugging process have become a hot topic in the field of software engineering. Given the great demand for software fault localization, an approach based on the artificial bee colony (ABC) algorithm is proposed to be integrated with other related techniques. In this process, the source program is initially instrumented after analyzing the dependence information. The test case sets are then compiled and run on the instrumented program, and execution results are input to the ABC algorithm. The algorithm can determine the largest fitness value and best food source by calculating the average fitness of the employed bees in the iterative process. The program unit with the highest suspicion score corresponding to the best test case set is regarded as the final fault localization. Experiments are conducted with the TCAS program in the Siemens suite. Results demonstrate that the proposed fault localization method is effective and efficient. The ABC algorithm can efficiently avoid the local optimum, and ensure the validity of the fault location to a larger extent.","1004-4132","","10.1109/JSEE.2015.00145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375766","software debugging;software fault localization;artificial bee colony (ABC) algorithm;program instrumentation","Software algorithms;Algorithm design and analysis;Systems engineering and theory;Instruments;Search problems;Software testing;Fault diagnosis;Software debugging","ant colony optimisation;iterative methods;program compilers;program debugging;program testing;software fault tolerance;software maintenance","automatic software fault localization;artificial bee colony;software debugging process;software development;software maintenance;software engineering;artificial bee colony algorithm;ABC algorithm;source program;test case sets;dependence information analysis;iterative process;program unit;TCAS program","","1","","","","","","","BIAI","BIAI Journals & Magazines"
"A comparative analysis of ant colony optimization for its applications into software testing","P. Vats; M. Mandot; A. Gosain","AIMACT, Banasthali University, Rajasthan, India; J.R.N. Rajasthan Vidyapith, Rajasthan, India; USICT, GGSIPU, New Delhi, India","2014 Innovative Applications of Computational Intelligence on Power, Energy and Controls with their impact on Humanity (CIPECH)","","2014","","","476","481","Ant Colony Optimization are metaheuristic algorithms that uses the search based algorithms as their base. It applies the natural phenomenon of finding the best possible path by the Ants that is covering the minimum distance from the food source to the ant colony, which will be followed by the rest of the ants, resulting into the optimized path. This phenomenon can be applied to provide optimized solutions to solve some complex computational problems. In this paper, we have carried out a review for the applications of the Ant colony Optimization algorithms in context to various level of the Software Testing, thus proving their worth in providing solutions to the various aspects of the Software Testing.","","978-1-4799-5871-9978-1-4799-5870","10.1109/CIPECH.2014.7019110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019110","Optimized;Metaheuristic;Code coverage;Pheromone;Swarm Intelligence","Ant colony optimization;Software testing;Software algorithms;Java;Algorithm design and analysis;Software","ant colony optimisation;program testing","ant colony optimization;software testing;meta-heuristic algorithms;search based algorithms","","","29","","","","","","IEEE","IEEE Conferences"
"PWTool, A novel automated tool for pairwise testing","R. K. Sungkur; H. Muhamodsaroar","Computer Science and Engineering Dept, University of Mauritius, Mauritius; Computer Science and Engineering Dept, University of Mauritius, Mauritius","2013 Africon","","2013","","","1","5","In the current culture of software development, every software development group consists of a dedicated software testing team whose main objective is to deliver bug-free products to the clients. No matter how hard the test engineers strive to catch all the defects before the product is released, they always creep in and often reappear even with the best manual testing processes. Because of this, savvy test managers have realised that automated testing is an essential component of successful development projects since once automated tests are created, they can easily be repeated and extended to perform tasks that would have been impossible or too time-consuming with manual testing. This paper investigates the suitability of pair-wise testing in automated testing and demonstrates how the In-Parameter Order (IPO) algorithm used in the generation of pair-wise combinations can be optimised. PWTool, a novel web-based application for generating pair-wise test cases mainly for online systems is introduced.","2153-0033;2153-0025","978-1-4673-5943-6978-1-4673-5940","10.1109/AFRCON.2013.6757749","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6757749","Automated Testing;Pair-wise Combinations;InParameter Order (IPO);Test Case;Vertical Growth","Testing;Greedy algorithms;Algorithm design and analysis;Arrays;Web pages;Generators;Heuristic algorithms","automatic testing;Internet;program testing;project management;software tools","PWTool;pairwise testing;software development group;software testing team;bug-free products;automated testing tool;software development project;in-parameter order algorithm;IPO algorithm;pair-wise combination generation;Web-based application;pair-wise test case generation;online systems","","","14","","","","","","IEEE","IEEE Conferences"
"Tag recommendation in software information sites","X. Xia; D. Lo; X. Wang; B. Zhou","College of Computer Science and Technology, Zhejiang University; School of Information Systems, Singapore Management University; College of Computer Science and Technology, Zhejiang University; College of Computer Science and Technology, Zhejiang University","2013 10th Working Conference on Mining Software Repositories (MSR)","","2013","","","287","296","Nowadays, software engineers use a variety of online media to search and become informed of new and interesting technologies, and to learn from and help one another. We refer to these kinds of online media which help software engineers improve their performance in software development, maintenance and test processes as software information sites. It is common to see tags in software information sites and many sites allow users to tag various objects with their own words. Users increasingly use tags to describe the most important features of their posted contents or projects. In this paper, we propose TagCombine, an automatic tag recommendation method which analyzes objects in software information sites. TagCombine has 3 different components: 1. multilabel ranking component which considers tag recommendation as a multi-label learning problem; 2. similarity based ranking component which recommends tags from similar objects; 3. tag-term based ranking component which considers the relationship between different terms and tags, and recommends tags after analyzing the terms in the objects. We evaluate TagCombine on 2 software information sites, StackOverflow and Freecode, which contain 47,668 and 39,231 text documents, respectively, and 437 and 243 tags, respectively. Experiment results show that for StackOverflow, our TagCombine achieves recall@5 and recall@10 scores of 0.5964 and 0.7239, respectively; For Freecode, it achieves recall@5 and recall@10 scores of 0.6391 and 0.7773, respectively. Moreover, averaging over StackOverflow and Freecode results, we improve TagRec proposed by Al-Kofahi et al. by 22.65% and 14.95%, and the tag recommendation method proposed by Zangerle et al. by 18.5% and 7.35% for recall@5 and recall@10 scores.","2160-1860;2160-1852","978-1-4673-2936-1978-1-4799-0345","10.1109/MSR.2013.6624040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624040","Software Information Sites;Online Media;Tag Recommendation;TagCombine","Software;Media;Software algorithms;Vectors;Prediction algorithms;Search problems;Educational institutions","learning (artificial intelligence);program testing;recommender systems;social networking (online);software maintenance;text analysis","software information sites;software engineers;online media;software development;software maintenance;software test processes;TagCombine;automatic tag recommendation method;multilabel ranking component;multilabel learning problem;similarity based ranking component;tag-term based ranking component;StackOverflow;Freecode;text documents;TagRec","","47","37","","","","","","IEEE","IEEE Conferences"
"AFIT's laboratory test equipment to optimise the integrated avionics systems for polish military aircrafts","S. Michalak; A. Szelmanowski; A. Pazur","Air Force Institute of Technology, Warsaw, Poland; Air Force Institute of Technology, Warsaw, Poland; Air Force Institute of Technology, Warsaw, Poland","2014 IEEE Metrology for Aerospace (MetroAeroSpace)","","2014","","","113","116","The paper has been intended to present research tools used at the Air Force Institute of Technology (AFIT) to activate and test hardware and software implemented in the integrated avionics systems. Particular attention has been paid to the research stand (built under the R&D project of the Ministry of Science and Higher Education) intended to optimize avionics systems integrated with digital data buses employed (according to, among other things, MIL-1553B and ARINC-429 standards). Specialized research equipment/test devices used to test software, including - apart from other things - test-patterns generators (within the range of display capability of the secondary head-up display (SHUD)) and information efficiency testers (for the SHUD and up-front control panel (UFCP) thereof) have also been presented.","","978-1-4799-2069","10.1109/MetroAeroSpace.2014.6865904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6865904","integrated avionics systems;measuring equipment;optimization of functionality","Aerospace electronics;Software;Laboratories;Military aircraft;Aircraft navigation;Radio navigation","aircraft testing;military aircraft;military avionics;military computing;program testing;test equipment","AFIT laboratory test equipment;integrated avionics systems;Polish military aircrafts;Air Force Institute of Technology;software testing;hardware testing;digital data buses;research equipment-test devices;test-patterns generators;secondary head-up display;SHUD;information efficiency testers;up-front control panel;UFCP","","","6","","","","","","IEEE","IEEE Conferences"
"Kinematic analysis and simulation of ultrasonic testing manipulator","X. Li; X. Zhao; J. Hao; Z. Lu; Y. Huang; L. Yang","School of Mechanical Engineering, Beijing Institute of Technology, 100081, China; School of Mechanical Engineering, Beijing Institute of Technology, 100081, China; School of Mechanical Engineering, Beijing Institute of Technology, 100081, China; School of Mechanical Engineering, Beijing Institute of Technology, 100081, China; School of Mechanical Engineering, Beijing Institute of Technology, 100081, China; School of Mechanical Engineering, Beijing Institute of Technology, 100081, China","2014 IEEE Far East Forum on Nondestructive Evaluation/Testing","","2014","","","148","152","Ultrasonic testing for these workpieces with complex shapes is still a challenge in NDT filed. In order to address this problem, this paper provide an effective solution for curved surface scanning using Staubli TX90XL manipulator with six freedom degrees. Here an arm-wrist separateness method is adopted to solve the inverse of manipulator and a shortest distance rule is used to optimize the inverse solutions. Furthermore, we have developed a 3D application software to simulate the ultrasonic trajectory planning for complex shape specimens. Finally, the validity of this scanning method is verified by the C-scan results of the specimen with a curved surface.","","978-1-4799-4730-0978-1-4799-4731-7978-1-4799-6286","10.1109/FENDT.2014.6928251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928251","ulrasonic testing;kinematic analysis;kinematic simulation","Manipulators;Kinematics;Joints;Acoustics;Testing;Trajectory;Shape","manipulator kinematics;optimisation;ultrasonic materials testing","kinematic analysis;ultrasonic testing manipulator simulation;workpieces;complex shapes;NDT filed;curved surface scanning;Staubli TX90XL manipulator;freedom degrees;arm-wrist separateness method;manipulator inverse;distance rule;inverse solution optimization;3D application software;ultrasonic trajectory planning;complex shape specimens;scanning method;C-scan results;curved surface specimen","","","8","","","","","","IEEE","IEEE Conferences"
"GRT at the SBST 2015 Tool Competition","L. Ma; C. Artho; C. Zhang; H. Sato; M. Hagiya; Y. Tanabe; M. Yamamoto","NA; NA; NA; NA; NA; NA; NA","2015 IEEE/ACM 8th International Workshop on Search-Based Software Testing","","2015","","","48","51","GRT (Guided Random Testing) is an automatic test generation tool for Java code, which leverages static and dynamic program analysis to guide run-time test generation. In this paper, we summarize competition results and experiences of GRT in participating in SBST 2015, where GRT ranked first with a score of 203.73 points over 63 Java classes from 10 packages of 9 open-source software projects.","","978-1-4673-7079","10.1109/SBST.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7173592","Software testing;automatic test case generation;program analysis;random testing","Benchmark testing;Java;Protocols;Software;Impurities;Manuals","automatic testing;Java;program diagnostics;public domain software","dynamic program analysis;static program analysis;run-time test generation;open-source software projects;Search-based Software Testing;guided random testing;Java code;automatic test generation tool;SBST 2015 Tool Competition;GRT","","8","11","","","","","","IEEE","IEEE Conferences"
"RisCal -- A Risk Estimation Tool for Software Engineering Purposes","C. Haisjackl; M. Felderer; R. Breu","NA; NA; NA","2013 39th Euromicro Conference on Software Engineering and Advanced Applications","","2013","","","292","299","Decision making in software engineering requires the consideration of risk information. The reliability of risk information is strongly influenced by the underlying risk estimation process which consists of the steps risk identification, risk analysis and risk prioritization. In this paper we present a novel risk estimation tool for software engineering pruposes called RisCal. RisCal is based on a generic risk model and supports the integration of manually and automatically determined metrics into the risk estimation. This makes the tool applicable for arbitrary software engineering activities like risk-based testing or release planning. We show how RisCal supports risk identification, analysis and prioritizations, provide an estimation example, and discuss its application to risk-based testing and release planning.","1089-6503;2376-9505","978-0-7695-5091","10.1109/SEAA.2013.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619524","Risk Estimation;Software Risk Management;Risk-based Testing;Release Planning;Test Management","Estimation;Measurement;Testing;Risk management;Planning;Software engineering","decision making;program testing;risk analysis;software metrics","RisCal;risk estimation tool;risk information;risk estimation process;risk identification;risk analysis;risk prioritization;generic risk model;manually determined metrics;automatically determined metrics;software engineering activities;risk-based testing;release planning;software engineering pruposes;decision making","","2","23","","","","","","IEEE","IEEE Conferences"
"How Does the UML Testing Profile Support Risk-Based Testing","S. Ali; T. Yue; A. Hoffmann; M. Wendland; A. Bagnato; E. Brosse; M. Schacher; Z. R. Dai","NA; NA; NA; NA; NA; NA; NA; NA","2014 IEEE International Symposium on Software Reliability Engineering Workshops","","2014","","","311","316","The increasing complexity of software-intensive systems raises a lot of challenges demanding new techniques for ensuring their overall quality. The risk of not meeting the expected level of quality has negative impact on business, customers, environment and people, especially in the context of safety/security-critical systems. The importance of risk assessment, analysis and management has been well understood both in the literature and practice, which has led to the definition of a number of well-known standards. In the recent years, Risk-Based Testing (RBT) is gaining more attention, especially focusing on test prioritization and selection based on risks. On the other hand, model-based testing (MBT) provides a systematic and automated way to facilitate rigorous testing of software-intensive systems. MBT has been an intense area of research and a large number of MBT techniques have been developed in literature and practice in the last decade. In this paper, we study the feasibility of combining RBT with MBT by using the upcoming version of UML Testing Profile (UTP 2) as the mechanism. We present potential traceability between RBT and UTP 2 concepts.","","978-1-4799-7377","10.1109/ISSREW.2014.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983859","Risk-based Testing;Model-based Testing;UML Testing Profile;Risk Assessment","Unified modeling language;Testing;Risk analysis;Security;Dynamic scheduling;Standards;Analytical models","program testing;risk analysis;Unified Modeling Language","UML testing profile;risk-based testing;model-based testing;software-intensive systems;test prioritization;test selection;MBT technique;UTP 2;RBT technique","","2","51","","","","","","IEEE","IEEE Conferences"
"An approach for feature-level bug prediction using test cases","P. Anand","Radisys Corporation, Bangalore, India","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","","2015","","","1111","1117","Bug Prediction approaches have traditionally generated a lot of interest primarily due to potential savings in terms of cost, manpower and reputation. Consequently, a number of approaches have been suggested based on code metrics, process metrics, previous defects, testing metrics and multivariate models. With respect to granularity of prediction, these approaches predict at the class level, file level, package level or binary level. This paper presents a novel approach to bug prediction by utilizing test cases execution path in the code in release i and ranks the software functionalities or features in decreasing order for future defects in release (i+1) due to the code churn. The approach derives importance from two facts - 1) The prediction is done at the feature level, instead of class, file, package or binary level, since it is an accepted fact in software systems that certain features are more critical than others and faulty working of these features can jeopardize the entire software system 2) The approach suggested is non-intrusive, in the sense that it can be easily integrated into existing software development life cycle without significant efforts. Due to unavailability of feature-based test cases and relatively less number of features in open source projects, which is a necessary requirement of the study, case studies were performed on twelve releases of four industrial projects. Additionally, the predictive accuracy was evaluated on eight releases of these four industrial projects using normalized discounted cumulative gain. These studies indicate the validity of this approach and demonstrates that the presented approach has an average normalized discounted cumulative gain of 0.684 for predicting the top 10 faulty features.","","978-1-4799-8792-4978-1-4799-8790-0978-1-4799-8791","10.1109/ICACCI.2015.7275759","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275759","Software bug prediction;software fault prediction;software defect prediction;code churn metrics;software defects;software reliability;software quality;test cases","Measurement;Testing;Industries;Computer bugs;Java;Software systems","program debugging;program testing;public domain software;software engineering","feature-level bug prediction;code metrics;process metrics;previous defects;testing metrics;multivariate models;class level;file level;package level;binary level;execution path;software functionalities;software development life cycle;open source projects;industrial projects;predictive accuracy;normalized discounted cumulative gain","","","36","","","","","","IEEE","IEEE Conferences"
"Study of bug prediction modeling using various entropy measures- a theoretical approach","H. D. Arora; V. Kumar; R. Sahni","Department of Applied Mathematics, Amity Institute of Applied Sciences, Amity University, Sector-125, Noida, Uttar Pradesh, India; Department of Mathematics, Amity School of Engineering and Technology, New Delhi-110061, India; Department of Applied Mathematics, Amity Institute of Applied Sciences, Amity University, Sector-125, Noida, Uttar Pradesh, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","5","Information theory concept was created by Claude Shannon in 1948. Entropy i.e. uncertainty measure informs how much information is there in an event. The more uncertain the event the more information it gives. In order to resolve this uncertainty, software testing is done and by running it, knowledge of a software system is gained. In this paper, a benchmark for modeling of bug prediction using different measures of entropy mainly Shannon, Renyi and Tsallis entropy have been presented. Information theory is also needed to enable the communication system to carry information from sender to receiver over a communication channel.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014697","Entropy;Information Theory;Uncertainty;Software Testing;Bug prediction;Software reliability growth models","Entropy;Mathematical model;Complexity theory;Software;Software reliability;Software measurement;Equations","entropy;program debugging;program testing;software reliability","bug prediction modeling;information theory;uncertainty measure;software testing;Shannon entropy;Renyi entropy;Tsallis entropy","","1","19","","","","","","IEEE","IEEE Conferences"
"Investigating the OO characteristics of software using CKJM metrics","B. Suri; S. Singhal","USICT, G.G.S.Indraprastha University, Sec-16C, Dwarka, New Delhi, India; USICT, G.G.S.Indraprastha University, Sec-16C, Dwarka, New Delhi, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","Software metrics are measures used to improve the software development process and quality. Various software metrics have been projected and analyzed over the years. This paper calculates and analyzes twelve object-oriented software metrics. The metrics calculated for our analysis were object-orented and based on the Chidamber and Kemerer metric suit. ckjm and intelliJIdea were the two previously alive open-source tools used for metric calculation. These metrics were tested for ten sample open-source programs written in Java. The frequency and descriptive analysis was then performed on the obtained results using SPSS tool. The effect of these metrics on the cohesion, coupling and inheritance characteristics of the considered ten programs was then analyzed.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359254","software metrics;object-oriented;ckjm;analysis;intelliJIdea","Software;Java;Software metrics;Couplings;Complexity theory","inheritance;Java;object-oriented methods;public domain software;software metrics;software quality","software OO characteristics;CKJM metric;software metric;software development process improvement;software quality improvement;object-oriented software metrics;Chidamber and Kemerer metric suit;ckjm;intelliJIdea;open-source programs;Java;frequency analysis;descriptive analysis;SPSS tool;cohesion characteristics;coupling characteristics;inheritance characteristics","","2","19","","","","","","IEEE","IEEE Conferences"
"A highly efficient particle swarm optimizer for super high-dimensional complex functions optimization","K. Lei","Intelligent Software and Software Engineering Laboratory Southwest University Chongqing, 400715, China","2014 IEEE 5th International Conference on Software Engineering and Service Science","","2014","","","310","313","Because of the complexity of super high-dimensional complex functions with the large numbers of global and local optima, the general particle swarm optimization methods are slow speed on convergence and easy to be trapped in local optima. In this paper, a highly efficient particle swarm optimizer is proposed, which employ the adaptive strategy of inertia factor, global optimum, search space and velocity in each cycle to plan large-scale space global search and refined local search as a whole according to the fitness change of swarm in optimization process of the functions, and to quicken convergence speed, avoid premature problem, economize computational expenses, and obtain global optimum. We test the new algorithm and compare it with other published methods on several super high dimensional complex functions, the experimental results showed clearly the revised algorithm can rapidly converge at high quality solutions.","2327-0594;2327-0586","978-1-4799-3279-5978-1-4799-3278-8978-1-4799-3277","10.1109/ICSESS.2014.6933570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933570","particle swarm optimizer;high-dimensional Complex function;premature problem","Convergence;Particle swarm optimization;Search problems;Optimization;Heuristic algorithms;Algorithm design and analysis;Equations","convergence of numerical methods;particle swarm optimisation;search problems","particle swarm optimizer;super high-dimensional complex functions optimization;global optima;local optima;general particle swarm optimization methods;global optimum;search space;large-scale space global search;refined local search;convergence speed;super high dimensional complex functions","","2","13","","","","","","IEEE","IEEE Conferences"
"Ranking Software Components for Pragmatic Reuse","M. Kessel; C. Atkinson","NA; NA","2015 IEEE/ACM 6th International Workshop on Emerging Trends in Software Metrics","","2015","","","63","66","Pragmatic software reuse, in which existing software components are invasively adapted for use in new projects, involves three main activities - selection, adaptation and integration. Most of the academic research into pragmatic research to date has focused on the second of these activities, adaptation, especially the definition of reuse plans and verification of invasive changes, even though the selection activity is arguably the most important and effort-intensive of the three activities. There is therefore a great deal of scope for improving the level of support provided by software search engines and recommendation tools to pragmatic reusers of software components. Test-driven search engines are particularly promising in this regard since they possess the inherent ability to &#x201C;evaluate&#x201D; components from the perspective of users' reuse scenarios. In this paper we discuss some of the main issues involved in improving the selection support for pragmatic reuse provided by test-driven search engines, describe some new metrics that can help address these issues, and present the outline of an approach for ranking software components in search results.","2327-0969;2327-0950","978-1-4673-7103-2978-1-4673-7102","10.1109/WETSoM.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7181592","","Pragmatics;Search engines;Software metrics;Software engineering;Software reusability","program testing;program verification;recommender systems;search engines;software reusability","software component ranking;pragmatic software reusability;software verification;software search engines;recommendation tools;test-driven search engines","","3","15","","","","","","IEEE","IEEE Conferences"
"Dynamic Random Testing with Parameter Adjustment","Z. Yang; B. Yin; J. Lv; K. Cai; S. S. Yau; J. Yu","NA; NA; NA; NA; NA; NA","2014 IEEE 38th International Computer Software and Applications Conference Workshops","","2014","","","37","42","In order to improve traditional Random Partition Testing (RPT) strategy, Dynamic Random Testing (DRT) strategy is proposed. By DRT, testing profile is dynamically updated according to the previous test case execution result. The effectiveness of DRT depends on parameter settings. In this paper, a strategy is presented for extending the DRT with parameter adjustment in order to guarantee that the optimized boundaries. Using this strategy, the parameters in DRT are adjusted based on testing history during testing process. Case studies are presented and analyzed to show the improvement of DRT.","","978-1-4799-3578","10.1109/COMPSACW.2014.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903102","software testing;dynamic random testing;adjusted dynamic random testing","Testing;Software;History;Sufficient conditions;Parameter estimation;Flexible printed circuits;Aerodynamics","dynamic testing;program testing","dynamic random testing;parameter adjustment;RPT strategy;random partition testing;DRT strategy;test case execution;testing history;software testing","","2","17","","","","","","IEEE","IEEE Conferences"
"Multi-objective optimization of test sequence generation using multi-objective firefly algorithm (MOFA)","N. Iqbal; K. Zafar; W. Zyad","Department of Computer Science National University of Computer and Emerging Sciences Lahore, Pakistan; Department of Computer Science National University of Computer and Emerging Sciences Lahore, Pakistan; Department of Computer Science National University of Computer and Emerging Sciences Lahore, Pakistan","2014 International Conference on Robotics and Emerging Allied Technologies in Engineering (iCREATE)","","2014","","","214","220","Software testing is one of the essential parts of the software development life cycle. In software industry, the testing cost can be approximately 50% of the total cost of a software project so efficient ways of testing software are crucially important in reducing costs, time and effort. There are two major methods of software testing; black-box testing (focuses only what the software can do) and white-box testing (tests the internal structure of the software under consideration thoroughly and the ultimate goal is to write test cases that force the program coverage.) For program coverage, identification of suitable paths is one of the major software testing problems. These test paths are known as test sequences. Generation of automated and effective test sequences is also a very difficult task in software testing process. In the proposed work, the problem “Test Sequence Generation” is considered as a multi-objective optimization problem by having two objectives to be optimized simultaneously, Oracle Cost, and Path Priority. In real time environment, there are many constraints which have to be fulfilled when dealing with an effective testing. So, such test sequences that meet multiple objectives simultaneously are generated in order to reduce the testing efforts. To solve this problem a recently developed algorithm “Multi-Objective Firefly Algorithm (MOFA)” is used. The problem “Test Sequence Generation” is first implemented by Firefly Algorithm and later by using MOFA-considering the problem as Multi-Objective Optimization Problem. The proposed technique implementing test sequences with multiple (two) objectives and its results are presented.","","978-1-4799-5132-1978-1-4799-5131-4978-1-4799-5130","10.1109/iCREATE.2014.6828368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6828368","Firefly algorithm;multi-objective firefly algorithm;objective function;cyclometic complexity;control flow graph","Optimization;Software;Software testing;Linear programming;Unified modeling language;Libraries","evolutionary computation;program testing","multiobjective optimization;test sequence generation;multiobjective firefly algorithm;MOFA;software testing;software development life cycle;software industry;software project cost;black-box testing;white-box testing;program coverage;oracle cost;path priority","","","19","","","","","","IEEE","IEEE Conferences"
"Exploring Test Suite Diversification and Code Coverage in Multi-Objective Test Case Selection","D. Mondal; H. Hemmati; S. Durocher","NA; NA; NA","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","","2015","","","1","10","Test case selection is a classic testing technique to choose a subset of existing test cases for execution, due to the limited budget and tight deadlines. While 'code coverage' is the state of practice among test case selection heuristics, recent literature has shown that `test case diversity' is also a very promising approach. In this paper, we first compare these two heuristics for test case selection in several real-world case studies (Apache Ant, Derby, JBoss, NanoXML and Math). The results show that neither of the two techniques completely dominates the other, but they can potentially be complementary. Therefore, we next propose a novel approach that maximizes both code coverage and diversity among the selected test cases using NSGA-II multi- objective optimization, and the results show a significant improvement in fault detection rate. Specifically, sometimes this novel approach detects up to 16%(Ant), 10%(JBoss), and 14% (Math) more faults compared to either of coverage or diversity-based approaches, when the testing budget is less than 20% of the entire test suite execution cost.","2159-4848","978-1-4799-7125","10.1109/ICST.2015.7102588","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102588","","Optimization;Testing;Fault detection;Linear programming;Shape;Diversity reception;Hamming distance","genetic algorithms;program testing;software fault tolerance","test suite diversification;multiobjective test case selection;testing technique;code coverage;test case selection heuristics;test case diversity;Apache Ant;Derby;JBoss;NanoXML;Math;code diversity;NSGA-II multiobjective optimization;fault detection rate;testing budget;test suite execution cost;genetic algorithm","","10","29","","","","","","IEEE","IEEE Conferences"
"Dynamic transmission network expansion planning. case study for 50 buses test power system","A. Simo; C. Barbulescu; S. Kilyeni; S. Renghea; V. Popescu","Politehnica University Timisoara, Romania, Power Systems Department, Power Systems Analysis and Optimization Research Center; Politehnica University Timisoara, Romania, Power Systems Department, Power Systems Analysis and Optimization Research Center; Politehnica University Timisoara, Romania, Power Systems Department, Power Systems Analysis and Optimization Research Center; Politehnica University Timisoara, Romania, Power Systems Department, Power Systems Analysis and Optimization Research Center; Politehnica University Timisoara, Romania, Power Systems Department, Power Systems Analysis and Optimization Research Center","2015 50th International Universities Power Engineering Conference (UPEC)","","2015","","","1","6","The paper is focusing on dynamic transmission network expansion planning (TNEP). The TNEP problem has been approached from the retrospective point of view. To achieve this goal, the authors are developing two software-tools in Matlab environment. Power flow computing is performed using conventional methods. Optimal power flow and network expansion are performed using artificial intelligence methods. Within this field, two techniques have been tackled: particle swarm optimization (PSO) and genetic algorithms (GA). Case study refers to 50 buses test power system developed within Power Systems Department.","","978-1-4673-9682-0978-1-4673-9683","10.1109/UPEC.2015.7339920","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339920","artificial intelligence;dynamic expansion planning;retrospective approach;transmission network;software-tool;optimization","Power system dynamics;Planning;Genetic algorithms;Optimization;Load flow;Mathematical model","genetic algorithms;mathematics computing;particle swarm optimisation;power engineering computing;power transmission planning;software tools","dynamic transmission network expansion planning;buses test power system;TNEP problem;software-tools;Matlab environment;power flow computing;artificial intelligence methods;particle swarm optimization;PSO;genetic algorithms;GA;Power Systems Department","","","14","","","","","","IEEE","IEEE Conferences"
"Priority Integration for Weighted Combinatorial Testing","E. Choi; T. Kitamura; C. Artho; A. Yamada; Y. Oiwa","NA; NA; NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","2","","242","247","Priorities (weights) for parameter values can improve the effectiveness of combinatorial testing. Previous approaches have employed weights to derive high-priority test cases either earlier or more frequently. Our approach integrates these order-focused and frequency-focused prioritizations. We show that our priority integration realizes a small test suite providing high-priority test cases early and frequently in a good balance. We also propose two algorithms that apply our priority integration to existing combinatorial test generation algorithms. Experimental results using numerous test models show that our approach improves the existing approaches w.r.t. Order-focused and frequency-focused metrics, while overheads in the size and generation time of test suites are small.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273624","Combinatorial testing;Pairwise testing;Prioritized Testing;Priority weight;Weight coverage;KL divergence","Optical wavelength conversion;Measurement;Browsers;Benchmark testing;Focusing;Algorithm design and analysis","program testing","priority integration;weighted combinatorial testing;high-priority test cases;order-focused prioritizations;frequency-focused prioritizations;combinatorial test generation algorithms","","5","10","","","","","","IEEE","IEEE Conferences"
"Optimized hybrid verification of embedded software","J. Behrend; A. Gruenhage; D. Schroeder; D. Lettnin; J. Ruf; T. Kropf; W. Rosenstiel","Department of Computer Engineering, University of Tu¨bingen, Sand 13, 72076 Tübingen, Germany; Department of Computer Engineering, University of Tu¨bingen, Sand 13, 72076 Tübingen, Germany; Graduate Program in Computer Science (PPGCC), Federal University of Santa Catarina, Campus Universitário s/n - Trindade - CEP 88040-900, Floriano´polis, SC - Brazil; Graduate Program in Computer Science (PPGCC), Federal University of Santa Catarina, Campus Universitário s/n - Trindade - CEP 88040-900, Floriano´polis, SC - Brazil; Department of Computer Engineering, University of Tu¨bingen, Sand 13, 72076 Tübingen, Germany; Department of Computer Engineering, University of Tu¨bingen, Sand 13, 72076 Tübingen, Germany; Department of Computer Engineering, University of Tu¨bingen, Sand 13, 72076 Tübingen, Germany","2014 15th Latin American Test Workshop - LATW","","2014","","","1","6","The verification of embedded software has become an important subject over the last years. However, neither standalone verification approaches, like simulation-based or formal verification, nor state-of-the-art hybrid/semiformal verification approaches are able to verify large and complex embedded software with hardware dependencies. This work presents an optimized scalable hybrid verification approach for the verification of embedded software with hardware dependencies using a mixed bottom-up/top-down algorithm with optimized static parameter assignment (SPA). These algorithms and methodologies like SPA and counterexample guided simulation are used to combine simulation-based and formal verification in a new way. SPA offers a way to interact between dynamic and static verification approaches based on an automated ranking heuristic of possible function parameters according to the impact on the model size. Furthermore, SPA inserts initialization code for specific function parameters into the source code under test and supports model building and optimization algorithms to reduce the state space. We have successfully applied this optimized hybrid verification methodology to an embedded software application: Motorola's Powerstone Benchmark suite. The results show that our approach scales better than stand-alone software model checkers to reach deep state spaces.","2373-0862","978-1-4799-4711","10.1109/LATW.2014.6841906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6841906","","Embedded software;Switches;Benchmark testing;Hardware;Monitoring;Software algorithms","embedded systems;optimisation;program diagnostics;program verification;source code (software)","embedded software verification;optimized scalable hybrid verification approach;mixed bottom-up-top-down algorithm;hardware dependency;static parameter assignment;optimized SPA;simulation-based verification;formal verification;static verification approach;dynamic verification approach;automated ranking heuristic;initialization code;specific function parameter;source code;model building algorithm;optimization algorithm;state space reduction","","4","30","","","","","","IEEE","IEEE Conferences"
"Taming a Fuzzer Using Delta Debugging Trails","Y. Pei; A. Christi; X. Fern; A. Groce; W. Wong","NA; NA; NA; NA; NA","2014 IEEE International Conference on Data Mining Workshop","","2014","","","840","843","Fuzzers, or random testing tools, are powerful tools for finding bugs. A major problem with using fuzzersis that they often trigger many bugs that are already known. The fuzzer taming problem addresses this issue by ordering bug-triggering random test cases generated by a fuzzer such that test cases exposing diverse bugs are found early in the ranking. Previous work on fuzzer taming first reduces each test case into a minimal failure-inducing test case using delta debugging, then finds the ordering by applying the Furthest Point First algorithm over the reduced test cases. During the delta debugging process, a sequence of failing test cases is generated (the ""delta debugging trail""). We hypothesize that these additional failing test cases also contain relevant information about the bug and could be useful for fuzzertaming. In this paper, we propose to use these additional failing test cases generated during delta debugging to help tame fuzzers. Our experiments show that this allows for more diverse bugs to be found early in the furthest point first ranking.","2375-9232;2375-9259","978-1-4799-4274-9978-1-4799-4275","10.1109/ICDMW.2014.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7022682","Software Testing;Automated Testing;Fuzzing;Fuzzer Taming;Test-Case Reduction","Computer bugs;Debugging;Testing;Engines;Software;Feature extraction;Conferences","program debugging;program testing;software tools","fuzzer taming;delta debugging trails;random testing tools;bug-triggering random test cases;minimal failure-inducing test case;furthest point first ranking algorithm","","","12","","","","","","IEEE","IEEE Conferences"
"Determining Integration and Test Orders in the Presence of Modularization Restrictions","W. K. G. Assunção; T. Colanzi; S. Vergilio; A. Pozo","NA; NA; NA; NA","2013 27th Brazilian Symposium on Software Engineering","","2013","","","31","40","The Integration and Test Order problem is very known in the software testing area. It is related to the determination of a test order of modules that minimizes stub creation effort, and consequently testing costs. A solution approach based on Multi-Objective and Evolutionary Algorithms (MOEAs) achieved promising results, since these algorithms allow the use of different factors and measures that can affect the stubbing process, such as number of attributes and operations to be simulated by the stub. However, works based on such approach do not consider different modularization restrictions related to the software development environment. For example, the fact that some modules can be grouped into clusters to be developed and tested by independent teams. This is a very common practice in most organizations, particularly in that ones that adopt a distributed development process. Considering this fact, this paper introduces an evolutionary and multi-objective strategy to deal with such restrictions. The strategy was implemented and evaluated with real systems and three MOEAs. The results are analysed in order to compare the algorithms performance, and to better understand the problem in the presence of modularization restrictions. We observe an impact in the costs and a more complex search, when restrictions are considered. The obtained solutions are very useful and the strategy is applicable in practice.","","978-0-7695-5165","10.1109/SBES.2013.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800179","Software testing;multi-objective evolutionary algorithms;distributed development","Software;Context;Couplings;Testing;Optimization;Software algorithms;Java","evolutionary computation;integrated software;program testing","test order determination;modularization restrictions;integration determination;MOEA;testing costs;integration and test order problem;software testing;multiobjective and evolutionary algorithms","","","24","","","","","","IEEE","IEEE Conferences"
"Automated Prioritization of Metrics-Based Design Flaws in UML Class Diagrams","M. R. V. Chaudron; B. Katumba; X. Ran","NA; NA; NA","2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications","","2014","","","369","376","The importance of software architecture in software development prolongs throughout the entire software life cycle. This is because quality of the architectural design defines the structural aspects of the system that are difficult to change, and hence will affect most of the subsequent development and maintenance activities. This paper considers software design flaws (related to the system structure) and not flaws identified at run time (by testing). These design flaws are akin to what is described in the literature as anti-patterns, bad smells or rotting design. Recently, two tools that have been developed for quality assurance of software designs represented in the UML notation: SDMetrics and Metric View. However these tools are not considered practical because they report many design flaws which are not considered by developers (false positives). This paper explores an approach that tries to identify which design flaws should be considered important and which are not. To this end, we propose an approach for automated prioritization of software design flaws (BX approach), to facilitate developers to focus on important design flaws more effectively. We designed and implemented a tool (PoSDef) that implements this approach. The BX approach and the PoSDef tool have been validated using two open source projects and one large industrial system. Our validation consists of comparing our approach and tool with the existing design flaw tools. The evaluation has shown that the proposed approach could facilitate developers to identify and prioritize important design flaws effectively.","1089-6503;2376-9505","978-1-4799-5795","10.1109/SEAA.2014.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928837","Model-based Development;UML;Class Diagrams;Software Quality Assurance;Metrics;Software Design Flaws;Software Quality","Measurement;Unified modeling language;Software design;Motion pictures;Fault diagnosis;Educational institutions","program verification;software architecture;software quality;Unified Modeling Language","automated prioritization;metric-based design flaws;UML class diagrams;software architecture;software development prolongs;software life cycle;quality assurance;SDMetrics;metricview;BX approach;PoSDef;software validation","","","23","","","","","","IEEE","IEEE Conferences"
"Improving prioritization of software weaknesses using security models with AVUS","S. Renatus; C. Bartelheimer; J. Eichler","Fraunhofer Institute for Applied and Integrated Security AISEC, Germany; Fraunhofer Institute for Applied and Integrated Security AISEC, Germany; Fraunhofer Institute for Applied and Integrated Security AISEC, Germany","2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation (SCAM)","","2015","","","259","264","Testing tools for application security have become an integral part of secure development life-cycles. Despite their ability to spot important software weaknesses, the high number of findings require rigorous prioritization. Most testing tools provide generic ratings to support prioritization. Unfortunately, ratings from established tools lack context information especially with regard to the security requirements of respective components or source code. Thus experts often spend a great deal of time re-assessing the prioritization provided by these tools. This paper introduces our lightweight tool AVUS that adjusts context-free ratings of software weaknesses according to a user-defined security model. We also present a first evaluation applying AVUS to a well-known open source project and the findings of a popular, commercially available application security testing tool.","","978-1-4673-7529-0978-1-4673-7528","10.1109/SCAM.2015.7335423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335423","Secure software development;vulnerability scoring;contextual enrichment;security metrics","Security;Context;Kernel;Testing;Measurement;Complexity theory","program testing;public domain software;safety-critical software;source code (software)","software weakness;security model;AVUS;software testing tool;source code;context-free rating;user-defined security model;open source project;application security testing tool","","","21","","","","","","IEEE","IEEE Conferences"
"Effective Test Strategy model for ensuring FTR of hot-fix","A. Agarwal; N. K. Garg","R Systems International Limited, Noida, India; R Systems International Limited, Noida, India","2014 International Conference on Reliability Optimization and Information Technology (ICROIT)","","2014","","","40","43","The purpose of this paper is to present best practices like; early deployment of testers, recording details to analyze the defect injection reason, increasing effectiveness of the unit testing by the developers, continuous upgrade of knowledge base, publishing of resolution notes (which highlights the problem, corrective / preventive actions and the affected modules), staging of production environment to validate the fix before shipping and increasing test automation to eliminate the regression defects. This paper elaborates developing a Test Strategy model which helps in ensuring First Time Right (FTR) delivery of Hot-Fix and hence improving customer satisfaction.","","978-1-4799-2995-5978-1-4799-3958-9978-1-4799-2996","10.1109/ICROIT.2014.6798292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798292","FTR;Defect Injection Analysis;Test Strategy;Hot-Fix;Customer Satisfaction","Reliability;Analytical models;Customer satisfaction;Software;Gold;Information services;Electronic publishing","automatic test software;customer satisfaction;program testing;regression analysis","test strategy model;FTR;hot-fix package;defect injection;unit testing;knowledge base upgrade;resolution notes;production environment;test automation;regression defect elimination;first time right delivery;customer satisfaction improvement","","","6","","","","","","IEEE","IEEE Conferences"
"A Discrete Particle Swarm Optimization for Covering Array Generation","H. Wu; C. Nie; F. Kuo; H. Leung; C. J. Colbourn","State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; Department of Computing, Hong Kong Polytechnic University, Hong Kong; Arizona State University, Tempe, AZ, USA","IEEE Transactions on Evolutionary Computation","","2015","19","4","575","591","Software behavior depends on many factors. Combinatorial testing (CT) aims to generate small sets of test cases to uncover defects caused by those factors and their interactions. Covering array generation, a discrete optimization problem, is the most popular research area in the field of CT. Particle swarm optimization (PSO), an evolutionary search-based heuristic technique, has succeeded in generating covering arrays that are competitive in size. However, current PSO methods for covering array generation simply round the particle's position to an integer to handle the discrete search space. Moreover, no guidelines are available to effectively set PSOs parameters for this problem. In this paper, we extend the set-based PSO, an existing discrete PSO (DPSO) method, to covering array generation. Two auxiliary strategies (particle reinitialization and additional evaluation of gbest) are proposed to improve performance, and thus a novel DPSO for covering array generation is developed. Guidelines for parameter settings both for conventional PSO (CPSO) and for DPSO are developed systematically here. Discrete extensions of four existing PSO variants are developed, in order to further investigate the effectiveness of DPSO for covering array generation. Experiments show that CPSO can produce better results using the guidelines for parameter settings, and that DPSO can generate smaller covering arrays than CPSO and other existing evolutionary algorithms. DPSO is a promising improvement on PSO for covering array generation.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2014.2362532","National Natural Science Foundation of China; Research Fund for the Doctoral Program of Higher Education of China; Science Fund for Creative Research Groups of the National Natural Science Foundation of China; Major Program of National Natural Science Foundation of China; Australian Research Council Linkage; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6919298","Combinatorial Testing;Covering Array Generation;Particle Swarm Optimization;Combinatorial testing (CT);covering array generation;particle swarm optimization (PSO)","Arrays;Testing;Particle swarm optimization;Servers;Optimization;Guidelines","combinatorial mathematics;evolutionary computation;particle swarm optimisation;search problems","discrete particle swarm optimization;covering array generation;DPSO;combinatorial testing;CT;evolutionary search-based heuristic technique;discrete search space;auxiliary strategies;particle reinitialization;gbest;parameter settings;conventional PSO;CPSO;software behavior","","27","39","","","","","","IEEE","IEEE Journals & Magazines"
"Assessing optimization based strategies for t-way test suite generation: The case for flower-based strategy","A. B. Nasser; Y. A. Sariera; A. A. Alsewari; K. Z. Zamli","Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, 26300 Kuantan, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, 26300 Kuantan, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, 26300 Kuantan, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, 26300 Kuantan, Malaysia","2015 IEEE International Conference on Control System, Computing and Engineering (ICCSCE)","","2015","","","150","155","Exhaustive testing is extremely difficult to perform owing to the large number of combinations. Thus, sampling and finding the optimal test suite from a set of feasible test cases becomes a central concern. Addressing this issue, the adoption of t-way testing (where t indicates the interaction strength) has come into the limelight. In order to summarize the achievements so far and facilitate future development, the main focus of this paper is, first, to present a critical comparison of adoption optimization algorithms (OA) as a basis of the t-way test suite generation strategy and, second, to propose a new t-way strategy based on Flower Pollination Algorithm, called Flower Strategy (FS). Analytical and experimental results demonstrate the applicability of FS for t-way test suite generation.","","978-1-4799-8252-3978-1-4799-8251","10.1109/ICCSCE.2015.7482175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482175","Analysis of t-way Strategies;Flower Pollination Algorithm;Combinatorial Testing Problem","Testing;Genetic algorithms;Biological cells;Optimization;Sociology;Statistics;Algorithm design and analysis","optimisation;program testing","optimization based strategies;t-way test suite generation;flower-based strategy;adoption optimization algorithms;flower pollination algorithm;FS","","2","47","","","","","","IEEE","IEEE Conferences"
"Methodology of automation process of wafer tests","A. Krzyzanowska; P. Maj; P. Grybos; R. Szczygiel; A. Koziol","AGH University of Science and Technology, Department of Measurement and Electronics, Cracow, Poland; AGH University of Science and Technology, Department of Measurement and Electronics, Cracow, Poland; AGH University of Science and Technology, Department of Measurement and Electronics, Cracow, Poland; AGH University of Science and Technology, Department of Measurement and Electronics, Cracow, Poland; AGH University of Science and Technology, Department of Measurement and Electronics, Cracow, Poland","2015 22nd International Conference Mixed Design of Integrated Circuits & Systems (MIXDES)","","2015","","","530","533","The paper presents a highly efficient system for automated wafer testing implemented on a single software platform. Building fully functional device requires often expensive and time consuming, manufacturing steps. Thus, the ASICs on the wafer ought to be tested in advance to ensure that only the chips with no manufacturing defects will be used in the further process. The presented testing system was used for the tests of the readout chip for hybrid pixel detectors working in a single photon counting mode. The aim of the system is to detect both digital and analog blocks' issues that might cause incorrect chip functioning. The system setup, software implementation, the methodology and the choice of the tests performed are described.","","978-8-3635-7807-7978-8-3635-7806","10.1109/MIXDES.2015.7208579","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208579","automated wafer test;wafer probe test;ASICs tests;IC tests optimization","Testing;Software;Detectors;Power supplies;Photonics;Probes","application specific integrated circuits;readout electronics;semiconductor device testing;semiconductor technology","wafer test automation process;software platform;ASIC;readout chip;hybrid pixel detector;photon counting mode;chip functioning","","","6","","","","","","IEEE","IEEE Conferences"
"A method of testability optimization based on improved particle swarm optimization","W. Hou; G. Yao; J. Yan","School of Reliability and System Engineering, Beihang University, Beijing, China; School of Reliability and System Engineering, Beihang University, Beijing, China; School of Reliability and System Engineering, Beihang University, Beijing, China","2014 Prognostics and System Health Management Conference (PHM-2014 Hunan)","","2014","","","451","455","This paper proposed an optimization algorithm based on information flow testability modeling to optimize the selection of TPs. This method is an improved particle swarm intelligence algorithm based on the traditional fuzzy particle swarm algorithm. And improvements were made according to the features of dependence matrix. Software simulation was made to proof this methods.","2166-563X;2166-5656","978-1-4799-7958-5978-1-4799-7957","10.1109/PHM.2014.6988213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6988213","testability dsign;optimization problem;particle swarm optimization;D-matrix","Particle swarm optimization;Optimization;Algorithm design and analysis;Educational institutions;Software algorithms;Software;Analytical models","design for testability;failure analysis;matrix algebra;particle swarm optimisation","testability optimization method;particle swarm optimization;optimization algorithm;TP selection;test point;information flow testability modelling;dependence matrix;software simulation;design for testability;failure analysis","","","9","","","","","","IEEE","IEEE Conferences"
"An Architecture-Based Multi-Objective Optimization Approach to Testing Resource Allocation","B. Yang; Y. Hu; C. Huang","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","IEEE Transactions on Reliability","","2015","64","1","497","515","Software systems are widely employed in society. With a limited amount of testing resource available, testing resource allocation among components of a software system becomes an important issue. Most existing research on the testing resource allocation problem takes a single-objective optimization approach, which may not adequately address all the concerns in the decision-making process. In this paper, an architecture-based multi-objective optimization approach to testing resource allocation is proposed. An architecture-based model is used for system reliability assessment, which has the advantage of explicitly considering system architecture over the reliability block diagram (RBD)-based models, and has good flexibility to different architectural alternatives and component changes. A system cost modeling approach which is based on well-developed software cost models is proposed, which would be a more flexible, suitable approach to the cost modeling of software than the approach adopted by others which is based on an empirical cost model. A multi-objective optimization model is developed for the testing resource allocation problem, in which the three major concerns in the testing resource allocation problem, i.e., system reliability, system cost, and the total amount of testing resource consumed, are taken into consideration. A multi-objective evolutionary algorithm (MOEA), called multi-objective differential evolution based on weighted normalized sum (WNS-MODE), is developed. Experimental studies are presented, and the experiments show several results. 1) The proposed architecture-based multi-objective optimization approach can identify the testing resource allocation strategy which has a good trade-off among optimization objectives. 2) The developed WNS-MODE is better than the MOEA developed in recent research, called HaD-MOEA, in terms of both solution quality and computational efficiency. 3) The WNS-MODE seems quite robust from the sensitivity analysis results.","0018-9529;1558-1721","","10.1109/TR.2014.2372411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975258","Hyper-volume indicator;multi-objective evolutionary algorithm;software architecture;software cost models;system reliability assessment","Testing;Resource management;Software;Software reliability;Optimization;Object oriented modeling","evolutionary computation;program testing;resource allocation;sensitivity analysis","sensitivity analysis;weighted normalized sum;WNS-MODE;multiobjective differential evolution;multiobjective evolutionary algorithm;system cost modeling approach;system reliability assessment;reliability block diagram;RBD-based model;single-objective optimization approach;software system;resource allocation testing;architecture-based multiobjective optimization approach","","5","71","","","","","","IEEE","IEEE Journals & Magazines"
"Distributed Online Test Generation for Model-Based Testing","T. Kanstrén; T. Kekkonen","NA; NA","2013 20th Asia-Pacific Software Engineering Conference (APSEC)","","2013","1","","255","262","In online model-based testing, test execution is interleaved with test generation. Test cases should be generated and executed with minimal delay, while still achieving targeted coverage criteria quickly. Extensive model analysis in such case is not possible as any delays in choosing the next step will immediately impact the response times of test execution. The algorithms thus need to be as fast as possible, where a limiting factor is the available computing power. Experts working on the test models used for the generation often need to be able to quickly edit the models, generate test cases, and use the feedback to further evolve the models. Reserving large-scale computing resources while editing the model is unnecessary, but performing the analysis on them for test generation can improve the execution response time significantly. In this paper, we present an approach and algorithm for distributing the online test generation analysis part concurrently over the network, while enabling the expert to work on the models and execute the test cases locally at the same time.","1530-1362;1530-1362","978-1-4799-2144-7978-1-4799-2143","10.1109/APSEC.2013.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805414","online test generation;optimization;algorithm;model-based testing;distributed testing","Generators;Testing;Analytical models;Computer architecture;Computational modeling;Algorithm design and analysis;Libraries","distributed processing;program testing","distributed online test generation;online model-based testing;online test generation analysis;test execution;software testing","","1","21","","","","","","IEEE","IEEE Conferences"
"A Review of Estimation Techniques to Reduce Testing Efforts in Software Development","K. Bareja; A. Singhal","NA; NA","2015 Fifth International Conference on Advanced Computing & Communication Technologies","","2015","","","541","546","In this growing world, technologies and methodologies are very vast and are growing at faster pace. Industries and customers prefer to work on high quality product in a very short time and with more efficiency. Customers prefer project that are not prone to faults and work effectively. In order to develop any project, several phases are involved in their development, but approximately half of the development time is spent in testing of the developed project. So, our main concern in this paper is estimation of testing efforts and how these efforts can be reduced so that good project is available in the market in lesser time as lot of development time is spent in testing. Estimation is the approximation of efforts spent in testing, if approximate value is much higher than it results in over estimation and if it is much lower than it results in under estimation. There are various different ways through which we can estimate the testing efforts. This paper presents the study on various estimation techniques to reduce testing efforts.","2327-0659;2327-0632","978-1-4799-8488-6978-1-4799-8487","10.1109/ACCT.2015.110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7079142","Testing;Development time;Testing efforts;Estimation Methods","Testing;Estimation;Data mining;Particle swarm optimization;Software systems;Syntactics","program testing;software development management;software quality","estimation techniques;software testing efforts;software development;high quality product;development time","","","13","","","","","","IEEE","IEEE Conferences"
"Using Fuzzy Logic and Symbolic Execution to Prioritize UML-RT Test Cases","E. J. Rapos; J. Dingel","NA; NA","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","","2015","","","1","10","The relative ease of test case generation associated with model-based testing can lead to an increased number of test cases being identified for any given system; this is problematic as it is becoming near impossible to run (or even generate) all of the possible tests in available time frames. Test case prioritization is a method of ranking the tests in order of importance, or priority based on criteria specific to a domain or implementation, and selecting some subset of tests to generate and run. Some approaches require the generation of all tests, and simply prioritize the ones to be run, however we propose an approach that would prevent unnecessary generation of tests through the use of symbolic execution trees to determine which tests provide the most benefit to coverage of execution. Our approach makes use of fuzzy logic, specifically fuzzy control systems, to prioritize test cases generated from these execution; the prioritization is based on natural language rules about testing priority. Within this paper we present our motivation, some background research, our methodology and implementation, results, and conclusions.","2159-4848","978-1-4799-7125","10.1109/ICST.2015.7102610","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102610","","Fuzzy logic;Testing;Fuzzy sets;Computational modeling;Unified modeling language;Fuzzy control;Natural languages","fuzzy logic;program testing;real-time systems;trees (mathematics);Unified Modeling Language","fuzzy logic;symbolic execution trees;UML-RT test case generation;model-based testing;test case prioritization;fuzzy control system;natural language rules","","1","18","","","","","","IEEE","IEEE Conferences"
"Effective Regression Testing Using Requirements and Risks","C. Hettiarachchi; H. Do; B. Choi","NA; NA; NA","2014 Eighth International Conference on Software Security and Reliability (SERE)","","2014","","","157","166","The use of system requirements and their risks enables software testers to identify more important test cases that can reveal faults associated with risky components. Having identified those test cases, software testers can manage the testing schedule more effectively by running such test cases earlier so that they can fix faults sooner. Some work in this area has been done, but the previous approaches and studies have some limitations, such as an improper use of requirements risks in prioritization and an inadequate evaluation method. To address the limitations, we implemented a new requirements risk-based prioritization technique and evaluated it considering whether the proposed approach can detect faults earlier overall. It can also detect faults associated with risky components earlier. Our results indicate that the proposed approach is effective for detecting faults early and even better for finding faults associated with risky components of the system earlier than the existing techniques.","","978-1-4799-4296","10.1109/SERE.2014.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6895426","regression testing;requirements risks-based testing;test case prioritization;empirical study","Software;Testing;Mathematical model;Complexity theory;Equations;Security;Measurement","program testing;regression analysis;risk management;scheduling","regression testing;system requirements;testing schedule management;requirements risk-based prioritization technique;fault detection;risky components","","4","23","","","","","","IEEE","IEEE Conferences"
"Redefining reliability evaluations for software-intensive systems","M. K. Jais","United States Army Test and Evaluation Command, Army Evaluation Command, 4120 Susquehanna Avenue, Aberdeen Proving Ground, Maryland 21005 USA","2015 Annual Reliability and Maintainability Symposium (RAMS)","","2015","","","1","4","The Department of Defense has increased the use of Major Automated Information Systems (MAIS); in particular Enterprise Resource Planning (ERP) systems seek to simplify business operations, optimize processes, and maximize efficient financial and logistics management capabilities. These automated information systems integrate all departments and functions across an organization into a single, integrated, centralized database software suite which serves all the internal departments' needs. Due to their unique characteristics and non-traditional programmatic structures, these system's often benefit from the evaluation of uniquely defined metrics than what is typically used for hardware centric systems. Often MAIS reliability requirements are at such a high level (essentially accounting only for the combined experience of all users simultaneously) that they fail to fully capture the experience of the individual end user. While system level Reliability, Availability, and Maintainability (RAM) requirements are necessary, there is a need to focus the evaluation on supplementary metrics that address end user level failures. This paper will discuss innovative approaches to defining reliability evaluations for MAIS programs. This includes understanding how the reliability requirements are defined and what they intend to measure, appropriate use of metrics such as server reliability versus end user experience, and the utilization of additional metrics to provide a meaningful software reliability evaluation to decision makers.","0149-144X","978-1-4799-6703-2978-1-4799-6702","10.1109/RAMS.2015.7105170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7105170","Information Army;System;Reliability;Software Evaluation;Test and Evaluation","Measurement;Servers;Random access memory;Software reliability;Software;Information systems","database management systems;enterprise resource planning;software maintenance;software reliability","software-intensive system;Department of Defense;major automated information systems;enterprise resource planning system;ERP system;business operation;financial management capability;logistics management capability;centralized database software suite;internal department need;nontraditional programmatic structure;hardware centric system;reliability requirement;system level reliability availability and maintainability requirements;RAM requirements;end user level failure;MAIS program;server reliability;software reliability evaluation;decision maker","","","6","","","","","","IEEE","IEEE Conferences"
"Measuring testing efficiency: An alternative approach","R. Majumdar; P. K. Kapur; S. K. Khatri","Amity School of Engineering &amp; Technology, Amity University Uttar Pradesh, Noida, India; Center for Interdisciplinary Research, Amity University Uttar Pradesh, Noida, India; Amity Institute of Information Technology, Amity University Uttar Pradesh, Noida, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","One of the basics of any IT business organization is its stability in the form of providing consistent product in vibrant and modest environment. Growth of quality software hence has become an expected constraint for software production teams. Enhancing Testing Efficiency being one of the prompting factors for the production of this high quality software. Thus, it is obligatory for software firms to orient themselves towards effective fault controlling mechanism by incorporating testing attribute for attaining improved efficiency, and thus reliability. This paper provides an empirical analysis of several testing attributes through data collection and applications of collected data using Analytical Hierarchy Process (AHP), which is used as a tool to classify the relative importance of these attributes for getting better testing efficiency. The purpose of this analysis is to examine the efficiency of testing during software development process. Then, the study calls for a strong need to improve testing methods using techniques such as AHP for proper testing solutions in lieu of old conventional approach of static testing. This paper demonstrates the perspective of Team Leader, Developer and Tester to assess the functioning of each of the attributes and can serve as an important indicator for measuring testing efficiency. Efficient -testing practices increase the ability of detection and elimination of software flaws right at the inception phase and thereby reduce the cost and time of rework. It further improves productivity, quality and sustainability of software industry.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359219","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359219","Testing Efficiency;Analytical Hierarchical Process;Success Factor","Testing;Phase measurement;Industries;Yttrium;Weight measurement;Software","analytic hierarchy process;DP industry;program testing;software fault tolerance","measuring testing efficiency;IT business organization;software production;software testing;software firm;fault controlling mechanism;data collection;analytical hierarchy process;AHP;software development process;software flaw right;software industry","","1","9","","","","","","IEEE","IEEE Conferences"
"A Fuzzy Expert System for Cost-Effective Regression Testing Strategies","A. Schwartz; H. Do","NA; NA","2013 IEEE International Conference on Software Maintenance","","2013","","","1","10","Different testing environments and software change characteristics can affect the choice of regression testing techniques. In our prior work, we developed adaptive regression testing (ART) strategies to investigate this problem. While the ART strategies showed promising results, we also found that the multiple criteria decision making processes required for the ART strategies are time-consuming, often inaccurate and inconsistent, and limited in their scalability. To address these issues, in this research, we develop and empirically study a fuzzy expert system (FESART) to aid decision makers in choosing the most cost-effective technique for a particular software version. The results of our study show that FESART is consistently more cost-effective than the previously proposed ART strategies. One of the biggest contributors to FESART being more cost-effective is the reduced time required to apply the strategy. This contribution has significant impact because a strategy that is less time-consuming will be easier for researchers and practitioners to adopt, and will provide even greater cost-savings for regression testing sessions.","1063-6773","978-0-7695-4981","10.1109/ICSM.2013.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676871","Regression testing;test case prioritization;adaptive regression testing strategy;AHP;fuzzy AHP;empirical studies","Expert systems;Testing;Subspace constraints;Fuzzy logic;Software;Decision making;Fuzzy sets","expert systems;fuzzy set theory;program testing","fuzzy expert system;cost-effective regression testing strategies;software change characteristics;testing environments;adaptive regression testing strategies;ART strategies;multiple criteria decision making processes;FESART;software version","","1","40","","","","","","IEEE","IEEE Conferences"
"Test-Algebra Execution in a Cloud Environment","W. Wu; W. Tsai; C. Jin; G. Qi; J. Luo","NA; NA; NA; NA; NA","2014 IEEE 8th International Symposium on Service Oriented System Engineering","","2014","","","59","69","A algebraic system, Test Algebra (TA), identifies faults in combinatorial testing for SaaS (Software-as-a-Service) applications. SaaS is a software delivery model that involves composition, deployment, and execution of mission application on cloud platforms. Testing SaaS applications is challenging because a large number of configurations needs to be tested. Faulty configurations should be identified and corrected before the delivery of SaaS applications. TA proposes an effective way to reuse existing test results to identify test results of candidate configurations. The TA also defines rules to permit results to be combined, and to identify the faulty interactions. Using the TA, configurations can be tested concurrently on different servers and in any order. This paper proposes one MapReduce design of TA concurrent execution in a cloud environment. The optimization of TA analysis is discussed. The proposed solutions are simulated using Hadoop in a cloud environment.","","978-1-4799-3616","10.1109/SOSE.2014.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825963","Combinatorial testing;MapReduce;SaaS","Testing;Software as a service;Hamming distance;Arrays;Databases;Fault diagnosis;Educational institutions","cloud computing;configuration management;object-oriented programming;optimisation;parallel programming;program testing;public domain software;software fault tolerance","test algebra execution;cloud environment;algebraic system;fault identification;combinatorial testing;SaaS;software-as-a-service;software delivery model;mission application execution;mission application deployment;mission application composition;configuration test results identification;concurrent configuration testing;MapReduce design;TA concurrent execution;TA analysis optimization;Hadoop","","12","19","","","","","","IEEE","IEEE Conferences"
"A Systematic Product Line Test Derivation from Activity Diagrams","S. Kang; J. Lee","NA; NA","2013 IEEE 16th International Conference on Computational Science and Engineering","","2013","","","240","247","The state of the art software product line testing methods attempted test derivation from product lines modeled as activity diagrams (ADs) with the test coverage goals of control flow and data flow. However, the existing methods applied widely different approaches to closely related problems with the consequence that the user of the methods cannot easily see the essence of product line test derivation. Moreover, the existing methods have no solution for P-use test derivation for data flow testing. This paper views this status as the result of not suitably handling variability modeling and binding formation and application and proposes a method that addresses these issues. This is done by introducing an explicit notation for product line AD, which makes clear the distinction between platform AD and product AD, and also by explicitly forming test artifact bindings for products and applying them at suitable steps in the test cases derivation paths.","","978-0-7695-5096","10.1109/CSE.2013.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755224","Software product line testing;Variability modeling;Activity diagram;Control flow testing;Data flow testing","Systematics;Testing;Software;Optimized production technology;Unified modeling language;Data models;Educational institutions","data flow analysis;program testing;software product lines","systematic product line test derivation;activity diagram;software product line testing;P-use test derivation;data flow testing;control flow testing","","","24","","","","","","IEEE","IEEE Conferences"
"Software Vulnerability Detection Based on Code Coverage and Test Cost","B. Shuai; H. Li; L. Zhang; Q. Zhang; C. Tang","NA; NA; NA; NA; NA","2015 11th International Conference on Computational Intelligence and Security (CIS)","","2015","","","317","321","In order to solve the problems of traditional Fuzzing technique for software vulnerability detection, a novel method based on code coverage and test cost is proposed. Firstly, static analysis is applied to calculate the code coverage information, including basic block coverage and new block coverage. In addition, test path diversity information is introduced to elevate path coverage, which is achieved based on the sequence alignment algorithm. Secondly, test cost is analyzed respectively from running time and loop structure. The loop structure is simplified using finite expansion manner. Thirdly, the genetic algorithm fitness function is constructed based on the code coverage and test cost to guide the test case generation. Experiments on realistic binary software show that the method could obtain higher vulnerability detection accuracy and efficiency than the traditional Fuzzing technique.","","978-1-4673-8660-9978-1-4673-8659","10.1109/CIS.2015.84","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7397098","code coverage;test cost;test path diversity;genetic algorithm","Software;Genetic algorithms;Security;Algorithm design and analysis;Software algorithms;Optimization;Search problems","genetic algorithms;program diagnostics;program testing;software reliability","software vulnerability detection;code coverage;test cost;Fuzzing technique;static analysis;basic block coverage;new block coverage;test path diversity information;sequence alignment algorithm;finite expansion;genetic algorithm fitness function;binary software","","1","30","","","","","","IEEE","IEEE Conferences"
"Adaptable test program set development ensures an adept test program","L. V. Kirkland","WesTest Engineering Corp., 810 Shepard Lane, Farmington, Utah 84025, USA","2014 IEEE AUTOTEST","","2014","","","372","377","Perhaps, at no other time in recent history, has adaptability or alter nativity or options been more important than it is now. With the varied technologies on each Unit Under Test (UUT) and the advent of new technology we must be very adaptable with Test Program Set (TPS) Development. Also, Automatic Test Equipment (ATE) capability is increasing exponentially, technology never stands still. Most certainly our TPS Development environment supports the need for the development of adaptability or alter nativity. Even with Automatic Test Equipment (ATE) “There was also evidence that IFTE, as an adaptable maintenance system that can support multiple platforms, can improve system readiness and availability, resulting in significant cost savings”1. We should promote process adaptability throughout the life-cycle of a system so technology can be used as needed. An Adept TPS is noticeable by the Test Program's characteristics. Characteristics of an Adept TPS are code refinement, test structure, instrument usage, ITA routing, test stability, test time, precision measurements, test requirements inclusion, % detect (as an automated measurement), test type, ITA design, and overall quality. Essentially, the bottom line is the proper use of instruments and technology (both hardware and software) as optimized by best practices are factors associated with an Adept TPS. Anytime shortcuts are taken or inconsistencies are exposed during TPS review or sell off it becomes apparent the TPS does not have the characteristics of an Adept TPS. In the past (not too long ago), it was incredibly frustrating to fix a test void using a legacy ATE with an active component ITA. It is beneficial to develop a set of guidelines for applying adaptive or alternative TPS development. How does a TPS developer become an adaptable/alternative TPS Developer? Actually, this is accomplished by the issuing of a best practices document, experience and sharing skill and know-how. Knowhow and sharing skill is often acquired by attending test conferences. Re-Host problems can arise when a test is commented out because the legacy TPS developer might lack the know how to integrate the test. Also, ATE problems play a critical part in TPS integration, noisy ATE or low performance instruments or even calibration play critical roles. The integration of certain tests in legacy ATE requires active components in the ITA due to inadequate ATE resources. So, do you forget about the test as outlined in the Test Requirements Document or do you integrate the test and make it work. When you review the legacy TPS and discover a test commented out, you wonder how this happened. Sometimes a test is commented out by the legacy TPS developer and sometimes they are commented out by the legacy TPS software supporter. This is one of the reasons why a re-host is often acknowledged as new development. No matter who commented out the test code, it is the responsibility (as per the Program Manager's direction) of the Re-host engineer to integrate the test. This paper will discuss adaptability or alternatively and other critical factors associated with TPS development and re-host. Also, discussed is the use of advanced technologies to integrate previously commented out tests.","1088-7725;1558-4550","978-1-4799-3005-0978-1-4799-3389","10.1109/AUTEST.2014.6935174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935174","","Timing;Instruments;Software;Test equipment;Hardware;Circuit faults","automatic test equipment;calibration;integrated software;software maintenance","adaptive TPS development;legacy TPS developer;TPS integration;calibration;legacy ATE;ATE resources;legacy TPS software supporter;test code;alternative TPS development;active component ITA;test type;test requirements;precision measurements;test time;test stability;ITA routing;instrument usage;test structure;code refinement;Adept TPS;process adaptability;adaptable maintenance system;IFTE;alter nativity;development of adaptability;ATE;automatic test equipment;test program set;UUT;unit under test;adept test program;adaptable test program set development","","1","4","","","","","","IEEE","IEEE Conferences"
"Comparative study of test suite filtering techniques","R. L. Bai; C. P. Indumathi; K. Selvamani","Dept. of Computer Science, Anna University Bit Campus, Tiruchirappalli, India; Dept. of Computer Science, Anna University Bit Campus, Tiruchirappalli, India; Dept. of Computer Science, Anna University CEG Campus, Chennai, India","2015 IEEE International Conference on Computational Intelligence and Computing Research (ICCIC)","","2015","","","1","6","Testing is necessary for software to know the quality and performance of the system. Testing phase initially starts from the generation of test cases. There are many ways to generate test cases but the critical task is to identify valid test cases at maintenance phase. Existing approach uses TestFilter method to reduce the size of test cases by satisfying the constraint. In this paper, TestFilter technique is compared with bee colony algorithm (BCO). Experimental study is conducted for both the technique. The comparative study shows that both the techniques reduce the test case effectively as well as the time and cost.","","978-1-4799-7849-6978-1-4799-7848-9978-1-4799-7847","10.1109/ICCIC.2015.7435787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7435787","Software Testing;Reduction Techniques;Bee Colony Optimization (BCO)","","program testing;software maintenance","test suite filtering techniques;software testing;maintenance phase;bee colony algorithm;BCO","","","","","","","","","IEEE","IEEE Conferences"
"GA based distribution network expansion. Part 2. Case study: IEEE-30 test system","S. Kilyeni; C. Barbulescu; C. Oros; A. Deacu","Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania; Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania; Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania; Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania","2014 International Conference on Applied and Theoretical Electricity (ICATE)","","2014","","","1","6","The renewable sources' influence regarding the distribution network expansion planning is tackled. The network expansion is discussed in two cases: with and without renewable sources. The obtained expansion solutions are analyzed and a final one is proposed by the authors. To achieve this goal network reconfiguration and N-1 contingecies are performed. The goal is to supply all the consumers for each operating condition. Also, the technical losses have to be minimized. The power flow is computed using conventional methods, but the optimal power flow and distribution network expansion are performed using genetic algorithms (GA). Thus, the authors have developed an own software tool in Matlab environment.","","978-1-4799-4161","10.1109/ICATE.2014.6972610","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972610","distribution network expamsio;renewable sources;reconfiguration;tehnical losses;software tool;optimization","Genetic algorithms;Planning;Software tools;Optimization;Load flow;Linear programming","genetic algorithms;IEEE standards;load flow;power distribution planning","Matlab environment;optimal power flow;N-1 contingecies;renewable sources;distribution network expansion planning;IEEE-30 test system;genetic algorithms;GA","","","12","","","","","","IEEE","IEEE Conferences"
"A generalized framework for multi release of a software under Distributed Environment","P. K. Kapur; O. Singh; A. K. Shrivastava","Center for Interdisciplinary Research, Amity University, Noida, U.P., India; Department of Operational Research, University of Delhi, India; NA","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","Current Market conditions do not allow the developer to spend too much time to build a software. Due to the fast changing technology, the fear that software might get obsolete also frights the developers. These concerns and the prevailing cut throat contention gave rise to distributed development strategies. Distributed development environment is a general phenomenon in today's software industry. Also, developers prefer to follow a multi release policy by introducing multiple upgraded versions of a software instead of delivering the entire product at one go. But the added functionalities further increase the existing complexity of the software and testing team may not be able to perfectly remove the faults leading to imperfect debugging or add more bugs due to lack of knowledge about the software in the initial phase known as error generation. We incorporated this real life phenomenon of imperfect debugging in multi-up gradations of software developed under distributed environment. Further we have proposed a generalized cost model for Multi-Up gradations of software developed in Distributed Environment under the effect of imperfect debugging. In multiple versions of software each version has its testing cost and reliability. For obtaining the optimal testing time in order to get the best reliable software with limited resources we have applied Multi Attribute Utility Theory by taking the convex combination of testing cost and reliability in multiple versions of software. To validate the analytical results of the proposed framework, numerical illustration is provided.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359365","Software Reliability;Multi Up-gradation;Distributed Environment;Cost Modelling;Multi Attribute Utility Theory (MAUT);Imperfect Debugging","Software;Testing;Debugging;Software reliability;Reliability theory;Computer bugs","DP industry;program debugging;program testing;software reliability;utility theory","distributed development strategy;distributed development environment;software industry;software testing;software debugging;error generation;multiupgradation software;software reliability;multiattribute utility theory","","","23","","","","","","IEEE","IEEE Conferences"
"Cumulative code churn: Impact on maintainability","C. Faragó; P. Hegedűs; R. Ferenc","Department of Software Engineering, University of Szeged, Hungary; Department of Software Engineering, University of Szeged, Hungary; Department of Software Engineering, University of Szeged, Hungary","2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation (SCAM)","","2015","","","141","150","It is a well-known phenomena that the source code of software systems erodes during development, which results in higher maintenance costs in the long term. But can we somehow narrow down where exactly this erosion happens? Is it possible to infer the future erosion based on past code changes? Do modifications performed on frequently changing code have worse effect on software maintainability than those affecting less frequently modified code? In this study we investigated these questions and the results indicate that code churn indeed increases the pace of code erosion. We calculated cumulative code churn values and maintainability changes for every version control commit operation of three open-source and one proprietary software system. With the help of Wilcoxon rank test we compared the cumulative code churn values of the files in commits resulting maintainability increase with those of decreasing the maintainability. In the case of three systems the test showed very strong significance and in one case it resulted in strong significance (p-values 0.00235, 0.00436, 0.00018 and 0.03616). These results support our preliminary assumption that modifying high-churn code is more likely to decrease the overall maintainability of a software system, which can be thought of as the generalization of the already known phenomena that code churn results in higher number of defects.","","978-1-4673-7529-0978-1-4673-7528","10.1109/SCAM.2015.7335410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335410","code churn;ISO/IEC 25010;source code maintainability;Wilcoxon test","Software systems;Density measurement;Maintenance engineering;Control systems;Open source software;IEC Standards","software engineering;software maintenance;source code (software);statistical testing","software development;software maintainability;code erosion;cumulative code churn value;open-source code;proprietary software system;Wilcoxon rank test","","1","26","","","","","","IEEE","IEEE Conferences"
"Evaluating a Multi-objective Hyper-Heuristic for the Integration and Test Order Problem","G. Guizzo; S. R. Vergilio; A. T. R. Pozo","NA; NA; NA","2015 Brazilian Conference on Intelligent Systems (BRACIS)","","2015","","","1","6","Multi-objective evolutionary algorithms (MOEAs) have been successfully applied for solving different software engineering problems. However, adapting and configuring these algorithms for a specific problem can demand significant effort from software engineers. Therefore, to help in this task, a hyper-heuristic, named HITO (Hyper-heuristic for the Integration and Test Order problem) was proposed to adaptively select search operators during the optimization process. HITO was successfully applied using NSGA-II for solving the integration and test order problem. HITO can use two hyper-heuristic selection methods: Choice Function and Multi-armed Bandit. However, a hypotheses behind this study is that HITO does not depend of NSGA-II and can be used with other MOEAs. To this aim, this paper presents results from evaluation experiments comparing the performance of HITO using two different MOEAs: NSGA-II and SPEA2. The results show that HITO is able to outperform both MOEAs.","","978-1-5090-0016","10.1109/BRACIS.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423906","hyper-heuristic;search-based software engineering;software testing;integration and test order;multi-objective optimization;genetic algorithms","Software algorithms;Evolutionary computation;Software engineering;Sociology;Statistics;Software;Optimization","genetic algorithms;program testing","multiobjective hyperheuristic;integration-and-test order problem;multiobjective evolutionary algorithm;MOEA;software engineering;optimization process;HITO;NSGA-II;hyperheuristic selection method;choice function;multiarmed bandit","","4","22","","","","","","IEEE","IEEE Conferences"
"Practical Software Quality Prediction","E. Shihab","NA","2014 IEEE International Conference on Software Maintenance and Evolution","","2014","","","639","644","Software systems continue to play an increasingly important role in our daily lives, making the quality of software systems an extremely important issue. Therefore, a significant amount of recent research focused on the prioritization of software quality assurance efforts. One line of work that has been receiving an increasing amount of attention is Software Defect Prediction (SDP), where predictions are made to determine where future defects might appear. Our survey showed that in the past decade, more than 100 papers were published on SDP. Nevertheless, the practical adoption of SDP to date is limited. In this paper, we highlight the findings of our thesis, which identifies the challenges that hinder the adoption of SDP in practice. These challenges include the fact that the majority of SDP research rarely considers the impact of defects when performing their predictions, seldom provides guidance on how to use the SDP results, and is too reactive and defect-centric in nature. Therefore, we propose approaches that tackle these challenges. First, we present approaches that predict high-impact defects. Our approaches illustrate how SDP research can be tailored to consider the impact of defects when making their predictions. Second, we present approaches that simplify SDP models so they can be easily understood and illustrates how these simple models can be used to assist practitioners in prioritizing the creation of unit tests in large software systems. These approaches illustrate how SDP research can provide guidance to practitioners using SDP. Then, we argue that organizations are interested in proactive risk management, which covers more than just defects. For example, risky changes may not introduce defects but they could delay the release of projects. Therefore, we present an approach that predicts risky changes, illustrating how SDP can be more encompassing (i.e., by predicting risk, not only defects) and proactive (i.e., by predicting changes before they are incorporated into the code base).","1063-6773","978-1-4799-6146","10.1109/ICSME.2014.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976158","Software Defect Predcition;Risky Software Changes;Software Quality","Predictive models;Industries;Software engineering;Communities;Education;Software systems","program testing;quality assurance;risk management;software quality","software quality prediction;software systems;software quality assurance;software defect prediction;SDP;high-impact defect prediction;proactive risk management;unit tests","","5","36","","","","","","IEEE","IEEE Conferences"
"A Subsumption Hierarchy of Test Case Prioritization for Composite Services","L. Mei; Y. Cai; C. Jia; B. Jiang; W. K. Chan; Z. Zhang; T. H. Tse","Department of Solutions Engineering and Operation Excellence, IBM Research—China, Beijing, China; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong; School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Pokfulam, Hong Kong","IEEE Transactions on Services Computing","","2015","8","5","658","673","Many composite workflow services utilize non-imperative XML technologies such as WSDL, XPath, XML schema, and XML messages. Regression testing should assure the services against regression faults that appear in both the workflows and these artifacts. In this paper, we propose a refinement-oriented level-exploration strategy and a multilevel coverage model that captures progressively the coverage of different types of artifacts by the test cases. We show that by using them, the test case prioritization techniques initialized on top of existing greedy-based test case prioritization strategy form a subsumption hierarchy such that a technique can produce more test suite permutations than a technique that subsumes it. Our experimental study of a model instance shows that a technique generally achieves a higher fault detection rate than a subsumed technique, which validates that the proposed hierarchy and model have the potential to improve the cost-effectiveness of test case prioritization techniques.","1939-1374;2372-0204","","10.1109/TSC.2014.2331683","National Key Basic Research Program of China; General Research Fund of the Research Grants Council of Hong Kong; National Natural Science Foundation of China; National Science and Technology Major Project of China; CCF-Tencent Open Research Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839018","Test case prioritization;service orientation;XPath;WSDL;XML messages","XML;Fault detection;Semantics;Testing;Business;Educational institutions","program testing;regression analysis;service-oriented architecture;Web Services Business Process Execution Language;XML","subsumption hierarchy;test case prioritization;composite workflow service;XML technology;regression testing;refinement-oriented level-exploration strategy;multilevel coverage model;Web Services Business Process Execution Language;WS-BPEL","","8","52","","","","","","IEEE","IEEE Journals & Magazines"
"A new approach for test case generation by discrete particle swarm optimization algorithm","A. Andalib; S. M. Babamir","Department of Computer Engineering, University of Kashan, Kashan, Iran; Department of Computer Engineering, University of Kashan, Kashan, Iran","2014 22nd Iranian Conference on Electrical Engineering (ICEE)","","2014","","","1180","1185","The increasing complexities in softwares, have reduced the efficiency of the common methods of software testing and urged the necessity to use new and optimal methods to produce test case (TC) to cover high percentage of target plan to find existing errors. Thus, today, the production of TC is considered to be an important aim in software testing methods. Particle Swarm Optimization (PSO) is an intelligent technique based on the collective movement of the particles inspired by social behavior of the flocks of birds and schools of fish. After full introduction of this algorithm and the reasons behind usage PSO, the present study will propose a method for automatic production of the TC, such that the highest code covering is done by the minimum TC. For better analysis of the results, a plan was investigated as a case study with the proposed method. As the evolutionary structures such as genetic algorithm (GA) with high percent of code covering are being used for a long time, thus to prove the optimization of the proposed method, the results are compared with the GA.","2164-7054","978-1-4799-4409-5978-1-4799-4410","10.1109/IranianCEE.2014.6999714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999714","Test Case;Particle Swarm Optimization;Merging two Arrays;PBest;GBest;PSO","Algorithm design and analysis;Equations;Genetic algorithms;Software algorithms;Particle swarm optimization;Birds;Marine animals","genetic algorithms;particle swarm optimisation;program testing","test case generation;discrete particle swarm optimization algorithm;software complexities;software testing;PSO;intelligent technique;particles collective movement;social behavior;automatic TC production;code covering;evolutionary structures;genetic algorithm;GA","","6","14","","","","","","IEEE","IEEE Conferences"
"Prevalent criteria's in regression test case selection techniques: An exploratory study","P. Dhareula; A. Ganpati","Computer Science Department, Himachal Pradesh University, Shimla (H.P), India; Computer Science Department, Himachal Pradesh University, Shimla (H.P), India","2015 International Conference on Green Computing and Internet of Things (ICGCIoT)","","2015","","","871","876","Regression testing is done after needful changes, ensuring that changes are working as required and does not produce unexpected results for a system under test. Note worthy difficulty in regression testing is selection of significant subgroup of test cases. This paper has analyzed techniques of regression test selection (RTS) for test case optimization in various domains. The study identified most prevalent criteria's used by various researchers. This study analyzed two broad groups of techniques under which test cases are optimized i.e. code-based and requirement-based techniques. Further most prevalent criteria's were identified and techniques were grouped under them. The study is also focused on the level of test granularity used by different researchers. Two main granularity levels were identified for code based testing i.e. fine granularity and coarse granularity. From this study it is also concluded that no such technique could be generalized because they are proposed for different domains of interest.","","978-1-4673-7910-6978-1-4673-7909","10.1109/ICGCIoT.2015.7380585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7380585","Regression testing;maintenance;test case selection;selection techniques","Testing;Software;Unified modeling language;Optimization;Fault diagnosis;Flow graphs;Databases","granular computing;program testing","regression test case selection technique;RTS;test case optimization;code-based testing","","","22","","","","","","IEEE","IEEE Conferences"
"Searching for models to evaluate software technology","F. G. de Oliveira Neto; R. Feldt; R. Torkar; P. D. L. Machado","Software Practices Laboratory, Universidade Federal Campina Grande, Brazil; Dept. of Computer Science and Engineering, Chalmers and Gothenburg University, Sweden; Dept. of Computer Science and Engineering, Chalmers and Gothenburg University, Sweden; Software Practices Laboratory, Universidade Federal Campina Grande, Brazil","2013 1st International Workshop on Combining Modelling and Search-Based Software Engineering (CMSBSE)","","2013","","","12","15","Modeling and abstraction is key in all engineering processes and have found extensive use also in software engineering. When developing new methodologies and techniques to support software engineers we want to evaluate them on realistic models. However, this is a challenge since (1) it is hard to get industry to give access to their models, and (2) we need a large number of models to systematically evaluate a technology. This paper proposes that search-based techniques can be used to search for models with desirable properties, which can then be used to systematically evaluate model-based technologies. By targeting properties seen in industrial models we can then get the best of both worlds: models that are similar to models used in industry but in quantities that allow extensive experimentation. To exemplify our ideas we consider a specific case in which a model generator is used to create models to test a regression test optimization technique.","","978-1-4673-6284","10.1109/CMSBSE.2013.6604430","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604430","Automatic Model Generation;Search-Based Techniques;Model-based Software Engineering Technology","Unified modeling language;Biological system modeling;Software;Generators;Computational modeling;Testing;Analytical models","optimisation;regression analysis;search problems;software engineering","software technology;software engineering;search-based techniques;model-based technologies;regression test optimization technique","","5","14","","","","","","IEEE","IEEE Conferences"
"Optimization of Combinatorial Testing by Incremental SAT Solving","A. Yamada; T. Kitamura; C. Artho; E. Choi; Y. Oiwa; A. Biere","NA; NA; NA; NA; NA; NA","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","","2015","","","1","10","Combinatorial testing aims at reducing the cost of software and system testing by reducing the number of test cases to be executed. We propose an approach for combinatorial testing that generates a set of test cases that is as small as possible, using incremental SAT solving. We present several search-space pruning techniques that further improve our approach. Experiments show a significant improvement of our approach over other SAT-based approaches, and considerable reduction of the number of test cases over other combinatorial testing tools.","2159-4848","978-1-4799-7125","10.1109/ICST.2015.7102599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102599","","Encoding;Testing;Optimization;Browsers;Linux;Object oriented modeling;Standards","combinatorial mathematics;computability;optimisation;program testing;search problems","combinatorial testing optimization;incremental SAT solving;software cost reduction;system testing;search-space pruning techniques","","10","32","","","","","","IEEE","IEEE Conferences"
"The mathematics of software testing using genetic algorithm","M. Boopathi; R. Sujatha; C. S. Kumar; S. Narasimman","Department of Mathematics, SSN College of Engineering, Kalavakkam, Chennai, TamilNadu, India-603110; Department of Mathematics, SSN College of Engineering, Kalavakkam, Chennai, TamilNadu, India-603110; Safety Research Institute, Atomic Energy Regulatory Board, Kalpakkam, TamilNadu, India-603102; Department of Mathematics, SSN College of Engineering, Kalavakkam, Chennai, TamilNadu, India-603110","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","6","A Markov Chain approach to estimate reliability of a software system using genetic algorithm is presented. In this approach, the code is initially converted into control flow graph and then reduced to a dd-graph. The fitness function of the genetic algorithm is calculated based on the path coverage. The edges of the dd-graph are assigned weights based on the Markov transition probability matrix and the value of the fitness function is calculated as the sum of weights of all edges of the dd-graph covered by the test suite. In this paper, arithmetic crossover and insertion mutation is applied for the floating point populations and genetic algorithm is used to generate test cases to achieve path coverage with less computation cost and time. The proposed approach results in identifying the most critical path of the system.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014677","Genetic Algorithm;Dd-graph;Markov Chain;Arithmetic Crossover;Mutation;Software Testing","Genetic algorithms;Biological cells;Testing;Sociology;Statistics;Markov processes;Next generation networking","flow graphs;genetic algorithms;Markov processes;probability;program testing;software reliability","software testing;genetic algorithm;Markov chain approach;software system reliability estimation;control flow graph;dd-graph;fitness function;Markov transition probability matrix;arithmetic crossover;insertion mutation;test case generation","","3","13","","","","","","IEEE","IEEE Conferences"
"Rank: A Tool to Check Program Termination and Computational Complexity","C. Alias; A. Darte; P. Feautrier; L. Gonnord","NA; NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops","","2013","","","238","238","Summary form only given. Proving the termination of a flowchart program can be done by exhibiting a ranking function, i.e., a function from the program states to a well-founded set that strictly decreases at each program step. In a previous paper , we proposed an algorithm to compute multidimensional affine ranking functions for flowcharts of arbitrary structure. Our method, although greedy, is provably complete for the class of rankings we consider. The ranking functions we generate can also be used to get upper bounds for the computational complexity (number of transitions) of the source program. This estimate is a polynomial, which means that we can handle programs with more than linear complexity. This abstract aims at presenting RANK, the tool that implements our algorithm.","","978-0-7695-4993-4978-1-4799-1324","10.1109/ICSTW.2013.75","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571638","","Computational complexity;Conferences;Abstracts;Electronic mail;Automata;Software testing","computational complexity;polynomials;program verification","Rank;flowchart program termination checking;computational complexity;multidimensional affine ranking functions;source program;polynomial;linear complexity","","1","","","","","","","IEEE","IEEE Conferences"
"Concurrent program semantic mutation testing based on abstract memory model","L. Cao; W. Zheng; D. Hu; H. Bai","College of Software &amp; Micro-electronics, Northwestern Polytechnical University, Xi'an, Shaanxi Province, CHINA; College of Software &amp; Micro-electronics, Northwestern Polytechnical University, Xi'an, Shaanxi Province, CHINA; College of Software &amp; Micro-electronics, Northwestern Polytechnical University, Xi'an, Shaanxi Province, CHINA; College of Software &amp; Micro-electronics, Northwestern Polytechnical University, Xi'an, Shaanxi Province, CHINA","2015 IEEE International Conference on Information and Automation","","2015","","","1200","1205","Due to the fact that concurrent program's semantics can't be understood by relaxed memory model correctly, some unexpected faults, which are difficult to be detected, exist during its multi-threaded cross-execution. Therefore, this paper intends to establish an abstract memory model for concurrent program, conduct formal description of the memory access process that may exist in program memory access, and then utilize model checking technique to find out the common concurrent program error scenarios on this basis. We further design the semantic mutation operator, and investigate how to optimize the mutation process and how to generate mutants and test suite automatically. This study can set up a formal specification of concurrent program memory access semantics and provide a theoretical basis for other researches in the field of testing. It also serves as a basis to propose a semantic mutation testing technique oriented towards parallel program to assist parallel code testing in industry.","","978-1-4673-9104-7978-1-4673-9103","10.1109/ICInfA.2015.7279469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279469","Software testing;memory model;semantic mutation testing;symbol execution;evolution algorithm","Semantics;Testing;Concurrent computing;Programming;Instruction sets;Optimization;Syntactics","concurrency control;formal specification;multi-threading;program testing;program verification;storage management","concurrent program semantic mutation testing;abstract memory model;multithreaded cross-execution;formal description;memory access process;model checking technique;concurrent program error scenarios;semantic mutation operator;mutation process optimization;formal specification;concurrent program memory access semantics;parallel program;parallel code testing","","1","17","","","","","","IEEE","IEEE Conferences"
"A survey of software reliability growth model selection methods for improving reliability prediction accuracy","M. I. M. Saidi; M. A. Isa; D. N. A. Jawawi; L. F. Ong","Department of Software Engineering, Universiti Teknologi Malaysia, 81310, Skudai, Johor, Malaysia; Department of Software Engineering, Universiti Teknologi Malaysia, 81310, Skudai, Johor, Malaysia; Department of Software Engineering, Universiti Teknologi Malaysia, 81310, Skudai, Johor, Malaysia; Department of Software Engineering, Universiti Teknologi Malaysia, 81310, Skudai, Johor, Malaysia","2015 9th Malaysian Software Engineering Conference (MySEC)","","2015","","","200","205","Reliability is a software quality characteristic that refer to the probability a system will work correctly over a period of time. Reliability prediction is important as it can be used to plan deployment, maintenance and test activities. This study assesses the efficiency of several techniques in software reliability model (SRM) selection and aims to find out the possible enhancement to improve software reliability accuracy. The result of the survey shows most of the SRM selection technique does not optimizes the model parameter.","","978-1-4673-8227-4978-1-4673-8226","10.1109/MySEC.2015.7475221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475221","software reliability;selection;accuracy;reliability prediction","Predictive models;Data models;Software reliability;Software;Market research;Optical fibers","probability;software quality;software reliability","growth model selection methods;reliability prediction accuracy;software quality;probability;software reliability model;SRM","","","27","","","","","","IEEE","IEEE Conferences"
"Optimal Weighted Combinational Models for Software Reliability Estimation and Analysis","C. Hsu; C. Huang","Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan","IEEE Transactions on Reliability","","2014","63","3","731","749","Software is currently a key part of many safety-critical and life-critical application systems. People always need easy- and instinctive-to-use software, but the biggest challenge for software engineers is how to develop software with high reliability in a timely manner. To assure quality, and to assess the reliability of software products, many software reliability growth models (SRGMs) have been proposed in the past three decades. The practical problem is that sometimes these selected SRGMs by companies or software practitioners disagree in their reliability predictions, while no single model can be trusted to provide consistently accurate results across various applications. Consequently, some researchers have proposed to use combinational models for improving the prediction capability of software reliability. In this paper, three enhanced weighted-combinations, namely weighted arithmetic, weighted geometric, and weighted harmonic combinations, are proposed. To solve the problem of determining proper weights for model combinations, we further study how to incorporate enhanced genetic algorithms (EGAs) with several efficient operators into weighted assignments. Experiments are performed based on real software failure data, and numerical results show that our proposed models are flexible enough to depict various software development environments. Finally, some management metrics are presented to both assure software quality and determine the optimal release strategy of software products under development.","0018-9529;1558-1721","","10.1109/TR.2014.2315966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786506","Genetic algorithm;non-homogeneous Poisson process;optimization problems;software development process;software reliability growth models;weighted combinations","Software reliability;Software;Data models;Predictive models;Genetic algorithms;Testing","combinatorial mathematics;genetic algorithms;software fault tolerance;software management;software metrics;software quality;software reliability","optimal weighted combinational models;software reliability growth models;SRGM;safety-critical application systems;life-critical application systems;software products;software quality;prediction capability;weighted arithmetic combinations;weighted geometric combinations;weighted harmonic combinations;enhanced genetic algorithms;EGA;real software failure data;software development environments;management metrics;optimal release strategy","","8","41","","","","","","IEEE","IEEE Journals & Magazines"
"Keynote abstracts","L. C. Briand","NA","2015 IEEE International Conference on Software Quality, Reliability and Security","","2015","","","xvii","xxii","Testing and verification problems in the software industry come in many different forms, due to significant differences across domains and contexts. But one common challenge is scalability, the capacity to test and verify increasingly large, complex systems. Another concern relates to practicality. Can the inputs required by a given technique be realistically provided by engineers? This talk reports on 10 years of research tackling verification and testing as a search and optimization problem, often but not always relying on abstractions and models of the system under test. Our observation is that most of the problems we faced could be re-expressed so as to make use of appropriate search and optimization techniques to automate a specific testing or verification strategy. One significant advantage of such an approach is that it often leads to solutions that scale in large problem spaces and that are less demanding in terms of the level of detail and precision required in models and abstractions. Their drawback, as heuristics, is that they are not amenable to proof and need to be thoroughly evaluated by empirical means. However, in the real world of software development, proof is usually not an option, even for smaller and critical systems. In practice, testing and verification is a means to reduce risk as much as possible given available resources and time. Concrete examples of problems we have addressed and that I will cover in my talk include schedulability analysis, stress/load testing, CPU usage analysis, robustness testing, testing closed-loop dynamic controllers, and SQL Injection testing. Most of these projects have been performed in industrial contexts and solutions were validated on industrial software. There are, however, many other examples in the literature, a growing research trend that has given rise to a new field of study named search-based software testing.","","978-1-4673-7989-2978-1-4673-7988","10.1109/QRS.2015.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272905","","","optimisation;program testing;program verification;search problems","scalable software testing;software verification;heuristic search;software industry;complex systems;optimization problem;optimization techniques;software development;schedulability analysis;stress testing;load testing;CPU usage analysis;robustness testing;closed-loop dynamic controllers testing;SQL injection testing;industrial contexts;industrial software","","","","","","","","","IEEE","IEEE Conferences"
"An efficient reduction approach to test suite","P. Liu","College of Computer Engineering and Science, Shanghai Business School, Shanghai, China","15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","","2014","","","1","5","The paper presents a novel reduction approach to test suite in regression testing. Our approach selects a test case from those redundant test cases with the same rank according to the boundary coverage taken as second coverage criterion. Different from the previous researches for the selection of the second coverage criterion to assist test suite reduction, our approach bases on our empirical point of view that some faults in the program lie on its boundary of the program. Therefore test cases picked by our approach have the higher probability to find software errors. An algorithm is proposed to realize our approach. In addition, we also discuss the relationship between our approach with three traditional heuristics. Through a simple exampke, we illustrate our algorithm for test suite reduction.","","978-1-4799-5604","10.1109/SNPD.2014.6888743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888743","test suite reduction;the boundary coverage;software testing;regression testing","Testing;Software;Heuristic algorithms;Minimization;Software engineering;Fault detection;Redundancy","probability;program testing","novel reduction approach;regression testing;boundary coverage;second coverage criterion;test suite reduction;software errors;probability","","","18","","","","","","IEEE","IEEE Conferences"
"A novel dynamic analysis of test cases to improve testing efficiency in object-oriented systems","Tao Hu; Gangyi Ding","School of Software, Beijing Institute of Technology, China; School of Software, Beijing Institute of Technology, China","2015 4th International Conference on Computer Science and Network Technology (ICCSNT)","","2015","01","","457","461","In this paper, we present a series of methods to improve testing efficiency especially for regression testing from a novel view, namely dynamic analysis of test cases suitable for class testing in object-oriented systems. We mine static call graphs and dynamic call trees to represent the static features and dynamic tests of the program. By graph analysis, we present a series of methods and testing criteria to evaluate test cases from the view of code coverage. These methods improve testing efficiency for class testing from the following aspects: automation; multi-angle evaluations of test cases; improvement and management of test cases; providing different prioritization criteria and optimization criteria for regression testing to meet different testing requirements etc. What's more, they can be used in large-scale OO systems, and the test results are quantifiable.","","978-1-4673-8173-4978-1-4673-8172","10.1109/ICCSNT.2015.7490789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490789","software testing;regression test;object-oriented system","Software;Optimization;Software testing;Object oriented modeling;Automation;Encoding","","","","","13","","","","","","IEEE","IEEE Conferences"
"Customizing k-Gram Based Birthmark through Partial Matching in Detecting Software Thefts","H. Lim","NA","2013 IEEE 37th Annual Computer Software and Applications Conference Workshops","","2013","","","1","4","The k-gram based birthmark is a method for comparing binary programs to find similar software, such as software thefts or common modules. The method directly compares opcode sequences, so it is susceptible to program changes, such as optimization or obfuscation. In this paper, we present a method for customizing the k-gram birthmark to allow slight changes of programs by employing partial matching of k-grams. We find the customized k-gram birthmark in Java application environments, and evaluate the customized birthmark in real-world Java applications. In the experimental results, we show that customization of k-gram birthmark improves the credibility and resilience in comparing binary programs.","","978-1-4799-2159","10.1109/COMPSACW.2013.5","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605754","software birthmark;software theft detection;software copyright protection","Software;Java;Resilience;Benchmark testing;Conferences;Sensitivity","copy protection;copyright;Java;pattern matching;software engineering","k-gram based birthmark;k-gram partial matching;software theft detection;Java application environments;credibility improvement;resilience improvement;software copyright protection","","1","14","","","","","","IEEE","IEEE Conferences"
"A Comparison of Different Defect Measures to Identify Defect-Prone Components","T. D. Oyetoyan; R. Conradi; D. S. Cruzes","NA; NA; NA","2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement","","2013","","","181","190","(Background) Defect distribution in software systems has been shown to follow the Pareto rule of 20-80. This motivates the prioritization of components with the majority of defects for testing activities. (Research goal) Are there significant variations between defective components and architectural hotspots identified by other defect measures? (Approach) We have performed a study using post-release data of an industrial Smart Grid application with a well-maintained defect tracking system. Using the Pareto principle, we identify and compare defect-prone and hotspots components based on four defect metrics. Furthermore, we validated the quantitative results against qualitative data from the developers. (Results) Our results show that at the top 25% of the measures 1) significant variations exist between the defective components identified by the different defect metrics and that some of the components persist as defective across releases 2) the top defective components based on number of defects could only identify about 40% of critical components in this system 3) other defect metrics identify about 30% additional critical components 4) additional quality challenges of a component could be identified by considering the pair wise intersection of the defect metrics. (Discussion and Conclusion) Since a set of critical components in the system is missed by using largest-first or smallest-first prioritization approaches, this study, therefore, makes a case for an all-inclusive metrics during defect model construction such as number of defects, defect density, defect severity and defect correction effort to make us better understand what comprises defect-prone components and architectural hotspots, especially in critical applications.","","978-0-7695-5078","10.1109/IWSM-Mensura.2013.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693238","defect distribution;defect measures;defect metrics;defect severity;defect correction effort;defect density;defect-prone component;Smart Grid;critical system;architectural hotspots","Measurement;Software;Smart grids;Predictive models;Testing;Maintenance engineering;Object oriented modeling","object-oriented programming;Pareto distribution;power engineering computing;program testing;smart power grids;software architecture;software metrics;software quality","defect measure;defect-prone component identification;defect distribution;software system;Pareto rule;component prioritization;testing activities;architectural hotspot;post-release data;industrial smart grid application;well-maintained defect tracking system;Pareto principle;defect metrics;quality challenges;largest-first prioritization approach;smallest-first prioritization approach;defect density;defect severity;defect correction;critical application","","","43","","","","","","IEEE","IEEE Conferences"
"Dynamical sequence generation performance testing method and its application","Y. Sun; W. Zheng","Beijing Jiaotong University, China; Beijing Jiaotong University, China","2013 IEEE International Conference on Intelligent Rail Transportation Proceedings","","2013","","","185","190","In order to improve the performance of the test efficiency and reduce the test cost,this paper is based on the data driven the thoughts and combined with the current mainstream test automation technology,putting forward a dynamic generation based on sequence control test thoughts of dynamic test automation method.This method considers the design of the testing progress optimization from both of the overall and partial aspects,making the test platform and the measured objects'statement information as a test-controling information and control the execution of performance sequence dynamicly, and generate every generation of peak performance sequence set. That has been applied to wireless block center (RBC) performance test, and has achieved good results of implementation.","","978-1-4673-5277-2978-1-4673-5278-9978-1-4673-5276","10.1109/ICIRT.2013.6696291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696291","Performance Test;Automated Test;Sequence Dynamic Generation;Paralleling Genetic Algorithms;Many Cars Concurrency RBC Switching","Testing;Optimization;Concurrent computing;Genetic algorithms;Heuristic algorithms;Observers;Sociology","automatic test pattern generation;automatic test software;genetic algorithms;rail traffic control","dynamical sequence generation performance testing method;performance improvement;test cost reduction;test efficiency;test automation technology;sequence control test;dynamic test automation method;testing progress optimization design;object statement information;test-controling information;performance sequence execution control;peak performance sequence set;wireless block center performance test;CTCS-3-level train operation control system ground","","","25","","","","","","IEEE","IEEE Conferences"
"Test Data Generation Algorithm of Combinatorial Testing Based on Differential Evolution","W. Jianfeng; W. Changan; J. Shouda","NA; NA; NA","2013 Third International Conference on Instrumentation, Measurement, Computer, Communication and Control","","2013","","","544","548","In this paper, we present a test data generation algorithm of combinatorial testing based on Differential Evolution, and introduce a selection and substitution based on the degree of unfinished interaction, in order to optimize the test case selected in further. An IPO-like strategy for generating the test suite is used to reduce the dimension for differential evolutionary computation in the optimization process and improve the efficiency of the algorithm. By preliminary experiments, the main parameters in DE algorithm are chosen. And in further experiments, we implement the algorithm in some typical instances to verify its effectiveness. Compare to other well-known algorithms, the final empirical results show the competitiveness of our algorithm in test suite size and running time.","","978-0-7695-5122","10.1109/IMCCC.2013.123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6840513","Momentum Term;test data generation;Differential Evolution;the degree of unfinished interaction","Vectors;Testing;Algorithm design and analysis;Optimization;Software algorithms;Computers;Software","combinatorial mathematics;data flow analysis;evolutionary computation;program testing","test data generation algorithm;combinatorial testing;IPO-like strategy;differential evolutionary computation;test suite size;running time","","","11","","","","","","IEEE","IEEE Conferences"
"Applying Swarm Ensemble Clustering Technique for Fault Prediction Using Software Metrics","R. A. Coelho; F. d. R. N. Guimarães; A. A. A. Esmin","NA; NA; NA","2014 13th International Conference on Machine Learning and Applications","","2014","","","356","361","Number of defects remaining in a system provides an insight into the quality of the system. Defect detection systems predict defects by using software metrics and data mining techniques. Clustering analysis is adopted to build the software defect prediction models. Cluster ensembles have emerged as a prominent method for improving robustness, stability and accuracy of clustering solutions. The clustering ensembles combine multiple partitions generated by different clustering algorithms into a single clustering solution. In this paper, the clustering ensemble using Particle Swarm Optimization algorithm (PSO) solution is proposed to improve the prediction quality. An empirical study shows that the PSO can be a good choice to build defect prediction software models.","","978-1-4799-7415","10.1109/ICMLA.2014.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033140","Software defect prediction;Particle swarm optimization;Cluster data;Ensemble clustering","Clustering algorithms;Software;Prediction algorithms;Software algorithms;Accuracy;Measurement;Particle swarm optimization","data mining;particle swarm optimisation;pattern clustering;program diagnostics;program testing;software fault tolerance;software metrics","swarm ensemble clustering technique;fault prediction;software metrics;defect detection systems;data mining techniques;software defect prediction models;clustering algorithms;particle swarm optimization algorithm solution;PSO solution","","2","27","","","","","","IEEE","IEEE Conferences"
"On the Correlation between the Effectiveness of Metamorphic Relations and Dissimilarities of Test Case Executions","Y. Cao; Z. Q. Zhou; T. Y. Chen","NA; NA; NA","2013 13th International Conference on Quality Software","","2013","","","153","162","Metamorphic testing (MT) is a property-based automated software testing method. It alleviates the oracle problem by testing programs against metamorphic relations (MRs), which are necessary properties among multiple executions of the target program. For a given problem, usually more than one MR can be identified. It is therefore of practical importance for testers to know the nature of good MRs, that is, which MRs are likely to have higher chances of revealing failures. To address this issue we investigate the correlation between the fault-detection effectiveness of MRs and the dissimilarity (distance) of test case execution profiles. Empirical study results reveal that there is a strong and statistically significant positive correlation between the fault-detection effectiveness and the distance. The findings of this research can help to develop automated means of selecting/prioritizing MRs for cost-effective metamorphic testing.","1550-6002;2332-662X","978-0-7695-5039","10.1109/QSIC.2013.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605921","Software testing;metamorphic testing;metamorphic relation;fault-detection effectiveness;execution dissimilarity;distance measurement;initial execution;follow-up execution","Testing;Measurement;Educational institutions;Correlation;Vectors;Encyclopedias;Computer science","fault diagnosis;program testing;statistical analysis","metamorphic relations;test case execution profile dissimilarities;MT;property-based automated software testing method;oracle problem;program testing;test case execution profile distance;empirical study;statistical analysis;fault-detection effectiveness;MR selection;metamorphic testing;MR prioritization","","9","22","","","","","","IEEE","IEEE Conferences"
"Exception Fault Localization in Android Applications","H. Mirzaei; A. Heydarnoori","NA; NA","2015 2nd ACM International Conference on Mobile Software Engineering and Systems","","2015","","","156","157","In software programs, most of the time, there is a chance of error, even though they are tested carefully. Finding error-related pieces of code is one of the most complicated tasks and it can make incorrect results if done manually. Semi-automated and fully-automated methods have been introduced to overcome this issue. The rapid growth of developing Smart Mobile Applications (SMAs) in recent years, competition among the development teams and many other factors have increased the chance of errors, and hence, the quality of these applications have reduced. There are two approaches to test SMAs in order to reach a high degree of quality: (i) using existing traditional methods and adapting them to SMA environments and (ii) introducing new special methods for SMAs. In this paper, we introduce a semi-automated hybrid method to localize exception errors in Android applications. The proposed approach includes the following three phases: extraction, execution and evaluation. In the extraction phase, all the information about the application under the test (AUT) such as the activity and object properties will be extracted. In the execution phase, we generate a set of test cases and execute them on the AUT. In the evaluation phase, we use test case traces, variable value patterns, and backward static slicing techniques to rank lines of application source code with respect to their relevance to that fault. To support localization of multiple errors in a single run of the approach, we introduce a classification measure on test case traces. Evaluations on nine open source Android applications of different sizes show that our method is effective in practice.","","978-0-7695-5566","10.1109/MobileSoft.2015.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7283055","Fault Localization;Android Applications","Androids;Humanoid robots;Software;Data mining;Java;Runtime;Mobile communication","Android (operating system);mobile computing;program slicing;program testing;software quality;source code (software)","exception fault localization;software programs;error-related code pieces;smart mobile applications;SMA testing;quality degree;semiautomated hybrid method;exception error localization;extraction phase;execution phase;evaluation phase;application-under-the-test;AUT;activity properties;object properties;test case generation;test case traces;variable value patterns;backward static slicing techniques;application source code line ranking;classification measure;test case traces;open source Android applications","","1","7","","","","","","IEEE","IEEE Conferences"
"Test case selection for networked production systems","A. Zeller; M. Weyrich","Institute of Industrial Automation and Software Engineering, University of Stuttgart, Germany; Institute of Industrial Automation and Software Engineering, University of Stuttgart, Germany","2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA)","","2015","","","1","4","This paper provides a discussion on the coming technological changes in process automation of networked production systems, which will change the testing procedure. In the smart factory of the future there will be no possibility to reach a test coverage of 100%, assuming a flexible automation with continuous reconfiguration and dynamic changes during runtime. Consequently, large amounts of test cases and powerful algorithms for their prioritization are needed in order to certify the correct functionality of the production systems in the network. A concept is presented on how to analyze and prioritize the enormous amount of test cases resulting from the changes during runtime. The proposed approach for test case selection utilizes information of the product, the process and the status of the for the prioritization and selection.","1946-0740;1946-0759","978-1-4673-7929-8978-1-4673-7928","10.1109/ETFA.2015.7301604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301604","test case selection;functional testing;test prioritizing;quality-assurance;smart factory;networked systems","Production systems;Automation;Testing;Reliability;Accuracy;History","factory automation;management of change","test case selection;networked production systems;technological change;process automation;smart factory of the future;flexible automation;continuous reconfiguration;dynamic change;product information;production system functionality","","","8","","","","","","IEEE","IEEE Conferences"
"A regression test selection technique by optimizing user stories in an Agile environment","Anita; N. Chauhan","Computer Engineering, YMCA, University of Science & Technology, Faridabad (Haryana), India; Computer Engineering, YMCA, University of Science & Technology, Faridabad (Haryana), India","2014 IEEE International Advance Computing Conference (IACC)","","2014","","","1454","1458","An Agile framework of software development has attracted major players of the software industry. This transition of approach has caused significant changes in terms of fast delivery, less documentation, more satisfaction and more interactions. Effective handling of frequent changes during development is one of the important accepting criteria for this framework by software professionals. Frequent changes during the sprint will cause aggregation of test cases in the suite and may effect the time to delivery of product to the customer. To overcome this issue of late delivery, an approach of test case selection is proposed in this paper. This approach takes into consideration weights of the undirected graph of the group of user story in a module and optimal nature of this proposed method removes other risks of the development of the user story.","","978-1-4799-2572-8978-1-4799-2571","10.1109/IAdCC.2014.6779540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6779540","Agile;Undirected Graph;Regression Test Selection;Testing;Optimal Connection","Testing;Software;Patents;Conferences;Manuals;Length measurement;Complexity theory","DP industry;graph theory;program testing;software prototyping","regression test selection technique;user stories optimization;agile environment;software development;software industry;test case aggregation;undirected graph","","","9","","","","","","IEEE","IEEE Conferences"
"On the Influence of the Number of Objectives in Evolutionary Autonomous Software Agent Testing","S. Kalboussi; S. Bechikh; M. Kessentini; L. B. Said","NA; NA; NA; NA","2013 IEEE 25th International Conference on Tools with Artificial Intelligence","","2013","","","229","234","Autonomous software agents are increasingly used in a wide range of applications. Thus, testing these entities is extremely crucial. However, testing autonomous agents is still a hard task since they may react in different manners for the same input over time. To address this problem, Nguyen et al. [6] have introduced the first approach that uses evolutionary optimization to search for challenging test cases. In this paper, we extend this work by studying experimentally the effect of the number of objectives on the obtained test cases. This is achieved by proposing five additional objectives and solving the new obtained problem by means of a Preference-based Many-Objective Evolutionary Testing (P-MOET) method. The obtained results show that the hardness of test cases increases with the rise of the number of objectives.","1082-3409;2375-0197","978-1-4799-2972-6978-1-4799-2971","10.1109/ICTAI.2013.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735254","Agent testing;many-objective optimization;users preferences","Testing;Autonomous agents;Optimization;Safety;Measurement;Sociology;Statistics","evolutionary computation;multi-agent systems;program testing;software agents","evolutionary autonomous software agent testing;evolutionary optimization;test cases;preference-based many-objective evolutionary testing;P-MOET method","","","24","","","","","","IEEE","IEEE Conferences"
"Analyzing relationship between the number of errors in review processes for embedded software development projects","T. Nakashima; K. Iwatay; Y. Anan; N. Ishii","Dept. of Culture-Information Studies, Sugiyama Jogakuen University, 17-3, Moto-machi, Hoshigaoka, Chikusa-ku, Nagoya, Aichi, 464-8662, Japan; Dept. of Business Administration, Aichi University, 4-60-6, Hiraike-cho, Nakamura-ku, Nagoya, 453-8777, Japan; Process Innovation H.Q, Omron Software Co., Ltd., Shiokoji-Horikawa, Shimogyo-ku Kyoto, 600-8234, Japan; Dept. of Information Science, Aichi Institute of Technology, 1247 Yachigusa, Yakusa-cho, Toyota, 470-0392, Japan","2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS)","","2015","","","485","490","In this study, we examine the relationship between review processes on embedded software development projects. We use the Shapiro-Wilk test and Spearman's rank correlation coefficient to analyze them. The results indicate that the data selected was not from a normally distributed population. The results of Spearman's rank correlation coefficient exhibite the following: (1) the number of errors identified in conceptual design review (CDR) has a slightly effect on the number of errors identified in subsequent review processes except for module design review (MDR), (2) the same holds for MDR and programming review (PGR), (3) the number of errors identified in functional design review (FDR) influences the number of errors identified in subsequent review processes except for MDR, (4) the precision of structural design (SD) has an effect on the precision of programming (PG).","","978-1-4799-8679","10.1109/ICIS.2015.7166641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166641","","Correlation;Embedded software;Distributed databases;Sociology;Information processing","embedded systems;program diagnostics;program testing;software engineering","embedded software development projects;Shapiro-Wilk test;Spearman rank correlation coefficient;conceptual design review;module design review;programming review;functional design review;structural design;programming precision","","","11","","","","","","IEEE","IEEE Conferences"
"8th International Workshop on Search-Based Software Testing (SBST 2015)","G. Gay; G. Antoniol","NA; NA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","2","","1001","1002","This paper is a report on the 8th International Workshop on Search-Based Software Testing at the 37th International Conference on Sofrware Engineering (ICSE). Search-Based Software Testing (SBST) is a form of Search-Based Software Engineering (SBSE) that optimizes testing through the use of computational search. SBST is used to generate test data, prioritize test cases, minimize test suites, reduce human oracle cost, verify software models, test service-orientated architectures, construct test suites for interaction testing, and validate real time properties. The objectives of this workshop are to bring together researchers and industrial practitioners from SBST and the wider software engineering community to share experience and provide directions for future research, and to encourage the use of search techniques to combine aspects of testing with other aspects of the software engineering lifecycle.Three full research papers, three short papers, and threeposition papers will be presented in the two-day workshop. Additionally, six development groups have pitted their test generation tools against a common set of programs and benchmarks, and will present their techniques and results. This report will give the background of the workshop and detail the provisional program.","0270-5257;1558-1225","978-1-4799-1934","10.1109/ICSE.2015.323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203148","","Conferences;Software engineering;Software;Software testing;Search problems;Measurement","","","","","3","","","","","","IEEE","IEEE Conferences"
"Automated testing of L-IRL robot programming language parser","M. Lutovac; D. Bojić; V. Kvrgić","NA; NA; NA","2013 21st Telecommunications Forum Telfor (TELFOR)","","2013","","","825","828","Parser testing represents the basis of compiler testing because the accuracy of parsing execution directly affects the accuracy of semantic analysis, optimization and object code generation. It should include tests for correct and expected, but also for unexpected and invalid cases. In this paper, techniques for testing the parser, as well as algorithms and tools for test sentence generation are discussed. Generation of negative test sentences by modifying the original language grammar is described. Positive and negative test cases generated by Grow algorithm and Purdom algorithm are applied to the testing of L-IRL robot programming language and obtained results are described in this paper.","","978-1-4799-1420-3978-1-4799-1419","10.1109/TELFOR.2013.6716357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716357","","Testing;Service robots;Generators;Robot programming;Software;Postal services","optimising compilers;program testing;programming language semantics;robot programming","automated L-IRL robot programming language parser testing;compiler testing;parsing execution;semantic analysis;compiler optimization;object code generation;test sentence generation;negative test sentences;language grammar;positive test case generation;negative test case generation;Grow algorithm;Purdom algorithm","","","11","","","","","","IEEE","IEEE Conferences"
"Search based techniques and mutation analysis in automatic test case generation: A survey","M. Dave; R. Agrawal","Department of Computer Science, Jagan Nath University, Jaipur, India; Department of Computer Science, Jagan Nath University, Jaipur, India","2015 IEEE International Advance Computing Conference (IACC)","","2015","","","795","799","Mutation testing is an effective adequacy criteria and has been researched upon a lot in the past decades, but lacks practical application due to its high cost. Mutation testing for test data generation has not been studied much. This paper is an up-to-date review of the technologies that have been applied with mutation testing for automatic generation of test data which is optimized regarding time, cost and code coverage. The survey reveals an increase in interest regarding the meta-heuristic techniques along with mutation testing.","","978-1-4799-8047-5978-1-4799-8046","10.1109/IADCC.2015.7154816","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7154816","Based Software Engineering;Evolutionary algorithms;test case generation;mutation score","Software engineering;Testing;Evolutionary computation;Software;Software algorithms;Reliability","evolutionary computation;program testing;search problems","search based techniques;mutation analysis;automatic test case generation;mutation testing;test data generation;code coverage;meta-heuristic techniques","","3","38","","","","","","IEEE","IEEE Conferences"
"Developing new Automatic Test Equipments (ATE) using systematic design approaches","H. A. Toku","Test Engineering Dept., Defence Systems Technologies Div., Aselsan INC., Ankara, Turkey","2013 IEEE AUTOTESTCON","","2013","","","1","7","Keeping Automatic Test Equipments (ATE) current with technology is one of the major challenges in automatic testing world. Needs and priorities can quickly evolve throughout the life cycle of ATEs and handling obsolescence via performing upgrades on hardware and software can be impossible after several years. While developing Test Program Sets (TPS), if existing ATE systems cannot meet the necessary requirements without inserting extra test devices or decreasing test coverage, then designing a new ATE can be seen inevitable. If a new ATE system is to be designed, it is very crucial that the requirements for the new ATE system should be identified before design process begins. Determining the requirements is a very critical stage in designing ATE because if enough effort is not focused and extended analysis is not carried out on determining the requirements, then the newly formed ATE system will likely fail to cover the test requirements of the DUTs. Setting up the hardware and software architecture is the next stage after the process of determining the requirements of the ATE system. Practical and cost effective solutions should be considered without compromising performance and capabilities of the test devices. The architectures should be suitable for future enhancements to the system. Throughout the design process, the design requirements, critical design descriptions, verification and validation procedures should be clearly documented and reviewed with relevant engineers. In this paper, the design process of a new ATE system by using systematic design approach is discussed. This process is followed during the design of the ATE system which is under use from the beginning of the year 2013. The challenges in the design process, determining the requirements and the formation of hardware and software architecture are explained in detail benefiting from real experiences.","1088-7725;1558-4550","978-1-4673-5683-1978-1-4673-5681","10.1109/AUTEST.2013.6645035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645035","Automatic Test Equipment;requirements development process;systematic design approach;rack design;PXI;LXI","Instruments;Hardware;Software;Software architecture;Switches;Computers;Cooling","automatic test equipment;automatic test software;automatic testing;design for testability","automatic test equipments;systematic design approaches;test program sets;ATE system requirements;hardware architecture;software architecture;design process","","4","4","","","","","","IEEE","IEEE Conferences"
"A Method to Test the Information Quality of Technical Documentation on Websites","O. Shpak; W. Löwe; A. Wingkvist; M. Ericsson","NA; NA; NA; NA","2014 14th International Conference on Quality Software","","2014","","","296","304","In software engineering, testing is one of the corner-stones of quality assurance. The idea of software testing can be applied to information quality as well. Technical documentation has a set of intended uses that correspond to use cases in a software system. Documentation is, in many cases, presented via software systems, e.g., web servers and browsers, and contains software, e.g., Javascript for user interaction, animation, and customization, etc. This makes it difficult to find a clear-cut distinction between a software system and technical documentation. However, we can assume that each use case of a technical documentation involves retrieval of some sort of information that helps a user answer a specific questions. To assess information testing as a method, we implemented QAnalytics, a tool to assess the information quality of documentation that is provided by a website. The tool is web-based and allows test managers and site owners to define test cases and success criteria, disseminate the test cases to testers, and to analyze the test results. This way, information testing is easily manageable even for non-technical stakeholders. We applied our testing method and tool in a case study. According to common perception, the website of Linnaeus University needs to be re-engineered. Our method and tool helped the stakeholders identify what information is presented well and which parts of the website that need to change. The test results allowed the design and development effort to prioritize actual quality issues and potentially save time and resources.","1550-6002;2332-662X","978-1-4799-7198-5978-1-4799-7197","10.1109/QSIC.2014.48","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958417","information quality;web analytics;web testing","Documentation;Standards;Gold;Educational institutions;Software;Software testing","document handling;program testing;software quality;Web sites","information quality;technical documentation;Web sites;software engineering;software testing;quality assurance;information testing;Linnaeus University","","3","11","","","","","","IEEE","IEEE Conferences"
"Using Mutation Analysis to Evolve Subdomains for Random Testing","M. Patrick; R. Alexander; M. Oriol; J. A. Clark","NA; NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops","","2013","","","53","62","Random testing is inexpensive, but it can also be inefficient. We apply mutation analysis to evolve efficient subdomains for the input parameters of eight benchmark programs that are frequently used in testing research. The evolved subdomains can be used for program analysis and regression testing. Test suites generated from the optimised subdomains outperform those generated from random subdomains with 10, 100 and 1000 test cases for uniform, Gaussian and exponential sampling. Our subdomains kill a large proportion of mutants for most of the programs we tested with just 10 test cases.","","978-0-7695-4993-4978-1-4799-1324","10.1109/ICSTW.2013.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571608","mutation testing input distribution test case generation search based evolution strategy","Optimization;Shape;Schedules;Gaussian distribution;Benchmark testing;Convergence","program diagnostics;program testing;regression analysis;sampling methods","mutation analysis;random testing;subdomain evolution;testing research;program analysis;regression testing;test suite;optimised subdomain;random subdomain;uniform sampling;Gaussian sampling;exponential sampling","","3","31","","","","","","IEEE","IEEE Conferences"
"Code coverage optimisation in genetic algorithms and particle swarm optimisation for automatic software test data generation","C. Koleejan; B. Xue; M. Zhang","School of Engineering and Computer Science, Victoria University of Wellington, PO Box 600, Wellington 6140, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, PO Box 600, Wellington 6140, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, PO Box 600, Wellington 6140, New Zealand","2015 IEEE Congress on Evolutionary Computation (CEC)","","2015","","","1204","1211","Automatic software test data generation is the process of generating a set of test cases for a given program which can achieve a high code coverage. Genetic algorithms (GAs) and particle swarm optimisation (PSO) can automatically evolve a set of test data, but the traditional representation in GAs and PSO produces solutions with a single set of data cases, which may not achieve good performance on programs with many complex conditions. This paper proposes a multi-vector representation in GAs and PSO, which can generate multiple sets of data cases in a single run, to generate test data for complex test programs. Experiments have been conducted to examine and compare the performance of GAs and PSO on six commonly used benchmark test programs and three newly developed programs with a relatively large number of complex conditions. The experimental results show that the proposed multi-vector representation can improve the performance of GAs and PSO on all the nine tested programs, achieving the optimal 100% code coverage on the relatively easy programs. PSO outperforms GAs in terms of both the code coverage and the computational efficiency, especially on the hard programs.","1089-778X;1941-0026","978-1-4799-7492-4978-1-4799-7491","10.1109/CEC.2015.7257026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257026","","Acceleration;Genetics","genetic algorithms;particle swarm optimisation;program testing;vectors","code coverage optimisation;genetic algorithms;particle swarm optimisation;automatic software test data generation;PSO;GA;data cases;multivector representation;benchmark test programs;complex conditions;code coverage;computational efficiency;hard programs","","1","30","","","","","","IEEE","IEEE Conferences"
"An ABC based approach to test case generation for BPEL processes","M. Daghaghzadeh; M. Babamir","Computer Engineering Department, University of Kashan, Iran; Computer Engineering Department, University of Kashan, Iran","ICCKE 2013","","2013","","","272","277","One of the important tasks in software testing process is test case generation. The automation of this task reduces the endeavor to generate test suite and contributes to selecting a subset of test cases from the test suite that can be executed in minimum time and optimally meets the test adequacy criteria. In this paper, we put forward a new approach for automating the generation of test cases for BPEL processes. A transition coverage criterion is used as the adequacy criterion. Our approach is based on meta-heuristic approach ABC (Artificial Bee Colony Optimization) which is inspired by the behavior of honey bees. We examine whether our approach outdo existing test optimization approaches in time, number of test cases and the percentage of coverage.","","978-1-4799-2093-8978-1-4799-2092","10.1109/ICCKE.2013.6682849","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6682849","Software Testing;BPEL processes;Test Case Generation;Bee Colony Artifial Algorithm","Testing;Service-oriented architecture;Sociology;Statistics;Generators","ant colony optimisation;program testing;Web Services Business Process Execution Language","ABC based approach;test case generation;BPEL processes;software testing process;test adequacy criteria;transition coverage criterion;artificial bee colony optimization;honey bee behavior;test optimization approach","","2","18","","","","","","IEEE","IEEE Conferences"
"Large-Scale Multiobjective Static Test Generation for Web-Based Testing with Integer Programming","M. Luan Nguyen; S. Cheung Hui; A. C.M. Fong","Nangyang Technological University, Singapore; Nanyang Technological University, Singapore; Auckland University of Technology, Auckland","IEEE Transactions on Learning Technologies","","2013","6","1","46","59","Web-based testing has become a ubiquitous self-assessment method for online learning. One useful feature that is missing from today's web-based testing systems is the reliable capability to fulfill different assessment requirements of students based on a large-scale question data set. A promising approach for supporting large-scale web-based testing is static test generation (STG), which generates a test paper automatically according to user specification based on multiple assessment criteria. And the generated test paper can then be attempted over the web by users for assessment purpose. Generating high-quality test papers under multiobjective constraints is a challenging task. It is a 0-1 integer linear programming (ILP) that is not only NP-hard but also need to be solved efficiently. Current popular optimization software and heuristic-based intelligent techniques are ineffective for STG, as they generally do not have guarantee for high-quality solutions of solving the large-scale 0-1 ILP of STG. To that end, we propose an efficient ILP approach for STG, called branch-and-cut for static test generation (BAC-STG). Our experimental study on various data sets and a user evaluation on generated test paper quality have shown that the BAC-STG approach is more effective and efficient than the current STG techniques.","1939-1382;2372-0050","","10.1109/TLT.2012.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365621","Testing;Linear programming;Web and internet services;Runtime;Sparse matrices;Integer linear programming;Computer aided instruction;Online services;Web-based testing;Testing;Linear programming;Web and internet services;Runtime;Sparse matrices;Integer linear programming;Computer aided instruction;Online services;static test generation;Testing;Linear programming;Web and internet services;Runtime;Sparse matrices;Integer linear programming;Computer aided instruction;Online services;multiobjective optimization;Testing;Linear programming;Web and internet services;Runtime;Sparse matrices;Integer linear programming;Computer aided instruction;Online services;integer programming","Testing;Linear programming;Web and internet services;Runtime;Sparse matrices;Integer linear programming;Computer aided instruction;Online services","computer aided instruction;integer programming;linear programming;quality control;testing;ubiquitous computing","large-scale multiobjective static test generation;Web-based testing;ubiquitous self-assessment method;online learning;high-quality test papers;integer linear programming;ILP;NP-hard problem;branch-and-cut;BAC-STG;test paper quality","","4","42","","","","","","IEEE","IEEE Journals & Magazines"
"Selecting Highly Efficient Sets of Subdomains for Mutation Adequacy","M. Patrick; R. Alexander; M. Oriol; J. A. Clark","NA; NA; NA; NA","2013 20th Asia-Pacific Software Engineering Conference (APSEC)","","2013","1","","91","98","Test selection techniques are used to reduce the human effort involved in software testing. Most research focusses on selecting efficient sets of test cases according to various coverage criteria for directed testing. We introduce a new technique to select efficient sets of sub domains from which new test cases can be sampled at random to achieve a high mutation score. We first present a technique for evolving multiple sub domains, each of which target a different group of mutants. The evolved sub domains are shown to achieve an average 160% improvement in mutation score compared to random testing with six real world Java programs. We then present a technique for selecting sets of the evolved sub domains to reduce the human effort involved in evaluating sampled test cases without reducing their fault finding effectiveness. This technique significantly reduces the number of sub domains for four of the six programs with a negligible difference in mutation score.","1530-1362;1530-1362","978-1-4799-2144-7978-1-4799-2143","10.1109/APSEC.2013.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805394","software testing;mutation analysis;test data generation;test case selection;input subdomains","Optimization;Testing;Schedules;Covariance matrices;Equations;Software;Fault diagnosis","Java;program testing;software fault tolerance","highly efficient subdomain sets;mutation adequacy;test selection techniques;software testing;test cases;Java programs;fault finding effectiveness;mutation score","","","38","","","","","","IEEE","IEEE Conferences"
"Random Grammar-Based Testing for Covering All Non-terminals","A. Dreyfus; P. Héam; O. Kouchnarenko","NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops","","2013","","","210","215","In the context of software testing, generating complex data inputs is frequently performed using a grammar-based specification. For combinatorial reasons, an exhaustive generation of the data - of a given size - is practically impossible, and most approaches are either based on random techniques or on coverage criteria. In this paper, we show how to combine these two techniques by biasing the random generation in order to optimise the probability of satisfying a coverage criterion.","","978-0-7695-4993-4978-1-4799-1324","10.1109/ICSTW.2013.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571632","Random testing;Grammar-based testing","Grammar;Arrays;Context;Software;Conferences;Software testing","formal specification;grammars;probability;program testing;random number generation","random grammar-based testing;software testing;complex data input;grammar-based specification;combinatorial reason;data generation;random generation;probability;coverage criterion","","2","25","","","","","","IEEE","IEEE Conferences"
"Graphical user interface testing using evolutionary algorithms","G. I. Laţiu; O. Creţ; L. Văcariu","Computer Science Department, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Computer Science Department, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Computer Science Department, Technical University of Cluj-Napoca, Cluj-Napoca, Romania","2013 8th Iberian Conference on Information Systems and Technologies (CISTI)","","2013","","","1","6","In software applications industry, the most important challenge is to ensure the maximum quality for software products. The demand for Graphical user interface (GUI) testing has very much increased in the last years. An important progress in this field has been achieved, from GUI manual testing to complete automate GUI testing. In this paper, optimization theory and evolutionary algorithms (EA) concepts were applied for GUI testing. In our approach, the GUI is completely separated from the code itself. The aim of the proposed methodology is to find the test case sequence(s) which produce(s) the maximum amount of changes inside the GUI while preserving all other constraints. The method starts by randomly generating the initial set of test cases, which is then improved through measuring and evaluating specific fitness functions. By using optimization theory, very good results were obtained for accelerating the automatic test of large interfaces.","2166-0727","978-989-98434-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615841","Graphical user interface;automatic testing;evolutionary testing;evolutionary algorithms","Graphical user interfaces;Testing;Software;Manuals;Evolutionary computation;Sociology;Statistics","DP industry;evolutionary computation;graphical user interfaces;optimisation;program testing","fitness functions;test case sequence;EA;optimization theory;automate GUI testing;GUI manual testing;software product quality;software applications industry;evolutionary algorithms;graphical user interface testing","","","22","","","","","","IEEE","IEEE Conferences"
"A tool for data flow testing using evolutionary approaches (ETODF)","S. A. Khan; A. Nadeem","Department of Computer Science, Mohammad Ali Jinnah University, Islamabad, Pakistan; Department of Computer Science, Mohammad Ali Jinnah University, Islamabad, Pakistan","2013 IEEE 9th International Conference on Emerging Technologies (ICET)","","2013","","","1","6","Software testing is one of the most important phases of software development lifecycle. Software testing can be categorized into two major types; white box testing and black box testing. Data flow testing is a white box testing technique that uses both flow of control and flow of data through the program for testing. Evolutionary testing selects and generates test data by applying optimizing search techniques. This paper discusses the architecture and implementation of an automated tool for data flow testing by applying genetic algorithm for the automatic generation of test paths for data flow testing based on selected criteria for data flow testing. Our tool generates random initial population of test paths and then based on the selected data flow testing criteria new paths are generated by applying a genetic algorithm. A fitness function in tool evaluates each chromosome (path) based on the selected data flow testing criteria and computes its fitness. We have applied one point crossover and mutation operators for the generation of new paths based on fitness value. The proposed research tool called ETODF is continuation of our previous research work [6] on data flow testing using evolutionary approaches. The tool ETODF (evolutionary testing of data flow) has been implemented in Java. In experiments with this tool, our implemented tool has much better results as compared to random testing.","","978-1-4799-3457-7978-1-4799-3456","10.1109/ICET.2013.6743535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6743535","Evolutionary Algorithm;Test Data;Data Flow Testing;Mutation;Crossover","Software testing;Genetic algorithms;Software;Semantics;Flow graphs;TV","genetic algorithms;program testing","data flow testing;evolutionary approach;software testing;software development lifecycle;white box testing;black box testing;control flow;data flow;test data selection;test data generation;search techniques;genetic algorithm;fitness function;one point crossover operator;mutation operator;fitness value;ETODF tool;evolutionary testing of data flow;Java","","1","25","","","","","","IEEE","IEEE Conferences"
"Energy-directed test suite optimization","D. Li; C. Sahin; J. Clause; W. G. J. Halfond","Computer Science Department University of Southern California, Los Angeles, CA, USA; Computer and Information Sciences Department University of Delaware, Newark, DE, USA; Computer and Information Sciences Department University of Delaware, Newark, DE, USA; Computer Science Department University of Southern California, Los Angeles, CA, USA","2013 2nd International Workshop on Green and Sustainable Software (GREENS)","","2013","","","62","69","Post-deployment in situ testing and validation techniques have become an important means of ensuring the reliability of mobile and embedded systems. However, these techniques do not take into consideration the amount of energy they consume, which is an issue of paramount concern for systems with limited energy budgets. In this paper we propose a new test suite minimization approach that allows developers to generate energy-efficient, minimized test suites. The approach is based on encoding minimization problems as integer linear programming problems. Our empirical evaluation shows that, compared to traditionally generated minimized test suites, the test suites generated by our approach are equally effective in terms of their test coverage, but can realize energy savings of up to 90 %.","","978-1-4673-6267","10.1109/GREENS.2013.6606423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606423","Energy use;Testing;Test suite optimization;Post-deployment validation","Minimization;Energy consumption;Androids;Humanoid robots;Testing;Instruments;Energy measurement","embedded systems;integer programming;linear programming;mobile computing;power aware computing;program testing","test suite minimization approach;energy-efficient minimized test suites;encoding minimization problems;integer linear programming problems;limited energy budgets;embedded systems;mobile systems;validation techniques;in situ testing;energy-directed test suite optimization","","5","22","","","","","","IEEE","IEEE Conferences"
"A statistical machine learning based modeling and exploration framework for run-time cross-stack energy optimization","C. Zhang; A. Ravindran","Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, USA; Department of Electrical and Computer Engineering, University of North Carolina at Charlotte, USA","2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)","","2013","","","136","137","As the complexity of many-core processors grow, meeting performance, energy, temperature, reliability, and noise requirements under dynamically changing operating conditions requires run-time optimization of all parts of the computing stack - architecture, system software, and applications. Unfortunately, the combination of design parameters for the entire computing stack results in an operating space of millions of points that must be explored and evaluated at run-time. In this paper, we present a statistical machine learning (SML) based modeling framework that can be used to rapidly explore such vast operating spaces. We construct a multivariate adaptive regression spline (MARS) based model that uses a number of architecture and application parameters as predictor variables to predict performance and power. We then use a Pareto-front exploring evolutionary algorithm to determine operating points for optimal power and performance. The operating points constituting the Pareto front are stored in look-up tables for runtime use. The proposed framework is applied to an ×264 video encoding application executing on a quad core processor. The microarchitectural predictor variables include core and cache parameters. The application predictor variables include the video resolution, and visual quality determined by the choice of the motion estimation algorithm. The model outputs the average frames per second (FPS) and the average power consumption. The MARS model has an R<sup>2</sup>of 0.9657 and 0.9467 respectively for FPS and power. For a video frame resolution of 480x320, and FPS of 20, a power saving of 55% can be obtained by jointly tuning the microarchitectural parameters and the visual quality.","","978-1-4673-5779-1978-1-4673-5776-0978-1-4673-5778","10.1109/ISPASS.2013.6557161","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557161","modeling;energy;optimization;run-time;cross stack","Visualization;Microarchitecture;Benchmark testing;Pareto optimization;Adaptation models;Power demand;Measurement","cache storage;computer architecture;evolutionary computation;image resolution;learning (artificial intelligence);motion estimation;multiprocessing systems;Pareto optimisation;power aware computing;regression analysis;splines (mathematics);statistical analysis;video coding","statistical machine learning based modeling framework;statistical machine learning based exploration framework;run-time cross-stack energy optimization;many-core processor complexity;performance requirement;energy requirement;temperature requirement;reliability requirement;noise requirement;run-time optimization;computing stack-architecture;system software;SML;multivariate adaptive regression spline based model;MARS;predictor variables;performance prediction;power prediction;Pareto-front exploring evolutionary algorithm;video encoding application;quad core processor;microarchitectural predictor variables;core parameters;cache parameters;video resolution;visual quality;motion estimation algorithm;average frames-per-second;average power consumption;look-up tables","","","6","","","","","","IEEE","IEEE Conferences"
"Comparative performance analysis of bat algorithm and bacterial foraging optimization algorithm using standard benchmark functions","Y. A. Alsariera; H. S. Alamri; A. M. Nasser; M. A. Majid; K. Z. Zamli","Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, UMP, 26300 Kuantan, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, UMP, 26300 Kuantan, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, UMP, 26300 Kuantan, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, UMP, 26300 Kuantan, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, UMP, 26300 Kuantan, Malaysia","2014 8th. Malaysian Software Engineering Conference (MySEC)","","2014","","","295","300","Optimization problem relates to finding the best solution from all feasible solutions. Over the last 30 years, many meta-heuristic algorithms have been developed in the literature including that of Simulated Annealing (SA), Genetic Algorithm (GA), Ant Colony Optimization (ACO), Particle Swarm Optimization (PSO), Harmony Search Algorithm (HS) to name a few. In order to help engineers make a sound decision on the selection amongst the best meta-heuristic algorithms for the problem at hand, there is a need to assess the performance of each algorithm against common case studies. Owing to the fact that they are new and much of their relative performance are still unknown (as compared to other established meta-heuristic algorithms), Bacterial Foraging Optimization Algorithm (BFO) and Bat Algorithm (BA) have been adopted for comparison using the 12 selected benchmark functions. In order to ensure fair comparison, both BFO and BA are implemented using the same data structure and the same language and running in the same platform (i.e. Microsoft Visual C# with .Net Framework 4.5). We found that BFO gives more accurate solution as compared to BA (with the same number of iterations). However, BA exhibits faster convergence rate.","","978-1-4799-5439","10.1109/MySec.2014.6986032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986032","metaheuristc optimization algorithms;metaheuristics algorithm;bat algorithm;bacterial foraging optimization algorithm","Microorganisms;Optimization;Barium;Sociology;Statistics;Benchmark testing;Heuristic algorithms","evolutionary computation","bat algorithm;bacterial foraging optimization algorithm;optimization problem;metaheuristic algorithm;simulated annealing;SA;genetic algorithm;GA;ant colony optimization;ACO;particle swarm optimization;PSO;harmony search algorithm;HS","","1","15","","","","","","IEEE","IEEE Conferences"
"Functional design and verification of automotive embedded software: An integrated system verification flow","M. Shedeed; G. Bahig; M. W. Elkharashi; M. Chen","Mentor Graphics, USA; Mentor Graphics, USA; Mentor Graphics, USA; Mentor Graphics, USA","2013 Saudi International Electronics, Communications and Photonics Conference","","2013","","","1","5","Automotive systems are diverse, extensively interactive, and multi-disciplinary by nature. We propose a flow that integrates the different environments and tools needed for modeling and simulation of sub-components at each abstraction level, namely, Model in the Loop, Model-to-Software in the Loop, Software in the Loop, and Hardware in the Loop. The proposed flow verifies the system at each of these abstraction levels in the automotive domain. We present a systematic methodology and verification flow for a detailed migration procedure between these different abstraction levels to fulfill complicated automotive system requirements. Our flow has been tested using a brake-bywire anti-locking car system use case. Experimental results show the efficiency of the proposed flow in discovering early incorrect system behavior at each abstraction level. A common graphical test design and generation tool complements the proposed flow at each level to ensure that the generated tests address the same system functionality at each abstraction level and optimizes the cost of test design and generation.","","978-1-4673-6195-8978-1-4673-6196-5978-1-4673-6194","10.1109/SIECPC.2013.6550793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6550793","Anti-lock Braking System (ABS);Automotive Software Component (SWC);AUTOSAR;Brake-by-Wire (BBW) brakes;Electronic Control Unit (ECU);Hardware in the Loop (HiL);Model in the Loop (MiL);Model-to-Software in the Loop (MiL-to-SiL);Software in the Loop (SiL)","Mathematical model;Automotive engineering;Vehicles;MATLAB;Wheels;Hardware","automatic test software;automotive electronics;digital simulation;embedded systems;program compilers;program testing;program verification","functional design;automotive embedded software verification;integrated system verification flow;automotive systems;subcomponent simulation;subcomponent modeling;model in the loop simulation;model-to-software in the loop simulation;software in the loop simulation;hardware in the loop simulation;brake-by-wire antilocking car system;early incorrect system behavior discovery;test design cost optimization;graphical test generation tool;graphical test design tool","","3","16","","","","","","IEEE","IEEE Conferences"
"Design and manufacturing of lauch support test set for Arirang-3","K. Young-Yun; C. Dong-Chul; C. Jong-Yeoun; K. Jae-Wook","Electrical Integration &amp; Test Team, Space Test Division; Electrical Integration &amp; Test Team, Space Test Division; Electrical Integration &amp; Test Team, Space Test Division; Lunar Exploration Research Team Korea Aerospace research Institute Daejeon, Korea","2013 IEEE AUTOTESTCON","","2013","","","1","8","Korea Aerospace Research Institute launched Korea's multi-purpose satellite, Arirang-3, on May 18, 2013. Arirang-3 has been performing its mission successfully. A Launch Support Test Set (LSTS) was used for the setting of the final satellite launch configuration and to monitor satellite status under the harsh launch environment. The LSTS starts its function several hours before launch by supplying electrical power to the satellite. After the powering up, the setting of the final launch configuration and the monitoring are conducted sequentially. Then the battery is continuously charged until about ten minutes to lift-off. The mission of LSTS ends with lift-off. The LSTS has been used from the Arirang-1 program to the Arirang-3 under severe launch environment in order to support successful launch. The LSTS is installed in the space for Electrical Ground Support Equipment (EGSE). Generally the launcher does not supply enough space located in the launcher or on the bunker under the launcher for EGSE movement and installation. So the LSTS has to be a compact size in order to handle easily and has optimized electrically. It was verified during the last satellite program that the LSTS requires several improvements in software and hardware. The LSTS for Arirang-3 has several upgrades compared to its predecessors. One is the use of a mountable controller, another is the backup power off function, and the third is the development of easily recognizable and changeable software. The others are introduced in this paper.","1088-7725;1558-4550","978-1-4673-5683-1978-1-4673-5681","10.1109/AUTEST.2013.6645034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645034","EGSE;Launch support test equipment;launch site;satellite final configuration","Satellites;Software;Hardware;Monitoring;Batteries;Manufacturing;Safety","aerospace testing;artificial satellites;test equipment","backup power off function;mountable controller;electrical ground support equipment;lift-off;harsh launch environment;final satellite launch configuration;Korea multipurpose satellite;ARIRANG-3;lauch support test set","","","3","","","","","","IEEE","IEEE Conferences"
"Getting more from requirements traceability: Requirements testing progress","C. Ziftci; I. Krüger","Department of Computer Science and Engineering, University of California, San Diego, La Jolla, USA; Department of Computer Science and Engineering, University of California, San Diego, La Jolla, USA","2013 7th International Workshop on Traceability in Emerging Forms of Software Engineering (TEFSE)","","2013","","","12","18","Requirements Engineering (RE) and Testing are important steps in many software development processes. It is critical to monitor the progress of the testing phase to allocate resources (person-power, time, computational resources) properly, and to make sure the prioritization of requirements are reflected during testing, i.e. more critical requirements are given higher priority and tested well. In this paper, we propose a new metric to help stakeholders monitor the progress of the testing phase from a requirements perspective, i.e. which requirements are tested adequately, and which ones insufficiently. Unlike existing progress related metrics, such as code coverage and MC/DC (modified condition/decision) coverage, this metric is on the requirements level, not source code level. We propose to automatically reverse engineer this metric from the existing test cases of a system. We also propose a method to evaluate this metric, and report the results of three case studies. On these case studies, our technique obtains results within 75.23% - 91.11% of the baseline on average.","2157-2186;2157-2194","978-1-4799-0495","10.1109/TEFSE.2013.6620148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620148","Requirements traceability;requirements testing progress;test monitoring;requirements coverage;automated analysis;reverse engineering","Testing;Measurement;Context;Lattices;Software;Vectors;Monitoring","program testing;reverse engineering;software engineering;software metrics","requirements traceability;requirements testing progress;progress related metrics;modified condition-decision coverage;source code level;reverse engineer;software development","","1","28","","","","","","IEEE","IEEE Conferences"
"Variable strength interaction test set generation using Multi Objective Genetic Algorithms","S. Sabharwal; M. Aggarwal","Department of Information Technology, NSIT, Delhi, India; Department of Information Technology, NSIT, Delhi, India","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","","2015","","","2049","2053","Combinatorial testing aims at identifying faults that are caused due to interactions of a small number of input parameters. It provides a technique to select a subset of exhaustive test cases covering all the t-way interactions without much loss of the fault detection capability. The test set generated is for a fixed value of t. In this paper, an approach is proposed to generate test set for a system where some variables have higher interaction strength among them as compared to that of the system. Variable Strength Covering Arrays are used for testing such systems. We propose to generate Variable Strength Covering Arrays using Multi objective optimization (Multi Objective Genetic Algorithms). We attempt to reduce the test set size while covering all the base level interactions of the system and higher strength interactions of its components. Experimental results indicate that the proposed approach generates results comparable to or better in some cases as compared to that of existing approaches.","","978-1-4799-8792-4978-1-4799-8790-0978-1-4799-8791","10.1109/ICACCI.2015.7275918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275918","Combinatorial Testing;Variable Strength Covering Arrays;Multi Objective Genetic Algorithms;Interaction Testing;t-way Testing","Genetic algorithms;Optimization;Software testing;Biological cells;Conferences;Particle swarm optimization","genetic algorithms;program testing;set theory;software fault tolerance","variable strength interaction test set generation;multiobjective genetic algorithms;combinatorial testing;fault identification;t-way interactions;fault detection capability;variable strength covering arrays;multiobjective optimization","","1","26","","","","","","IEEE","IEEE Conferences"
"Does the Failing Test Execute a Single or Multiple Faults? An Approach to Classifying Failing Tests","Z. Yu; C. Bai; K. Cai","NA; NA; NA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","1","","924","935","Debugging is an indispensable yet frustrating activity in software development and maintenance. Thus, numerous techniques have been proposed to aid this task. Despite the demonstrated effectiveness and future potential of these techniques, many of them have the unrealistic single-fault failure assumption. To alleviate this problem, we propose a technique that can be used to distinguish failing tests that executed a single fault from those that executed multiple faults in this paper. The technique suitably combines information from (i) a set of fault localization ranked lists, each produced for a certain failing test and (ii) the distance between a failing test and the passing test that most resembles it to achieve this goal. An experiment on 5 real-life medium-sized programs with 18, 920 multiple-fault versions, which are shipped with number of faults ranging from 2 to 8, has been conducted to evaluate the technique. The results indicate that the performance of the technique in terms of evaluation measures precision, recall, and F-measure is promising. In addition, for the identified failing tests that executed a single fault, the technique can also properly cluster them.","0270-5257;1558-1225","978-1-4799-1934","10.1109/ICSE.2015.102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194638","distance calculation;fault localization;binary classification;debugging","Maintenance engineering;Fault diagnosis;Debugging;Indexes;Software;Classification algorithms;Testing","pattern classification;program debugging;program testing;software fault tolerance;software maintenance","failing test classification;debugging;software development;software maintenance;single-fault failure assumption;fault localization;evaluation measures precision;recall;F-measure","","1","46","","","","","","IEEE","IEEE Conferences"
"Reuse distance analysis for locality optimization in loop-dominated applications","C. Lezos; G. Dimitroulakos; K. Masselos","University of Peloponnese, Department of Informatics and Telecommunications, Terma Karaiskaki, 22100 Tripoli, Greece; University of Peloponnese, Department of Informatics and Telecommunications, Terma Karaiskaki, 22100 Tripoli, Greece; University of Peloponnese, Department of Informatics and Telecommunications, Terma Karaiskaki, 22100 Tripoli, Greece","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","1237","1240","This paper discusses MemAddIn, a compiler assisted dynamic code analysis tool that analyzes C code and exposes critical parts for memory related optimizations on embedded systems that can heavily affect systems performance, power and cost. The tool includes enhanced features for data reuse distance analysis and source code transformation recommendations for temporal locality optimization. Several of data reuse distance measurement algorithms have been implemented leading to different trade-offs between accuracy and profiling execution time. The proposed tool can be easily and seamlessly integrated into different software development environments offering a unified environment for application development and optimization. The novelties of our work over a similar optimization tool are also discussed. MemAddIn has been applied for the dynamic computation of data reuse distance for a number of different applications. Experimental results prove the effectiveness of the tool through the analysis and optimization of a realistic image processing application.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0442","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092578","Data reuse distance analysis;locality optimization;memory hierarchy optimization","Optimization;Algorithm design and analysis;Random access memory;Distance measurement;Arrays;Histograms","embedded systems;image processing;optimisation;program compilers;program diagnostics;software reusability;source code (software)","locality optimization;loop-dominated applications;MemAddIn;compiler assisted dynamic code analysis tool;C code;memory related optimizations;embedded systems;data reuse distance analysis;source code transformation recommendations;temporal locality optimization;data reuse distance measurement algorithms;profiling execution time;software development environments;realistic image processing application;optimization","","","20","","","","","","IEEE","IEEE Conferences"
"Flexible high performance architecture for boundary scan execution hardware","T. Borroz","System Test Group, Teradyne, Inc., North Reading, MA, USA","2015 IEEE AUTOTESTCON","","2015","","","232","235","IEEE 1149.1 boundary scan has become very popular since its introduction in the 1990s. It is now used routinely in defense and aerospace testing, and has become commonplace in new TPSes. But mil/aero quality requirements demand functional test as well. This means that boundary scan tests and functional tests must coexist in this environment. Boundary scan test requires access to the same UUT I/O signals as functional test. This is because many boundary scan tests, such as interconnect test, require control and observation of these signals to obtain full fault coverage. Functional test requires high-performance dynamic channels on the UUT I/O signals. The boundary scan requirements are less demanding. This paper proposes the use of configurable test hardware to address both requirements. This is technically achievable now that test systems can make use of FPGAs that can be efficiently reconfigured at run time. This paper proposes a boundary scan test architecture that can be combined with an existing functional test architecture in such a system. The boundary scan hardware architecture would be based on a specially designed processor optimized for boundary scan which would support up to 8 TAP ports. All available pins not used for TAP ports could be used for parallel I/O signals up to the constraints imposed by the limitations of the FPGA and fixturing. The custom processor could include throughput enhancing features such as pipelined double buffered DMA data transfer. With proper design, such a processor could run continuously as long as it was connected to a host computer that was fast enough to keep it supplied with data. Such a custom processor should be able to closely approach the device limited test time in throughput-critical situations, such as flash programming. It should be possible to straightforwardly control such hardware using a general purpose boundary scan runtime software library, which would present its client with an API that is conceptually similar to existing boundary scan languages like SVF and STAPL.","","978-1-4799-8190-8978-1-4799-8189","10.1109/AUTEST.2015.7356494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7356494","Boundary scan;JTAG;IEEE 1149.1;Flash programming;Device limited test time;DLTT","Programming;Hardware;Field programmable gate arrays;Pins;Testing;Ports (Computers);Flash memories","boundary scan testing;field programmable gate arrays;software libraries","flexible high performance architecture;boundary scan execution hardware;STAPL;SVF;boundary scan languages;API;general purpose boundary scan runtime software library;flash programming;pipelined double buffered DMA data transfer;parallel I/O signals;TAP ports;processor;boundary scan hardware architecture;functional test architecture;boundary scan test architecture;FPGA;configurable test hardware;boundary scan requirements;high-performance dynamic channels;fault coverage;interconnect test;UUT I/O signals;IEEE 1149.1 boundary scan","","1","8","","","","","","IEEE","IEEE Conferences"
"Worst-Case Execution Time Test Generation for Augmenting Path Maximum Flow Algorithms Using Genetic Algorithms","V. Arkhipov; M. Buzdalov; A. Shalyto","NA; NA; NA","2013 12th International Conference on Machine Learning and Applications","","2013","2","","108","111","Worst-case execution time tests can be tricky to create for various computer science algorithms. To reduce the amount of human effort, authors suggest using search-based optimization techniques, such as genetic algorithms. This paper addresses difficult test generation for several maximum flow algorithms from the augmenting path family. The presented results show that the genetic approach is reasonably good for the well-studied algorithms and superior for the capacity scaling algorithms. Moreover, tests which are generated against one algorithm seem to be hard for other algorithms of this family.","","978-0-7695-5144","10.1109/ICMLA.2013.180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786090","maximum flow;test generation;worst-case execution time;performance;genetic algorithms","Generators;Wheels;Genetic algorithms;Algorithm design and analysis;Software algorithms;Genetics;Optimization","computer science;genetic algorithms;program testing;search problems;software engineering","worst case execution time test generation;augmenting path maximum flow algorithms;genetic algorithms;computer science algorithms;search based optimization techniques","","2","10","","","","","","IEEE","IEEE Conferences"
"Toolset and Program Repository for Code Coverage-Based Test Suite Analysis and Manipulation","D. Tengeri; Á. Beszédes; D. Havas; T. Gyimóthy","NA; NA; NA; NA","2014 IEEE 14th International Working Conference on Source Code Analysis and Manipulation","","2014","","","47","52","Code coverage is often used in academic and industrial practice of white-box software testing. Various test optimization methods, e.g. Test selection and prioritization, rely on code coverage information, but other related fields benefit from it as well, such as fault localization. These methods require access to the fine details of coverage information and efficient ways of processing this data. The purpose of the (free) SoDA library and toolset is to provide an efficient set of data structures and algorithms which can be used to prepare, store and analyze in various ways data related to code coverage. The focus of SoDA is not on the calculation of coverage data (such as instrumentation and test execution) but on the analysis and manipulation of test suites based on such information. An important design goal of the library was to be usable on industrial-size programs and test suites. Furthermore, there is no limitation on programming language, analysis granularity and coverage criteria. In this paper, we demonstrate the purpose and benefits of the library, the associated toolset, which also includes a graphical user interface, as well as possible usage scenarios. SoDA also includes a repository of prepared programs, which are from small to large sizes and can be used for experimentation and as a benchmark for code coverage related research.","","978-1-4799-6148","10.1109/SCAM.2014.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975635","Regression testing;test suite analysis;test suite optimization;code coverage;program repository","Measurement;Libraries;Algorithm design and analysis;Data structures;Testing;Instruments;Graphical user interfaces","optimisation;program testing","program repository;toolset repository;code coverage;test suite analysis;test suite manipulation;white-box software testing;optimization methods;fault localization;SoDA library;data structures;data algorithms;industrial size programs;programming language","","6","17","","","","","","IEEE","IEEE Conferences"
"A Hybrid Binary Multi-objective Particle Swarm Optimization with Local Search for Test Case Selection","L. S. d. Souza; R. B. C. Prudêncio; F. d. A. Barros","NA; NA; NA","2014 Brazilian Conference on Intelligent Systems","","2014","","","414","419","During the software testing process a variety of test suites can be generated in order to evaluate and assure the quality of the products. However, in some contexts the execution of all suites does not fit the available resources (time, people, etc). In such cases, the suites could be automatically reduced based on some selection criterion. Automatic Test Case (TC) selection could be used to reduce the suites based on some selection criterion. This process can be treated as an optimization problem, aiming to find a subset of TCs which optimizes one or more objective functions (i.e., selection criteria). In this light, we developed two new mechanisms for TC selection which consider two objectives simultaneously: maximize branch coverage while minimizing execution cost (time). These mechanisms were implemented using multi-objective techniques based on Particle Swarm Optimization (PSO). Additionally, we create hybrid multi-objective selection algorithms in order to improve the results. The experiments were performed on the space program from the SIR repository, attesting the feasibility of the proposed hybrid strategies.","","978-1-4799-5618","10.1109/BRACIS.2014.80","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984866","","Measurement;Linear programming;Search problems;Particle swarm optimization;Optimization;Testing;Wheels","particle swarm optimisation;program testing;search problems","SIR repository;space program;hybrid multiobjective selection algorithms;PSO;minimizing execution cost;branch coverage;optimization problem;TC selection;automatic test case;selection criterion;software testing process;test case selection;local search;hybrid binary multiobjective particle swarm optimization","","2","10","","","","","","IEEE","IEEE Conferences"
"General Optimization Strategies for Refining the In-Parameter-Order Algorithm","S. Gao; J. Lv; B. Du; Y. Jiang; S. Ma","NA; NA; NA; NA; NA","2014 14th International Conference on Quality Software","","2014","","","21","26","In-Parameter-Order (IPO) algorithm is an effective strategy of combinatorial testing. And several variants of the algorithm have been developed for reducing the runtime and size of test cases or for dealing with certain problems in test case generation, such as IPOG, IPOG-F and IPOG-F2. In this paper, the general optimization strategies, which can be applied to these variants of the algorithm, are proposed to make each value of all parameters more evenly distributed in the test cases. The proposed optimization strategies mainly focus on choosing values for the extension to an additional parameter during the horizontal growth of the algorithm and filling values for don't care positions. Experimental results show that the proposed optimization strategies are effective in reducing runtime and producing smaller size of test suites with the increase of the domain size.","1550-6002;2332-662X","978-1-4799-7198-5978-1-4799-7197","10.1109/QSIC.2014.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958383","Combinatorial testing;IPO;IPOG;IPOG-F;IPOGF2;don't care positions","Testing;Optimization;Greedy algorithms;Algorithm design and analysis;Software algorithms;Software;Electronic mail","optimisation;program testing","general optimization strategies;in-parameter-order algorithm;IPO algorithm;combinatorial testing;test case generation;IPOG-F2;IPOG;IPOG-F","","3","21","","","","","","IEEE","IEEE Conferences"
"Test Suite Reduction by Combinatorial-Based Coverage of Event Sequences","Q. Mayo; R. Michaels; R. Bryce","NA; NA; NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops","","2014","","","128","132","Combinatorial-based criteria are useful in several studies for test suite generation, prioritization, and minimization. In this paper, we extend previous work by using combinatorial-based criteria for test suite reduction. We use criteria that are based on combinatorial coverage of events and consider the order in which events occur. A simple combinatorial-based criterion covers t-way events and does not differentiate between the order of events. The event pair (e<sub>1</sub>, e<sub>2</sub>) is counted the same as if it occurs in the order (e<sub>2</sub>, e<sub>1</sub>). We also use two criteria that consider the order in which events occur since different orderings of events may be valuable during testing. First, the consecutive sequence-based criterion counts all event sequences in different orders, but they must occur adjacent to each other. The sequence-based criterion counts pairs in all orders without the requirement that events must be adjacent. We evaluate the new criteria on three GUI applications. We use 2way inter-window coverage in our studies. All of the 2way combinatorial-based criteria are effective in reducing the test suites and maintaining close to 100% fault finding effectiveness. Our future work examines larger test suites, higher strength coverage, techniques to partition event data, and further empirical studies.","","978-1-4799-5790","10.1109/ICSTW.2014.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825647","Test suite reduction;Combinatorial testing;GUI testing","Software engineering;Paints;Graphical user interfaces;Minimization;Conferences;Software testing","combinatorial mathematics;program testing","test suite reduction;combinatorial-based coverage;event sequences;t-way events;event pair;events ordering;sequence-based criterion;GUI applications;2way interwindow coverage;2way combinatorial-based criteria","","3","19","","","","","","IEEE","IEEE Conferences"
"State based testing using swarm intelligence","F. Mehboob; A. A. A. Jilani; M. Abbass","Computer Science Department, National University of Sciences And Technology/NUST/Islamabad, Pakistan; Computer Science Department, FAST/National University of Computer &amp; Engineering Sciences, Islamabad, Pakistan; Computer Science Department, National University of Sciences And Technology/NUST/Islamabad, Pakistan","2013 Science and Information Conference","","2013","","","630","635","Automatic data flow testing scrutinizes the flow of data within models by using data flow analysis rules. To certify accurate data flow within states we have to contemplate the data values. The investigation of data flow forms a foundation of data flow testing by bearing in mind defines and uses of the variables. Empirical studies have shown that existing state- based approaches are not competent in uncovering state based faults. State-based testing scrutinizes state changes and its behavior without concentrating on the internal details, thus data faults remained uncovered. It has been observed that many state based approaches don't offer complete definition-use path complete coverage and also are ineffective in terms of detection of data flow faults. Our work starts from these observations to view automatic data flow analysis problem to solve with heuristic technique. In this research, an approach is presented for automatic testing of data flow of UML state machine models and automatically generates test cases. Data flow testing problem is view as an optimization problem while selecting optimal number of feasible test cases in providing complete or maximum def-use paths coverage. An optimal solution is investigated by exploiting the heuristic search technique Ant Colony Optimization Algorithm. We implemented this approach in a tool named data flow testing tool. Effectiveness of our proposed approach is analyzed by applying it to UML state based models representing the dynamic system behavior. Experimentation is performed for the validation of this approach.","","978-0-9893193-0","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6661805","State-Based Testing;Ant Colony Optimization;Data Flow Testing;Coverage Criteria","Testing;Unified modeling language;Data models;Software;Analytical models;Ant colony optimization","ant colony optimisation;data flow analysis;finite state machines;program testing;search problems;swarm intelligence;Unified Modeling Language","state based testing;swarm intelligence;automatic data flow testing;data flow analysis rule;data flow fault;UML state machine;heuristic search technique;ant colony optimization algorithm;dynamic system behavior","","","40","","","","","","IEEE","IEEE Conferences"
"A unified approach for successive release of a software under two types of imperfect debugging","O. Singh; P. K. Kapur; A. K. Shrivastava; L. Das","Department of Operational Research University of Delhi, India; Center for Interdisciplinary Research, Amity University, Noida UP, India; Department of Operational Research University of Delhi, India; Department of Electronics and Communincations Engineering, Amity University, Greater Noida, UP, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","6","The competitive essence of market and the rapid turnover in the technology is shrinking the life of software. A software with bounded functionality cannot survive the high tides of contention. This raise the need for multiple up gradations of the software and consequently the customary approach towards software development process as observed in practice by most of the software firms today is iterative in nature. They are putting in lots of efforts to mark their presence in the market through periodic functional enrichment. But, functional enhancement adds to the existing complexity of the software and at the same time higher prospects of error. On the grounds of increasing complexity and partial insight into the software, the testing team may not prove to be competent enough to perfectly fix the faults by removing or correcting them after failure is detected. Hence, a fault might go unperceived by surviving the selected test cases performed by the testing unit resulting in a phenomenon termed as imperfect debugging. Another possible scenario is when an error get replaced by another one leading to error generation. In this paper we have developed a two stage fault detection and correction model in the presence of two types of imperfect debugging for multiple releases of a software. The proposed model has been validated on real data set for four releases.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014695","Unified Approach;Multi Release;Imperfect Debugging;Non-Homogeneous Poisson Process (NHPP);Software reliability Growth Model (SRGM)","Software;Software measurement","program debugging;program testing;software fault tolerance","multiple software release;imperfect debugging;software development process;software complexity;software testing unit;two stage fault detection model;two stage fault correction model","","3","17","","","","","","IEEE","IEEE Conferences"
"Predicting Cumulative Number of Failures in Software Using an ANN-PSO Based Approach","M. Bisi; N. K. Goyal","NA; NA","2015 International Conference on Computational Intelligence and Networks","","2015","","","9","14","Software project managers need information such as cumulative number of failures present in a software after testing a certain period of time to determine release time of software. In this paper, an artificial neural network (ANN) based model which uses a new network architecture is proposed to predict cumulative number of failures in software. An extra layer is added between input layer and hidden layer of ANN which uses logarithmic activation function to scale the inputs of ANN. An ANN-PSO based approach is developed in which Particle Swarm Optimization (PSO) method is used to train the ANN. The experiment is carried out using three data sets available in literature and results are compared with existing models found in literature. The results shown that the proposed method is able to produce better prediction than some existing models.","2375-5822","978-1-4799-7549-5978-1-4799-7548","10.1109/CINE.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7053795","Artificial neural network;Particle swarm optimization;Logarithmic scaling;Encoded input;Number of failure prediction","Data models;Artificial neural networks;Software;Predictive models;Training;Testing;Software reliability","learning (artificial intelligence);neural nets;particle swarm optimisation;program testing;project management;software management;software reliability","ANN-PSO based approach;software project managers;artificial neural network based model;network architecture;cumulative number prediction;software failure;input layer;hidden layer;logarithmic activation function;particle swarm optimization method;ANN training","","1","27","","","","","","IEEE","IEEE Conferences"
"STAR: Stack Trace Based Automatic Crash Reproduction via Symbolic Execution","N. Chen; S. Kim","Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","IEEE Transactions on Software Engineering","","2015","41","2","198","220","Software crash reproduction is the necessary first step for debugging. Unfortunately, crash reproduction is often labor intensive. To automate crash reproduction, many techniques have been proposed including record-replay and post-failure-process approaches. Record-replay approaches can reliably replay recorded crashes, but they incur substantial performance overhead to program executions. Alternatively, post-failure-process approaches analyse crashes only after they have occurred. Therefore they do not incur performance overhead. However, existing post-failure-process approaches still cannot reproduce many crashes in practice because of scalability issues and the object creation challenge. This paper proposes an automatic crash reproduction framework using collected crash stack traces. The proposed approach combines an efficient backward symbolic execution and a novel method sequence composition approach to generate unit test cases that can reproduce the original crashes without incurring additional runtime overhead. Our evaluation study shows that our approach successfully exploited 31 (59.6 percent) of 52 crashes in three open source projects. Among these exploitable crashes, 22 (42.3 percent) are useful reproductions of the original crashes that reveal the crash triggering bugs. A comparison study also demonstrates that our approach can effectively outperform existing crash reproduction approaches.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2363469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926857","Crash reproduction;static analysis;symbolic execution;test case generation;optimization;Crash reproduction;static analysis;symbolic execution;test case generation;optimization","Computer crashes;Arrays;Indexes;Color;Optimization;Explosions;Software","program debugging;program testing;project management;public domain software;system recovery","STAR;stack trace based automatic crash reproduction;software crash reproduction;debugging;record-replay approach;post-failure-process approach;scalability issues;object creation challenge;crash stack traces;backward symbolic execution;method sequence composition approach;unit test case generation;open source projects","","11","64","","","","","","IEEE","IEEE Journals & Magazines"
"SimLatte: A Framework to Support Testing for Worst-Case Interrupt Latencies in Embedded Software","T. Yu; W. Srisa-an; M. B. Cohen; G. Rothermel","NA; NA; NA; NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation","","2014","","","313","322","Embedded systems tend to be interrupt-driven, yet the presence of interrupts can affect system dependability because there can be delays in servicing interrupts. Such delays can occur when multiple interrupt service routines and interrupts of different priorities compete for resources on a given CPU. For this reason, researchers have sought approaches by which to estimate worst-case interrupt latencies (WCILs) for systems. Most existing approaches, however, are based on static analysis. In this paper, we present SIMLATTE, a testing-based approach for finding WCILs. SIMLATTE uses a genetic algorithm for test case generation that converges on a set of inputs and interrupt arrival points that are likely to expose WCILs. It also uses an opportunistic interrupt invocation approach to invoke interrupts at a variety of feasible locations. Our evaluation of SIMLATTE on several non-trivial embedded systems reveals that it is considerably more effective and efficient than random testing. We also determine that the combination of the genetic algorithm and opportunistic interrupt invocation allows SIMLATTE to perform better than it can when using either one in isolation.","2159-4848","978-1-4799-2255","10.1109/ICST.2014.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823893","Testing;Embedded Software;Interrupt Latencies;Genetic Algorithm","Biological cells;Testing;Delays;Genetic algorithms;Sociology;Statistics;Generators","embedded systems;genetic algorithms;program diagnostics","SimLatte;support testing;worst-case interrupt latencies;embedded software;system dependability;multiple interrupt service routines;CPU;WCIL;static analysis;genetic algorithm;test case generation;opportunistic interrupt invocation;nontrivial embedded systems;random testing","","4","36","","","","","","IEEE","IEEE Conferences"
"Particle swarm optimization techniques. Power systems applications","D. Cristian; C. Barbulescu; S. Kilyeni; V. Popescu","&#x201C;Politehnica&#x201D; University of Timisoara, Timisoara, Romania; &#x201C;Politehnica&#x201D; University of Timisoara, Timisoara, Romania; &#x201C;Politehnica&#x201D; University of Timisoara, Timisoara, Romania; &#x201C;Politehnica&#x201D; University of Timisoara, Timisoara, Romania","2013 6th International Conference on Human System Interactions (HSI)","","2013","","","312","319","Nowadays, there is a huge interest regarding the use of artificial intelligence techniques. From the associated methods, the current work is focusing on particle swarm optimization (PSO) study. The authors aim to present a synthesis regarding the PSO applications within the power system field. Two issues are addressed within the paper. Firstly, the PSO parameter tuning using mathematical test functions. Secondly, the conclusions are applied in case of optimal power flow (OPF) computing for small scale test power systems. For both issues the methodologies are presented, software tools have been developed. The research work is going to be continued having as a goal to develop a PSO based software designed for transmission network expansion (in case of complex power systems).","2158-2246;2158-2254","978-1-4673-5637-4978-1-4673-5635-0978-1-4673-5636","10.1109/HSI.2013.6577841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6577841","mathematical test functions;mathematical model;optimal power flow;particle swarm optimization;software","Aerospace electronics;Software algorithms;Power systems;Software tools;Standards;Artificial intelligence;Particle swarm optimization","load flow;parameter estimation;particle swarm optimisation;power engineering computing;swarm intelligence","PSO-based software;software tools;small scale test power systems;OPF;optimal power flow;mathematical test functions;PSO parameter tuning;PSO applications;artificial intelligence technique;power system applications;particle swarm optimization technique","","1","16","","","","","","IEEE","IEEE Conferences"
"Hybrid model to improve Bat algorithm performance","R. Gupta; N. Chaudhary; S. K. Pal","Department of Computer Science, Faculty of Mathematical Sciences, University of Delhi, INDIA; Department of Computer Science, Faculty of Mathematical Sciences, University of Delhi, INDIA; Scientific Analysis Group Lab, Defense Research &amp; Development Organization, Delhi, INDIA","2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","","2014","","","1967","1970","Bat Algorithm is one of the successful metaheuristic algorithms, which is used prominently for the purpose of optimization. But its inherent feature of non-changing parameters with the various iterations makes it less appropriate for optimization of software cost estimation techniques like COCOMO. So the current study proposes a hybrid model for the improvement of Bat algorithm by enhancing the search (global) and thus helping in optimizing the fitness function by generating new solutions. The data set used for testing is NASA 63 and the fitness function used for cost estimation is Mean Magnitude of Relative Error (MMRE). The simulations are done using MATLAB version R2010a. Results shows a better MMRE for the hybrid model as compared to the original Bat algorithm used for the optimization of COCOMO II for software cost estimation.","","978-1-4799-3080-7978-1-4799-3078","10.1109/ICACCI.2014.6968649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968649","Bat Algorithm;Genetic Algorithm;Optimization;COCOMO II;Software Cost Estimation","Genetic algorithms;Software;Estimation;Software algorithms;Mathematical model;Optimization;Algorithm design and analysis","optimisation;search problems;software cost estimation","hybrid model;bat algorithm performance;metaheuristic algorithms;optimization;nonchanging parameters feature;software cost estimation techniques;COCOMO;search global;fitness function;NASA 63;mean magnitude of relative error;MMRE;MATLAB version R2010a","","1","12","","","","","","IEEE","IEEE Conferences"
"Exploration of system availability during software-based self-testing in many-core systems under test latency constraints","M. A. Skitsas; C. A. Nicopoulos; M. K. Michael","KIOS Research Center, University of Cyprus, Nicosia, Cyprus; Department of ECE, University of Cyprus, Nicosia, Cyprus; KIOS Research Center, University of Cyprus, Nicosia, Cyprus","2014 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)","","2014","","","33","39","As technology scales, the increased vulnerability of modern systems due to unreliable components becomes a major problem in the era of multi-/many-core architectures. Recently, several on-line testing techniques have been proposed, aiming towards error detection of wear-out/aging-related defects that can appear during the lifetime of a system. In this work, we investigate the relation between system test latency and testtime overhead in multi-/many-core systems with shared LastLevel Cache (LLC) for periodic Software-Based Self-Testing (SBST), under different test scheduling policies. The investigated scheduling policies primarily vary the number of cores concurrently under test in the overall system testing session. Our extensive, workload-driven dynamic exploration reveals that there is an inverse relation between the two test measures; as the number of cores concurrently under test increases, system test latency decreases, but at the cost of significantly increased test time, which sacrifices system availability for running normal workloads. Under given system test latency constraints, which should be utilized in order to be able to control system recovery time in the event of an error detection, our exploration framework identifies the scheduling policy under which overall test time overhead is minimized and, hence, system availability is maximized. Without any loss of generality, a 16-core system is explored in a full-system, execution-driven simulation framework running multi-threaded PARSEC workloads [1].","1550-5774;2377-7966","978-1-4799-6155-9978-1-4799-6154","10.1109/DFT.2014.6962088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6962088","","Availability;Measurement;Scheduling;Optimization;Benchmark testing;Processor scheduling","automatic testing;cache storage;error detection;multiprocessing systems;program testing","system availability;software-based self-testing;SBST;many-core systems;test latency constraints;multicore architectures;error detection;wear-out-aging-related defects;testtime overhead;last level cache;LLC;test scheduling policy;workload-driven dynamic exploration;16-core system;multithreaded PARSEC workloads","","1","22","","","","","","IEEE","IEEE Conferences"
"An Industrial Case Study: PaRent (Parallel & Concurrent) Testing for Complex Mixed-Signal Devices","J. Dworak; P. Gui; Q. Khasawneh","NA; NA; NA","2015 IEEE 24th North Atlantic Test Workshop","","2015","","","33","38","Testing of every manufactured chip is an essential and crucial step in the semiconductor manufacturing process. It helps to ensure that customers get working chips that meet all the specifications. This is necessary to avoid the consequences and penalties that are incurred if a faulty chip is found by the customers. The current trend in the semiconductor industry is to attempt to increase the complexity of the chip while lowering its cost. This complicates the testing process because more testing is needed at a lower cost. Many solutions have been proposed, such as the use of cheaper testers and the use of embedded instruments for test and debug. Nevertheless, some tests and test requirements may not be amenable to these approaches. The multi-site test technique reduces the test cost by testing many units (i.e. physical chips) simultaneously. Concurrent testing is a related technique to reduce the test cost by testing many blocks within a single chip simultaneously. The PaRent approach, described in this paper, is a parallel (multi-site) and concurrent testing approach in which both techniques are used at the same time. This paper presents an industrial case study and describes the Design-for-Testability (DFT) requirements for successful PaRent testing of a high-volume mixed-signal System-on-Chip (SoC). Specifically, it describes the testing obstacles faced while testing a Power-Management Integrated Circuit (PMIC) that was not designed with concurrent testing in mind. Hardware and software optimizations for Automatic Test Equipment (ATE) to enhance the capabilities of PaRent testing are also described.","","978-1-4673-7417-0978-1-4673-7416","10.1109/NATW.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7147651","Parallel Testing;Concurrent Testing;Mixed-Signal Testing;Scheduling","Testing;Software;Instruments;Discrete Fourier transforms;Complexity theory;Hardware;Switched-mode power supply","cost reduction;integrated circuit testing;mixed analogue-digital integrated circuits;power integrated circuits;semiconductor industry;system-on-chip","PaRent testing process;parallel and concurrent testing;complex mixed-signal device;ATE;automatic test equipment;PMIC;power-management integrated circuit;SoC;system-on-chip;DFT;design-for-testability;test cost reduction;multisite test technique;chip complexity;semiconductor industry","","","21","","","","","","IEEE","IEEE Conferences"
"Validating Second-Order Mutation at System Level","P. Reales Mateo; M. Polo Usaola; J. L. Fernández Alemán","University of Castilla-La Mancha, Ciudad Real; University of Castilla-La Mancha, Ciudad Real; University of Murcia, Murcia","IEEE Transactions on Software Engineering","","2013","39","4","570","587","Mutation has been recognized to be an effective software testing technique. It is based on the insertion of artificial faults in the system under test (SUT) by means of a set of mutation operators. Different operators can mutate each program statement in several ways, which may produce a huge number of mutants. This leads to very high costs for test case execution and result analysis. Several works have approached techniques for cost reduction in mutation testing, like n-order mutation where each mutant contains n artificial faults instead of one. There are two approaches to n-order mutation: increasing the effectiveness of mutation by searching for good n-order mutants, and decreasing the costs of mutation testing by reducing the mutants set through the combination of the first-order mutants into n-order mutants. This paper is focused on the second approach. However, this second use entails a risk: the possibility of leaving undiscovered faults in the SUT, which may distort the perception of the test suite quality. This paper describes an empirical study of different combination strategies to compose second-order mutants at system level as well as a cost-risk analysis of n-order mutation at system level.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2012.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6216382","Empirical evaluation;high-order mutation;mutation testing","Algorithm design and analysis;Concrete;Educational institutions;Benchmark testing;Optimization;Software testing","program testing;software fault tolerance","second order mutation;system level;software testing technique;system under test;SUT;mutation operators;test case execution;cost risk analysis","","7","53","","","","","","IEEE","IEEE Journals & Magazines"
"Studying the influence of standard compiler optimizations on symbolic execution","S. Dong; O. Olivo; L. Zhang; S. Khurshid","Department of Electrical and Computer Engineering, University of Texas at Austin, 78712, USA; Department of Computer Science, University of Texas at Austin, 78712, USA; Department of Computer Science, University of Texas at Dallas, 75080, USA; Department of Electrical and Computer Engineering, University of Texas at Austin, 78712, USA","2015 IEEE 26th International Symposium on Software Reliability Engineering (ISSRE)","","2015","","","205","215","Systematic testing plays a vital role in increasing software reliability. A particularly effective and popular approach for systematic testing is symbolic execution, which analyzes a large number of program behaviors using symbolic inputs. Even though symbolic execution is among the most studied analyses during the last decade, scaling it to real-world applications remains a key challenge. This paper studies how a class of semantics-preserving program transformations, namely compiler optimizations, which are designed to enhance performance of standard program execution (using concrete inputs), influence traditional symbolic execution. As an enabling technology, the study uses KLEE, a well-known symbolic execution engine based on the LLVM compiler infrastructure, and focuses on 33 optimization flags of LLVM. Our specific research questions include: (1) how different optimizations influence the performance of symbolic execution for Unix Coreutils, (2) how the influence varies across two different program classes, and (3) how the influence varies across three different back-end constraint solvers. Some of our findings surprised us. For example, applying the 33 optimizations in a pre-defined order provides a slowdown (compared to applying no optimization) for a majority of the Coreutils when using the basic depth-first search with no constraint caching. The key finding of our work is that standard compiler optimizations need to be used with care when performing symbolic execution for creating tests that provide high code coverage. We hope our study motivates future research on harnessing the power of symbolic execution more effectively for enhancing software reliability, e.g., by designing program transformations specifically to scale symbolic execution or by studying broader classes of traditional compiler optimizations in the context of different search heuristics, memoization, and other strategies employed by modern symbolic execution tools.","","978-1-5090-0406-5978-1-5090-0405","10.1109/ISSRE.2015.7381814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7381814","","Optimization;Program processors;Software reliability;Standards;Context;Benchmark testing","optimising compilers;program testing;software reliability;tree searching;Unix","standard compiler optimizations;systematic testing;software reliability;symbolic inputs;semantics-preserving program transformations;KLEE;symbolic execution engine;LLVM compiler infrastructure;Unix Coreutils;back-end constraint solvers;depth-first search","","5","61","","","","","","IEEE","IEEE Conferences"
"SAT-Based Bounded Software Model Checking for Embedded Software: A Case Study","Y. Kim; M. Kim","NA; NA","2014 21st Asia-Pacific Software Engineering Conference","","2014","1","","55","62","Conventional manual testing often misses corner case bugs in complex embedded software, which can incur large economic loss. To overcome the weakness of manual testing, automated program analysis/testing techniques such as software model checking and concolic testing have been proposed. This paper makes a detailed report on the application of a SAT-based bounded software model checking technique using CBMC to busy box ls which is loaded on a large number of embedded devices such as smart phones and network equipments. In this study, CBMC demonstrated its effectiveness by detecting four bugs of busy box ls, but also showed limitations for the loop analysis. In addition, we report the importance of calculating minimum iterations to exit a loop (MIEL) to prevent false negatives in practice.","1530-1362;1530-1362","978-1-4799-7426-9978-1-4799-7425","10.1109/APSEC.2014.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091291","Embedded software;Software model checking;Case study","Model checking;Software;Computer bugs;Upper bound;Manuals;Optimized production technology","embedded systems;formal verification","SAT-based bounded software model checking;complex embedded software;manual testing;automated program analysis;concolic testing;CBMC;busy box;embedded devices;smart phones;network equipments;loop analysis;false negatives","","1","31","","","","","","IEEE","IEEE Conferences"
"Towards a model for optimizing technical debt in software products","N. Ramasubbu; C. F. Kemerer","Joseph M. Katz Graduate School of Business University of Pittsburgh Pittsburgh, PA, USA; Joseph M. Katz Graduate School of Business University of Pittsburgh Pittsburgh, PA, USA","2013 4th International Workshop on Managing Technical Debt (MTD)","","2013","","","51","54","There is a growing interest in applying the technical debt metaphor to investigate issues related to the tradeoff of the likely long-term costs associated with software design shortcuts for expected short-term business benefits in terms of increased earlier functionality. We propose an optimization model that contrasts the patterns of technical debt accumulation in a software product with the patterns of consumer adoption of the product throughout its evolution. This facilitates a rigorous and balanced analysis of the pros and cons of accumulating technical debt at various lifecycle stages of a software product. We discuss the use of the optimization model to derive policies for managing technical debt and the potential for empirical tests of the model and other future interdisciplinary research.","","978-1-4673-6443","10.1109/MTD.2013.6608679","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608679","technical debt;software platforms;customer satisfaction;customization;software quality;customer adoption","Software;Conferences;Business;Trajectory;Optimization;Context;Product development","optimisation;software engineering","software products;technical debt metaphor;software design shortcuts;expected short-term business benefits;optimization model;technical debt accumulation","","3","35","","","","","","IEEE","IEEE Conferences"
"Efficient probabilistic testing of model transformations using search","L. M. Rose; S. Poulding","Department of Computer Science, University of York, Deramore Lane, Heslington, YO10 5GH, UK; Department of Computer Science, University of York, Deramore Lane, Heslington, YO10 5GH, UK","2013 1st International Workshop on Combining Modelling and Search-Based Software Engineering (CMSBSE)","","2013","","","16","21","Checking the output of a test case for correctness-applying a test oracle-is challenging for many types of software, including model transformations. Decreasing the number of test cases that are executed during testing will therefore reduce the costs involved in testing a model transformation. However, there is a trade-off: to be confident that the transformation fulfils its specification requires the execution of sufficient test cases to fully exercise the transformation. In this paper, we demonstrate a process that derives a method of sampling random models which enables the test engineer to balance testing cost and testing efficacy. The output of the process is an optimised probability distribution over the models on which the transformation acts; test sets that efficiently exercise the transformation may then be derived by sampling models from the optimised distribution. Furthermore, we describe benefits and challenges of combining model-driven engineering and search-based software engineering tools and techniques, which include conflating metamodels with grammars to enable grammar-based search techniques over a set of models, and the need to increase the scalability of model-driven engineering tools to make them more amenable to search.","","978-1-4673-6284","10.1109/CMSBSE.2013.6604431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604431","","Testing;Software;Grammar;Probability distribution;Software algorithms;Statistical analysis;Manuals","grammars;program testing;sampling methods;search problems;software cost estimation;statistical distributions","model transformations;probabilistic testing;scalability;grammar-based search techniques;metamodels;search-based software engineering;model-driven engineering;optimised probability distribution;testing efficacy;testing cost;sampling random models","","2","15","","","","","","IEEE","IEEE Conferences"
"Root system growth for global optimization","H. Chen; X. Liang; W. Yuan; L. Sun; M. He; N. Ji","School of Computer Science and Software, Tianjin Polytechnic University, 300387, China; School of Computer Science and Software, Tianjin Polytechnic University, 300387, China; School of Computer Science and Software, Tianjin Polytechnic University, 300387, China; School of Computer Science and Software, Tianjin Polytechnic University, 300387, China; School of Computer Science and Software, Tianjin Polytechnic University, 300387, China; School of Computer Science and Software, Tianjin Polytechnic University, 300387, China","2015 IEEE International Conference on Information and Automation","","2015","","","2098","2103","This paper proposed a novel bio-inspired optimizer, namely the root system growth algorithm (RSGA), which adopts the root foraging, memory and communication, and auxin-regulated mechanisms of the root system. When tested against benchmark functions, the RSGA markedly outperforms the CMA-ES, PSO, GA, and DE algorithms in terms of accuracy, robustness and convergence speed.","","978-1-4673-9104-7978-1-4673-9103","10.1109/ICInfA.2015.7279634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279634","Plant Root Growth;Foraging;Global Optimization;Swarm Intelligence;RSGA","Benchmark testing;Optimization;Genetic algorithms;Algorithm design and analysis;Standards;Convergence;Particle swarm optimization","optimisation","global optimization;bioinspired optimizer;root system growth algorithm;RSGA;root foraging;auxin-regulated mechanisms","","","11","","","","","","IEEE","IEEE Conferences"
"Automated memory leak diagnosis by regression testing","M. Ghanavati; A. Andrzejak","Institute of Computer Science, Heidelberg University, Germany; Institute of Computer Science, Heidelberg University, Germany","2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation (SCAM)","","2015","","","191","200","Memory leaks are tedious to detect and require significant debugging effort to be reproduced and localized. In particular, many of such bugs escape classical testing processes used in software development. One of the reasons is that unit and integration tests run too short for leaks to manifest via memory bloat or degraded performance. Moreover, many of such defects are environment-sensitive and not triggered by a test suite. Consequently, leaks are frequently discovered in the production scenario, causing elevated costs. In this paper we propose an approach for automated diagnosis of memory leaks during the development phase. Our technique is based on regression testing and exploits existing test suites. The key idea is to compare object (de-)allocation statistics (collected during unit/integration test executions) between a previous and the current software version. By grouping these statistics according to object creation sites we can detect anomalies and pinpoint the potential root causes of memory leaks. Such diagnosis can be completed before a visible memory bloat occurs, and in time proportional to the execution of test suite. We evaluate our approach using real leaks found in 7 Java applications. Results show that our approach has sufficient detection accuracy and is effective in isolating the leaky allocation site: true defect locations rank relatively high in the lists of suspicious code locations if the tests trigger the leak pattern. Our prototypical system imposes an acceptable instrumentation and execution overhead for practical memory leak detection even in large software projects.","","978-1-4673-7529-0978-1-4673-7528","10.1109/SCAM.2015.7335415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7335415","Automated debugging;memory leak;regression testing;software tests;version comparison","Resource management;Software;Testing;Java;Instruments;Leak detection;Debugging","Java;program debugging;software engineering;statistical testing;storage management","automated memory leak diagnosis;regression testing;software development;memory bloat;Java application;practical memory leak detection;software project;suspicious code locations","","2","40","","","","","","IEEE","IEEE Conferences"
"A comparison study of binary multi-objective Particle Swarm Optimization approaches for test case selection","L. S. de Souza; R. B. C. Prudêncio; F. d. A. Barros","Department of Informatics of Federal Institute of Education Science and Technology of North of Minas Gerais (IFNMG), Pirapora, Minas Gerais, Brazil; Center of Informatics (CIn), Federal University of Pernam-buco (UFPE), Recife, Pernambuco, Brazil; Center of Informatics (CIn), Federal University of Pernam-buco (UFPE), Recife, Pernambuco, Brazil","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","2164","2171","During the software testing process many test suites can be generated in order to evaluate and assure the quality of the products. In some cases the execution of all suites cannot fit the available resources (time, people, etc). Hence, automatic Test Case (TC) selection could be used to reduce the suites based on some selection criterion. This process can be treated as an optimization problem, aiming to find a subset of TCs which optimizes one or more objective functions (i.e., selection criteria). The majority of search-based works focus on single-objective selection. In this light, we developed mechanisms for functional TC selection which considers two objectives simultaneously: maximize requirements coverage while minimizing cost in terms of TC execution effort. These mechanisms were implemented by deploying multi-objective techniques based on Particle Swarm Optimization (PSO). Due to the drawbacks of original binary version of PSO we implemented five binary PSO algorithms and combined them with a multi-objective versions of PSO in order to create new optimization strategies applied to TC selection. The experiments were performed on two real test suites, revealing the feasibility of the proposed strategies and the differences among them.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900522","","Particle swarm optimization;Linear programming;Vectors;Optimization;Equations;Testing;Mathematical model","particle swarm optimisation;program testing","binary multiobjective particle swarm optimization;test case selection;software testing process;automatic test case;objective functions;single objective selection;multiobjective techniques;PSO","","3","23","","","","","","IEEE","IEEE Conferences"
"Sensitivity analysis of Parallel Cell Coordinate System in Many-objective Particle Swarm Optimization","W. Hu; G. G. Yen; X. Zhang","School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan 610054, China; School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK 74075 USA; School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan 610054, China","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","2641","2648","Parallel Cell Coordinate System (PCCS) was proposed to evaluate the individual fitness in an archive and access the population progress in the evolutionary environment. In a Many-objective Optimization Problem (MaOP), it is much harder to tradeoff the convergence and diversity than in a Multiobjective Optimization Problem. To more effectively tackle the MaOPs, the PCCS and the aggregation-based approach are integrated into a Many-objective Optimization Particle Swarm Optimization (MaOPSO). In this paper, the sensitivity of PCCS is examined with respect to the number of objectives and the maximum size of an archive. The experimental results indicate that the MaOPSO performs better than MOEA/D in terms of IGD and HV metrics on the WFG test suit, and PCCS is not sensitive to the number of objectives and the maximum size of an archive.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900440","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900440","particle swarm optimization;many-objective optimization problem (MaOP);many-objective optimization particle swarm optimization (MaOPSO);parallel cell coordinate system (PCCS)","Optimization;Convergence;Sociology;Statistics;Particle swarm optimization;Sensitivity","particle swarm optimisation;sensitivity analysis","sensitivity analysis;parallel cell coordinate system;many-objective particle swarm optimization;PCCS;many-objective optimization problem;MaOP;aggregation-based approach;MOEA/D algorithm;IGD metric;HV metric;WFG test suite","","1","31","","","","","","IEEE","IEEE Conferences"
"A fast heuristic particle swarm optimization algorithm for circles packing problem with the equilibrium constraints","Kaiyou Lei","Intelligent Software and Software Engineering Laboratory, Southwest University, Chongqing, 400715, China","Proceeding of the 11th World Congress on Intelligent Control and Automation","","2014","","","3573","3576","The packing problem with the behavioral constraints is difficult to solve due to its NP-hard nature. Particle swarm optimization (PSO) is quick in convergence, but likely to be premature at the initial stage. Considering its characteristics, a fast heuristic PSO algorithm for this problem is proposed, which employ the heuristic method to get the initial approximate position of global optimum by randomly arranging round existing circles in peripheral with counter-clockwise movement, and the refined search of improved PSO as a whole to plan large-scale space global search according to the fitness change, and to quicken convergence speed, avoid premature problem, economize computational expenses, and obtain global optimum. The proposed algorithm is tested and compared it with other published methods on constrained layout examples, demonstrated that the algorithm is superior to the exiting algorithms in performance.","","978-1-4799-5825","10.1109/WCICA.2014.7053310","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7053310","heuristic method;particle swarm optimization;behavioral constraints;premature;constrained layout","Layout;Particle swarm optimization;Search problems;Optimization;Convergence;Heuristic algorithms;Software algorithms","bin packing;computational complexity;convergence;particle swarm optimisation;search problems","fast heuristic particle swarm optimization algorithm;circles packing problem;equilibrium constraints;behavioral constraints;PSO;initial approximate position;global optimum;counter-clockwise movement;refined search;large-scale space global search;fitness change;convergence speed;computational expenses;published methods;constrained layout examples","","","12","","","","","","IEEE","IEEE Conferences"
"The first Tunisian fuel cell test station","M. Barbouche; F. Krout; M. Chiha; K. Charradi; A. Zakarya; R. Chtourou","Photovoltaic Laboratory, Centre for Research and Energy Technologies CRTEn, BP 95, Hammam Lif 2050, Tunisia; Photovoltaic Laboratory, Centre for Research and Energy Technologies CRTEn, BP 95, Hammam Lif 2050, Tunisia; Photovoltaic Laboratory, Centre for Research and Energy Technologies CRTEn, BP 95, Hammam Lif 2050, Tunisia; Photovoltaic Laboratory, Centre for Research and Energy Technologies CRTEn, BP 95, Hammam Lif 2050, Tunisia; Photovoltaic Laboratory, Centre for Research and Energy Technologies CRTEn, BP 95, Hammam Lif 2050, Tunisia; Photovoltaic Laboratory, Centre for Research and Energy Technologies CRTEn, BP 95, Hammam Lif 2050, Tunisia","2013 International Conference on Electrical Engineering and Software Applications","","2013","","","1","5","Great efforts are presently being undertaken to develop fuel cell applications. In this study, a design and realization of a fuel cell test station is described. The control of the stack temperature and the optimization of some parameters, such as the acquisition and equilibrium time for each value of the current density are necessary steps for an objective and trustworthy comparison of the performance data. Our test bench comprises two major parts hardware and software. The hardware part comprises an electrical cabinet, a power supply, an acquisition chain, an electronic load, mass flow controllers, temperature controller, solenoid valves, and solid state relay. The assemblies of the different parts of the test station are made considering our project objectives, such as, security, size (Length: 1000mm, Width: 800mm, Height: 1900mm), accessibility and mobility...etc. The software part includes not only the development of the pipelines and instrumentations diagrams but also the design of the rack using solid-works software. An interface was developed in the LABVIEW environment to enable mass flow controller and the solenoid valves control. It also allows the automatic data acquisition (fuel cell power, temperature and pressure). Preliminary measurements are made with a PEMFC (25 cm2) to make out the effect of temperature, relative humidity, back pressure and in the end the effect of cell number on the fuel cells power. The results may be used to find the best operating conditions.","","978-1-4673-6301-3978-1-4673-6302-0978-1-4673-6300","10.1109/ICEESA.2013.6578369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578369","Test station;Labview;Fuel cell;Virtual Instrumentation;polarization curve","Fuel cells;Temperature measurement;Hydrogen;Valves;Instruments;Current density","current density;data acquisition;flow control;fuel cell power plants;proton exchange membrane fuel cells;solenoids;temperature control;test facilities;virtual instrumentation","relative humidity;PEMFC;fuel cell power;data acquisition;LABVIEW environment;solid-works software;instrumentations diagrams;pipelines;solid state relay;solenoid valves;temperature controller;mass flow controllers;electronic load;acquisition chain;power supply;electrical cabinet;current density;optimization;Tunisian fuel cell test station;size 1000 mm;size 800 mm;size 1900 mm","","","5","","","","","","IEEE","IEEE Conferences"
"Optimizing Quality Assurance Strategies through an Integrated Quality Assurance Approach -- Guiding Quality Assurance with Assumptions and Selection Rules","F. Elberzhager; T. Bauer","NA; NA","2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications","","2014","","","402","405","Quality assurance activities are often still expensive or do not offer the expected quality. A recent trend aimed at overcoming this problem is tighter integration of several quality assurance techniques such as analysis and testing in order to exploit synergy effects and thus reduce costs or improve the coverage of quality assurance activities. However, one main challenge in exploiting such benefits is that knowledge about the relationships between many different factors is needed, such as the quality assurance techniques considered, the number of defects, the remaining defect-proneness, or product and budget data. Such knowledge is often not available. Based on a combined analysis and testing methodology called In QA, we developed an iterative rule-based procedure that considers several factors in order to gather knowledge and allows deriving different strategies to guide the quality assurance activities. We derived several specific and reasonable strategies to demonstrate the approach.","1089-6503;2376-9505","978-1-4799-5795","10.1109/SEAA.2014.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928844","analysis;testing;integration;tool prototype;rules;assumptions;prediction;guidance","Quality assurance;Testing;Inspection;Context;Calibration;Concrete","knowledge based systems;program testing;quality assurance;software quality","quality assurance activities;quality assurance techniques;product data;budget data;testing methodology;InQA;iterative rule-based procedure","","","6","","","","","","IEEE","IEEE Conferences"
"Modern Software Tools for New Computing Architectures Development","A. Y. Drozdov; S. V. Novikov; V. E. Vladislavlev; Y. N. Fonin","NA; NA; NA; NA","2014 International Conference on Engineering and Telecommunication","","2014","","","72","76","Microprocessor architecture development is a long and complicated process. The use of modern advanced software tools helps to accelerate the development process and increases the efficiency of the produced processor. This paper presents basic software tools and their quality improvements. Tool chain development is performed in parallel with the development of the architecture. There are open source solutions, such as the GNU Compiler Collection or LLVM, enabling the rapid prototyping of a compiler for the new architecture. But for the effective use of the architectural features more complex tools like Universal Translating Library are used. This library contains advanced algorithms for analysis and optimization, as well as a mechanism for their integration with any technological chain like GNU or LLVM. Before development of microprocessor design is completed functional simulator is used for testing and debugging of other software components like operating system or applications. Simulator plays the role of the reference model of the microprocessor. The simulator execution time is critical for development so most heavy parts should be optimized. In this paper it is proposed a flag computation method which is two times faster than the popular method of table based flags computation. In the segment of embedded operating systems compact real-time OS are widely used. These OS are linked with the user applications. They do not support multitasking, but support multithreading. One example of such OS is a specialized real-time operating system Milandr OS. Milandr OS was designed to control the operation of integrated modules. Language PPDL is designed for rapid prototyping and verification of new architectures, as well as creation of system software. Its basic principle: single description - several components. From a unified description of core microprocessor or coprocessor its Verilog-description and a number of software components can be generated: QEMU based simulator, assembler/disassembler, the interface to connect to the debugger, and a set of tests. The most important stage in the development of the microprocessor is verification process. Automatic test generators are used to solve this problem. Automatically generated tests allow debugging of functional simulator and then debugging of RTL-architecture description. Integration of verification system and PPDL results in significant reduction of routine verification work. One approach to launch of a new microprocessor on the market is a binary compatibility with existing solutions by using dynamic translation. Besides compatibility dynamic translation solves the issues of improving performance by optimizing JIT-compiler. Elbrus microprocessor is an example of x86 compatible system.","","978-1-4799-7012-4978-1-4799-7011","10.1109/EnT.2014.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7121437","microprocessor;toolchain;compiler;simulator;operating system;verification;binary converter;JIT","Computer architecture;Microprocessors;Registers;Operating systems;Hardware;Libraries","computer architecture;coprocessors;embedded systems;hardware description languages;operating systems (computers);program assemblers;program compilers;program debugging;program testing;program verification;public domain software;software libraries;software prototyping;software quality;software tools","QEMU based simulator;assembler/disassembler;verification process;automatic test generator;functional simulator;RTL-architecture description;verification system;compatibility dynamic translation;JIT-compiler;Elbrus microprocessor;x86 compatible system;Verilog-description;coprocessor;core microprocessor;system software;language PPDL;integrated module;real-time operating system Milandr OS;multithreading;embedded operating systems compact real-time OS;flags computation;software component;software debugging;software testing;microprocessor design;universal translating library;rapid prototyping;LLVM;GNU compiler collection;open source solution;tool chain development;quality improvement;microprocessor architecture development;computing architectures development;software tool","","","14","","","","","","IEEE","IEEE Conferences"
"An Efficient Algorithm for Constraint Handling in Combinatorial Test Generation","L. Yu; Y. Lei; M. Nourozborazjany; R. N. Kacker; D. R. Kuhn","NA; NA; NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation","","2013","","","242","251","Combinatorial testing has been shown to be a very effective testing strategy. An important problem in combinatorial testing is dealing with constraints, i.e., restrictions that must be satisfied in order for a test to be valid. In this paper, we present an efficient algorithm, called IPOG-C, for constraint handling in combinatorial testing. Algorithm IPOG-C modifies an existing combinatorial test generation algorithm called IPOG to support constraints. The major contribution of algorithm IPOG-C is that it includes three optimizations to improve the performance of constraint handling. These optimizations can be generalized to other combinatorial test generation algorithms. We implemented algorithm IPOG-C in a combinatorial test generation tool called ACTS. We report experimental results that demonstrate the effectiveness of algorithm IPOG-C. The three optimizations increased the performance by one or two orders of magnitude for most subject systems in our experiments. Furthermore, a comparison of ACTS to three other tools suggests that ACTS can perform significantly better for systems with more complex constraints.","2159-4848","978-0-7695-4968-2978-1-4673-5961","10.1109/ICST.2013.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569736","Combinatorial Testing;Constraint Handling;Test Genration","Optimization;Algorithm design and analysis;Testing;History;NIST;Databases;Browsers","combinatorial mathematics;constraint handling;optimisation","ACTS;optimizations;IPOG-C;combinatorial test generation;constraint handling","","26","20","","","","","","IEEE","IEEE Conferences"
"Extracting the Combinatorial Test Parameters and Values from UML Sequence Diagrams","P. Satish; A. Paul; K. Rangarajan","NA; NA; NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops","","2014","","","88","97","In the current practice, the Combinatorial Test Design Model (CTDM) is designed by the test designers manually leveraging their experience in testing. Their involvement, perception, domain knowledge and testing proficiency are needed to analyze the requirements document and design the test model. Till date we know of no automated method that has eased the process of deriving the Combinatorial Test Design Model. Requirements document and analysis artifacts like UML activity diagrams and sequence diagrams hold information on parameters, values and constraints of the underlying CTDM. Our research focus is to develop a tool that assists test designers in coming up with the CTDM. This paper presents an approach to extract CTDM related information such as parameters and values from sequence diagrams. Our key contribution in this paper includes proposing a rule-based method for identifying the model elements from the sequence diagrams with the supporting rules and extraction algorithms. The rules have been applied onto individual sequence diagrams and results qualitatively discussed based on the general understanding of the requirements.","","978-1-4799-5790","10.1109/ICSTW.2014.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825643","combinatorial testing;sequence diagram;test design model","Unified modeling language;Algorithm design and analysis;Testing;Analytical models;Optimized production technology;Computed tomography;Computational modeling","program testing;Unified Modeling Language","combinatorial test parameter extraction;UML sequence diagrams;combinatorial test design model;CTDM;involvement;perception;domain knowledge;testing proficiency;UML activity diagrams;rule-based method","","3","32","","","","","","IEEE","IEEE Conferences"
"Refining a Randomized Post-optimization Method for Covering Arrays","X. Li; Z. Dong; H. Wu; C. Nie; K. Cai","NA; NA; NA; NA; NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops","","2014","","","143","152","Combinatorial testing is an efficient technique that reveals the faults caused by parameters interaction in software systems. It uses covering arrays as test suites to avoid the combinatorial explosion of parameter values. However, the construction of covering arrays with minimum size is still a challenging problem. This is because most of the existing methods to construct covering arrays result in extensive repetition of coverage. To address this challenging problem, some researchers proposed a randomized post-optimization method to reduce the number of rows in a covering array that has been already constructed. It repeatedly adjusts the array without loss of coverage. However, it seems that this method is not efficient as desired. In order to improve its efficiency, we propose a refined algorithm to replace the randomized algorithm adopted in the method. This avoids the local optima problem that arises in randomized algorithm. Experimental results are presented to confirm the benefits of the refined algorithm. In the cases of CA(N; 6, k, 2) (38 ≤ k ≤ 50) generated by IPO, this refined algorithm not only reduces more number of rows than the randomized algorithm, but also produces covering arrays that are smaller than previously best known ones. Furthermore, we provide a reasonable estimator to predict the extent to which covering arrays can be improved by this refined algorithm.","","978-1-4799-5790","10.1109/ICSTW.2014.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825649","","Prediction algorithms;Testing;Upper bound;Optimization;Conferences;Software","combinatorial mathematics;optimisation;program testing","randomized post optimization method;covering arrays;combinatorial testing;software systems;combinatorial explosion;parameter values;randomized algorithm","","","28","","","","","","IEEE","IEEE Conferences"
"Applying Multi-core Model Checking to Hardware-Software Partitioning in Embedded Systems","A. Trindade; H. Ismail; L. Cordeiro","NA; NA; NA","2015 Brazilian Symposium on Computing Systems Engineering (SBESC)","","2015","","","102","105","We present an alternative approach to solve the hardware and software partitioning problem, which uses Bounded Model Checking (BMC) based on Satisfiability Modulo Theories (SMT) in conjunction with a multi-core support using Open Multi-Processing. The multi-core approach allows initializing many verification instances based on processors cores numbers available to the model checker. Each instance checks for a different optimum value until the optimization problem is satisfied. The goal is to show that multi-core model-checking techniques can be effective, in particular cases, to find the optimal solution of the hardware-software partitioning problem. We compare the experimental results of our proposed approach with conventional algorithms.","2324-7894","978-1-5090-0182","10.1109/SBESC.2015.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423221","hardware-software co-design;hardware-software partitioning;optimization;model checking;multi-core;OpenMP","Hardware;Software;Optimization;Genetic algorithms;Benchmark testing;Model checking;Multicore processing","computability;embedded systems;formal verification;hardware-software codesign;multiprocessing systems;optimisation","multicore model checking techniques;embedded systems;bounded model checking;BMC;satisfiability modulo theories;SMT;open multiprocessing;processor cores;optimization problem;hardware-software partitioning problem;hardware-software co-design","","3","20","","","","","","IEEE","IEEE Conferences"
"Evaluation of diverse compiling for software-fault detection","A. Höller; N. Kajtazovic; T. Rauter; K. Römer; C. Kreiner","Institute for Technical Informatics, Graz University of Technology; Institute for Technical Informatics, Graz University of Technology; Institute for Technical Informatics, Graz University of Technology; Institute for Technical Informatics, Graz University of Technology; Institute for Technical Informatics, Graz University of Technology","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","531","536","Although software fault prevention techniques improve continually, faults remain in every complex software system. Thus safety-critical embedded systems need mechanisms to tolerate software faults. Typically, these systems use static redundancy to detect hardware faults during operation. However, the reliability of a redundant system not only depends on the reliability of each version, but also on the dissimilarity between them. Thus, researchers have investigated ways to automatically add cost-efficient diversity to software to increase the efficiency of redundancy strategies. One of these automated software diversification methods is diverse compiling, which exploits the diversity introduced by different compilers and different optimization flags. Today, diverse compiling is used to improve the hardware fault tolerance and to avoid common defects from compilers. However, in this paper we show that diverse compiling also enhances the software fault tolerance by increasing the chance of finding defects in the source code of the executed software during runtime. More precisely, the memory is organized differently, when using different compilers and compiler flags. This enhances the chance of detecting memory-related software bugs, such as missing memory initialization, during runtime. Here we experimentally quantify the efficiency of diverse compiling for software fault tolerance and we show that diverse compiling can help to detect up to about 70% of memory-related software bugs.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092445","","Software;Fault tolerance;Fault tolerant systems;Hardware;Optimization;Computer bugs;Benchmark testing","program compilers;program debugging;software fault tolerance;software reliability;source code (software)","diverse compiling evaluation;software-fault detection;complex software system;safety-critical embedded systems;software fault prevention techniques;static redundancy;redundant system reliability;cost-efficient diversity;automated software diversification methods;hardware fault tolerance;source code;compiler flags;memory-related software bugs;missing memory initialization","","2","25","","","","","","IEEE","IEEE Conferences"
"Guided Mutation Testing for JavaScript Web Applications","S. Mirshokraie; A. Mesbah; K. Pattabiraman","Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC, Canada; Department of Electrical and Computer Engineering, University of British Columbia, 2332 Main Mall, Vancouver, BC, Canada","IEEE Transactions on Software Engineering","","2015","41","5","429","444","Mutation testing is an effective test adequacy assessment technique. However, there is a high computational cost in executing the test suite against a potentially large pool of generated mutants. Moreover, there is much effort involved in filtering out equivalent mutants. Prior work has mainly focused on detecting equivalent mutants after the mutation generation phase, which is computationally expensive and has limited efficiency. We propose an algorithm to select variables and branches for mutation as well as a metric, called FunctionRank, to rank functions according to their relative importance from the application's behaviour point of view. We present a technique that leverages static and dynamic analysis to guide the mutation generation process towards parts of the code that are more likely to influence the program's output. Further, we focus on the JavaScript language, and propose a set of mutation operators that are specific to Web applications. We implement our approach in a tool called MUTANDIS. The results of our empirical evaluation show that (1) more than 93 percent of generated mutants are non-equivalent, and (2) more than 75 percent of the surviving non-equivalent mutants are in the top 30 percent of the ranked functions.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2371458","NSERC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960094","mutation testing;JavaScript;equivalent mutants;guided mutation generation;web applications;Mutation testing;JavaScript;equivalent mutants;guided mutation generation;web applications","Testing;Measurement;Heuristic algorithms;Complexity theory;Performance analysis;Instruments;IEEE Computer Society","Java;program diagnostics;program testing","guided mutation testing;JavaScript Web applications;test adequacy assessment technique;computational cost;test suite execution;equivalent mutants;mutation generation phase;variable selection;FunctionRank;function ranking;relative function importance;application behaviour;static analysis;dynamic analysis;program output;mutation operators;nonequivalent mutants;empirical evaluation;MUTANDIS tool;Web applications","","5","50","","","","","","IEEE","IEEE Journals & Magazines"
"Temperature-aware software-based self-testing for delay faults","Y. Zhang; Z. Peng; J. Jiang; H. Li; M. Fujita","School of Software Engineering, Tongji University, China; Embedded Systems Lab, Linköping University, Sweden; School of Software Engineering, Tongji University, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, China; VLSI Design and Education Center, University of Tokyo, Japan","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","423","428","Delay defects under high temperature have been one of the most critical factors to affect the reliability of computer systems, and the current test methods don't address this problem properly. In this paper, a temperature-aware software-based self-testing (SBST) technique is proposed to self-heat the processors within a high temperature range and effectively test delay faults under high temperature. First, it automatically generates high-quality test programs through automatic test instruction generation (ATIG), and avoids over-testing caused by nonfunctional patterns. Second, it exploits two effective powerintensive program transformations to self-heat up the processors internally. Third, it applies a greedy algorithm to search the optimized schedule of the test templates in order to generate the test program while making sure that the temperature of the processor under test is within the specified range. Experimental results show that the generated program is successful to guarantee delay test within the given temperature range, and achieves high test performance with functional patterns.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0744","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092427","","Delays;Program processors;Temperature distribution;Power dissipation;Circuit faults;Schedules","automatic test pattern generation;fault diagnosis;greedy algorithms;reliability","temperature-aware software-based self-testing;delay faults;reliability;computer systems;SBST technique;high-quality test program;automatic test instruction generation;ATIG;power-intensive program transformation;greedy algorithm","","2","14","","","","","","IEEE","IEEE Conferences"
"ANN-based dredging operation parameters optimization and software development","Wei Li; Guo-Jun Hong; Wei Shu; Gen-Ke Yang","CCCC National Engineer Research Center of Dredging Technology and Equipment Co. Ltd., Shanghai 201208, China; CCCC National Engineer Research Center of Dredging Technology and Equipment Co. Ltd., Shanghai 201208, China; CCCC National Engineer Research Center of Dredging Technology and Equipment Co. Ltd., Shanghai 201208, China; Shanghai Jiao Tong University, China","2015 12th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)","","2015","","","311","315","In this paper, the trailing suction hopper dredger dredging production is used to be an optimization objective. By analyzing the impacts of dredging operation behaviors and system uptime factors, an artificial neural network algorithm for parameter optimization of dredging operations is proposed. Based on the field construction parameters of the trailing suction hopper dredger, the method integrates the genetic algorithm for solving common the dredging job of production optimization problem, and gives the optimized parameters. Adopting the field collecting data, the simulation test is carried out on the VC++ platform. The simulation results show that the method provided in this paper can be effective to carry on the dredging operations modeling and parameter optimization problem. Given the same operating conditions, it is able to meet the production constraints. Meanwhile, based on this method, a dredging operation decision support software program is developed with VC++. The software is based on actual demand, the use of algorithms automatically give the construction parameters of the optimization.","","978-1-4673-8266","10.1109/ICCWAMTIP.2015.7493999","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7493999","Artificial neural networks;parameter optimization;decision support software;dredging production","Optimization;Software;Data models;Object oriented modeling;Neural networks;Production;Analytical models","","","","","8","","","","","","IEEE","IEEE Conferences"
"Towards Validating Complexity-Based Metrics for Software Product Line Architectures","A. Marcolino; E. Oliveira; I. Gimenes; T. U. Conte","NA; NA; NA; NA","2013 VII Brazilian Symposium on Software Components, Architectures and Reuse","","2013","","","69","79","Software product line (PL) is an approach that focuses on software reuse and has been successfully applied for specific domains. The PL architecture (PLA) is one of the most important assets, and it represents commonalities and variabilities of a PL. The analysis of the PLA, supported by metrics, can be used as an important indicator of the PL quality and return on investment (ROI). This paper presents the replication of a controlled experiment for validating complexity metrics for PLAs. In particular, in this replication we are focused on evaluating how subjects less-qualified than the subjects from the original experiment evaluate complexity of a PLA by means of generated specific products. It was applied a PLA variability resolution model of a given PL to a sample of subjects from at least basic knowledge on UML modeling, PL and variability management. Apart of the selection of different subjects, the same original experiment conditions were kept. The proposed PLA complexity metrics were experimentally validated based on their application to a set of 35 derived products from the Arcade Game Maker (AGM) PL. Normality tests were applied to the metrics observed values, thus, pointing out their non-normality. Therefore, the non-parametric Spearman's correlation ranking technique was used to demonstrate the correlation between the CompPLA metric and the complexity rate given by the subjects to each derived product. Such a correlation was strong and positive. The results obtained in this replication shown that even less-qualified subjects, compared to the subjects from the original experiment, are able to rate the complexity of a PLA by means of its generated products, thus corroborating the results of the original experiment and providing more evidence that the composed metric for complexity (CompPLA) can be used as a relevant indicator for measuring the complexity of PLA based on their derived products.","","978-1-4799-2531","10.1109/SBCARS.2013.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6685792","Correlation Analysis;Emprical Validation;Metrics;Replication;Software Product Line Architecture","Programmable logic arrays;Measurement;Complexity theory;Correlation;Unified modeling language;Computer architecture;Software","nonparametric statistics;program verification;software architecture;software metrics;software quality;software reusability","complexity-based metrics validation;software product line architectures;software reuse;software PLA;PL quality;return on investment;ROI;PLA variability resolution model;UML modeling;variability management;PLA complexity metrics;Arcade Game Maker;AGM PL;nonparametric Spearman correlation ranking technique;CompPLA metric;complexity rate","","","20","","","","","","IEEE","IEEE Conferences"
"An SMT Based Method for Optimizing Arithmetic Computations in Embedded Software Code","H. Eldib; C. Wang","Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA, USA","IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","","2014","33","11","1611","1622","We present a new method for optimizing the source code of embedded control software with the objective of minimizing implementation errors in the linear fixed-point arithmetic computations caused by overflow, underflow, and truncation. Our method relies on the use of the satisfiability modulo theory (SMT) solver to search for alternative implementations that are mathematically equivalent but require a smaller bit-width, or implementations that use the same bit-width but have a larger error-free dynamic range. Our systematic search of the bounded implementation space is based on a new inductive synthesis procedure, which is guaranteed to find a valid solution as long as such solution exists. Furthermore, we propose an incremental optimization procedure, which applies the synthesis procedure only to small code regions-one at a time-as opposed to the entire program, which is crucial for scaling the method up to programs of realistic size and complexity. We have implemented our new method in a software tool based on the Clang/LLVM compiler frontend and the Yices SMT solver. Our experiments, conducted on a set of representative benchmarks from embedded control and digital signal processing applications, show that the method is both effective and efficient in optimizing arithmetic computations in embedded software code.","0278-0070;1937-4151","","10.1109/TCAD.2014.2341931","NSF; ONR; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926926","Fixed point arithmetic;inductive program synthesis;satisfiability modulo theory (SMT) solver;superoptimization","Finite wordlength effects;Optimization;Microcontrollers;Embedded software;Dynamic range;Benchmark testing","computability;embedded systems;fixed point arithmetic;optimisation;program compilers;source code (software)","digital signal processing;embedded control;Yices SMT solver;LLVM compiler frontend;incremental optimization procedure;inductive synthesis procedure;satisfiability modulo theory solver;linear fixed-point arithmetic computations;embedded control software;source code optimization;embedded software code;arithmetic computations optimization;SMT based method","","5","29","","","","","","IEEE","IEEE Journals & Magazines"
"A defect dependency based approach to improve software quality in integrated software products","S. A. Karre; Y. R. Reddy","Software Engineering Research Center, International Institute of Information Technology, Hyderabad, India; Software Engineering Research Center, International Institute of Information Technology, Hyderabad, India","2015 International Conference on Evaluation of Novel Approaches to Software Engineering (ENASE)","","2015","","","110","117","Integrated software products are complex in design. They are prone to defects caused by integrated and non-integrated modules of the entire integrated software suite. In such software products, a small proportion of defects are fixed as soon as they are reported. Rest of the defects are targeted for fixes in future product release cycles. Among such targeted defects, most of them seem to be insignificant and innocuous in the current version but have the potential to become acute in future versions. In this paper, we propose an approach to study defect dependency of the reported defect using a dependency metric. Identifying the dependency of a defect in an integrated product suite can help the product stake-owners to prioritize them and help improve software quality.","","978-989-758-143","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320343","Defect Dependency;Defect Dataset;Dependency Metric;Software Quality;Integrated Software Products;Rule-based Classification","Software;Yttrium;Measurement;Testing;Data mining;Training;Influenza","","","","","20","","","","","","IEEE","IEEE Conferences"
"On development of change point based generalized SRGM for software with multiple releases","N. Nijhawan; A. G. Aggarwal","Department of Operational Research, University of Delhi, India; Department of Operational Research, University of Delhi, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","The concept of change point has been widely discussed with respect to fault removal process of single release software only. In this paper, we propose a discrete generalized modeling framework for multi-release software system by applying the concept of change point on some standard discrete probability distributions to model the fault removal process. Parameter estimation has been done and goodness-of-fit is analyzed on real life 4-release failure dataset.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359362","Generalized framework;fault removal process;multi-release;change point","Software;Testing;Parameter estimation;Probability distribution;Software reliability;Data models;Mathematical model","fault diagnosis;parameter estimation;program debugging;software engineering;statistical distributions","SRGM;fault removal process;single release software;discrete generalized modeling framework;multirelease software system;standard discrete probability distribution;parameter estimation","","2","20","","","","","","IEEE","IEEE Conferences"
"Proteum/FL: A tool for localizing faults using mutation analysis","M. Papadakis; M. E. Delamaro; Y. Le Traon","Interdisciplinary Center for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg; Instituto de Ciências Matemáticas e de Computação, Universidade de São Paulo, São Carlos, Brazil; Interdisciplinary Center for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg","2013 IEEE 13th International Working Conference on Source Code Analysis and Manipulation (SCAM)","","2013","","","94","99","Fault diagnosis is the process of analyzing programs with the aim of identifying the code fragments that are faulty. It has been identified as one of the most expensive and time consuming tasks of software development. Even worst, this activity is usually accomplished based on manual analysis. To this end, automatic or semi-automatic fault diagnosis approaches are useful in assisting software developers. Hence, they can play an essential role in decreasing the overall development cost. This paper presents Proteum/FL, a mutation analysis tool for diagnosing previously detected faults. Given an ANSI-C program and a set of test cases, Proteum/FL returns a list of program statements ranked according to their likelihood of being faulty. The tool differs from the rest of the mutation analysis and fault diagnosis tools by employing mutation analysis as a means of diagnosing program faults. It therefore demonstrates the effective use of mutation in supporting both testing and debugging activities.","","978-1-4673-5739","10.1109/SCAM.2013.6648189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6648189","Mutation analysis;fault localization;software debugging;software testing","Fault diagnosis;Optimization;Conferences;Software;Software testing;Syntactics","ANSI standards;fault diagnosis;program debugging;program diagnostics;program testing;software fault tolerance","Proteum/FL;fault localization;fault diagnosis process;program analysis process;faulty code fragment identification;software development;semiautomatic fault diagnosis approach;software developers;ANSI-C program;program statements;fault diagnosis tools;mutation analysis tools;debugging activities;testing activities","","2","21","","","","","","IEEE","IEEE Conferences"
"A Novel Algorithm for Multi-Path Test Data Generation","W. Shitao; W. Hao","NA; NA","2013 Fourth International Conference on Digital Manufacturing & Automation","","2013","","","58","60","Automatically generating test data to cover multiple paths is a challenging problem. This paper presents a new niching PSO algorithm, called MNPSO, to deal with this problem. In MNPSO, all particles are dynamically divided into several sub-populations and each sub-population has its global best position. The best positions of each particle are recorded for sub-populations. Test data is generated for one target path in each sub-population and the center of each sub-population is its global best positions. The results of experiments showed that MNPSO improved the efficiency of multi-path test data generation and multi-path coverage.","","978-0-7695-5016","10.1109/ICDMA.2013.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6597933","software testing;test data generation;multi-path coverage;niching PSO","Manufacturing;Automation","automatic test pattern generation;particle swarm optimisation;program testing","automatic multipath test data generation;niching PSO algorithm;MNPSO;subpopulation;global best position;multipath coverage","","","7","","","","","","IEEE","IEEE Conferences"
"Developing software reliability growth model for multi up gradations with faults of different severity and related release time problem","A. Tickoo; P. K. Kapur; S. K. Khatri","Amity School of Engineering, Amity University, Noida UP, India; Amity Centre for Interdisciplinary, Research Amity University, Noida UP, India; Amity Institute of Information Technology, Amity University Noida UP, India","2015 International Conference on Futuristic Trends on Computational Analysis and Knowledge Management (ABLAZE)","","2015","","","376","382","In order to deal with the stiff competition and survive in the market, software companies need to keep upgrading their software by bringing new changes and improvising it. By adding new functionalities company aims to improve the software performance and functionality. But software up gradation does not always lead to quality enhancement, there is a possibility of adding new faults while up gradation, which causes rise in total amount of faults. In this work a software reliability growth model (SRGM) has been proposed for multi up gradations with faults of different severity and related release time problem. An optimization equation for cost model is described to determine the optimum time of release in multi up-gradation of software. Here we classify the faults into three different types depending upon the effort and time consumed for their removal; simple, hard and complex faults. We validated this model on a data set for software with four releases.","","978-1-4799-8433-6978-1-4799-8432","10.1109/ABLAZE.2015.7155023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155023","Non-homogenous Poisson Process (NHPP);Fault Severity;Multi-up gradation;Software Reliability Growth Model (SRGM);Probability distribution function(PDF)","Software;Testing;Debugging;Mathematical model;Software reliability;Market research","software quality;software reliability","software reliability growth model;multi up gradations;software companies;software performance;software functionality;software quality enhancement;optimization equation;software multi up-gradation","","","15","","","","","","IEEE","IEEE Conferences"
"Automated Oracle Data Selection Support","G. Gay; M. Staats; M. Whalen; M. P. E. Heimdahl","Department of Computer Science & Engineering, University of South Carolina; Google, Inc.; Department of Computer Science and Engineering, University of Minnesota; Department of Computer Science and Engineering, University of Minnesota","IEEE Transactions on Software Engineering","","2015","41","11","1119","1137","The choice of test oracle-the artifact that determines whether an application under test executes correctly-can significantly impact the effectiveness of the testing process. However, despite the prevalence of tools that support test input selection, little work exists for supporting oracle creation. We propose a method of supporting test oracle creation that automatically selects the oracle data-the set of variables monitored during testing-for expected value test oracles. This approach is based on the use of mutation analysis to rank variables in terms of fault-finding effectiveness, thus automating the selection of the oracle data. Experimental results obtained by employing our method over six industrial systems (while varying test input types and the number of generated mutants) indicate that our method-when paired with test inputs generated either at random or to satisfy specific structural coverage criteria-may be a cost-effective approach for producing small, effective oracle data sets, with fault finding improvements over current industrial best practice of up to 1,435 percent observed (with typical improvements of up to 50 percent).","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2436920","NASA; NSF; US National Science Foundation (NSF); Fonds National de la Recherche, Luxembourg; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7112189","Testing;Test Oracles;Oracle Data;Oracle Selection;Verification;Testing;test oracles;oracle data;oracle selection;verification","Testing;Monitoring;Software;Aerospace electronics;Training;Electronic mail;Computer crashes","program testing;program verification","automated oracle data selection support;mutation analysis;software testing;test oracle;oracle creation;specific structural coverage criteria","","5","40","","","","","","IEEE","IEEE Journals & Magazines"
"Implementing Agile in old technology projects","S. Chopra","Pitney Bowes India, C-31 Radhey Puri Near Krishna Nagar Delhi 110051 India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","4","With this paper, I would like to share my experience of implementing Agile in obsolete technology projects. This is a reality that lot of our projects still run on old/obsolete technologies and customer base for these projects may be huge. These projects can be cash cows for the organizations and huge cost/effort is required to migrate them to latest technologies. Consider a banking or airlines application running on COBOL or FoxPro. Complex logics are built into these applications and they may be running successfully for the past many decades. Implementing Agile in such projects raises unique set of challenges. Can we really do rapid development with these projects? Can we commit that our development and testing efforts will be completed in a sprint in the next two weeks? Is our infrastructure supporting Agile development? And above all, are our teams ready to adopt Agile? I would like to share my experience of implementing Agile in old technology projects, the issues I came across and the path we followed to resolve the issues. Lot of issues are dependent on organization culture and other internal/external factors but the details mentioned below will definitely be helpful to understand the problems in detail along with ways to tackle it.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014757","Agile;Old technologies;Sustenance;Technical Debt;Agile Testing;Development practices","Automation;Organizations;Testing;Measurement;Software;Batch production systems;Manuals","COBOL;program testing;software prototyping","Agile testing;banking;airline application;COBOL;FoxPro;complex logics;Agile development","","1","5","","","","","","IEEE","IEEE Conferences"
"Recent progress of software-related electromagnetic compatibility","S. Yuan","Department of Communication Engineering, Feng Chia University, Taichung, Taiwan, R.O.C.","2014 15th Latin American Test Workshop - LATW","","2014","","","1","4","Electronic products are now essential to daily lives. Although power-saving are well known for consumers, electromagnetic compatibility (EMC) issues are the most critical issues which determine the pass/fail criteria of electronic products coming into the market. Since modern software can control almost any device hardware features and EMC is just one of them, software is capable of controlling it. Thus, software-related EMC for modern digital systems is essential for all electronic products. This paper is a modest attempt to review key developments that mark the history and recent progress in the software-related EMC researches.","2373-0862","978-1-4799-4711","10.1109/LATW.2014.6841899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6841899","Softwre-related EMC;Software-related EMC modeling;Software-related EMC solutions;Program optimization for EMC","Decision support systems;Electromagnetic compatibility;Optimization","computational electromagnetics;consumer electronics;electromagnetic compatibility;electronic products;energy conservation","software-related electromagnetic compatibility;electronic products;power saving;EMC;modern digital system;consumer electronics","","","39","","","","","","IEEE","IEEE Conferences"
"Competition over Resources: A New Optimization Algorithm Based on Animals Behavioral Ecology","S. Mohseni; R. Gholami; N. Zarei; A. R. Zadeh","NA; NA; NA; NA","2014 International Conference on Intelligent Networking and Collaborative Systems","","2014","","","311","315","In the recent years, many heuristic optimization algorithms have been developed. A majority of these heuristic algorithms have been derived from the behavior of biological or physical systems in nature. In this paper, we propose a new optimization algorithm based on competitive behavior of animal groups. In the proposed algorithm, the whole population is divided into a number of groups. In each group, the best searching agent spreads its children in its owned territory. Any group which is not able to find rich resources will be eliminated form competition. The competition gradually results in an increase in population of wealthy group which gives a fast convergence to proposed optimization algorithm. In the following, after a detailed explanation of the algorithm and pseudo code, we compare it to other existing algorithms, including genetics and particle swarm optimizations. Applying the proposed algorithm on various benchmark cost functions, shows faster and superior results compared to other optimization algorithms.","","978-1-4799-6387-4978-1-4799-6386","10.1109/INCoS.2014.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057107","optimization algorithm;animals behavioral ecology;nature inspired algorithm","Optimization;Sociology;Statistics;Benchmark testing;Algorithm design and analysis;Heuristic algorithms;Animals","behavioural sciences computing;ecology;optimisation;software agents","animals behavioral ecology;heuristic optimization algorithms;heuristic algorithms;physical systems;competitive behavior;animal groups;searching agent;pseudo code;particle swarm optimizations;cost functions","","9","19","","","","","","IEEE","IEEE Conferences"
"Search-based data-flow test generation","M. Vivanti; A. Mis; A. Gorla; G. Fraser","University of Lugano Lugano, Switzerland; Saarland University Saarbr&#x00FC;ckenM, Germany; Saarland University Saarbr&#x00FC;ckenM, Germany; University of Sheffield Sheffield, UK","2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)","","2013","","","370","379","Coverage criteria based on data-flow have long been discussed in the literature, yet to date they are still of surprising little practical relevance. This is in part because 1) manually writing a unit test for a data-flow aspect is more challenging than writing a unit test that simply covers a branch or statement, 2) there is a lack of tools to support data-flow testing, and 3) there is a lack of empirical evidence on how well data-flow testing scales in practice. To overcome these problems, we present 1) a search-based technique to automatically generate unit tests for data-flow criteria, 2) an implementation of this technique in the Evosuite test generation tool, and 3) a large empirical study applying this tool to the SF100 corpus of 100 open source Java projects. On average, the number of coverage objectives is three times as high as for branch coverage. However, the level of coverage achieved by Evosuite is comparable to other criteria, and the increase in size is only 15%, leading to higher mutation scores. These results counter the common assumption that data-flow testing does not scale, and should help to re-establish data-flow testing as a viable alternative in practice.","1071-9458;2332-6549","978-1-4799-2366","10.1109/ISSRE.2013.6698890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6698890","data-flow coverage;search based testing;unit testing","Testing;Java;Search problems;Standards;Equations;Writing;Optimization","data flow analysis;Java;program testing;public domain software","search-based data-flow test generation;coverage criteria;data-flow aspect;data-flow testing;EVOSUITE test generation tool;SF100 corpus;open source Java projects","","20","36","","","","","","IEEE","IEEE Conferences"
"Application of Parallel Computing in Robust Optimization Design Using MATLAB","H. Wang; Z. Lu; H. Zhao; H. Feng","NA; NA; NA; NA","2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC)","","2015","","","1228","1231","This article provides an effective way to alleviate the long running defect of multi-objective intelligent optimization algorithms and enhance its ability to solve the robust optimization (RO) design. It describes the implementation of parallel computing for intelligent optimization algorithms based on the MATLAB Parallel Computing Toolbox (PCT) and Distributed Computing Server (DCS). In order to test the effectiveness of the proposed method, the parallel multi-objective bacterial colony chemo taxis (PMOBCC) algorithm and parallel dichotomy (PD) are applied to run a class of bi-level inverse multi-objective robust optimization (BI-MORO) design. A set of experiments are tested by using different number of MATLAB workers. The results illustrated that it is convenient and effective to use MATLAB for parallelizing the intelligent optimization algorithms.","","978-1-4673-7723-2978-1-4673-7722","10.1109/IMCCC.2015.263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406042","Parallel computing Toolbox;MATLAB;intel-ligent optimization algorithms;robust optimization","Parallel processing;MATLAB;Optimization;Robustness;Algorithm design and analysis;Computers;Mathematical model","ant colony optimisation;parallel processing;software tools","robust optimization design;MATLAB;multiobjective intelligent optimization algorithms;parallel computing toolbox;PCT;distributed computing server;DCS;parallel multiobjective bacterial colony chemo taxis algorithm;PMOBCC algorithm;parallel dichotomy;bilevel inverse multiobjective robust optimization;BI-MORO design","","1","9","","","","","","IEEE","IEEE Conferences"
"Development of test method to verify water induced corona on composite insulator housing","P. Sidenvall; N. Sundin; I. Gutman; L. Carlshem; R. Kleveborn","STRI, Ludvika, Sweden; STRI, Ludvika, Sweden; STRI, Ludvika, Sweden; Svenska kraftn&#x00E4;t, Sundbyberg, Sweden; Kleveborn Consulting, Tyres&#x00F6;, Sweden","2014 IEEE Electrical Insulation Conference (EIC)","","2014","","","450","454","Two main criteria for the optimal design of corona/grading rings for composite insulation system include limit of E-field to avoid corona from a metal part and limit E-field to avoid water-induced corona on composite surface. At present only the first criterion is verified by the standard RIV test. This paper presents an innovative laboratory test method that can be used as a verification of the composite insulation system design to avoid water induced corona on the surface. Several approaches of producing water induced corona have been tested and evaluated. Pre-tests were performed on a small test object to optimize the different procedures. Tests were then performed on a full scale 420 kV composite insulator to verify the complete procedure. Different measuring techniques have also been evaluated. 3D calculation of the electric field for all tested structures has also been performed in Comsol software and compared to E-field criteria. Results from these calculations have also been compared with corona inception levels from the different tests and a good correlation was found. The innovative non-standard Water Drop Corona Induced test seems to be the best practical option for further development.","2334-0975","978-1-4799-2789-0978-1-4799-2787-6978-1-4799-2786","10.1109/EIC.2014.6869428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6869428","composite insulator;test method;water corona","Corona;Insulators;Electric fields;Standards;Rain;Metals;Laboratories","composite insulators;corona;electric field measurement;insulation testing;optimisation;packaging;water","composite insulator housing;corona-grading rings;composite insulation system;composite surface;standard RIV test;laboratory test method;measuring techniques;3D calculation;electric field;Comsol software;E-field criteria;corona inception levels;nonstandard water drop corona induced test;voltage 40 kV","","","8","","","","","","IEEE","IEEE Conferences"
"A New Local Search-Based Multiobjective Optimization Algorithm","B. Chen; W. Zeng; Y. Lin; D. Zhang","Software School, Xiamen University, Xiamen, Fujian, China; Software School, Xiamen University, Xiamen, Fujian, China; School of Information Science and Engineering, Xiamen University, Xiamen, Fujian, China; School of Information Science and Engineering, Xiamen University, Xiamen, Fujian, China","IEEE Transactions on Evolutionary Computation","","2015","19","1","50","73","In this paper, a new multiobjective optimization framework based on nondominated sorting and local search (NSLS) is introduced. The NSLS is based on iterations. At each iteration, given a population P, a simple local search method is used to get a better population P', and then the nondominated sorting is adopted on P ∪ P' to obtain a new population for the next iteration. Furthermore, the farthest-candidate approach is combined with the nondominated sorting to choose the new population for improving the diversity. Additionally, another version of NSLS (NSLS-C) is used for comparison, which replaces the farthest-candidate method with the crowded comparison mechanism presented in the nondominated sorting genetic algorithm II (NSGA-II). The proposed method (NSLS) is compared with NSLS-C and the other three classic algorithms: NSGA-II, MOEA/D-DE, and MODEA on a set of seventeen bi-objective and three tri-objective test problems. The experimental results indicate that the proposed NSLS is able to find a better spread of solutions and a better convergence to the true Pareto-optimal front compared to the other four algorithms. Furthermore, the sensitivity of NSLS is also experimentally investigated in this paper.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2014.2301794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6718037","Multiobjective optimization;nondominated sorting;local search;diversity;test problems;Diversity;local search;multiobjective optimization;nondominated sorting;test problems","Sociology;Statistics;Optimization;Sorting;Search methods;Convergence;Algorithm design and analysis","genetic algorithms;Pareto optimisation;search problems","local search-based multiobjective optimization algorithm;multiobjective optimization framework;nondominated sorting and local search;NSLS;farthest-candidate approach;nondominated sorting genetic algorithm II;NSGA-II;MOEA/D-DE algorithm;MODEA algorithm;Pareto-optimal front","","37","85","","","","","","IEEE","IEEE Journals & Magazines"
"BODMA: A novel metaheuristic algorithm for binary optimization problems based on open source Development Model Algorithm","H. Behzadi Khormouji; H. Hajipour; H. Rostami","Computer Eng. Department, Persian Gulf University of Bushehr, Bushehr 75168, Iran; Computer Eng. Department, Persian Gulf University of Bushehr, Bushehr 75168, Iran; Computer Eng. Department, Persian Gulf University of Bushehr, Bushehr 75168, Iran","7'th International Symposium on Telecommunications (IST'2014)","","2014","","","49","54","Open source Development Model Algorithm (ODMA) is one of the recently proposed heuristic algorithms that is inspired by open source software development mechanism and community's behaviors. The superior performance of this algorithm has been proven among the other most well-known algorithms such as genetic algorithm (GA) and particle swarm optimization (PSO). However, the original version of this algorithm is suitable for continuous problems, so it cannot be applied to binary problems directly. In this paper, a binary version of this algorithm is proposed. A comparative study with binary PSO and GA over 7 benchmark functions is conducted to draw a conclusion. The results prove that the proposed binary Open source Development Model Algorithm (BODMA) is able to significantly outperform others on majority of the benchmark functions.","","978-1-4799-5359-2978-1-4799-5358","10.1109/ISTEL.2014.7000668","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000668","meta-heuristic algorithm;evolutionary computation;binary optimization;transfer function;open source development model algorithm","Software;Optimization;Software algorithms;Mathematical model;Equations;Benchmark testing;Convergence","optimisation;public domain software;software engineering","binary open source development model algorithm;BODMA;binary optimization problems;metaheuristic algorithm;open source software development mechanism;community behaviors;genetic algorithm;GA;particle swarm optimization;PSO;benchmark functions","","","17","","","","","","IEEE","IEEE Conferences"
"Uncertainty-aware reliability analysis and optimization","F. Khosravi; M. Müller; M. Glaß; J. Teich","Hardware/Software Co-Design Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Germany; Hardware/Software Co-Design Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Germany; Hardware/Software Co-Design Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Germany; Hardware/Software Co-Design Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Germany","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","97","102","Due to manufacturing tolerances and aging effects, future embedded systems have to cope with unreliable components. The intensity of such effects depends on uncertain aspects like environmental or usage conditions such that highly safety-critical systems are pessimistically designed for worst-case mission profiles. In this work, we propose to explicitly model the uncertain characteristics of system components, i. e. we model components using reliability functions with parameters distributed between a best and worst case. Since destructive effects like temperature may affect several components simultaneously (e. g. those in the same package), a correlation between uncertainties of components exists. The proposed uncertainty-aware method combines a formal analysis approach and a Monte Carlo simulation to consider uncertain characteristics and their different correlations. It delivers a holistic view on the system's reliability with best/worst/average-case behavior and also insights on variance and quantiles. But, existing optimization approaches typically assume design objectives to be single values or to follow a predefined distribution. As a remedy, we propose a dominance criterion for meta-heuristic optimization approaches like evolutionary algorithms that enables the comparison of system implementations with arbitrarily distributed characteristics. Our presented experimental results show that (a) the proposed analysis comes at low overhead while capturing existing uncertainties with sufficient accuracy, and (b) the optimization process is significantly enhanced when guiding the search process by additional aspects like variance and the 95% quantile, delivering better system implementations as found by an uncertainty-oblivious optimization approach.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0319","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092365","","Reliability;Optimization;Uncertainty;Correlation;Data structures;Boolean functions;Embedded systems","embedded systems;evolutionary computation;Monte Carlo methods;optimisation;safety-critical software","uncertainty-aware reliability analysis;meta-heuristic optimization approach;embedded systems;safety-critical systems;worst-case mission profiles;system component uncertain characteristics;destructive effects;formal analysis approach;Monte Carlo simulation;best/worst/average-case behavior;design objectives;dominance criterion;evolutionary algorithm;uncertainty-oblivious optimization approach","","5","11","","","","","","IEEE","IEEE Conferences"
"Fuzzy entropy-based framework for multi-faceted test case classification and selection: an empirical study","M. Kumar; A. Sharma; R. Kumar","Department of Computer Application, Galgotias University, Greater Noida, UP, India; Department of CSE, Krishna Institute of Engineering and Technology, Ghaziabad, UP, India; School of Mathematics and Computer Application, Thapar University, Patiala, Punjab, India","IET Software","","2014","8","3","103","112","Software testing is complex, ambiguous, labour-intensive, costly, error prone and a core activity of software development. Devising the cost-effective and adequate strategies for software test cases optimisation has been one of the research issues in software testing for a long time. Existing techniques of test case optimisation are not providing the optimal solution to the test cases optimisation problem in terms of precision, completeness, cost and adequacy. The authors have already proposed a fuzzy logic-based multi-faceted measurement framework for test cases classification and fitness evaluation. Though, it reduces testing efforts, cost, incompleteness and increases adequacy, but, still there is ambiguity in classification and selection of some test cases due to ambiguity in fitness of test cases. Hence, there is a strong need to devise a technique to measure suitably and resolve the ambiguity in software test cases classification and selection problem. In this paper, the authors have unified their earlier proposed framework by introducing fuzzy entropy-based approach. The proposed unified framework chunks out the high ambiguity test cases and selects low ambiguity test cases for exercising on SUT (Software under Test). The proposed unified framework is tested on artefacts of benchmark applications, and the results show that the proposed unified framework enhances the classification accuracy by reducing ambiguity, and increases the number of test cases classified accurately.","1751-8806;1751-8814","","10.1049/iet-sen.2012.0198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6829971","","","entropy;fuzzy logic;fuzzy set theory;pattern classification;program testing;software engineering","software development;multifaceted test case classification;multifaceted test case selection;fuzzy entropy-based framework;fuzzy logic-based multifaceted measurement framework;test case fitness evaluation;software test case classification;software test case selection problem;SUT;ambiguity estimation;ambiguity reduction","","3","25","","","","","","IET","IET Journals & Magazines"
"Test Case Prioritization Based on Information Retrieval Concepts","J. Kwon; I. Ko; G. Rothermel; M. Staats","NA; NA; NA; NA","2014 21st Asia-Pacific Software Engineering Conference","","2014","1","","19","26","In regression testing, running all a system's test cases can require a great deal of time and resources. Test case prioritization (TCP) attempts to schedule test cases to achieve goals such as higher coverage or faster fault detection. While code coverage-based approaches are typical in TCP, recent work has explored the use of additional information to improve effectiveness. In this work, we explore the use of Information Retrieval (IR) techniques to improve the effectiveness of TCP, particularly for testing infrequently tested code. Our approach considers the frequency at which elements have been tested, in additional to traditional coverage information, balancing these factors using linear regression modeling. Our empirical study demonstrates that our approach is generally more effective than both random and traditional code coverage-based approaches, with improvements in rate of fault detection of up to 4.7%.","1530-1362;1530-1362","978-1-4799-7426-9978-1-4799-7425","10.1109/APSEC.2014.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091286","","Testing;Fault detection;Training;Java;Mathematical model;Linear regression;Information retrieval","information retrieval;program testing;regression analysis","test case prioritization;information retrieval concepts;regression testing;TCP;fault detection;code coverage;IR techniques;tested code;linear regression modeling","","2","22","","","","","","IEEE","IEEE Conferences"
"Testing elastic systems with surrogate models","A. Gambi; W. Hummer; S. Dustdar","Vienna University of Technology, Austria; Vienna University of Technology, Austria; Vienna University of Technology, Austria","2013 1st International Workshop on Combining Modelling and Search-Based Software Engineering (CMSBSE)","","2013","","","8","11","We combine search-based test case generation and surrogate models for black-box system testing of elastic systems. We aim to efficiently generate tests that expose functional errors and performance problems related to system elasticity. Elastic systems dynamically change their resources allocation to provide consistent quality of service in face of workload fluctuations. However, their ability to adapt could be a double edged sword if not properly designed: They may fail to acquire the right amount of resources or even fail to release them. Blackbox system testing may expose such problems by stimulating system elasticity with suitable sequences of interactions. However, finding such sequences is far from trivial because the number of possible combinations of requests over time is unbounded. In this paper, we analyze the problem of generating test cases for elastic systems, we cast it as a search-based optimization combined with surrogate models, and present the conceptual framework that supports its execution.","","978-1-4673-6284","10.1109/CMSBSE.2013.6604429","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604429","model based testing;load testing;genetic algorithm;surrogate models","Elasticity;Testing;Predictive models;Search problems;Computational modeling;Quality of service;Genomics","genetic algorithms;program testing;quality of service;search problems;software performance evaluation","search-based optimization;workload fluctuations;quality of service;performance problems;functional errors;black-box system testing;search-based test case generation;surrogate models;elastic systems testing","","3","25","","","","","","IEEE","IEEE Conferences"
"High performance C programming","V. K. Myalapalli; J. K. Myalapalli; P. R. Savarapu","JNTUK University College of Engineering, Vizianagaram, Andhra Pradesh, India; Chaitanya Bharathi Institute of Technology, Gandipet, Hyderabad, India; Andhra University College of Engineering, Visakhapatnam, Andhra Pradesh, India","2015 International Conference on Pervasive Computing (ICPC)","","2015","","","1","6","Optimization is one of the desired objectives in software engineering to ensure that program makes proficient utilization of system resources. Optimization entails making program(s) bug free, reduced time and space complexity, portable etc. As such in this paper we present sundry tuning techniques for rewriting a statement which increases the efficiency of the statement in the area of time and space complexity. The denouements of our schemes and methodologies advocate that security and performance i.e. time and space complexities are enhanced. Our analysis can serve as hand held Tuning and White Box Testing Tool for C programmers.","","978-1-4799-6272","10.1109/PERVASIVE.2015.7087005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7087005","C Optimization;C Tuning;C Best Practices;C Tactics;C Performance Tuning;C Code Optimization","Optimization;Registers;Tuning;Memory management;Complexity theory;Arrays;Program processors","C language;parallel programming;program debugging;program testing;safety-critical software;software portability","high-performance C programming;software engineering;system resource utilization;bug-free programs;program optimization;time complexity reduction;space complexity reduction;portable software;sundry tuning techniques;statement rewriting;statement efficiency improvement;security enhancement;performance enhancement;white box testing tool","","2","6","","","","","","IEEE","IEEE Conferences"
"Optimization of discrete and continuous testsuite using heuristic algorithm: A comparative study","S. P. Tripathy; P. M. Sahu; D. Kanhar","MCA Department, National Institute of Science and Technology, Berhampur, India; Roland Institute of Technology, Berhampur, India; CSE Department, National Institute of Science and Technology, Berhampur, India","2013 IEEE Conference on Information & Communication Technologies","","2013","","","841","846","Software testing plays a major role for developing better quality products. The more test will be conducted, products will be finer and finer. Hence, testing is must for software development. Another side of testing is spending much money on it because people will work rigorously to generate the test suite and executing it. As we know that, no software is bug free software; we cannot assure that the testing which has been done for particular software is sufficient. To get a cost effective testing strategy, one should go for optimization of testsuite. This paper uses Genetic algorithm used to optimize the discrete as well as continuous test suite with a comparative study. Genetic algorithm plays a major role to have a sound weight on optimization of testsuite. Testing can be justified by taking discrete testsuite and can be compared with the result of optimization of continuous testsuite.","","978-1-4673-5758-6978-1-4673-5759-3978-1-4673-5757","10.1109/CICT.2013.6558211","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6558211","Genetic Algorithm;selection;crossover;mutation;testsuite","Genetic algorithms;Biological cells;Redundancy;Testing;Optimization;Sociology;Statistics","costing;genetic algorithms;product development;program testing;software quality","discrete testsuite optimization;continuous testsuite optimization;heuristic algorithm;software testing;quality product development;software development;cost effective testing strategy;genetic algorithm;sophistical","","","15","","","","","","IEEE","IEEE Conferences"
"Artificial intelligence based dynamic transmission network expansion planning","A. Simo; S. Kilyeni; C. Barbulescu","Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania; Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania; Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania","2015 8th International Conference on Human System Interaction (HSI)","","2015","","","54","60","The paper is focusing on dynamic transmission network expansion planning (TNEP). The TNEP problem has been approached from the retrospective and prospective point of view. To achieve this goal, the authors are developing two software-tools in Matlab environment. Power flow computing is performed using conventional methods. Optimal power flow and network expansion are performed using artificial intelligence methods. Within this field, two techniques have been tackled: particle swarm optimization (PSO) and genetic algorithms (GA). The case study refers to well-known IEEE 24 RTS test power system.","2158-2246;2158-2254","978-1-4673-6936","10.1109/HSI.2015.7170643","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170643","artificial intellingence;dynamic expansion planning;retrospectiv approach;optimization;transmission network;software-tool","Power system dynamics;Planning;Sociology;Statistics;Genetic algorithms;Optimization","genetic algorithms;load flow;particle swarm optimisation;planning (artificial intelligence);power engineering computing;software tools;transmission networks","artificial intelligence based dynamic transmission network expansion planning;dynamic TNEP;software-tools;Matlab environment;power flow computing;optimal power flow;particle swarm optimization;PSO;genetic algorithms;GA;IEEE 24 RTS test power system","","1","13","","","","","","IEEE","IEEE Conferences"
"On the Effect of Counters in Guard Conditions When State-Based Multi-objective Testing","N. Asoudeh; Y. Labiche","NA; NA","2015 IEEE International Conference on Software Quality, Reliability and Security - Companion","","2015","","","105","114","During test case generation from an extended finite state machine (EFSM), the counter problem is caused by the presence of guard conditions that refer to counter variables. Because such variables are initialized and updated by transitions in the EFSM, every traversal of the state machine graph is not necessarily feasible, i.e., executable. The problem manifests itself by the fact that a transition, a sequence of transitions, or a more complex behavior in the state machine, has to be repeatedly triggered to eventually trigger a specific behavior (another transition). In this paper we define different manifestations of the counter problem and experiment with a new search based solution for that problem. We also investigate how the counter problem affects a multi-objective genetic algorithm that generates test suites from an EFSM. We evaluate our solution and compare it with an existing one, using three different case studies.","","978-1-4673-9598","10.1109/QRS-C.2015.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322131","Counter problem;Extended Finite State Machines;Genetic algorithms;Multi-objective optimization;Software testing","Radiation detectors;Testing;Biological cells;Genetic algorithms;Search problems;Automata;Optimization","finite state machines;genetic algorithms;graph theory;program testing","guard condition;multiobjective software testing;test case generation;extended finite state machine;EFSM;finite state graph;multiobjective genetic algorithm","","2","42","","","","","","IEEE","IEEE Conferences"
"An Extreme Learning Machine based on Quantum Particle Swarm Optimization and its application in handwritten numeral recognition","X. Sun; L. Qin","School of Computer, Electronics and information, Guangxi University, Nanning, China; School of Computer, Electronics and information, Guangxi University, Nanning, China","2014 IEEE 5th International Conference on Software Engineering and Service Science","","2014","","","323","326","The Extreme Learning Machine algorithm was proposed by Prof. Guangbin Huang in 2004. It is a single hidden layer feedforward neural network. It has attracted extensive research of many scholars because of its fast speed, simple implementation and good generalization performance. In this paper, Quantum Particle Swarm Optimization was introduced to extreme learning machine to solve the problem of complex network structure which is caused by random assignments to the input weights and biases of hidden nodes. The QPSO is used in the process to select the input weights and biases instead of random assignment. Then extreme learning machine uses the result produced by QPSO to train the network. Thus can improve the prediction accuracy and response speed to unknown data and gain a more compact network structure. The proposed method is used in handwritten numeral recognition application in the end. And it gets an approving performance.","2327-0594;2327-0586","978-1-4799-3279-5978-1-4799-3278-8978-1-4799-3277","10.1109/ICSESS.2014.6933573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933573","extreme learning machine;network structure;Quantum Particle Swarm Optimization;prediction accuracy;Handwritten Numeral Recognition","Testing;Training;Neural networks;Accuracy;Particle swarm optimization;Handwriting recognition;Classification algorithms","feedforward neural nets;handwritten character recognition;learning (artificial intelligence);particle swarm optimisation","extreme learning machine;quantum particle swarm optimization;handwritten numeral recognition;single hidden layer feedforward neural network;generalization performance;random assignments;input weights;hidden nodes;QPSO;compact network structure","","","12","","","","","","IEEE","IEEE Conferences"
"Standardize on common test hardware with a flexible switching matrix","J. Harnack; B. Brice","Automated Test Product Manager, National Instruments, Austin, Texas, United States of America; Automated Test Product Manager, National Instruments, Austin, Texas, United States of America","2013 IEEE AUTOTESTCON","","2013","","","1","5","As product complexity continues to increase, test engineers are challenged with improving test coverage on a variety of devices while keeping costs at a minimum. When finished products incorporate hundreds or thousands of devices, developing and maintaining separate test systems for individual components lacks scalability. An approach to this problem is to standardize on a single platform and develop a small number of test systems that can each test tens or hundreds of different devices. Test engineers are increasingly seeing the benefits of developing, deploying, and supporting a reduced number of test systems that use common components. Although many factors such as system assurance and platform longevity influence the long-term success of these systems, the core requirements consist of modular, reconfigurable hardware. A switching matrix is the main hardware component that gives a single test system the flexibility to test a variety of devices. With a switching matrix, you can connect all of your instruments and test points through a series of relays and close relevant relays via software. Switching matrices provide several benefits such as simple connectivity and test repeatability; however, it's usually not practical to have a switching matrix that completely matches the I/O of your instruments. For example, if you have a 1,000 V DMM, a 2 A power supply, and a 1.5 GHz digitizer, you're unlikely to find a COTS switching matrix that maximizes those I/O points. Selecting a switching matrix involves evaluating various trade-offs between density, power, bandwidth, and so on and optimizing the cost of your test system. This paper examines the trade-offs you should evaluate when choosing a switching matrix along with considerations for reducing cost and size.","1088-7725;1558-4550","978-1-4673-5683-1978-1-4673-5681","10.1109/AUTEST.2013.6645061","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645061","","Switches;Relays;Instruments;Software;Multiplexing;Hardware;Nickel","switching circuits;test equipment","avionics;test repeatability;connectivity;relays;flexibility;test systems;flexible switching matrix;common test hardware","","","4","","","","","","IEEE","IEEE Conferences"
"Using Design of Experiments to Optimise a Decision of Sufficient Testing","M. Malekzadeh; I. Bate; S. Punnekkat","NA; NA; NA","2015 41st Euromicro Conference on Software Engineering and Advanced Applications","","2015","","","53","60","Testing of safety-critical embedded systems is an important and costly endeavor. To date researchers and practitioners have been mainly focusing on the design and application of diverse testing strategies, but leaving the test stopping criteria as an ad hoc decision and an open research issue. In our previous work, we proposed a convergence algorithm that informs the tester when the current testing strategy does not seem to be revealing new insight into the worst-case timing properties of tasks and hence should be stopped. This algorithm was shown to be successful but its trial and error tuning of parameters was an issue. In this paper, we use the Design of Experiment (DOE) approach to optimise the algorithm's performance and to improve its scalability. During our experimental evaluations the optimised algorithm showed improved performance by achieving relatively the same results with 42% less testing cost as compared to our previous work. The algorithm also has better scalability and opens up a new path towards achieving cost effective non-functional testing of real-time embedded systems.","1089-6503;2376-9505","978-1-4673-7585-6978-1-4673-7584","10.1109/SEAA.2015.79","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7302431","Testing;Safety;ALARP;Design of Experiment;Optimisation;Real-time System","Testing;Convergence;Algorithm design and analysis;Scalability;Tuning;Time factors;Measurement","design of experiments;embedded systems;program testing","design of experiments;safety-critical embedded system;system testing decision;testing strategy;test stopping criteria;convergence algorithm;worst-case timing properties;DOE approach;realtime embedded systems","","2","16","","","","","","IEEE","IEEE Conferences"
"Static Analysis Driven Cache Performance Testing","A. Banerjee; S. Chattopadhyay; A. Roychoudhury","NA; NA; NA","2013 IEEE 34th Real-Time Systems Symposium","","2013","","","319","329","Real-time, embedded software are constrained by several non-functional requirements, such as timing. With the ever increasing performance gap between the processor and the main memory, the performance of memory subsystems often pose a significant bottleneck in achieving the desired performance for a real-time, embedded software. Cache memory plays a key role in reducing the performance gap between a processor and main memory. Therefore, analyzing the cache behaviour of a program is critical for validating the performance of an embedded software. In this paper, we propose a novel approach to automatically generate test inputs that expose the cache performance issues to the developer. Each such test scenario points to the specific parts of a program that exhibit anomalous cache behaviour along with a set of test inputs that lead to such undesirable cache behaviour. We build a framework that leverages the concepts of both static cache analysis and dynamic test generation to systematically compute the cache-performance stressing test inputs. Our framework computes a test-suite which does not contain any false positives. This means that each element in the test-suite points to a real cache performance issue. Moreover, our test generation framework provides an assurance of the test coverage via a well-formed coverage metric. We have implemented our entire framework using Chronos worst case execution time (WCET) analyzer and LLVM compiler infrastructure. Several experiments suggest that our test generation framework quickly converges towards generating cache-performance stressing test cases. We also show the application of our generated test-suite in design space exploration and cache performance optimization.","1052-8725","978-1-4799-2006","10.1109/RTSS.2013.39","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728886","Performance testing;Test generation;Cache performance","Instruments;Embedded software;Testing;Performance analysis;Cache memory;Abstracts","cache storage;embedded systems;program compilers;program diagnostics;software performance evaluation","anomalous cache behaviour;static cache analysis;dynamic test generation;test-suite;test coverage;coverage metric;Chronos worst case execution time;WCET analyzer;LLVM compiler infrastructure;cache-performance stressing test cases;design space exploration;cache performance optimization;real-time embedded software","","4","19","","","","","","IEEE","IEEE Conferences"
"Generation of Single Input Change Test Sequences for Conformance Test of Programmable Logic Controllers","J. Provost; J. Roussel; J. Faure","Assistant Professorship for Safe Embedded Systems, Technische Universität München, Garching bei M&#x00FC;nchen, Germany; ENS Cachan, LURPA, Cachan, France; ENS Cachan, LURPA, Cachan, France","IEEE Transactions on Industrial Informatics","","2014","10","3","1696","1704","Conformance test is a functional test technique which is aiming to check whether an implementation, seen as a black-box with inputs/outputs, conforms to its specification. Numerous theoretical worthwhile results have been obtained in the domain of conformance test of finite state machines. The optimization criterion, which is usually selected to build the test sequence, is the minimum-length criterion. Based on experimental results, this paper focuses on the generation of a single input change (SIC) test sequence from a specification model represented as a Mealy machine; such a sequence is aiming at preventing from erroneous test verdicts due to incorrect detection of synchronous input changes by the programmable logic controller (PLC) under test. A method based on symbolic calculus to obtain the part of the specification that can be tested with a SIC sequence is first presented. Then, an algorithm to build the SIC test sequence is detailed; three solutions are proposed, according to the connectivity properties of the SIC-testable part.","1551-3203;1941-0050","","10.1109/TII.2014.2315972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784098","Conformance test;formal methods;Mealy machine;programmable logic controller (PLC);single input change (SIC);test sequence;test verdict","Silicon carbide;Informatics;IEC standards;Input variables;Calculus;Software;Circuit faults","conformance testing;finite state machines;programmable controllers","single input change test sequences generation;programmable logic controller conformance test;finite state machines;optimization criterion;minimum-length criterion;SIC test sequence generation;specification model;Mealy machine;PLC;symbolic calculus","","13","42","","","","","","IEEE","IEEE Journals & Magazines"
"Hardware and software performance of image processing applications on reconfigurable systems","A. Mishra; M. Agarwal; K. S. Raju","Electrical & Electronics Group, Birla Institute of Technology & Science, Pilani, Rajasthan, India; Electrical & Electronics Group, Birla Institute of Technology & Science, Pilani, Rajasthan, India; Digital System Group, CEERI, Pilani, Rajasthan, India","2015 Annual IEEE India Conference (INDICON)","","2015","","","1","5","Field Programmable Gate Arrays (FPGAs) have been extensively used in accelerating applications in many digital domains, examples include image and signal processing. These applications have been abundantly tested in high level languages like C, C++ and Matlab programming. Many standard libraries exist for image processing applications like OpenCV for end to end solutions. Applications centered around these libraries when implemented on embedded platforms like ARM or Power PC consumes considerable amount of processing time. In the last decade, these applications have been heavily tested on FPGAs as hardware (HW) for higher performance gain. Many optimizations and architectures have been proposed in this area examples are parallelism extraction, operation scheduling, pipelining, loop unrolling etc.. In this paper, we present a combination of optimizations and architecture for image processing applications on FPGAs. For software (SW), LLVM compiler has been used for applying optimizations and finding SW execution time on the Ubuntu machine. For HW generation and optimization, Vivado-HLS has been used and tested with four filters on Virtex-5 ML507 kit. The result shows the performance comparison of four C programs as software/hardware with respect to time and resource consumption.","2325-9418","978-1-4673-7399-9978-1-4673-7398","10.1109/INDICON.2015.7443611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7443611","Hardware;Software;Vivado;Optimizations;High Level Synthesis;Field programmable gate arrays","Optimization;Field programmable gate arrays;Parallel processing;Clocks;Software;Image processing;Algorithm design and analysis","C++ language;field programmable gate arrays;image processing;optimising compilers;reconfigurable architectures","image processing;reconfigurable system;field programmable gate arrays;FPGA;high level language;C program;C++;Matlab programming;OpenCV;ARM;LLVM compiler;Ubuntu machine;Vivado-HLS","","1","11","","","","","","IEEE","IEEE Conferences"
"Optimal release time determination for multi upgradation SRGM with faults of different severity using genetic algorithm","A. Tickoo; P. K. Kapur; S. K. Khatri; A. K. Verma","Amity School of Engineering, Amity University Noida UP, India; Amity Center for Interdisciplinary Research, Amity University Noida UP, India; Amity Institute of Information Technology, Amity University Noida UP, India; Haugesund/Stord University College, Haugesund/Stord University, Norway","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","As the competition among the software companies is increasing, firms have become more demand driven. Advancements in technology and up gradations have become important force in software markets today. By adding new features company aims to improve the quality of the software in terms of performance and functionality. But these new features cause an increase in the fault content and hence the complexity of the software. Therefore the testing team needs to keep track of these faults along with the faults that have been left from the previous release. In the proposed model we have developed a multi up gradation software reliability model upon classifying the faults into three different types depending on the effort and time consumed for their removal: simple, hard and complex faults. In this paper optimization equation for cost model is described to determine the optimal release time in multi up-gradation of software, here we have also taken into account the undetected faults from the previous release. We have used Genetic algorithm approach for optimization. The validation of the proposed model was done on real life data set for four releases of software.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359322","Genetic Algorithm (GA);Software Reliability Growth Model (SRGM);Non-homogenous Poisson Process (NHPP);Fault Severity;Multi-up gradation","Testing;Software;Mathematical model;Debugging;Software reliability;Computer bugs;Genetic algorithms","genetic algorithms;software quality;software reliability","optimal release time determination;multiupgradation SRGM;software market;software quality;fault content;gradation software reliability model;optimization equation;genetic algorithm approach","","","24","","","","","","IEEE","IEEE Conferences"
"JST: An automatic test generation tool for industrial Java applications with strings","I. Ghosh; N. Shafiei; G. Li; W. Chiang","Software Systems Innovation Group, Fujitsu Laboratories of America, Sunnyvale, CA, USA; Department of Computer Science and Engineering, York University, Toronto, ON, Canada; Software Systems Innovation Group, Fujitsu Laboratories of America, Sunnyvale, CA, USA; School of Computing, University of Utah, Salt Lake City, UT, USA","2013 35th International Conference on Software Engineering (ICSE)","","2013","","","992","1001","In this paper we present JST, a tool that automatically generates a high coverage test suite for industrial strength Java applications. This tool uses a numeric-string hybrid symbolic execution engine at its core which is based on the Symbolic Java PathFinder platform. However, in order to make the tool applicable to industrial applications the existing generic platform had to be enhanced in numerous ways that we describe in this paper. The JST tool consists of newly supported essential Java library components and widely used data structures; novel solving techniques for string constraints, regular expressions, and their interactions with integer and floating point numbers; and key optimizations that make the tool more efficient. We present a methodology to seamlessly integrate the features mentioned above to make the tool scalable to industrial applications that are beyond the reach of the original platform in terms of both applicability and performance. We also present extensive experimental data to illustrate the effectiveness of our tool.","0270-5257;1558-1225","978-1-4673-3076-3978-1-4673-3073","10.1109/ICSE.2013.6606649","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606649","","Automata;Java;Libraries;Concrete;Testing;Numerical models;Semantics","Java;program testing;software libraries;software tools;string matching","automatic test generation tool;industrial Java applications;high coverage test suite;industrial strength Java applications;numeric-string hybrid symbolic execution engine;symbolic Java PathFinder platform;JST tool;Java library components;data structures;string constraints;regular expressions;integer numbers;floating point numbers","","11","24","","","","","","IEEE","IEEE Conferences"
"When to stop testing under warranty using SRGM with change-point","P. K. Kapur; S. K. Khatri; O. Singh; A. K. Shrivastava","Amity International Business School, Amity University, Noida, India; Amity Institute of Information Technology, Amity University, Noida, India; Department of Operational Research, University of Delhi, New Delhi, India; Department of Operational Research, University of Delhi, New Delhi, India","2014 Conference on IT in Business, Industry and Government (CSIBIG)","","2014","","","1","7","When a vendor sells a product, there is usually an implied promise that a product will do something. Under the law, when you buy the product, you can reasonably expect that the product will perform as promised. When a product is purchased and used by a customer then it might be possible that a difference between the product performance and customer expectation crops in. As a result the insurance policy of warranty comes into the picture. A warranty is simply a formal promise by a vendor that the product is defect free, meaning that it will do what it promises to do, and that if it fails to do so, then vendor will go about rectifying defects. But servicing a warranty engages additional costs to the manufacturer and this has an effect on the profit levels. Warranty cost may be reduced by providing higher reliable product. The conflict existing in the context of software reliability, warranty period and selling price need to be considered jointly. In this paper, we formulated an optimization problem that determines the optimal testing time and price with change-point under with warranty. The factors like fixed cost of testing and debugging during testing phase and warranty period cost of testing up to the release time considered in the problem of maximization of the profit.","","978-1-4799-3064-7978-1-4799-3063-0978-1-4799-3062","10.1109/CSIBIG.2014.7056961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056961","Software Reliability Growth Model (SRGM);Release Time;Change-Point;Warranty;Profit","Software;Testing;Lead;IEL","cost reduction;optimisation;program debugging;program testing;software reliability;warranties","stop testing;SRGM;change-point;customer expectation;product performance;insurance policy;profit levels;warranty cost reduction;selling price;warranty period;optimization problem;optimal testing time;software reliability growth model","","2","20","","","","","","IEEE","IEEE Conferences"
"Search-Based Synthesis of Probabilistic Models for Quality-of-Service Software Engineering (T)","S. Gerasimou; G. Tamburrelli; R. Calinescu","NA; NA; NA","2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2015","","","319","330","The formal verification of finite-state probabilistic models supports the engineering of software with strict quality-of-service (QoS) requirements. However, its use in software design is currently a tedious process of manual multiobjective optimisation. Software designers must build and verify probabilistic models for numerous alternative architectures and instantiations of the system parameters. When successful, they end up with feasible but often suboptimal models. The EvoChecker search-based software engineering approach and tool introduced in our paper employ multiobjective optimisation genetic algorithms to automate this process and considerably improve its outcome. We evaluate EvoChecker for six variants of two software systems from the domains of dynamic power management and foreign exchange trading. These systems are characterised by different types of design parameters and QoS requirements, and their design spaces comprise between 2E+14 and 7.22E+86 relevant alternative designs. Our results provide strong evidence that EvoChecker significantly outperforms the current practice and yields actionable insights for software designers.","","978-1-5090-0025-8978-1-5090-0024","10.1109/ASE.2015.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372021","Probabilistic Model Checking;Model Synthesis;Genetic Algorithms;Search-Based Software Engineering;Model Repair","Probabilistic logic;Quality of service;Software systems;Markov processes;Optimization;Software engineering","genetic algorithms;probability;program testing;program verification;quality of service","search-based synthesis;probabilistic models;quality-of-service software engineering;formal verification;finite-state probabilistic models;quality-of-service requirements;software design;suboptimal models;EvoChecker search-based software engineering approach;multiobjective optimisation genetic algorithms;dynamic power management;foreign exchange trading;design parameters;QoS requirements;design space","","8","68","","","","","","IEEE","IEEE Conferences"
"Testing the cooperation of autonomous robotic agents","R. Lill; F. Saglietti","Informatik 11 - Software Engineering, Friedrich-Alexander-Universit&#x00E4;t Erlangen-N&#x00FC;rnberg, Martensstr. 3, 91058, Germany; Informatik 11 - Software Engineering, Friedrich-Alexander-Universit&#x00E4;t Erlangen-N&#x00FC;rnberg, Martensstr. 3, 91058, Germany","2014 9th International Conference on Software Engineering and Applications (ICSOFT-EA)","","2014","","","287","296","This article proposes an approach to testing the cooperative behaviour of autonomous software-based agents with safety-relevant tasks. It includes the definition of different model-based testing criteria based on the coverage of Coloured Petri Net entities as well as the automatic generation of appropriate test cases. The multi-objective optimization problem considered addresses both the maximization of interaction coverage and the minimization of the amount of test cases required. The approach developed for its solution makes use of genetic algorithms. The resulting automatic test case generation process is presented in this article together with the experiences gained by applying it to cooperating autonomous forklifts.","","978-9-8975-8124-3978-1-4799-7688","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293874","Autonomous Systems;Coloured Petri Nets;Genetic Algorithms;Optimized Test Case Generation;Structural Testing","Testing;Robots;Petri nets;Image color analysis;Sociology;Statistics;Optimization","","","","","18","","","","","","IEEE","IEEE Conferences"
"A Novel Quantum-Behaved Particle Swarm Optimization Algorithm","J. Zhao; H. Liu","NA; NA","2015 14th International Symposium on Distributed Computing and Applications for Business Engineering and Science (DCABES)","","2015","","","94","97","A novel Quantum-behaved Particle Swarm Optimization algorithm with probability (P-QPSO) is introduced to improve the global convergence property of QPSO. In the proposed algorithm, all the particles keep the original evolution with large probability, and do not update the position of particles with small probability, and re-initialize the position of particles with small probability. Seven benchmark functions are used to test the performance of P-QPSO. The results of experiment show that the proposed technique can increase diversity of population and converge more rapidly than other evolutionary computation methods.","","978-1-4673-6593-2978-1-4673-6592","10.1109/DCABES.2015.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429565","particle swarm optimization algorithm;quantum-behaved;probability;benchmark function","Convergence;Sociology;Statistics;Optimization;Particle swarm optimization;Software algorithms;Benchmark testing","particle swarm optimisation;probability;quantum computing","quantum-behaved particle swarm optimization algorithm;probability;P-QPSO;global convergence property;evolutionary computation methods","","","8","","","","","","IEEE","IEEE Conferences"
"Parameter Tuning for Search-Based Test-Data Generation Revisited: Support for Previous Results","A. Kotelyanskii; G. M. Kapfhammer","NA; NA","2014 14th International Conference on Quality Software","","2014","","","79","84","Although search-based test-data generators, like EvoSuite, efficiently and automatically create effective JUnit test suites for Java classes, these tools are often difficult to configure. Prior work by Arcuri and Fraser revealed that the tuning of EvoSuite with response surface methodology (RSM) yielded a configuration of the test data generator that did not outperform the default configuration. Following the experimental design and protocol described by Arcuri and Fraser, this paper presents the results of a study that lends further support to prior results: like RSM, the EvoSuite configuration identified by the well-known Sequential Parameter Optimization Toolbox (SPOT) failed to significantly outperform the default settings. Although this result is negative, it furnishes further empirical evidence of the challenge associated with tuning a complex search-based test data generator. Moreover, the outcomes of the presented experiments also suggests that EvoSuite's default parameters have been set by experts in the field and are thus suitable for use in future experimental studies and industrial testing efforts.","1550-6002;2332-662X","978-1-4799-7198-5978-1-4799-7197","10.1109/QSIC.2014.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958390","parameter tuning;test data generation;empirical studies","Tuning;Java;Sociology;Statistics;Testing;Measurement;Generators","optimisation;program testing;response surface methodology;software tools","parameter tuning;search-based test-data generation;EvoSuite;JUnit test suites;Java classes;response surface methodology;RSM;experimental design;sequential parameter optimization toolbox;SPOT","","2","24","","","","","","IEEE","IEEE Conferences"
"EATBit: Effective automated test for binary translation with high code coverage","H. Guo; Z. Wang; C. Wu; R. He","State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences; Department of Computer Science and Engineering, University of California, San Diego","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","6","Binary translation makes it convenient to emulate one instruction set by another. Nowadays, it is growing in popularity in various applications, especially the embedded platforms. When it comes to the test of binary translators, traditional methodologies which still mainly rely on manual unit test is costly, labor intensive and often not adequate to test complicated algorithms in the translators. Some standard benchmark suites, like SPEC CPU2006, are compiled with different compilation options for further tests. However, the translation modules still have over 30% of their code unexecuted after such tests, according to our experimental results. Methodologies based on randomization can generate a vast variety of tests, thus improve the code coverage in the translation system. In this paper, we propose such an approach named EATBit. Test binaries are generated with randomly selected instructions and operands. The binaries and a large amount of input data are then refined to exclude invalid ones. Experimental results on a real binary translator demonstrate that EATBit can not only improve code coverage by over 20%, but also find some new bugs in the translator successfully.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.097","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800298","","Optimization;Registers;Wrapping;Computer bugs;Program processors;Benchmark testing","automatic test software;instruction sets;program interpreters;program testing","EATBit approach;effective automated test;binary translation system;high code coverage;instruction set;manual unit test;test complicated algorithms;standard benchmark suites;SPEC CPU2006;test binaries;embedded platforms","","","19","","","","","","IEEE","IEEE Conferences"
"Re-Using Generators of Complex Test Data","S. Poulding; R. Feldt","NA; NA","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","","2015","","","1","10","The efficiency of random testing can be improved by sampling test inputs using a generating program that incorporates knowledge about the types of input most likely to detect faults in the software-under-test (SUT). But when the input of the SUT is a complex data type--such as a domain-specific string, array, record, tree, or graph--creating such a generator may be time- consuming and may require the tester to have substantial prior experience of the domain. In this paper we propose the re-use of generators created for one SUT on other SUTs that take the same complex data type as input. The re-use of a generator in this way would have little overhead, and we hypothesise that the re-used generator will typically be as least as efficient as the most straightforward form of random testing: sampling test inputs from the uniform distribution. We investigate this proposal for two data types using five generators. We assess test efficiency against seven real-world SUTs, and in terms of both structural coverage and the detection of seeded faults. The results support the re-use of generators for complex data types, and suggest that if a library of generators is to be maintained for this purpose, it is possible to extend library generators to accommodate the specific testing requirements of newly-encountered SUTs.","2159-4848","978-1-4799-7125","10.1109/ICST.2015.7102605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102605","","Generators;Testing;Libraries;Probability distribution;Optimization;Proposals;Data models","data handling;fault tolerant computing;program testing","test data generator reuse;random testing;fault detection;software-under-test;SUT;test efficiency;data type;structural coverage;seeded fault detection;library generators","","","13","","","","","","IEEE","IEEE Conferences"
"Parallelized genetic operations for SBST using Hadoop MapReduce framework","G. Mayandi; C. Arumugam","Department of Computer Science and Engineering, SSN College of Engineering, Chennai, India; Department of Computer Science and Engineering, SSN College of Engineering, Chennai, India","2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies","","2014","","","1686","1691","Search Based Software Testing (SBST) is one of the most explored fields in Software Testing. It suffers from the optimization problem to execute the Software Under Test (SUT). This problem is addressed mostly using Genetic Algorithm (GA) and it involves three operations namely selection, crossover and mutation to accomplish a global search to yield fitness solution to run the SUT successfully. In existing work, GA is combined with Hadoop MapReduce to give Parallel Genetic Algorithm (PGA). Here, mapper function performs parallel fitness computation and reducer function performs the GA. This PGA generates test suite that makes the entire SUT to get executed. This paper makes an attempt to existing by parallelizing fitness calculation and GA operations to generate search test data for the SUT based on branch coverage criteria.","","978-1-4799-3914-5978-1-4799-3913","10.1109/ICACCCT.2014.7019396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019396","Genetic Algorithm;Parallel Genetic Algorithm;Search Based Software Testing;Test Data;Software Under Test","Optimization","data handling;genetic algorithms;parallel algorithms;program testing","parallelized genetic operations;SBST;Hadoop MapReduce framework;software testing;software under test;SUT;genetic algorithm;selection operations;crossover operations;mutation operations;parallel genetic algorithm;mapper function;parallel fitness computation;PGA;test suite;GA operations;branch coverage criteria","","","13","","","","","","IEEE","IEEE Conferences"
"Releasing sooner or later: An optimization approach and its case study evaluation","Jason Ho; G. Ruhe","Department of Computer Science, University of Calgary, Alberta, Canada; Department of Computer Science, University of Calgary, Alberta, Canada","2013 1st International Workshop on Release Engineering (RELENG)","","2013","","","21","24","Decisions about the release date need to balance between the degree of readiness (quality) of the product and the potential competitive advantage and added value of (early) delivery. Based on an existing optimization approach for solving the maximum value release planning problem for a fixed release time, we provide a re-optimization approach for which includes local and global re-planning exchange operations to determine the outcome of varying the release dates. The proposed method determines a set of trade-off solutions related to (i) the value of the features implemented, (ii) their estimated quality, and (iii) the release time. The approach is evaluated by a real-world case study taken from an ongoing project which is to develop the new main release 2.0 for the release planning decision support platform ReleasePlanner™ v1.6.","","978-1-4673-6441","10.1109/RELENG.2013.6607692","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607692","Release engineering;when-to-release;decision support;software quality;case study","Testing;Planning;Software;Optimization;Software reliability;Business","optimisation;product quality;production planning","optimization approach;product quality;early delivery;maximum value release planning problem;reoptimization approach;global replanning exchange operations;local replanning exchange operations;release planning decision support platform ReleasePlanner v1.6","","","13","","","","","","IEEE","IEEE Conferences"
"Design and deployment of OpenStack-SDN based test-bed for EDoS","P. Singh; S. Manickam","National Advanced IPv6 Centre (NAv6), Universiti Sains Malaysia, Penang, Malaysia; National Advanced IPv6 Centre (NAv6), Universiti Sains Malaysia, Penang, Malaysia","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","5","High fidelity experimental facilities play an important role in evaluating new technologies such as cloud computing and software defined network (SDN). In this paper, we highlight how OpenDaylight can be integrated with OpenStack to provide a powerful SDN-based networking solution for OpenStack Clouds. It provides practical application of the future network standards leveraging SDN technology. We will discuss the important elements of designing and implementing OpenStack-SDN testbed for virtual networks that integrates additional capabilities compared to existing SDN testbeds. We will also provide an overview of setting up the testbed with the necessary hardware and components required to build this testbed.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359327","EDoS;SDN;DDoS;OpenStack;Test Bed","Cloud computing;Computer architecture;Monitoring;Random access memory;Servers;Control systems;Computer crime","cloud computing;computer network security;open systems;software defined networking;virtual private networks","OpenStack-SDN based test-bed;EDoS;cloud computing;software defined network;OpenDaylight;SDN-based networking solution;OpenStack clouds;SDN technology;virtual networks","","1","24","","","","","","IEEE","IEEE Conferences"
"Adaptive Multiobjective Particle Swarm Optimization Based on Parallel Cell Coordinate System","W. Hu; G. G. Yen","School of Information and Software Engineering, University of Electronic Science and Technology of China, Chengdu, Sichuan, China; School of Electrical and Computer Engineering, Oklahoma State University, Stillwater, OK, USA","IEEE Transactions on Evolutionary Computation","","2015","19","1","1","18","Managing convergence and diversity is essential in the design of multiobjective particle swarm optimization (MOPSO) in search of an accurate and well distributed approximation of the true Pareto-optimal front. Largely due to its fast convergence, particle swarm optimization incurs a rapid loss of diversity during the evolutionary process. Many mechanisms have been proposed in existing MOPSOs in terms of leader selection, archive maintenance, and perturbation to tackle this deficiency. However, few MOPSOs are designed to dynamically adjust the balance in exploration and exploitation according to the feedback information detected from the evolutionary environment. In this paper, a novel method, named parallel cell coordinate system (PCCS), is proposed to assess the evolutionary environment including density, rank, and diversity indicators based on the measurements of parallel cell distance, potential, and distribution entropy, respectively. Based on PCCS, strategies proposed for selecting global best and personal best, maintaining archive, adjusting flight parameters, and perturbing stagnation are integrated into a self-adaptive MOPSO (pccsAMOPSO). The comparative experimental results show that the proposed pccsAMOPSO outperforms the other eight state-of-the-art competitors on ZDT and DTLZ test suites in terms of the chosen performance metrics. An additional experiment for density estimation in MOPSO illustrates that the performance of PCCS is superior to that of adaptive grid and crowding distance in terms of convergence and diversity.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2013.2296151","Fundamental Research Funds for the Central Universities; China Scholarship Council for financial support; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6692894","Particle swarm optimization (PSO);multiobjective particle swarm optimization (MOPSO);multi-objective problem (MOP);parallel cell coordinate system, adaptive parameter;Adaptive parameter;multiobjective particle swarm optimization;multiobjective problem;parallel cell coordinate system;particle swarm optimization","Convergence;Sociology;Statistics;Particle swarm optimization;Hypercubes;Estimation;Optimization","evolutionary computation;Pareto optimisation;particle swarm optimisation","adaptive multiobjective particle swarm optimization;parallel cell coordinate system;convergence management;diversity management;Pareto-optimal front;leader selection;archive maintenance;perturbation;PCCS;evolutionary environment;self-adaptive MOPSO;density estimation;crowding distance;adaptive grid","","62","70","","","","","","IEEE","IEEE Journals & Magazines"
"Research and optimal design for MVB baseband transmission system","J. Song; F. Xu; F. Zhou; X. Gong","Industrial Control System Reliability Testing Center, China Software Testing Center, Beijing, China; Industrial Control System Reliability Testing Center, China Software Testing Center, Beijing, China; Industrial Control System Reliability Testing Center, China Software Testing Center, Beijing, China; Industrial Control System Reliability Testing Center, China Software Testing Center, Beijing, China","2013 IEEE International Conference on Intelligent Rail Transportation Proceedings","","2013","","","42","47","Regarding frequency band allocation problem faced with the channel multiplexing when combination of train control network and information network, according to the original definition of random process power spectrum, a mathematical model of MVB code is established and a MVB frame head frequency spectrum function is deduced, and different MVB frames power spectrum are calculated and simulated. Then using a design of band-limited filter which can ensure normal safe operation of the baseband transmission system, at the same time which can reduces interference of the baseband signal on the high frequency carrier signal, thus optimizing the bandwidth of the MVB baseband transmission system. Finally, verify the effectiveness of the design through simulation analysis and experimental testing.","","978-1-4673-5277-2978-1-4673-5278-9978-1-4673-5276","10.1109/ICIRT.2013.6696265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696265","MVB;Baseband transmission system;Power spectrum;Band-limited filter","","filtering theory;interference suppression;mathematical analysis;multiplexing;railway communication","MVB baseband transmission;optimal design;frequency band allocation problem;channel multiplexing;train control network;information network;random process power spectrum;mathematical model;MVB code;MVB frame head frequency spectrum function;band-limited filter;safe operation;baseband transmission system;interference;baseband signal;high frequency carrier signal;simulation analysis;experimental testing","","","16","","","","","","IEEE","IEEE Conferences"
"PSO techniques for optimal power flow. Parameters tuning by mathematical test functions","D. Cristian; A. Simo; C. Barbulescu; S. Kilyeni; F. Solomonesc; M. Cornoiu","Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara, Timisoara, Romania; Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara, Timisoara, Romania; Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara, Timisoara, Romania; Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara, Timisoara, Romania; Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara, Timisoara, Romania; Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara, Timisoara, Romania","2013 IEEE Grenoble Conference","","2013","","","1","6","The paper is focusing on Particle Swarm Optimization (PSO) algorithm. Several variants of the PSO algorithm are studied. To evaluate their performances a software tool has been developed in Matlab environment. Three reference mathematical test functions have been used for this purpose. This paper represents a necessary step requested by the optimal power flow (OPF) computing approach. Currently the authors are focusing on developing a PSO based OPF computing algorithm. The most suitable PSO algorithm variant is provided based on the analyses within the current paper.","","978-1-4673-5669","10.1109/PTC.2013.6652258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6652258","particle swarm optimization;unified version;optimal power flow;mathematical test functions","Aerospace electronics;Software algorithms;Standards;Software tools;Particle swarm optimization;Load flow;Algorithm design and analysis","load flow;particle swarm optimisation;power engineering computing;software tools","PSO technique;optimal power flow;parameter tuning;mathematical test function;particle swarm optimization;software tool;Matlab environment;OPF computing algorithm","","1","18","","","","","","IEEE","IEEE Conferences"
"Entropy-based test generation for improved fault localization","J. Campos; R. Abreu; G. Fraser; M. d'Amorim","Faculty of Engineering of University of Porto, Portugal; Faculty of Engineering of University of Porto, Portugal; University of Sheffield, United Kingdom; Federal University of Pernambuco, Brazil","2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2013","","","257","267","Spectrum-based Bayesian reasoning can effectively rank candidate fault locations based on passing/failing test cases, but the diagnostic quality highly depends on the size and diversity of the underlying test suite. As test suites in practice often do not exhibit the necessary properties, we present a technique to extend existing test suites with new test cases that optimize the diagnostic quality. We apply probability theory concepts to guide test case generation using entropy, such that the amount of uncertainty in the diagnostic ranking is minimized. Our ENTBUG prototype extends the search-based test generation tool EVOSUITE to use entropy in the fitness function of its underlying genetic algorithm, and we applied it to seven real faults. Empirical results show that our approach reduces the entropy of the diagnostic ranking by 49% on average (compared to using the original test suite), leading to a 91% average reduction of diagnosis candidates needed to inspect to find the true faulty one.","","978-1-4799-0215","10.1109/ASE.2013.6693085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693085","Fault localization;Test case generation","Entropy;Cognition;Uncertainty;Debugging;Genetic algorithms;Sociology;Statistics","belief networks;entropy;inference mechanisms;probability;program diagnostics;program testing;software fault tolerance","fitness function;EVOSUITE;search-based test generation tool;ENTBUG prototype;diagnostic ranking;entropy;test case generation;probability theory concepts;diagnostic quality;test suite;diagnostic quality;passing-failing test cases;spectrum-based Bayesian reasoning;improved fault localization;entropy-based test generation","","17","40","","","","","","IEEE","IEEE Conferences"
"Quality assurance for Product development using Agile","A. Agarwal; N. K. Garg; A. Jain","R Systems International Limited, Noida, India; R Systems International Limited, Noida, India; R Systems International Limited, Noida, India","2014 International Conference on Reliability Optimization and Information Technology (ICROIT)","","2014","","","44","47","Achieving the highest level of quality for the software product being delivered is the goal of any IT organization. Every organization wants to implement processes and practices that would help achieving this goal of increasing the quality of a software product. There are so many models available today that an organization can use for developing a software but considering the dynamics of today s world where technology is changing at a fast pace and innovative products are hitting the market at great speed, these organizations do not want a software development model that consume lot of time and efforts, and hence most of the business houses are moving towards Agile approach for software development. With this change in approach for software development, the big question is that how to ensure Quality of product developed using Agile model. The traditional approach has a separate phase for testing a software product which ensures that an independent team has validated the product per specified requirements. However, with Agile way of software development, this leverage of involvement of independent test teams and test levels has taken a back. This paper, will highlight broadly the role of QA within Agile development model, with focus on fresh thoughts and approaches to improve the overall quality of product developed using Agile methodology. The use and importance of Metrics for accessing the Quality within Agile model will also be discussed.","","978-1-4799-2995-5978-1-4799-3958-9978-1-4799-2996","10.1109/ICROIT.2014.6798281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798281","Quality;Agile;Development Model","Testing;Measurement;Software;Concrete;Companies","product development;quality assurance;software quality","quality assurance;product development;software product quality;IT organization;business houses;software development;independent test teams;test levels;QA;agile development model","","2","6","","","","","","IEEE","IEEE Conferences"
"Practical semantic test simplification","S. Zhang","University of Washington, USA","2013 35th International Conference on Software Engineering (ICSE)","","2013","","","1173","1176","We present a technique that simplifies tests at the semantic level. We first formalize the semantic test simplification problem, and prove it is NP-hard. Then, we propose a heuristic algorithm, SimpleTest, that automatically transforms a test into a simpler test, while still preserving a given property. The key insight of SimpleTest is to reconstruct an executable and simpler test that exhibits the given property from the original one. Our preliminary study on 7 real-world programs showed the usefulness of SimpleTest.","0270-5257;1558-1225","978-1-4673-3076-3978-1-4673-3073","10.1109/ICSE.2013.6606671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606671","","Debugging;Input variables;Semantics;Computer bugs;Testing;Java;Indexes","optimisation;program debugging;program testing","practical semantic test simplification problem;NP-hard problem;heuristic algorithm;SimpleTest;real-world programs","","1","11","","","","","","IEEE","IEEE Conferences"
"FPGA Implementation of Software Defined Radio-Based Flight Termination System","A. R. Panda; D. Mishra; H. K. Ratha","Department of Computer Data Processing, Integrated Test Range, Defence Research, and Development Organization, New Delhi, India; Department of Computer Science and Engineering, Siksha “O” Anusandhan University, Bhubaneshwar, India; Integrated Test Range, Defence Research, and Development Organization, New Delhi, India","IEEE Transactions on Industrial Informatics","","2015","11","1","74","82","This paper proposes a field-programmable gate array (FPGA)-based software defined radio (SDR) implemented flight termination system (FTS). This is purely a new kind of implementation of digital FTS in SDR platform. The applied design procedure replaces a multiple platform-based system with a single platform. It also guarantees reconfigurable, interoperable, portable, and handy FTS, and maintains errorless, bug free, and reliable implementation. Real-time flight termination operation demands a very highly reliable and ruggedized platform. Hence, the FTS is implemented in FPGA. In order to minimize hardware resources and to enable future upgradation, efficient optimization technique has been applied. LabVIEW, a high-level programming language is used to simulate and implement the system in real time and enables rapid prototyping. The system was validated at subsystems level by measurements of different parameters in various intermediate stages of processing, and further was validated as an integrated system at real-time telecommunication operation environment.","1551-3203;1941-0050","","10.1109/TII.2014.2364557","Defence Research and Development Organization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935091","Field-programmable gate array (FPGA);flight termination system (FTS);real-time system;software defined radio (SDR);Field-programmable gate array (FPGA);flight termination system (FTS);real-time system;software defined radio (SDR)","Field programmable gate arrays;Real-time systems;Frequency modulation;Software;Vehicles;Receiving antennas","aerospace engineering;field programmable gate arrays;software radio;virtual instrumentation","FPGA based SDR implementation;software defined radio-based flight termination system;field-programmable gate array based software defined radio;digital FTS;real-time flight termination operation;efficient optimization technique;high-level programming language;LabVIEW","","5","37","","","","","","IEEE","IEEE Journals & Magazines"
"MOGA-UDTG: Automated uniformly distributed testing approach","K. Choudhary; G. N. Purohit","Dept. of CSE &amp; IT, ITM University, Gurgaon, India; Dept. of AIM &amp; ACT, Banasthali Vidyapeeth, Banasthali, India","2014 Recent Advances in Engineering and Computational Sciences (RAECS)","","2014","","","1","5","Software Testing is the critical task for any software which helps in building trust and confidence in the product. Usually, the emphasis is on Manual Testing but to reduce cost there is a need of automation in software testing process. This paper proposes a method to generate automated uniformly distributed test cases based on the platform of multi-objective genetic algorithm (MOGA). The proposed method is analyzed on the domain of Pareto-Optimality, Dominance and Multi-objective Genetic Algorithm.","","978-1-4799-2291-8978-1-4799-2290","10.1109/RAECS.2014.6799540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799540","Multi-objective Genetic Algorithm;MOGA;UDTG;Pareto Optimal;Non-dominated Sorting;Fitness function etc.","Genetic algorithms;Pareto optimization;Evolutionary computation;Sociology;Complexity theory","genetic algorithms;Pareto optimisation;program testing","MOGA-UDTG;automated uniformly distributed testing;software testing;manual testing;Pareto-optimality;dominance genetic algorithm;multiobjective genetic algorithm","","1","10","","","","","","IEEE","IEEE Conferences"
"A fast hybrid optimization algorithm based on TS and PSO for circles packing problem with the equilibrium constraints","K. Lei","Intelligent Software and Software Engineering Laboratory Southwest University Chongqing, 400715, China","2014 IEEE 5th International Conference on Software Engineering and Service Science","","2014","","","314","317","The packing problem with the behavioral constraints is difficult to solve due to its NP-hard nature. Tabu search (TS) has strong global search ability but the convergence accuracy is low. particle swarm optimization (PSO) is quick in convergence, but likely to be premature at the initial stage. Considering both the advantages and disadvantages, a fast hybrid optimization algorithm based on improved TS and PSO for this problem is proposed, which employ the novel intensification search and diversification search balance strategy of TS and the refined search of PSO as a whole to plan large-scale space global search according to the fitness change, and to quicken convergence speed, avoid repeated search work, economize computational expenses, and obtain global optimum. The proposed algorithm is tested and compared it with other published methods on constrained layout examples, demonstrated that the revised algorithm is feasible and efficient.","2327-0594;2327-0586","978-1-4799-3279-5978-1-4799-3278-8978-1-4799-3277","10.1109/ICSESS.2014.6933571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933571","tabu search;particle swarm optimization;premature;constrained layout","Layout;Search problems;Optimization;Convergence;Particle swarm optimization;Algorithm design and analysis;Computers","bin packing;computational complexity;particle swarm optimisation","fast hybrid optimization algorithm;TS;PSO;circles packing problem;equilibrium constraints;NP-hard nature;tabu search;global search ability;particle swarm optimization;intensification search;diversification search balance strategy;large-scale space global search","","","9","","","","","","IEEE","IEEE Conferences"
"Scenario specification based testing model generation","B. Liang; P. Liu; H. Miao","School of Computer Engineering and Science, Shanghai University, Shanghai 200072, China; Shanghai Key Laboratory of Computer Software Testing &amp; Evaluating, Shanghai 201112, China; School of Computer Engineering and Science, Shanghai University, Shanghai 200072, China","2013 IEEE/ACIS 12th International Conference on Computer and Information Science (ICIS)","","2013","","","335","340","Building a simplified model for testing complex software system has been highlighted for optimizing test generation. This paper presents an approach to generating the constrained FSM with the scenario. Firstly, we use FSM to describe the behavior model of the target system. Then, we study the method of modeling scenarios with UML diagrams, including use case diagram, activity diagram, sequence diagram and statechart diagram. We describe how to achieve the constraint process by means of mapping and projection operations between FSM and UML diagrams. Finally, we obtain a reduced FSM by using UML activity diagrams to constrain FSM. The main contribution of the paper is to present an effective modeling method to optimize testing generation from the model.","","978-1-4799-0174","10.1109/ICIS.2013.6607863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607863","FSM;scenario;UML diagrams;mapping;projection","Unified modeling language;Testing;XML;Vectors;Software systems;Semantics","finite state machines;formal specification;program testing;Unified Modeling Language","scenario specification based testing model generation;complex software system testing;test generation optimization;constrained FSM generation;UML diagrams;use case diagram;activity diagram;sequence diagram;statechart diagram;mapping operations;projection operations","","","12","","","","","","IEEE","IEEE Conferences"
"An efficient optimization algorithm for multi-output MPRM circuits with very large number of input variables","D. Bu; J. Jiang","School of Software Engineering, Tongji University, Shanghai, China; School of Software Engineering, Tongji University, Shanghai, China","2014 IEEE 7th Joint International Information Technology and Artificial Intelligence Conference","","2014","","","228","232","By incorporating cube transformation and local transformation, a heuristic algorithm is proposed for MPRM (mixed-polarity Reed-Muller) logic optimization. The proposed algorithm can be applied to multi-output circuits with very large number of input variables, and can adaptively decide to use cube transformation or local transformation during the optimization process. The proposed algorithm is implemented in C language and tested by using several MCNC and IWLS'93 benchmark circuits with many input and output variables. Experimental results show that, in comparison with other algorithms, the proposed algorithm can obtain good optimized results and can significantly improve the time efficiency of MPRM optimization.","","978-1-4799-4419-4978-1-4799-4420-0978-1-4799-4421","10.1109/ITAIC.2014.7065040","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7065040","cube transformation;local transformation;MPRM circuits;logic optimization;heuristic algorithm","Optimization;Algorithm design and analysis;Input variables;Merging;Computers;Heuristic algorithms;Complexity theory","logic circuits;optimisation","mixed-polarity Reed-Muller logic optimization;heuristic algorithm;local transformation;cube transformation;multioutput MPRM circuits","","","10","","","","","","IEEE","IEEE Conferences"
"To optimize mobile HCI information from distributed cognition theory and context-awareness technology","H. Jia; F. Zhao; F. Kong","School of Software, Beijing University of Posts and Telecommunications, Beijing, China; School of Software, Beijing University of Posts and Telecommunications, Beijing, China; Beijing Jiaotong University, Beijing, China","5th IET International Conference on Wireless, Mobile and Multimedia Networks (ICWMMN 2013)","","2013","","","280","283","In view of distributed cognition theory, human action is determined by cognition resource distribution and is relevant to context. In this paper, we put forward the principles of human-computer interaction (HCI) optimization for mobile devices with analysis of user's distributed cognition resources and context of mobile devices. Following the design principles, it is easy for people to implement HCI optimization. We use the principles for optimizing HCI in a storage management system. Thereafter user experience testing is done to illustrate effectiveness of the principles.","","978-1-84919-726","10.1049/cp.2013.2425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6827842","human-computer interaction;cognition resource;mobile devices;optimization","","human computer interaction;mobile computing;optimisation;storage management","mobile HCI information;context-awareness technology;distributed cognition theory;cognition resource distribution;human- computer interaction optimization;mobile device;HCI optimization;storage management system","","","","","","","","","IET","IET Conferences"
"Deriving Usage Model Variants for Model-Based Testing: An Industrial Case Study","H. Samih; H. L. Guen; R. Bogusch; M. Acher; B. Baudry","NA; NA; NA; NA; NA","2014 19th International Conference on Engineering of Complex Computer Systems","","2014","","","77","80","The strong cost pressure of the market and safety issues faced by aerospace industry affect the development. Suppliers are forced to continuously optimize their life-cycle processes to facilitate the development of variants for different customers and shorten time to market. Additionally, industrial safety standards like RTCA/DO-178C require high efforts for testing single products. A suitably organized test process for Product Lines (PL) can meet standards. In this paper, we propose an approach that adopts Model-based Testing (MBT) for PL. Usage models, a widely used MBT formalism that provides automatic test case generation capabilities, are equipped with variability information such that usage model variants can be derived for a given set of features. The approach is integrated in the professional MBT tool MaTeLo. We report on our experience gained from an industrial case study in the aerospace domain.","","978-1-4799-5482-7978-1-4799-5481","10.1109/ICECCS.2014.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923121","Product Line;Model-based Testing;Usage Model;Orthogonal Variability Model;Requirements","Testing;Unified modeling language;Software;Atmospheric modeling;Computational modeling;Markov processes;Aerospace industry","aerospace computing;aerospace industry;aerospace testing;product life cycle management;production engineering computing","usage model variants;model-based testing;cost pressure;aerospace industry;life-cycle processes;industrial safety standards;RTCA/DO-178C;product testing;product lines;PL;automatic test case generation capabilities;MBT tool;MaTeLo;aerospace domain","","1","16","","","","","","IEEE","IEEE Conferences"
"Dynamically Testing GUIs Using Ant Colony Optimization (T)","S. Carino; J. H. Andrews","NA; NA","2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2015","","","138","148","In this paper we introduce a dynamic GUI test generator that incorporates ant colony optimization. We created two ant systems for generating tests. Our first ant system implements the normal ant colony optimization algorithm in order to traverse the GUI and find good event sequences. Our second ant system, called AntQ, implements the antq algorithm that incorporates Q-Learning, which is a behavioral reinforcement learning technique. Both systems use the same fitness function in order to determine good paths through the GUI. Our fitness function looks at the amount of change in the GUI state that each event causes. Events that have a larger impact on the GUI state will be favored in future tests. We compared our two ant systems to random selection. We ran experiments on six subject applications and report on the code coverage and fault finding abilities of all three algorithms.","","978-1-5090-0025-8978-1-5090-0024","10.1109/ASE.2015.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372003","","Graphical user interfaces;Testing;Ant colony optimization;Generators;Context;Heuristic algorithms;Computer architecture","ant colony optimisation;graphical user interfaces;learning (artificial intelligence);program testing","dynamic GUI test generator;ant colony optimization algorithm;AntQ;Q-Learning;behavioral reinforcement learning technique;fitness function;code coverage;fault finding abilities","","2","38","","","","","","IEEE","IEEE Conferences"
"Estimating personalized risk ranking using laboratory test and medical knowledge (UMLS)","M. A. Patil; S. Bhaumik; S. Paul; S. Bissoyi; R. Roy; S. Ryu","Samsung Advanced Institute of Technology, India; Samsung Advanced Institute of Technology, India; Samsung Advanced Institute of Technology, India; Samsung Advanced Institute of Technology, India; Samsung Advanced Institute of Technology, India; Samsung Advanced Institute of Technology, Korea","2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2013","","","1274","1277","In this paper, we introduce a Concept Graph Engine (CG-Engine) that generates patient specific personalized disease ranking based on the laboratory test data. CG-Engine uses the Unified Medical Language System database as medical knowledge base. The CG-Engine consists of two concepts namely, a concept graph and its attributes. The concept graph is a two level tree that starts at a laboratory test root node and ends at a disease node. The attributes of concept graph are: Relation types, Semantic types, Number of Sources and Symmetric Information between nodes. These attributes are used to compute the weight between laboratory tests and diseases. The personalized disease ranking is created by aggregating the weights of all the paths connecting between a particular disease and contributing abnormal laboratory tests. The clinical application of CG-Engine improves physician's throughput as it provides the snapshot view of abnormal laboratory tests as well as a personalized disease ranking.","1094-687X;1558-4615","978-1-4577-0216","10.1109/EMBC.2013.6609740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6609740","Personalized Risk;Concept Graph;Laboratory test;Disease Ranking;UMLS","Laboratories;Diseases;Unified modeling language;Diabetes;Semantics;Liver diseases","diseases;knowledge based systems;knowledge representation;medical information systems;tree data structures","concept graph engine;CG-engine;patient specific personalized disease ranking;laboratory test data;unified medical language system database;medical knowledge;relation-type concept graph;semantic-type concept graph;number-of-source-and-symmetric information;clinical application;UMLS","Algorithms;Body Weight;Databases, Factual;Diabetes Mellitus;Diagnosis, Computer-Assisted;Humans;Liver Diseases;Risk Assessment;Semantics;Software;Unified Medical Language System","","11","","","","","","IEEE","IEEE Conferences"
"Test Scenario Generation for Reliability Tactics from UML Sequence Diagram","X. Qiu; L. Zhang","NA; NA","2014 21st Asia-Pacific Software Engineering Conference","","2014","1","","11","18","In safety-critical systems (e.g. Ship control system, avionics system), various reliability tactics are applied for guaranteeing the safe operation of the system. Therefore, much more attentions during the testing process are expected to be paid on these tactics. However, since developers cannot identify the reliability tactics from the system design effectively, the traditional testing approaches cannot obtain the test scenarios associated with reliability tactics directly. Based on our previous works about the aspect specifications of reliability tactics, we propose an approach for generating the test scenarios associated with reliability tactics from Sequence Diagram of Unified Modeling Language (UML). Owing to the separation of reliability tactics from the base model, the Label Transition Systems (LTSs) of the base and tactic aspect model can be obtained as intermediate models of generation process respectively. Then the action sequences of above LTSs are generated. Following the proposed principles, we can acquire the test scenarios associated with reliability tactics through integrating the action sequences of tactic aspect models with the ones of the base model. An avionics system is used to evaluate the availability of our approach. The results show that 1) our approach can effectively identify the test scenarios associated with reliability tactics from all possible test scenarios of system design, 2) the aspect specifications of reliability tactics can alleviate the input effort of test scenario generation for different functional fragments. We also exemplify the advantage of our approach for supporting the model changes.","1530-1362;1530-1362","978-1-4799-7426-9978-1-4799-7425","10.1109/APSEC.2014.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091285","Reliability Tactics;Aspect-Oriented Modeling;Test Scenario Generation;UML Sequence Diagram;LTS","Unified modeling language;Aerospace electronics;Reliability engineering;Heart beat;Testing;Optimized production technology","avionics;program testing;safety-critical software;software reliability;Unified Modeling Language","test scenario generation;reliability tactics;UML sequence diagram;safety-critical systems;system design;Unified Modeling Language;sequence diagram;label transition systems;LTS;generation process;action sequences;tactic aspect models;avionics system;model changes","","","22","","","","","","IEEE","IEEE Conferences"
"Research on regression test case selection based on improved genetic algorithm","M. Huang; S. Guo; X. Liang; X. Jiao","Software Technology Institute, Dalian Jiao Tong University, Dalian, Liaoning, 116028, China; Mechanical Engineering Institute, Dalian Jiao Tong University, Dalian, Liaoning, 116028, China; Software Technology Institute, Dalian Jiao Tong University, Dalian, Liaoning, 116028, China; School of Management, Dalian University of Technology, Dalian, Liaoning, 116023, China","Proceedings of 2013 3rd International Conference on Computer Science and Network Technology","","2013","","","256","259","In order to get the optimum selection of the test cases, an approach based on an improved genetic algorithm is proposed. Genetic operators of selection, crossover and mutation are improved, which effectively improve the algorithm's global search ability. Experiments are carried out to verify the feasibility of the improved algorithm. The result shows, compared with classical algorithm, the improved algorithm greatly improves the search performance; and it is effective in solving regression test case selection problem.","","978-1-4799-0561","10.1109/ICCSNT.2013.6967108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6967108","Regression Test;Test Case Selection Problem;Genetic Algorithm","Testing;Genetic algorithms;Software;Algorithm design and analysis;Libraries;Sensitivity;Optimization","genetic algorithms;program testing;regression analysis;search problems","regression test case selection;improved genetic algorithm;global search ability","","","19","","","","","","IEEE","IEEE Conferences"
"The impact of parameter adjustment strategies on the performance of particle swarm optimization algorithm","Z. Xun; L. Juelong; X. Jianchun; W. Ping; Y. Qiliang","College of Defense Engineering, PLA University of Science and Technology, Nanjing, 21007, China; College of Defense Engineering, PLA University of Science and Technology, Nanjing, 21007, China; College of Defense Engineering, PLA University of Science and Technology, Nanjing, 21007, China; College of Defense Engineering, PLA University of Science and Technology, Nanjing, 21007, China; College of Defense Engineering, PLA University of Science and Technology, Nanjing, 21007, China","The 27th Chinese Control and Decision Conference (2015 CCDC)","","2015","","","5206","5211","To determine the reasonable parameter settings of particle swarm optimization (PSO) algorithm, this paper discusses the impact of the time-varying inertia weight and velocity-based mutation strategies on the performance of PSO algorithm. The performance of the PSO algorithm with these two kinds of parameters adjustment strategies are tested through four well-known benchmark functions. The simulation results show that the PSO algorithm has better convergence performance with the quickly decreasing inertia weight. Also, the velocity-based mutation strategy will slow down the convergence speed of PSO algorithm if the global solutions over the adjacent generations are close to each other.","1948-9439;1948-9447","978-1-4799-7017-9978-1-4799-7016","10.1109/CCDC.2015.7162853","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7162853","particle swarm optimization;inertia weight;mutation strategy;convergence speed;search precision","Convergence;Benchmark testing;Algorithm design and analysis;Software algorithms;Particle swarm optimization;Sociology;Statistics","particle swarm optimisation","velocity-based mutation strategy;time-varying inertia weight;PSO algorithm;particle swarm optimization algorithm;parameter adjustment strategy","","","12","","","","","","IEEE","IEEE Conferences"
"EvoSuite at the SBST 2013 Tool Competition","G. Fraser; A. Arcuri","NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops","","2013","","","406","409","EvoSuite is a mature research prototype that automatically generates unit tests for Java code. This paper summarizes the results and experiences in participating at the unit testing competition held at SBST 2013, where EvoSuite ranked first with a score of 156.95.","","978-0-7695-4993-4978-1-4799-1324","10.1109/ICSTW.2013.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571661","test case generation;search-based testing;testing classes;search-based software engineering","Conferences;Java;Software testing;Software engineering;Benchmark testing;Instruments","","","","8","9","","","","","","IEEE","IEEE Conferences"
"A vulnerability scanning tool for session management vulnerabilities","R. Lukanta; Y. Asnar; A. I. Kistijantoro","School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia; School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia; School of Electrical Engineering and Informatics, Institut Teknologi Bandung, Indonesia","2014 International Conference on Data and Software Engineering (ICODSE)","","2014","","","1","6","Session management vulnerabilities can be categorized as a group of vulnerability that is still often discovered. Session management vulnerabilities consist of session fixation, CSRF, and insufficient cookies attributes. Based on OWASP Top 10 2013, issues on session management are ranked on 2<sup>nd</sup> place, while CSRF on 8<sup>th</sup>. To detect session management vulnerabilities, we developed a vulnerability scanning tool extending an existing open source tool, namely Nikto. To validate our tool, we have performed two types of testing, which are a functional and a field testing. In functional testing, we created some synthetic test cases to prove all the functionalities can function well. In the field testing, we used some existing projects and we can conclude that Nikto failed to execute some test cases and also found some false negative. The false negative is caused by the error in detecting random token performed by CSRF detector.","","978-1-4799-7996-7978-1-4799-8175-5978-1-4799-7995","10.1109/ICODSE.2014.7062682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7062682","session management vulnerabilities;vulnerability scanning;black box testing;Nikto;Google Chrome extension","Testing;Detectors;Browsers;Facebook;Software;Authentication;Uniform resource locators","program testing;search engines","vulnerability scanning tool;session management vulnerabilities;session fixation;insufficient cookies attributes;OWASP;open source tool;Nikto;functional testing;field testing;synthetic test cases;false negative;random token detection;CSRF detector;cross-site request forgery","","1","10","","","","","","IEEE","IEEE Conferences"
"On the Functional Test of the Register Forwarding and Pipeline Interlocking Unit in Pipelined Processors","P. Bernardi; R. Cantoro; L. Ciganda; B. Du; E. Sanchez; M. S. Reorda; M. Grosso; O. Ballan","NA; NA; NA; NA; NA; NA; NA; NA","2013 14th International Workshop on Microprocessor Test and Verification","","2013","","","52","57","When the result of a previous instruction is needed in the pipeline before it is available, a “data hazard” occurs. Register Forwarding and Pipeline Interlock (RF&PI) are mechanisms suitable to avoid data corruption and to limit the performance penalty caused by data hazards in pipelined microprocessors. Data hazards handling is part of the microprocessor control logic; its test can hardly be achieved with a functional approach, unless a specific test algorithm is adopted. In this paper we analyze the causes for the low functional testability of the RF&PI logic and propose some techniques able to effectively perform its test. In particular, we describe a strategy to perform Software-Based Self-Test (SBST) on the RF&PI unit. The general structure of the unit is analyzed, a suitable test algorithm is proposed and the strategy to observe the test responses is explained. The method can be exploited for test both at the end of manufacturing and in the operational phase. Feasibility and effectiveness of the proposed approach are demonstrated on both an academic MIPS-like processor and an industrial System-on-Chip based on the Power Architecture™.","1550-4093;2332-5674","978-1-4799-3246","10.1109/MTV.2013.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926101","test;processors;register forwarding;SBST","Registers;Pipelines;Program processors;Vectors;Hazards;Testing;Circuit faults","data handling;optimising compilers;pipeline processing;program testing","functional test;register forwarding;pipeline interlocking unit;pipelined processors;software-based self-test;SBST;test algorithm;MIPS-like processor;industrial system-on-chip","","7","10","","","","","","IEEE","IEEE Conferences"
"Constructing Defect Predictors and Communicating the Outcomes to Practitioners","T. Taipale; M. Qvist; B. Turhan","NA; NA; NA","2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement","","2013","","","357","362","Background: An alternative to expert-based decisions is to take data-driven decisions and software analytics is the key enabler for this evidence-based management approach. Defect prediction is one popular application area of software analytics, however with serious challenges to deploy into practice. Goal: We aim at developing and deploying a defect prediction model for guiding practitioners to focus their activities on the most problematic parts of the software and improve the efficiency of the testing process. Method: We present a pilot study, where we developed a defect prediction model and different modes of information representation of the data and the model outcomes, namely: commit hotness ranking, error probability mapping to the source and visualization of interactions among teams through errors. We also share the challenges and lessons learned in the process. Result: In terms of standard performance measures, the constructed defect prediction model performs similar to those reported in earlier studies, e.g. 80% of errors can be detected by inspecting 30% of the source. However, the feedback from practitioners indicates that such performance figures are not useful to have an impact in their daily work. Pointing out most problematic source files, even isolating error-prone sections within files are regarded as stating the obvious by the practitioners, though the latter is found to be helpful for activities such as refactoring. On the other hand, visualizing the interactions among teams, based on the errors introduced and fixed, turns out to be the most helpful representation as it helps pinpointing communication related issues within and across teams. Conclusion: The constructed predictor can give accurate information about the most error prone parts. Creating practical representations from this data is possible, but takes effort. The error prediction research done in Elektrobit Wireless Ltd is concluded to be useful and we will further improve the presentations made from the error prediction data.","1949-3770;1949-3789","978-0-7695-5056","10.1109/ESEM.2013.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681379","prediction algorithms;software testing;machine learning algorithms;data-driven decisions;error prediction","Predictive models;Testing;Software;Error probability;Accuracy;Data mining;Data models","data visualisation;error statistics;learning (artificial intelligence);program testing;software metrics","defect predictor construction;outcome communication;expert-based decisions;data-driven decisions;software analytics;evidence-based management approach;testing process;data information representation;hotness ranking;error probability mapping;interaction source;interaction visualization;performance measures;source files;error-prone section isolation;machine learning algorithms","","3","7","","","","","","IEEE","IEEE Conferences"
"Towards Identification of Software Improvements and Specification Updates by Comparing Monitored and Specified End-User Behavior","T. Roehm; B. Bruegge; T. Hesse; B. Paech","NA; NA; NA; NA","2013 IEEE International Conference on Software Maintenance","","2013","","","464","467","Support of end-user needs is an important success factor for a software application. In order to optimize the support of end-user needs, developers have to be aware of them and their evolution over time. But a communication gap between developers and users leads to ignorance of developers about how users use their application. Also, developer assumptions about user behavior are rarely tested and corrected if they are wrong. Consequently, many software applications have a mediocre support of user needs and user problems as well as changes in user needs are detected rather late. In this paper, we present a research agenda addressing this problem by comparing use case descriptions to monitored user actions. More specifically, we propose to monitor user actions using instrumentation, detect the current use case of a user using machine learning, and compare use case steps to monitored user actions. By detecting differences between both, we identify mismatches between user behavior and developer assumptions reflected in use case descriptions. Those mismatches can serve as starting points to identify software improvements, to test the use case specification and identify updates, and to revise training programs. Finally, we sketch a plan to evaluate our approach.","1063-6773","978-0-7695-4981","10.1109/ICSM.2013.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676933","User needs;User monitoring;Use case detection;Comparison of observed and specified behavior;Specification testing;Reverse modeling;Machine learning;Software evolution","Monitoring;Usability;Instruments;User interfaces;Object oriented modeling;Training","learning (artificial intelligence);program testing;software process improvement","software improvement identification;software specification updates;specified end-user behavior;monitored end-user behavior;user action monitoring;machine learning;use case descriptions;use case specification testing;training programs","","1","21","","","","","","IEEE","IEEE Conferences"
"Leveraging program equivalence for adaptive program repair: Models and first results","W. Weimer; Z. P. Fry; S. Forrest","Computer Science Department, University of Virginia, Charlottesville, USA; Computer Science Department, University of Virginia, Charlottesville, USA; Computer Science Department, University of New Mexico, Albuquerque, USA","2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2013","","","356","366","Software bugs remain a compelling problem. Automated program repair is a promising approach for reducing cost, and many methods have recently demonstrated positive results. However, success on any particular bug is variable, as is the cost to find a repair. This paper focuses on generate-and-validate repair methods that enumerate candidate repairs and use test cases to define correct behavior. We formalize repair cost in terms of test executions, which dominate most test-based repair algorithms. Insights from this model lead to a novel deterministic repair algorithm that computes a patch quotient space with respect to an approximate semantic equivalence relation. This allows syntactic and dataflow analysis techniques to dramatically reduce the repair search space. Generate-and-validate program repair is shown to be a dual of mutation testing, suggesting several possible cross-fertilizations. Evaluating on 105 real-world bugs in programs totaling 5MLOC and involving 10,000 tests, our new algorithm requires an order-of-magnitude fewer test evaluations than the previous state-of-the-art and is over three times more efficient monetarily.","","978-1-4799-0215","10.1109/ASE.2013.6693094","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693094","Automated program repair;mutation testing;program equivalence;search-based software engineering","Maintenance engineering;Approximation algorithms;Testing;Algorithm design and analysis;Adaptation models;Optimization;Search problems","cost reduction;data flow analysis;deterministic algorithms;program debugging;program testing;software maintenance","program equivalence;adaptive program repair;software bugs;automated program repair;cost reduction;generate-and-validate program repair methods;test cases;repair cost;test-based repair algorithms;deterministic repair algorithm;approximate semantic equivalence relation;patch quotient space;dataflow analysis techniques;syntactic analysis techniques;repair search space reduction;mutation testing;cross-fertilizations","","42","55","","","","","","IEEE","IEEE Conferences"
"Tracking down high coverage configuration using clustering and fault detection","S. Dhanalakshmi; S. C. Devi","Software Engineering, Indian Institute of Information Technology, Srirangam, Tiruchirappalli, Tamilnadu, India; Department of Computer Science Engineering, Bharathidasan Institute of Technology campus, Anna University Tiruchirappalli, Tamilnadu, India","2015 Online International Conference on Green Engineering and Technologies (IC-GET)","","2015","","","1","5","Mostly all software systems are highly configured, it has many benefits but there is a difficulty of software testing because there will be unique errors could be hidden in any of the configurations and undergoing testing for each of the configurations will lead to expensive testing and it is also impractical. The dependable systems will have some mechanism for fault tolerance in software testing. If the rate of the fault detection is calculated then the coverage of the configuration can be easily generated. First load the application for which it is going to be tested by using our test case prioritization approach and loading the dataset for the test case for the given application. After this process, need to assign the individual ids for all the test cases in the test case dataset. Also it is able to add the test cases in the dynamic nature. Then to compute the test case prioritization, first built the dependency structure for the test cases. Through the approach get the height and weight matrix for the test cases after this computation the test cases. The cosine similarity values between the test cases. In the similarity values it will show how it is highly related with the other test cases. Thus the clustering approach is introduced for grouping the test cases. These test cases are analyzed for measuring their relevancy and relationship between the test cases using their constrains and the clustering of the test cases is done for the better result in the rate of fault detect. With the Average percentage fault detection the graph is drawn and it shown the high coverage configurations.","","978-1-4673-9781-0978-1-4673-8625","10.1109/GET.2015.7453839","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7453839","dependency structure;simularity measure;clustering;fault detection","Software;Fault detection;Software testing;Software engineering;Decision trees;Classification algorithms","configuration management;fault diagnosis;pattern clustering;program testing","high coverage configuration;clustering;software testing;test case prioritization approach;test cases dependency structure;cosine similarity values;average percentage fault detection","","","13","","","","","","IEEE","IEEE Conferences"
"Optimizing software integration in component-based embedded systems by using simulated annealing","M. Steindl; M. Niemetz; J. Mottok; S. Racek","AVL Software and Functions GMBH, D-93059 Regensburg, Im Gewerbepark B27, Germany; University of Applied Sciences Regensburg, Laboratory for Safe and Secure Systems (LaS), Seybothstr. 2, D-93053 Regensburg, Germany; University of Applied Sciences Regensburg, Laboratory for Safe and Secure Systems (LaS), Seybothstr. 2, D-93053 Regensburg, Germany; University of West Bohemia, Department of Computer Science, Pilsen, Czech Republic","Eurocon 2013","","2013","","","446","451","The definition of the integration order in a component based embedded system is often a crucial part in the software development process. The integration sequence is affected by many parameters and there is a lack of methods to determine an order which fits best. This decision often depends on integrators expertise. This article considers two key parameters of software integration which are present in most development processes and describes an approach to determine a most suitable integration order.","","978-1-4673-2232-4978-1-4673-2230","10.1109/EUROCON.2013.6625020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625020","integration order;embedded systems;simulated annealing;schedule;test effort","Complexity theory;Software;Schedules;Simulated annealing;Testing","object-oriented programming;simulated annealing;software engineering","software integration optimization;component-based embedded systems;simulated annealing;software development process;integration sequence;integration order;embedded systems","","","20","","","","","","IEEE","IEEE Conferences"
"Day 2: Mini-tutorial: Challenges to the design and optimization of cyber-physical systems","Z. Peng","Embedded Systems Laboratory, Linkoping University, Sweden","2014 9th International Design and Test Symposium (IDT)","","2014","","","xvii","xvii","Summary form only given. We are witnessing an exponential increase of cyber-physical systems where the computational components interact with the physical world in a tightly manner. More and more of these systems are nowadays used for safety-critical applications, such as automotive electronics and medical equipment. These safety-critical applications impose stringent requirements on reliability, efficiency, low-power and testability of the underlying VLSI hardware implementation. With silicon technology scaling, however, VLSI circuits is built with smaller transistors, perform at higher clock frequencies, run at lower voltage levels, and operate very often at higher temperature. All these have major negative impact on reliability, performance, power-efficiency and testability. We are therefore facing the challenges of how to address all these technical problems and their interplay with the stringent real-time requirements imposed by many safety-critical applications. This talk will discuss the design of such cyber-physical systems by considering both fault-tolerance and real-time requirements at the same time. It will describe several key challenges and some emerging solutions to the design and optimization of such systems. In particular, it will present time-redundancy based fault-tolerance techniques to address transient faults which have become more and more common in nano-scale technology. It will also describe several design tradeoffs including hardware/software co-design solutions for the optimization of cyber-physical systems.","2162-0601;2162-061X","978-1-4799-8200","10.1109/IDT.2014.7038574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7038574","","Optimization;Integrated circuit reliability;Very large scale integration;Hardware;Real-time systems;Fault tolerance","fault tolerant computing;hardware-software codesign","cyber-physical system design;cyber-physical system optimization;safety-critical applications;VLSI hardware implementation;very large scale integration;silicon technology scaling;VLSI circuits;transistors;clock frequency;reliability requirement;efficiency requirement;low-power requirement;testability requirement;time-redundancy based fault-tolerance techniques;transient faults;nanoscale technology;hardware-software codesign","","","","","","","","","IEEE","IEEE Conferences"
"Faster mutation-based fault localization with a novel mutation execution strategy","P. Gong; R. Zhao; Z. Li","Department of Computer Science, Beijing University of Chemical Technology, Beijing 100029, P.R. China; Department of Computer Science, Beijing University of Chemical Technology, Beijing 100029, P.R. China; Department of Computer Science, Beijing University of Chemical Technology, Beijing 100029, P.R. China","2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","","2015","","","1","10","Checking program entities for finding faults is extremely tedious for developers. Fault localization techniques are designed to give a rank list of the probability that program entities incur faults to assist developers to locate faults. Mutation-based fault localization is a recently proposed fault localization approach via mutation analysis. With improved effectiveness, the mutation-based fault localization also brings huge execution cost. To reduce the execution cost of mutation-based fault localization technique, this paper proposes a dynamic mutation execution strategy which includes execution optimizations on both mutants and test cases. As fewer mutants and test cases are executed with the presented strategy, the fault localization process will be faster. The empirical studies show that mutation-based fault localization with the dynamic strategy we proposed, called Faster-MBFL, can reduce mutant-test execution times by 32.4% to 87% with keeping the fault localization accuracy unchanged; further, the additional run time required by utilizing our strategy can be ignored.","","978-1-4799-1885","10.1109/ICSTW.2015.7107448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7107448","Software Debugging;Mutation-based Fault Localization;Mutation Cost Reduction;Dynamic Mutation Execution Strategy","History;Optimization;Measurement;Software;Conferences;Testing;Accuracy","probability;program debugging;program testing;software fault tolerance","mutation-based fault localization approach;mutation execution strategy;program entity checking;mutation analysis;dynamic mutation execution strategy;fault localization process;Faster-MBFL;mutant-test execution times","","8","31","","","","","","IEEE","IEEE Conferences"
"An improved artificial bee colony (ABC) algorithm for large scale optimization","Y. Liang; Y. Liu; L. Zhang","School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China","2013 2nd International Symposium on Instrumentation and Measurement, Sensor Network and Automation (IMSNA)","","2013","","","644","648","Artificial bee colony (ABC) algorithm as a new optimization algorithm invented recently has been applied to solve many kinds of combinatorial and numerical function optimization problems. The existing forms of ABC algorithms perform well in most cases. However, ABC algorithm is still lack of capacity for optimizing high dimensional problems without taking the interactions within each dimensional variables into consideration. Inspired by Cooperative Coevolution (CC), this paper adjusts ABC algorithm with cooperative coevolving which we call CCABC. Iteratively, CCABC can discover the relations of the high dimensional variables, considering those relationship dimensions as the same group, and then CCABC optimizes the whole group instead of a single dimension. We test CCABC algorithm on a set of large scale optimization benchmarks and compare the performance with that of original ABC algorithm and two classic CC frameworks CCVIL and DECC-G. Experimental results show that CCABC algorithm outperforms CCVIL, DECC-G, and original ABC algorithm in almost all of the experiments and can solve large scale optimization problems efficiently.","","978-1-4799-2716-6978-1-4799-2715","10.1109/IMSNA.2013.6743359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6743359","artificial bee colony;large scale optimization;cooperative coevolution;dynamic group strategy","Heuristic algorithms;Optimization;Benchmark testing;Algorithm design and analysis;Software algorithms;Evolutionary computation;Vectors","ant colony optimisation;evolutionary computation","artificial bee colony;ABC algorithm;large scale optimization;cooperative coevolution;CCABC;CCVIL;DECC-G","","5","21","","","","","","IEEE","IEEE Conferences"
"Searching inside group approach for combination test suite reduction","H. Chen; X. Pan","School of Computer Science and Technology, Xi'an University of Posts &amp; Telecommunications, Xi'an 710121, China; School of Computer Science and Technology, Xi'an University of Posts &amp; Telecommunications, Xi'an 710121, China","2015 IEEE 7th International Conference on Cybernetics and Intelligent Systems (CIS) and IEEE Conference on Robotics, Automation and Mechatronics (RAM)","","2015","","","87","92","For generating the combination test suite, we have proposed a combination test data global optimization mechanism. Firstly, an encoding process is used to create a one-to-one correspondence between each test case in its complete set and the gene in a binary code sequence. Based on this process, the combination test data generating problem has been translated into a binary genetic code optimization problem. Then, the ethnic group evolution algorithm (EGEA) is used to search the binary code space to find the optimal binary code sequence. In order to refine the genetic structure of each group, a novel ethic group searching approach, searching inside group process is presented. The simulations show this searching process is feasible, which improves the efficiency of optimizing genetic structure and reducing test case set observably.","2326-8239;2326-8123","978-1-4673-7338-8978-1-4673-7337","10.1109/ICCIS.2015.7274553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274553","ethnic group evolution algorithm;searching among group process;searching inside group process;combination test suite optimization","Conferences;Random access memory;Decision support systems","binary codes;genetic algorithms;program testing;search problems","searching inside group approach;combination test suite reduction;combination test data global optimization mechanism;encoding process;one-to-one correspondence;test case;combination test data generating problem;binary genetic code optimization problem;ethnic group evolution algorithm;EGEA;binary code space;optimal binary code sequence;genetic structure optimization;ethic group searching approach;searching process;software testing","","","13","","","","","","IEEE","IEEE Conferences"
"Isolating failing test cases: A comparative experimental study of clustering techniques","J. Farjo; W. Masri; H. Hajj","Department of Electrical and Computer Engineering, American University of Beirut, Lebanon; Department of Electrical and Computer Engineering, American University of Beirut, Lebanon; Department of Electrical and Computer Engineering, American University of Beirut, Lebanon","2013 Third International Conference on Communications and Information Technology (ICCIT)","","2013","","","73","77","Researchers have applied cluster analysis onto execution profiles induced by test cases in order to solve problems in the area of software testing and analysis. The employed clustering techniques varied, and no study was conducted to rate these techniques in terms of their effectiveness in this specific domain. This work aims at doing so experimentally. Specifically, given test sets comprising passing and failing test cases, we measure the performance of each technique at isolating the failing test cases from the passing cases. The study included one technique from each of the six main families of clustering algorithms. Our results suggested the following ranking of the evaluated techniques from best to worst: DBSCAN, K-Means, Agglomerative-AGNES and WaveCluster, Fuzzy-FCM, and K-Subspace.","","978-1-4673-5307-6978-1-4673-5306","10.1109/ICCITechnology.2013.6579525","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6579525","cluster analysis;software testing and analysis;execution profile","Clustering algorithms;Software;Measurement;Noise;Wavelet transforms;Security;Software testing","failure analysis;pattern clustering;program testing;statistical analysis","K-Subspace;Fuzzy-FCM;WaveCluster;Agglomerative-AGNES;K-Means;DBSCAN;clustering algorithms;passing cases;failing test cases;software testing;execution profiles;cluster analysis","","1","33","","","","","","IEEE","IEEE Conferences"
"Component based reliability assessment from UML models","V. Chourey; M. Sharma","Computer Science and Engineering Department, Medi-Caps Institute, Indore, Madhya Pradesh, India; Computer Engineering Department, IET, DAVV, Indore, Madhya Pradesh, India","2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","","2015","","","772","778","Model based development and testing techniques have ventured diverse research directions to assure quality of the software product. Models developed during architecture and design phases are efficient tools to assess quality at an early development stage. However, testing the extra-functional or non-functional properties of software systems is not frequently practised eg. reliability. The motivation to our work is to model the context of execution which is significant in system reliability analysis. In this paper we visualize the components of complex software systems and their interactions in the form of Functional Flow Diagram (FFD). This notation specifies the dynamic aspect of system behavior as the context of execution. To further asses reliability, the FFD is translated into Reliability Block Diagram (RBD). The relative importance of the components in terms of reliability is evaluated and is associated with prioritization of the component. The model is simple but significant for system maintenance, improvisation and modification. This model supports analysis and testing through better understanding of the interacting components and their reliabilities.","","978-1-4799-8792-4978-1-4799-8790-0978-1-4799-8791","10.1109/ICACCI.2015.7275704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275704","","Unified modeling language;Testing;Computational modeling;Software reliability;Estimation;Analytical models","flowcharting;object-oriented programming;program testing;program visualisation;software architecture;software maintenance;software quality;software reliability;Unified Modeling Language","component based reliability assessment;UML models;model based development;testing techniques;software product quality;architecture phases;design phases;quality assessment;extra-functional properties;nonfunctional properties;system reliability analysis;components visualization;complex software systems;functional flow diagram;FFD;system behavior;reliability block diagram;RBD;component prioritization;system maintenance;system improvisation;system modification","","1","26","","","","","","IEEE","IEEE Conferences"
"Research on FJSP based on CUDA parallel cellular particle swarm optimization algorithm","Liu Shenghui; Zhang Shuli","School of Software, Harbin University of Science and Technology, China; School of Software, Harbin University of Science and Technology, China","International Conference on Software Intelligence Technologies and Applications & International Conference on Frontiers of Internet of Things 2014","","2014","","","325","329","This paper raised a Cellular particle swarm optimization, (CPSO) algorithm based on GPU parallel, which could reduce the computing time for processing amounts of data and solving Flexible job shop scheduling problem(FJSP) complex problems. The implementation of proposed algorithm combines the GPU parallel technology and traditional CPSO algorithm. The method accelerates the convergence rate of the particle swarm by using a large amounts of GPU threads to process each particle. In the experimental part, we use the benchmark data to test the performance of proposed algorithm. Comparing with serial CPSO, the proposed method can achieve higher computing speed ratio under the premise that ensure the optimal solution.","","978-1-84919-970","10.1049/cp.2014.1583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284267","Cellular particle swarm;parallel computing;CUDA","","graphics processing units;job shop scheduling;multi-threading;parallel architectures;particle swarm optimisation","FJSP;CUDA parallel cellular particle swarm optimization algorithm;computing time;flexible job shop scheduling problem;complex problems;GPU parallel technology;CPSO algorithm;GPU threads","","","","","","","","","IET","IET Conferences"
"Test &amp; Evaluation of Cognitive and Dynamic Spectrum Access Radios Using the Cognitive Radio Test System","E. Sollenberger; F. Romano; C. Dietrich","NA; NA; NA","2015 IEEE 82nd Vehicular Technology Conference (VTC2015-Fall)","","2015","","","1","2","As the demand placed on wireless networks continues to rapidly grow, devices must utilize available resources as efficiently as possible. Cognitive Radio (CR) provides an attractive solution to this problem, wherein devices learn from their experiences and make intelligent choices to optimize their own performance along with that of the network as a whole. Spectrum sharing is one promising method that has gained much interest as seen by the recent development of LTE-Unlicensed and because of the large amount of spectrum that the federal government plans to open to the public for this very purpose. Research in this area is ongoing and many approaches have been proposed and studied. While the theoretical concepts and algorithms related to this technology are developed, it is important that an efficient test and evaluation methodology exist in order to bridge the gap from theory to practice. The Cognitive Radio Test System (CRTS) is being developed for just this purpose. CRTS is being developed using the CORNET testbed at Virginia Tech, which consists of 48 Software-Defined Radio (SDR) nodes spread through a building on campus. A flexible and extensible base framework for CRTS has been developed and its initial capabilities demonstrated.","","978-1-4799-8091-8978-1-4799-8090","10.1109/VTCFall.2015.7391168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391168","","Cathode ray tubes;Cognitive radio;Engines;Testing;Optimization;Performance evaluation;Approximation algorithms","cognitive radio;radio spectrum management;software radio","SDR nodes;software-defined radio nodes;CORNET testbed;CRTS;cognitive radio test system;spectrum sharing;wireless networks","","","2","","","","","","IEEE","IEEE Conferences"
"Improving Analogy-Based Software Cost Estimation through Probabilistic-Based Similarity Measures","P. Phannachitta; J. Keung; A. Monden; K. Matsumoto","NA; NA; NA; NA","2013 20th Asia-Pacific Software Engineering Conference (APSEC)","","2013","1","","541","546","The performance of software cost estimation based on analogy reasoning depends upon the measures that specifying the similarity between software projects. This paper empirically investigates the use of probabilistic-based distance functions to improve the similarity measurement. The probabilistic-based distance functions are considerably more robust, because they collect the implicit correlation between the occurrences of project feature attributes. This information gain enables the constructed estimation model to be more concise and comprehensible. The study compares 6 probabilistic-based distance functions against the commonly-used Euclidian distance. We empirically evaluate the implemented cost estimation model using 5 real-world datasets collected from the PROMISE repository. The result shows a significant improvement in terms of error reduction, that implies an estimation based on probabilistic-based distance functions achieve higher accuracy on average, and the peak performance significantly outperforms the Euclidian distance based on Wilcox on signed-rank test.","1530-1362;1530-1362","978-1-4799-2144-7978-1-4799-2143","10.1109/APSEC.2013.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6805449","Software Cost Estimation;Analogy;Heterogeneous Distance Function;Machine Learning","Estimation;Software;Probabilistic logic;Accuracy;Measurement uncertainty;Equations","probability;project management;software cost estimation;software management","analogy-based software cost estimation;probabilistic-based similarity measures;analogy reasoning;probabilistic-based distance functions;project feature attributes;Euclidian distance;cost estimation model;PROMISE repository;error reduction;Wilcoxon signed-rank test","","2","13","","","","","","IEEE","IEEE Conferences"
"Marine electrical plant model code optimization to achieve soft real-time execution","M. Malenshek; M. Boley; M. Burke","Military Marine, Woodward, Inc., Fort Collins, CO USA; Military Marine, Woodward, Inc., Fort Collins, CO USA; Technical Consulting, MathWorks, Inc., Natick, MA USA","2013 IEEE Electric Ship Technologies Symposium (ESTS)","","2013","","","116","120","This paper describes the process of optimizing a 450 volt shipboard power plant model. The model is developed in MathWorks<sup>®</sup>Simulink<sup>®</sup>and used for software-in-the-loop (SIL) simulation of real-time control systems. At a total block count in excess of 135,000, the model complexity reached the point it could not be compiled. Additionally, one of the project requirements was to maintain soft real-time execution when compiled. Through the use of a number of optimization techniques, these goals were achieved. The key optimizations discussed are architectural changes, legacy code conversion, and continuous to discrete conversion.","","978-1-4673-5245-1978-1-4673-5243-7978-1-4673-5244","10.1109/ESTS.2013.6523721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6523721","","Mathematical model;Optimization;Software packages;Real-time systems;Testing;Computational modeling;Accuracy","marine power systems;optimisation;power system control;real-time systems;ships","marine electrical plant;code optimization;soft real-time execution;shipboard power plant;MathWorks;Simulink;software-in-the-loop;SIL simulation;real-time control systems;legacy code conversion;voltage 450 V","","","6","","","","","","IEEE","IEEE Conferences"
"Applying Parameter Value Weighting to a Practical Application","S. Fujimoto; H. Kojima; H. Nakagawa; T. Tsuchiya","NA; NA; NA; NA","2014 IEEE International Symposium on Software Reliability Engineering Workshops","","2014","","","130","131","This paper reports a case study where pair-wise testing was applied to a real-world program. In particular we focus on weighting, an added feature which allows the tester to prioritize particular parameter values. In our previous work we proposed a weighting method that can reflect given weights in the resulting test suite more directly than can existing methods. To asses the effects of weighting in a practical testing process, we compare the number of execution times of the program's methods among three pair-wise test suites, including the test suite generated by our weighting method and those generated by an existing test case generation tool with and without the weighting option. The results show that the effects of weighting were most clearly observed when our weighting method was used.","","978-1-4799-7377","10.1109/ISSREW.2014.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983821","pair-wise testing;weighting;combinatorial interaction testing","Testing;Local area networks;High definition video;Browsers;Random access memory;Greedy algorithms;Weight measurement","program testing","parameter value weighting;pair-wise testing;program testing;practical testing process;program method execution time","","","3","","","","","","IEEE","IEEE Conferences"
"A commercial EM solver using the BI-RME method","C. Carceller; F. J. Pérez; J. Gil; C. Vicente; V. E. Boria; B. Gimeno; M. Guglielmi","Grupo de Aplicaciones de Microondas. Universitat Politècnica de Valencia. Valencia, Spain; Aurora Software and Testing S. L. Valencia, Spain; Aurora Software and Testing S. L. Valencia, Spain; Aurora Software and Testing S. L. Valencia, Spain; Grupo de Aplicaciones de Microondas. Universitat Politècnica de Valencia. Valencia, Spain; Departamento de Física Aplicada y Electromagnetismo, Instituto de Ciencia de Materiales (ICMUV) Universitat de Valéncia. Valencia, Spain; European Space Research and Technology Centre - European Space Agency. Noordwijk, The Netherlands","2014 International Conference on Numerical Electromagnetic Modeling and Optimization for RF, Microwave, and Terahertz Applications (NEMO)","","2014","","","1","4","In this paper, we present FEST3D, a powerful software tool capable of efficiently analyzing most passive components commonly used in space and ground applications. By combining different approaches of the Boundary Integral - Resonant Mode Expansion (BI-RME) method, this tool is able to perform an accurate EM modeling of components with complicated geometries in extremely short computational times (orders of magnitude less than software tools based on 3D segmentation techniques).","","978-1-4799-2820","10.1109/NEMO.2014.6995705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6995705","","Electromagnetic waveguides;Cavity resonators;Three-dimensional displays;Transmission line matrix methods;Microwave filters;Waveguide junctions","boundary integral equations;electromagnetic fields;software tools","EM solver;BI-RME method;FEST3D;software tool;passive components;boundary integral resonant mode expansion method;EM modeling;3D segmentation techniques","","","29","","","","","","IEEE","IEEE Conferences"
"Software designed GNSS system emulator","M. Pollina; O. Desenfans; R. Fiengo","M3 Systems; M3 Systems; National Instruments","2014 Tyrrhenian International Workshop on Digital Communications - Enhanced Surveillance of Aircraft and Vehicles (TIWDC/ESAV)","","2014","","","141","144","This paper presents STELLA NGC: an open Software-Designed GNSS Simulator that provides the complete toolbox for GNSS testing. Indeed, this GNSS SDR is not only capable to simulate GNSS signals according to user-defined scenario, but it is also capable to Record & Replay “real GNSS signals” and to support performance evaluation ranging from low level signal processing to statistics on trajectories. Last but not least, STELLA NGC offers the possibility to add a broad variety of interference signals on top of the GNSS signals. This makes our solution a complete GNSS test and measurement platform. With this single platform, the user can access all generic and custom functions he needs, ensuring long-term evolution and optimizing return on investment.","","978-1-4799-3094","10.1109/TIWDC-ESAV.2014.6945465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945465","GNSS;test & measurements","Global Positioning System;Receivers;Trajectory;Interference;Testing;Satellites;Radio frequency","Long Term Evolution;satellite navigation;signal denoising;software radio;statistical analysis","Global Navigation Satellite System trajectory;Long Term Evolution;STELLA NGC statistics;low level signal processing;GNSS SDR signal interference;open software designed GNSS simulator testing;software designed GNSS emulator measurement platform","","","6","","","","","","IEEE","IEEE Conferences"
"IMDCT optimization of AVS-P3 decoding algorithm in DSP","C. Li; R. Hu; W. Tu; L. Zhang; Y. Wang","National Engineering Research Center for Multimedia Software, Wuhan University, China; National Engineering Research Center for Multimedia Software, Wuhan University, China; National Engineering Research Center for Multimedia Software, Wuhan University, China; National Engineering Research Center for Multimedia Software, Wuhan University, China; Public Course Department, Wuhan Railway Vocational College of Technology, Hubei, China","2013 IEEE International Conference on Information and Automation (ICIA)","","2013","","","1364","1368","In order to write AVS-P3 audio decoding algorithms in the digital signal processor(DSP) chip which supports fixed-point algorithms, AVS-P3 audio decoding floating-point algorithms need to be converted to fixed-point algorithms. Owing to the high complexity of IMDCT(Inverse Modified Discrete Cosine Transform) module windowing in AVS-P3 decoder algorithm, we propose an improved windowing algorithm in this paper. The test results show that the time complexity of the module optimized is reduced by 17.04% and the space complexity of the module optimized is reduced by 32.9%. The proposed algorithm is better than the original algorithm in time and space performance.","","978-1-4799-1334","10.1109/ICInfA.2013.6720506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6720506","AVS-P3;Audio Decoding;DSP;IMDCT","Decoding;Algorithm design and analysis;Optimization;Digital signal processing;Signal processing algorithms;Complexity theory;Standards","computational complexity;decoding;digital signal processing chips;discrete cosine transforms;optimisation","IMDCT optimization;digital signal processor chip;DSP chip;AVS-P3 audio decoding floating point algorithms;fixed point algorithms;inverse modified discrete cosine transform module windowing;AVS-P3 decoder algorithm;time complexity","","","11","","","","","","IEEE","IEEE Conferences"
"Development of a test-bed for knowledge-intensive system architecture optimization","D. Selva","Cornell University, USA","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","","2014","","","3394","3398","System Architecture [1] is the high-level design of a system, defining the main elements of function and form, the mapping between function and form, and the interfaces between elements of function and form, and between the system and the surrounding context. Several studies have highlighted the importance of system architecture [2], as it is a point in the design process of unique leverage: most of the lifecycle cost of a system is committed after the system architecture phase[3], and a similar argument can be made for performance. The author introduced the VASSAR framework for knowledge-intensive architecture optimization [4]. The VASSAR framework is essentially an architecture evaluation and optimization framework, which combines a rule-based engine for evaluation with several heuristic optimization strategies. The VASSAR framework has been applied to multiple large aerospace systems [5-7]. The performance of several domain-independent and domain-specific heuristics was also compared in some preliminary experiments [8].","1062-922X","978-1-4799-3840","10.1109/SMC.2014.6974452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974452","","Computer architecture;Optimization;Space vehicles;Topology;Conferences;Instruments;Joining processes","knowledge based systems;optimisation;program testing;software architecture","test-bed;knowledge-intensive system architecture optimization;high-level design;design process;system lifecycle cost;VASSAR framework;architecture evaluation;optimization framework;rule-based engine;heuristic optimization strategies;aerospace systems;domain-independent heuristics;domain-specific heuristics;value assessment of system architectures using rules","","","13","","","","","","IEEE","IEEE Conferences"
"Which compiler optimization options should I use for detecting data races in multithreaded programs?","C. Jia; W. K. Chan","City University of Hong Kong Tat Chee Avenue, Hong Kong; City University of Hong Kong Tat Chee Avenue, Hong Kong","2013 8th International Workshop on Automation of Software Test (AST)","","2013","","","53","56","Different compiler optimization options may produce different versions of object code. To the best of our knowledge, existing studies on concurrency bug detection in the public literature have not reported the effects of different compiler optimization options on detection effectiveness. This paper reports a preliminary but the first study in the exploratory nature to investigate this aspect. The study examines the happened-before based predictive data race detection scenarios on four benchmarks from the PARSEC 3.0 suite compiled under six different GNU GCC optimization options. We observe from the data set that the same race detection technique may produce different sets of races or different detection probabilities under different optimization scenarios. Based on the observations, we formulate two hypotheses for future investigations.","","978-1-4673-6161","10.1109/IWAST.2013.6595791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595791","Race detection;compiler optimization option;empirical study;hypothesis formulation","Optimization;Benchmark testing;Concurrent computing;Detectors;Computer bugs;Instruction sets;Linux","multi-threading;optimising compilers;program debugging","compiler optimization;multithreaded program;object code;concurrency bug detection;predictive data race detection;PARSEC 3.0 suite;GNU GCC optimization","","2","21","","","","","","IEEE","IEEE Conferences"
"RTL IP abstraction into optimized embedded software","N. Bombieri; D. Forrini; F. Fummi; M. Laurenzi; S. Vinco","Department of Computer Science, University of Verona, Italy; Department of Computer Science, University of Verona, Italy; Department of Computer Science, University of Verona, Italy; Department of Computer Science, University of Verona, Italy; Department of Computer Science, University of Verona, Italy","East-West Design & Test Symposium (EWDTS 2013)","","2013","","","1","5","Modern SoCs gain a high level of parallelism by using both general purpose processors and a number of data processing entities (DPE), dedicated to certain heavy functionalities. As a consequence, most systems devote DPEs to executing functions with high performance rather than using dedicated hardware. Reusing already existing and pre-verified IPs through abstraction methodologies is a key idea to meet time-to-market requirements and to reduce the error risk. Rough abstraction techniques lead to non efficient software code, that results in being very limited by hardware communication protocols and data types. This paper proposes an abstraction methodology that produces optimized code. Protocol refinement and data redefinitions are exploited to increase software performance. The effectiveness of the methodology has been proven by applying it to industrial designs.","","978-1-4799-2096-9978-1-4799-2095","10.1109/EWDTS.2013.6673144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6673144","","IP networks;Optimization;Hardware design languages;Protocols;Hardware;Software;Data models","embedded systems;multiprocessing systems;scheduling;system-on-chip","RTL IP abstraction;optimized embedded software;MPSoC;multiprocessing system-on-chip;parallelism level;general purpose processors;data processing entities;DPE;time-to-market requirements;error risk reduction;rough abstraction techniques;software code;hardware communication protocols;data types;protocol refinement;data redefinitions;software performance;industrial designs;register transfer level","","1","14","","","","","","IEEE","IEEE Conferences"
"Using the combinatorial optimization approach for DVS in high performance processors","D. R. Sulaiman; B. S. Ahmed","Electrical Engineering Department, Engineering College, Salahaddin University-Hawler, Erbil - Kurdistan - Iraq; Software Engineering Department, Engineering College, Salahaddin University-Hawler, Erbil - Kurdistan - Iraq","2013 The International Conference on Technological Advances in Electrical, Electronics and Computer Engineering (TAEECE)","","2013","","","105","109","Currently, in high performance computer systems, processors are faster and have vastly increased in performance and computational power. It is important for system designers to get an early estimation of power dissipation to meet the challenging methodologies for power dissipation reduction and optimization. In addition to the current design methodologies, the designer might need to consider the factors that affect the power and the interaction of these factors in practice. This paper presents a design technique for dynamic voltage scaling (DVS) for microprocessor's power dissipation control using the combinatorial design approach. The DVS unit dynamically alter processor's throughput for energy-efficiency by scaling down the supply voltage as well as clock frequency such that the actual delay of the chip meets the target performance. Whilst the combinatorial design is used to get an optimal interaction of the factors that affect the power to get optimal power dissipation estimation for the designer. Simulation and results are used to verify the theoretical background and optimization of the design approach which shows satisfactory results.","","978-1-4673-5613-8978-1-4673-5612-1978-1-4673-5611","10.1109/TAEECE.2013.6557204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557204","software interaction testing;Power reduction;Optimization algorithm;combinatorial designs","Voltage control;Delays;Capacitance;Software reliability;Software","combinatorial mathematics;energy conservation;microprocessor chips;optimisation;parallel processing;power aware computing","combinatorial optimization approach;DVS;high performance processors;high performance computer systems;computational power;power dissipation reduction;power dissipation optimization;dynamic voltage scaling;microprocessor power dissipation control;combinatorial design approach;processor throughput;energy-efficiency;supply voltage;clock frequency","","2","24","","","","","","IEEE","IEEE Conferences"
"Particle Swarm Optimization with non-linear velocity","A. J. Malik; F. A. Khan","Department of Software Engineering, Foundation University Rawalpindi Campus, New Lalazar, Rawalpindi, Pakistan; Department of Software Engineering, Foundation University Rawalpindi Campus, New Lalazar, Rawalpindi, Pakistan","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","","2014","","","602","607","Particle Swarm Optimization (PSO), a population based optimization technique, has two intrinsic problems of slow convergence and tendency to converge prematurely. In order to overcome these problems, we propose an improvement to the velocity update equation of the standard PSO algorithm in which particles of a swarm tend to move towards the global best position more rapidly as compared to the local best position. Two different non-linear weight factors are multiplied with the two parts of the velocity update equation; one that tends to move the particle to the global best position, while the other tends to move the particle back to its local best position achieved so far. By introducing the separate weight factors, a significant improvement in the results is seen. We test the proposed algorithm on six benchmark functions and the simulation results are presented. The results indicate that the proposed algorithm does not converge prematurely and its convergence speed is faster than the standard PSO algorithm.","1062-922X","978-1-4799-3840","10.1109/SMC.2014.6973974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973974","Particle Swarm Optimization;Sigmoid Function;Swarm Intelligence","Standards;Convergence;Equations;Particle swarm optimization;Optimization;Mathematical model;Benchmark testing","particle swarm optimisation","particle swarm optimization;nonlinear velocity;population based optimization technique;nonlinear weight factors","","1","17","","","","","","IEEE","IEEE Conferences"
"Educational and research tools for network optimization","J. Izquierdo-Zaragoza; P. Pavon-Marino","Universidad Polit&#x00E9;cnica de Cartagena, Cuartel de Antiguones, Plaza del Hospital 1, 30202, Spain; Universidad Polit&#x00E9;cnica de Cartagena, Cuartel de Antiguones, Plaza del Hospital 1, 30202, Spain","2013 15th International Conference on Transparent Optical Networks (ICTON)","","2013","","","1","4","This paper presents Net2Plan tool and Java Optimization Modeler (JOM) library and discusses their application as research tools and educational resources in the field of network optimization. Net2Plan is designed to assist users in the definition and comparative evaluation of their original network planning algorithms, as well as in the simulation and testing of connection admission control algorithms, and network protection/restoration schemes. In its turn, JOM is a library which helps to model optimization problems in the Java language and solve them interfacing with integrated linear and non-linear solvers. JOM promotes a fast problem prototyping following a vectorial MATLAB-like syntax. The combined use of Net2Plan and JOM gives users from industry and academia a complete environment to simulate, analyze, dimension, optimize and evaluate the performance of their network designs. Both tools can be publicly downloaded from their respective websites.","2161-2064;2162-7339","978-1-4799-0683","10.1109/ICTON.2013.6602690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602690","Network planning and optimization;Modeling framework;Discrete-event simulation;Java","Optimization;Algorithm design and analysis;Java;Libraries;Planning;Mathematical model;Open source software","computer aided instruction;Java;software prototyping;telecommunication computing;telecommunication congestion control;telecommunication engineering education;telecommunication network planning","educational resources;research tools;network optimization;Net2Plan tool;Java Optimization Modeler library;JOM;network planning algorithms;connection admission control algorithms;network protection scheme;network restoration scheme;Java language;integrated linear solvers;integrated nonlinear solvers;network designs;Web sites;vectorial MATLAB-like syntax","","3","4","","","","","","IEEE","IEEE Conferences"
"Energy efficient software development life cycle - An approach towards smart computing","S. K. Sharma; P. K. Gupta; R. Malekian","Department of Computer Science and Engineering, Jaypee University of Information & Technology, Waknaghat, Solan, Himachal Pradesh, India; Department of Electrical, Electronic, and Computer Engineering, University of Pretoria, Pretoria, South Africa; Department of Electrical, Electronic, and Computer Engineering, University of Pretoria, Pretoria, South Africa","2015 IEEE International Conference on Computer Graphics, Vision and Information Security (CGVIS)","","2015","","","1","5","It is difficult to consider that the computers and its devices can play a vital role in environment pollution. There is also a myth that these devices use less energy. According to a survey $ 250 billion per year spent on powering computers worldwide only about 50% of that power is spent computing - the rest is wasted idling [1]. In this paper, we have proposed energy efficient software development life cycle (SDLC) model which is composed by six stages. These stages are Green requirement analysis, Green design, Green implementation, Green testing, Green maintenance and Green analysis. The last stage green analysis is very important because all the other life cycle stages get connected to this green analysis phase in a cyclic manner. The reason behind it When one role of one stage is over, then green analysis phase verifies and validate that phase to ensure the ecofriendly development of software. In green analysis phase, we have used the important concepts which can be implemented towards implementing smart computing techniques like Internet-of-Things, Cloud Computing, virtualization, solutions for green data centre, power optimization, and grid computing. Proposed energy efficient SDLC follows the iterative approach instead of the sequential approach, as if we want to make change in any of the phase then the same could be done easily.","","978-1-4673-7437-8978-1-4673-7436","10.1109/CGVIS.2015.7449881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7449881","SDLC;Green Computing;Smart Computing;Energy Sustainable;Energy Conservation","Green products;Software;Energy efficiency;Testing;Computational modeling;Encoding;Maintenance engineering","cloud computing;green computing;grid computing;Internet of Things;software development management;software maintenance;virtualisation","energy efficient software development life cycle;smart computing;SDLC model;green requirement analysis;green design;green implementation;green maintenance;green analysis;Internet-of-Things;cloud computing;virtualization;green data centre;power optimization;grid computing","","","18","","","","","","IEEE","IEEE Conferences"
"Using cloud computing to enhance automatic test equipment testing and maintenance capabilities","D. D. Reitze","Systems Engineer, Northrop Grumman Corporation, Electronic Systems - Defensive Systems Division, Rolling Meadows, Il 60008","2013 IEEE AUTOTESTCON","","2013","","","1","6","The purpose of this paper is to present a conceptual approach and to make practical recommendations on how to improve the current Automatic Test Equipment (ATE) testing and maintenance capabilities by utilizing the existing cloud computing model to build a globally linked ATE maintenance system. The basic tenet of the ATE community is to support a multi-tiered maintenance concept which, in general, is a three tiered system that is composed of organizational maintenance (O-level), intermediate maintenance (I-level), and depot maintenance (D-level) organizations. The goal of the ATE is to (1) quickly and accurately detect and isolate each fault, (2) provide software tools for analyzing historical data, and (3) gather, manage, and distribute accurate and reliable maintenance information for the failed Unit Under Test (UUT). The ATE system should provide services that will (1) maintain a repository of information that will improve fault detection and isolation, allow for off-platform assessments, document failures, and help quantify corrective actions, (2) reduce false UUT pulls, and (3) reduce repair time by prompting repair procedures. Furthermore, the ATE system should provide additional services that will help optimize the time to diagnose problems by using collected failure information and by recommending entry points into the Test Program Set (TPS) software. It should also present information to the ATE maintainer to aid in informed repair decisions which could be in the form of pilot debrief results, platform Built In Test (BIT) results, O-level test outcomes and corrective actions, and maintenance and usage history of the platform and UUT. So, based on this definition of ATE maintenance the use of cloud computing can be used to provide services to improve the overall ATE testing throughput which will result in bottom line improvements to ATE life cycle costs. By using cloud computing, which is defined to be “a model for enabling ubiquitous, convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction”, users can develop cloud computing models that will provide access to application software and databases that can be used to build a globally linked ATE maintenance system. This paper will discuss the essential characteristics of the cloud computing models and define the various flavors of cloud offerings available to designers today. This paper will also analyze the cloud computing model to arrive at a conceptual approach that can be used to enhance the current ATE Testing and Maintenance capabilities. Practical recommendations will be discussed on how to transform the current ATE Testing and Maintenance capabilities into the specific cloud computing model offerings in order to help configure a globally linked ATE maintenance system.","1088-7725;1558-4550","978-1-4673-5683-1978-1-4673-5681","10.1109/AUTEST.2013.6645048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645048","ATE;Cloud Computing;architecture;maintenance","Cloud computing;Computational modeling;Maintenance engineering;Organizations;Security;Standards organizations","automatic test software;cloud computing;fault diagnosis;maintenance engineering","built in test;test program set software;failure information;repair;fault isolation;fault detection;unit under test;depot maintenance;intermediate maintenance;organizational maintenance;globally linked ATE maintenance system;automatic test equipment testing;cloud computing","","1","4","","","","","","IEEE","IEEE Conferences"
"Multi-objective Cross-Project Defect Prediction","G. Canfora; A. De Lucia; M. Di Penta; R. Oliveto; A. Panichella; S. Panichella","NA; NA; NA; NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation","","2013","","","252","261","Cross-project defect prediction is very appealing because (i) it allows predicting defects in projects for which the availability of data is limited, and (ii) it allows producing generalizable prediction models. However, existing research suggests that cross-project prediction is particularly challenging and, due to heterogeneity of projects, prediction accuracy is not always very good. This paper proposes a novel, multi-objective approach for cross-project defect prediction, based on a multi-objective logistic regression model built using a genetic algorithm. Instead of providing the software engineer with a single predictive model, the multi-objective approach allows software engineers to choose predictors achieving a compromise between number of likely defect-prone artifacts (effectiveness) and LOC to be analyzed/tested (which can be considered as a proxy of the cost of code inspection). Results of an empirical evaluation on 10 datasets from the Promise repository indicate the superiority and the usefulness of the multi-objective approach with respect to single-objective predictors. Also, the proposed approach outperforms an alternative approach for cross-project prediction, based on local prediction upon clusters of similar classes.","2159-4848","978-0-7695-4968-2978-1-4673-5961","10.1109/ICST.2013.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569737","Cross-project defect prediction;multi-objective optimization;search-based software engineering","Predictive models;Software;Data models;Measurement;Logistics;Inspection;Accuracy","genetic algorithms;pattern clustering;regression analysis;search problems;software engineering;software management","multiobjective cross-project defect prediction accuracy;generalizable prediction models;project heterogeneity;multiobjective logistic regression model;genetic algorithm;defect-prone artifacts;LOC testing;Promise repository;software engineers;LOC analysis;GA","","36","21","","","","","","IEEE","IEEE Conferences"
"Adaptation of Coupling-Based Reliability Testing for Safety-Relevant Software","M. Meitner; F. Saglietti","NA; NA","26th International Conference on Architecture of Computing Systems 2013","","2013","","","1","7","This article presents a refined approach to quantitative software reliability assessment taking into account coverage of interactions and relevance of variables. For this purpose, an automatic test generation procedure is presented, based on a multi-objective optimization problem to be solved by genetic algorithms. The applicability of the resulting approach is finally illustrated via an interaction-intensive component-based example.","","978-3-8007-3492","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468873","","Software reliability;Testing;Reliability theory;Optimization;Automated test generation","","","","","","","","","","","VDE","VDE Conferences"
"A Dynamic Multiarmed Bandit-Gene Expression Programming Hyper-Heuristic for Combinatorial Optimization Problems","N. R. Sabar; M. Ayob; G. Kendall; R. Qu","Data Mining and Optimization Research Group, Centre for Artificial Intelligent, Universiti Kebangsaan Malaysia, Bangi, Malaysia; Data Mining and Optimization Research Group, Centre for Artificial Intelligent, Universiti Kebangsaan Malaysia, Bangi, Malaysia; ASAP Research Group, School of Computer Science, University of Nottingham, Nottingham, U.K.; ASAP Research Group, School of Computer Science, University of Nottingham, Nottingham, U.K.","IEEE Transactions on Cybernetics","","2015","45","2","217","228","Hyper-heuristics are search methodologies that aim to provide high-quality solutions across a wide variety of problem domains, rather than developing tailor-made methodologies for each problem instance/domain. A traditional hyper-heuristic framework has two levels, namely, the high level strategy (heuristic selection mechanism and the acceptance criterion) and low level heuristics (a set of problem specific heuristics). Due to the different landscape structures of different problem instances, the high level strategy plays an important role in the design of a hyper-heuristic framework. In this paper, we propose a new high level strategy for a hyper-heuristic framework. The proposed high-level strategy utilizes a dynamic multiarmed bandit-extreme value-based reward as an online heuristic selection mechanism to select the appropriate heuristic to be applied at each iteration. In addition, we propose a gene expression programming framework to automatically generate the acceptance criterion for each problem instance, instead of using human-designed criteria. Two well-known, and very different, combinatorial optimization problems, one static (exam timetabling) and one dynamic (dynamic vehicle routing) are used to demonstrate the generality of the proposed framework. Compared with state-of-the-art hyper-heuristics and other bespoke methods, empirical results demonstrate that the proposed framework is able to generalize well across both domains. We obtain competitive, if not better results, when compared to the best known results obtained from other methods that have been presented in the scientific literature. We also compare our approach against the recently released hyper-heuristic competition test suite. We again demonstrate the generality of our approach when we compare against other methods that have utilized the same six benchmark datasets from this test suite.","2168-2267;2168-2275","","10.1109/TCYB.2014.2323936","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824192","CHeSC;dynamic optimization;gene expression programming;hyper-heuristic;timetabling;vehicle routing;CHeSC;dynamic optimization;gene expression programming;hyper-heuristic;timetabling;vehicle routing","Vehicle dynamics;Biological cells;Optimization;Search problems;Heuristic algorithms;Programming;Gene expression","combinatorial mathematics;dynamic programming;search problems","dynamic multiarmed bandit-gene expression programming hyper-heuristic;combinatorial optimization problems;search methodologies;high level strategy;heuristic selection mechanism;hyper-heuristic framework;online heuristic selection mechanism","Algorithms;Artificial Intelligence;Cybernetics;Models, Genetic;Software","34","51","","","","","","IEEE","IEEE Journals & Magazines"
"An evolutionary approach for test program compaction","R. Cantoro; M. Gaudesi; E. Sanchez; P. Schiavone; G. Squillero","Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy; Politecnico di Torino, Torino, Italy","2015 16th Latin-American Test Symposium (LATS)","","2015","","","1","6","The increasing complexity of electronic components based on microprocessors and their use in safety-critical application - like automotive devices - make reliability a critical aspect. During the life cycle of such products, it is needed to periodically check whether the processor cores are working correctly. In most cases, this task is performed by running short, fast and specialized test programs that satisfies in-field testing requirements. This paper proposes a method that exploits an evolutionary-computation technique for the automatic compaction of these in-field oriented test programs. The aim of the proposed approach is twofold: reduce execution time and memory occupation, while maintaining the fault coverage of the original test program. Experimental results gathered on miniMIPS, a freely available 5-stage pipelined processor core, demonstrate the effectiveness of the proposed technique.","2373-0862","978-1-4673-6710","10.1109/LATW.2015.7102406","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102406","on-line testing;software based self-test;computational intelligence","Compaction;Optimization;Microprocessors;Circuit faults;Built-in self-test;Clocks","evolutionary computation;integrated circuit reliability;microprocessor chips","test program compaction;electronic component complexity;microprocessors;automotive devices;reliability;evolutionary-computation technique;in-field oriented test programs;miniMIPS;5-stage pipelined processor core","","2","16","","","","","","IEEE","IEEE Conferences"
"Investigating the Correspondence between Mutations and Static Warnings","C. A. D. Araújo; M. E. Delamaro; J. C. Maldonado; A. M. R. Vincenzi","NA; NA; NA; NA","2015 29th Brazilian Symposium on Software Engineering","","2015","","","1","10","This paper provides evidences on the correspondence between mutations and static warnings. We used mutation operators as a fault model to evaluate the direct correspondence between mutations and static warnings. The main advantage of using mutation operators is that they generate a large number of programs containing faults of different types, which can be used to decide the ones most probable to be detected by static analyzers. Since static analyzers, in general, report a substantial number of false positive warnings, the intention of this study is to define a prioritization approach of static warnings based on the probability they correspond to a true positive and lead to detect software faults. The results obtained for a set of open-source programs indicate that a correspondence exist when considering specific mutation operators such that static warnings may be prioritized based on their correspondence level with mutations.","","978-1-4673-9272","10.1109/SBES.2015.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328004","Software testing;mutants;warnings;static analysis;static analyzer evaluation","Java;Software;XML;Testing;Standards;Databases;Data mining","program diagnostics;public domain software;software fault tolerance","static warnings;static analyzers;false positive warnings;prioritization approach;open-source programs;software faults;specific mutation operators;correspondence level","","","29","","","","","","IEEE","IEEE Conferences"
"Security testing in the cloud by means of ethical worm","E. Benkhelifa; T. Welsh","Faculty of Computing, Engineering and Sciences, Staffordshire University, ST 18 OAD, UK; Faculty of Computing, Engineering and Sciences, Staffordshire University, STI8 OAD, UK","2013 IEEE Globecom Workshops (GC Wkshps)","","2013","","","500","505","As Cloud Computing continues to evolve the majority of research tends to lean towards optimising, securing and improving Cloud technologies. Less work appears which leverages the architectural and economic advantages of the Cloud. This paper examines the Cloud as a security testing environment, having a number of purposes such as penetration testing, and the dynamic creation and testing of environments for learning about malicious processes and testing security concepts. A novel experiment into malicious software propagation using ethical worms is developed and tested as a proof of concept to be adopted as a novel approach for security testing in the cloud. The work presented in the paper is unprecedented, to the best of the authors' knowledge.","2166-0077","978-1-4799-2851","10.1109/GLOCOMW.2013.6825037","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825037","Cloud Computing;Ethical Worm;Cloud Security/Testing;Malicious and Malware","Grippers;Security;Testing;Cloud computing;Computer architecture;Biological system modeling","cloud computing;invasive software;program testing;software architecture","ethical worm;cloud computing;cloud technology;architectural advantage;economic advantage;security testing environment;penetration testing;dynamic creation;malicious software propagation","","3","15","","","","","","IEEE","IEEE Conferences"
"Detecting Hardware Trojan through heuristic partition and activity driven test pattern generation","Xue Mingfu; Hu Aiqun; Li Guyue","Research Center of Information Security, College of Information Science and Engineering, Southeast University, Sipailou 2nd, 210096, Nanjing, China; Research Center of Information Security, College of Information Science and Engineering, Southeast University, Sipailou 2nd, 210096, Nanjing, China; Research Center of Information Security, College of Information Science and Engineering, Southeast University, Sipailou 2nd, 210096, Nanjing, China","2014 Communications Security Conference (CSC 2014)","","2014","","","1","6","Hardware Trojan has emerged as an impending security threat to many critical systems. However, detecting hardware Trojan is extremely difficult due to Trojans are always triggered by rare events. Side-channel signal analysis is effective in detecting Trojan but facing the challenge with process variation and environment noise in nanotechnology. Moreover, side-channel approaches that analyze global signals cannot scale well to large circuits. This paper presents a heuristic partition and test pattern generation based localized signal analysis method for hardware Trojan detection. First, we partition the design into regions controlled by scan chains. Then a test vector ordering algorithm is used to generate optimized vectors which can magnify the activity in the target region where Trojan may be located. At last, power ports are placed in each region to measure the localized transient current anomalies for Trojan detection, while a signal calibration technique is used to eliminate the negative effect of process variation and noise. We evaluate our approach on ISCAS89 benchmark circuits and the results show that the proposed scheme can magnify the detection sensitivity in multiples from the state-of-the-art. Two further benefits of this method are that it can scale well to large circuits and determine Trojan's location.","","978-1-84919-844","10.1049/cp.2014.0728","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6992221","hardware security;hardware Trojan detection;heuristic partition;test pattern generation","","automatic test pattern generation;invasive software;nanotechnology","hardware Trojan detection;heuristic partition;activity driven test pattern generation;security threat;side-channel signal analysis;process variation;environment noise;nanotechnology;global signal analysis;localized signal analysis method;scan chains;test vector ordering algorithm;localized transient current anomaly measurement;ISCAS89 benchmark circuits;signal calibration technique;power ports","","6","","","","","","","IET","IET Conferences"
"Towards a framework for automatic correction of anti-patterns","R. Morales","SWAT, Polytechnique Montr&#x00E9;al, Canada","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","","2015","","","603","604","One of the biggest concerns in software maintenance is design quality; poor design hinders software maintenance and evolution. One way to improve design quality is to detect and correct anti-patterns (i.e., poor solutions to design and implementation problems), for example through refactorings. There are several approaches to detect anti-patterns, that rely on metrics and structural properties. However, finding a specific solution to remove anti-patterns is a challenging task as candidate refactorings can be conflicting and their number very large, making it costly. Hence, development teams often have to prioritize the refactorings to be applied on a system. In addition to this, refactoring is risky, since non-experienced developers can change the behaviour of a system, without a comprehensive test suite. Therefore, there is a need for tools that can automatically remove anti-patterns. We will apply meta-heuristics to propose a technique for automated refactoring that improves design quality.","1534-5351","978-1-4799-8469","10.1109/SANER.2015.7081891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081891","","Software systems;Software maintenance;Measurement;Space exploration;Software engineering;Correlation","software maintenance;software quality","automatic antipattern correction;software maintenance;software design quality;software evolution;antipattern detection;automatic antipattern removal;metaheuristics;automated refactoring","","1","8","","","","","","IEEE","IEEE Conferences"
"Efficient Leveraging of Symbolic Execution to Advanced Coverage Criteria","S. Bardin; N. Kosmatov; F. Cheynier","NA; NA; NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation","","2014","","","173","182","Automatic test data generation (ATG) is a major topic in software engineering. In this paper, we bridge the gap between the coverage criteria supported by state-of-the-art white-box ATG technologies, especially Dynamic Symbolic Execution, and advanced coverage criteria found in the literature. We define a new testing criterion, label coverage, and prove it to be both expressive and amenable to efficient automation. We propose several innovative techniques resulting in an effective blackbox support for label coverage, while a direct approach induces an exponential blow-up of the search space. Experiments show that our optimisations yield very significant savings allowing to leverage ATG to label coverage with only a slight overhead.","2159-4848","978-1-4799-2255","10.1109/ICST.2014.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823879","Testing;symbolic execution;coverage criteria","Instruments;Testing;Labeling;Integrated circuits;Automation;Standards;Aerospace electronics","program testing;software engineering","search space;black-box support;label coverage;testing criterion;advanced coverage criteria;dynamic symbolic execution;state-of-the-art white-box ATG technologies;software engineering;automatic test data generation","","9","43","","","","","","IEEE","IEEE Conferences"
"Distribution system expansion planning with renewable sources. Case study: IEEE 33 test system","C. Barbulescu; S. Kilyeni; A. Simo; A. Vernica","Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania; Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania; Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania; Power Systems Department, Power Systems Analysis and Optimization Research Center, Politehnica University Timisoara, Romania","2015 IEEE Eindhoven PowerTech","","2015","","","1","6","The renewable sources' influence regarding the distribution network expansion planning is tackled. Two cases are involved into discussion: with and without renewable sources. The obtained expansion solutions are analyzed for each case and a final one is proposed. Thus, network reconfiguration is used and technical losses minimization. The power flow is computed using conventional methods, but the optimal power flow and distribution network expansion are performed using genetic algorithms (GA). Thus, the authors have developed an own software-tool in Matlab environment.","","978-1-4799-7693-5978-1-4799-7692","10.1109/PTC.2015.7232590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7232590","power distribution;power system planning;renewable energy sources;software-tool","Planning;Load flow;Genetic algorithms;Biological cells;Sociology;Statistics","genetic algorithms;IEEE standards;load flow;minimisation;power distribution planning;renewable energy sources","IEEE 33 Test System;renewable source;distribution network expansion planning;distribution power system;distribution power system expansion planning;technical loss minimization;optimal power flow;genetic algorithm;GA","","","10","","","","","","IEEE","IEEE Conferences"
"Porting and Optimizing SOAP2 on Loongson Architecture","Q. Luo; G. Liu; Z. Ming; F. Xiao","NA; NA; NA; NA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","566","570","SOAP2 is a high precision, high performance and high efficiency alignment tool which is developed by the Beijing Genomics Institute. SOAP2 has become one of the mainstream biological sequence alignment software. However, SOAP2 is developed for X86 architecture, so we port and optimize SOAP2 to Loongson platform. Firstly, we make SOAP2 to be able to run on Loongson machine correctly. Second, its speed, sensitivity and hits of result are improved. This paper has comprehensively conducted a transplantation and optimization of SOAP2 in many aspects including using Loongson multimedia instructions, using thread pool and a new algorithm proposed by this paper on Loongson platform. We tested it on human chromosome 1 and found that optimized SOAP2 not only reduced memory overhead but also improved alignment speed by about 55%, sensitivity from about 66.8% to about 77% and always reported more mapped reads and occurrences. At the same time, in the process of optimization, this paper also proposes a method for applying Loongson multimedia instructions more efficiently. This provides a new guidance for software transplantation and optimization on Loongson platform.","","978-1-4799-8937-9978-1-4799-8936","10.1109/HPCC-CSS-ICESS.2015.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336218","SOAP;Loongson;optimization;SIMD;sequence alignment","Registers;Multimedia communication;Bioinformatics;Algorithm design and analysis;Genomics;Optimization;Software","bioinformatics;multimedia computing;parallel processing;software tools","SOAP2 optimization;SOAP2 porting;high efficiency alignment tool;biological sequence alignment software;X86 architecture;Loongson architecture platform;Loongson multimedia instructions;human chromosome;reduced memory overhead;software transplantation;thread pool","","1","16","","","","","","IEEE","IEEE Conferences"
"Exploiting abstraction, learning from random simulation, and SVM classification for efficient dynamic prediction of software health problems","M. N. Velev; C. Zhang; P. Gao; A. D. Groce","Aries Design Automation, LLC; School of EECS, Oregon State University; Aries Design Automation, LLC; School of EECS, Oregon State University","Sixteenth International Symposium on Quality Electronic Design","","2015","","","412","418","We present industrial experience on software health monitoring. Our goal was to determine whether we can predict abnormal behavior, based on data captured from software system interfaces. To analyze the system state and predict software health problems, we used Support Vector Machine (SVM) based analysis. To train the SVM, we exploited random testing with feedback and swarm testing with feedback to generate traces that exercise diverse scenarios, including both normal and abnormal behaviors that can be classified based on the system state after completing an API call. We then used the resulting classifier produced by the SVM-based analysis to predict whether an API call will result in abnormal behavior, given the input values to the API, and other system information. We applied this procedure to a subset of the API functions in the YAFFS2 flash file system, with the objective of predicting whether the health parameter of available free space will go below a threshold, relative to the total space in the flash file system. For several API functions, we achieved prediction accuracy of over 96%. We attribute the high prediction accuracy to using random testing with feedback that is optimized to produce execution traces with highly diverse behavior, which combined with the chosen representation of the system state and length of the traces resulted in a sufficient number of training vectors with diverse numeric values for the API functions of interest.","1948-3287;1948-3295","978-1-4799-7581-5978-1-4799-7580","10.1109/ISQED.2015.7085461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7085461","Abstraction;learning;SVM;software health monitoring","Testing;Support vector machines;Training;File systems;Instruments;Monitoring","application program interfaces;pattern classification;program diagnostics;program testing;software engineering;support vector machines","abstraction;random simulation learning;SVM classification;software health problems;software health monitoring;software system interfaces;support vector machine;random testing;feedback;swarm testing;API call;API functions;YAFFS2 flash file system","","","21","","","","","","IEEE","IEEE Conferences"
"An enhanced architecture for high performance BIST TPG","Nandini priya M.; R. Brindha","II M.E VLSI DESIGN, Avinashilingam Institute for Home Science and Higher, Education for Women-University, Coimbatore, India; Avinashilingam Institute for Home Science and Higher, Education for Women-University, Coimbatore, India","2015 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)","","2015","","","1","6","This paper proposes a methodology to generate the multiple test patterns varying in single bit position for built-in-self-test (BIST). The traditional patterns which were generated using Linear feedback shift registers lack correlation between consecutive test vectors. So, in order to improve correlation between the subsequent test vectors, the patterns were produced using Gray counter and Decoder. The Area optimization is achieved by reducing the total number of gate count to implement the design. In order to optimize the power, the number of toggles between the subsequent test vectors is curtailed. The generated test patterns have an advantage of minimum transition sequence. Simulation results on multiplier circuit shows a reduction of 54% in area overhead and 12% in power overhead compared to pattern generation using Reconfigurable Johnson counter and LFSR. 100% fault coverage is achieved while generating patterns using gray counter, decoder and accumulator architecture. Time coverage is same as time required for generating patterns using existing methodology. The methodology for producing the test vectors for BIST is coded using VHDL and simulations were performed with ModelSim 10.0b. The Area utilization and the power report were obtained with the help of Xilinx ISE 9.1 software.","","978-1-4799-6818-3978-1-4799-6817-6978-1-4799-6816","10.1109/ICIIECS.2015.7193023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7193023","Built-in-self-test (BIST);Test Pattern Generator (TPG);Multiple Single Input Change Vector (MSIC);Linear feedback shift registers (LFSR)","Radiation detectors;Clocks;Built-in self-test;Decoding;Circuit faults;Logic gates","built-in self test;counting circuits;logic testing;multiplying circuits;shift registers","BIST;TPG;built-in-self-test;linear feedback shift registers;gray counter;gray decoder;area optimization;multiplier circuit;Johnson counter;LFSR;accumulator architecture;VHDL;ModelSim 10.0b;area utilization;Xilinx ISE 9.1 software","","","14","","","","","","IEEE","IEEE Conferences"
"Network management and performance monitoring using Software Defined Networks","Veena S; R. P. Rustagi; K. N. B. Murthy","PES University, Bangalore, India; PES University, Bangalore, India; PES University, Bangalore, India","20th Annual International Conference on Advanced Computing and Communications (ADCOM)","","2014","","","29","31","Nowadays Computer Networks are in constant evolution. New methodologies and techniques are required to manage the network. Software Defined Network (SDN) is a new approach to manage and maintain the network infrastructure. Mininet is the most popular emulator to understand and experience how SDN works. Our proposed research addresses the following problems. i) Enhance default version of Mininet to create the custom topologies in a user friendly manner without explicitly writing any programs. The users/researchers can even specify the different link parameters like Bandwidth, Latency etc. through a simple text file and create the network topology of their choice to test their innovative ideas and protocols. ii) Our next focus is to collect the information from the SDN controller, preserve it for historical analysis and present via a web interface. iii) Use SDN to reduce the Layer-2 broadcast traffic in Data Centres. This is being achieved by making use of hierarchical switch connectivity with pseudo MAC addresses. iv) Our last focus is the introduction of Cross Layer Optimization algorithms for better resource utilization in Data Centres. These new techniques help the network administrator and the end user to experience better application and network performance.","","978-1-4673-6509-3978-1-4673-6508","10.1109/ADCOM.2014.7103245","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7103245","Address Resolution Protocol;Data Centre;Mininet;Openflow;Software Defined Network","Network topology;Switches;Topology;Protocols;Software defined networking;Random access memory","access protocols;computer centres;computer network management;computer network performance evaluation;resource allocation;software defined networking;telecommunication network topology","network performance monitoring;network management;software defined networks;computer networks;network infrastructure;Mininet;network topology;protocols;SDN controller;historical analysis;Web interface;Layer-2 broadcast traffic;data centres;hierarchical switch connectivity;pseudo MAC addresses;cross layer optimization algorithm","","","11","","","","","","IEEE","IEEE Conferences"
"Openstack Cloud Performance Optimization using Linux Services","M. A. Ismail; M. F. Ismail; H. Ahmed","NA; NA; NA","2015 International Conference on Cloud Computing (ICCC)","","2015","","","1","4","Cloud computing is an emerging trend for online computing and resource management. OpenStack is one of the most widely used open source platform for establishing public and private clouds. Besides its implementation issues much of the importance is being given on its performance optimization. This paper focuses on such a study to improve performance of an OpenStack Cloud and its further optimization by configuring the Linux services like rpcbind for Remote Procedure Calls (RPC). The benchmarking is done by executing multiple MapReduce benchmarks on a configured multi-node Hadoop cluster on the cloud as a software service. The results indicates that after configuring the rpcbind service in the cloud, the performance of its software services enhances by means of reduced elapsed time of the benchmarks executed.","","978-1-4673-6618-2978-1-4673-6617","10.1109/CLOUDCOMP.2015.7149648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7149648","","Benchmark testing;Cloud computing;Linux;Servers;Optimization;Measurement","cloud computing;Linux;open systems;parallel processing;public domain software;remote procedure calls;resource allocation;software performance evaluation","OpenStack cloud performance optimization;Linux services;cloud computing;online computing;resource management;open source platform;public clouds;private clouds;remote procedure calls;RPC;MapReduce benchmarks;multinode Hadoop cluster;cloud as a software service;rpcbind service","","7","8","","","","","","IEEE","IEEE Conferences"
"On Combining Model-Based Analysis and Testing","M. Saadatmand; M. Sjödin","NA; NA","2013 10th International Conference on Information Technology: New Generations","","2013","","","260","266","Testing a computer system is a challenging task, both due to the large number of possible test cases and the limited resources allocated for testing activities. This means that only a subset of all possible test cases can be chosen to test a system, and therefore the decision on the selection of test cases becomes important. The result of static analysis of a system can be used to help with this decision, in the context of model-based development of systems, this means that the analysis performed on a system model can be used to prioritize and guide the testing efforts. Furthermore, since models allow expression of non-functional requirements (such as performance, timing and security), model-guided testing can be used to direct testing towards specific parts of the system which have large impact on such requirements. In this paper, we focus on modeling and trade-off analysis of non-functional requirements and how static analysis helps to identify problematic parts of a system and thus guide the selection of test cases to target such parts.","","978-0-7695-4967-5978-0-7695-4967","10.1109/ITNG.2013.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614319","Model-based development;static analysis;model-based testing;non-functional requirements;test-case prioritization","Unified modeling language;Testing;Analytical models;Software;Security;Timing;Batteries","program diagnostics;systems analysis","model-based analysis;model-based testing;computer system testing;static analysis;model-based development;nonfunctional requirements;model-guided testing;trade-off analysis","","3","20","","","","","","IEEE","IEEE Conferences"
"Optimization of the peak-load condenser operation for a fossil-fired generation unit with ACC","H. Yang; S. Lu","Thermal Department, Hebei Electric Power Research Institute, Shijiazhuang, Hebei PRC; Thermal Department, Hebei Electric Power Research Institute, Shijiazhuang, Hebei PRC","2014 IEEE PES Asia-Pacific Power and Energy Engineering Conference (APPEEC)","","2014","","","1","4","For improving the economic benefit of the generation unit with ACC (air-cooled condenser) in summer season, a peak-load condenser system was updated for the ACC system. Optimization of the peak-load condenser operation was studied using the site test and system simulation methods. Based on the site test data, certain characteristic performance parameter was calculated such as cleaniness factor and etc. Final optimized operation scheme was studied using the simulation software for various ambient conditions. It is shown that, based on the current equipment conditions, for most of the weather conditions except for winter, the peak-load condenser with one cooling pump in operation can improve the unit performance.","2157-4839;2157-4847","978-1-4799-7537","10.1109/APPEEC.2014.7066045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7066045","Optimization;Peak-load Condenser;Fossil-fired Generation Unit;ACC (air-cooled condenser)","Cooling;Poles and towers;Load modeling;Data models;Adaptation models;Optimization;Meteorology","condensers (steam plant);optimisation;pumps;steam power stations","fossil-fired generation unit;peak-load condenser operation optimization;air-cooled condenser;ACC system;site test method;system simulation method;cooling pump","","","","","","","","","IEEE","IEEE Conferences"
"The potential of polyhedral optimization: An empirical study","A. Simbürger; S. Apel; A. Größlinger; C. Lengauer","University of Passau, Germany; University of Passau, Germany; University of Passau, Germany; University of Passau, Germany","2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2013","","","508","518","Present-day automatic optimization relies on powerful static (i.e., compile-time) analysis and transformation methods. One popular platform for automatic optimization is the polyhedron model. Yet, after several decades of development, there remains a lack of empirical evidence of the model's benefits for real-world software systems. We report on an empirical study in which we analyzed a set of popular software systems, distributed across various application domains. We found that polyhedral analysis at compile time often lacks the information necessary to exploit the potential for optimization of a program's execution. However, when conducted also at run time, polyhedral analysis shows greater relevance for real-world applications. On average, the share of the execution time amenable to polyhedral optimization is increased by a factor of nearly 3. Based on our experimental results, we discuss the merits and potential of polyhedral optimization at compile time and run time.","","978-1-4799-0215","10.1109/ASE.2013.6693108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693108","","Optimization;Benchmark testing;Arrays;Analytical models;Program processors;Multimedia communication;Time measurement","optimisation;optimising compilers;program diagnostics","polyhedral optimization;present-day automatic optimization;static analysis;transformation methods;polyhedron model;real-world software systems;polyhedral analysis;program execution","","2","37","","","","","","IEEE","IEEE Conferences"
"Coordination of directional overcurrent relays in meshed power systems using hybrid genetic algorithm optimization","F. B. Bottura; W. M. S. Bernardes; M. Oleskovicz; E. N. Asada; S. A. de Souza; M. J. Ramos","University of S&#x00E3;o Paulo, S&#x00E3;o Carlos School of Engineering, Brazil; University of S&#x00E3;o Paulo, S&#x00E3;o Carlos School of Engineering, Brazil; University of S&#x00E3;o Paulo, S&#x00E3;o Carlos School of Engineering, Brazil; University of S&#x00E3;o Paulo, S&#x00E3;o Carlos School of Engineering, Brazil; Companhia de Transmiss&#x00E3;o de Energia El&#x00E9;trica Paulista, Department of Operation Analysis, Brazil; Companhia de Transmiss&#x00E3;o de Energia El&#x00E9;trica Paulista, Department of Operation Analysis, Brazil","12th IET International Conference on Developments in Power System Protection (DPSP 2014)","","2014","","","1","6","This work presents a Hybrid Genetic Algorithm (HGA) for directional overcurrent relays coordination in a real meshed power system, which is part of the Brazilian power transmission system. The HGA combines linear programming and genetic algorithm to find the correct relay settings that enable system coordination. The coordination is achieved by minimizing the relay operating time. The power system was simulated via CAPE (Computer-Aided Protection Engineering) software for short circuits studies. Solutions calculated by the HGA are compared with those obtained from the Discrete Particle Swarm Optimization (DPSO) technique which is a modified version of the classical particle swarm optimization. Finally, HGA provides good results for coordination of the considered overcurrent relays in the test system, according to the results given by the DPSO.","","978-1-84919-834","10.1049/cp.2014.0146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822954","Power system protection;interconnected systems;genetic algorithms;linear programming;optimization","","genetic algorithms;linear programming;minimisation;overcurrent protection;particle swarm optimisation;power system protection;power transmission protection;relay protection","hybrid genetic algorithm optimization;HGA;directional overcurrent relay coordination;real meshed power system;Brazilian power transmission system;linear programming;relay operating time minimization;computer-aided protection engineering software;CAPE software;short circuit study;discrete particle swarm optimization technique;DPSO technique","","1","","","","","","","IET","IET Conferences"
"Integrating assertion stack and caching to optimize constraint solving","Quanchen Zou; Jing An; Wei Huang; Wenqing Fan","School of Computer Science, Communication University of China, Beijing, China; School of Computer Science, Communication University of China, Beijing, China; School of Computer Science, Communication University of China, Beijing, China; School of Computer Science, Communication University of China, Beijing, China","2015 4th International Conference on Computer Science and Network Technology (ICCSNT)","","2015","01","","397","401","Although significant advances have been made over the past few years, constraint solving is still the main bottleneck in symbolic execution. In fact, it often dominates the cost in overall performance. Thus, it is important to reduce the overhead in calling a constraint solver by exploring domain specific insights. In this paper, we propose a novel approach to optimize constraint solving. Our approach integrates two existing strategies: assertion stack and caching. We implemented a proof-of-concept tool based on Symbolic Pathfinder, a popular symbolic execution framework for JAVA programs. We have evaluated the tool on a set of open source software and the results showed that our approach can significantly improve the performance of constraint solving.","","978-1-4673-8173-4978-1-4673-8172","10.1109/ICCSNT.2015.7490777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490777","symbolic execution;constraint solving;software testing","Java;Concrete;Time factors;Constraint optimization;Open source software","constraint handling;program diagnostics","constraint solving;assertion stack;caching;Symbolic Pathfinder;symbolic execution framework;JAVA programs;open source software","","","20","","","","","","IEEE","IEEE Conferences"
"A Low-Rank Approximation-Based Transductive Support Tensor Machine for Semisupervised Classification","X. Liu; T. Guo; L. He; X. Yang","Department of Mathematics, South China University of Technology, Guangzhou, China; Department of Mathematics, South China University of Technology, Guangzhou, China; School of Computer Science and Software Engineering, Computer Vision Institute, Shenzhen University, Shenzhen, China; School of Software Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Image Processing","","2015","24","6","1825","1838","In the fields of machine learning, pattern recognition, image processing, and computer vision, the data are usually represented by the tensors. For the semisupervised tensor classification, the existing transductive support tensor machine (TSTM) needs to resort to iterative technique, which is very time-consuming. In order to overcome this shortcoming, in this paper, we extend the concave-convex procedure-based transductive support vector machine (CCCP-TSVM) to the tensor patterns and propose a low-rank approximation-based TSTM, in which the tensor rank-one decomposition is used to compute the inner product of the tensors. Theoretically, concave-convex procedure-based TSTM (CCCP-TSTM) is an extension of the linear CCCP-TSVM to tensor patterns. When the input patterns are vectors, CCCP-TSTM degenerates into the linear CCCP-TSVM. A set of experiments is conducted on 23 semisupervised classification tasks, which are generated from seven second-order face data sets, three third-order gait data sets, and two third-order image data sets, to illustrate the performance of the CCCP-TSTM. The results show that compared with CCCP-TSVM and TSTM, CCCP-TSTM provides significant performance gain in terms of test accuracy and training speed.","1057-7149;1941-0042","","10.1109/TIP.2015.2403235","National Science Foundation of China; Major Project of the National Social Science Foundation of China; Open Project of Key Laboratory of Symbolic Computation and Knowledge Engineering through the Chinese Ministry of Education; Science and Technology Plan Project of Guangzhou City; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7041228","Concave-convex procedure;semi-supervised tensor classification;tensor rank-one decomposition;transductive support tensor machine (TSTM);transductive support vector machine (TSVM);Concave-convex procedure;semi-supervised tensor classification;tensor rank-one decomposition;transductive support tensor machine (TSTM);transductive support vector machine (TSVM)","Tensile stress;Optimization;Support vector machines;Vectors;Educational institutions;Classification algorithms;Semisupervised learning","computer vision;face recognition;image classification;learning (artificial intelligence)","approximation-based transductive support tensor machine;machine learning;pattern recognition;image processing;computer vision;semisupervised tensor classification;iterative technique;concave-convex procedure-based transductive support vector machine;tensor patterns;approximation-based TSTM;tensor rank-one decomposition;concave-convex procedure-based TSTM;linear CCCP-TSVM;face data sets;gait data sets;image data sets;CCCP-TSTM","","13","62","","","","","","IEEE","IEEE Journals & Magazines"
"URMG: Enhanced CBMG-based method for automatically testing web applications in the cloud","X. Xu; H. Jin; S. Wu; L. Tang; Y. Wang","Services Computing Technology and System Lab and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan 430074, China; Services Computing Technology and System Lab and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan 430074, China; Services Computing Technology and System Lab and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan 430074, China; Services Computing Technology and System Lab and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan 430074, China; Services Computing Technology and System Lab and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan 430074, China","Tsinghua Science and Technology","","2014","19","1","65","75","To satisfy the rapid growth of cloud technologies, a large number of web applications have been developed and deployed, and these applications are being run in clouds. Due to the scalability provided by clouds, a single web application may be concurrently visited by several millions or billions of users. Thus, the testing and performance evaluations of these applications are increasingly important. User model based evaluations can significantly reduce the manual work required, and can enable us to determine the performance of applications under real runtime environments. Hence, it has become one of the most popular evaluation methods in both industry and academia. Significant efforts have focused on building different kinds of models using mining web access logs, such as Markov models and Customer Behavior Model Graph (CBMG). This paper proposes a new kind of model, named the User Representation Model Graph (URMG), which is built based on CBMG. It uses an algorithm to refine CBMG and optimizes the evaluations execution process. Based on this model, an automatic testing and evaluation system for web applications is designed, implemented, and deployed in our test cloud, which is able to execute all of the analysis and testing operations using only web access logs. In our system, the error rate caused by random access to applications in the execution phase is also reduced, and the results show that the error rate of the evaluation that depends on URMG is 50% less than that which depends on CBMG.","1007-0214","","10.1109/TST.2014.6733209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6733209","cloud;web application;performance evaluation;customer behavior;user representation","Topology;Testing;Performance evaluation;User interfaces;Data models;Delays;Clustering algorithms","cloud computing;data mining;graph theory;Markov processes;program testing;software performance evaluation","URMG;enhanced CBMG-based method;automatic Web applications testing;cloud technologies;performance evaluations;Web access log mining;Markov models;customer behavior model graph;user representation model graph;evaluations execution process;test cloud;random applications access;execution phase","","","","","","","","","TUP","TUP Journals & Magazines"
"A Novel Developer Ranking Algorithm for Automatic Bug Triage Using Topic Model and Developer Relations","T. Zhang; G. Yang; B. Lee; E. K. Lua","NA; NA; NA; NA","2014 21st Asia-Pacific Software Engineering Conference","","2014","1","","223","230","Recently, bug resolution has become a pivotal issue for software maintenance where recommendations for appropriate fixers are an important task. Some approaches (e.g., Social network and machine learning techniques) exist that can achieve automatic bug triage (i.e., Developer recommendation). This paper proposes a new method to recommend the most suitable fixer for bug resolution. Different from previous approaches, the proposed approaches combine topic model and developer relations (e.g., Bug reporter and assignee) to capture developers' interest and experience on specific bug reports, we can arrange for the most appropriate developer to fix a new bug when it comes in. We evaluate the performance of our method using three large-scale open-source projects, including Eclipse, Mozilla Fire fox, and Net beans. The experimental results reveal that our approach outperforms other recommendation methods for developers.","1530-1362;1530-1362","978-1-4799-7426-9978-1-4799-7425","10.1109/APSEC.2014.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091314","topic model;developer relations;developer recommendation;automatic bug triage;software maintenance","Computer bugs;Correlation;Feature extraction;Accuracy;Social network services;Testing;Algorithm design and analysis","program debugging;public domain software;software maintenance","developer ranking algorithm;automatic bug triage;topic model;developer relations;bug resolution;software maintenance;social network;machine learning techniques;bug reporter;bug assignee;developer interest;developer experience;large-scale open-source projects;Eclipse;Mozilla Firefox;Netbeans","","8","22","","","","","","IEEE","IEEE Conferences"
"An advanced software model for optimization of self-organizing neural networks oriented on implementation in hardware","M. Kolasa; R. Długosz","UTP University of Science and Technology, Faculty of Telecommunications, Computer Science and Electrical Engineering, ul. Kaliskiego 7, 85-796 Bydgoszcz, Poland; UTP University of Science and Technology, Faculty of Telecommunications, Computer Science and Electrical Engineering, ul. Kaliskiego 7, 85-796 Bydgoszcz, Poland","2015 22nd International Conference Mixed Design of Integrated Circuits & Systems (MIXDES)","","2015","","","266","271","In this paper we present an advanced software tool designed for a multi-criteria optimization of self-organizing neural networks (SOMs) for their effective implementation in hardware. Problems that we have to deal with in this type of implementations are radically different from those that occur in only pure software realizations. Therefore, although there are many available systems to simulate NNs, they are not useful for our purposes. The proposed system allows to investigate the influence of various physical constraints on the learning process of the NN. It enables a modification of more than sixty parameters, so almost any learning scenario, as well as almost each configuration of the NN can be tested. It is possible to run multiple tests in accordance with a created lists of tasks, in which particular parameters are changed in loops with a certain range and with a given step. This allows to carry out in a relatively short time thousands of simulations for different combinations of particular parameters. Finally, it allows to select the most efficient combinations of the parameters looking from the point of view of the effective transistor level implementation.","","978-8-3635-7807-7978-8-3635-7806","10.1109/MIXDES.2015.7208524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208524","Self-organizing maps;Modeling;Optimization;Hardware Design","Neurons;Artificial neural networks;Hardware;Topology;Computational modeling;Complexity theory;Software","application specific integrated circuits;hardware-software codesign;optimisation;self-organising feature maps","advanced software tool;multi-criteria optimization;self-organizing neural networks;SOM;physical constraints;learning process;transistor level implementation","","2","27","","","","","","IEEE","IEEE Conferences"
"Particle Swarm Optimization for Multiple Dipole Modeling of Space Equipment","E. Carrubba; A. Junge; F. Marliani; A. Monorchio","Department of Information Engineering, University of Pisa, Pisa, Italy; European Space Agency, European Space Research and Technology Centre, Noordwijk, The Netherlands; European Space Agency, European Space Research and Technology Centre, Noordwijk, The Netherlands; Department of Information Engineering, University of Pisa, Pisa, Italy","IEEE Transactions on Magnetics","","2014","50","12","1","10","An advanced modeling algorithm based on particle swarm optimization (PSO) has been developed to solve multiple dipole modeling (MDM) problems in space applications. MDM is a method to represent spacecraft units as a set of equivalent magnetic dipoles able to reconstruct, in the far-field distance, the same magnetostatic field. This procedure allows preparing a magnetic model of the spacecraft during design and development phases. Moreover, it allows refined prediction of magnetic cleanliness for space missions with equipment susceptible to magnetic fields. Indeed, owing to the increase of missions requiring magnetostatic cleanliness, such characterization becomes increasingly important. To validate the PSO procedure, synthetic data have been initially used, generated using a software simulator. Algorithm performance has been tested through measured data acquired using the Mobile Coil Facility located at the European Space Research and Technology Centre in The Netherlands. Starting from measured data, the algorithm iteratively identifies the values of the unknowns, positions, and magnetic moments of the equivalent dipoles that best match the measured field. Since the problem is ill posed, several solutions are possible. To develop a reliable algorithm, some test cases have been analyzed where the expected solution is known. This allowed improving the algorithm leading to satisfying results.","0018-9464;1941-0069","","10.1109/TMAG.2014.2334277","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6851131","Aerospace testing;inverse problem;magnetic field measurement;magnetic moments;particle swarm optimization (PSO)","Magnetic field measurement;Particle swarm optimization;Inverse problems;Magnetic moments;Space vehicles;Aerospace testing","aerospace instrumentation;aerospace testing;hygiene;inverse problems;magnetic field measurement;magnetic moments;magnetostatics;particle swarm optimisation;space vehicles","magnetic field measurement;space equipment susceptibility;ill pose problem;European Space Research and Technology Centre;software simulator;PSO procedure;magnetostatic cleanliness;space missions;magnetic model;magnetostatic field;equivalent magnetic dipole;spacecraft units;MDM problems;space applications;advanced modeling algorithm;multiple dipole modeling problems;particle swarm optimization","","4","25","","","","","","IEEE","IEEE Journals & Magazines"
"Randomizing regression tests using game theory","N. Kukreja; W. G. J. Halfond; M. Tambe","University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA; University of Southern California, Los Angeles, USA","2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2013","","","616","621","As software evolves, the number of test-cases in the regression test suites continues to increase, requiring testers to prioritize their execution. Usually only a subset of the test cases is executed due to limited testing resources. This subset is often known to the developers who may try to “game” the system by committing insufficiently tested code for parts of the software that will not be tested. In this new ideas paper, we propose a novel approach for randomizing regression test scheduling, based on Stackelberg games for deployment of scarce resources. We apply this approach to randomizing test cases in such a way as to maximize the testers' expected payoff when executing the test cases. Our approach accounts for resource limitations (e.g., number of testers) and provides a probabilistic distribution for scheduling test cases. We provide an example application of our approach showcasing the idea of using Stackelberg games for randomized regression test scheduling.","","978-1-4799-0215","10.1109/ASE.2013.6693122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693122","","Games;Testing;Security;Equations;Vectors;Game theory;Schedules","game theory;processor scheduling;program testing;regression analysis;statistical distributions","randomizing regression tests;game theory;software evolution;regression test suites;randomizing regression test scheduling;Stackelberg games;probabilistic distribution;scheduling test cases;randomized regression test scheduling","","2","20","","","","","","IEEE","IEEE Conferences"
"PADM: Page Rank-Based Anomaly Detection Method of Log Sequences by Graph Computing","X. Yan; W. Zhou; Y. Gao; Z. Zhang; J. Han; G. Fu","NA; NA; NA; NA; NA; NA","2014 IEEE 6th International Conference on Cloud Computing Technology and Science","","2014","","","700","703","With the popularity of various software applications in cloud computing, software exception becomes an important issue. How to detect the exceptions more quickly seems to be crucial for the software service company. To solve the above problem, this paper presents an efficient log anomaly detection method named PADM (Page Rank-based Anomaly Detection Method) based on the graph computing algorithm. In this method, the logs are transformed into a graph to represent the complex relationship between the log records, then we design an extended Page Rank algorithm based on the graph to get the importance score for each log. After that, we compare the scores to that of the training logs to determine whether they are abnormal or not. Finally, we compare PADM with other anomaly detection methods on the real logs, and the results show that it outperforms the currently widely used mechanisms with higher accuracy, lower time complexity and better scalability.","","978-1-4799-4093","10.1109/CloudCom.2014.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7037742","graph representation;anomaly detection;pagerank value;graph computing;log sequences","Time complexity;Algorithm design and analysis;Markov processes;Testing;Scalability;Software;Training","graph theory;security of data;Web sites","PADM;Page Rank-based anomaly detection method;log sequences;graph computing;software exception;software service company;log anomaly detection method;log records;training logs","","4","8","","","","","","IEEE","IEEE Conferences"
"Examining the effectiveness of machine learning algorithms for prediction of change prone classes","R. Malhotra; M. Khanna","Department of Software Engineering, Delhi Technological University, 110042, India; Acharaya Narendra Dev College, University of Delhi, 110019, India","2014 International Conference on High Performance Computing & Simulation (HPCS)","","2014","","","635","642","Managing change in the early stages of a software development life cycle is an effective strategy for developing a good quality software at low costs. In order to manage change, we use software quality models which can efficiently predict change prone classes and hence guide developers in appropriate distribution of limited resources. This study examines the effectiveness of ten machine learning algorithms for developing such software quality models on three object-oriented software data sets. We also compare the performance of machine learning algorithms with the widely used logistic regression technique and statistically rank various algwith the help of Friedman test.","","978-1-4799-5313-4978-1-4799-5312-7978-1-4799-5311","10.1109/HPCSim.2014.6903747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903747","Change proneness;Object- Oriented metrics;Open source;Software Quality","Measurement;Prediction algorithms;Software;Software algorithms;Data models;Predictive models;Support vector machines","learning (artificial intelligence);object-oriented programming;regression analysis;software development management;software quality","change prone classes prediction;change management;software development life cycle;software quality models;machine learning algorithms;object-oriented software data sets;logistic regression technique;Friedman test","","2","37","","","","","","IEEE","IEEE Conferences"
"Package Optimization of the Cilium-Type MEMS Bionic Vector Hydrophone","L. Linxian; Z. Wendong; Z. Guojun; X. Chenyang","Science and Technology on Electronic Test and Measurement Laboratory, Key Laboratory of Instrumentation Science and Dynamic Measurement, Ministry of Education, North University of China, Taiyuan, China; Science and Technology on Electronic Test and Measurement Laboratory, Key Laboratory of Instrumentation Science and Dynamic Measurement, Ministry of Education, North University of China, Taiyuan, China; Science and Technology on Electronic Test and Measurement Laboratory, Key Laboratory of Instrumentation Science and Dynamic Measurement, Ministry of Education, North University of China, Taiyuan, China; Science and Technology on Electronic Test and Measurement Laboratory, Key Laboratory of Instrumentation Science and Dynamic Measurement, Ministry of Education, North University of China, Taiyuan, China","IEEE Sensors Journal","","2014","14","4","1185","1192","We optimize the package of the cilium-type MEMS bionic vector hydrophone introduced by Chenyang and Wendong in 2007, which has the disadvantages of a low receiving sensitivity, narrow frequency band, and fluctuating frequency response curve. Initially, a full parametric analysis of frequency response and sensitivity with different material sound transparent cap were made. Then, we propose and realize an umbrella-type packaged structure with high receiving sensitivity and wide frequency band. The theoretical analysis and simulation analysis are conducted by ANSYS software and LMS virtual. Lab acoustic software. Finally, to verify the practicability of the package, the umbrella-type packaged hydrophone calibration was carried out in the National Defense Underwater Acoustics Calibration Laboratory of China. The test results show that the performance of umbrella-type hydrophone has been greatly improved compared with the previous packaged hydrophone: exhibiting a receiving sensitivity of -178 dB (increasing by 20 dB, 0-dB reference 1 V/μPa), the frequency response ranging from 20 Hz to 2 kHz (broaden one times), the fluctuation of frequency response curve within ±2 dB, and a good dipole directivity.","1530-437X;1558-1748;2379-9153","","10.1109/JSEN.2013.2293669","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678570","MEMS;umbrella-type packaged structure;receiving sensitivity;frequency response","Sonar equipment;Acoustics;Sensitivity;Resonant frequency;Materials;Vectors;Frequency response","biocybernetics;calibration;electronics packaging;hydrophones;microsensors;optimisation","China;National Defense Underwater Acoustics Calibration Laboratory;umbrella-type packaged hydrophone calibration;Lab acoustic software;LMS virtual;ANSYS software;material sound transparent cap;parametric analysis;narrow frequency band;fluctuating frequency response curve;cilium-type MEMS bionic vector hydrophone;package optimization;gain -178 dB;gain 20 dB;frequency 20 Hz to 2 kHz","","5","21","","","","","","IEEE","IEEE Journals & Magazines"
"Compatibility Testing Service for Mobile Applications","T. Zhang; J. Gao; J. Cheng; T. Uehara","NA; NA; NA; NA","2015 IEEE Symposium on Service-Oriented System Engineering","","2015","","","179","186","As more and more mobile applications are developed, mobile app testing and quality assurance have become very important. Due to the diversity of mobile devices and platforms, compatibility testing for mobile apps has been identified as one urgent and challenging issue. There are two major reasons contributing to this issue. They are: a) the large number of mobile devices with diverse features and platforms which are upgraded frequently, b) a higher cost and complexity in mobile app compatibility testing. This paper proposes one optimized compatibility testing strategy using a statistical approach to reduce test costs, and improve engineer's operation efficiency. The paper provides a solution to generate an optimized compatibility test sequence for mobile apps using the K-Means statistical algorithm. A compatibility testing service has been proposed for mobile apps. Moreover, two case study results are reported to demonstrate its potential application and effectiveness.","","978-1-4799-8356","10.1109/SOSE.2015.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7133527","software testing;mobile testing;compatibility testing;clustering algorithm;test coverage","Mobile communication;Testing;Mobile handsets;Servers;Databases;Home appliances;Analytical models","mobile computing;statistical analysis","compatibility testing service;mobile app testing;quality assurance;mobile devices;mobile app compatibility testing;statistical approach;k-means statistical algorithm","","6","24","","","","","","IEEE","IEEE Conferences"
"Security analysis for IPv6 neighbor discovery protocol","F. Xiaorong; L. Jun; J. Shizhun","Software Quality Testing Engineering Research Center, China Electronic Product Reliability and Environmental Testing Research Institute, Guangzhou, Guangdong, 510610, China; Software Quality Testing Engineering Research Center, China Electronic Product Reliability and Environmental Testing Research Institute, Guangzhou, Guangdong, 510610, China; Software Quality Testing Engineering Research Center, China Electronic Product Reliability and Environmental Testing Research Institute, Guangzhou, Guangdong, 510610, China","2013 2nd International Symposium on Instrumentation and Measurement, Sensor Network and Automation (IMSNA)","","2013","","","303","307","Neighbor Discovery Protocol (NDP) is used by IPv6 nodes to discover other nodes on the link, which assigns link-layer address to find routers, so as to obtain reachability information about the paths to active neighbors. This paper presents security threats and deeply analysis for IPv6 NDP and discusses about typical attacks in details. Meanwhile, the attack tools developed in accordance with NDP are demonstrated, which provides certain theoretical basis for improving the security feature of NDP. Finally, an improved security strategy based on IPSec AH and MAC address option is proposed which aims at providing effectively defense against denial of service attacks and redirection attacks. The optimized NDP process has certain significance for strengthening the security of IPv6 network.","","978-1-4799-2716-6978-1-4799-2715","10.1109/IMSNA.2013.6743275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6743275","IPv6;Neighbor Discovery Protocol;Security Attack;Authentication","Protocols;IP networks;Authentication;Computer crime;Servers;Instrumentation and measurement","access protocols;computer network security;IP networks;telecommunication network routing;transport protocols","security analysis;IPv6 neighbor discovery protocol;IPv6 nodes;node discovery;link-layer address assignment;router finding;reachability information;active neighbor path;security threat;IPv6 NDP;attack tools;security feature;security strategy;IPSec AH;MAC address option;denial of service attack;redirection attack;NDP process optimization;IPv6 network security","","2","8","","","","","","IEEE","IEEE Conferences"
"Bitwidth-optimized hardware accelerators with software fallback","A. Klimovic; J. H. Anderson","Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada","2013 International Conference on Field-Programmable Technology (FPT)","","2013","","","136","143","We propose the high-level synthesis of an FPGA-based hybrid computing system, where the implementations of compute-intensive functions are available in both software, and as hardware accelerators. The accelerators are optimized to handle common-case inputs, as opposed to worst-case inputs, allowing accelerator area to be reduced by 28%, on average, while retaining the majority of performance advantages associated with a hardware versus software implementation. When inputs exceed the range that the hardware accelerators can handle, a software fallback is automatically triggered. Optimization of the accelerator area is achieved by reducing datapath widths based on application profiling of variable ranges in software (under typical datasets). The selected widths are passed to a high-level synthesis tool which generates the accelerator for a given function. The optimized accelerators with software fallback capability are generated automatically by our framework, with minimal user intervention. Our study explores the trade-offs of delay and area for benchmarks implemented on an Altera Cyclone II FPGA.","","978-1-4799-2198-0978-1-4799-2199","10.1109/FPT.2013.6718343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6718343","","Hardware;Benchmark testing;Program processors;Optimization;Dynamic range;Field programmable gate arrays","field programmable gate arrays;high level synthesis;optimisation","bitwidth-optimized hardware accelerators;high-level synthesis;FPGA-based hybrid computing system;compute-intensive functions;performance advantages;optimization;accelerator area;datapath widths;optimized accelerators;software fallback capability;Altera Cyclone II FPGA","","1","21","","","","","","IEEE","IEEE Conferences"
"Optimizing binary translation of dynamically generated code","B. Hawkins; B. Demsky; D. Bruening; Q. Zhao","University of California, Irvine; University of California, Irvine; Google, Inc; Google, Inc","2015 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)","","2015","","","68","78","Dynamic binary translation serves as a core technology that enables a wide range of important tools such as profiling, bug detection, program analysis, and security. Many of the target applications often include large amounts of dynamically generated code, which poses a special performance challenge in maintaining consistency between the source application and the translated application. This paper introduces two approaches for optimizing binary translation of JITs and other dynamic code generators. First we present a system of efficient source code annotations that allow developers to demarcate dynamic code regions and identify code changes within those regions. The second technique avoids the annotation and source code requirements by automatically inferring the presence of a JIT and instrumenting its write instructions with translation consistency operations. We implemented these techniques in DynamoRIO and demonstrate performance improvements over the state-of-the-art DBT systems on JIT applications as high as 7.3× over base DynamoRIO and Pin.","","978-1-4799-8161","10.1109/CGO.2015.7054188","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7054188","","Optimization;Engines;Instruments;Benchmark testing;Security;Complexity theory;Data structures","program diagnostics;program interpreters;security of data;software fault tolerance;software maintenance;software performance evaluation;source code (software);system recovery","dynamic binary translation optimization;profiling;bug detection;program analysis;security;consistency maintenance;JIT;dynamic code generators;source code annotations;dynamic code regions;code change identification;translation consistency operations;performance improvements;DynamoRIO","","8","29","","","","","","IEEE","IEEE Conferences"
"An Automatic Device for Power Line Quality Test and Life Test of Switch","H. Huang; G. Wang; J. Xu; F. Zhang","NA; NA; NA; NA","2015 Sixth International Conference on Intelligent Systems Design and Engineering Applications (ISDEA)","","2015","","","108","111","This article is intended to design an automated device for power line quality and life test. The designs of this device contain three various parts: mechanical system, control and PC software. The control algorithm and the program execution flow will be described in detail in this paper. Using the raspberry pi to control the step motor and to realize the switch of machine on and off is the main purpose. The device can work automatically and be controlled remotely. It works well and it achieves our requirements.","","978-1-4673-9393-5978-1-4673-9392","10.1109/ISDEA.2015.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7462574","Raspberry Pi Step Motor Automated Device Power Line Quality Test","Switches;Software;Standards;Gears;Force;Optimization","control engineering computing;machine control;power supply quality;stepping motors","automatic device;power line quality test;switch life test;raspberry pi;step motor","","","4","","","","","","IEEE","IEEE Conferences"
"Reformulating Branch Coverage as a Many-Objective Optimization Problem","A. Panichella; F. M. Kifetew; P. Tonella","NA; NA; NA","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","","2015","","","1","10","Test data generation has been extensively investigated as a search problem, where the search goal is to maximize the number of covered program elements (e.g., branches). Recently, the whole suite approach, which combines the fitness functions of single branches into an aggregate, test suite-level fitness, has been demonstrated to be superior to the traditional single-branch at a time approach. In this paper, we propose to consider branch coverage directly as a many-objective optimization problem, instead of aggregating multiple objectives into a single value, as in the whole suite approach. Since programs may have hundreds of branches (objectives), traditional many-objective algorithms that are designed for numerical optimization problems with less than 15 objectives are not applicable. Hence, we introduce a novel highly scalable many-objective genetic algorithm, called MOSA (Many-Objective Sorting Algorithm), suitably defined for the many- objective branch coverage problem. Results achieved on 64 Java classes indicate that the proposed many-objective algorithm is significantly more effective and more efficient than the whole suite approach. In particular, effectiveness (coverage) was significantly improved in 66% of the subjects and efficiency (search budget consumed) was improved in 62% of the subjects on which effectiveness remains the same.","2159-4848","978-1-4799-7125","10.1109/ICST.2015.7102604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102604","","Optimization;Search problems;Genetic algorithms;Sorting;Sociology;Statistics;Next generation networking","genetic algorithms;Java;program testing;search problems","branch coverage reformulation;many-objective optimization problem;test data generation;search problem;fitness functions;test suite-level fitness;many-objective genetic algorithm;MOSA;many-objective sorting algorithm;many-objective branch coverage problem;Java classes","","22","26","","","","","","IEEE","IEEE Conferences"
"Are Anti-patterns Coupled? An Empirical Study","W. Ma; L. Chen; Y. Zhou; B. Xu; X. Zhou","NA; NA; NA; NA; NA","2015 IEEE International Conference on Software Quality, Reliability and Security","","2015","","","242","251","The interactions between anti-patterns are claimed to affect maintenance. However, little work has been conducted to examine how anti-patterns interact. In this paper, we aim to investigate which pairs of anti-patterns tend to be coupled, i.e., interact with each other. We employ Fisher's exact test and Wilcoxon rank-sum test to identify coupled anti-patterns in the same class and coupled classes. Analyzing the relationships amongst 10 kinds of anti-patterns in five open-source projects, our results show that 1) several kinds of anti-patterns tend to be coupled, but some are conflicting, 2) the effect of anti-patterns on their dependent or co-changed ones are significant but small, 3) in ArgoUML, Xalan and Xerces-J, the classes infected with dependent anti-patterns are mostly (69.9% ~ 100%) modified in maintenance activities. Our findings offer empirical evidences for the existence of anti-pattern interactions, which provides valuable implications for practitioners and researchers.","","978-1-4673-7989-2978-1-4673-7988","10.1109/QRS.2015.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272939","anti-pattern;interaction;coupled;inter-pattern;maintenance","Maintenance engineering;Java;Software maintenance;Unified modeling language;XML;Open source software","program testing;public domain software;software maintenance","Fisher exact test;Wilcoxon rank-sum test;coupled anti-pattern identification;open-source projects;Xerces-J;ArgoUML;Xalan;anti-pattern interactions","","2","31","","","","","","IEEE","IEEE Conferences"
"Towards a Scalable Cloud Platform for Search-Based Probabilistic Testing","L. M. Rose; S. Poulding; R. Feldt; R. F. Paige","NA; NA; NA; NA","2013 IEEE International Conference on Software Maintenance","","2013","","","480","483","Probabilistic testing techniques that sample input data at random from a probability distribution can be more effective at detecting faults than deterministic techniques. However, if overly large (and therefore expensive) test sets are to be avoided, the probability distribution from which the input data is sampled must be optimised to the particular software-under-test. Such an optimisation process is often resource-intensive. In this paper, we present a prototypical cloud platform-and architecture-that permits the optimisation of such probability distributions in a scalable, distributed and robust manner, and thereby enables cost-effective probabilistic testing.","1063-6773","978-0-7695-4981","10.1109/ICSM.2013.77","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676937","","Testing;Probabilistic logic;Unified modeling language;Software;Computer architecture;Robustness;Databases","cloud computing;program testing;statistical distributions","scalable cloud platform;search-based probabilistic testing;probability distribution;software-under-test;cost-effective probabilistic testing","","","11","","","","","","IEEE","IEEE Conferences"
"A model-based embedded control hardware/software co-design approach for optimized sensor selection of industrial systems","K. M. Deliparaschos; K. Michail; S. G. Tzafestas; A. C. Zolotas","Department of Electrical and Computer Engineering and Informatics, Cyprus University of Technology, Limassol, CY; Department of Electrical and Computer Engineering and Informatics, Cyprus University of Technology, Limassol, CY; School of Electrical and Computer Engineering, National Technical University of Athens, GR; School of Engineering, University of Lincoln, UK","2015 23rd Mediterranean Conference on Control and Automation (MED)","","2015","","","889","894","In this work, a Field Programmable Gate Array (FPGA)-based embedded software platform coupled with a software-based plant, forming a Hardware-In-the-Loop (HIL), is used to validate a systematic sensor selection framework. The systematic sensor selection framework combines multi-objective optimization, Linear-Quadratic-Gaussian (LQG) control, and the nonlinear model of a maglev suspension. The physical process that represents the suspension plant is realized in a high-level system modeling environment, while the LQG controller is implemented on an FPGA. FPGAs allow to rapidly evaluate algorithms and test designs under real-world scenarios avoiding heavy time penalty associated with Hardware Description Language (HDL) simulators. Moreover, the HIL technique implemented shows a significant speed-up in the required execution time when compared to the software-based model.","","978-1-4799-9936","10.1109/MED.2015.7158858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158858","Sensor selection;embedded control;FPGA design;maglev;electromagnetic suspension;observer-based controller;Hardware-In-the-Loop;FPGA-In-the-Loop","Mathematical model;Field programmable gate arrays;Hardware design languages;MATLAB;Control systems;Analytical models","control engineering computing;embedded systems;field programmable gate arrays;hardware description languages;hardware-software codesign;linear quadratic Gaussian control;magnetic levitation;magnetic variables control;optimisation","model-based embedded control hardware-software codesign approach;optimized sensor selection;industrial systems;field programmable gate array-based embedded software platform;FPGA;hardware-in-the-loop;HIL;systematic sensor selection framework;multiobjective optimization;linear-quadratic-Gaussian control;LQG;maglev suspension;nonlinear model;suspension plant;high-level system modeling environment;hardware description language simulators;HDL;HIL technique;software-based model","","2","18","","","","","","IEEE","IEEE Conferences"
"Using Collective Intelligence to Support Multi-objective Decisions: Collaborative and Online Preferences","D. Cinalli; L. Martí; N. Sanchez-Pi; A. C. B. Garcia","NA; NA; NA; NA","2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)","","2015","","","82","85","This research indicates a novel approach of evolutionary multi-objective optimization algorithms meant for integrating collective intelligence methods into the optimization process. The new algorithms allow groups of decision makers to improve the successive stages of evolution via users' preferences and collaboration in a direct crowdsourcing fashion. They can, also, highlight the regions of Pareto frontier that are more relevant to the group of decision makers as to focus the search process mainly on those areas. As part of this work we test the algorithms performance when face with some synthetic problem as well as a real-world case scenario.","","978-1-4673-9775","10.1109/ASEW.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426642","Collective intelligence;preferences;reference points;evolutionary multi-objective optimization algorithms","Optimization;Collaboration;Statistics;Sociology;Benchmark testing;Evolutionary computation","decision making;evolutionary computation;outsourcing;Pareto optimisation","Pareto frontier;crowdsourcing fashion;evolutionary multiobjective optimization algorithm;online preference;collaborative preference;multiobjective decision support;collective intelligence","","","10","","","","","","IEEE","IEEE Conferences"
"Static Prediction of Loop Iteration Counts Using Machine Learning to Enable Hot Spot Optimizations","D. Tetzlaff; S. Glesner","NA; NA","2013 39th Euromicro Conference on Software Engineering and Advanced Applications","","2013","","","300","307","In general, program execution spends most of the time in a small fraction of code called hot spots of the program. These regions where optimization would be most beneficial are mainly composed of loops and must be identified to enable hot spot optimizations. Consequently, identifying hot spots involves determining loop iteration counts arising at run-time of the program, which is often not knowable in advance at run-time and even less statically knowable at compile time of the application by using only static analyses. In this paper we present a sophisticated approach using machine learning techniques to automatically generate heuristics that provide the compiler with knowledge of this run-time behavior, hence yielding more precise heuristics than those generated by pure static analyses. Our experimental results demonstrate the accuracy of our approach and show the general applicability to a wide range of programs with different behavior as we have used 175 programs of 12 benchmark suites in total from different real-world application domains for our experiments. Among others, our approach eliminates the need for manual annotations of run-time information, which automates and facilitates the development of complex software, thus improving the software engineering process.","1089-6503;2376-9505","978-0-7695-5091","10.1109/SEAA.2013.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619525","machine learning;system analysis;compiler optmization","Benchmark testing;Optimization;Training;Accuracy;Software engineering;Program processors","learning (artificial intelligence);optimising compilers;program control structures;program diagnostics","machine learning;hot-spot optimizations;program execution;static loop iteration count prediction;program run-time behavior;application compile time;static analysis;automatic heuristics generation;benchmark suites;run-time information annotation;complex software development;software engineering process improvement","","1","35","","","","","","IEEE","IEEE Conferences"
"Experimental test system for distance estimation of standardized passive UHF RFID systems","S. Kunze; R. Poschl; A. Grzemba","Deggendorf Institute of Technology, Campus Freyung, D-94078 Freyung, Germany; Deggendorf Institute of Technology, Campus Freyung, D-94078 Freyung, Germany; Deggendorf Institute of Technology, D-94441, Germany","2015 25th International Conference Radioelektronika (RADIOELEKTRONIKA)","","2015","","","30","33","In this paper an experimental test setup for the signal-strength based distance estimation of passive radio-frequency identification (RFID) tags is proposed. The system shall use freely available ultra high frequency (UHF) tags, and shall require no modifications of the tags or the communication protocol. A test setup based on a software defined radio (SDR) system is implemented. However, first measurements with this simple setup show, that a reliable distance estimation is not possible. Due to interference effects, such as multi-path propagation, the signal strength is not mainly dominated by its distance dependency. Therefore, the test system is modified and a number of optimization approaches is proposed. The goal hereby is to reduce the impact of interference on the signal strength, so that the measured value exhibits a mainly distance dependent behavior. Besides frequency hopping, various forms of antenna diversity are discussed. Using these optimization techniques it's possible to implement a test system that allows a fairly reliable distance estimation under controlled laboratory conditions.","","978-1-4799-8119-9978-1-4799-8117","10.1109/RADIOELEK.2015.7128981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7128981","","Signal to noise ratio;Radiofrequency identification;Antenna measurements;Frequency measurement;Estimation;Spatial diversity;Antennas","antennas;distance measurement;estimation theory;optimisation;radiofrequency identification;radiofrequency interference;radiotelemetry;software radio","Standardized Passive UHF RFID Systems;signal-strength based distance estimation;ultrahigh frequency radiofrequency identification system;communication protocol;software defined radio system;SDR system;interference effect;multipath propagation;optimization approach;frequency hopping system;antenna diversity;optimization technique","","","7","","","","","","IEEE","IEEE Conferences"
"Multi-objective Construction of an Entire Adequate Test Suite for an EFSM","N. Asoudeh; Y. Labiche","NA; NA","2014 IEEE 25th International Symposium on Software Reliability Engineering","","2014","","","288","299","In this paper we propose a method and a tool to generate test suites from extended finite state machines, accounting for multiple (potentially conflicting) objectives. We aim at maximizing coverage and feasibility of a test suite while minimizing similarity between its test cases and minimizing overall cost. Therefore, we define a multi-objective genetic algorithm that searches for optimal test suites based on four objective functions. In doing so, we create an entire test suite at once as opposed to test cases one at a time. Our approach is evaluated on two different case studies, showing interesting initial results.","1071-9458;2332-6549","978-1-4799-6033-0978-1-4799-6032","10.1109/ISSRE.2014.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982635","state-based testing;EFSM;genetic algorithm;multi-objective optimization;case studies","Biological cells;Testing;Genetic algorithms;Linear programming;Optimization;Sociology;Statistics","finite state machines;genetic algorithms;program testing","multiobjective construction;test suite;EFSM;extended finite state machines;multiobjective genetic algorithm","","7","45","","","","","","IEEE","IEEE Conferences"
"Architecting to Ensure Requirement Relevance: Keynote TwinPeaks Workshop","J. Bosch","NA","2015 IEEE/ACM 5th International Workshop on the Twin Peaks of Requirements and Architecture","","2015","","","1","2","Research has shown that up to two thirds of features in software systems are hardly ever used or not even used at all. This represents a colossal waste of R&amp;D resources and occurs across the industry. On the other hand, product management and many others work hard at interacting with customers, building business cases and prioritizing requirements. A fundamentally different approach to deciding what to build is required: requirements should be treated as hypothesis throughout the development process and constant feedback from users and systems in the field should be collected to dynamically reprioritize and change requirements. This requires architectural support beyond the current state of practice as continuous deployment, split testing and data collection need to be an integral part of the architecture. In this paper, we present a brief overview of our research and industry collaboration to address this challenge.","","978-1-4673-7100-1978-1-4673-7099","10.1109/TwinPeaks.2015.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7184704","requirements engineering;software architecture;data-driven development","Companies;Software;Computer architecture;Industries;Testing","formal specification;formal verification;software architecture;software development management;systems analysis","requirement relevance;software system;product management;softwasre development process;constant user feedback;architectural support;continuous deployment;split testing;data collection;industry collaboration","","","6","","","","","","IEEE","IEEE Conferences"
"Low power area optimized novel architecture for Software Defined Radio in FPGA","J. Varghese; L. Mathews","Dept. of Electronics and Communication, Mar Baselios College of Engineering and Technology, Trivandrum, India; Dept. of Electronics and Communication, Mar Baselios College of Engineering and Technology, Trivandrum, India","2014 IEEE International Conference on Advanced Communications, Control and Computing Technologies","","2014","","","522","526","Field Programmable Gated Array (FPGA) provides a way for developing system on chip reconfigurable modules with high performance. In this paper, FPGA architecture for recovering audio signals from digitally modulated frequency wave is proposed, which would be a starting point for developing an efficient Software Defined Radio (SDR) architecture. At the modulator and demodulator sections, a Digital Frequency Generator (DFG) is used for generating the carrier wave by exploiting the quarter wave symmetry of sine or cosine waves with dynamic range of more than 90dB. Digital Phase Locked Loop (DPLL) with DFG is used to demodulate the Frequency Modulated (FM)audio signals. Simulation and synthesis are done using Xilinx 13.1. The proposed architecture works around a frequency of 106MHz and uses 1.032 K equivalent gates when tested using XC3S1600E-4fg320, Spartan 3E board with power consumption of around 120mW.","","978-1-4799-3914-5978-1-4799-3913","10.1109/ICACCCT.2014.7019140","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019140","Adaptive Booth's Algorithm;DFM;DPLL;FPGA;SDR","Finite impulse response filters;OFDM;Read only memory;Logic gates;Mixers;Frequency modulation;Detectors","field programmable gate arrays;frequency modulation;phase locked loops;software radio","low power area optimized novel architecture;software defined radio;field programmable gated array;FPGA architecture;audio signal recovery;digital modulated frequency;SDR architecture;digital frequency generator;DFG;quarter wave symmetry;digital phase locked loop;DPLL;frequency modulated audio signals;Xilinx 13.1;XC3S1600E-4fg320;Spartan 3E board","","","12","","","","","","IEEE","IEEE Conferences"
"Research on axle press-fit defects detection optimizing","C. Peng; X. Gao; Y. Yang; A. Wang; D. Wu; J. Zhao","School of Physical Science and Technology, Southwest Jiaotong University, Chengdu, Sichuan Province, China; School of Physical Science and Technology, Southwest Jiaotong University, Chengdu, Sichuan Province, China; Chengdu Lead Science & Technology Co., Ltd., Sichuan Province, China; Chengdu Lead Science & Technology Co., Ltd., Sichuan Province, China; Chengdu Lead Science & Technology Co., Ltd., Sichuan Province, China; Chengdu Foreign Language School, Chengdu, Sichuan Province, China","2015 IEEE Far East NDT New Technology & Application Forum (FENDT)","","2015","","","277","281","The wheel-set maintenance mainly depends on ultrasonic nondestructive testing techniques to detect defects. Wheels are pressed or shrunk on the axle, and the press-fit has significant influence on ultrasonic detection at press-fit part of the in service axle comparing with bare axle. Limited by the complex testing conditions of the press-fit interface, higher amplitude noise is present in the echo signal. In this paper, simulation and axle detection experiments and filtering algorithms optimization are used to improve the signal-noise ratio. The phased array ultrasonic probe parameters and arrangements are optimized by software simulation and reference axle testing. A series of defect detection experiments between bare axle and axle with press-fit have been complemented by using phased array ultrasonic. Ultrasonic signal processing and denoising algorithm has been conducted to effectively improve the signal-noise ratio.","","978-1-4673-7001-1978-1-4673-7000-4978-1-4673-6999","10.1109/FENDT.2015.7398354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398354","Defect;Detection;Phased Array Ultrasonic;Axle Press-fit","Axles;Acoustics;Testing;Wheels;Arrays;Probes;Fatigue","axles;echo;filtering theory;maintenance engineering;mechanical engineering computing;optimisation;press fitting;signal denoising;ultrasonic materials testing;wheels","axle press-fit defect detection optimization;wheel-set maintenance;ultrasonic nondestructive testing techniques;ultrasonic detection;service axle;complex testing conditions;press-fit interface;amplitude noise;echo signal;filtering algorithm optimization;signal-noise ratio improvement;phased array ultrasonic probe parameters;phased array ultrasonic probe arrangements;software simulation;axle testing;ultrasonic signal processing;denoising algorithm","","","9","","","","","","IEEE","IEEE Conferences"
"SnabbSwitch user space virtual switch benchmark and performance optimization for NFV","M. Paolino; N. Nikolaev; J. Fanguede; D. Raho","Virtual Open Systems Grenoble, France; Virtual Open Systems Grenoble, France; Virtual Open Systems Grenoble, France; Virtual Open Systems Grenoble, France","2015 IEEE Conference on Network Function Virtualization and Software Defined Network (NFV-SDN)","","2015","","","86","92","New paradigms in networking industry, such as Software Defined Networking (SDN) and Network Functions Virtualization (NFV), require the hypervisors to enable the execution of Virtual Network Functions in virtual machines (VMs). In this context, the virtual switch function is critical to achieve carrier grade performance, hardware independence, advanced features and programmability. SnabbSwitch is a virtual switch designed to run in user space with carrier grade performance targets, based on an efficient architecture which has driven the development of vhost-user (now also adopted by OVS-DPDK, the user space implementation of OVS based on Intel DPDK), easy to deploy and to program through its Lua scripting layer. This paper presents the SnabbSwitch virtual switch implementation along with its novelties (the vhost-user implementation and the usage of a trace compiler) and code optimizations, which have been merged in the mainline project repository. Extensive benchmarking activities, whose results are included in this paper, have been carried on to compare SnabbSwitch with other virtual switching solutions (i.e., OVS, OVS-DPDK, Linux Bridge, VFIO and SR-IOV). These results show that SnabbSwitch performs as well as hardware based solutions, such as SR-IOV and VFIO, while allowing for additional functional and flexible operation; they show also that SnabbSwitch is faster than the vhost-user based version (user space) of OVS-DPDK.","","978-1-4673-6884","10.1109/NFV-SDN.2015.7387411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387411","","Switches;Hardware;Kernel;Virtualization;Benchmark testing;Optimization","operating systems (computers);optimisation;software defined networking;virtual machines","SnabbSwitch user space virtual switch benchmark;performance optimization;NFV;networking industry;software defined networking;SDN;network functions virtualization;virtual network functions;virtual machines;VM;virtual switch function;carrier grade performance;hardware independence;advanced features;advanced programmability;carrier grade;trace compiler;code optimizations","","10","31","","","","","","IEEE","IEEE Conferences"
"Benchmarking SDL and CLASP lifecycle","I. El rhaffari; O. Roudiès","Siweb Team, Ecole Mohammadia d'Ingénieurs, Mohammed V-Agdal University, Morocco; Siweb Team, Ecole Mohammadia d'Ingénieurs, Mohammed V-Agdal University, Morocco","2014 9th International Conference on Intelligent Systems: Theories and Applications (SITA-14)","","2014","","","1","6","Processes for secure software development play a crucial role in the software lifecycle. They help organizations to meet security requirements throughout the development lifecycle. Among these processes, OWASP's CLASP and Microsoft's SDL are leaders for security support in the software life cycle. This has prompted researchers to compare and evaluate these two approaches in order to use them in an opportunistic manner. However, these studies focus mainly on the activities identified in each of these approaches. We think that the interested parties point of view is important. So, our research question is: what are the main concerns for the various stakeholders in a secure development lifecycle? And how SDL and CLASP contribute to meet these concerns? This paper aims to study and compare the two approaches with considering three dimensional viewpoints: security and security audit viewpoint, software engineering viewpoint and decider viewpoint according to the stakeholders involved in these processes. Our comparison is based on a number of criteria that we classified according to these 3 viewpoints.","","978-1-4799-3567-3978-1-4799-3566","10.1109/SITA.2014.6847280","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6847280","SDL;CLASP;Benchmark;intentional;security;security audit;software lifecycle","Security;Software;Measurement;Documentation;Benchmark testing;Optimization;Business","benchmark testing;security of data;software performance evaluation;software product lines","Microsoft SDL lifecycle benchmarking;OWASP CLASP lifecycle benchmarking;secure software development lifecycle;software lifecycle;security requirements;security support;security audit viewpoint;software engineering viewpoint;decider viewpoint","","1","17","","","","","","IEEE","IEEE Conferences"
"Identifying Domain Experts in the Blogosphere -- Ranking Blogs Based on Topic Consistency","P. Berger; P. Hennig; C. Meinel","NA; NA; NA","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","","2013","1","","252","259","Current ranking algorithms, such as Page Rank, Technorati authority, and BI-Impact, favor blogs that report on a diversity of topics since those attract a large audience and thus more visitors, links, and comments. On the other side, niche blogs with a very specific topic only attract a small audience and thus have only a small reach. This results in a low ranking from today's blog retrieval systems. We argue that the consistency of a blog, i.e. how focused an author reports on a single topic, is a sign for expert knowledge. To find these blogs is particular important for other domain experts to identify blogs that they would like to follow and stay in active contact. To ease the retrieval of expert blogs, i.e. to separate them from the mass of blogs that report on random topics, we introduce a metric for blogs based on topic consistency. We divide the consistency ranking in four different aspects: (1) intra-post, (2) inter-post, (3) intra-blog, and (4) inter-blog consistency. By evaluating the metric with a test data set of 12,000 crawled blogs, we demonstrate the plausibility of our approach.","","978-0-7695-5145-6978-1-4799-2902","10.1109/WI-IAT.2013.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690023","Web Mining;Social Media;Weblogs;Ranking;Topic Consistency;Experts;Blog Analysis","Blogs;Vectors;Measurement;Joining processes;Media;Publishing;Market research","data integrity;information retrieval;software metrics;Web sites","blog consistency;domain expert identification;expert blog retrieval systems;topic consistency;intrapost consistency ranking;interpost consistency ranking;intrablog consistency ranking;interblog consistency ranking;blogosphere;test data;crawled blogs;metric evaluation","","1","26","","","","","","IEEE","IEEE Conferences"
"Applications of Artificial Bee Colony Optimization technique: Survey","K. S. Kaswan; S. Choudhary; K. Sharma","Department of Computer Science, Banasthali University, Rajasthan- INDIA; Department of Computer Science, Banasthali University, Rajasthan- INDIA; Department of Computer Science & Engineering, Delhi Technological University, INDIA","2015 2nd International Conference on Computing for Sustainable Global Development (INDIACom)","","2015","","","1660","1664","In this paper, an overviews of different application areas are discussed, where the Artificial Bee Colony (ABC) optimization algorithm has applied. Artificial bee colony was introduced by Karabora in 2005. It was developed to solve real parameter optimization problem. BCO's (Bee Colony Optimization) foraging behavior is simulated in artificial bee colony. This system portraying organized team-work well coordinated interaction, labor division, simultaneous task performance and well-knit communication.","","978-9-3805-4416-8978-9-3805-4415-1978-9-3805-4414","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7100529","Swarm Intelligence (SI);Artificial Bee Colony Optimization (ABC);Bee Colony Optimization (BCO);IABC;CABC","Optimization;Algorithm design and analysis;Clustering algorithms;Benchmark testing;Software algorithms;Prediction algorithms;Heuristic algorithms","optimisation","artificial bee colony optimization technique;ABC optimization algorithm;BCO;bee colony optimization foraging behavior","","1","29","","","","","","IEEE","IEEE Conferences"
"Optimum Design of a Dual-Range Force Sensor for Achieving High Sensitivity, Broad Bandwidth, and Large Measurement Range","J. Jiang; W. Chen; J. Liu; W. Chen; J. Zhang","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; Mechatronics Group, Singapore Institute of Manufacturing Technology, Singapore; School of Mechanical Engineering, Beihang University, Beijing, China","IEEE Sensors Journal","","2015","15","2","1114","1123","Force control is very crucial in nanoimprint lithography (NIL). It is necessary to develop a high-performance force sensor to provide real-time force feedback for the control process. Due to the unique procedure of NIL, the developed force sensor should include a high sensitivity, broad bandwidth, and large measurable range. However, these characteristics are normally conflicting in nature and cannot be physically avoided by any force transducers so far. To address this problem, this paper presents a novel dual-range force sensor, and uses a heuristic multiobjective optimization method to make a tradeoff among these characteristics. This method is based on the particle swarm optimization algorithm, meanwhile employs the Pareto ranking scheme to find optimal solutions. Through proper optimization, not only the three characteristics are compromised, the lowest stress concentration of the sensor body is maintained as well. To demonstrate the effectiveness of the optimization, numerical simulations with finite-element software COMSOL are conducted. A prototype sensor is then fabricated according to the optimization results. The simulation and prototype test results indicate that the optimized sensor has a resolution down to 800 $\mu $ N, a bandwidth up to 150 Hz, and a measurable range up to 180 N. All the results prove that the developed force sensor possesses a good property for high-performance force measurement, and satisfies the needs of NIL as well.","1530-437X;1558-1748;2379-9153","","10.1109/JSEN.2014.2360885","National Natural Science Foundation of China; Specialized Research Fund for the Doctoral Program of Higher Education of China; Innovation Foundation of BUAA for Ph.D. Graduates; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913502","Force sensor;compliant mechanism;nanoimprint;structural optimization;Force sensor;compliant mechanism;nanoimprint;structural optimization","Force sensors;Optimization;Sensitivity;Force measurement;Force;Bandwidth","finite element analysis;force sensors;nanolithography;Pareto optimisation;particle swarm optimisation;prototypes;soft lithography","nanoimprint lithography;prototype sensor;finite-element software COMSOL;numerical simulations;stress concentration;Pareto ranking scheme;particle swarm optimization algorithm;heuristic multiobjective optimization method;dual-range force sensor","","7","25","","","","","","IEEE","IEEE Journals & Magazines"
"Application of improved genetic algorithm in automatic test paper generation","Kui Zhang; Lingchen Zhu","Hubei Province Key Laboratory of Intelligent Information, Processing and Real-time Industrial, College of Computer Science & Technology, Wuhan University of Science & Technology, China; Hubei Province Key Laboratory of Intelligent Information, Processing and Real-time Industrial, College of Computer Science & Technology, Wuhan University of Science & Technology, China","2015 Chinese Automation Congress (CAC)","","2015","","","495","499","Automatic test paper generation system is to automatically generate papers by computer from test database with many constraint conditions according to requirements of teachers and teaching. It could greatly reduce teachers' work, and make the difficulty coefficient of test paper reasonable. The system plays an important role in reform of examination system. Traditional genetic algorithm uses binary code, but because the binary string is too long, it cannot control well the number of question types. However, the system with decimal code could avoid that problem. In the process of genetic manipulation, crossover operation takes subsection crossover, i.e. single point crossover within one question type. Therefore, the whole chromosome is multi-point crossover, which makes the result more reasonable. This system makes use of global optimization and fast convergence speed of genetic algorithm to design an intelligent algorithm for automatically generating test papers. We have established and described the chromosome structure of test paper and the fitness function, designed genetic operators, and completed corresponding genetic algorithm application software to realize the automatic generation of test papers. Experimental results show that the automatic test paper generation system based on genetic algorithm achieves optimization of efficiency and reasonability of difficulty coefficient of test paper.","","978-1-4673-7189-6978-1-4673-7188","10.1109/CAC.2015.7382551","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382551","Automatic test paper generation system;Genetic algorithm;Subsection crossover","Genetic algorithms;Sociology;Statistics;Encoding;Biological cells;Genetics;Optimization","binary codes;computer aided instruction;convergence;genetic algorithms;teaching","genetic algorithm;automatic test paper generation;test database;teachers;teaching requirements;examination system;binary code;binary string;decimal code;genetic manipulation;crossover operation;subsection crossover;single point crossover;multipoint crossover;global optimization;fast convergence speed;intelligent algorithm;chromosome structure;fitness function;genetic operators","","2","7","","","","","","IEEE","IEEE Conferences"
"Quantification of Software Changes through Probabilistic Symbolic Execution (N)","A. Filieri; C. S. Pasareanu; G. Yang","NA; NA; NA","2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2015","","","703","708","Characterizing software changes is fundamental for software maintenance. However existing techniques are imprecise leading to unnecessary maintenance efforts. We introduce a novel approach that computes a precise numeric characterization of program changes, which quantifies the likelihood of reaching target program events (e.g., assert violations or successful termination) and how that evolves with each program update, together with the percentage of inputs impacted by the change. This precise characterization leads to a natural ranking of different program changes based on their probability of execution and their impact on target events. The approach is based on model counting over the constraints collected with a symbolic execution of the program, and exploits the similarity between program versions to reduce cost and improve the quality of analysis results. We implemented our approach in the Symbolic PathFinder tool and illustrate it on several Java case studies, including the evaluation of different program repairs, mutants used in testing, or incremental analysis after a change.","","978-1-5090-0025-8978-1-5090-0024","10.1109/ASE.2015.78","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372058","","Probabilistic logic;Maintenance engineering;Software;Java;IP networks;Probability;Computational modeling","Java;probability;program testing;software maintenance","software change quantification;probabilistic symbolic execution;software maintenance;program update;Symbolic PathFinder tool;Java;program repairs;program testing","","5","23","","","","","","IEEE","IEEE Conferences"
"Leveraging the Crowd: How 48,000 Users Helped Improve Lync Performance","R. Musson; J. Richards; D. Fisher; C. Bird; B. Bussone; S. Ganguly","Microsoft; Microsoft; Microsoft Research; Microsoft Research; Microsoft; Microsoft","IEEE Software","","2013","30","4","38","45","Performance is a critical component of customer satisfaction with network-based applications. Unfortunately, accurately evaluating the performance of collaborative software that operates in extremely heterogeneous environments is difficult with traditional techniques such as modeling workloads or testing in controlled environments. To evaluate performance of an application in the wild during development, the authors deployed early versions of the software, collecting performance data from application users for key usage scenarios. The analysis package they used produces visualizations to help development teams identify and prioritize performance issues by focusing on performance early in the development cycle, evaluating progress, identifying defects, and estimating timelines.","0740-7459;1937-4194","","10.1109/MS.2013.67","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509371","software performance;data collection;data analysis;performance monitoring;software analytics;software data visualization","Software development;Performance evaluation;Customer satisfaction;Software quality;Analytical models","groupware;software engineering","Lync performance;customer satisfaction;collaborative software;software development;data visualization","","13","2","","","","","","IEEE","IEEE Journals & Magazines"
"On Hardware Variability and the Relation to Software Variability","C. Brink; E. Kamsties; M. Peters; S. Sachweh","NA; NA; NA; NA","2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications","","2014","","","352","355","In mechatronic and embedded systems, variability stretches from customer-visible features to implementation features, which manifest in software, hardware, and mechanical parts. A good example are automotive systems, which are usually implemented as product lines. There are close connections between hardware and software during the development of such product lines. For example, software usually needs to be heavily tuned towards processors characteristics or optimized for a specific memory size. The problem is that different lifecycles of hardware and software make it difficult to maintain all variability in a single model. In this paper, the notion of hardware variability is discussed. We suggest that software and hardware variability should be kept in separate models. We argue that hardware variability and software variability models should only be loosely coupled. This allows an easier exchange of hardware platforms and variants as well as a test during the configuration whether hardware and software fit to each other. To address this, we propose an approach that distinguishes between software and hardware variants by using separate variability models. Therefore, we introduce a hardware variability model, which has a strong focus on the description of hardware properties. Furthermore, we introduce a concept for modeling the dependencies between hardware and software variants to combine them during the configuration.","1089-6503;2376-9505","978-1-4799-5795","10.1109/SEAA.2014.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928834","Product Lines;Hardware and Software Variability;Dependencies","Hardware;Software;Temperature measurement;Automotive engineering;Temperature sensors;Monitoring;Abstracts","embedded systems;mechatronics;software product lines","mechatronic systems;embedded systems;customer-visible features;implementation features;software product line development;memory size;software life-cycle;hardware life cycle;hardware variability model;software variability model","","1","14","","","","","","IEEE","IEEE Conferences"
"QP test: a dependence test for quadratic array subscripts","J. Zhao; R. Zhao; L. Han; J. Xu","State Key Laboratory of Mathematical Engineering and Advanced Computing, People's Republic of China; State Key Laboratory of Mathematical Engineering and Advanced Computing, People's Republic of China; State Key Laboratory of Mathematical Engineering and Advanced Computing, People's Republic of China; State Key Laboratory of Mathematical Engineering and Advanced Computing, People's Republic of China","IET Software","","2013","7","5","271","282","Traditional dependence tests detect dependences with linear array subscripts, but only give passive results to those with non-linear expressions. It may result in a multitude of pseudo-dependences. To maximise the parallelism of applications and improve an optimising compiler's ability of detecting dependences between program statements, it is necessary to develop a non-linear dependence test to eliminate these pseudo-dependences. This study presents a new non-linear dependence test by analysing the optimal solution of the quadratic subscripts with the index bounds constraints. The authors prove that the non-linear dependences caused by subscripts, which can be written in the form of quadratic programming model, are able to be detected, and introduce a non-linear dependence testing algorithm based on quadratic programming. The effectiveness of this algorithm is verified. The authors developed a prototype implementation of the test with the Open64 compiler and evaluated it using some real world applications from Perfect Club benchmarks and Spec2006 benchmark suites. The experimental results indicate that, compared to existing testing methods, the quadratic programming (QP) test is more efficient for quadratic cases.","1751-8806;1751-8814","","10.1049/iet-sen.2012.0142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650217","","","parallel processing;program compilers;quadratic programming","program compiler;parallel processing;perfect club benchmarks;Open64 compiler;prototype implementation;quadratic programming model;index bounds constraints;quadratic subscripts;nonlinear dependence;program statements;pseudo dependences;nonlinear expressions;linear array subscripts;quadratic array subscripts;dependence test;QP test","","","49","","","","","","IET","IET Journals & Magazines"
"Face image captcha generation using particle swarm optimization approach","P. Aruna; R. Kanchana","Department of CSE, SriGuru Institute of Technology; Department of CSE, SriGuru Institute of Technology","2015 IEEE International Conference on Engineering and Technology (ICETECH)","","2015","","","1","5","CAPTCHA is a software programming which is introduced to differentiate the human from the robots. CATCHA intends to generate a code which can only be identified by the human and machines cannot. In the real world, due to the massive increase in the usage of smart phones, tablets and other devices with the touch screen functionality poses a many online security threats. The traditional CAPTCHA requires a help of keyboard input and does dependant of language which will not be efficient in the smart phone devices. The face CAPTCHA is the one which intends to generate a CAPTCHA by using a combination of facial images and the fake images. It is based on generating a CAPTCHA with noised real face images and the fake images which cannot be identified by the machines but humans do. In the existing work, genetic algorithm is used to select the optimized face images by using which the better optimized fpso CAPTCHA can be created. However this work lacks from the local convergence problem where it can only select the best images within the local region. To overcome this problem in this work, the particle swarm optimization method is propose which can generate the globalize solution. Particle Swarm Optimization (PSO) is a popular and bionic algorithm based on the social behavior associated with bird flocking for optimization problems. The experimental tests that were conducted were proved that the proposed methodology improves in accuracy and generates an optimized solution than the existing methodologies.","","978-1-4799-1854-6978-1-4799-1853","10.1109/ICETECH.2015.7275016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7275016","Face Images;CAPTCHA;Particle Swarm Optimization;Distorted Image","CAPTCHAs;Face;Distortion;Authentication;Particle swarm optimization;Feature extraction","face recognition;genetic algorithms;particle swarm optimisation;security of data","social behavior;PSO;bionic algorithm;local convergence problem;genetic algorithm;fake images;particle swarm optimization approach;face image captcha generation","","","7","","","","","","IEEE","IEEE Conferences"
"A discrete particle swarm optimization box-covering algorithm for fractal dimension on complex networks","L. Kuang; F. Wang; Y. Li; H. Mao; M. Lin; F. Yu","State Key Laboratory of Software Engineering, Computer School, Wuhan University, Wuhan, P.R. China; State Key Laboratory of Software Engineering, Computer School, Wuhan University, Wuhan, P.R. China; State Key Laboratory of Software Engineering, Computer School, Wuhan University, Wuhan, P.R. China; State Key Laboratory of Software Engineering, Computer School, Wuhan University, Wuhan, P.R. China; Wuhan FiberHome International Technologies Co., Ltd., Wuhan, P.R. China; State Key Laboratory of Software Engineering, Computer School, Wuhan University, Wuhan, P.R. China","2015 IEEE Congress on Evolutionary Computation (CEC)","","2015","","","1396","1403","Researchers have widely investigated the fractal property of complex networks, in which the fractal dimension is normally evaluated by box-covering method. The crux of box-covering method is to find the solution with minimum number of boxes to tile the whole network. Here, we introduce a particle swarm optimization box-covering (PSOBC) algorithm based on discrete framework. Compared with our former algorithm, the new algorithm can map the search space from continuous to discrete one, and reduce the time complexity significantly. Moreover, because many real-world networks are weighted networks, we also extend our approach to weighted networks, which makes the algorithm more useful on practice. Experiment results on multiple benchmark networks compared with state-of-the-art algorithms show that this PSOBC algorithm is effective and promising on various network structures.","1089-778X;1941-0026","978-1-4799-7492-4978-1-4799-7491","10.1109/CEC.2015.7257051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257051","","Time complexity;Fractals;Complex networks;Greedy algorithms;Clustering algorithms;Benchmark testing;Optimization","complex networks;computational complexity;fractals;particle swarm optimisation;search problems","discrete particle swarm optimization box-covering algorithm;fractal dimension;complex networks;PSOBC algorithm;search space;time complexity;weighted networks;multiple benchmark networks;network structures","","","30","","","","","","IEEE","IEEE Conferences"
"Rate-distortion optimization for high dynamic range video coding","M. Azimi; M. T. Pourazad; P. Nasiopoulos","NA; NA; NA","2014 6th International Symposium on Communications, Control and Signal Processing (ISCCSP)","","2014","","","310","313","The genuine viewing quality of High Dynamic Range (HDR) data brought an evolution to content production and display manufacture markets. However, the efficiency of compression and transmission of HDR content still needs to be improved. In this paper we modify the H.264/AVC standard to better characterize HDR content. We propose a new Lagrangian multiplier that strikes a balance between the bit-rate and distortion of the HDR video. The updated Lagrange multiplier was implemented on the H.264/AVC reference software. Our experiment results show that the HDR-VDP2 quality scores of the videos encoded by the HDR-accustomed encoder are higher than the ones encoded with the reference encoder. Moreover, subjective tests confirmed that the visual quality of the compressed HDR videos using our proposed method is higher than the one encoded by the reference codec<sup>1</sup>.","","978-1-4799-2890","10.1109/ISCCSP.2014.6877876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877876","High Dynamic Range;HDR;Low Dynamic Range;video encoder;subjective evaluation;Lagrangian multiplier","Video coding;Bit rate;Dynamic range;Optimization;Standards;Software","optimisation;video coding","rate distortion optimization;high dynamic range video coding;content production;display manufacture markets;HDR content;H.264/AVC standard;Lagrangian multiplier;HDR video;H.264/AVC reference software;visual quality;reference encoder","","1","14","","","","","","IEEE","IEEE Conferences"
"Parallelization for space trajectory optimization","M. Schlueter; M. Munetomo","Institute of Space and Astronautical Science, Japan Aerospace Exploration Agency, Sagamihara, Kanagawa 252-5210, Japan; Information Initiative Center, Hokkaido University Sapporo, Sapporo 060-0811, Japan","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","832","839","The impact of parallelization on the optimization process of space mission trajectories is investigated in this contribution. As space mission trajectory reference model, the well known Cassini1 benchmark, published by the European Space Agency (ESA), is considered and solved here with the MIDACO optimization software. It can be shown that significant speed ups can be gained by applying parallelization.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900356","Space Trajectory;Cassini Mission;MIDACO;Par-allelization;openMP","Benchmark testing;Optimization;Trajectory;Space missions;Software;Databases;Educational institutions","aerospace computing;ant colony optimisation;parallel processing;trajectory optimisation (aerospace)","space trajectory optimization;space mission trajectory reference model;Cassini1 benchmark;European Space Agency;ESA;MIDACO optimization software;parallelization;ant colony optimization","","1","16","","","","","","IEEE","IEEE Conferences"
"Copper bonding on thin top metal bonding pad","R. Melanio; R. Altar; R. Cervantes","ON Semiconductor Philippines Incorporated Golden Mile Business Park - Special Economic Zone Governor's Drive, Carmona, Cavite, 4116 Philippines; ON Semiconductor Philippines Incorporated Golden Mile Business Park - Special Economic Zone Governor's Drive, Carmona, Cavite, 4116 Philippines; ON Semiconductor Philippines Incorporated Golden Mile Business Park - Special Economic Zone Governor's Drive, Carmona, Cavite, 4116 Philippines","36th International Electronics Manufacturing Technology Conference","","2014","","","1","4","Among the key drivers of development today is cost reduction. To achieve this, one of the measures undertaken is to convert the wire used for the wirebond process from gold to copper in several devices. Conversion from Au to Cu wire is particularly challenging for thin top metal devices because the bond pad structure is more prone to damage. This paper discusses the characterization and optimization studies undertaken in successful copper wire conversion of thin top metal devices. The successful conversion from Au wire to Cu wire resulted in 70% cost reduction. A device with circuit under pad (CUP) die technology has been selected as the representative for thin top metal copper wire conversion. SOIC 14L was the package of choice for this qualification. The representative die used has a top metal thickness of ~0.675μm and a pad metal composition of Al with 0.5% Cu. This qualification enables introduction of new products that combine CUP die technology and copper wire combination as well as copper wire bonding capability for thin top metal. At present, production is currently running on copper wire using thick bond pads of >3μm. This parameter was used as baseline for free-air ball and 2<sup>nd</sup>bond. A different bond parameter optimization approach was used for this project as the wire being used is harder and the bond pad structure is prone to pad damage. Only the first bond parameter was meticulously optimized, others were just fine tuned. Based on the statistical analysis software, the most significant parameter change is the bond force. Responses to the wire conversion have been closely monitored. Among the responses monitored are free-air ball (FAB) characteristics, non-stick on pad (NSOP) elimination, ball shear test, wire pull test, crater test and pad metal displacement (PMD). Cross-sectioning and delayering of the bond pads have also been performed to verify the result of the pad metal displacement measurement equipment. All the wirebond responses that have been monitored indicate that the optimization of the bond force proved to be successful since all the responses required passed the criteria set at assembly site. Aside from wire bond response, reliability test responses have also been monitored. No failures were observed after TC 500 and after other reliability tests. By meeting the required wirebond and reliability responses, this study shows that copper wire bonding for thin top metal devices is achievable. The savings expected in successfully converting Au wire to Cu wire for this package has been obtained without any sacrifice to quality.","1089-8190","978-1-4799-8209","10.1109/IEMT.2014.7123111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123111","","Wires;Copper;Gold;Monitoring;Reliability;Temperature measurement","cable testing;copper;cost reduction;displacement measurement;gold;lead bonding;optimisation;semiconductor device packaging;semiconductor device reliability;shearing;statistical analysis","wirebond responses;reliability test responses;metal displacement measurement equipment;pad delayering;cross-sectioning;PMD;pad metal displacement;crater test;wire pull test;ball shear test;NSOP elimination;nonstick on pad elimination;FAB characteristics;statistical analysis software;bond parameter optimization approach;free-air ball;CUP die technology;pad metal composition;top metal thickness;SOIC;circuit under pad die technology;wire conversion;optimization;wirebond process;cost reduction;thin top metal bonding pad devices;Cu;Au","","","2","","","","","","IEEE","IEEE Conferences"
"A literature review of Bee Colony optimization algorithms","R. Gulati; P. Vats","HMR Institute of Technology & Management, New Delhi, India; HMR Institute of Technology & Management, New Delhi, India","2014 Innovative Applications of Computational Intelligence on Power, Energy and Controls with their impact on Humanity (CIPECH)","","2014","","","499","504","Bee Colony optimization techniques are inspired by the high level of mutual intelligence shown by the natural bees in the food foraging process. It is a population based natural search algorithm which provides the base to solve metaheuristic computational optimization problems. In this paper we have carried out a literature review of the applications of BCO into various areas of computational problems where they prove their worth in providing optimized solutions. We have further carried out a tabular comparison of the work performed by the various researchers by applying the the BCO as optimization algorithms for solving the Optimization Problems.","","978-1-4799-5871-9978-1-4799-5870","10.1109/CIPECH.2014.7019121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019121","Bee Colony Optimization;Scheduling;Optimization;Foraging behavior","Optimization;Algorithm design and analysis;Testing;Search problems;Software algorithms;MATLAB","optimisation;search problems","metaheuristic computational optimization problems;population based natural search algorithm;food foraging process;natural bees;mutual intelligence;bee colony optimization algorithms","","","31","","","","","","IEEE","IEEE Conferences"
"DITEC (DoD-Centric and Independent Technology Evaluation Capability): A Process for Testing Security","J. Romero-Mariona","NA","2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops","","2014","","","24","25","Information Assurance (IA) is one of the Department of Defense's (DoD) top priorities today. IA technologies are constantly evolving to protect critical information from the growing number of cyber threats. Furthermore, DoD spends millions of dollars each year procuring, maintaining, and discontinuing various IA and Cyber technologies. Today, there is no process and/or standardized method for making informed decisions about which IA technologies are better/best. Due to this, efforts for selecting technologies go through very disparate evaluations that are often times non-repeatable and very subjective. DITEC (DoD-centric and Independent Technology Evaluation Capability) is a new capability that streamlines IA technology evaluation. DITEC defines a Process for evaluating whether or not a product meets DoD needs, Security Metrics for measuring how well needs are met, and a Framework for comparing various products that address the same IA technology area. DITEC seeks to reduce the time and cost of creating a test plan and expedite the test and evaluation effort for considering new IA technologies, consequently streamlining the deployment of IA products across DoD and increasing the potential to meet its needs.","","978-1-4799-5790","10.1109/ICSTW.2014.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825634","Security;Information Assurance;Security Metrics;Evaluation;Decision-making Support","Measurement;US Department of Defense;Computer security;Conferences;Usability","data protection;decision making;military computing;security of data","DITEC;DoD-centric and independent technology evaluation capability;security testing process;information assurance;Department of Defense;IA technologies;critical information protection;cyber threats;cyber technologies;IA technologies;informed decision making;security metrics","","2","3","","","","","","IEEE","IEEE Conferences"
"Optimization of association rule mining","P. A. Fatah; I. Hamarash","Dept. of Software Engineering, Salahaddin University-Erbil, Kurdistan, Iraq; Dept. of Electrical Engineering, Salahaddin University-Erbil, Kurdistan, Iraq","2015 Internet Technologies and Applications (ITA)","","2015","","","275","280","This paper introduces an optimization approach for association rule mining in the time-memory domain. The approach splits the running mode of the traditional data mining algorithm into two phases. The first phase is designed to calculate all item sets in every transaction together with their frequencies (without pruning) and indexes their accumulation in a database. This procedure needs the fetch cycle of each transaction only once which reduces fetching transactions' I/O reasonably. In the second phase, the item sets and their frequencies are used in rule producing. a new algorithm has been designed, implemented, coded, verified and tested on real data. The approach enables users to change their queries and criteria using the second phase only which reduces the cost effectively.","","978-1-4673-9557-1978-1-4799-8036-9978-1-4799-8035","10.1109/ITechA.2015.7317409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7317409","data mining;optimization;apriori variation;association rule","Algorithm design and analysis;Buffer storage;Time-frequency analysis","data mining;optimisation;query processing","association rule mining optimization;two step breakdown variation;apriori algorithm;optimization approach;time-memory domain;fetching transactions IO;queries","","","5","","","","","","IEEE","IEEE Conferences"
"A novel PSO for portfolio optimization based on heterogeneous multiple population strategy","X. Yin; Q. Ni; Y. Zhai","College of Software Engineering, Southeast University, Nanjing, China, Key Lab of Computer Network &amp; Information Integration, MOE, P.R. China; College of Software Engineering, Southeast University, Nanjing, China, Key Lab of Computer Network &amp; Information Integration, MOE, P.R. China; College of Software Engineering, Southeast University, Nanjing, China, Key Lab of Computer Network &amp; Information Integration, MOE, P.R. China","2015 IEEE Congress on Evolutionary Computation (CEC)","","2015","","","1196","1203","The problem of portfolio selection in the field of financial engineering has received more attention in recent years. This paper presents a novel heterogeneous multiple population particle swarm optimization algorithm (HMPPSO) for solving a generalized Markowitz mean-variance portfolio selection model. The proposed HMPPSO is based on heterogeneous multiple population strategy, in which the whole population is divided into several sub-populations and all the sub-populations evolve with different PSO variants. The communication between the sub-populations is executed at regular intervals to maintain the information exchange inside the entire population and coordinate exploration and exploitation according to certain migration rules. The generalized portfolio selection model is classified as a quadratic mixed-integer programming model for which no computational efficient algorithms have been proposed. We employ the proposed HMPPSO to find the solution for the model and compare the performance of HMPPSO with several classic PSO variants. The test data set is the weekly prices from March, 1992 to September, 1997 including the following indices: Hang Seng in Hong Kong, DAX 100 in Germany, FTSE 100 in UK, S&amp;P 100 in USA and Nikkei 225 in Japan. The computational results demonstrate that HMPPSO is much effective and robust, especially for problems with high dimensions, thus provides an effective solution for the portfolio optimization problem.","1089-778X;1941-0026","978-1-4799-7492-4978-1-4799-7491","10.1109/CEC.2015.7257025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257025","","Portfolios;Sociology;Statistics;Optimization;Computational modeling;Standards;Particle swarm optimization","investment;particle swarm optimisation","financial engineering;HMPPSO;heterogeneous multiple population particle swarm optimization algorithm;generalized Markowitz mean-variance portfolio selection model;information exchange;generalized portfolio selection model;Hong Kong;Hang Seng;Germany;portfolio optimization problem","","","19","","","","","","IEEE","IEEE Conferences"
"Plenary keynote address Tuesday","A. de Geus","Synopsys Corp., USA","2014 International Test Conference","","2014","","","8","9","From a dizzying array of emerging ""smart"" niche end-products, to major market trend shifts, to ecosystem reconfigurations at every level, the semiconductor industry is morphing at an unprecedented pace. The move from planar to FinFET transistors on top of the shift from scale complexity to systemic complexity-all coupled with challenges in power, performance, reliability and more-demand a level of systemic collaboration from silicon to software as never seen before. The good news is that we're up for it! In his presentation, Aart will give an overview of how the breadth and depth of competence across the ecosystem is enabling modern design, including test, diagnosis, and yield optimization solutions to keep up with that most amazing of races: Moore's Law.","1089-3539;2378-2250","978-1-4799-4722","10.1109/TEST.2014.7035273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7035273","","","modems;MOSFET;semiconductor industry","plenary keynote address tuesday;dizzying array;emerging smart niche end-products;market trend shifts;ecosystem reconfigurations;semiconductor industry;FinFET transistors;scale complexity;systemic complexity collaboration;modern design;test solutions;diagnosis solutions;yield optimization solutions;Moore's law","","","","","","","","","IEEE","IEEE Conferences"
"Network utilization optimizer for SD-WAN","L. Vdovin; P. Likin; A. Vilchinskii","MERA, Nizhny Novgorod, Russia; MERA, Nizhny Novgorod, Russia; MERA, Nizhny Novgorod, Russia","2014 International Science and Technology Conference (Modern Networking Technologies) (MoNeTeC)","","2014","","","1","4","the question how to use the maximum of network possibilities is still open. ISPs and large distributed companies use only 40-50% of total network bandwidth. Technology that helps to increase network bandwidth utilization and redundancy is crucial and there is still no generic and simple solution for this problem. The SDN architecture advocates the separation of data and control plane, and helps to simplify the network management and maintenance due to logically centralized software. Basing on this approach, our team has implemented a simple solution solving network utilization issue. Remote SDN controller runs on high performance server and this enables to apply relatively complex per-flow global routing algorithms. An application tracks network state. In case of link fault, the flows affected by outage are re-routed over alternative path. A whole network acts as the single distributed L2 switch from external connections perspective, but solution architecture allows to change a whole network representation from L2 switch to distributed L3 router. Application was developed by using OpenFlow technologies at data plane devices. The application uses modified Dijkstra algorithm. The algorithm searches for the route with the best spare capacity based on actual network utilization. Also the algorithm allows to control route length over per-hop penalty. So the developed application allows to apply per-flow policing in terms of bandwidth and latency. Nowadays OpenFlow controllers don't have a standardized API and it makes it impossible to change a controller for your application. To avoid this issue an OpenFlow independent Controller-Application specific interface has been developed. Interface uses application specific proprietary message format optimized to increase configuration performance. So our application is flexible in choosing OpenFlow controller. Characteristics for our prototype have been defined based on performance characteristics of Yarnet ISP located in Yaroslavl and it should work with 30 nodes (each node has at least 3 connections per switch) and establish 5000 flows per second and has traffic outage less than 1 second. The characteristics were measured using simulated and target test environment. Developed application will be used as the framework to implement traffic policing features, QoS, bandwidth and latency reservation.","","978-1-4799-7595-2978-1-4799-7593","10.1109/MoNeTeC.2014.6995604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6995604","Software-defined networking;Software-defined-WAN;OpenFlow;Floodlight","Switches;Bandwidth;Network topology;Prototypes;Peer-to-peer computing;Routing","quality of service;software defined networking;telecommunication network routing;telecommunication traffic;wide area networks","network utilization optimizer;SD-WAN;network bandwidth utilization;network management;network maintenance;logically centralized software;remote SDN controller;high performance server;per-flow global routing algorithms;distributed L2 switch;distributed L3 router;modified Dijkstra algorithm;OpenFlow independent controller-application specific interface;application specific proprietary message format;Yarnet ISP;Yaroslavl;traffic policing features;QoS;latency reservation","","4","11","","","","","","IEEE","IEEE Conferences"
"A qualitative analysis on the specification mining techniques","A. R. Priya; M. Mythily","Department of Computer Science and Engineering, Karunya University, Coimbatore, India; Department of Computer Science and Engineering, Karunya University, Coimbatore, India","2013 IEEE International Conference ON Emerging Trends in Computing, Communication and Nanotechnology (ICECCN)","","2013","","","199","202","Formal Specifications plays a vital role in several dimensions of software engineering such as testing, optimizing, refactoring, documenting, debugging and repair. Formal Specifications are generally descriptions for legal program behavior. In Conventional approaches, specifications are constructed manually and later automatic mining techniques emerges with mining rules focusing on temporal logic and language specifications. Addition of metrics in mining leads to improvement in performance and thereby providing a step towards the support for real world applications. This paper focuses on giving a detailed overview of specification mining techniques, thereby stating the need for specifications utility in industrial perspective. This paper also serves as a guide to gain additional knowledge and the impact of metrics over specification miners.","","978-1-4673-5036-5978-1-4673-5037-2978-1-4673-5035","10.1109/ICE-CCN.2013.6528492","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6528492","Metrics;Mining;Specification;Temporal property","Measurement;Software;Data mining;Java;Educational institutions;Formal specifications;Learning automata","data mining;formal specification;law;program debugging;program testing;software maintenance;software metrics;temporal logic","specification mining techniques;formal specifications;software engineering;legal program behavior;automatic mining techniques;temporal logic;language specifications;metrics impact;testing;optimisation;refactoring;debugging","","","15","","","","","","IEEE","IEEE Conferences"
"Extending design space optimization heuristics for use with stochastic colored Petri nets","C. Bodenstein; A. Zimmermann","Systems & Software Engineering, Ilmenau University of Technology, P.O. Box 100 565, 98684, Germany; Systems & Software Engineering, Ilmenau University of Technology, P.O. Box 100 565, 98684, Germany","2015 Annual IEEE Systems Conference (SysCon) Proceedings","","2015","","","139","144","Automatic design optimization of complex systems is a time-consuming task. This paper presents multiphase variants of well-known heuristics for an efficient indirect system optimization with simulation. The tradeoff between accuracy and achievable speedup is analyzed for examples including a benchmark function as well as simulated values for a stochastic colored Petri net application. The analysis are carried out with TOE (TimeNET optimization environment), a software prototype implemented for this task recently.","","978-1-4799-5927","10.1109/SYSCON.2015.7116742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116742","","Benchmark testing;Cost function;Accuracy;Simulated annealing;Annealing;Numerical models","heuristic programming;optimisation;Petri nets","design space optimization heuristics;automatic design optimization;complex systems;time-consuming task;multiphase variants;indirect system optimization;benchmark function;stochastic colored Petri net application;TOE;TimeNET optimization environment;software prototype","","1","22","","","","","","IEEE","IEEE Conferences"
"Exploiting narrow-width values for improving non-volatile cache lifetime","G. Duan; S. Wang","State Key Laboratory of Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiang Su, China; State Key Laboratory of Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiang Su, China","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","4","Due to the high cell density, low leakage power consumption, and less vulnerability to soft errors, non-volatile memory technologies are among the most promising alternatives for replacing the traditional DRAM and SRAM technologies used in implementing main memory and caches in the modern microprocessor. However, one of the difficulties is the limited write endurance of most non-volatile memory technologies. In this paper, we propose to exploit the narrow-width values to improve the lifetime of non-volatile last level caches. Leading zeros masking scheme is first proposed to reduce the write stress to the upper half of the narrow-width data. To balance the write variations between the upper half and the lower half of the narrow-width data, two swap schemes, the swap on write (SW) and swap on replacement (SRepl), are proposed. To further reduce the write stress to non-volatile caches, we adopt two optimization schemes, the multiple dirty bit (MDB) and read before write (RBW), to improve their lifetime. Our experimental results show that by combining all our proposed schemes, the lifetime of non-volatile caches can be improved by 245% on average.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800266","","Nonvolatile memory;Random access memory;System-on-chip;Stress;Optimization;Computer architecture;Microprocessors","cache storage;circuit optimisation;DRAM chips;low-power electronics;SRAM chips","low leakage power consumption;nonvolatile cache lifetime;narrow-width values;leading zeros masking scheme;optimization schemes;multiple dirty bit;read before write;DRAM;SRAM","","","16","","","","","","IEEE","IEEE Conferences"
"Analyzing relationship between the number of errors in review and debug processes for embedded software development projects","T. Nakashima; K. Iwata; Y. Anan; N. Ishii","Dept. of Culture-Information Studies, Sugiyama Jogakuen University, 17-3, Moto-machi, Hoshigaoka, Chikusa-ku, Nagoya, Aichi, 464-8662, Japan; Dept. of Business Administration, Aichi University, 4-60-6, Hiraike-cho, Nakamura-ku, Nagoya, Aichi, 453-8777, Japan; Process Innovation H.Q, Omron Software Co., Ltd., Shiokoji-Horikawa, Shimogyo-ku Kyoto, 600-8234, Japan; Dept. of Information Science, Aichi Institute of Technology, 1247 Yachigusa, Yakusa-cho, Toyota, Aichi, 470-0392, Japan","2013 IEEE/ACIS 12th International Conference on Computer and Information Science (ICIS)","","2013","","","405","409","In this study, we examine the effect of reviewing processes for embedded software development projects. We analyze the relationship between review processes and debugging process using the Shapiro-Wilk test and Spearman's rank correlation coefficient. The results indicate that the data selected was not from a normally distributed population and that review processes can reduce the number of development errors.","","978-1-4799-0174","10.1109/ICIS.2013.6607874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607874","","Debugging;Distributed databases;Correlation;Sociology;Embedded software","embedded systems;program debugging;project management;software development management;statistical testing","review process;debug process;embedded software development projects;Shapiro-Wilk test;Spearman rank correlation coefficient","","","11","","","","","","IEEE","IEEE Conferences"
"An open framework to deploy heterogeneous wireless testbeds for Cyber-Physical Systems","M. Szczodrak; Y. Yang; D. Cavalcanti; L. P. Carloni","Columbia University, USA; Philips Research North America, USA; Philips Research North America, USA; Columbia University, USA","2013 8th IEEE International Symposium on Industrial Embedded Systems (SIES)","","2013","","","215","224","We present an open framework for the efficient deployment of heterogeneous wireless testbeds for Cyber-Physical Systems (CPS). The testbed architecture, which can be configured and optimized for each particular deployment, consists of a low-power wireless network (LPWN) of embedded devices, a backbone network, and a server back-end. Our framework, whose source code is publicly available, includes a comprehensive set of software tools for deploying, testing, reconfiguring, and evaluating the CPS application software and the supporting firmware. We discuss the architecture, the framework properties, and the hardware resources that are necessary to deploy an experimental testbed. We present two case studies built with our framework: an outdoor lighting installation in a commercial parking lot and an indoor university building instrumentation. Using the two deployments, we present experiments normally conducted by CPS engineers to better understand the environment in which the CPS is deployed. The results of these experiments show the feasibility of the proposed framework in assisting CPS research and development.","2150-3109;2150-3117","978-1-4799-0658","10.1109/SIES.2013.6601494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601494","","Universal Serial Bus;Servers;Sensors;Routing protocols","automatic test software;building management systems;computer networks;cybernetics;embedded systems;firmware;lighting;program testing;radio networks;software performance evaluation;software tools;source coding;telecommunication power management","open framework;cyber-physical systems;heterogeneous wireless testbed deployment;CPS application software;testbed architecture;low-power wireless network;LPWN;embedded devices;backbone network;server back-end;source code;software tools;CPS application software deployment;CPS application software testing;CPS application software reconfiguration;CPS application software evaluation;firmware;hardware resources;outdoor lighting installation;commercial parking lot;indoor university building instrumentation","","5","30","","","","","","IEEE","IEEE Conferences"
"Analyzing Relationship between the Amount of Effort in Review Processes and the Total Amount of Effort for Embedded Software Development Projects","K. Iwata; T. Nakashima; Y. Anan; N. Ishii","NA; NA; NA; NA","2015 3rd International Conference on Applied Computing and Information Technology/2nd International Conference on Computational Science and Intelligence","","2015","","","310","315","In this study, we examined the relationships between the amount of effort identified in review processes and the total amount of effort in end of projects by using data accumulated by a company. We grouped the data by the amount of effort in a review process, and a volume of target projects to find out the relationships. We analyzed the relationship using the Shapiro-Wilk test and Spearman's rank correlation coefficient. The results mean that a volume of the target project can create groups more efficiently to find out a relationship between the amount of effort in review processes and the total amount of effort. They indicate that the total amount of effort can probably be estimated by using the amount of effort in review processes which is grouped by a volume of target projects.","","978-1-4673-9642-4978-1-4673-9641","10.1109/ACIT-CSI.2015.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336080","","Software;Debugging;Distributed databases;Sociology;Statistics;Companies;Testing","embedded systems;project management;software engineering;software houses;software management","embedded software development projects;review processes;total amount of effort;Shapiro-Wilk test;Spearman's rank correlation coefficient;target project volume","","","11","","","","","","IEEE","IEEE Conferences"
"Energy-efficiency indicators for e-services","J. Arnoldus; J. Gresnigt; K. Grosskop; J. Visser","Software Improvement Group, Amsterdam, The Netherlands; Logius, Ministry of the Interior and Kingdom Relations, Den Haag, The Netherlands; Software Improvement Group, Amsterdam, The Netherlands; Software Improvement Group, Amsterdam, The Netherlands","2013 2nd International Workshop on Green and Sustainable Software (GREENS)","","2013","","","24","29","Great strides have been made to increase the energy efficiency of hardware, data center facilities, and network infrastructure. These Green IT initiatives aim to reduce energy-loss in the supply chain from energy grid to computing devices. However, the demand for computation comes from software applications that perform business services. Therefore, to measure and improve efficiency for entire systems, energy-efficiency indicators are needed at the level of services. We have designed an initial set of indicators for energy-efficiency of e-services and we have tested them on two e-government services of the Dutch national government. We explain how these indicators serve as a starting point for energy-optimization initiatives, supported by appropriate contractual agreements between service owners and suppliers.","","978-1-4673-6267","10.1109/GREENS.2013.6606418","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606418","","Energy consumption;Hardware;Optimization;Measurement;Software;Servers;Government","government data processing;green computing;optimisation;power aware computing","suppliers;service owners;contractual agreements;energy-optimization initiatives;e-government services;Dutch national government;energy-efficiency indicators;business services;software applications;computing devices;energy grid;supply chain;green IT initiatives;network infrastructure;data center facilities","","2","14","","","","","","IEEE","IEEE Conferences"
"Improving quality in an electrical safety testing laboratory by using a simulation-based tool","P. Marelli; M. Cóccola; R. Portillo; A. R. Tymoschuk","Universidad Tecnológica Nacional - Facultad Regional Santa Fe, Lavaise 610, 3000, ARGENTINA; Universidad Tecnológica Nacional - Facultad Regional Santa Fe, Lavaise 610, 3000, ARGENTINA; Universidad Tecnológica Nacional - Facultad Regional Santa Fe, Lavaise 610, 3000, ARGENTINA; Universidad Tecnológica Nacional - Facultad Regional Santa Fe, Lavaise 610, 3000, ARGENTINA","2015 Winter Simulation Conference (WSC)","","2015","","","3322","3332","This paper presents a simulation model developed with SIMIO software for representing the activities performed in an electrical measurement and test laboratory. The main goal is focused on the optimization and monitoring of the performance standards applied in the laboratory to certificate products for selling. The computer model allows identifying the principal weakness and bottlenecks of the process. Moreover, the performance measurements generated by simulation experiments are used for making decisions to enhance the current operation procedures and quality of service.","1558-4305","978-1-4673-9743-8978-1-4673-9741-4978-1-4673-9742","10.1109/WSC.2015.7408494","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7408494","","Humidity;Manuals","automatic test software;decision making;electric variables measurement;electrical safety;quality of service;standards","electrical safety testing laboratory;simulation-based tool;SIMIO software;electrical measurement;performance standard monitoring;certificate products;performance measurement;decision making;quality of service","","1","9","","","","","","IEEE","IEEE Conferences"
"Comparison of Model-Based Error Localization algorithms for C designs","U. Repinski; J. Raik","Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia; Department of Computer Engineering, Tallinn University of Technology, Raja 15, 12618, Estonia","East-West Design & Test Symposium (EWDTS 2013)","","2013","","","1","4","The paper addresses Model-Based Design Error Localization in C designs. We consider a localization algorithm that is implemented with Dynamic Slicing and simulation using C code animation. The localization algorithm has been integrated into the FoREnSiC automated debugging system. Different ranking algorithms are compared and their ranking accuracy for Error Localization is measured by experimental results on the Siemens benchmark set. A new contribution of the paper is the observation that a simple error ranking metric that takes into account only information from failed sequences has the least average deviation from exact localization.","","978-1-4799-2096-9978-1-4799-2095","10.1109/EWDTS.2013.6673203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6673203","","Heuristic algorithms;Forensics;Algorithm design and analysis;Accuracy;Radiation detectors;Data models;Measurement","C language;computer animation;program debugging;program slicing;program verification;program visualisation;software maintenance;software metrics","model-based design error localization;C designs;model-based design error localization algorithm;dynamic slicing;dynamic simulation;C code animation;FoREnSiC automated debugging system;ranking algorithms;ranking accuracy;Siemens benchmark set;error ranking metric;least average deviation;formal repair environment for simple C","","","12","","","","","","IEEE","IEEE Conferences"
"Graphical User Interface Testing Optimization for Water Monitoring Applications","G. Latiu; O. Cret; L. Vacariu","NA; NA; NA","2013 19th International Conference on Control Systems and Computer Science","","2013","","","640","645","In geospatial applications water monitoring has become a major topic of interest (in the last decades), since the raise of global concerns about climate changes and pollution in general. The number of parameters that need monitoring in a watershed is significant and the variety of sensors is also very important. In modern sensor networks, sensors are not just simple data collectors, but also actuators that need complex programming. Developing functionally correct and complete graphical user interfaces (GUI) for such complex systems is a real challenge that needs automatic support. The GUIs must be intuitive and user friendly, being the image of the application. A good quality of the GUI is necessary and the diminishing of testing cost becomes an important requirement. This paper presents an original automatic testing method for large GUIs testing using genetic algorithms. The method was implemented into an application called Water-EvoGuiTest and the experimental results obtained into the Cyberwater platform for monitoring pollution on rivers confirm the quality of this approach.","2379-0474;2379-0482","978-1-4673-6140-8978-0-7695-4980","10.1109/CSCS.2013.32","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569332","graphical user interface;water monitoring;automatic testing;evolutionary testing;genetic algorithms","Graphical user interfaces;Testing;Monitoring;Software;Sensors;Genetic algorithms;Rivers","environmental science computing;genetic algorithms;graphical user interfaces;water pollution control;water resources","graphical user interface;testing optimization;water monitoring application;watershed monitoring;GUI;genetic algorithm;Water-EvoGuiTest application;cyberwater platform;pollution monitoring","","","15","","","","","","IEEE","IEEE Conferences"
"Optimization of best polarity searching for mixed polarity reed-muller logic circuit","L. Xiao; Z. He; L. Ruan; R. Zhang; T. Xia; X. Wang","State Key Laboratory of Software Development Environment, Beihang University; State Key Laboratory of Software Development Environment, Beihang University; State Key Laboratory of Software Development Environment, Beihang University; School of Electronic and Information Engineering, Beihang University Beijing 100191, China; School of Electronic and Information Engineering, Beihang University Beijing 100191, China; School of Electronic and Information Engineering, Beihang University Beijing 100191, China","2015 28th IEEE International System-on-Chip Conference (SOCC)","","2015","","","275","280","At present, although genetic algorithm (GA) is widely used in best polarity searching of MPRM logic circuit, there are few literatures pay attention to the polarity conversion sequence of the polarity set waiting for evaluation. An improved best polarity searching approach (IBPSA) based on GA is presented to optimize the polarity conversion sequence of polarity set and speed up the best polarity searching of MPRM logic circuits. In addition, we present an improved nearest neighbor (INN) to obtain the best polarity conversion sequence of the polarity set waiting for evaluation in each generation of GA and apply elitism strategy to IBPSA to guarantee its global convergence. Our proposed IBPSA is implemented in C and a comparative analysis has been presented for MCNC benchmark circuits. The experimental results show that the IBPSA can greatly reduce the time of best polarity searching of MPRM logic circuits compared to the approaches neglecting polarity conversion sequence.","2164-1706","978-1-4673-9094-1978-1-4673-9093","10.1109/SOCC.2015.7406962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406962","Mixed Polarity Reed-Muller;Nearest Neighbor;Genetic Algorithm;Polarity Optimization;Polarity Conversion Sequence","Genetic algorithms;Optimization;Logic circuits;Biological cells;Logic functions;Reflective binary codes;Encoding","benchmark testing;genetic algorithms;logic circuits;logic testing","best polarity searching;mixed polarity Reed-Muller logic circuit;genetic algorithm;polarity conversion sequence;improved nearest neighbor;comparative analysis;MCNC benchmark circuits;IBPSA","","3","10","","","","","","IEEE","IEEE Conferences"
"Optimization of green agri-food supply chain network using chaotic PSO algorithm","Q. Tao; Z. Huang; C. Gu; C. Zhang","Department of Computer Science, Guangdong University of Education, Guangzhou, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Department of Computer Science, Zhongkai University of Agriculture and Engineering, Guangzhou, China; School of Software, Sun Yat-sen University, Guangzhou, China","Proceedings of 2013 IEEE International Conference on Service Operations and Logistics, and Informatics","","2013","","","462","467","In this paper, a chaotic Particle Swarm Optimization (CPSO) algorithm is presented to solve the green agri-food supply chain network (GASCN). The GASCN design is critical to reduce the total transportation cost for efficient and effective supply chain management. The traditional supply chain does not adequately satisfy the expectance of all the customers, therefore new model of supply chain of great urgency to be exploited. The main contribution of this paper is to find an optimal solution for GASCN problem and propose a new solution based on CPSO to optimize the GASCN. To show the efficacy of the CPSO algorithm, the algorithm is tested on three cases. Results show better performance of the CPSO in GASCN by both optimization speed and solution quality as compared to GA and CGA, especially when the scale of problem is large.","","978-1-4799-0530-0978-1-4799-0529","10.1109/SOLI.2013.6611459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6611459","supply chain network;chaotic Particle Swarm Optimization;efficacy;optimization","Optimization;Supply chains;Transportation;Algorithm design and analysis;Genetic algorithms;Educational institutions;Particle swarm optimization","agriculture;cost reduction;design engineering;environmental factors;food technology;particle swarm optimisation;supply chain management","green agrifood supply chain management network;chaotic PSO algorithm;particle swarm optimization;GASCN;transportation cost reduction","","","16","","","","","","IEEE","IEEE Conferences"
"Optimizing software design migration from structured programming to object oriented paradigm","S. Siddik; A. U. Gias; S. M. Khaled","Institute of Information Technology, University of Dhaka, Bangladesh; Institute of Information Technology, University of Dhaka, Bangladesh; Institute of Information Technology, University of Dhaka, Bangladesh","16th Int'l Conf. Computer and Information Technology","","2014","","","1","6","Several industries are using legacy softwares, developed with Structured Programming (SP) approach, that should be migrated to Object Oriented Paradigm (OOP) for ensuring better software quality parameters like modularity, manageability and extendability. Automating SP to OOP migration is pivotal as it could reduce time that take in the manual process. Given this potential benefit, the issue is yet to be addressed by researchers. This paper addresses the scenario by modeling this problem as a graph clustering problem where SP functions and function calls are vertices and edges respectively. The challenge evolving the problem is to find optimized clusters from graphs. To aid this problem, certain heuristic algorithms based on Monte Carlo and Greedy approaches are being developed. The proposed algorithms have been tested against a collection of real and synthetic data. The numerical results show that greedy algorithms are faster and produced better results than the average performance of Monte Carlo based approaches.","","978-1-4799-3497-3978-1-4799-3496","10.1109/ICCITechn.2014.6997320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6997320","Legacy Code;Software Design;Call Graph;DSM;Graph Clustering","Clustering algorithms;Equations;Software;Information technology;Greedy algorithms;Mathematical model;Monte Carlo methods","graph theory;greedy algorithms;Monte Carlo methods;object-oriented programming;pattern clustering;software maintenance;structured programming","software design migration;structured programming;object oriented paradigm;legacy softwares;software quality parameter;OOP migration;graph clustering problem;SP function;function calls;heuristic algorithm;Monte Carlo approach;greedy approach;greedy algorithm","","1","18","","","","","","IEEE","IEEE Conferences"
"Scaling Size and Parameter Spaces in Variability-Aware Software Performance Models (T)","M. Kowal; M. Tschaikowski; M. Tribastone; I. Schaefer","NA; NA; NA; NA","2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2015","","","407","417","In software performance engineering, what-if scenarios, architecture optimization, capacity planning, run-time adaptation, and uncertainty management of realistic models typically require the evaluation of many instances. Effective analysis is however hindered by two orthogonal sources of complexity. The first is the infamous problem of state space explosion -- the analysis of a single model becomes intractable with its size. The second is due to massive parameter spaces to be explored, but such that computations cannot be reused across model instances. In this paper, we efficiently analyze many queuing models with the distinctive feature of more accurately capturing variability and uncertainty of execution rates by incorporating general (i.e., non-exponential) distributions. Applying product-line engineering methods, we consider a family of models generated by a core that evolves into concrete instances by applying simple delta operations affecting both the topology and the model's parameters. State explosion is tackled by turning to a scalable approximation based on ordinary differential equations. The entire model space is analyzed in a family-based fashion, i.e., at once using an efficient symbolic solution of a super-model that subsumes every concrete instance. Extensive numerical tests show that this is orders of magnitude faster than a naive instance-by-instance analysis.","","978-1-5090-0025-8978-1-5090-0024","10.1109/ASE.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372029","","Unified modeling language;Mathematical model;Analytical models;Computational modeling;Software performance;Numerical models;Uncertainty","differential equations;queueing theory;software performance evaluation;software product lines","variability-aware software performance models;software performance engineering;what-if scenarios;architecture optimization;capacity planning;run-time adaptation;uncertainty management;state space explosion;queuing models;product-line engineering methods;ordinary differential equations","","9","41","","","","","","IEEE","IEEE Conferences"
"A Study on the Efficiency Aspect of Data Race Detection: A Compiler Optimization Level Perspective","C. Jia; W. K. Chan","NA; NA","2013 13th International Conference on Quality Software","","2013","","","35","44","Dynamically detecting data races in multithreaded programs incurs significant slowdown and memory overheads. Many existing techniques have been put forward to improve the performance slowdown through different dimensions such as sampling, detection precision, and data structures to track the happened-before relations among events in execution traces. Compiling the program source code with different compiler optimization options, such as reducing the object code size as the selected optimization objective, may produce different versions of the object code. Does optimizing the object code with a standard optimization option help improve the performance of the precise online race detection? To study this question and a family of related questions, this paper reports a pilot study based on four benchmarks from the PARSEC 3.0 suite compiled with six GCC compiler optimization options. We observe from the empirical data that in terms of performance slowdown, the standard optimization options behave comparably to the optimization options for speed and code size, but behave quite different from the baseline option. Moreover, in terms of memory cost, the standard optimization options incur similar memory costs as the baseline option and the option for speed, and consume less memory than the option for code size.","1550-6002;2332-662X","978-0-7695-5039","10.1109/QSIC.2013.58","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605907","data race;race detection;compiler optimization option;empirical study;efficiency","Optimization;Benchmark testing;Standards;Concurrent computing;Program processors;Computer bugs;Memory management","multi-threading;optimising compilers;program debugging;storage management","memory costs;code size;speed size;GCC compiler optimization options;PARSEC 3.0 suite;online race detection;program source code;execution traces;data structures;detection precision;performance slowdown improvement;memory overheads;multithreaded programs;compiler optimization level perspective;data race detection;efficiency aspect","","4","28","","","","","","IEEE","IEEE Conferences"
"Accelerating artificial bee colony algorithm by using an external archive","Hui Wang; Zhijian Wu; Xinyu Zhou; S. Rahnamayan","State Key Laboratory of Software Engineering, Wuhan University, 430072, China; State Key Laboratory of Software Engineering, Wuhan University, 430072, China; State Key Laboratory of Software Engineering, Wuhan University, 430072, China; Department of Electrical, Computer and Software Engineering, University of Ontario Institute of Technology, 2000 Simcoe Street North, Oshawa, L1H 7K4, Canada","2013 IEEE Congress on Evolutionary Computation","","2013","","","517","521","Artificial bee colony (ABC) is a new optimization technique which has shown to be competitive with some wellknown evolutionary algorithms. However, ABC is good at exploration but poor at exploitation. Inspired by JADE (adaptive differential evolution with optional external archive), this paper proposes an improved ABC (IABC) algorithm with an external archive, which stores some best solutions during the search process to guide the search of ABC. Experiments are conducted on several benchmark functions. Computational results show that our approach achieves promising performance in terms of solution accuracy and convergence speed.","1089-778X;1941-0026","978-1-4799-0454-9978-1-4799-0453-2978-1-4799-0451-8978-1-4799-0452","10.1109/CEC.2013.6557612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557612","Artificial bee colony (ABC);swarm intelligence;external archive;global optimization","Sociology;Statistics;Search problems;Convergence;Optimization;Benchmark testing;Particle swarm optimization","evolutionary computation;optimisation;search problems","artificial bee colony algorithm;external archive;IABC algorithm;optimization technique;evolutionary algorithms;JADE;adaptive differential evolution;improved ABC algorithm;search process;benchmark functions;solution accuracy;convergence speed","","1","12","","","","","","IEEE","IEEE Conferences"
"Uniformly Evaluating and Comparing Ranking Metrics for Spectral Fault Localization","C. Ma; Y. Zhang; T. Zhang; Y. Lu; Q. Wang","NA; NA; NA; NA; NA","2014 14th International Conference on Quality Software","","2014","","","315","320","Spectral fault localization (SFL) is one automatic fault-localization technique, which uses ranking metric to rank the risk of fault existence in each program entity after dynamically collecting the testing information. The effectiveness evaluation and comparison of ranking metrics are two important research problems. In this paper, we provide a uniformly theoretical investigation framework on longitudinally evaluating ranking metrics and horizontally comparing them for SFL techniques under any single fault scenario. We propose a generic vector table model as a novel device of thoroughly understanding various SFL techniques. By investigating rankings' mathematical formula of statements in the vector table model, the performance of different SFL techniques could be systematically analysed and compared. Under table model-driven evaluation framework, seven typical metrics as examples are explored, the existing equivalent group is extended, and the new relation of two equivalent groups is found. Our framework overcomes limitations of current empirical and theoretical approaches, and can theoretically evaluate the advantage and disadvantage of a SFL technique and compare different SFL techniques.","1550-6002;2332-662X","978-1-4799-7198-5978-1-4799-7197","10.1109/QSIC.2014.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958419","Vector table model;performance evaluation;performance comparison;spectral fault localization (SFL)","Vectors;Mathematical model;Testing;Analytical models;Performance evaluation;Debugging","fault tolerant computing;program testing;software metrics;vectors","ranking metrics;spectral fault localization;automatic fault-localization technique;generic vector table model;table model-driven evaluation","","1","12","","","","","","IEEE","IEEE Conferences"
"Introducing Continuous Delivery of Mobile Apps in a Corporate Environment: A Case Study","S. Klepper; S. Krusche; S. Peters; B. Bruegge; L. Alperowitz","NA; NA; NA; NA; NA","2015 IEEE/ACM 2nd International Workshop on Rapid Continuous Software Engineering","","2015","","","5","11","Software development is conducted in increasingly dynamic business environments. Organizations need the capability to develop, release and learn from software in rapid parallel cycles. The abilities to continuously deliver software, to involve users, and to collect and prioritize their feedback are necessary for software evolution. In 2014, we introduced Rugby, an agile process model with workflows for continuous delivery and feedback management, and evaluated it in university projects together with industrial clients. Based on Rugby's release management workflow we identified the specific needs for project-based organizations developing mobile applications. Varying characteristics and restrictions in projects teams in corporate environments impact both process and infrastructure. We found that applicability and acceptance of continuous delivery in industry depend on its adaptability. To address issues in industrial projects with respect to delivery process, infrastructure, neglected testing and continuity, we extended Rugby's workflow and made it tailor able. Eight projects at Capgemini, a global provider of consulting, technology and outsourcing services, applied a tailored version of the workflow. The evaluation of these projects shows anecdotal evidence that the application of the workflow significantly reduces the time required to build and deliver mobile applications in industrial projects, while at the same time increasing the number of builds and internal deliveries for feedback.","","978-1-4673-7067","10.1109/RCoSE.2015.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167166","Release Management;Configuration Management;Continuous Integration;Continuous Delivery;User Feedback;User Involvement;Agile Methods;Software Evolution","Software;Mobile communication;Servers;Organizations;Testing;Measurement","business data processing;mobile computing;organisational aspects;software prototyping;workflow management software","mobile application continuous delivery process;corporate environment;software development;dynamic business environments;rapid parallel cycles;Rugby agile process model;software evolution;feedback management;Rugby release management workflow;project-based organizations;outsourcing services","","1","31","","","","","","IEEE","IEEE Conferences"
"Calculation of regulators for the problems of mechatronics by means of the numerical optimization method","V. Zhmud; L. Dimitrov; O. Yadrishnikov","Novosibirsk State Technical University, Russia; Technical University of Sophia, Bulgaria; Novosibirsk State Technical University, Russia","2014 12th International Conference on Actual Problems of Electronics Instrument Engineering (APEIE)","","2014","","","739","744","Methods for numerical optimization of regulators has a number of advantages as compared with the analytical methods. The use of modern software, together with well-known and newest methods of forming of the optimization criteria in the form of cost functions provides a set of methods, successfully tested on a number of tasks. These methods, in contrast to analytical ones, allow effectively calculation of robust regulators for objects even in the case of the simultaneous presence in these of non-linear elements, units of lag, discrete elements (such as sampling-and-hold analog storage) and other complicating singularities.","","978-1-4799-6020-0978-1-4799-6019-4978-1-4799-6018","10.1109/APEIE.2014.7040784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7040784","Optimization;regulator;control;mechatronics","Optimization;Regulators;Transient analysis;Harmonic analysis;Linear programming;Mechatronics;Process control","controllers;mechatronics;optimisation","numerical optimization method;mechatronics;robust regulators;nonlinear elements;discrete elements","","20","6","","","","","","IEEE","IEEE Conferences"
"Spectrum-based fault localization tool with test case preprocessor","P. Daniel; K. Y. Sim","Faculty of Engineering, Computing and Science, Swinburne University of Technology Sarawak Campus, Kuching, Malaysia; Faculty of Engineering, Computing and Science, Swinburne University of Technology Sarawak Campus, Kuching, Malaysia","2013 IEEE Conference on Open Systems (ICOS)","","2013","","","162","167","Spectrum-based Fault Localization (SBFL) is an emerging debugging technique that assists software developers to locate faulty code in software. By utilizing code execution information (spectra), SBFL metrics rank lines of codes in software according to their likeliness to be faulty. However, recent studies showed that contradicting, duplicated or noisy spectra may deteriorate the ranking accuracy of SBFL metrics. In this paper, we propose and develop a novel SBFL tool with test case preprocessor to filter out test cases with contradicting, duplicated or other noisy spectra. Case studies conducted on real life faulty programs show that the proposed SBFL tool with test case preprocessor has successfully improved the performance of SBFL metrics in majority of the cases studied.","","978-1-4799-0285-9978-1-4799-3152","10.1109/ICOS.2013.6735067","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735067","Debugging Tool;Spectrum-based Fault Localization;Test Case Preprocessor;Software Analysis","Noise measurement;Harmonic analysis;Power harmonic filters;Graphical user interfaces;Schedules","program compilers;program debugging;software fault tolerance","spectrum based fault localization tool;test case preprocessor;debugging technique;software developers;code execution information;SBFL metrics rank lines","","1","16","","","","","","IEEE","IEEE Conferences"
"An If-While-If Model-Based Performance Evaluation of Ranking Metrics for Spectra-Based Fault Localization","C. Ma; T. Tan; Y. Chen; Y. Dong","NA; NA; NA; NA","2013 IEEE 37th Annual Computer Software and Applications Conference","","2013","","","609","618","Spectra-based fault localization (SFL) is an automatic fault-localization technique which has received a lot of attention due to its simplicity and effectiveness. SFL uses ranking metric (RM) to rank the risk of fault existence in each program entity after dynamically collecting the necessary information. The evaluation of RMs for SFL has recently become a research focus. To evaluate the average performance of RMs for SFL with different single-fault types, an If-While-If (IWI) model-based approach is presented in this paper. Firstly, through investigating rankings of statements in the IWI model, this paper takes an optimal RM known as an example to analyze its localization effectiveness for five types of single-fault. Secondly, a generic hierarchical method is given in the IWI model to precisely calculate the average performance of RMs. Two experiments, that calculate the average performance of the optimal RM on the IWI model and actual programs, are conducted with five single-fault types. The experimental results agree with theoretical analyses. It is found that the average performance of the optimal RM is related to the number of test cases and the number of program cycles, and the fault type. The IWI model could function as large programs to effectively evaluate RMs for different fault types.","0730-3157","978-0-7695-4986","10.1109/COMPSAC.2013.99","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649889","If-While-If (IWI) model;spectra-based fault localization (SFL);ranking metric (RM );performanc evaluatione","Performance evaluation;Syntactics;Educational institutions;Mathematical model;Software;Analytical models","fault tolerant computing;performance evaluation;software metrics","if-while-if model-based performance evaluation;ranking metrics;spectra-based fault localization;SFL technique;program entity;RM evaluation;IWI model-based approach;generic hierarchical method;program cycles;fault type","","1","15","","","","","","IEEE","IEEE Conferences"
"On fuzzy expert system development using computer-aided software engineering tools","N. A. Polkovnikova; V. M. Kureichik","Federal State-Owned Autonomy Educational Establishment of Higher Vocational Education «Southern Federal University», Russia; Federal State-Owned Autonomy Educational Establishment of Higher Vocational Education «Southern Federal University», Russia","Proceedings of IEEE East-West Design & Test Symposium (EWDTS 2014)","","2014","","","1","4","The purpose of this article is to demonstrate a way of intellectualization of automated information and diagnostic systems using knowledge bases, databases and algorithms for the formalization of procedures in terms of the development of an expert system for marine diesel engines. The aim of this work is to develop an expert system's architecture with data mining tools for solving the problem of technical exploitation of marine diesel engines based on fragmented, unreliable and possibly inaccurate information. The architecture of such expert system allows moving from normal monitoring to ""information monitoring"" in the specialized intelligent human-machine systems. Application of data mining technology allows optimizing database processing queries that retrieve the required information from the actual data in order to detect important patterns. An approach based on data mining and fuzzy logic in the expert system is shown on an example of solving technical exploitation of marine diesel engines problem.","","978-1-4799-7630","10.1109/EWDTS.2014.7027063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027063","","Expert systems;Fuzzy logic;Diesel engines;Pragmatics;Monitoring","computer aided software engineering;data mining;diesel engines;expert systems;fuzzy logic;man-machine systems;marine engineering;mechanical engineering computing;program diagnostics;query processing;software tools","fuzzy expert system development;computer-aided software engineering tools;automated diagnostic systems;automated information systems;knowledge bases;marine diesel engines;expert system architecture;data mining tools;information monitoring;specialized intelligent human-machine systems;data mining technology;database processing queries;fuzzy logic","","","7","","","","","","IEEE","IEEE Conferences"
"Benchmarking Optimization Algorithms: An Open Source Framework for the Traveling Salesman Problem","T. Weise; R. Chiong; J. Lassig; K. Tang; S. Tsutsui; W. Chen; Z. Michalewicz; X. Yao","UBRI, School of Computer Science and Technology, University of Science and Technology of China, Hefei, 230026, CHINA; Science and Information Technology, The University of Newcastle, Callaghan, 2308, AUSTRALIA; Department of Computer Science, University of Applied Sciences Zittau/Gorlitz, Gorlitz, 02826, GERMANY; UBRI, School of Computer Science and Technology, University of Science and Technology of China, Hefei, 230026, CHINA; Department of Management and Information Science, Hannan University, Matsubara, 580-0032, JAPAN; Computer Science Department, Colorado State University, Fort Collins, CO 80523 USA; School of Computer Science, The University of Adelaide, Adelaide, 5005, AUSTRALIA; UBRI, School of Computer Science and Technology, University of Science and Technology of China, Hefei, 230026, CHINA","IEEE Computational Intelligence Magazine","","2014","9","3","40","52","We introduce an experimentation procedure for evaluating and comparing optimization algorithms based on the Traveling Salesman Problem (TSP). We argue that end-of-run results alone do not give sufficient information about an algorithm's performance, so our approach analyzes the algorithm's progress over time. Comparisons of performance curves in diagrams can be formalized by comparing the areas under them. Algorithms can be ranked according to a performance metric. Rankings based on different metrics can then be aggregated into a global ranking, which provides a quick overview of the quality of algorithms in comparison. An open source software framework, the TSP Suite, applies this experimental procedure to the TSP. The framework can support researchers in implementing TSP solvers, unit testing them, and running experiments in a parallel and distributed fashion. It also has an evaluator component, which implements the proposed evaluation process and produces detailed reports. We test the approach by using the TSP Suite to benchmark several local search and evolutionary computation methods. This results in a large set of baseline data, which will be made available to the research community. Our experiments show that the tested pure global optimization algorithms are outperformed by local search, but the best results come from hybrid algorithms.","1556-603X;1556-6048","","10.1109/MCI.2014.2326101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853446","","Runtime;Benchmark testing;Optimization;Traveling salesman problems;Approximation algorithms;Software algorithms;Time measurement","evolutionary computation;mathematics computing;public domain software;search problems;travelling salesman problems","optimization algorithm benchmarking;traveling salesman problem;experimentation procedure;TSP;performance curves;performance metric;open source software framework;TSP suite;local search;evolutionary computation methods;pure global optimization algorithms","","26","48","","","","","","IEEE","IEEE Journals & Magazines"
"Numerical modelling complements physical testing in staged design of ocean wave-driven pump","R. Boileau; W. Raman-Nair; M. Graham","National Research Council of Canada, St. John's, Canada; National Research Council of Canada, St. John's, Canada; Burin Campus, College of the North Atlantic, Burin, Canada","2014 Oceans - St. John's","","2014","","","1","6","National Research Council Canada (NRC) is collaborating with College of the North Atlantic (CNA) to develop an ocean wave-driven pump to supply the CNA shore-based aquaculture centre in Lord's Cove, Newfoundland. NRC provides a combination of physical model testing under controlled laboratory conditions and computer simulation using numerical modelling tools developed at NRC and commercial software. The CNA plans to demonstrate the sustainability of a shore-based aquaculture centre in the context of rural Newfoundland. The efficiency of the centre depends in part on its energy consumption, a major part of which is supplying seawater to a facility some distance from the ocean and above sea level. The development of a seawater supply pump driven by the power of ocean waves would reduce the electrical costs for the centre. The NRC is supporting a staged design program through numerical and physical model testing to select a robust design optimized for the local sea conditions at the Lord's Cove mooring site. The wave pump is inspired by designs for wave energy converters. But since wave energy converters have not converged on a single design, there are major risks and costs in designing a wave pump, from construction and testing of physical models for competing designs to mooring failure and potential loss of the device in the hurricane-force storms typical of southern Newfoundland. Using lessons learned in the energy sector, risks can be mitigated by systematically testing the behaviour and response of proposed concepts through a staged design program, starting from characterizing the hydrodynamic characteristics of a simplified model through progressively larger, more complex physical and numerical models used in functional tests. This approach systematically directs the design process toward a single prototype optimized for performance in specified wave conditions using scaled model tests in advance of the most costly stage of sea trials. The goals of the staged design program for the CNA wave-driven pump are to (1) evaluate various design concepts (2) select a design suited to the CNA mooring site (3) optimize the design for a typical sea state and (4) specify a mooring robust enough to keep the platform on station during the sometimes extreme sea states at Lord's Cove. The program began with characterization of the chosen mooring site outside Lord's Cove. Processed wave buoy data from this site informed the estimation of mooring line loads using numerical tools developed by NRC and typical sea states on which to focus design optimization. In the second stage, a numerical tool was developed to estimate pump output and validated against test results for a physical pump. In the third stage, various wave pump platform design concepts were evaluated based on estimation of hydrodynamic characteristics. The designs were characterized using a combination of physical model tests in the NRC 200-metre wave tank in St. John's and computer simulations using Ansys Aqwa and Matlab. A sample pump platform design is shown as a scale model and a numerical mesh in Fig. 1. In the fourth stage, which will precede a full-scale prototype deployment at sea, a design has been selected and an operational scale model wave pump platform has been tested in the NRC wave facilities. Numerical modelling is used to choose a mooring for the full-scale deployment. In this staged design program, the use of physical model tests and computer simulation are complementary methods used alternatively to solve potentially costly and time-consuming problems. A selection of the problems encountered in the design of the wave pump platform is presented along with a critique of the solutions applied by NRC to build confidence in a wave-driven pump design tailored for the CNA site.","0197-7385","978-1-4799-4918-2978-1-4799-4920-5978-1-4799-4919","10.1109/OCEANS.2014.7003281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7003281","wave-powered pump;wave energy converter;shore-based aquaculture","Numerical models;Mathematical model;Computational modeling;Testing;Load modeling;Pistons;Solid modeling","aquaculture;energy consumption;failure (mechanical);hydrodynamics;numerical analysis;ocean waves;optimisation;prototypes;pumps;risk management;seawater;storms","ocean wave-driven pump;National Research Council Canada;College of the North Atlantic;CNA shore-based aquaculture centre;physical model testing;controlled laboratory conditions;computer simulation;numerical modelling tools;shore-based aquaculture centre sustainability;rural Newfoundland;energy consumption;sea level;seawater supply pump;Lord's Cove mooring site;ocean waves;electrical cost reduction;staged design program;numerical model testing;robust design optimization;local sea conditions;wave energy converters;mooring failure;hurricane-force storms;southern Newfoundland;energy sector;risk mitigation;hydrodynamic characteristics;functional tests;wave conditions;scaled model tests;CNA mooring site;sea states;wave buoy data processing;mooring line load estimation;design optimization;pump output estimation;physical pump;wave pump platform design characteristics;wave pump platform design concepts;hydrodynamic characteristics estimation;physical model tests;St. John's;computer simulations;Ansys Aqwa;numerical mesh;full-scale prototype deployment;operational scale model;operational scale model wave pump platform;NRC wave facilities;full-scale deployment","","","8","","","","","","IEEE","IEEE Conferences"
"Automated Crash Filtering for ARM Binary Programs","K. Eom; J. Paik; S. Mok; H. Jeon; E. Cho; D. Kim; J. Ryu","NA; NA; NA; NA; NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","2","","478","483","This paper aims to help to differentiate security related crashes from benign vulnerabilities, using static taint-analysis. To achieve this goal, we propose a tool named Crash Filter, which determines if a crash can be made to be exploitable or not, by analyzing ARM binary codes. We envision that the proposed analysis would help to timely fix security-critical bugs.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273656","crashes;static program analyses;binary analyses;compiler optimization techniques","Registers;Flow graphs;Security;Computer bugs;Algorithm design and analysis;Software","program compilers;program debugging;program diagnostics;program testing;safety-critical software;software tools","crash filtering;ARM binary program;security related crash;vulnerability;static taint-analysis;Crash Filter;security-critical bug;software testing tool;compiler optimization","","2","30","","","","","","IEEE","IEEE Conferences"
"Design of Bioprosthetic Aortic Valves using biaxial test data","Y. Dabiri; K. Paulson; J. Tyberg; J. Ronsky; I. Ali; E. Di Martino; K. Narine","University of Calgary, Alberta, Canada; Zymetrix Center at the University of Calgary, Alberta, Canada; University of Calgary, Alberta, Canada; University of Calgary, Alberta, Canada; University of Calgary, Alberta, Canada; University of Calgary, Alberta, Canada; University of Calgary, Alberta, Canada","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","3319","3322","Bioprosthetic Aortic Valves (BAVs) do not have the serious limitations of mechanical aortic valves in terms of thrombosis. However, the lifetime of BAVs is too short, often requiring repeated surgeries. The lifetime of BAVs might be improved by using computer simulations of the structural behavior of the leaflets. The goal of this study was to develop a numerical model applicable to the optimization of durability of BAVs. The constitutive equations were derived using biaxial tensile tests. Using a Fung model, stress and strain data were computed from biaxial test data. SolidWorks was used to develop the geometry of the leaflets, and ABAQUS finite element software package was used for finite element calculations. Results showed the model is consistent with experimental observations. Reaction forces computed by the model corresponded with experimental measurements when the biaxial test was simulated. As well, the location of maximum stresses corresponded to the locations of frequent tearing of BAV leaflets. Results suggest that BAV design can be optimized with respect to durability.","1094-687X;1558-4615","978-1-4244-9271-8978-1-4244-9270","10.1109/EMBC.2015.7319102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319102","","Mathematical model;Stress;Valves;Strain;Geometry;Finite element analysis;Numerical models","finite element analysis;prosthetics;stress-strain relations;tensile testing","bioprosthetic aortic valve design;biaxial test data;numerical model;durability optimization;constitutive equations;biaxial tensile tests;Fung model;SolidWorks;leaflet geometry;computer simulations;ABAQUS finite element software package;reaction forces;stress-strain curves","Aortic Valve;Computer Simulation;Finite Element Analysis;Heart Valve Prosthesis;Prosthesis Design;Stress, Mechanical;Tensile Strength","1","6","","","","","","IEEE","IEEE Conferences"
"X-ANOVA and X-Utest features for Android malware analysis","R. Raphael; V. P.; B. Omman","Department of Computer Science and Engineering, SCMS School of Engineering and Technology, Ernakulam, Kerala, India; Department of Computer Science and Engineering, SCMS School of Engineering and Technology, Ernakulam, Kerala, India; Department of Computer Science and Engineering, SCMS School of Engineering and Technology, Ernakulam, Kerala, India","2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","","2014","","","1643","1649","In this paper we proposed a static analysis framework to classify the android malware. The three different feature likely (a) opcode (b) method and (c) permissions are extracted from the each android .apk file. The dominant attributes are aggregated by modifying two different ranked feature methods such as ANOVA to Extended ANOVA (X-ANOVA) and Wann-Whiteney U-test to Extended U-Test (X-U-Test). These two statistical feature ranking methods retrieve the significant features by removing the irrelevant attributes based on their score. Accuracy of the proposed system is computed by using three different classifiers (J48, ADAboost and Random forest) as well as voted classification technique. The X-U-Test exhibits better accuracy results compared with X-ANOVA. The highest accuracy 89.36% is obtained with opcode while applying X-U-Test and X-ANOVA shows high accuracy of 87.81% in the case of method as a feature. The permission based model acquired highest accuracy in independent (90.47%) and voted (90.63%) classification model.","","978-1-4799-3080-7978-1-4799-3078","10.1109/ICACCI.2014.6968608","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968608","Android Malware;Mobile Malware;ANOVA;Wann-Whiteney Test;U-Test;Feature Ranking;Classifiers","Malware;Accuracy;Smart phones;Training;Analysis of variance;Mathematical model;Equations","Android (operating system);invasive software;learning (artificial intelligence);program diagnostics;program testing;statistical analysis","X-ANOVA;X-Utest features;Android malware analysis;static analysis;opcode;Wann-Whiteney U-test;extended U-Test;X-U-Test;AdaBoost;random forest","","1","24","","","","","","IEEE","IEEE Conferences"
"Virtual view distortion estimation for depth map coding","C. Yang; P. An; D. Liu; L. Shen","Key Laboratory of Advanced Displays and System Application, Ministry of Education, Shanghai, China; Key Laboratory of Advanced Displays and System Application, Ministry of Education, Shanghai, China; Key Laboratory of Advanced Displays and System Application, Ministry of Education, Shanghai, China; Key Laboratory of Advanced Displays and System Application, Ministry of Education, Shanghai, China","2015 Visual Communications and Image Processing (VCIP)","","2015","","","1","4","Multi-view video plus depth (MVD) format is a three-dimensional (3D) video representation. The depth map in MVD provides the scene geometry information and is used to render the virtual view through Depth Image Based Rendering (DIBR). In this paper, a virtual view distortion estimation function based on the characteristics of both texture image and depth map is proposed which can estimate virtual view distortion induced by depth map compression accurately, and the function is implemented to the Rate Distortion Optimization (RDO) in the depth map coding. Compared with the View Synthesis Optimization (VSO) in 3D-HEVC Test Model (HTM) reference software, the experimental results demonstrate that the proposed method can improve the BD-PSNR of virtual view for 0.26 dB on average, and the encoding time has reduced for 31% on average due to the low complexity of the proposed function.","","978-1-4673-7314-2978-1-4673-7313","10.1109/VCIP.2015.7457845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457845","3D video;depth map distortion;depth map encoding;rate distortion optimization;virtual view distortion","Distortion;Encoding;Measurement;Image coding;Image analysis;Three-dimensional displays;Optimization","data compression;image representation;image texture;optimisation;rendering (computer graphics);video coding","encoding time;HTM reference software;3D-HEVC test model;VSO;view synthesis optimization;RDO;rate distortion optimization;depth map compression;depth map characteristics;texture image characteristics;DIBR;depth image-based rendering;scene geometry information;3D video representation;three-dimensional video representation;MVD format;multiview video-plus-depth format;depth map coding;virtual view distortion estimation","","","21","","","","","","IEEE","IEEE Conferences"
"Hierarchical similarity measurement model of program execution","A. Reungsinkonkarn","Department of Computer Information System, Assumption University, Bangkok Thailand 10240","2013 IEEE 4th International Conference on Software Engineering and Service Science","","2013","","","255","261","The concept of similarity measurement is widely used in a number of research areas such as data mining, software testing and debugging, and software security. Several researches have been conducted to continuously improve and explore for a better and new measurement of similarity. The similarity measurement mostly deals with data, items, or features of a program. However, there is no similarity measurement for output of program execution which can be various data types including primitive, abstract and heterogeneous data types such as numeric, character, list, tree, table (relation) and object. Lack of these measurements can reduce the effectiveness of software testing techniques or the quality of vulnerability detection methods to a certain degree. This paper proposes hierarchical similarity measurement model for measuring program execution. The measurement is categorized and arranged into successive levels according to its data type, and its calculation for a higher level data type is performed based on lower level data types. Some operations performed on our similarity measurement are briefly introduced. In addition, we present a similarity graph that can be used in an optimization process.","2327-0594;2327-0586","978-1-4673-5000-6978-1-4673-4997-0978-1-4673-4998","10.1109/ICSESS.2013.6615300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615300","Similarity measurement;Data mining;Optimization;Evolution computation;Software engineering","Remuneration;Bismuth;Arrays;Weaving","graph theory;optimisation;program diagnostics","hierarchical similarity measurement model;program execution;data mining;software testing;software debugging;software security;vulnerability detection methods;similarity graph;optimization process","","2","9","","","","","","IEEE","IEEE Conferences"
"Test suite prioritisation using trace events technique","K. Rajarathinam; S. Natarajan","Department of Computer Science and Engineering, Velammal College of Engineering and Technology, Madurai, Tamilnadu, India; Department of Electronics and Communication Engineering, Velammal College of Engineering and Technology, Madurai, Tamilnadu, India","IET Software","","2013","7","2","85","92","The size of the test suite and the duration of time determines the time taken by the regression testing. Conversely, the testers can prioritise the test cases by the use of a competent prioritisation technique to obtain an increased rate of fault detection in the system, allowing for earlier corrections, and getting higher overall confidence that the software has been tested suitably. A prioritised test suite is more likely to be more effective during that time period than would have been achieved via a random ordering if execution needs to be suspended after some time. An enhanced test case ordering may be probable if the desired implementation time to run the test cases is proven earlier. This research work's main intention is to prioritise the regressiontesting test cases. In order to prioritise the test cases some factors are considered here. These factors are employed in the prioritisation algorithm. The trace events are one of the important factors, used to find the most significant test cases in the projects. The requirement factor value is calculated and subsequently a weightage is calculated and assigned to each test case in the software based on these factors by using a thresholding technique. Later, the test cases are prioritised according to the weightage allocated to them. Executing the test cases based on the prioritisation will greatly decreases the computation cost and time. The proposed technique is efficient in prioritising the regression test cases. The new prioritised subsequences of the given unit test suites are executed on Java programs after the completion of prioritisation. Average of the percentage of faults detected is an evaluation metric used for evaluating the 'superiority' of these orderings.","1751-8806;1751-8814","","10.1049/iet-sen.2011.0203","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519507","","","","","","","33","","","","","","IET","IET Journals & Magazines"
"Smart grid dispatch optimization control techniques for transactive energy systems","S. A. Chandler; J. H. Rinaldi; R. B. Bass; L. Beckett","Department of Electrical and Computer Engineering, Portland State University, Oregon, USA; Department of Electrical and Computer Engineering, Portland State University, Oregon, USA; Department of Electrical and Computer Engineering, Portland State University, Oregon, USA; Predictive Learning Utility Systems, Portland, Oregon, USA","2014 IEEE Conference on Technologies for Sustainability (SusTech)","","2014","","","51","54","Transactive smart-grid systems require control techniques to manage forecasting, dispatch optimization, feeder dynamic segmentation, interconnect and micro-grid operations analysis, and other services. Two dispatch optimization tools used in a virtual controller are compared using the same input data: a mixed integer linear programming micro-grid dispatch system, and an artificial neural network (ANN) dispatch system, each of which have been developed using specifications compliant with the Pacific Northwest National Laboratory Smart Grid Demonstration (SGD) project transactive energy nodal system model. The characteristics of these separate optimization techniques are documented from a control perspective, and a distribution substation communications compliant service architecture is developed to use either simulator output set based on an operations context or grid-operator preference such as timing or least-cost. Methods for real-time use of both models are reviewed, and the application of test cases specific to comparison of the systems is explored, using historical time-series inputs from the SGD project and other relevant scenarios. A scalable software architecture is recommended which may function as a smart-grid system controller where separate program optimization methods may be applied in parallel, enabling results to be used in real-time for a risk based assessment of competing operations strategies for an interconnect, control area or micro-grid.","","978-1-4799-5238","10.1109/SusTech.2014.7046217","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7046217","smart grid;transactive energy control;microgrid operations;dispatch optimization","Smart grids;Real-time systems;Optimization;Timing;Control systems;Load management;Databases","distributed power generation;integer programming;linear programming;neural nets;power distribution control;power engineering computing;power generation dispatch;smart power grids;time series","smart grid dispatch optimization control techniques;transactive energy systems;transactive smart-grid systems;virtual controller;mixed integer linear programming;microgrid dispatch system;artificial neural network;ANN;Pacific Northwest National Laboratory Smart Grid Demonstration project;SGD project;distribution substation communications compliant service architecture;grid-operator preference;historical time-series inputs;scalable software architecture;separate program optimization methods;risk based assessment","","2","7","","","","","","IEEE","IEEE Conferences"
"Offline Synthesis of Online Dependence Testing: Parametric Loop Pipelining for HLS","J. Liu; S. Bayliss; G. A. Constantinides","NA; NA; NA","2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines","","2015","","","159","162","Loop pipelining is probably the most important optimization method in high-level synthesis (HLS), allowing multiple loop iterations to execute in a pipeline. In this paper, we extend the capability of loop pipelining in HLS to handle loops with uncertain memory behaviours. We extend polyhedral synthesis techniques to the parametric case, offloading the uncertainty to parameter values determined at run time. Our technique then synthesizes lightweight runtime checks to detect the case where a low initiation interval (II) is achievable, resulting in a run-time switch between aggressive (fast) and conservative (slow) execution modes. This optimization is implemented into an automated source-to-source code transformation framework with Xilinx Vivado HLS as one RTL generation backend. Over a suite of benchmarks, experiments show that our optimization can implement transformed pipelines at almost same clock frequency as that generated directly with Vivado HLS, but with approximately 10× faster initiation interval in the fast case, while consuming approximately 60% more resource.","","978-1-4799-9969","10.1109/FCCM.2015.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160061","High-level Synthesis;Loop Pipelining;Polyhedral Analysis;FPGA","Pipeline processing;Benchmark testing;Arrays;Optimization;Pipelines;Runtime;Field programmable gate arrays","circuit optimisation;field programmable gate arrays;high level synthesis;logic design;logic testing;source code (software)","offline synthesis;online dependence testing;parametric loop pipelining;optimization method;high-level synthesis;multiple loop iterations;uncertain memory behaviours;polyhedral synthesis techniques;run-time switch;conservative execution modes;aggressive execution modes;automated source-to-source code transformation framework;Xilinx Vivado HLS;RTL generation backend;clock frequency;High-level synthesis","","5","16","","","","","","IEEE","IEEE Conferences"
"A model to assess the effectiveness of fault prediction techniques for quality assurance","L. Kumar; S. K. Rath","Dept. CS&E, National Institute of Technology, Rourkela, India; Dept. CS&E, National Institute of Technology, Rourkela, India","2015 Annual IEEE India Conference (INDICON)","","2015","","","1","6","Fault prediction techniques aim to predict faulty module in order to reduce the effort to be applied in later phase of software development. Majority of the approaches available in literature for fault prediction are based on regression analysis and neural network techniques. It is observed that numerous software metrics are also being used as input for fault prediction. In this paper, a cost evaluation model has been proposed for Object-Oriented software which performs cost based analysis for misclassification of faults. Appropriately, this work focuses on inspecting the usability of fault prediction. Chidamber and Kemerer (CK) metrics suite has been considered to provide requisite input data to design the model using logistic regression and hybrid approach of Neural network and Particle Swarm Optimization (Neuro-PSO and Modified Neuro-PSO). Here, fault considered as dependent variable and CK metric suite are as independent variables. A case study of Eclipse JDT core has been considered for predicting a comparative study of performances of two approaches. Fault prediction is found to be useful where normalized estimated fault removal cost (NEcost) was less than certain threshold value. Modified Neuro-PSO model obtained promising results in terms of cost analysis when compared with those of Neuro-PSO and logistic regression.","2325-9418","978-1-4673-7399-9978-1-4673-7398","10.1109/INDICON.2015.7443413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7443413","CK Metric;Fault prediction;Eclipse JDT core;Genetics algorithm;Neuro-PSO;Modified Neuro-PSO;regression analysis","Measurement;Testing;Mathematical model;Predictive models;Neural networks;Software;Logistics","neural nets;object-oriented methods;particle swarm optimisation;regression analysis;software fault tolerance;software metrics;software quality","fault prediction techniques;quality assurance;software development;regression analysis;neural network techniques;software metrics;cost evaluation model;object-oriented software;cost based analysis;fault misclassification;Chidamber-Kemerer metrics suite;logistic regression;particle swarm optimization;Eclipse JDT core;normalized estimated fault removal cost;modified neuro-PSO model","","1","23","","","","","","IEEE","IEEE Conferences"
"AEP - Automatic Exchange of Embedded System Software Parameters","R. D. C. C. Soldi; A. A. M. Fröhlich","NA; NA","2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing","","2013","","","1938","1943","The process of debugging embedded system software is a non-trivial task that consumes a lot of time, once it needs a thorough inspection of the entire source code to make sure that there is no behavior beyond expectations. Coding and testing embedded systems software, timing and hardware is even more defiant, once developers need to find out how to optimize the use of the scarce resources since the test itself will compete with the application under test by the scarce system resources. Also, both run in proper platforms, that depends on operating systems, architecture, vendors, debugging tool, etc. This makes embedded systems more susceptible to errors as well as specification failures. This paper presents AEP, a tool to help developers in the process of debugging embedded systems. The main idea of this tool is emulating various possible system configuration to try to find errors in the application. An XML file contains all required information to perform automated compilation, emulation and debugging, and there is no need of human interference. The evaluation of AEP was in terms of memory consumption and time to perform debugging. The obtained results indicate that even with no previous information this tool can produce helpful data for developers to find and fix bugs.","","978-0-7695-5088","10.1109/HPCC.and.EUC.2013.278","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832161","embedded system;debug","Testing;Debugging;Embedded systems;Hardware;Emulation","embedded systems;program debugging;program testing;source code (software);XML","AEP;embedded system software parameter automatic exchange;embedded system software debugging;source code;embedded systems software coding;embedded systems software testing;XML file;memory consumption","","","19","","","","","","IEEE","IEEE Conferences"
"Considerations on application of selective hardening based on software fault tolerance techniques","F. Restrepo-Calle; S. Cuenca-Asensi; A. Martínez-Alvarez; F. L. Kastensmidt","Universidad Nacional de Colombia, Bogota, Colombia; University of Alicante, Alicante, Spain; University of Alicante, Alicante, Spain; Universidade Federal do Rio Grande do Sul (UFRGS), Porto Alegre, Brazil","2015 16th Latin-American Test Symposium (LATS)","","2015","","","1","6","This paper analyses the nature of fault tolerance software-based techniques and the influence of their overheads to determine an efficient strategy for applying those techniques in a selective way. Several considerations that have to be taken into account are presented in this work. These include an analysis of fault coverage and overheads when selective hardening is adopted; side effects of selective protection based on software; and the need of new criticality metrics, apart from those used for hardware-based techniques (e.g., AVF), to facilitate and prioritize the selection of resources to be protected.","2373-0862","978-1-4673-6710","10.1109/LATW.2015.7102509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102509","reliability;software fault-tolerance;selective","Registers;Software;Circuit faults;Hardware;Microprocessors;Reliability;Measurement","electronic engineering computing;radiation hardening (electronics);software fault tolerance","selective hardening;software fault tolerance;fault tolerance software-based techniques;fault coverage analysis;hardware-based techniques","","2","28","","","","","","IEEE","IEEE Conferences"
"Generating test cases for Q-learning algorithm","L. Kumaresan; A. Chamundeswari","Computer Science and Engineering, SSN College of Engineering, Chennai, 603110, India; Computer Science and Engineering, SSN College of Engineering, Chennai, 603110, India","2013 Fourth International Conference on Computing, Communications and Networking Technologies (ICCCNT)","","2013","","","1","7","In this paper, the work addresses the notion of generating test cases by applying Q-learning algorithm in source code to obtain the optimal solution. The test cases are generated manually using the state diagram and automatically using obtained optimal solution from Q-learning algorithm. Here, the shortest path algorithm is chosen and the optimal solution is obtained for each and every initial states. It mainly focuses on the analysis between the manually generated test cases and automatically generated test cases.","","978-1-4799-3926-8978-1-4799-3925","10.1109/ICCCNT.2013.6726657","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6726657","Test case generation;Q-learning;testful","Instruments;Testing;Software algorithms;Java;Learning (artificial intelligence);Robot sensing systems","formal specification;graph theory;learning (artificial intelligence);optimisation;program testing","test case generation;Q-learning algorithm;source code;state diagram;shortest path algorithm;supervised learning;optimization problem","","","11","","","","","","IEEE","IEEE Conferences"
"Optimizing PV system performance considering the impacts of non-uniform irradiance and partial shading","M. Jamil; M. Ehtesham","Department of Electrical Engineering, Jamia Millia Islamia, New Delhi; Department of Electrical Engineering, Jamia Millia Islamia, New Delhi","2015 International Conference on Energy Economics and Environment (ICEEE)","","2015","","","1","6","As the photovoltaic (PV) power generation is becoming increasingly attractive due to the availability of resources and environmental benefits, it is significantly important to control and optimize the output power to ensure stability and security of power system. In this paper performance analysis of PV systems at various Insolations, Temperature, Partial shading were examined and their relationship with output power has been investigated in MATLAB/Simulink environment, whereas the test for Tilt angle was carried out using PVSYST software. Further the simulated results were optimized using Fuzzy Logic Toolbox of MATLAB and it was concluded that optimum output power lies in the medium range, whereas current and voltage in low range. The simulated model corresponds to the practical 2 KW panel installed and MPP is tracked using MB technology incorporating pyranometer. Also the optimum tilt angle for panel is investigated and found that it concurs with the latitude of installed panel location.","","978-1-4673-7492-7978-1-4673-7491-0978-1-4673-7490","10.1109/EnergyEconomics.2015.7235090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7235090","Photovoltaic (PV);insolations;partial shading;optimization;MPPT;pyranometer","Mathematical model;Temperature;Integrated circuit modeling;MATLAB;Maximum power point trackers;Semiconductor diodes","optimisation;photovoltaic power systems;solar cells;sunlight","PV system performance optimization;nonuniform irradiance;partial shading;photovoltaic power generation;PV power generation;MATLAB;Simulink environment;tilt angle;PVSYST software;fuzzy logic toolbox;MPP tracking;MB technology","","3","10","","","","","","IEEE","IEEE Conferences"
"COMP: Compiler Optimizations for Manycore Processors","L. Song; M. Feng; N. Ravi; Y. Yang; S. Chakradhar","NA; NA; NA; NA; NA","2014 47th Annual IEEE/ACM International Symposium on Microarchitecture","","2014","","","659","671","Applications executing on multicore processors can now easily offload computations to many core processors, such as Intel Xeon Phi coprocessors. However, it requires high levels of expertise and effort to tune such offloaded applications to realize high-performance execution. Previous efforts have focused on optimizing the execution of offloaded computations on many core processors. However, we observe that the data transfer overhead between multicore and many core processors, and the limited device memories of many core processors often constrain the performance gains that are possible by offloading computations. In this paper, we present three source-to-source compiler optimizations that can significantly improve the performance of applications that offload computations to many core processors. The first optimization automatically transforms offloaded codes to enable data streaming, which overlaps data transfer between multicore and many core processors with computations on these processors to hide data transfer overhead. This optimization is also designed to minimize the memory usage on many core processors, while achieving the optimal performance. The second compiler optimization re-orders computations to regularize irregular memory accesses. It enables data streaming and factorization on many core processors, even when the memory access patterns in the original source codes are irregular. Finally, our new shared memory mechanism provides efficient support for transferring large pointer-based data structures between hosts and many core processors. Our evaluation shows that the proposed compiler optimizations benefit 9 out of 12 benchmarks. Compared with simply offloading the original parallel implementations of these benchmarks, we can achieve 1.16x-52.21x speedups.","1072-4451;2379-3155","978-1-4799-6998","10.1109/MICRO.2014.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011425","compiler optimizations;manycore coprocessors;offload;Intel MIC","Coprocessors;Program processors;Data transfer;Microwave integrated circuits;Optimization;Benchmark testing;Multicore processing","data structures;optimising compilers;shared memory systems;source code (software);storage management","source-to-source compiler optimizations;compiler optimization for manycore processors;offloaded codes;data streaming;multicore processors;data transfer overhead hiding;memory usage minimization;vectorization;memory access patterns;shared memory mechanism;pointer-based data structures","","4","31","","","","","","IEEE","IEEE Conferences"
"Artificial intelligence based TNEP. Part 2: Case studies","F. Solomonesc; C. Barbulescu; S. Kilyeni; O. Pop; D. Cristian","&#x201C;Politehnica&#x201D; University of Timisoara, Romania, Power Systems Department; &#x201C;Politehnica&#x201D; University of Timisoara, Romania, Power Systems Department; &#x201C;Politehnica&#x201D; University of Timisoara, Romania, Power Systems Department; &#x201C;Politehnica&#x201D; University of Timisoara, Romania, Power Systems Department; &#x201C;Politehnica&#x201D; University of Timisoara, Romania, Power Systems Department","2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics (SACI)","","2013","","","395","398","The paper is focusing on transmission network expansion planning (TNEP) problem solved using artificial intelligence techniques. It is divided into two parts. The 1<sup>st</sup> part is dedicated to the particle swarm optimization (PSO) and genetic algorithm (GA) concepts and mechanisms. The mathematical models and the associated software tool are also presented. Practical considerations are discussed. The 2<sup>nd</sup> part is focusing on case studies. 13 buses test power system, developed by the authors and IEEE 24 RTS have been used. The research work is going to be used in case of the Romanian power system (over 1000 buses).","","978-1-4673-6400-3978-1-4673-6397","10.1109/SACI.2013.6609006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6609006","","Power systems;Genetic algorithms;Artificial intelligence;Software tools;Educational institutions;Planning;Algorithm design and analysis","artificial intelligence;genetic algorithms;particle swarm optimisation;power system analysis computing;power transmission planning;software tools","artificial intelligence based TNEP;transmission network expansion planning problem;particle swarm optimization;PSO;genetic algorithm;GA;mathematical models;software tool;IEEE 24 RTS;Romanian power system;buses test power system","","","4","","","","","","IEEE","IEEE Conferences"
"A Differential Evolution with Replacement Strategy for Real-Parameter Numerical Optimization","C. Xu; H. Huang; S. Ye","School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","1617","1624","Differential Evolution (DE) has been widely used as a continuous optimization technique for several problems like electromagnetic optimization, bioprocess system optimization and so on. However, during the optimization process, DE's population may stagnate local optima where the algorithm has to spend a large number of function evaluations to get rid of them. This paper presents an improved DE algorithm (denoted as RSDE) which combines two Replacement Strategies (RS). The motivation of RS is that replacing an unimproved individual and replacing a premature population using RS which can enhance the DE exploitation performance and exploration performance respectively. We tested the RSDE performance using the newly Single Objective Real-Parameter Numerical Optimization problems provided by the CEC 2014 Special Session and Competition. Moreover, computational results, convergence figures and the performance of these two RS will be presented to discuss the feature of RSDE.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900468","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900468","Differential Evolution;Replacement Strategy;Single Objective Real-Parameter Numerical Optimization problem","Sociology;Statistics;Vectors;Optimization;Convergence;Linear programming;Algorithm design and analysis","evolutionary computation","differential evolution;replacement strategy;DE;continuous optimization technique;RSDE algorithm;single objective real-parameter numerical optimization problems","","8","9","","","","","","IEEE","IEEE Conferences"
"Efficient software implementation of ring-LWE encryption","R. de Clercq; S. S. Roy; F. Vercauteren; I. Verbauwhede","KU Leuven, Department of Electrical Engineering - ESAT/COSIC and iMinds, Belgium; KU Leuven, Department of Electrical Engineering - ESAT/COSIC and iMinds, Belgium; KU Leuven, Department of Electrical Engineering - ESAT/COSIC and iMinds, Belgium; KU Leuven, Department of Electrical Engineering - ESAT/COSIC and iMinds, Belgium","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","339","344","Present-day public-key cryptosystems such as RSA and Elliptic Curve Cryptography (ECC) will become insecure when quantum computers become a reality. This paper presents the new state of the art in efficient software implementations of a post-quantum secure public-key encryption scheme based on the ring-LWE problem. We use a 32-bit ARM Cortex-M4F microcontroller as the target platform. Our contribution includes optimization techniques for fast discrete Gaussian sampling and efficient polynomial multiplication. Our implementation beats all known software implementations of ring-LWE encryption by a factor of at least 7. We further show that our scheme beats ECC-based public-key encryption schemes by at least one order of magnitude. At medium-term security we require 121 166 cycles per encryption and 43 324 cycles per decryption, while at a long-term security we require 261 939 cycles per encryption and 96 520 cycles per decryption. Gaussian sampling is done at an average of 28.5 cycles per sample.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092411","ring learning with errors (ring-LWE);software implementation;post-quantum secure;public-key encryption;discrete Gaussian sampling;number theoretic transform","Polynomials;Encryption;Registers;Table lookup;Indexes;Software;Gaussian distribution","Gaussian processes;optimisation;public key cryptography;sampling methods","public-key cryptosystems;software implementation;quantum computers;RSA;elliptic curve cryptography;ECC;ARM Cortex-M4F microcontroller;post-quantum secure public-key encryption scheme;optimization techniques;fast discrete Gaussian sampling;polynomial multiplication;ring-LWE encryption;medium-term security;decryption;word length 32 bit","","6","19","","","","","","IEEE","IEEE Conferences"
"Comparative performance evaluation of teaching learning based optimization against genetic algorithm on benchmark functions","M. Ebraheem; T. R. Jyothsna","Dept. of Electrical and Electronics Engg., GITAM Institute of Technology, GITAM University, Visakhapatnam, India; Dept. of Electrical Engineering, Andhra University college of Engineering, AU, Visakhapatnam, India","2015 IEEE Power, Communication and Information Technology Conference (PCITC)","","2015","","","327","331","In this paper effectiveness of newly introduced `teaching learning based optimization' (TLBO) is evaluated against different benchmark optimization problems. The effectiveness, then, is compared with the performance of genetic algorithm (GA) using the same parameters as used with TLBO. The functions on which the two algorithms applied in this work are rastrigin function, quartic, rosenbrock, six hump camel back. And the results are validated. Both the algorithms are applied on these functions using the same software (MATLAB) on the same platform also with the same number of population and elite group. The effectiveness of the TLBO is compared with that of GA based on the speed, average solution.","","978-1-4799-7455-9978-1-4799-7454","10.1109/PCITC.2015.7438185","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7438185","Teaching Learning based optimization;Genetic Algorithm;Benchmark functions","Genetic algorithms;Optimization;Sociology;Statistics;Benchmark testing;Genetics;Education","genetic algorithms;teaching","performance evaluation;teaching learning based optimization;genetic algorithm;benchmark functions;TLBO;benchmark optimization problems;GA;Matlab;hump camel back;rosenbrock;quartic function;rastrigin function","","","5","","","","","","IEEE","IEEE Conferences"
"Which of My Failures are Real? Using Relevance Ranking to Raise True Failures to the Top","Z. Gao; A. M. Memon","NA; NA","2015 30th IEEE/ACM International Conference on Automated Software Engineering Workshop (ASEW)","","2015","","","62","69","GUI reference testing is performed to detect regression errors in a modified or patched GUI software. Test cases are executed on the original and modified GUIs, differences in the states of GUI widgets across versions indicate potential defects. However, various factors (e.g., position, flakiness, resolution) create problems for accurate GUI state collection, leading to spurious state mismatches, and hence false positives, these need to be weeded out manually. In this paper, we show that the problem of false positives is significant, often inundating the tester with a large number of false bug reports, requiring a disproportionate amount of manual effort. We develop an entropy-based approach to rank each GUI widget property, and use it to determine whether a state mismatch (indicative of a bug) is real or a false positive. Our empirical evaluation shows that this ranking helps to percolate real bugs to the top of a set of reported bugs, thereby helping to economize tester time.","","978-1-4673-9775","10.1109/ASEW.2015.7","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426638","","Graphical user interfaces;Testing;Software;Computer bugs;Entropy;Computer science;Electronic mail","graphical user interfaces;program testing","GUI reference testing;regression error detection;GUI state collection;entropy-based approach;GUI widget property;graphic user interface","","","22","","","","","","IEEE","IEEE Conferences"
"Maximising axiomatization coverage and minimizing regression testing time","M. Wagner","Optimisation and Logistics Group, School of Computer Science, The University of Adelaide","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","2885","2892","The correctness of program verification systems is of great importance, as they are used to formally prove that safety- and security-critical programs follow their specification. One of the contributing factors to the correctness of the whole verification system is the correctness of the background axiomatization, which captures the semantics of the target program language. We present a framework for the maximization of the proportion of the axiomatization that is used (“covered”) during testing of the verification tool. The diverse set of test cases found not only increases the trust in the verification system, but it can also be used to reduce the time needed for regression testing.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900324","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900324","","Testing;Java;Semantics;Calculus;Optimization;Evolutionary computation;Complexity theory","formal specification;minimisation;program verification;regression analysis;safety-critical software;statistical testing","axiomatization coverage maximisation;regression testing time minimization;program verification system correctness;security-critical programs;safety-critical programs;background axiomatization;program language;verification tool;verification system","","1","19","","","","","","IEEE","IEEE Conferences"
"Combining technical trading rules using parallel particle swarm optimization based on Hadoop","F. Wang; P. L. H. Yu; D. W. Cheung","Department of Computer Science, The University of Hong Kong, Pokfulam, Hong Kong; Department of Statistics and Actuarial Science, The University of Hong Kong, Pokfulam Road, Hong Kong; Department of Computer Science, The University of Hong Kong, Pokfulam, Hong Kong","2014 International Joint Conference on Neural Networks (IJCNN)","","2014","","","3987","3994","Technical trading rules have been utilized in the stock markets to make profit for more than a century. However, no single trading rule can ever be expected to predict the stock price trend accurately. In fact, many investors and fund managers make trading decisions by combining a bunch of technical indicators. In this paper, we consider the complex stock trading strategy, called Performance-based Reward Strategy (PRS), proposed by [1]. Instead of combining two classes of technical trading rules, we expand the scope to combine the seven most popular classes of trading rules in financial markets, resulting in a total of 1059 component trading rules. Each component rule is assigned a starting weight and a reward/penalty mechanism based on rules' recent profit is proposed to update their weights over time. To determine the best parameter values of PRS, we employ an improved time variant particle swarm optimization (TVPSO) algorithm with the objective of maximizing the annual net profit generated by PRS. Due to a large number of component rules and swarm size, the optimization time is significant. A parallel PSO based on Hadoop, an open source parallel programming model of MapReduce, is employed to optimize PRS more efficiently. The experimental results show that PRS outperforms all of the component rules in the testing period.","2161-4407;2161-4393","978-1-4799-1484-5978-1-4799-6627","10.1109/IJCNN.2014.6889599","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6889599","","Optimization;Testing;Particle swarm optimization;Radio frequency;Training;Equations;Mathematical model","decision making;parallel programming;particle swarm optimisation;pricing;profitability;public domain software;stock markets","technical trading rules;Hadoop;stock market;profit;stock price prediction;trading decision making;stock trading strategy;performance-based reward strategy;financial market;component rule;weight assignment;reward mechanism;time variant particle swarm optimization;TVPSO algorithm;parallel PSO;open source parallel programming model;MapReduce;PRS optimization;penalty mechanism","","4","22","","","","","","IEEE","IEEE Conferences"
"Dynamic battery charging voltage optimization for the longer battery runtime and lifespan","T. Imai; H. Yamaguchi","User Experience Software Development, Lenovo Japan; Platform Technology, Lenovo Japan","2013 IEEE 2nd Global Conference on Consumer Electronics (GCCE)","","2013","","","219","223","When a battery is kept to be fully charged, battery lifespan is reduced due to storage deterioration. When a battery is often charged and discharged, battery lifespan is also reduced due to cycle deterioration. The Dual Mode battery introduced in this paper provides Runtime mode and Lifespan mode for laptop PCs to balance the battery runtime and lifespan. A battery is charged by a higher charging voltage in Runtime mode to get the longer battery runtime, while the battery is charged by a lower charging voltage in Lifespan mode to get the longer battery lifespan. This paper discusses automatic mode switching between Runtime mode and Lifespan mode by monitoring battery usage, and proposes the algorithm to get both the long battery runtime and the longer battery lifespan. This paper also shows that how battery lifespan is increased by the proposed algorithm based on the user tests conducted over 1 year.","2378-8143","978-1-4799-0892-9978-1-4799-0890","10.1109/GCCE.2013.6664804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664804","battery;battery lifespan;battery runtime;Dual Mode Battery;Runtime mode;Lifespan mode;Relative State of Charge","Batteries;Runtime;Switches;Discharges (electric);Portable computers;Monitoring;Software algorithms","optimisation;secondary cells","dynamic battery charging voltage optimization;battery runtime mode;battery lifespan mode;storage deterioration;cycle deterioration;dual mode battery;laptop PC;automatic mode switching;battery usage monitoring","","1","6","","","","","","IEEE","IEEE Conferences"
"A scan segmentation architecture for power controllability and reduction","Z. Jiang; D. Xiang; K. Shen","Tsinghua National Laboratory for Information Science and Technology, School of Software, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology, School of Software, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology, Dept. of Computer Science and Tech., Tsinghua University, Beijing 100084, China","2015 28th IEEE International System-on-Chip Conference (SOCC)","","2015","","","269","274","With the chip size entering the micro-nano level, the increasing power consumption during the chip testing process becomes the bottleneck of chip production and testing. Prior work has been mainly focused on reducing power dissipation in either shift cycle or capture cycle, however, there has been limited work on reducing the peak power in both shift and capture cycles at the same time. Moreover, there has been no work on the problem of capture power controllability. This paper proposes a new power-aware scan segment architecture, which can accurately control the power of shift and capture cycles at the same time with small area overhead. Meanwhile, we devise sophisticated algorithms of dependency checking and scan segments partitioning, which can directly reduce simultaneously switching activity of flip-flops by iterative optimizing scan segments grouping. To the best of our knowledge, this paper is the first of its kind to study the problem of power controllability considering both structure dependency and clock trees' impact. Extensive experiments have been performed on reference circuit ISCAS89 and IWLS2005 to verify the effectiveness of the proposed architecture.","2164-1706","978-1-4673-9094-1978-1-4673-9093","10.1109/SOCC.2015.7406961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406961","scan testing;scan segmentation;power-aware testing;controllable capture cycle;design for testability","Clocks;Switches;Registers;Automatic test pattern generation;Computer architecture;Controllability","clocks;integrated circuit testing;low-power electronics;power control;trees (mathematics)","power controllability;power reduction;chip testing;chip production;power dissipation;power-aware scan segment architecture;flip-flops;iterative optimizing scan segments grouping;clock trees;ISCAS89;IWLS2005","","1","16","","","","","","IEEE","IEEE Conferences"
"Genetic programming for Reverse Engineering","M. Harman; W. B. Langdon; W. Weimer","University College London, CREST centre, UK; University College London, CREST centre, UK; University of Virginia, Virginia, USA","2013 20th Working Conference on Reverse Engineering (WCRE)","","2013","","","1","10","This paper overviews the application of Search Based Software Engineering (SBSE) to reverse engineering with a particular emphasis on the growing importance of recent developments in genetic programming and genetic improvement for reverse engineering. This includes work on SBSE for remodularisation, refactoring, regression testing, syntax-preserving slicing and dependence analysis, concept assignment and feature location, bug fixing, and code migration. We also explore the possibilities for new directions in research using GP and GI for partial evaluation, amorphous slicing, and product lines, with a particular focus on code transplantation. This paper accompanies the keynote given by Mark Harman at the 20<sup>th</sup> Working Conference on Reverse Engineering (WCRE 2013).","1095-1350;2375-5369","978-1-4799-2931","10.1109/WCRE.2013.6671274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671274","","Reverse engineering;Software;Optimization;Genetic programming;Search problems;Testing;Measurement","genetic algorithms;product development;program debugging;program slicing;program testing;regression analysis;reverse engineering;software engineering;software maintenance;software reusability","search based software engineering;reverse engineering;genetic programming;genetic improvement;SBSE;remodularisation;refactoring;regression testing;syntax-preserving slicing;dependence analysis;concept assignment;feature location;bug fixing;code migration;GP;GI;partial evaluation;amorphous slicing;product lines;code transplantation","","10","93","","","","","","IEEE","IEEE Conferences"
"Genetic algorithm based distribution network expansion planning","S. Kilyeni; C. Barbulescu; A. Simo; R. Teslovan; C. Oros","Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania; Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania; Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania; Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania; Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania","2014 49th International Universities Power Engineering Conference (UPEC)","","2014","","","1","6","The methods belonging to the artificial intelligence field are becoming very popular within the power system domain. Within the literature several applications are able to be highlighted: load forecasting, power flow computing, power flow optimization, network expansion. The paper is focusing on developing a software tool based on genetic algorithms. The goal of the paper is to use it for distribution network expansion planning. For the moment, the case studies are represented by small scale test power system, but the work is heading towards real, complex distribution networks.","","978-1-4799-6557-1978-1-4799-6556","10.1109/UPEC.2014.6934812","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6934812","distribution network;mathematical model;expansion planning;genetic algorithm","Planning;Load flow;Genetic algorithms;Software tools;Optimization;Biological cells","genetic algorithms;load flow;load forecasting;power distribution planning;power engineering computing","genetic algorithm;distribution network expansion planning;artificial intelligence field;power system domain;load forecasting;power flow computing;power flow optimization","","","12","","","","","","IEEE","IEEE Conferences"
"Concept localization using n-gram Information Retrieval model and Control Flow Graph","N. Jain; R. Garg; I. Chawla","Department of Computer Science, Jaypee Institute of Information Technology, Noida, Uttar Pradesh, India; Department of Computer Science, Jaypee Institute of Information Technology, Noida, Uttar Pradesh, India; Department of Computer Science, Jaypee Institute of Information Technology, Noida, Uttar Pradesh, India","Confluence 2013: The Next Generation Information Technology Summit (4th International Conference)","","2013","","","29","34","Developing software involves many phases such as designing, coding and testing. Once the software is released, a separate team is responsible for maintaining the software. Nowadays many researchers and users work on Open Source Software to enhance its functionalities and to mould it according to their needs. Most of the time, a user or developer wants to locate a specific feature in software for the purpose of enhancement or removing a fault, which is known as concept localization. Automatic concept localization gives relevant files to the users as per the requirement. We have implemented n-gram, an Information Retrieval model to retrieve the names of the relevant files from the source code and incorporated Control Flow Graph (CFG) which helped us to determine the files encapsulating the functionality, in the correct order. We conducted tests on numerous grounds such as different threshold values (0.4, 0.6 and 0.8), N value (2 and 3) and varying query length. On examination, we obtained recall of 74% and precision of 65% on threshold value of 0.6 using trigram (i.e. n=3). Control Flow Graph significantly contributed in improving the ranking of relevant files.","","978-1-84919-846","10.1049/cp.2013.2289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832304","Concept Localization;N-Gram;CFG;Tf-Id;Ranking documents;Precision;Recall","","graph theory;information retrieval;public domain software;software engineering","-gram information retrieval model;control flow graph;open source software;automatic concept localization;CFG;software developmemt","","","","","","","","","IET","IET Conferences"
"AURORA: AUtomatic RObustness coveRage Analysis Tool","A. Gargantini; M. Guarnieri; E. Magri","NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation","","2013","","","463","470","Code coverage is usually used as a measurement of testing quality and as adequacy criterion. Unfortunately, code coverage is very sensitive to modifications of the code structure, and, therefore, we can achieve the same degree of coverage with different testing effort by writing the same program in syntactically different ways. For this reason, code coverage can provide the tester with misleading information. In order to understand how a testing criterion is affected by code structure modifications, we have introduced a way to measure the sensitivity of coverage to code changes by means of code-to-code transformations. However the manual execution of the robustness analysis is tedious, time consuming and error prone. In order to solve these issues we present AURORA, a tool that automates the robustness analysis process and leverages the capabilities offered from several existing tools. AURORA has an extendible architecture that concretely supports the tester in the execution of the robustness analysis. Due to this extendible architecture, each user can personalize the robustness analysis to his/her needs. AURORA allows the user to add new transformations by using TXL, which is a programming language specifically designed to support source transformation tasks. It performs the coverage evaluation by using existing code coverage tools and is based on the use of the JUnit framework.","2159-4848","978-0-7695-4968-2978-1-4673-5961","10.1109/ICST.2013.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569761","Code Coverage; Testing Criteria; Code Transformations; Coverage Robustness","Testing;Robustness;Indexes;Grammar;Optimized production technology;Sensitivity;Computer architecture","program testing;systems analysis","AURORA;automatic robustness coverage analysis tool;code coverage;testing quality;adequacy criterion;code structure modifications;code-to-code transformations;TXL;JUnit framework","","","23","","","","","","IEEE","IEEE Conferences"
"Poster: An Efficient Equivalence Checking Method for Petri Net Based Models of Programs","S. Bandyopadhyay; D. Sarkar; C. Mandal","NA; NA; NA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","2","","827","828","The initial behavioural specification of any software programs goes through significant optimizing and parallelizing transformations, automated and also human guided, before being mapped to an architecture. Establishing validity of these transformations is crucial to ensure that they preserve the original behaviour. PRES+ model (Petri net based Representation of Embedded Systems) encompassing data processing is used to model parallel behaviours. Being value based with inherent scope of capturing parallelism, PRES+ models depict such data dependencies more directly; accordingly, they are likely to be more convenient as the intermediate representations (IRs) of both the source and the transformed codes for translation validation than strictly sequential variable-based IRs like Finite State Machines with Data path (FSMDs) (which are essentially sequential control flow graphs (CFGs)). In this work, a path based equivalence checking method for PRES+ models is presented.","0270-5257;1558-1225","978-1-4799-1934","10.1109/ICSE.2015.268","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7203089","Equivalence checking;PRES+ model;FSMD model","Computational modeling;Petri nets;Data models;Sparks;Benchmark testing;Embedded systems","embedded systems;formal specification;parallel programming;Petri nets;program interpreters;program verification;software architecture;source code (software)","Petri net based models;behavioural specification;software programs;optimizing transformations;parallelizing transformations;architecture;PRES+ model;Petri net based representation of embedded systems;data processing;parallel behaviours;parallelism;data dependencies;intermediate representations;source codes;transformed codes;translation validation;sequential variable-based IR;finite state machines with data path;FSMD;sequential control flow graphs;CFG;path based equivalence checking method","","4","7","","","","","","IEEE","IEEE Conferences"
"Identifying usability risk: A survey study","J. T. Sambantha Moorthy; S. bin Ibrahim; M. N. Mahrin","Advanced Informatics School, Universiti Teknologi of Malaysia (UTM), Kuala Lumpur, Malaysia; Advanced Informatics School, Universiti Teknologi of Malaysia (UTM), Kuala Lumpur, Malaysia; Advanced Informatics School, Universiti Teknologi of Malaysia (UTM), Kuala Lumpur, Malaysia","2014 8th. Malaysian Software Engineering Conference (MySEC)","","2014","","","148","153","As defined in various quality models, usability is recognized as an important attribute of software quality. Failing to address usability requirements in a software product could lead to poor quality and high usability problems in software product. Research is still in progress to introduce the best methods for reducing usability problems and increase the rate of successful usable software products. Studies have shown that problems in software products can also be controlled using Software Risk Management methods, even though these problems cannot be eliminated totally. Using Software Risk Management, problems in software products are dealt before it occurs. This paper presents usability problems as a risk factor and by managing usability risk at earlier phases of the development process, successful and high usability software products can be developed. Unfortunately, currently there is little effort in identifying, analyzing and prioritizing potential usability risks at earlier phases of the development process. This paper focuses on usability risk identification as it is the first stage in usability risk management. This paper presents the results of an industry survey based on the opinion of Malaysian Public Sector involving sample size of 330 software developers and software projects managers regarding potential usability risk that could occur during Software Development Life Cycle (SDLC). Our finding has identified 42 potential usability risks, defined as a list for further risk analysis in future.","","978-1-4799-5439","10.1109/MySec.2014.6986005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986005","usability;usability risk;risk management;risk identification","Usability;Risk management;Safety;Testing;Documentation;Maintenance engineering","risk management;software quality;software reusability","software product usability risk identification;software quality;software usability requirements;software risk management methods;software development life cycle;SDLC","","","31","","","","","","IEEE","IEEE Conferences"
"A Study of the Impact of Bit-Flip Errors on Programs Compiled with Different Optimization Levels","B. Sangchoolie; F. Ayatolahi; R. Johansson; J. Karlsson","NA; NA; NA; NA","2014 Tenth European Dependable Computing Conference","","2014","","","146","157","In this paper we study the impact of compiler optimizations on the error sensitivity of twelve benchmark programs. We conducted extensive fault injection experiments where bit-flip errors were injected in instruction set architecture registers and main memory locations. The results show that the percentage of silent data corruptions (SDCs) in the output of the optimized programs is only marginally higher compare to that observed for the non-optimized programs. This suggests that compiler optimizations can be used in safety- and mission-critical systems without increasing the risk that the system produces undetected erroneous outputs. In addition, we investigate to what extent the source code implementation of a program affects the error sensitivity of a program. To this end, we perform experiments with five implementations of a bit count algorithm. In this investigation, we consider the impact of the implementation as well as compiler optimizations. The results of these experiments give valuable insights into how compiler optimizations can be used to reduce error sensitive of registers and main memory sections. They also show how sensitive locations requiring additional protection, e.g., by the use of software-based fault tolerance techniques, can be identified.","","978-1-4799-3804","10.1109/EDCC.2014.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821100","fault injection;error sensitivity;single bit-flips;compiler optimization;soft error","Optimization;Registers;Sensitivity;Benchmark testing;Program processors;Hardware;Reliability","instruction sets;optimisation;program compilers","bit-flip errors;programs compilers;optimization levels;compiler optimizations;error sensitivity;benchmark programs;instruction set architecture;silent data corruptions;SDC;safety- and mission-critical systems;source code implementation;bit count algorithm;software based fault tolerance techniques","","12","19","","","","","","IEEE","IEEE Conferences"
"Comparative analysis of GSM coverage prediction with measurement results for urban areas using statistical nonparametric mapping","H. Lučkin; E. Lučkin; M. Škrbić","BH Telecom, Sarajevo, Bosnia and Herzegovina; BH Telecom, Sarajevo, Bosnia and Herzegovina; University of Sarajevo, Bosnia and Herzegovina","2015 38th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","","2015","","","452","456","Planning, building and optimization process of radio access networks is a complex dynamical activity, which requires a lot of planning effort and time. The final goal is to build a radio network, which consists of minimum infrastructural components, and offers the best coverage and service quality. In order to consider all the possibilities of GSM signal coverage during planning time, and before the installation of equipment on certain location, it is very important to do software simulation of the GSM signal coverage of the observed area. Consequently, it is important to use an optimal predictive model which, using the appropriate digital maps, will offer the most realistic picture of the future radio access network. Taking this into account, subsequent optimization process and re-analysis would be reduced to a minimum. In this paper, comparative analysis of the software prediction GSM coverage area signal with the results obtained by measurements after the implementation of planned radio infrastructure at the proposed base station, will be presented. In order to assess the credibility of the software predicted GSM signal coverage area, measurements of GSM signal in urban area were performed. Final results were verified using statistical nonparametric chi square test. Further, the software prediction relation in regard to the measurement signal after the implementation of system is defined, and potential causes are treated.","","978-9-5323-3082-3978-9-5323-3085","10.1109/MIPRO.2015.7160314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160314","GSM;prediction;measurements;Chi-square test","Area measurement;Software measurement;GSM;Software;Sociology;Statistics;Ground penetrating radar","cellular radio;optimisation;statistical analysis;telecommunication network planning","GSM coverage prediction;urban areas;statistical nonparametric mapping;optimization process;building process;planning process;radio access networks;complex dynamical activity;GSM signal coverage;optimal predictive model;digital maps;radio infrastructure;GSM signal measurements;statistical nonparametric chi square test","","","6","","","","","","IEEE","IEEE Conferences"
"A genetic algorithm for task allocation in collaborative software developmentusing formal concept analysis","S. Chakraverty; A. Sachdeva; A. Singh","Department of Computer Engineering, NetajiSubhas Institute of Technology, New Delhi, India; Department of Computer Engineering, NetajiSubhas Institute of Technology, New Delhi, India; Department of Computer Engineering, NetajiSubhas Institute of Technology, New Delhi, India","International Conference on Recent Advances and Innovations in Engineering (ICRAIE-2014)","","2014","","","1","6","Software development is no longer an isolated or localized task but a collaborative process with well coordinated contributions from personnel across the globe. Such an approach boosts productivity, but also poses challenges that must be met. One of them is to formally analyze the realms of software development tasks and the teams that are commissioned to perform them to derive the full set of conceptual units that describe these domains in terms of the needed proficiencies. Then, the best possible matching between the cohesive task-sets and the inter-coordinating teams must be obtained. In this paper, we present a model for Collaborative Software Development that addresses these issues. We employ Formal Concept Analysis to generate the concept lattices in the domains of tasks and teams in terms of various skills. We employ Genetic Algorithm, a meta-heuristic that stochastically scans the search space in a guided manner to generate the best possible pairings between task concepts and team concepts. Results show that this approach forms cohesive task sets, identifies sets of homogeneous teams and produces optimum task-team mappings that gives high skills utilization and provides a basis for coordinated and reliable operation. The GA yields a range of non-inferior solutions giving wide scope of tradeoff between various objectives.","","978-1-4799-4040-0978-1-4799-4041","10.1109/ICRAIE.2014.6909305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909305","Collaborative Software Development;Genetic Algorithm;Formal Concept Analysis;Pareto Optimal Ranking","Encoding;Testing;Unified modeling language;Software;Welding;Databases;Buildings","formal concept analysis;genetic algorithms;search problems;software development management","genetic algorithm;task allocation;collaborative software development;formal concept analysis;personnel;cohesive task-sets;intercoordinating teams;concept lattices;search space;optimum task-team mappings;GA;noninferior solutions","","1","13","","","","","","IEEE","IEEE Conferences"
"A novel optimization approach for revenue maximization in mobile data pricing","H. Wang; L. Wang; F. Kong; L. Sun; J. Yong","School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; School of Software, Dalian University of Technology, China; Graduate School of Information, Production and System, Waseda University, Japan","2015 IEEE International Conference on Communications (ICC)","","2015","","","6918","6923","With the popularity of network utility, network pricing is becoming an emerging research hotspot. This paper studies the revenue maximization problem based on network utilization optimizing approach. In order to reduce the implemental complexity, we present a SG (Super Group) method whose time complexity is O(1) to regroup users. A precision control variable ε is introduced to control the group size. We then design a distribution related network resource reschedule scheme called RR (Resource Reschedule scheme) to optimize the network utilization. Two important factors, resource threshold and monitoring timeslot, which will affect the dynamic reschedule process were proposed and tested in the simulation experiment. After combining SG method and RR scheme, we make the pricing process faster and more practical. We also prove that our new approach can achieve the same or even more revenue gaining than original usage-based pricing scheme.","1550-3607;1938-1883","978-1-4673-6432-4978-1-4673-6431","10.1109/ICC.2015.7249428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7249428","","Pricing;Bandwidth;Nickel;Monitoring;Servers;Software;Multimedia communication","mobile communication;optimisation;pricing;telecommunication scheduling","optimization approach;revenue maximization problem;mobile data pricing;network pricing;SG method;super group method;precision control variable;resource reschedule scheme;resource threshold;monitoring timeslot;dynamic reschedule process;RR scheme;usage-based pricing scheme","","","21","","","","","","IEEE","IEEE Conferences"
"A novel approach for test case generation from UML activity diagram","A. K. Jena; S. K. Swain; D. P. Mohapatra","School of Computer Engineering, KIIT University, Bhubaneswar, Odisha, India; School of Computer Engineering, KIIT University, Bhubaneswar, Odisha, India; Dept. of Computer Sc. &amp; Engg., National Institute of Technology, Rourkela, Odisha, India","2014 International Conference on Issues and Challenges in Intelligent Computing Techniques (ICICT)","","2014","","","621","629","Software testing approaches are mainly divided into three types i.e. code based testing, specification based testing and model based testing. In model based testing, the testing can be started from the design process at the beginning phase. So, early detection of faults can be achieved by using this approach. An approach for the generation of test cases from UML (Unified Modelling Language) activity diagram using genetic algorithm has been presented in this paper. Early detection of faults can be achieved by this approach and will certainly reduce the time, cost and effort of the developer to a large extent. We propose a model to generate an Activity Flow Table (AFT) from an Activity Diagram and then convert it to Activity Flow Graph (AFG). Coverage criteria are very important in test case generation. By using activity coverage criterion we traverse the AFG and the test paths are generated. Finally, we generate the test cases from these paths. Genetic Algorithm has been applied to generate test cases and also to optimise them. The model is implemented on a case study of ATM Withdrawal system.","","978-1-4799-2900-9978-1-4799-2899","10.1109/ICICICT.2014.6781352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6781352","Activity Diagram;Test Case Generation;Genetic Algorithm","Unified modeling language;Online banking","flow graphs;genetic algorithms;program testing;Unified Modeling Language","test case generation;UML activity diagram;software testing approaches;code based testing;specification based testing;model based testing;Unified Modelling Language;genetic algorithm;activity flow table;AFT;activity diagram;activity flow graph;AFG;coverage criteria;ATM withdrawal system","","7","29","","","","","","IEEE","IEEE Conferences"
"Optimized hardware for suboptimal software: The case for SIMD-aware benchmarks","J. M. Cebrián; M. Jahre; L. Natvig","Dept. of Computer and Information Science (IDI), NTNU Trondheim, NO-7491, Norway; Dept. of Computer and Information Science (IDI), NTNU Trondheim, NO-7491, Norway; Dept. of Computer and Information Science (IDI), NTNU Trondheim, NO-7491, Norway","2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)","","2014","","","66","75","Evaluation of new architectural proposals against real applications is a necessary step in academic research. However, providing benchmarks that keep up with new architectural changes has become a real challenge. If benchmarks don't cover the most common architectural features, architects may end up under/over estimating the impact of their contributions. In this work, we extend the PARSEC benchmark suite with SIMD capabilities to provide an enhanced evaluation framework for new academic/industry proposals. We then perform a detailed energy and performance evaluation of this commonly used application set on different platforms (Intel<sup>®</sup>and ARM<sup>®</sup>processors). Our results show how SIMD code alters scalability, energy efficiency and hardware requirements. Performance and energy efficiency improvements depend greatly on the fraction of code that we can actually vectorize (up to 50×). Our enhancements are based in a custom built wrapper library compatible with SSE, AVX and NEON to facilitate general vectorization. We aim to distribute the source code to reinforce the evaluation process of new proposals for computing systems.","","978-1-4799-3606-9978-1-4799-3604","10.1109/ISPASS.2014.6844462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844462","Performance Analysis;Benchmarking;Energy Efficiency;MSRs;Vectorization;SIMD","Benchmark testing;Program processors;Libraries;Registers;Hardware;Runtime;Bridges","benchmark testing;energy conservation;microprocessor chips;multi-threading;power aware computing;reduced instruction set computing","optimized hardware;suboptimal software;SIMD-aware benchmarks;PARSEC benchmark suite;performance evaluation;energy evaluation;SIMD code;energy efficiency;hardware requirements;code vectorization;custom built wrapper library;SSE;AVX;NEON;general vectorization;source code;single instruction multiple data","","7","24","","","","","","IEEE","IEEE Conferences"
"The resource optimization of heterogeneous network interfaces in wireless mobile devices","D. Ban; J. In; H. Woo","Intelligence Platform Lab., Software R&amp;D Center, Samsung Electronics, Suwon, South Korea; Intelligence Platform Lab., Software R&amp;D Center, Samsung Electronics, Suwon, South Korea; Intelligence Platform Lab., Software R&amp;D Center, Samsung Electronics, Suwon, South Korea","2014 IEEE International Conference on Communications (ICC)","","2014","","","1235","1241","Although using high-quality streaming services becomes more common on mobile devices, the increment of streaming quality, which generally requires the larger playback bandwidth, frequently incurs undesirable delays or jitters that hamper user experiences. Recent mobile devices usually equip multiple network interfaces and their concurrent utilization can be beneficial to retain the necessary bandwidth. This, however, requires the more energy consumption of mobile devices whose resource is limited due to battery-based operations. For this reason, streaming performance and energy consumption should not be independently considered. Under multiple network interface utilization, we investigate the characteristics of each component, based on two-level convex optimization, and propose a solution to optimize them together. The algorithm is tested on Android phone where LTE and Wi-Fi networks are concurrently enabled. Compared to LTE only method, it shows that the proposed approach can obtain 1.8 times good-put increment only with the half amount of energy consumption. Our approach can be utilized for almost all mobile devices without server-side modifications.","1550-3607;1938-1883","978-1-4799-2003","10.1109/ICC.2014.6883490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6883490","Access Network;Multiple Network Interfaces;Convex Optimization;Streaming;Energy;Good-put","Network interfaces;Power demand;Mobile handsets;Optimization;Bandwidth;Resource management;Quality of service","Android (operating system);convex programming;Long Term Evolution;media streaming;wireless LAN","resource optimization;heterogeneous network interfaces;wireless mobile devices;streaming quality;streaming performance;energy consumption;two-level convex optimization;Android phone;Long Term Evolution;LTE;Wi-Fi networks","","","29","","","","","","IEEE","IEEE Conferences"
"Multi up gradation model under distributed environment","P. K. Kapur; A. K. Shrivastava; A. Kumar; B. D. Shivhare","Center for Interdisciplinary Research, Amity University Sector-125 Noida U.P, India; Department of Operational Research, University of Delhi, Delhi, India; Department of Computer Science, U.P Technical University, India; Department of Information Technology, Amity University, Greater Noida UP, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","6","In the prevailing scenario, development of distributed systems is gaining popularity to get the benefits of early and enhanced developments. Software Reliability Growth Models (SRGMs) play a significant role in quantitative assessment of software reliability. Software's developed in Distributed Development Environment (DDE) are characterized by enhanced availability and increased reliability. Software with their components developed in distributed environment show an astounded trend as they have high speed and elevated reliability performance. In this study we present multi-up gradations modeling for removal of fault for newly developed and reused components in distributed environment using probability distribution functions. Explicit expressions for the mean number of individual types of faults are obtained by considering `n' reused components and `p' newly developed components. And for such systems, we have used some probability distribution functions for finding best method to capture the faults lying dormant in the system and likewise have selected accurate SRGM for distributed environment. Due to uncertain market forces and stout contention, the developers cannot satisfy the customers with software having static and limited functionalities. Customers generally expect periodic modifications and add-ons in the software to maintain their inclination towards the same product. To keep a pace with these fast changing preferences of the customers, distributed development environment with multiple periodic up-gradations is the need of hour, hence turning the customer's urge to developer's obligation. We are introducing a modeling framework of a software which is developed under distributed environment and upgraded time to time. To validate the analytical results of the proposed framework, numerical illustration is provided.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014699","Muti- Up gradations;Distributed Environment;Non Homogeneous Poisson Process (NHPP);Software Reliability;Release;Probability Density Function","Software;Testing;Software reliability;Distribution functions;Mathematical model;Fault detection","distributed processing;object-oriented programming;software fault tolerance;statistical distributions","multi up gradation model;distributed environment;distributed system;software reliability growth model;SRGM;quantitative assessment;distributed development environment;DDE;reliability performance;fault removal;probability distribution function;market forces;stout contention;periodic up-gradation","","1","20","","","","","","IEEE","IEEE Conferences"
"Voltage regulation optimization in low voltage network based on Voltage Quality Index","M. Kopicka; J. Drapela; D. Topolanek","Brno University of Technology, Faculty of Electrical Engineering and Communication, Czech Republic; Brno University of Technology, Faculty of Electrical Engineering and Communication, Czech Republic; Brno University of Technology, Faculty of Electrical Engineering and Communication, Czech Republic","Proceedings of the 2014 15th International Scientific Conference on Electric Power Engineering (EPE)","","2014","","","241","246","The paper deals with an optimization procedure in terms of a network voltage parameters improvement. A software based approach is used to select optimal type of voltage regulator, its power and ideal location for application in a Low-Voltage (LV) test network, where it is required to enhance Voltage Quality (VQ). The test network presents a case where application of on-load tap changer transformer instead of conventional distribution transformer MV/LV is not adequate to maintain voltage profile in the LV system in limits. Therefore other type of voltage regulator has to be employed to increase VQ in the entire LV network. Principal measure for VQ evaluation there is adopted a Voltage Quality Index expressing the VQ level in dependence on selected and specified input parameters, which may be voltage magnitude, unbalance, harmonic distortion, flicker level and frequency. Within optimization process, maximal current carrying capacity of lines, network losses and cost of solutions are also taken into account.","","978-1-4799-3807","10.1109/EPE.2014.6839541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6839541","Voltage Quality;Voltage Quality Index;PSCAD;voltage regulator;voltage profile;optimization","Voltage control;Regulators;Optimization;Load modeling;Density estimation robust algorithm;PSCAD;Indexes","optimisation;power engineering computing;power supply quality;voltage regulators","voltage regulation optimization;voltage quality index;network voltage parameters improvement;optimization procedure;software based approach;low-voltage test network;LV system;VQ level;voltage profile;voltage magnitude;harmonic distortion;flicker level;network losses;cost of solutions;voltage regulator","","3","9","","","","","","IEEE","IEEE Conferences"
"Performance testing framework: Evaluating the impact on the system speed","P. Bot; C. Vatamanu; D. Gavrilut; R. Benchea","Bitdefender Antimalware Laboratory Iasi, Romania; Bitdefender Antimalware Laboratory Iasi, Romania; Bitdefender Antimalware Laboratory Iasi, Romania; Bitdefender Antimalware Laboratory Iasi, Romania","2014 Second Workshop on Anti-malware Testing Research (WATeR)","","2014","","","1","6","The world we live in now is defined by the word “speed” and any device, technology, or system that doesn't keep up is rejected or replaced immediately. Because of this, one of the biggest concerns today is “optimization”. Its purpose is to reduce the impact on the user's device. The Anti-Virus industry is also confronting with this challenge. Although the first concern is to keep the user safe, providing a flawless protection, it is crucial to reduce the impact brought on the user's system, preventing him to disable or uninstall the AV solution and thus remaining unprotected. The increased number of malware types/families as well as their complexity generated the need for complicated detection methods, which means a constant evaluation is needed. Because of these reasons, our antimalware laboratory has developed a generic framework for measuring the impact that the AV solutions have on the system they are installed on. This system was designed to be easily configurable, managing the big number of changes that occur every day and fast so that every update released to the users can be tested. Also, this framework is used to test and develop new technologies that improve the performance of our AV product.","","978-1-4799-6070","10.1109/WATeR.2014.7015753","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7015753","performance;generic framework;impact;user interaction","Servers;Databases;Gold;Computers;Laboratories;Operating systems","data protection;DP industry;invasive software;program testing;software performance evaluation","performance testing framework;system speed impact;antivirus industry;AV solution;malware type;flawless protection","","","9","","","","","","IEEE","IEEE Conferences"
"Study of guided wave transmission through complex junction in sodium cooled reactor","Q. Elie; F. Le Bourdais; K. Jezzine; V. Baronian","Non Destructive Testing Department at the French Atomic Energy Commission (CEA), Saclay, 91191 Gif sur Yvette CEDEX, France; Non Destructive Testing Department at the French Atomic Energy Commission (CEA), Saclay, 91191 Gif sur Yvette CEDEX, France; Non Destructive Testing Department at the French Atomic Energy Commission (CEA), Saclay, 91191 Gif sur Yvette CEDEX, France; Non Destructive Testing Department at the French Atomic Energy Commission (CEA), Saclay, 91191 Gif sur Yvette CEDEX, France","2015 4th International Conference on Advancements in Nuclear Instrumentation Measurement Methods and their Applications (ANIMMA)","","2015","","","1","5","Ultrasonic guided wave techniques are seen as suitable candidates for the inspection of welded structures within sodium cooled fast reactors (SFR), as the long range propagation of guided waves without amplitude attenuation can overcome the accessibility problem due to the liquid sodium. In the context of the development of the Advanced Sodium Test Reactor for Industrial Demonstration (ASTRID), the French Atomic Commission (CEA) investigates non-destructive testing techniques based on guided wave propagation. In this work, guided wave NDT methods are applied to control the integrity of welds located in a junction-type structure welded to the main vessel. The method presented in this paper is based on the analysis of scattering matrices peculiar to each expected defect, and takes advantage of the multi-modal and dispersive characteristics of guided wave generation. In a simulation study, an algorithm developed using the CIVA software is presented. It permits selecting appropriate incident modes to optimize detection and identification of expected flawed configurations. In the second part of this paper, experimental results corresponding to a first validation step of the simulation results are presented. The goal of the experiments is to estimate the effectiveness of the incident mode selection in plates. The results show good agreement between experience and simulation.","","978-1-4799-9918-7978-1-4799-9917","10.1109/ANIMMA.2015.7465511","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7465511","","Inspection;Junctions;Transmission line matrix methods;Welding;Scattering;Solid modeling;Inductors","fission reactor cooling;liquid metal fast breeder reactors;nondestructive testing;ultrasonic waves","complex junction;sodium cooled fast reactors;ultrasonic guided wave techniques;guided wave transmission;welded structures;SFR;Advanced Sodium Test Reactor for Industrial Demonstration;ASTRID;French Atomic Commission;CEA;nondestructive testing technique;junction-type structure;scattering matrices;guided wave generation;CIVA software;flawed configuration","","2","6","","","","","","IEEE","IEEE Conferences"
"Testability of object-oriented systems: An AHP-based approach for prioritization of metrics","P. Khanna","Siemens Technology and Services, Private Limited, CT DC AA E P-IE BDG3, IFFCO Tower, Plot No. 3, Sector 29, Gurgaon 122001, India","2014 International Conference on Contemporary Computing and Informatics (IC3I)","","2014","","","273","281","This paper investigates testability from the perspective of metrics used in an object-oriented system. The idea is to give an overview of object oriented design metrics with the prioritization of same keeping testability as the overall goal. We have used Analytic Hierarchy Process (AHP) method to attain which metric is mostly used and is best for testability.","","978-1-4799-6629","10.1109/IC3I.2014.7019595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019595","AHP;testability;CK metrics suite;MOOD Metrics Model","Measurement;Testing;Software;Complexity theory;Encapsulation;Couplings;Analytic hierarchy process","analytic hierarchy process;object-oriented programming;program testing;software metrics","testability;metrics prioritization;analytic hierarchy process;AHP-based approach;object-oriented systems","","","17","","","","","","IEEE","IEEE Conferences"
"Stress analysis and structure optimization of copper cylinders based on 3D packaging","W. Jiang; L. Wang","Harbin University of Science and Technology, 405# material college in HUST No.4 Linyuan Road, XiangFang Zone, China; Harbin University of Science and Technology, 405# material college in HUST No.4 Linyuan Road, XiangFang Zone, China","2015 16th International Conference on Electronic Packaging Technology (ICEPT)","","2015","","","226","230","In this paper, the thermal cyclic loading was adopted to analyze TSV copper cylinders stress distribution by using simulation software ANSYS, a cycle is 29min, the temperature range from-55 to 125°C. In order to study the stress of copper cylinders in TSV, at the first a set of suitable parameters were used to simulate the stress, then the structure parameters were optimized in this paper, and take copper cylinders maximum von-mises stress as response, by using MINITAB software and Taguchi orthogonal test method to analyse the influence of the copper cylinders diameter, copper cylinders height, copper cylinders pitch on the stress of copper cylinders in TSV. The simulation based on a set of suitable parameters, the results shown that the maximum von-mises stress was located at the outer corner of the copper cylinders in contact with the bottom position. Orthogonal test method was used, which found that the copper cylinders diameter had a big influence on maximum von-mises stress of copper cylinders, copper cylinders height followed, copper cylinders pitch minimal impact. By average of main effects plot found that within the parameters selected range, maximum von-mises follows a non-monotonic trend for TSV diameter and TSV height and also found that with the increase of the TSV pitch the stress between the outer corner and the bottom of the copper cylinders decrease gradually.","","978-1-4673-7999-1978-1-4673-7998","10.1109/ICEPT.2015.7236581","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7236581","simulation;thermal cyclic loading;copper cylinders;parameters;Taguchi orthogonal test","Finite element analysis;Substrates;Packaging","integrated circuit packaging;stress analysis;three-dimensional integrated circuits","stress analysis;structure optimization;3D packaging;thermal cyclic loading;TSV copper cylinders stress distribution;maximum von-mises stress;Taguchi orthogonal test method;MINITAB software","","","9","","","","","","IEEE","IEEE Conferences"
"Measuring Malaysian M-commerce user behaviour","S. S. Tzuaan; A. Sivaji; L. T. Yong; M. H. T. Zanegenh; L. Shan","Software Testing Laboratory, Product Quality, Reliability Engineering, MIMOS Berhad, Technology Park Malaysia, 57000, Kuala Lumpur, Malaysia; Software Testing Laboratory, Product Quality, Reliability Engineering, MIMOS Berhad, Technology Park Malaysia, 57000, Kuala Lumpur, Malaysia; Faculty of Computing and Informatics, Multimedia University, Persiaran Multimedia, 63100, Cyberjaya, Selangor, Malaysia; Faculty of Computing and Informatics, Multimedia University, Persiaran Multimedia, 63100, Cyberjaya, Selangor, Malaysia; Faculty of Computing and Informatics, Multimedia University, Persiaran Multimedia, 63100, Cyberjaya, Selangor, Malaysia","2014 International Conference on Computer and Information Sciences (ICCOINS)","","2014","","","1","6","M-Commerce is expected to play an even larger role, with over half a billion customers following the trend to shop via mobile devices by 2016. According to the Malaysian Ministry of Tourism and Culture, tourism has been identified as one of the twelve National Key Economic Area to drive the country's growth. This gave rise to this research objective which is to determine the attention ranking of Malaysians while booking a hotel using a mobile devise. Lab Based Usability Testing with eye tracking technology was carried out with Malaysians to achieve this objective. Results revealed that Malaysian are highly attracted to both the “booking” and “price” information, followed by the “ratings”, “introduction” and lastly the “title” of the hotel, which is in contrast to other countries.","","978-1-4799-4390-6978-1-4799-4391-3978-1-4799-4392","10.1109/ICCOINS.2014.6868405","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868405","M-Commerce;Tourism;Usability Testing;Power Distance","Heating;Usability;Mobile communication;Testing;Market research;Decision making;Pricing","behavioural sciences computing;gaze tracking;human factors;mobile commerce;mobile handsets;travel industry","Malaysian m-commerce user behaviour measurement;mobile devices;Malaysian Ministry of Tourism and Culture;National Key Economic Area;country growth;hotel booking;mobile device;lab based usability testing;eye tracking technology;booking information;price information;hotel title;hotel introduction;hotel ratings","","3","27","","","","","","IEEE","IEEE Conferences"
"Design and Test of Adaptive Computing Fabrics for Scalable and High-Efficiency Cognitive SoC Applications","P. Nsame; G. Bois; Y. Savaria","NA; NA; NA","2014 IEEE 23rd North Atlantic Test Workshop","","2014","","","48","51","In this paper, a new adaptive computing fabric (ACF) that achieves both real-time multi-mode/multi-rate adaptation and lower error floor for cognitive SoC applications is presented. The VLSI architecture of the ACF is experimentally shown to meet the DVB, 802.3an and 802.ad target specifications. Our design delivers a 10-14 bit error rate (BER) with a bit energyto- noise density of Eb/N0=5dB with an energy-efficiency of 0.61pJ/bit. Experiments are conducted comparing Low-Density Parity-Check (LDPC) codes error correction performance in the presence of unreliable circuits due to aggressive manufacturing defect rates and/or run-time defect rates from components enabled by SoC integration. We report on a 201.6Gbps 65nm CMOS design and Xilinx FPGA prototype, which demonstrates in hardware how real-time adaptive techniques can accelerate decoding convergence and lower the error floor. Finally, We show experimentally that our ACF design can achieve energyefficiency throughput speed-ups at scale in the range of 200x to 5000x as compared to the same algorithm running in software (optimized C program) on a single CPU core.","","978-1-4799-5135-2978-1-4799-5134","10.1109/NATW.2014.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6875448","Cognitive SoC; Adaptive Computing Fabric; Reliable Communication;Embedded Test Generation","Parity check codes;Decoding;Throughput;Digital video broadcasting;Energy efficiency;Computer architecture;Fabrics","adaptive codes;adaptive decoding;CMOS logic circuits;cognitive systems;error correction codes;error statistics;field programmable gate arrays;integrated circuit design;integrated circuit noise;integrated circuit testing;parity check codes;system-on-chip;VLSI","adaptive computing fabrics testing;ACF testing;real-time multimode-multirate adaptation;lower error floor;cognitive SoC application;VLSI architecture;DVB standard;IEEE 802.3an standard;IEEE 802.ad standard;bit error rate;BER;bit energy-to-noise density;energy-efficiency;error correction performance;low-density parity-check code;LDPC code;CMOS ACF design;Xilinx ACF FPGA prototype;decoding convergence;optimized C program;single CPU core;bit rate 201.6 Gbit/s;size 65 nm","","","10","","","","","","IEEE","IEEE Conferences"
"Towards testing variability intensive systems using user reviews","J. L. Rodas; D. Méndez-Acuña; J. A. Galindo; D. Benavides; J. Cárdenas","University of Milagro, Ecuador; INRIA - Rennes, France; INRIA - Rennes, France; University of Seville, Spain; University of Milagro, Ecuador","2015 10th Computing Colombian Conference (10CCC)","","2015","","","39","46","Variability intensive systems are software systems that describe a large set of diverse and different configurations that share some characteristics. This high number of configurations makes testing such systems an expensive and error-prone task. For example, in the Android ecosystem we can find up to 24 different valid configurations, thus, making it impossible to test an application on all of them. To alleviate this problem, previous research suggest the selection of a subset of test cases that maximize the changes of finding errors while maximizing the diversity of configurations. Concretely, the proposals focus on the prioritization and selection of tests, so only relevant configurations are tested according to some criterion. In this paper, we envision the use of user reports to prioritize and select meaningful tests. To do this, we explore the use of recommender systems as a possible improvement to the selection of test cases in intensive variability systems.","","978-1-4673-9464-2978-1-4673-9463","10.1109/ColumbianCC.2015.7333410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333410","Sistemas de alta variabilidd;Modelos de características;Sistemas de recomendación;Android","Androids;Humanoid robots;Testing;Software;Silicon;Electronic mail;Silicon compounds","program testing;recommender systems","variability intensive systems testing;user reviews;software systems;Android ecosystem;recommender systems;test case selection","","","16","","","","","","IEEE","IEEE Conferences"
"Model-based energy optimization of automotive control systems","J. Katoen; T. Noll; H. Wu; T. Santen; D. Seifert","Software Modeling and Verification Group, RWTH Aachen University, Germany; Software Modeling and Verification Group, RWTH Aachen University, Germany; Software Modeling and Verification Group, RWTH Aachen University, Germany; Advanced Technology Labs (ATL) Europe, Microsoft Research, Aachen, Germany; Advanced Technology Labs (ATL) Europe, Microsoft Research, Aachen, Germany","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","761","766","Reducing the energy consumption of controllers in vehicles requires sophisticated regulation mechanisms. Better power management can be enabled by allowing the controller to shut down sensors, actuators or embedded control units in a way that keeps the car safe and comfortable for the user, with the goal of optimizing the (average or maximal) energy consumption. This paper proposes an approach to systematically explore the design space of SW/HW mappings to determine energy-optimal deployments. It employs constraint-solving techniques for generating deployment candidates and probabilistic analyses for computing the expected energy consumption of the respective deployment. The feasibility and scalability of the method is demonstrated by several case studies.","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513608","","Energy consumption;Software;Hardware;Vehicles;Optimization;Space exploration;Probabilistic logic","","","","4","9","","","","","","IEEE","IEEE Conferences"
"Automated generation of state abstraction functions using data invariant inference","P. Tonella; C. D. Nguyen; A. Marchetto; K. Lakhotia; M. Harman","Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; Fondazione Bruno Kessler, Trento, Italy; University College London, UK; University College London, UK","2013 8th International Workshop on Automation of Software Test (AST)","","2013","","","75","81","Model based testing relies on the availability of models that can be defined manually or by means of model inference techniques. To generate models that include meaningful state abstractions, model inference requires a set of abstraction functions as input. However, their specification is difficult and involves substantial manual effort. In this paper, we investigate a technique to automatically infer both the abstraction functions necessary to perform state abstraction and the finite state models based on such abstractions. The proposed approach uses a combination of clustering, invariant inference and genetic algorithms to optimize the abstraction functions along three quality attributes that characterize the resulting models: size, determinism and infeasibility of the admitted behaviors. Preliminary results on a small e-commerce application are extremely encouraging because the automatically produced models include the set of manually defined gold standard models.","","978-1-4673-6161","10.1109/IWAST.2013.6595795","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595795","Model inference;Abstraction functions;Model-based testing;Search-based software engineering","Concrete;Abstracts;Object oriented modeling;Measurement;Testing;Optimization;Genetic algorithms","finite state machines;genetic algorithms;inference mechanisms;pattern clustering","automated generation;state abstraction functions;data invariant inference;model based testing;model inference techniques;finite state models;clustering;genetic algorithms;quality attributes;e-commerce application","","2","16","","","","","","IEEE","IEEE Conferences"
"Optimization of low-efficiency traffic in OpenFlow Software Defined Networks","J. Saldana; F. Pascual; D. de Hoz; J. Fernández-Navajas; J. Ruiz-Mas; D. R. Lopez; D. Florez; J. A. Castell; M. Nuñez","I3A, University of Zaragoza Ada Byron Building, 50018, Zaragoza, Spain; Telefonica I+D Don Ramon de la Cruz 82-84, 28006 Madrid, Spain; I3A, University of Zaragoza Ada Byron Building, 50018, Zaragoza, Spain; I3A, University of Zaragoza Ada Byron Building, 50018, Zaragoza, Spain; I3A, University of Zaragoza Ada Byron Building, 50018, Zaragoza, Spain; Telefonica I+D Don Ramon de la Cruz 82-84, 28006 Madrid, Spain; Telefonica I+D Don Ramon de la Cruz 82-84, 28006 Madrid, Spain; Telefonica I+D Don Ramon de la Cruz 82-84, 28006 Madrid, Spain; Telefonica I+D Don Ramon de la Cruz 82-84, 28006 Madrid, Spain","International Symposium on Performance Evaluation of Computer and Telecommunication Systems (SPECTS 2014)","","2014","","","550","555","This paper proposes a method for optimizing bandwidth usage in Software Defined Networks (SDNs) based on OpenFlow. Flows of small packets presenting a high overhead, as the ones generated by emerging services, can be identified by the SDN controller, in order to remove header fields that are common to any packet in the flow, only during their way through the SDN. At the same time, several packets can be multiplexed together in the same frame, thus reducing the number of sent frames. Four kinds of small-packet traffic flows are considered (VoIP, UDP and TCP-based online games, and ACKs from TCP flows). Both IPv4 and IPv6 are tested, and significant bandwidth savings (up to 68 % for IPv4 and 78 % for IPv6) can be obtained for the considered kinds of traffic.","","978-1-4799-5745","10.1109/SPECTS.2014.6879992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6879992","Software Defined Networks;multiplexing;traffic optimization;compression","IP networks;Decision support systems","Internet telephony;telecommunication computing;transport protocols","low-efficiency traffic optimization;OpenFlow software defined networks;bandwidth usage optimization;SDN;small-packet traffic flows;VoIP;UDP;TCP-based online games;ACK;TCP flow;voice-over-Internet protocol;user defined protocol;transport control protocol;IPv4;IPv6;bandwidth savings","","4","18","","","","","","IEEE","IEEE Conferences"
"Early design stage thermal evaluation and mitigation: The locomotiv architectural case","T. Sassolas; C. Sandionigi; A. Guerre; A. Aminot; P. Vivet; H. Boussetta; L. Ferro; N. Peltier","CEA, LIST, 91191 Gif-sur-Yvette CEDEX, France; CEA, LIST, 91191 Gif-sur-Yvette CEDEX, France; CEA, LIST, 91191 Gif-sur-Yvette CEDEX, France; CEA, LIST, 91191 Gif-sur-Yvette CEDEX, France; CEA, LETI, Minatec Campus, Grenoble, France; DOCEA Power, 166 rue du Rocher de Lorzier 38430 Moirans, France; DOCEA Power, 166 rue du Rocher de Lorzier 38430 Moirans, France; DOCEA Power, 166 rue du Rocher de Lorzier 38430 Moirans, France","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","2","To offer more computing power to modern SoCs, transistors keep scaling in new technology nodes. Consequently, the power density is increasing, leading to higher thermal risks. Thermal issues need to be addressed as early as possible in the design flow, when the optimization opportunities are the highest. For early design stages, architects rely on virtual prototypes to model their designs' behavior with an adapted trade-off between accuracy and simulation speed. Unfortunately, accurate virtual prototypes fail to encompass thermal effects timescale. In this paper, we demonstrate that less accurate high-level architectural models, in conjunction with efficient power and thermal simulation tools, provide an adapted environment to analyze thermal issues and design software thermal mitigation solutions in the case of the Locomotiv MPSoC architecture.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800528","","Computer architecture;Adaptation models;Accuracy;Analytical models;Thermal analysis;Runtime;Software","optimisation;scaling circuits;system-on-chip;thermal management (packaging)","thermal evaluation;modern SoC;transistors scaling;power density;thermal risks;optimization;accurate virtual prototypes;power simulation tool;thermal simulation tool;design software thermal mitigation solutions;locomotiv MPSoC architecture","","","4","","","","","","IEEE","IEEE Conferences"
"Exploring Model-Based Repositories for a Broad Range of Industrial Applications and Challenges","T. Yue; S. Ali; A. Descour; Q. Gan; M. Liaaen; G. M. Merkesvik; B. K. Nielsen; J. Nygård; B. O. Olafsen; A. L. Waal","NA; NA; NA; NA; NA; NA; NA; NA; NA; NA","2014 14th International Conference on Quality Software","","2014","","","37","46","Nowadays, systems are becoming increasingly complex and large and the process of developing such large-scale systems is becoming complicated with high cost and enormous effort required. Such a complicated process has a prominent challenge to ensure the quality of delivered artifacts. Therefore there is clearly a need to facilitate reuse of developed artifacts (e.g., requirements, architecture, tests) and enable automated analyses such as risk analyses, prioritizing test cases, change impact analysis, with the objective to reduce cost, effort and improve quality. Model-based engineering provides a promising mechanism to facilitate reuse and enable automation. The key idea is to use models as the backbone of structuring repositories that contain reusable artifacts (e.g., test cases, requirements). Such a backbone model is subse-quently used to enable various types of automation such as model-based testing and automated rule verification. In this paper, we report 12 industrial projects from five different industry domains that all require the construction of model-based repositories to enable various types of automation. We believe using models as the backbone to structure repositories for the purpose of enabling different types of automation in different contexts is a new and non-conventional model-based development research approach. This exploratory paper will serve the basis for future research to derive a generic model-based repository.","1550-6002;2332-662X","978-1-4799-7198-5978-1-4799-7197","10.1109/QSIC.2014.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958385","Model-based Repository;Model-based Repository Engineering;Backbone Model;Automation;Analysis;Optimization;Generation","Unified modeling language;Training;Automation;Analytical models;Cancer;Testing;Optimization","formal verification;program testing;software quality","model-based repository;large-scale system;risk analysis;model-based engineering;model-based testing;automated rule verification","","1","21","","","","","","IEEE","IEEE Conferences"
"Replicating and Re-Evaluating the Theory of Relative Defect-Proneness","M. D. Syer; M. Nagappan; B. Adams; A. E. Hassan","School of Computing, Queen’s University, Kingston, ON, Canada; School of Computing, Queen’s University, Kingston, ON, Canada; Genie Informatique et Genie Logiciel, Ecole Polytechnique de Montreal, Campus de l’Universite de Montreal; School of Computing, Queen’s University, Kingston, ON, Canada","IEEE Transactions on Software Engineering","","2015","41","2","176","197","A good understanding of the factors impacting defects in software systems is essential for software practitioners, because it helps them prioritize quality improvement efforts (e.g., testing and code reviews). Defect prediction models are typically built using classification or regression analysis on product and/or process metrics collected at a single point in time (e.g., a release date). However, current defect prediction models only predict if a defect will occur, but not when, which makes the prioritization of software quality improvements efforts difficult. To address this problem, Koru et al. applied survival analysis techniques to a large number of software systems to study how size (i.e., lines of code) influences the probability that a source code module (e.g., class or file) will experience a defect at any given time. Given that 1) the work of Koru et al. has been instrumental to our understanding of the size-defect relationship, 2) the use of survival analysis in the context of defect modelling has not been well studied and 3) replication studies are an important component of balanced scholarly debate, we present a replication study of the work by Koru et al. In particular, we present the details necessary to use survival analysis in the context of defect modelling (such details were missing from the original paper by Koru et al.). We also explore how differences between the traditional domains of survival analysis (i.e., medicine and epidemiology) and defect modelling impact our understanding of the size-defect relationship. Practitioners and researchers considering the use of survival analysis should be aware of the implications of our findings.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2361131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914599","Survival Analysis;Cox Models;Defect Modelling;Survival analysis;Cox models;defect modelling","Analytical models;Hazards;Software;Measurement;Data models;Mathematical model;Predictive models","program diagnostics;software quality;software reliability","relative defect-proneness theory;survival analysis techniques;source code module;size-defect relationship;defect modelling;software system defects","","3","47","","","","","","IEEE","IEEE Journals & Magazines"
"Run-time performance analysis of non-agent based solution for Inter Process Synchronization problem","D. Bhargava; M. Sinha; R. C. Poonia","Amity Institute of Information Technology, Amity University, Jaipur, India; BIT-Mesra Jaipur, India; Amity Institute of Information Technology, Amity University, Jaipur, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","5","Inter Process Synchronization is aimed at providing interrupt free and deadlock free well-structured communication among processes. The approach used is generally non-agent oriented approach which may lead to deadlock situation. This paper analyzes the runtime performance of non-agent based solution to IPS Problem. In this paper the testing is carried out and the results are obtained under time intervals T1 to T10 for eleven different parameters. Further based upon the correlation among parameterized variables, only five dependent variables were selected during the test conducted. The analysis is based upon the testing conducted on Window 7, a GUI based operating system. The results motivate the need of agent oriented approach to resolve the IPS problem.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359321","Inter Process Synchronization;software agents;correlation;dependence of variables;Inter Process Synchronization Manager","Correlation;Synchronization;Testing;Performance analysis;System recovery;Computer architecture;Continuous wavelet transforms","graphical user interfaces;operating systems (computers);software agents;synchronisation","run-time performance analysis;nonagent-based solution;interprocess synchronization problem;interrupt free-deadlock free well-structured communication;nonagent oriented approach;IPS problem;T1 time intervals;T10 time intervals;parameterized variables;dependent variables;Window 7;GUI based operating system;agent oriented approach","","1","9","","","","","","IEEE","IEEE Conferences"
"Using Java optimized processor as an intellectual property core beside a RISC processor in FPGA","M. E. Khazaee; S. Hoseinzadeh","Amirkabir University of Technology, Department of Computer Engineering, Iran; Electrical and Computer Engineering Science, Research Islamic Azad University, Iran","Proceedings of IEEE East-West Design & Test Symposium (EWDTS 2014)","","2014","","","1","6","Advances in computer technology have made this technology part of everyone's daily life. This in turn has created a demand for various applications to run on different machines. Java programming language was introduced to address this demand. However, to execute the Java programs, the computers must have a machine, called Java Virtual Machine (JVM) that translates its generic codes to machine-specific instructions. Traditionally, JVMs have been implemented in software. This is primarily due to the inherent flexibility of software. However, software JVMs often suffer from their lower speed and memory overhead. Consequently, some efforts have been put into hardware implementations of a JVM. These hardware implementations were not safe from those problems as well. In this paper, a new architecture for hardware implementation of Java Virtual Machine is proposed. This architecture mitigates the problems that hardware implementations of Java Virtual Machine typically have. The proposed architecture is appropriate for designers who use FPGAs as their development platform, or use IP cores in their designs.","","978-1-4799-7630","10.1109/EWDTS.2014.7027048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027048","Hardware JVM;Bytecode;Native Instruction;Native Code;Bytecode to Native Instruction Translator;Java Optimized Processor;JOP","Java;Computer architecture;Hardware;Virtual machining;Reduced instruction set computing;Central Processing Unit","field programmable gate arrays;Java;microprocessor chips;program interpreters;reduced instruction set computing;virtual machines","Java optimized processor;intellectual property core;RISC processor;FPGA;Java programming language;Java virtual machine;generic code translation;machine-specific instructions;hardware implementation;JVM architecture;IP cores","","","8","","","","","","IEEE","IEEE Conferences"
"Hardware/software infrastructure for ASIC commissioning and rapid system prototyping","P. Reichel; J. Doge","Fraunhofer Institute for Integrated Circuits IIS, Design Automation Division EAS, Zeunerstr. 38, 01069 Dresden, Germany; Fraunhofer Institute for Integrated Circuits IIS, Design Automation Division EAS, Zeunerstr. 38, 01069 Dresden, Germany","2014 International Conference on ReConFigurable Computing and FPGAs (ReConFig14)","","2014","","","1","6","FPGAs are a key enabling technology for rapid and efficient system prototyping and initial commissioning of newly developed integrated circuits. One major aspect is the setup and control of interface components between devices under test (DUT) and the FPGA infrastructure. So, as to maintain high flexibility in conjunction with the ability to deal with changes of requirements and use cases, as well as unforeseen or faulty behavior of the DUT, we propose a novel reconfigurable hardware/software infrastructure. IP blocks, such as register files or interface components to external hardware are attached as leafs to a tree-like communication system optimized for alterations. It is designed as an Embedded Linux compatible CPU subsystem to be accessed from user space via a uniform and portable kernel driver. Thus, it implements transparent access to custom functionality from user applications without specific knowledge concerning the hardware/software coupling.","2325-6532","978-1-4799-5944-0978-1-4799-5943","10.1109/ReConFig.2014.7032532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7032532","rapid system prototyping;hardware/software co-design;Embedded Linux;image sensor characterization;ASIC commissioning;FPGA","Hardware;Kernel;Field programmable gate arrays;Registers;System-on-chip;Linux","application specific integrated circuits;embedded systems;field programmable gate arrays;hardware-software codesign;Linux;software prototyping","reconfigurable hardware-software infrastructure;ASIC commissioning;application specific integrated circuits;rapid system prototyping;FPGA;field programmable gate array;device under test;DUT;embedded Linux compatible CPU subsystem;hardware-software coupling","","","29","","","","","","IEEE","IEEE Conferences"
"A ranking-based lung nodule image classification method using unlabeled image knowledge","F. Zhang; Y. Song; W. Cai; Y. Zhou; M. Fulham; S. Eberl; S. Shan; D. Feng","BMIT Research Group, School of IT, University of Sydney, Australia; BMIT Research Group, School of IT, University of Sydney, Australia; BMIT Research Group, School of IT, University of Sydney, Australia; The Russell H. Morgan Department of Radiology and Radiological Science, Johns Hopkins University School of Medicine, Baltimore, USA; Department of PET and Nuclear Medicine, Royal Prince Alfred Hospital, Australia; Department of PET and Nuclear Medicine, Royal Prince Alfred Hospital, Australia; School of Software, Dalian University of Technology, China; BMIT Research Group, School of IT, University of Sydney, Australia","2014 IEEE 11th International Symposium on Biomedical Imaging (ISBI)","","2014","","","1356","1359","In this paper, we propose a novel semi-supervised classification method for four types of lung nodules, i.e., well-circumscribed, vascularized, juxta-pleural and pleural-tail, in low dose computed tomography (LDCT) scans. The proposed method focuses on classifier design by incorporating the knowledge extracted from both training and testing datasets, and contains two stages: (1) bipartite graph construction, which presents the direct similar relationship between labeled and unlabeled images, (2) ranking score calculation, which computes the possibility of unlabeled images for each of the given four types. Our proposed method is evaluated on a publicly available dataset and clearly demonstrates its promising classification performance.","1945-7928;1945-8452","978-1-4673-1961","10.1109/ISBI.2014.6868129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868129","Lung nodule;classification;bipartite graph;ranking score","Testing;Lungs;Training;Bipartite graph;Support vector machines;Educational institutions;Image classification","computerised tomography;dosimetry;graph theory;image classification;knowledge acquisition;learning (artificial intelligence);lung;medical image processing","ranking-based lung nodule image classification;unlabeled image knowledge;semisupervised classification method;well-circumscribed lung nodules;vascularized lung nodules;juxta-pleural lung nodules;pleural-tail lung nodules;low dose computed tomography;LDCT;knowledge extraction;bipartite graph construction;ranking score calculation","","3","11","","","","","","IEEE","IEEE Conferences"
"An SMT based method for optimizing arithmetic computations in embedded software code","H. Eldib; C. Wang","Department of ECE, Virginia Tech, Blacksburg, 24061, USA; Department of ECE, Virginia Tech, Blacksburg, 24061, USA","2013 Formal Methods in Computer-Aided Design","","2013","","","129","136","We present a new method for optimizing the C/C++ code of embedded control software with the objective of minimizing implementation errors in the linear fixed-point arithmetic computations caused by overflow, underflow, and truncation. Our method relies on the use of an SMT solver to search for alternative implementations that are mathematically equivalent but require a smaller bit-width, or implementations that use the same bit-width but have a larger error-free dynamic range. Our systematic search of the bounded implementation space is based on an inductive synthesis procedure, which guarantees to find a solution as long as such solution exists. Furthermore, the synthesis procedure is applied incrementally to small code regions - one at a time - as opposed to the entire program, which is crucial for scaling the method to programs of realistic size and complexity. We have implemented our method in a software tool based on the Clang/LLVM compiler and the Yices SMT solver. Our experiments, conducted on a set of representative benchmarks from embedded control and DSP applications, show that the method is both effective and efficient in optimizing fixed-point arithmetic computations in embedded software code.","","978-0-9835678-3","10.1109/FMCAD.2013.6679401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6679401","","Finite wordlength effects;Optimization;Microcontrollers;Skeleton;Dynamic range;Benchmark testing;Digital signal processing","C++ language;computability;embedded systems;floating point arithmetic;program compilers","SMT based method;arithmetic computations optimization;embedded control software code;C/C++ code;linear fixed-point arithmetic computations;error-free dynamic range;bounded implementation space;systematic search;inductive synthesis procedure;Clang-LLVM compiler;Yices SMT solver;software tool;DSP applications","","5","23","","","","","","IEEE","IEEE Conferences"
"Multi GNSS constellation deeply coupled GNSS/INS integration for automotive application using a software defined GNSS receiver","M. Langer; G. F. Trommer","Institute of Systems Optimization, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Institute of Systems Optimization, Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany","2014 IEEE/ION Position, Location and Navigation Symposium - PLANS 2014","","2014","","","1105","1112","In this paper a non-coherent deeply coupled GNSS/INS system using a multi constellation software defined GNSS receiver is assessed regarding the advantages of multi constellation (GPS/GALILEO/GLONASS) GNSS/INS integration compared to a GPS only based GPS/INS integrated approach. Both navigation systems are used to evaluate the performance of a moving automotive platform during real world test drives in an urban environment. The navigation solutions are compared to a RTK reference solution. The software GNSS receiver design is presented focusing on processing data from multiple GNSS systems including estimation of clock offsets between different GNSS systems. Several combinations of GNSS systems GPS/GLONASS, GPS/GALILEO, GPS only, GLONASS only and GALILEO only are evaluated.","2153-3598;2153-358X","978-1-4799-3320-4978-1-4799-3319","10.1109/PLANS.2014.6851480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6851480","","Global Positioning System;Satellites;Receivers;Kalman filters;Equations;Frequency measurement","automobiles;Global Positioning System;inertial navigation;radio receivers;software radio","multiGNSS constellation deeply coupled GNSS/INS integration;automotive application;noncoherent deeply coupled GNSS-INS system;multiconstellation software defined GNSS receiver design;GPS;GALILEO;GLONASS;navigation systems;performance evaluation;real world test drives;urban environment;RTK reference solution;clock offset estimation","","1","7","","","","","","IEEE","IEEE Conferences"
"Software Designed GNSS system emulator","M. Pollina; O. Desenfans; R. Fiengo; B. Eged","M3 Systems; M3 Systems; National Instruments; National Instruments","2014 IEEE Metrology for Aerospace (MetroAeroSpace)","","2014","","","404","407","In this paper we present an open Software Designed GNSS Simulator that is not only capable to simulate GNSS signals according to user defined scenario but that also provides the capabilities to support record & replay “real signal” and to support performance evaluation ranging from low level signal processing to statistics on trajectories. Thanks to the software radio approach, the GNSS signal generator becomes a highly configurable tool that can support multiple test & measurement applications dedicated to GNSS community. With a single platform the user can access all generic and custom functions he needs ensuring long term evolution and optimized return on investment.","","978-1-4799-2069","10.1109/MetroAeroSpace.2014.6865958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6865958","GNSS;SDR","Global Positioning System;Receivers;Trajectory;Radio frequency;Satellites;Testing;Constellation diagram","satellite navigation;signal processing;software radio","software designed GNSS system emulator;GNSS signals;low level signal processing;software radio approach","","","5","","","","","","IEEE","IEEE Conferences"
"Optimizing post-silicon conformance checking","L. Lei; K. Cong; F. Xie","Department of Computer Science, Portland State University, OR 97207, USA; Department of Computer Science, Portland State University, OR 97207, USA; Department of Computer Science, Portland State University, OR 97207, USA","2013 IEEE 31st International Conference on Computer Design (ICCD)","","2013","","","499","502","Virtual prototypes of hardware devices, a.k.a, virtual devices, are increasingly used to enable early software development before silicon prototypes/devices are available. In previous work, we presented a post-silicon conformance checking approach to detecting interface state inconsistencies between a silicon device and its virtual device. In this paper, we present an optimization, adaptive concretization, to reduce the overhead incurred by symbolic execution, a key technique used in our conformance checking approach. We have evaluated our optimized approach on three Ethernet adapters and their virtual devices. The results demonstrate that it is effective and efficient: 21 inconsistencies are discovered and time usages are reduced by an order of magnitude, comparing to the previous approach.","1063-6404","978-1-4799-2987","10.1109/ICCD.2013.6657092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6657092","Post-silicon validation;conformance checking;symbolic execution","Silicon devices;Concrete;Computer bugs;Silicon;Optimization;Registers;Hardware","conformance testing;local area networks;optimisation;peripheral interfaces;software prototyping;symbol manipulation","post-silicon conformance checking optimization;virtual prototypes;hardware devices;early software development;interface state inconsistencies;adaptive concretization;symbolic execution;Ethernet adapters;virtual devices","","1","14","","","","","","IEEE","IEEE Conferences"
"A novel hybrid swarm based approach for curriculum based course timetabling problem","C. W. Fong; H. Asmuni; W. S. Lam; B. McCollum; P. McMullan","Software Engineering Research Group, Software Engineering Department, Universiti Teknologi Malaysia, 81310 UTM Skudai, Johor, Malaysia; Software Engineering Research Group, Software Engineering Department, Universiti Teknologi Malaysia, 81310 UTM Skudai, Johor, Malaysia; Software Engineering Research Group, Software Engineering Department, Universiti Teknologi Malaysia, 81310 UTM Skudai, Johor, Malaysia; Department of Computer Science, Queen's University Belfast, Belfast BT7 1NN United Kingdom; Department of Computer Science, Queen's University Belfast, Belfast BT7 1NN United Kingdom","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","544","550","This work applies a hybrid approach in solving the university curriculum-based course timetabling problem as presented as part of the 2<sup>nd</sup> International Timetabling Competition 2007 (ITC2007). The core of the hybrid approach is based on an artificial bee colony algorithm. Past methods have applied artificial bee colony algorithms to university timetabling problems with high degrees of success. Nevertheless, there exist inefficiencies in the associated search abilities in term of exploration and exploitation. To improve the search abilities, this work introduces a hybrid approach entitled nelder-mead great deluge artificial bee colony algorithm (NMGD-ABC) where it combined additional positive elements of particle swarm optimization and great deluge algorithm. In addition, nelder-mead local search is incorporated into the great deluge algorithm to further enhance the performance of the resulting method. The proposed method is tested on curriculum-based course timetabling as presented in the ITC2007. Experimental results reveal that the proposed method is capable of producing competitive results as compared with the other approaches described in literature.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900453","","Educational institutions;Sociology;Statistics;Optimization;Search problems;Tin;Particle swarm optimization","educational courses;particle swarm optimisation;search problems","hybrid swarm;university curriculum-based course timetabling problem;2nd International Timetabling Competition 2007;ITC2007;nelder-mead great deluge artificial bee colony algorithm;nelder-mead local search;particle swarm optimization","","","21","","","","","","IEEE","IEEE Conferences"
"Analysis and optimization of lever propelled wheelchair","S. Agarwal; S. Gautam","Department of Mechanical Engineering, Krishna Institute of Engineering and Technology, Ghaziabad, India; Department of Mechanical Engineering, Krishna Institute of Engineering and Technology, Ghaziabad, India","2014 Innovative Applications of Computational Intelligence on Power, Energy and Controls with their impact on Humanity (CIPECH)","","2014","","","433","440","A wheelchair is a commonly used assistive device used by disabled people for mobility. An appropriate wheelchair gives them a sense of independence and encourages them to become productive members of the society. It gives them a feeling of belongingness and inclusion in the society. But for centuries now in India, there has been a clear vacuum of thoughts over a redesign of traditional wheelchairs. A new design of existing levered wheelchair is proposed here. This new design would increase the reach capabilities and social boundaries of disabled which inhibit the lives of the disabled day to day. The paper is divided into two parts. In the first part a comparative study has been done of push rim wheelchair and lever wheelchair. In the second part, Optimization of the lever wheelchair has been done. The comparison has been done on various parameters such as peak velocity, average velocity, acceleration, retardation, Stroke frequency, Reduction in effort, Mechanical efficiency, Obstacle climbing ability, Turning radius, the distance travelled per stroke, Pressure cuts and burns caused during propelling the chair. Biomechanical testing is also done considering the heart rate and oxygen level consumption. This information can be used to determine the energy demand, which is intrinsically connected to a wheelchair's mechanical efficiency. Feedback from test subjects was used for the optimization of the chair. The optimization mainly focus on the ergonomics and efficiency of design and for that the main emphasis is given on factors like the position of Lever-drive system, Configuration of wheel, chassis and Foot Rest. This optimized design would make the chair more efficient and easy to use by enabling the person to drive, steer, and stop without even touching the wheels, thus, eliminating the risk of friction burns and protecting the wrists and shoulders from repetitive stresses. The analysis is done using software like ANSYS and CATIA and validation of results is conducted with the users and non-users of wheel chair.","","978-1-4799-5871-9978-1-4799-5870","10.1109/CIPECH.2014.7019124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019124","Lever propelled wheelchair;push rim wheelchair;comparison;optimization;ergonomics;mechanical efficiency","Wheelchairs;Heart rate;Propulsion;Optimization;Wheels;Optical wavelength conversion;Testing","biomechanics;ergonomics;handicapped aids;optimisation;wheelchairs","peak velocity;average velocity;acceleration;retardation;stroke frequency;effort reduction;Mechanical efficiency;Obstacle climbing ability;Turning radius;pressure cuts;burns;heart rate;oxygen level consumption;energy demand;ergonomics;CATIA;ANSYS;lever-drive system;biomechanical testing;social boundaries;mobility;disabled people;assistive device;lever propelled wheelchair;optimization","","","28","","","","","","IEEE","IEEE Conferences"
"Understanding DevOps & bridging the gap from continuous integration to continuous delivery","M. Virmani","Development Manager IBM Rational","Fifth International Conference on the Innovative Computing Technology (INTECH 2015)","","2015","","","78","82","As part of Agile transformation in past few years we have seen IT organizations adopting continuous integration principles in their software delivery lifecycle, which has improved the efficiency of development teams. With the time it has been realized that this optimization as part of continuous integration - alone - is just not helping to make the entire delivery lifecycle efficient or is not driving the organization efficiency. Unless all the pieces of a software delivery lifecycle work like a well oiled machine - efficiency of organization to optimize the delivery lifecycle can not be met. This is the problem which DevOps tries to address. This paper tries to cover all aspects of Devops applicable to various phases of SDLC and specifically talks about business need, ways to move from continuous integration to continuous delivery and its benefits. Continuous delivery transformation in this paper is explained with a real life case study that how infrastructure can be maintained just in form of code (IAAC). Finally this paper touches upon various considerations one must evaluate before adopting DevOps and what kind of benefits one can expect.","","978-1-4673-7551","10.1109/INTECH.2015.7173368","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7173368","DevOps;Continuous Integration;Continuous Delivery;Infrastructure as a Code (IAAC)","Software;Organizations;Testing;Automation;Optimization;Production","integrated software;software prototyping","DevOps;continuous integration;continuous delivery;agile transformation;software delivery lifecycle;SDLC;IAAC","","10","8","","","","","","IEEE","IEEE Conferences"
"Optimization of Object Queries on Collections Using Annotations for the String Valued Attributes","V. K. S. Nerella; S. K. Madria; T. Weigert","NA; NA; NA","2013 IEEE 37th Annual Computer Software and Applications Conference","","2013","","","313","318","Object oriented programming languages raised the level of abstraction by supporting the explicit first class query constructs in the programming codes. The query constructs can be optimized by leveraging the techniques of query optimization from the domain of databases. The existing optimization approaches such as JQL, however, incur high run time overhead as optimizations are performed only at run time. Therefore, in this paper, we propose an approach that performs the query optimization at compile time utilizing the metadata annotations in the source code. The proposed approach first collects the data from the sample execution of the program and extracts the essential metadata for the string valued attributes. Then, the annotations consisting of the metadata values associated with string attributes are generated in the source code and the histograms are built using those annotations. The selectivity estimates of the predicates and the joins in the query are computed from the histograms. Next, the query plan is generated at compile time through the maximum selectivity heuristic. The query plan is modified at run time in cases of significant updates to the string data. The approach also incorporates the cache heuristics that determine whether to cache the query result or not. The cached query results are incrementally maintained up-to-date. Our experimental results demonstrate that our approach has reduced the run time of the program more than the earlier approaches such as JQL.","0730-3157","978-0-7695-4986","10.1109/COMPSAC.2013.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649843","Annotations;string data;Query Plan;Selectivity;Compile time","Query processing;Benchmark testing;Histograms;Maintenance engineering;Optimization;Java;Time-frequency analysis","meta data;object-oriented programming;optimisation;query processing","optimization;object queries;string valued attributes;object oriented programming languages;query optimization;high run time overhead;metadata annotations;metadata values;query plan;compile time;cached query;JQL","","","15","","","","","","IEEE","IEEE Conferences"
"Small file access optimization based on GlusterFS","X. Tao; L. Alei","School of Software Engineering, Shanghai Jiao Tong University, China; School of Software Engineering, Shanghai Jiao Tong University, China","Proceedings of 2014 International Conference on Cloud Computing and Internet of Things","","2014","","","101","104","This paper describes a strategy to optimize small file's reading and writing performance on traditional distributed file system. Traditional distributed file system like GlusterFS stores data within local file system (XFS, EXT3, EXT4, etc.), which shows a significant bottleneck on file metadata lookup. We try to re-design metadata structure by merging small file into large file, thus to reduce size of metadata, so we can store the whole files' metadata inside main memory. We design and implement the whole strategy on GlusterFS, test results show a great performance optimization on small file operation.","","978-1-4799-4764-5978-1-4799-4765","10.1109/CCIOT.2014.7062514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7062514","small file;distributed file system;metadata;optimization;glusterfs","Reliability;Performance evaluation","distributed databases;meta data;storage management;table lookup","small file access optimization;GlusterFS;small file reading performance;small file writing performance;distributed file system;data storage;local file system;file metadata lookup;metadata structure redesign;metadata size reduction","","1","14","","","","","","IEEE","IEEE Conferences"
"Genetic Improvement of Programs","W. B. Langdon","NA","2014 16th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing","","2014","","","14","19","Genetic programming can optimise software, including: evolving test benchmarks, generating hyper-heuristics by searching meta-heuristics, generating communication protocols, composing telephony systems and web services, generating improved hashing and C++ heap managers, redundant programming and even automatic bug fixing. Particularly in embedded real-time or mobile systems, there may be many ways to trade off expenses (such as time, memory, energy, power consumption) vs. Functionality. Human programmers cannot try them all. Also the best multi-objective Pareto trade off may change with time, underlying hardware and network connection or user behaviour. It may be GP can automatically suggest different trade offs for each new market. Recent results include substantial speed up by evolving a new version of a program customised for a special case.","","978-1-4799-8448-0978-1-4799-8447","10.1109/SYNASC.2014.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034660","genetic programming;GP;Automatic software re-engineering;Bowtie2;multiple objective exploration;search based software engineering;SBSE;GPGPU","Genetic programming;Software engineering;Graphics processing units;Evolutionary computation;Grammar","genetic algorithms;Pareto optimisation;software performance evaluation","genetic program improvement;genetic programming;software optimisation;test benchmarks;hyper-heuristics;meta-heuristics;communication protocols;telephony systems;Web services;improved hashing;C++ heap managers;automatic bug fixing;mobile systems;embedded real-time systems;human programmers;multiobjective Pareto trade off;GP","","2","51","","","","","","IEEE","IEEE Conferences"
"Testing properties of dataflow program operators","Z. Xu; M. Hirzel; G. Rothermel; K. Wu","University of Nebraska, Lincoln, USA; IBM Watson Research, Yorktown Heights, NY, USA; University of Nebraska, Lincoln, USA; IBM Watson Research, Yorktown Heights, NY, USA","2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2013","","","103","113","Dataflow programming languages, which represent programs as graphs of data streams and operators, are becoming increasingly popular and being used to create a wide array of commercial software applications. The dependability of programs written in these languages, as well as the systems used to compile and run these programs, hinges on the correctness of the semantic properties associated with operators. Unfortunately, these properties are often poorly defined, and frequently are not checked, and this can lead to a wide range of problems in the programs that use the operators. In this paper we present an approach for improving the dependability of dataflow programs by checking operators for necessary properties. Our approach is dynamic, and involves generating tests whose results are checked to determine whether specific properties hold or not. We present empirical data that shows that our approach is both effective and efficient at assessing the status of properties.","","978-1-4799-0215","10.1109/ASE.2013.6693071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693071","","Testing;Aggregates;Optimization;Semantics;Ports (Computers);System recovery;Program processors","data flow analysis;program testing","testing properties;dataflow program operators;dataflow programming languages;data streams;commercial software applications;semantic properties","","4","26","","","","","","IEEE","IEEE Conferences"
"SMT Malleability in IBM POWER5 and POWER6 Processors","A. Morari; C. Boneti; F. J. Cazorla; R. Gioiosa; C. Cher; A. Buyuktosunoglu; P. Bose; M. Valero","Barcelona Supercomputing Center, Barcelona and Univesitat Politecnica de Catalunya; Schlumberger BRGC, Rio de Janeiro; Barcelona Supercomputing Center, Barcelona and Spanish National Research Council; Barcelona Supercomputing Center, Barcelona; IBM T.J. Watson Research Center, Yorktown Heights; IBM T.J. Watson Research Center, Yorktown Heights; IBM T.J. Watson Research Center, Yorktown Heights; Barcelona Supercomputing Center, Barcelona and Spanish National Research Council","IEEE Transactions on Computers","","2013","62","4","813","826","While several hardware mechanisms have been proposed to control the interaction between hardware threads in an SMT processor, few have addressed the issue of software-controllable SMT performance. The IBM POWER5 and POWER6 are the first high-performance processors implementing a software-controllable hardware-thread prioritization mechanism that controls the rate at which each hardware-thread decodes instructions. This paper shows the potential of this basic mechanism to improve several target metrics for various applications on POWER5 and POWER6 processors. Our results show that although the software interface is exactly the same, the software-controlled priority mechanism has a different effect on POWER5 and POWER6. For instance, hardware threads in POWER6 are less sensitive to priorities than in POWER5 due to the in order design. We study the SMT thread malleability to enable user-level optimizations that leverage software-controlled thread priorities. We also show how to achieve various system objectives such as parallel application load balancing, in order to reduce execution time. Finally, we characterize user-level transparent execution on POWER5 and POWER6, and identify the workload mix that best benefits from it.","0018-9340;1557-9956;2326-3814","","10.1109/TC.2012.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6138854","Malleability;simultaneous multithreading;hardware-thread priorities;IBM POWER5;IBM POWER6","Instruction sets;Hardware;Benchmark testing;Kernel;Linux","microprocessor chips;multi-threading;resource allocation","SMT malleability;IBM POWER5 processor;IBM POWER6 processor;hardware mechanism;hardware thread;software-controllable SMT performance;SMT processor;software-controllable hardware-thread prioritization mechanism;software interface;software-controlled priority mechanism;user-level optimization;parallel application load balancing;execution time;user-level transparent execution;simultaneous multithreading","","2","28","","","","","","IEEE","IEEE Journals & Magazines"
"A methodology for model-based development and automated verification of software for aerospace systems","L. Märtin; M. Schatalov; M. Hagner; U. Goltz; O. Maibaum","Institute for Programming and Reactive Systems, TU Braunschweig, 38106, Germany; Institute for Programming and Reactive Systems, TU Braunschweig, 38106, Germany; Institute for Programming and Reactive Systems, TU Braunschweig, 38106, Germany; Institute for Programming and Reactive Systems, TU Braunschweig, 38106, Germany; Simulation and Software Technology, German Aerospace Center, 38108 Braunschweig, Germany","2013 IEEE Aerospace Conference","","2013","","","1","19","Today's software for aerospace systems typically is very complex. This is due to the increasing number of features as well as the high demand for safety, reliability, and quality. This complexity also leads to significant higher software development costs. To handle the software complexity, a structured development process is necessary. Additionally, compliance with relevant standards for quality assurance is a mandatory concern. To assure high software quality, techniques for verification are necessary. Besides traditional techniques like testing, automated verification techniques like model checking become more popular. The latter examine the whole state space and, consequently, result in a full test coverage. Nevertheless, despite the obvious advantages, this technique is rarely yet used for the development of aerospace systems. In this paper, we propose a tool-supported methodology for the development and formal verification of safety-critical software in the aerospace domain. The methodology relies on the V-Model and defines a comprehensive work flow for model-based software development as well as automated verification in compliance to the European standard series ECSS-E-ST-40C. Furthermore, our methodology supports the generation and deployment of code. For tool support we use the tool SCADE Suite (Esterel Technology), an integrated design environment that covers all the requirements for our methodology. The SCADE Suite is well established in avionics and defense, rail transportation, energy and heavy equipment industries. For evaluation purposes, we apply our approach to an up-to-date case study of the TET-1 satellite bus. In particular, the attitude and orbit control software is considered. The behavioral models for the subsystem are developed, formally verified, and optimized.","1095-323X;1095-323X","978-1-4673-1813-6978-1-4673-1812-9978-1-4673-1811","10.1109/AERO.2013.6496950","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496950","","Model checking;Standards;Automata;Europe;Software systems","aerospace computing;aerospace safety;attitude control;control engineering computing;formal verification","software automated verification;software complexity;structured development process;quality assurance;software quality;automated verification techniques;tool-supported methodology;formal verification;safety-critical software;aerospace domain systems;V-Model;model-based software development;European standard series ECSS-E-ST-40C;SCADE Suite;TET-1 satellite bus;orbit control software;attitude control software;code deployment;code generation;heavy equipment industries;defense industries;avionics industries;rail transportation industries;energy industries;heavy equipment industries","","4","28","","","","","","IEEE","IEEE Conferences"
"Energy Efficient HPC on Embedded SoCs: Optimization Techniques for Mali GPU","I. Grasso; P. Radojkovic; N. Rajovic; I. Gelado; A. Ramirez","NA; NA; NA; NA; NA","2014 IEEE 28th International Parallel and Distributed Processing Symposium","","2014","","","123","132","A lot of effort from academia and industry has been invested in exploring the suitability of low-power embedded technologies for HPC. Although state-of-the-art embedded systems-on-chip (SoCs) inherently contain GPUs that could be used for HPC, their performance and energy capabilities have never been evaluated. Two reasons contribute to the above. Primarily, embedded GPUs until now, have not supported 64-bit floating point arithmetic - a requirement for HPC. Secondly, embedded GPUs did not provide support for parallel programming languages such as OpenCL and CUDA. However, the situation is changing, and the latest GPUs integrated in embedded SoCs do support 64-bit floating point precision and parallel programming models. In this paper, we analyze performance and energy advantages of embedded GPUs for HPC. In particular, we analyze ARM Mali-T604 GPU - the first embedded GPUs with OpenCL Full Profile support. We identify, implement and evaluate software optimization techniques for efficient utilization of the ARM Mali GPU Compute Architecture. Our results show that, HPC benchmarks running on the ARM Mali-T604 GPU integrated into Exynos 5250 SoC, on average, achieve speed-up of 8.7X over a single Cortex-A15 core, while consuming only 32% of the energy. Overall results show that embedded GPUs have performance and energy qualities that make them candidates for future HPC systems.","1530-2075","978-1-4799-3800-1978-1-4799-3799","10.1109/IPDPS.2014.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877248","High performance computing;Embedded GPUs;Optimization;Performance analysis","Graphics processing units;Benchmark testing;Vectors;Optimization;Kernel;Computer architecture;System-on-chip","graphics processing units;optimisation;parallel architectures;power aware computing;system-on-chip","energy efficient HPC;embedded SoC;optimization techniques;HPC;embedded systems-on-chip;embedded GPU;64-bit floating point arithmetic;parallel programming languages;OpenCL;CUDA;64-bit floating point precision;parallel programming models;ARM Mali-T604 GPU;OpenCL Full Profile support;software optimization techniques;ARM Mali GPU compute architecture;Exynos 5250 SoC;single Cortex-A15 core","","20","33","","","","","","IEEE","IEEE Conferences"
"Ant Colony Optimization, Genetic Programming and a hybrid approach for credit scoring: A comparative study","R. Aliehyaei; S. Khan","School of Computer Science, Columbus State University, USA; School of Computer Science, Columbus State University, USA","The 8th International Conference on Software, Knowledge, Information Management and Applications (SKIMA 2014)","","2014","","","1","5","Credit scoring is a commonly used method for evaluating the risk involved in granting credits. Both Genetic Programming (GP) and Ant Colony Optimization (ACO) have been investigated in the past as possible tools for credit scoring. This paper reports an investigation into the relative performances of GP, ACO and a new hybrid GP-ACO approach, which relies on the ACO technique to produce the initial populations for the GP technique. Performance of the hybrid approach has been compared with both the GP and ACO approaches using two well-known benchmark data sets. Experimental results demonstrate the dependence of GP and ACO classification accuracies on the input data set. For any given data set, the hybrid approach performs better than the worse of the other two methods. Results also show that use of ACO in the hybrid approach has only a limited impact in improving GP performance.","","978-1-4799-6399","10.1109/SKIMA.2014.7083391","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7083391","Credit scoring;evolutionary computation;genetic programming;ant colony optimization","Accuracy;Genetic programming;Computational modeling;Ant colony optimization;Evolutionary computation;Benchmark testing;Data mining","ant colony optimisation;finance;genetic algorithms","ant colony optimization;genetic programming;credit scoring;hybrid GP-ACO approach;ACO technique;GP technique","","2","22","","","","","","IEEE","IEEE Conferences"
"Face hallucination via Cauchy regularized sparse representation","S. Qu; R. Hu; S. Chen; Z. Wang; J. Jiang; C. Yang","National Engineering Research Center for Multimedia Software, Computer School of Wuhan Univ., China; National Engineering Research Center for Multimedia Software, Computer School of Wuhan Univ., China; National Engineering Research Center for Multimedia Software, Computer School of Wuhan Univ., China; National Engineering Research Center for Multimedia Software, Computer School of Wuhan Univ., China; National Engineering Research Center for Multimedia Software, Computer School of Wuhan Univ., China; National Engineering Research Center for Multimedia Software, Computer School of Wuhan Univ., China","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","","2015","","","1216","1220","In dictionary-learning-based face hallucination, the testing image is represented as a linear combination of the training samples, and how to obtain the optimal coefficients is the primary issue. Sparse representation (SR) has ever been widely used in face hallucination, however, due to the fact that SR overemphasizes the sparsity, the obtained linear combination coefficients turn out far aggressively sparse, then leading to unsatisfactory hallucinated results. In this paper, we present a moderately sparse prior model for face hallucination problem with the L1 norm penalty in classic SR replaced by a Cauchy penalty term. An iterative optimization is further presented to solve the minimization of Cauchy regularized objective function. The experimental results on public face database demonstrate that our method is much more effective than state-of-the-art methods.","1520-6149;2379-190X","978-1-4673-6997","10.1109/ICASSP.2015.7178163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178163","Super-resolution;face hallucination;sparse representation;Cauchy regularization","Face;Training;Databases;Image resolution;Dictionaries;Signal resolution;Image reconstruction","compressed sensing;face recognition;image representation;image resolution;iterative methods;optimisation","face hallucination;Cauchy regularization;sparse representation;iterative optimization;face superresolution","","3","20","","","","","","IEEE","IEEE Conferences"
"LTE user plane data transmission design based on layout optimization","Z. Gao; L. Huang; R. Zhang","School of Information Science and Technology, Xiamen University, Xiamen, Fujian, the P. R. of China; School of Information Science and Technology, Xiamen University, Xiamen, Fujian, the P. R. of China; School of Information Science and Technology, Xiamen University, Xiamen, Fujian, the P. R. of China","2015 IEEE 9th International Conference on Anti-counterfeiting, Security, and Identification (ASID)","","2015","","","51","55","The user plane is one of the most important issues of LTE (Long Term Evolution) system design. The implementation of the user plane data transmission is directly related to the real-time performance and efficiency of the system. Based on the research of the protocol stack data processing, the software architecture of the user plane and data transmission submodules are designed in this paper. Then the solution of the interface between L1C (L1 Layer Control Plane) and physical layer are put forward and implemented on the hardware platform of BCM (Broadcom). At last, the performance of the whole system tested on the hardware is given, and the results show that the designed architecture and interface can meet the demand of LTE system.","2163-5056","978-1-4673-7140-7978-1-4673-7139-1978-1-4673-7138","10.1109/ICASID.2015.7405660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405660","user plane;interface;layout optimization","Downlink;Uplink;Message systems;Long Term Evolution;Data communication;Physical layer;Software architecture","data communication;Long Term Evolution;protocols","LTE user plane data transmission design;layout optimization;Long Term Evolution system design;protocol stack data processing;user plane software architecture;data transmission submodule design;physical layer;L1 layer control plane;BCM hardware platform;broadcom hardware platform","","","8","","","","","","IEEE","IEEE Conferences"
"Rule-based multi-state gravitational search algorithm for discrete optimization problem","I. Ibrahim; Z. Ibrahim; H. Ahmad; Z. M. Yusof","Universiti Malaysia Pahang, Pekan, Malaysia; Universiti Malaysia Pahang, Pekan, Malaysia; Universiti Malaysia Pahang, Pekan, Malaysia; Universiti Malaysia Pahang, Pekan, Malaysia","2015 4th International Conference on Software Engineering and Computer Systems (ICSECS)","","2015","","","142","147","Gravitational search algorithm swarm (GSA) is a metaheuristic optimization algorithm, which is based on the Newton's law of gravity and the law of motion, has been successfully applied to solve various optimization problems in real-value search space. Later, binary gravitational search algorithm (BGSA) is designed to solve discrete optimization problems. In this study, rule-based multi-state gravitational search algorithm (RBMSGSA) algorithm is proposed to solve discrete combinatorial optimization problems. The algorithm operates based on a simplified mechanism of transition between two states. The algorithm able to produce feasible solution in solving traveling salesman problem (TSP), one of the most intensively studied discrete combinatorial optimization problems. To evaluate the performances of the proposed algorithm and the BGSA, several experiments using six sets of selected benchmarks instances of traveling salesman problem (TSP) are conducted. The experimental results showed the newly introduced approach consistently outperformed the BGSA in all TSP benchmark instances used.","","978-1-4673-6722-6978-1-4673-6721","10.1109/ICSECS.2015.7333099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333099","rule-based;multi-state;gravitational search algorithm;discrete combinatorial optimization problem;travelling salesman problem","Approximation algorithms;Optimization;Algorithm design and analysis;Benchmark testing;Search problems;Cities and towns;Heuristic algorithms","search problems;travelling salesman problems","rule-based multistate gravitational search algorithm;metaheuristic optimization algorithm;Newton gravity law;motion law;real-value search space;binary gravitational search algorithm;RBMSGSA;discrete combinatorial optimization problems;state transition mechanism;traveling salesman problem;TSP benchmark instance","","","20","","","","","","IEEE","IEEE Conferences"
"Hardware/software partitioning of embedded System-on-Chip applications","J. W. Tang; Y. W. Hau; M. Marsono","Faculty of Electrical Engineering, Universiti Teknologi Malaysia, Malaysia; Faculty of Biosciences and Medical Engineering, Universiti Teknologi Malaysia, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi Malaysia, Malaysia","2015 IFIP/IEEE International Conference on Very Large Scale Integration (VLSI-SoC)","","2015","","","331","336","HW/SW partitioning is an important development step during HW/SW co-design to ensure application performance in embedded System-on-Chip (SoC). This paper formulates the optimization of HW/SW partitioning aiming at maximizing streaming throughput with predefined area constraint, targeted for multi-processor system with hardware accelerator sharing capability. Two software-oriented and the second hardware-oriented greedy heuristic algorithms for HW/SW partitioning are proposed and tested on several random graphs and one multimedia application (MP3 decoder). Results show that the best result from both proposed greedy algorithms produce 93.6% near-optimal solution compared to brute force ground truth with faster HW/SW partitioning time.","2324-8440;2324-8432","978-1-4673-9140-5978-1-4673-9139","10.1109/VLSI-SoC.2015.7314439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314439","Embedded system;hardware/software partitioning;streaming applications","Hardware;Throughput;Program processors;Digital audio players;Decoding;Greedy algorithms","embedded systems;graph theory;greedy algorithms;hardware-software codesign;multiprocessing systems;optimisation;system-on-chip","hardware/software partitioning;HW/SW partitioning;HW/SW co-design;embedded system-on-chip;SoC;optimization;streaming throughput;area constraint;multiprocessor system;hardware accelerator sharing capability;software-oriented greedy heuristic algorithms;second hardware-oriented greedy heuristic algorithms;random graphs;multimedia application;MP3 decoder","","5","22","","","","","","IEEE","IEEE Conferences"
"QCN Based Dynamically Load Balancing: QCN Weighted Flow Queue Ranking","S. A. Pistirica; O. Poncea; M. C. Caraman","NA; NA; NA","2015 20th International Conference on Control Systems and Computer Science","","2015","","","197","204","Ethernet has become the primary network protocol used due to its undeniable advantages such as low cost, high speeds or ease of management. Being a best effort protocol, the IEEE Data Center Bridging Task Group developed a series of Layer 2 enhancements including Quantized Congestion Notification (QCN) and enabled a loss less environment. In this paper we propose a method of dynamic network load balancing based on QCN, named QCN Weighted Flow Queue Ranking (QCN-WFQR). Several local and system wide congestion indicatives are computed related to a series of nodes within a network, based on which different decisions to balance the load in the congestion aware computer network can be taken. QCN-WFQR design is generic, so it can be used in traditional networks and in software defined networks as well, being the new approach in computer networking. The algorithm was implemented and tested in a discrete network simulator (NS3) where we have analyzed different profiles of congestion indicatives variations. Also, we propose one method of using QCN-WFQR in distributed and parallel file systems and one method of flows migrations in SDN achieving better load balanced networks.","2379-0474;2379-0482","978-1-4799-1780-8978-1-4799-1779","10.1109/CSCS.2015.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168429","QCN;Congestion notification;Software-defined networking;Distributed systems","Ports (Computers);Protocols;Throughput;Control systems;Computational modeling;Standards;Software algorithms","parallel processing;protocols;queueing theory;resource allocation;software defined networking;telecommunication congestion control;telecommunication traffic","load balancing;quantized congestion notification;QCN weighted flow queue ranking;QCN-WFQR;network protocol;congestion aware computer network;discrete network simulator;distributed system;parallel file system;software-defined networking;SDN","","3","15","","","","","","IEEE","IEEE Conferences"
"Optimizing Memory Access with Fast Address Computation on a MIPS Architecture","Q. Ao; G. Jin; W. Su; S. Cai; S. Chen","NA; NA; NA; NA; NA","2014 9th IEEE International Conference on Networking, Architecture, and Storage","","2014","","","143","147","A 64-bit RISC processor is designed for large applications that need large memory address. Due to the restriction of the instruction fixed length, loading a 64-bit address needs a number of instructions, leading to a penalty both of memory performance and memory consumption. This paper describes an address computation method based on hardware and software co-design. In our extended MIPS processor which supports register + register addressing, we achieve an approximate effect of memory access as their 32-bit counterparts, we propose a software load-address method, which simplifies the calculation of 64-bit address. We implement our methods in the 64-bit OpenJDK 6 on MIPS, and give both performance and consumption comparisons for SPECjvm2008 and Dacapo. The experimental results show that the performance of SPECjvm2008 is improved by 5.1%, the performance of Dacapo is improved by 7.3% and near to 24% for some benchmarks. The size of method generated by JVM compiler is reduced by an average of 13%.","","978-1-4799-4087","10.1109/NAS.2014.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923174","load address;Java virtual machine;memory access;dynamic compilation","Registers;Software;Optimization;Benchmark testing;Java;Loading;Memory management","hardware-software codesign;Java;program compilers;reduced instruction set computing;virtual machines","memory access optimization;MIPS architecture;RISC processor;instruction fixed length;memory performance;memory consumption;address computation method;hardware and software codesign;extended MIPS processor;register addressing;software load-address method;OpenJDK 6;SPECjvm2008;Dacapo;JVM compiler;word length 64 bit","","2","11","","","","","","IEEE","IEEE Conferences"
"Network synchronization of periodic multiple source flows","F. Lorenz; D. Teichmann; R. Farana","VŠB - Technical University of Ostrava / Faculty of Mechanical Engineering, Department of control systems and instrumentation, Czech Republic; VŠB - Technical University of Ostrava / Faculty of Mechanical Engineering, Institute of Transport, Czech Republic; VŠB - Technical University of Ostrava / Faculty of Mechanical Engineering, Department of control systems and instrumentation, Czech republic","Proceedings of the 2015 16th International Carpathian Control Conference (ICCC)","","2015","","","300","303","The article deals with the creation of a mathematical model of a real problem with four discreet flows. Transport processes are repeated at predetermined intervals and they must be time coordinated in three coordination places deployed in a triangular configuration. The main criterion is the amount of downtime of transported elements emerging in coordination places. The optimization task is solved by methods of mathematical programming. The presented paper contains a mathematical model of the task being solved. The proposed model is tested with real data. Computational experiments were conducted in the optimization software Xpress - IVE.","","978-1-4799-7370-5978-1-4799-7369","10.1109/CarpathianCC.2015.7145093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7145093","mathematical model;transport process;mathematical programming;optimization software","Mechanical engineering;Instruments;Optimization","mathematical programming;network theory (graphs);transportation","network synchronization;periodic multiple source flows;mathematical model;discreet flows;transport process;coordination places;triangular configuration;transported element downtime;optimization task;mathematical programming;Xpress IVE optimization software","","","7","","","","","","IEEE","IEEE Conferences"
"Integrated software platform for advanced design and optimization of industrial manufacturing control system","F. A. Cavadini; D. Manzocchi; M. Mazzolini; A. Brusaferri","Synesis Consortium, Kilometro Rosso science and technology park, Via Stezzano, 87 - 24126 Bergamo, Italy; Synesis Consortium, Kilometro Rosso science and technology park, Via Stezzano, 87 - 24126 Bergamo, Italy; Synesis Consortium, Kilometro Rosso science and technology park, Via Stezzano, 87 - 24126 Bergamo, Italy; Institute of Industrial Technologies and Automation - National Research Council Via Bassini, 15 - 20133 Milano, Italy","2013 IEEE 18th Conference on Emerging Technologies & Factory Automation (ETFA)","","2013","","","1","8","This contribution presents an innovative model for development and validation of automation systems for complex and large industrial plants. An integrated software platform has been developed in order to validate and test the final control application, reducing the errors made in the development phase, improving modularity and adaptability of the control and reducing time and cost for commissioning. Exploiting a modular control architecture for the plant supervisor allows implementation of optimization algorithms inside it. The usage of a DES simulator and the integration of a time-synchronization mechanism within the complete software and control architecture enable the validation of the intelligent supervisor using the simulation model as test bed before deploying it over the real industrial plant.","1946-0740;1946-0759","978-1-4799-0864-6978-1-4799-0862","10.1109/ETFA.2013.6647996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6647996","","Control systems;Software;Automation;Computer architecture;IEC standards;Hardware","industrial control;integrated manufacturing systems;optimisation;synchronisation","integrated software platform;industrial manufacturing control system;innovative model;automation systems;modular control architecture;intelligent plant supervisor;optimization algorithms;DES simulator;time-synchronization mechanism;real industrial plant","","2","14","","","","","","IEEE","IEEE Conferences"
"Development of sequential optimizational algorithms for object detection in images","A. A. Druki; V. G. Spitsyn","Department of Computer Science, Tomsk Polytechnic University (TPU), Russia; Department of Computer Science, Tomsk Polytechnic University (TPU), Russia","2015 International Siberian Conference on Control and Communications (SIBCON)","","2015","","","1","5","Development of high quality object detection system is a challenging task, not fully solved nowadays. The relevance of this study is stipulated by the necessity of designing techniques, algorithms, and programs improving the efficiency of automatic objects detection on images with complex backgrounds. Purpose: The aim of this work is to improve the efficiency of automatic number plate detection on images with complex backgrounds using methods, algorithms, and programs invariant to affine and projective transformations. Findings: the problem of detection or detection of the number plate of the vehicle images can be effectively solved using CNN algorithms based on the adaptive boosting. Two convolutional neural networks (CNNs) with different configurations are designed. The first convolutional neural network (CNN) provides the preliminary plate detection while the second provides its final detection so as to compensate classification errors received by the first CNN. As a result, the optimally efficient training algorithm has been selected. The software system based on these algorithms is suggested to provide the high-efficiency automatic plate detection.","","978-1-4799-7103-9978-1-4799-7102","10.1109/SIBCON.2015.7147046","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7147046","convolution neural networks;artificial intelligence;adaptive algorithms;license plate detection","Neurons;Training;Convolution;Algorithm design and analysis;Classification algorithms;Testing;Vehicles","affine transforms;learning (artificial intelligence);neural nets;object detection;optimisation;road vehicles","software system;optimally efficient training algorithm;convolutional neural network;adaptive boosting;CNN algorithms;vehicle images;projective transformation;affine transformation;program invariant;automatic number plate detection;complex background;automatic object detection;sequential optimizational algorithm","","2","12","","","","","","IEEE","IEEE Conferences"
"High dynamic HIL model for complete software testing solution of Hev/Ev","V. V. Nair; P. Pathiyil","Robert Bosch Eng. and Business Solution Pvt. Ltd; Robert Bosch Eng. and Business Solution Pvt. Ltd","2015 IEEE International Transportation Electrification Conference (ITEC)","","2015","","","1","7","The paper emphasizes on development of a high dynamic HIL (Hardware In Loop) module setup for performing the complete software module test for hybrid components for any typical hybrid electric/electric vehicle. The concept elucidates road to lab ideology from real time drive patterns to testing the same for use and mis-use cases at HIL environment. A unique methodology employing Monte-carlo optimization algorithm is used to convert real world road load datas for various terrain topology to a representative drive cycle with duration less than 3600 s/cycle called real-time drive cycle (RTDC). RTDC is derived with OEM specified drive log datas. RTDC would be given as input to a high dynamic HEV/EV vehicle module together with driver inputs. Based on energy distribution, load cycle for electric machine, inverter, high voltage battery etc. are derived. The derived load cycle for hybrid components are conditioned for various thermal, environmental loads and ECU states to generate a load cycle to performing multiple component level test cases for example electric machine load cycle (speed and torque versus time). Each load cycle generated will be specific for O.E.M torque structure under restricted functionality conditions. The load cycle serves as an input to the lab test environment called Lab-drives, which is a HIL system.","","978-1-5090-1911-3978-1-5090-1912","10.1109/ITEC-India.2015.7386869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7386869","","Vehicle dynamics;Vehicles;Synchronous motors;Solid modeling;Load modeling;Inverters;Torque","hybrid electric vehicles;Monte Carlo methods;program testing","software testing;HEV;hardware in loop module;hybrid electric/electric vehicle;Monte-Carlo optimization algorithm;real-time drive cycle;OEM;energy distribution;electric machine;inverter;high voltage battery;ECU;Lab-drives","","2","15","","","","","","IEEE","IEEE Conferences"
"Application of the multi-objective Alliance Algorithm to a benchmark aerodynamic optimization problem","V. Lattarulo; T. Kipouros; G. T. Parks","Engineering Design Centre, Department of Engineering, University of Cambridge, Trumpington Street, CB2 1PZ, UK; Engineering Design Centre, Department of Engineering, University of Cambridge, Trumpington Street, CB2 1PZ, UK; Engineering Design Centre, Department of Engineering, University of Cambridge, Trumpington Street, CB2 1PZ, UK","2013 IEEE Congress on Evolutionary Computation","","2013","","","3182","3189","This paper introduces a new version of the multiobjective Alliance Algorithm (MOAA) applied to the optimization of the NACA 0012 airfoil section, for minimization of drag and maximization of lift coefficients, based on eight section shape parameters. Two software packages are used: XFoil which evaluates each new candidate airfoil section in terms of its aerodynamic efficiency, and a Free-Form Deformation tool to manage the section geometry modifications. Two versions of the problem are formulated with different design variable bounds. The performance of this approach is compared, using two indicators and a statistical test, with that obtained using NSGA-II and multi-objective Tabu Search (MOTS) to guide the optimization. The results show that the MOAA outperforms MOTS and obtains comparable results with NSGA-II on the first problem, while in the other case NSGA-II is not able to find feasible solutions and the MOAA is able to outperform MOTS.","1089-778X;1941-0026","978-1-4799-0454-9978-1-4799-0453-2978-1-4799-0451-8978-1-4799-0452","10.1109/CEC.2013.6557959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557959","","Optimization;Automotive components;Standards;Algorithm design and analysis;Aerodynamics;Geometry;Linear programming","aerodynamics;drag;genetic algorithms;mechanical engineering computing;search problems;software packages;statistical testing","multiobjective alliance algorithm;benchmark aerodynamic optimization problem;MOAA;NACA 0012 airfoil section;drag minimization;lift coefficient maximization;section shape parameters;software packages;XFoil;aerodynamic efficiency;free-form deformation tool;section geometry modification management;design variable bounds;statistical test;multiobjective tabu search;MOTS","","3","30","","","","","","IEEE","IEEE Conferences"
"Advanced diagnosis: SBST and BIST integration in automotive E/E architectures","F. Reimann; M. Glaß; J. Teich; A. Cook; L. R. Gómez; D. Ull; H. Wunderlich; U. Abelein; P. Engelke","University of Erlangen-Nuremberg, Germany; University of Erlangen-Nuremberg, Germany; University of Erlangen-Nuremberg, Germany; University of Stuttgart, Germany; University of Stuttgart, Germany; University of Stuttgart, Germany; University of Stuttgart, Germany; AUDI AG, Ingolstadt, Germany; Infineon Technologies AG, Neubiberg, Germany","2014 51st ACM/EDAC/IEEE Design Automation Conference (DAC)","","2014","","","1","6","The constantly growing amount of semiconductors in automotive systems increases the number of possible defect mechanisms, and therefore raises also the effort to maintain a sufficient level of quality and reliability. A promising solution to this problem is the on-line application of structural tests in key components, typically ECUs. In this work, an approach for the optimized integration of both Software-Based Self-Tests (SBST) and Built-In Self-Tests (BIST) into E/E architectures is presented. The approach integrates the execution of the tests non-intrusively, i. e., it (a) does not affect functional applications and (b) does not require costly changes in the communication schedules or additional communication overhead. Via design space exploration, optimized implementations with respect to multiple conflicting objectives, i. e., monetary costs, safety, test quality, and required execution time are derived.","0738-100X","978-1-4799-3017","10.1145/2593069.2602971","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881423","","Built-in self-test;Safety;Hardware;Logic gates;Automotive engineering;Optimization;System-on-chip","automotive electronics;built-in self test;computerised monitoring;nondestructive testing;semiconductor device reliability","software-based self-tests;SBST integration;built-in self-tests;BIST integration;automotive E/E architectures;automotive systems;structural tests;ECU;design space exploration","","5","20","","","","","","IEEE","IEEE Conferences"
"Virtual machine placement optimization in SDN-aware federated clouds","T. S. Somasundaram; K. Govindarajan","Department of Computer Technology, Madras Institute of Technology, Anna University, Chennai, India; Department of Computer Technology, Madras Institute of Technology, Anna University, Chennai, India","2015 IEEE International Conference on Electro/Information Technology (EIT)","","2015","","","379","385","The cloud providers provide computing, storage and network resources in an on-demand manner and pay as per usage mode. Nevertheless, the cloud consumers are facing issue to find out the suitable cloud resources that will meet their application requirements. Hence, the proposed research work introduces the cloud brokering concept that acts as the mediator between the user and the cloud provider to perform the mapping between the user application requirements with the available cloud resources. In addition to that, in this research paper, we introduced Software-Defined Networking (SDN) based networking to manage and configure the networks in a dynamic manner. However, the conventional cloud brokering concepts selects the resources from a single cloud provider; in some circumstances, it fails to satisfy the user requests in a single cloud provider. Henceforth, the cloud broker selects the cloud resources across the multiple cloud resources known as federated clouds. In these scenarios, the placement of virtual machines is the most challenging and complex issue, hence, in this research work, a Multiple Knapsack Problem (MKP) has been designed to solve the complex virtual machine placement problem in the SDN-aware federated clouds. It is simulated and tested based on the real-world application traces and the various performance metrics such as execution time, execution cost, and user satisfaction values are measured.","2154-0373;2154-0357","978-1-4799-8802-0978-1-4799-8801","10.1109/EIT.2015.7293373","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293373","Cloud Computing;Software-Defined Networking;Federated Clouds;Cloud Broker;Cloud Management System;Multiple Knapsack Problem","Virtual machining;Resource management;Quality of service;Delays;Optimization;Cloud computing","cloud computing;knapsack problems;optimisation;software defined networking;virtual machines","virtual machine placement;optimization;SDN-aware federated clouds;cloud providers;cloud consumers;software-defined networking;cloud brokering concepts;multiple knapsack problem;MKP","","","11","","","","","","IEEE","IEEE Conferences"
"Remote laboratory for testing processor cores in FPGA device","K. Saksida; A. Trost","University of Ljubljana, Faculty of Electrical Engineering, Ljubljana, Slovenia; University of Ljubljana, Faculty of Electrical Engineering, Ljubljana, Slovenia","2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","","2014","","","172","177","In the paper we present generic hardware verification structures for efficient testing of the custom processor cores in FPGA devices. Hardware verification environment consists of hardware debug structures: control machine, memory access port, input and output data buffers and interface logic. The verification structures are integrated and synthesized with the processor core under test and implemented on the FPGA device. The structures can be customized and easily integrated in the hardware development flow. A software support for the hardware verification consists of a data acquisition driver running on the server with HTML5 graphical interface. The software enables either local testing or setup of a remote laboratory for testing of the processor cores. The application of the remote laboratory in the educational process is presented. The presented hardware verification structures are optimized for testing the soft programmable processor cores and are vendor independent. The software support is based on open languages and protocols and the scripting tools enable quick customization. We present advantages of our solution compared to commercial general purpose on-chip logical analyzers. The benefit of our approach is that it can be used with the standard programmable design tools on the low cost platforms and provides two abstraction levels of debugging.","","978-953-233-077-9978-953-233-081","10.1109/MIPRO.2014.6859555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859555","","Hardware;Field programmable gate arrays;Remote laboratories;Process control;Software;Servers","data acquisition;field programmable gate arrays;graphical user interfaces;hypermedia markup languages;program debugging;program processors;program verification","remote laboratory;processor cores testing;FPGA device;generic hardware verification structures;hardware debug structures;control machine;memory access port;input-output data buffers;interface logic;hardware development flow;software support;data acquisition driver;HTML5 graphical interface;local testing;remote laboratory setup;educational process;soft programmable processor cores;scripting tools;protocols;on-chip logical analyzers;standard programmable design tools;low cost platforms","","","13","","","","","","IEEE","IEEE Conferences"
"PSO based transmission network expansion","D. Cristian; A. Simo; C. Barbulescu; S. Kilyeni","&#x0022;Politehnica&#x0022; University of Timisoara, Romania, Power Systems Department; &#x0022;Politehnica&#x0022; University of Timisoara, Romania, Power Systems Department; &#x0022;Politehnica&#x0022; University of Timisoara, Romania, Power Systems Department; &#x0022;Politehnica&#x0022; University of Timisoara, Romania, Power Systems Department","2013 48th International Universities' Power Engineering Conference (UPEC)","","2013","","","1","6","The authors are focusing on transmission network expansion planning (TNEP) based on particle swarm optimization (PSO) method. The development algorithm has been implemented within a software tool. Two case studies have been used to present the results: IEEE 24 RTS (well-known from the literature) and 25 buses test power system (developed within the Power Systems Department). The developed methodology is able to be applied in case of a complex power system.","","978-1-4799-3254","10.1109/UPEC.2013.6714958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714958","particle swarm intelligence;optimal power flow;power system;software tool;transmission network expansion","Sociology;Statistics;Software tools;Software algorithms;Load flow;Linear programming","particle swarm optimisation;power transmission planning;software tools","transmission network expansion planning;TNEP;particle swarm optimization;PSO;software tool;IEEE 24 RTS;25 buses test power system;power system department;complex power system","","","20","","","","","","IEEE","IEEE Conferences"
"Probabilistic network usage by means of equivalent bilateral exchanges. Case study for the Romanian Power System","O. Pop; C. Barbulescu; S. Kilyeni; A. Deacu; R. Schiopu; S. Renghea","Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania; Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania; Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania; Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania; Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania; Politehnica University of Timisoara, Power Systems Analysis and Optimization Research Center, Timisoara, Romania","2014 49th International Universities Power Engineering Conference (UPEC)","","2014","","","1","6","This paper analysis allocation method based on equivalent bilateral exchanges considering the existence of uncertainties from electric power systems. The theoretical part address on a software tool developed in Mathlab environment and contains a part dedicated to probabilistic power flow computing and another part allocated to network usage allocated to generators and consumers. This software tool is tested on a real system based on West, South-West and North-West parts of Romanian Power System and it provide useful results.","","978-1-4799-6557-1978-1-4799-6556","10.1109/UPEC.2014.6934811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6934811","load variation;Monte Carlo simulation;network usage;equivalent bilateral exchanges;uncertainties","Generators;Probabilistic logic;Probability;Load flow;Resource management;Software tools","load flow;Monte Carlo methods","probabilistic network usage;equivalent bilateral exchanges;Romanian power system;electric power systems;Mathlab environment;probabilistic power flow computing","","","15","","","","","","IEEE","IEEE Conferences"
"Schnauzer: scalable profiling for likely security bug sites","W. Arthur; B. Mammo; R. Rodriguez; T. Austin; V. Bertacco","Advanced Computer Architecture Laboratory, University of Michigan, Ann Arbor; Advanced Computer Architecture Laboratory, University of Michigan, Ann Arbor; Advanced Computer Architecture Laboratory, University of Michigan, Ann Arbor; Advanced Computer Architecture Laboratory, University of Michigan, Ann Arbor; Advanced Computer Architecture Laboratory, University of Michigan, Ann Arbor","Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)","","2013","","","1","11","Software bugs comprise the greatest threat to computer security today. Though enormous effort has been expended on eliminating security exploits, contemporary testing techniques are insufficient to deliver software free of security vulnerabilities. In this paper we propose a novel approach to security vulnerability analysis: dynamic control frontier profiling. Security exploits are often buried in rarely executed code paths hidden just beyond the path space explored by end-users. Therefore, we develop Schnauzer, a distributed sampling technology to discover the dynamic control frontier, which forms the line of demarcation between dynamically executed and unseen paths. This frontier may then be used to direct tools (such as white-box fuzz testers) to attain a level of testing coverage currently unachievable. We further demonstrate that the dynamic control frontier paths are a rich source of security bugs, sensitizing many known security exploits.","","978-1-4673-5525-4978-1-4673-5524","10.1109/CGO.2013.6494998","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494998","","Security;Testing;Computer bugs;Instruments;Software;Buffer overflows;Sociology","distributed processing;program debugging;program diagnostics;program testing;security of data","coverage testing;distributed sampling technology;code paths;dynamic control frontier profiling;security vulnerability analysis;contemporary testing techniques;computer security;software bugs;security bug sites;scalable profiling;Schnauzer","","","35","","","","","","IEEE","IEEE Conferences"
"Approximating Attack Surfaces with Stack Traces","C. Theisen; K. Herzig; P. Morrison; B. Murphy; L. Williams","NA; NA; NA; NA; NA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","2","","199","208","Security testing and reviewing efforts are a necessity for software projects, but are time-consuming and expensive to apply. Identifying vulnerable code supports decision-making during all phases of software development. An approach for identifying vulnerable code is to identify its attack surface, the sum of all paths for untrusted data into and out of a system. Identifying the code that lies on the attack surface requires expertise and significant manual effort. This paper proposes an automated technique to empirically approximate attack surfaces through the analysis of stack traces. We hypothesize that stack traces from user-initiated crashes have several desirable attributes for measuring attack surfaces. The goal of this research is to aid software engineers in prioritizing security efforts by approximating the attack surface of a system via stack trace analysis. In a trial on Windows 8, the attack surface approximation selected 48.4% of the binaries and contained 94.6% of known vulnerabilities. Compared with vulnerability prediction models (VPMs) run on the entire codebase, VPMs run on the attack surface approximation improved recall from .07 to .1 for binaries and from .02 to .05 for source files. Precision remained at .5 for binaries, while improving from .5 to .69 for source files.","0270-5257;1558-1225","978-1-4799-1934","10.1109/ICSE.2015.148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7202964","stack traces;security;vulnerability;models;testing;reliability;attack surface","Computer crashes;Security;Measurement;Approximation methods;Software;Predictive models;Surface treatment","decision making;program diagnostics;project management;software engineering","attack surface approximation;security testing;effort reviewing;software projects;vulnerable code identification;decision-making;software development;stack trace analysis;attack surface measurement;Windows 8","","8","44","","","","","","IEEE","IEEE Conferences"
"Adaptive hierarchical motion estimation optimization for scalable HEVC","A. Abdelazim; A. M. Hamza","Dept. Electrical and Computer Engineering, American University of the Middle-East, Eqaila, Kuwait; Dept. Electrical and Computer Engineering, American University of the Middle-East, Eqaila, Kuwait","2015 IEEE 8th GCC Conference & Exhibition","","2015","","","1","5","The scalable extension of the HEVC Video Coding Standard (H.265) offers elaborate mechanisms for motion vector prediction and estimation. S-HEVC builds on the standard by extending predictor lists for Coding Unit blocks, utilizing base-layer information in the inference of enhancement-layer Coding Units. The complex, exhaustive search schemes in use can be aided by hierarchical optimizations in subpixel motion estimation, which we propose for slow-moving CUs per frame. In this paper we implement and test an adaptive optimization of motion estimation in the standard (SHM 6.1 software release), based on a statistical analysis of the behavior of subpixel motion vector differentials in each spatial mode per Coding Unit. We propose that the least granular mode (64×64 PEL macro-block in current release) contains sufficient information at subpixel levels to decide best-mode selection, i.e., whether a complete recursion through the inner partitions (higher granularity) is required in the estimation of a CU motion vector. We further propose that subpixel motion estimation overheads can be avoided below a set threshold, given conditions set in base and enhancement layer motion estimation for priorly computed modes in the same CU. Both optimization methods are tested across a diverse set of video sequences, producing negligible quality penalties at for a sizable reduction in encoding time.","","978-1-4799-8422","10.1109/IEEEGCC.2015.7060088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7060088","","Encoding;Motion estimation;Standards;Vectors;Scalability;Video coding;Optimization","adaptive estimation;data compression;motion estimation;optimisation;video codecs;video coding","adaptive hierarchical motion estimation optimization;S-HEVC;video coding standard;H.265;motion vector prediction;motion vector estimation;enhancement-layer coding units;least granular mode","","","7","","","","","","IEEE","IEEE Conferences"
"Oil extraction optimization from Xanthoceras sorbifolia with response surface methodology","D. Liu; Y. Zhao; J. Shen","Life science college, Jiamusi University Jiamusi, China; Life science college, Jiamusi University Jiamusi, China; Life science college, Jiamusi University Jiamusi, China","2013 International Conference on Materials for Renewable Energy and Environment","","2013","1","","274","277","This paper has discussed the ultrasonic oil extraction from Xanthoceras sorbifolia (Yellowhorn) seeds. The oil yield ratio and its range have been claimed by single-factor test. Taking the ultrasonic power, ultrasonic time and solid-liquid ratio as the independent variables and oil yield as the response value, the research was carried out to study the interaction among the independent variables and the affect to oil yield by Box-Behnken methodology. The stimulated predicted model of regression equation was conducted by Design Expert software. The study revealed that, when choosing petroleum ether as the solvent, the extraction temperature and time are 60°C and 3 hours. The optimum condition for extracting yellowhorn oil was using ultrasonication power: 300 W, ultrasonic time: 16 minutes and solid-liquid ratio: 1:8 (g: ml). The oil yield ratio was 60.95% whose relative deviation was 0.58% compared with the predicted value, which was 61.53%. This demonstrated that this model reasonably applies to the oil extraction ratio of yellowhorn.","","978-1-4799-3336-5978-1-4799-3335-8978-1-4799-3334","10.1109/ICMREE.2013.6893664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6893664","Xanthoceras sorbifolia;ultrasonication;Response surface methodology (RSM);oil yield","Acoustics;Mathematical model;Response surface methodology;Equations;Software;Petroleum;Optimization","biofuel;ultrasonic applications;vegetable oils","oil extraction optimization;Xanthoceras sorbifolia;response surface methodology;ultrasonic oil extraction;ellowhorn seeds;solid-liquid ratio;Box-Behnken methodology;regression equation;Design Expert software;petroleum ether solvent;ultrasonication process;power 300 W;time 16 min","","","11","","","","","","IEEE","IEEE Conferences"
"A PSO-optimized type-2 fuzzy logic controller for navigation of multiple mobile robots","Z. T. Allawi; T. Y. Abdalla","College of Education for Humanitarian Studies, University of Baghdad, Baghdad, Iraq; Department of Computer Engineering, University of Basrah, Basrah, Iraq","2014 19th International Conference on Methods and Models in Automation and Robotics (MMAR)","","2014","","","33","39","In this paper, an Interval Type-2 Fuzzy Logic controller was used to control the robot cooperation and target reaching tasks during navigation for multiple mobile robots. This controller had been optimized by the Particle Swarm Optimization algorithm and the Hybrid Reciprocal Velocity Obstacles algorithm was also used to handle collision avoidance. This controller was tested on Webots™ robot simulation software and experimentally applied on two real E-puck mobile robots. The simulation and experimental results had shown that the optimized controller outperformed the same controller without optimization.","","978-1-4799-5081-2978-1-4799-5082","10.1109/MMAR.2014.6957321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6957321","Particle Swarm Optimization;Interval Type-2 Fuzzy Logic;Hybrid Reciprocal Velocity Obstacles;Navigation;Multiple Mobile Robots","Mobile robots;Collision avoidance;Optimization;Fuzzy logic;Robot sensing systems","collision avoidance;control engineering computing;fuzzy control;mobile robots;particle swarm optimisation","PSO-optimized type-2 fuzzy logic controller;particle swarm optimization;robot navigation;interval type-2 fuzzy logic controller;robot cooperation;target reaching task;hybrid reciprocal velocity obstacles algorithm;collision avoidance;Webots robot simulation software;E-puck mobile robots","","5","31","","","","","","IEEE","IEEE Conferences"
"Rate-distortion optimization with adaptive weighted distortion in high Efficiency Video Coding","B. Li; J. Xu; H. Li","University of Science and Technology of China, Hefei, China; Microsoft Research Asia, Beijing, China; University of Science and Technology of China, Hefei, China","2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)","","2013","","","481","484","This paper presents an adaptive weighted distortion optimization algorithm used in the Rate-Distortion Optimization (RDO) process of the High Efficiency Video Coding (HEVC). RDO is an important tool to improve the coding efficiency. Usually the distortion weights of different color components are equal or predetermined. In this paper, an adaptive weighted distortion optimization algorithm is introduced to improve the coding efficiency. The distortion weight is estimated according to the previous coded pictures belonging to the same temporal level, such that encoding complexity is almost unchanged. With the proposed adaptive weighted distortion optimization method, on average about 3.3% and up to 10.6% bit-saving are obtained based on the latest HEVC reference software, HM-8.0 and the corresponding common test conditions. The proposed algorithm can also be applied to other coding schemes such as H.264/MPEG-4 AVC.","0271-4302;2158-1525","978-1-4673-5762-3978-1-4673-5760-9978-1-4673-5761","10.1109/ISCAS.2013.6571885","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571885","","Encoding;Optimization;Video coding;PSNR;Algorithm design and analysis;Transform coding;Color","distortion;image colour analysis;video coding","rate-distortion optimization;high efficiency video coding;adaptive weighted distortion optimization algorithm;RDO process;coding efficiency;color components;distortion weight;encoding complexity;adaptive weighted distortion optimization method;HEVC reference software;HM-8.0;coding schemes;H.264/MPEG-4 AVC","","","10","","","","","","IEEE","IEEE Conferences"
"Energy consumption in reconfigurable mpsoc architecture: Two-level caches optimization oriented approach","A. Bengueddach; B. Senouci; S. Niar; B. Beldjilali","University of Oran Es-Senia Department of Computer Science BP 1524, El-M'Naouer 31000, Oran Algérie; University of Valenciennes Hainaut-Cambrésis ISTV2 - Le Mont Houy LAMIH Laoratory 59313 Valenciennes Cedex9; University of Valenciennes Hainaut-Cambrésis ISTV2 - Le Mont Houy LAMIH Laoratory 59313 Valenciennes Cedex9; University of Oran Es-Senia Department of Computer Science BP 1524, El-M'Naouer 31000, Oran Algérie","2013 8th IEEE Design and Test Symposium","","2013","","","1","6","In order to meet the ever-increasing computing requirement in embedded market, multiprocessor chips were proposed as the best way out. In this work we investigate the estimation of the energy consumption in embedded MPSoC system. One of the efficient solutions to reduce the energy consumption is to reconfigure the caches memories. This approach was applied for one cache level/one processor architecture. The main contribution of this paper is to explore two level data cache (L1/L2) multiprocessor architecture by estimating the energy consumption. Using a simulation platform (Multi2Simj, we first built a multiprocessor architecture, and then we propose a new modified CPACT algorithm that tunes the two-level caches memory hierarchy (L1 & L2). The caches tuning approach is based on three parameters: cache size, line size, and associativity. In this approach, and in order to find the best cache configuration, the software application is divided into several intervals and we generate automatically the best cache configuration for each interval of the application. Finally, the approach is validated using a set of open source benchmarks, Spec2006, Splash-2 and MediaBench and we discuss the performance in terms of speedup and energy reduction.","2162-0601;2162-061X","978-1-4799-3525","10.1109/IDT.2013.6727118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727118","Embedded system;MPSoC;Cache memories;Reconfigurable architecture;Energy consumption;Optimization","Tuning;Energy consumption;Computer architecture;Software;Benchmark testing;Hardware;Load modeling","embedded systems;microprocessor chips;system-on-chip","energy consumption reduction;reconfigurable MPSoC architecture;two-level caches optimization oriented approach;computing requirement;embedded system;one cache level;one processor architecture;L1-L2 multiprocessor architecture;simulation platform;Multi2Sim;modified CPACT algorithm;caches tuning approach;associativity;software application;cache size;line size;open source benchmarks;Spec2006;Splash-2;MediaBench;speedup;multiprocessor system on chip","","","16","","","","","","IEEE","IEEE Conferences"
"A K-means clustering with optimized initial center based on Hadoop platform","Kunhui Lin; Xiang Li; Zhongnan Zhang; Jiahong Chen","Software School of Xiamen University, China; Software School of Xiamen University, China; Software School of Xiamen University, China; Software School of Xiamen University, China","2014 9th International Conference on Computer Science & Education","","2014","","","263","266","With the explosive growth of data, the traditional clustering algorithms running on separate servers can not meet the demand. To solve the problem, more and more researchers implement the traditional clustering algorithms on the cloud computing platforms, especially for K-means clustering. But, few researchers pay attention to the K-means clustering structure, and most of researchers optimized the model of the cloud computing platform to raise the computing speed of K-means clustering. However the problem of instability caused by the random initial centers still exists. In this paper, we propose a K-means clustering algorithm with optimized initial centers based on data dimensional density. This method avoids the deficiency of the random initial centers and improves the stability of the K-means clustering. The experimental results show that the approach achieves a good performance on K-means, and improves the accuracy of K-means clustering on the test set.","","978-1-4799-2951-1978-1-4799-2949","10.1109/ICCSE.2014.6926466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926466","MapReduce;K-means clustering;Initial center;Density","Educational institutions;Clustering algorithms;Computers","cloud computing;data handling;pattern clustering","K-means clustering;optimized initial center;Hadoop platform;cloud computing platforms;random initial centers;data dimensional density;MapReduce programming model","","","9","","","","","","IEEE","IEEE Conferences"
"Static strength and durability analysis of the gear-rack for an aircraft slat","T. Zhang; W. Cui; T. Yu; B. Song","School of Aeronautics, Northwestern Polytechnical University, Xi'an 710072, P.R. China; School of Aeronautics, Northwestern Polytechnical University, Xi'an 710072, P.R. China; School of Aeronautics, Northwestern Polytechnical University, Xi'an 710072, P.R. China; School of Aeronautics, Northwestern Polytechnical University, Xi'an 710072, P.R. China","2013 International Conference on Quality, Reliability, Risk, Maintenance, and Safety Engineering (QR2MSE)","","2013","","","217","220","Duo to the limitation of the gear-rack's structure profile for an aircraft slat, the maximum stress value got from test is not the value under actual load. In this paper, based on finite element model of gear-rack built by analysis software ABAQUS, the maximum stress value of gear-rack is determined by force analysis. Thus the strength of the gear-rack is checked, and finally the fatigue life is estimated. At the same time, strength test of gear-rack shows that calculation results and test are well consistent. This method can provide a foundation for the design optimization and reliability design of gear-rack for aircraft slat.","","978-1-4799-1016-8978-1-4799-1014","10.1109/QR2MSE.2013.6625569","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625569","gear-rack;finite element analysis;strength;fatigue life;reliability;ABAQUS","Stress;Finite element analysis;Fatigue;Gears;Aircraft;Materials;Reliability","aerospace components;design engineering;fatigue testing;finite element analysis;gears;mechanical strength;optimisation;reliability;stress analysis","reliability design;design optimization;strength test;fatigue life;maximum stress value;ABAQUS;analysis software;finite element model;gear-rack structure profile;aircraft slat;durability analysis;static strength","","","10","","","","","","IEEE","IEEE Conferences"
"Lightweight fault detection in parallelized programs","L. Tan; M. Feng; R. Gupta","CSE Department, UC Riverside; NEC Laboratories America; CSE Department, UC Riverside","Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)","","2013","","","1","11","A popular approach for producing parallel software is to develop a sequential version of an application and then incrementally introduce parallel constructs to parallelize different parts of the application. During the parallelization process, programming errors may be introduced, causing concurrency bugs. In this paper we develop a technique for runtime detection of data dependence faults (i.e., data races and atomicity violations) introduced during parallelization. By leveraging the availability of two versions of the program, the sequential one and the parallelized one, we comparison check dynamic data dependences exercised during executions of the two versions to identify faults. To reduce the cost of comparison checking we develop three optimizations. The first optimization causes only a subset of dynamically exercised data dependences to be comparison checked. The second optimization shows that not all instances of a dynamically exercised data dependence need to be comparison checked. The third optimization shows that static analysis of parallelizing constructs can be exploited to eliminate the need for executing the parallelized version altogether. In addition, our solution is applicable when different program executions on the same input may follow different execution paths, it is effective in situations where the fault introduced manifests itself rarely during execution, and it is also effective in pinpointing the location of the fault in the program. We implemented and evaluated our approach using ten benchmarks. The experimental results indicate an average slowdown of 3× to perform fault detection.","","978-1-4673-5525-4978-1-4673-5524","10.1109/CGO.2013.6494979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494979","program parallelization;debugging;dependence violation;data races;atomicity violations","Optimization;Computer bugs;Benchmark testing;Concurrent computing;Debugging;Pipeline processing","benchmark testing;concurrency control;parallel programming;program diagnostics;program verification;software fault tolerance","lightweight fault detection;parallelized programs;parallel software;sequential application version;parallelization process;programming errors;concurrency bugs;run-time data dependence fault detection technique;data races;atomicity violations;dynamic data dependences;comparison checking;static analysis;program executions;program fault location pinpointing;benchmarks","","1","34","","","","","","IEEE","IEEE Conferences"
"Nested Pattern Queries Processing Optimization over Multi-dimensional Event Streams","F. Xiao; M. Aritsugi","NA; NA","2013 IEEE 37th Annual Computer Software and Applications Conference","","2013","","","74","83","Recently, many modern applications have required complex event processing technology to analyze multi-dimensional stream big data available in real-time data feeds. To address the above requirement, we develop a novel real-time event stream processing method called multi-query optimization strategy (MQOS). MQOS aims at exploiting not only common sub-expressions among nested event pattern queries, but also replicas of the appropriate common operators' results for the queries needed to minimize recalculation and re-communication costs. We first design a triaxial hierarchy consisting of nested query pattern, nested query concept and operator type hierarchies to specify the relationship among the sub-expressions of queries. Next, based on the triaxial hierarchy, we devise a cost-based heuristic to find an optimized query execution plan with minimum costs of operators and communications. We then propose three reuse schemes of common sub-expressions: nested query pattern-based, nested query concept-based and operator type-based reuse schemes. By integrating the optimized query execution plan-find approach with the three reuse schemes, we present the MQOS to achieve nested pattern queries processing optimization. Finally, our experiments tested on StreamBase under different workload conditions demonstrate the superiority of MQOS.","0730-3157","978-0-7695-4986","10.1109/COMPSAC.2013.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649801","Complex event processing (CEP);Multi-dimensional event stream;Nested pattern query","Optimization;Query processing;Companies;Time factors;Metals;Information management;Data handling","optimisation;query processing","optimized query execution plan-find approach;operator type hierarchies;nested query concept;nested query pattern;triaxial hierarchy;MQOS;multi-query optimization strategy;novel real-time event stream processing method;multi dimensional stream big data;complex event processing technology;multidimensional event streams;nested pattern queries processing optimization","","1","22","","","","","","IEEE","IEEE Conferences"
"Data-Delineation in Software Binaries and its Application to Buffer-Overrun Discovery","D. Gopan; E. Driscoll; D. Nguyen; D. Naydich; A. Loginov; D. Melski","NA; NA; NA; NA; NA; NA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","1","","145","155","Detecting memory-safety violations in binaries is complicated by the lack of knowledge of the intended data layout, i.e., the locations and sizes of objects. We present lightweight, static, heuristic analyses for recovering the intended layout of data in a stripped binary. Comparison against DWARF debugging information shows high precision and recall rates for inferring source-level object boundaries. On a collection of benchmarks, our analysis eliminates a third to a half of incorrect object boundaries identified by an IDA Pro-inspired heuristic, while retaining nearly all valid object boundaries. In addition to measuring their accuracy directly, we evaluate the effect of using the recovered data for improving the precision of static buffer-overrun detection in the defect-detection tool CodeSonar/x86. We demonstrate that CodeSonar's false-positive rate drops by about 80% across our internal evaluation suite for the tool, while our approximation of CodeSonar's recall only degrades about 25%.","0270-5257;1558-1225","978-1-4799-1934","10.1109/ICSE.2015.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194569","reverse engineering;data delineation;binary analysis;static analysis;buffer overrun detection","Registers;Layout;Benchmark testing;Approximation methods;Libraries;Accuracy;Optimization","program diagnostics;security of data;software tools","software security;CodeSonar/x86;defect-detection tool;static analysis;memory-safety violation detection;buffer-overrun discovery;software binary;DDA;data delineation analysis","","2","29","","","","","","IEEE","IEEE Conferences"
"The Effect of GoF Design Patterns on Stability: A Case Study","A. Ampatzoglou; A. Chatzigeorgiou; S. Charalampidou; P. Avgeriou","Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands; Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece; Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands; Institute of Mathematics and Computer Science, University of Groningen, Groningen, Netherlands","IEEE Transactions on Software Engineering","","2015","41","8","781","802","Stability refers to a software system's resistance to the “ripple effect”, i.e., propagation of changes. In this paper, we investigate the stability of classes that participate in instances/occurrences of GoF design patterns. We examine whether the stability of such classes is affected by (a) the pattern type, (b) the role that the class plays in the pattern, (c) the number of pattern occurrences in which the class participates, and (d) the application domain. To this end, we conducted a case study on about 65.000 Java open-source classes, where we performed change impact analysis on classes that participate in zero, one (single pattern), or more than one (coupled) pattern occurrences. The results suggest that, the application of design patterns can provide the expected “shielding” of certain pattern-participating classes against changes, depending on their role in the pattern. Moreover, classes that participate in coupled pattern occurrences appear to be the least stable. The results can be used for assessing the benefits and liabilities of the use of patterns and for testing and refactoring prioritization, because less stable classes are expected to require more effort while testing, and urge for refactoring activities that would make them more resistant to change propagation.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2015.2414917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7066925","D.2.2 Design Tools and Techniques;D.2.3.a Object-oriented programming,;D.2.8 Metrics/Measurement;Design Tools and Techniques;Object-oriented programming;Metrics/Measurement","Stability analysis;Couplings;Abstracts;Measurement;Production facilities;Open source software","Java;object-oriented programming;public domain software","GoF design pattern;stability;software system resistance;ripple effect;Java open-source class;change impact analysis;coupled pattern occurrence;pattern-participating class;refactoring prioritization;refactoring activity;change propagation","","15","53","","","","","","IEEE","IEEE Journals & Magazines"
"Acceptance-Based Software Architecture Deployment for Improvement of Existing Applications","H. Klee; M. Buchholz; T. Materna; K. Dietmayer","NA; NA; NA; NA","2015 IEEE Symposium Series on Computational Intelligence","","2015","","","1832","1837","A lot of approaches are already published to solve software architecture deployment problems. Most of them are intended for academic use and assume that the software components can be deployed freely on the hardware components. But for an improvement of existing applications, non-functional constraints will have a high influence on the acceptance of the automatically generated solutions. In this paper, the organization of the engineers and their tasks as well as the amount of changes regarding to a currently applied system are considered. To gain a smart and reduced interface between hardware components, a method is presented to reduce the communication overhead for an existing architecture. Additionally, the deployment problem is restricted by the amount of changes in comparison to an initial deployment. This approach is tested on a realistic case study to show that it is possible to achieve high improvements with only small changes of the system.","","978-1-4799-7560","10.1109/SSCI.2015.255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7376832","","Hardware;Software design;Software architecture;Automotive engineering;Industries;Optimization","object-oriented programming;software architecture","acceptance-based software architecture deployment;software components;hardware components;nonfunctional constraints;communication overhead","","2","15","","","","","","IEEE","IEEE Conferences"
"WSS/EDFA-based optimization strategies for software defined optical networks","H. Carvalho; M. Svolenski; M. Garrich; M. Nascimento; F. Margarido; F. Cabelo; L. Mariote; A. C. Bordonalli; J. Oliveira","CPqD, Campinas, São Paulo, Brazil; CPqD, Campinas, São Paulo, Brazil; CPqD, Campinas, São Paulo, Brazil; CPqD, Campinas, São Paulo, Brazil; CPqD, Campinas, São Paulo, Brazil; CPqD, Campinas, São Paulo, Brazil; CPqD, Campinas, São Paulo, Brazil; School of Electrical and Computer Engineering, University of Campinas, São Paulo, Brazil; CPqD, Campinas, São Paulo, Brazil","2015 SBMO/IEEE MTT-S International Microwave and Optoelectronics Conference (IMOC)","","2015","","","1","5","Global optimization of optical network elements (NEs) is a potential solution to provide optical signal-to-noise ratio (OSNR) requirements for high spectrally-efficient (SE) modulation formats, essential in next-generation optical networks. In this context, software defined networking (SDN) is a suitable paradigm that allows for global monitoring and NEs actuation by decoupling data and control planes. By taking that into account, we review our recently proposed SDN dual-optimization application for EDFAs and WSS-based ROADMs which targets the optimization of the OSNR. In this work we detail the implementation of the application based on a state-machine approach. The application is tested using a SDN controller in a metropolitan optical network testbed. Experimental results show OSNR improvements of up to 10 dB for different application strategies and for different number of ROADM nodes in cascade.","","978-1-4673-9492-5978-1-5090-0431","10.1109/IMOC.2015.7369086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7369086","Equalization;optical links;OSNR;ROADMs;SDN","Attenuation;Optical noise;Erbium-doped fiber amplifiers;Equalizers;Optical filters;Signal to noise ratio","erbium;next generation networks;optical fibre amplifiers;optical fibre networks;software defined networking","WSS/EDFA-based optimization strategies;software defined optical networks;optical network elements;optical signal-to-noise ratio;OSNR;next-generation optical networks;software defined networking;WSS-based ROADM;metropolitan optical network testbed","","2","6","","","","","","IEEE","IEEE Conferences"
"Comparing a hybrid branch and bound algorithm with evolutionary computation methods, local search and their hybrids on the TSP","Y. Jiang; T. Weise; J. Lässig; R. Chiong; R. Athauda","Joint USTC-Birmingham Research Institute in Intelligent Computation and Its Applications (UBRI), School of Computer Science and Technology, University of Science and Technology of China; Hefei, Anhui, China, 230027; Joint USTC-Birmingham Research Institute in Intelligent Computation and Its Applications (UBRI), School of Computer Science and Technology, University of Science and Technology of China; Hefei, Anhui, China, 230027; Department of Computer Science, University of Applied Sciences Zittau/Görlitz; D-02826, Germany; Faculty of Science and Information Technology, The University of Newcastle; Callaghan, NSW 2308, Australia; Faculty of Science and Information Technology, The University of Newcastle; Callaghan, NSW 2308, Australia","2014 IEEE Symposium on Computational Intelligence in Production and Logistics Systems (CIPLS)","","2014","","","148","155","Benchmarking is one of the most important ways to investigate the performance of metaheuristic optimization algorithms. Yet, most experimental algorithm evaluations in the literature limit themselves to simple statistics for comparing end results. Furthermore, comparisons between algorithms from different “families” are rare. In this study, we use the TSP Suite - an open source software framework - to investigate the performance of the Branch and Bound (BB) algorithm for the Traveling Salesman Problem (TSP). We compare this BB algorithm to an Evolutionary Algorithm (EA), an Ant Colony Optimization (ACO) approach, as well as three different Local Search (LS) algorithms. Our comparisons are based on a variety of different performance measures and statistics computed over the entire optimization process. The experimental results show that the BB algorithm performs well on very small TSP instances, but is not a good choice for any medium to large-scale problem instances. Subsequently, we investigate whether hybridizing BB with LS would give rise to similar positive results like the hybrid versions of EA and ACO have. This turns out to be true - the “Memetic” BB algorithms are able to improve the performance of pure BB algorithms significantly. It is worth pointing out that, while the results presented in this paper are consistent with previous findings in the literature, our results have been obtained through a much more comprehensive and solid experimental procedure.","","978-1-4799-4500","10.1109/CIPLS.2014.7007174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007174","","Algorithm design and analysis;Runtime;Software algorithms;Cities and towns;Complexity theory;Optimization;Benchmark testing","ant colony optimisation;evolutionary computation;public domain software;search problems;travelling salesman problems;tree searching","memetic BB algorithms;ACO;EA;large-scale problem instances;TSP instances;performance measures;local search algorithms;ant colony optimization approach;evolutionary algorithm;traveling salesman problem;open source software framework;TSP Suite;metaheuristic optimization algorithms;hybrid branch and bound algorithm","","3","30","","","","","","IEEE","IEEE Conferences"
"Determining the Factors Influencing Enterprise Social Software Usage: Development of a Measurement Instrument for Empirical Assessment","M. Kügler; S. Smolnik; P. Raeth","NA; NA; NA","2013 46th Hawaii International Conference on System Sciences","","2013","","","3635","3644","With social software's rapid advancement in the private realm, corporations aim to adopt its technologies and to transfer social software benefits, such as enhanced collaboration and knowledge sharing, to their organizations. However, enterprise social software platforms (ESSPs) often lack substantial attention. While some research into users' motives to adopt social software in the private realm exists, the body of empirical studies on ESSP adoption is rather limited. Our research objective therefore is to contribute to closing this gap in research by determining the factors influencing employees' ESSP usage. Building on previous research, we present an ESSP adoption model. We then propose an according measurement instrument for empirically assessing the factors influencing ESSP usage. A card sorting and item ranking approach is conducted in order to preliminarily validate the measurement instrument. We eventually suggest methods of classical test theory for the further validation of the measurement instrument and the assessment of the ESSP adoption model.","1530-1605;1530-1605","978-1-4673-5933-7978-0-7695-4892","10.1109/HICSS.2013.173","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6480284","Social software;Enterprise 2.0;Information systems adoption;Social capital;Innovation diffusion;Organizational climate;Measurement instrument","Software;Organizations;Technological innovation;Context;Instruments;Collaboration;Meteorology","knowledge management;personnel;social sciences computing;sorting;technology management","enterprise social software usage influencing factor determination;empirical assessment;measurement instrument development;corporation technology adoption;social software benefits;knowledge sharing;corporation collaboration;enterprise social software platforms;employee ESSP usage;ESSP adoption model;item ranking approach;card sorting approach","","12","101","","","","","","IEEE","IEEE Conferences"
"An optimized GPU-accelerated FDTD method for microwave imaging using a fast nonlinear inverse scattering algorithm","G. Chen; J. Stang; M. Haynes; M. Moghaddam","The University of Southern California, Los Angeles, CA, 90089; The University of Southern California, Los Angeles, CA, 90089; The University of Southern California, Los Angeles, CA, 90089; The University of Southern California, Los Angeles, CA, 90089","2014 USNC-URSI Radio Science Meeting (Joint with AP-S Symposium)","","2014","","","251","251","Summary form only given: Microwave imaging has received considerable attention as a low cost, non-invasive, non-ionizing method for breast cancer detection. In previous work, we have presented a time-domain nonlinear inverse scattering algorithm with multiparameter optimization for microwave imaging. In order to apply this algorithm to an experimental system that we have developed, it is crucial to have an accurate forward model of the imaging cavity. In this presentation, the modeling of the cavity using an in-house GPU accelerated finite-difference-time-domain (FDTD) method will be introduced, demonstrating several optimizations for increased computational efficiency and accuracy. S-parameter simulations of the cavity antennas comparing the results with the commercial software packages Ansys HFSS and CST MWS will be shown. Finally, results from microwave imaging tests of our GPU accelerated inversion algorithm using this fast forward model for both breast cancer detection and for real-time thermal monitoring of focused hyperthermia will be presented.The imaging cavity is a dodecagon consisting of 12 panels. Each panel has three dual-band bow-tie patch antennas operating at 915MHZ and 2.1GHz. In order to accurately capture the fine geometry of the cavity, we have utilized a nonuniform orthogonal mesh. The electrical field grid distance varies slowly in each direction, while the magnetic field resides in the middle of two adjacent electrical field. Though in this scenario the electrical field no long resides in the middle of two adjacent magnetic field points, which may result in first-order error locally, it has been shown by Monk1 that second-order error can still be achieved globally. In addition, we exploit the fact that within one Yee cell, the electrical field and magnetic field in each direction are half grids away to create an anisotropically filled Yee grid. This implementation maintains the accuracy of the cavity model with reduced grids and thus reduced cost of computation.","","978-1-4799-3746","10.1109/USNC-URSI.2014.6955634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6955634","","Cavity resonators;Computational modeling;Microwave imaging;Microwave theory and techniques;Microwave antennas;Finite difference methods","cancer;electric fields;error analysis;finite difference time-domain analysis;graphics processing units;magnetic fields;microstrip antennas;microwave imaging;multifrequency antennas;optimisation;patient diagnosis;S-parameters","second-order error;Yee cell;anisotropically filled Yee grid;first-order error;magnetic field point;electrical field grid distance;nonuniform orthogonal mesh;fine geometry capture;dual-band bow-tie patch antenna;dodecagon;hyperthermia;thermal monitoring;GPU accelerated inversion algorithm;CST MWS;Ansys HFSS;commercial software package;cavity antenna;S-parameter simulation;finite-difference-time-domain method;imaging cavity;multiparameter optimization;time-domain nonlinear inverse scattering algorithm;breast cancer detection;nonionizing method;fast nonlinear inverse scattering algorithm;microwave imaging;optimized GPU-accelerated FDTD method;frequency 915 MHz;frequency 2.1 GHz","","","1","","","","","","IEEE","IEEE Conferences"
"Geographical Test Data Generation by Simulated-Annealing","K. Hou; J. Huang; X. Bai","NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","3","","472","477","Location-based services (LBS) have been widely used in many applications like navigation and recommendation, especially for mobile systems. Testing of LBS services is hard due to the difficulties to cover location areas and to evaluate the correctness of location information for LBS query with a given address. To cope with the difficulties, the paper proposed a testing framework to search geographical test data using simulated-annealing algorithm, and to validate the query results by comparing across different LBS platforms. The heuristic search is based on defect clustering assumption. That is, defects intend to cluster in certain areas. Hence, areas with detected defects deserve more test cases in the follow-up testing. A Bayes classifier is used to predict defect probability of geographical areas, and to guide position data generation in the annealing algorithm. Experiments on real LBS platforms showed that with acceptable cost, the proposed method can considerably enhance test effectiveness.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.258","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273406","Test Data Generation;Location-Based Services;Simulated-Annealing","Testing;Optimization;Androids;Humanoid robots;Cooling;Search problems;Temperature","Bayes methods;geographic information systems;mobile computing;navigation;query processing;recommender systems;simulated annealing","geographical test data generation;simulated annealing;location-based services;navigation;recommendation;mobile systems;LBS query;Bayes classifier;defect probability","","","12","","","","","","IEEE","IEEE Conferences"
"Automatic text summarization based on multi-agent particle swarm optimization","H. Asgari; B. Masoumi; O. S. Sheijani","Department of Computer and Information Technology Engineerin Science and Research Branch, Islamic Azad University; Department of Computer and g Information Technology Engineering Qazvin Islamic Azad University Qazvin, Iran; Department of Computer and Information Technology Engineering Qazvin Islamic Azad University Qazvin, Iran","2014 Iranian Conference on Intelligent Systems (ICIS)","","2014","","","1","5","Text summarization is the objective extraction of some parts of the text, such as sentence and paragraph, as the document abstract. If there are documents with a large amount of information, extractive text summarization would be arisen as an NP-complete problem. To solve these problems, metaheuristic algorithms are used. In this paper, a method based on multi-agent particle swarm optimization approach is proposed to improve the extractive text summarization. In this method, each particle will be upgraded with the status of multi-agent systems. The proposed method is tested on DUC 2002 standard documents and analyzed by ROUGE evaluation software. The experimental results show that this method has better performance than other compared methods.","","978-1-4799-3351-8978-1-4799-3350","10.1109/IranianCIS.2014.6802592","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6802592","Text summarization;Particle swarm optimization;Multi-Agent Systems;Extractive method","Particle swarm optimization;Data mining;Computers;Equations;Information technology;Software;Mathematical model","multi-agent systems;particle swarm optimisation;text analysis","automatic text summarization;multiagent particle swarm optimization;objective text extraction;document abstract;NP-complete problem;meta-heuristic algorithms;extractive text summarization improvement;particle upgrade;DUC 2002 standard documents;ROUGE evaluation software","","4","18","","","","","","IEEE","IEEE Conferences"
"Cross-Layer Self-Adaptive/Self-Aware System Software for Exascale Systems","R. Gioiosa; G. Kestor; D. J. Kerbyson; A. Hoisie","NA; NA; NA; NA","2014 IEEE 26th International Symposium on Computer Architecture and High Performance Computing","","2014","","","326","333","The extreme level of parallelism coupled with the limited available power budget expected in the exascale era brings unprecedented challenges that demand optimization of performance, power and resiliency in unison. Scalability on such systems is of paramount importance, while power and reliability issues may change the execution environment in which a parallel application runs. To solve these challenges exascale systems will require an introspective system software that combines system and application observations across all system stack layers with online feedback and adaptation mechanisms. In this paper we propose the design of a novel self-aware, selfadaptive system software in which a kernel-level Monitor, which continuously inspects the evolution of the target system through observation of Sensors, is combined with a user-level Controller, which reacts to changes in the execution environment, explores opportunities to increase performance, save power and adapts applications to new execution scenarios. We show that the monitoring system accurately monitors the evolution of parallel applications with a runtime overhead below 1-2%. As a test case, we design and implement a runtime system that aims at optimizing application's performance and system power consumption on complex hierarchical architectures. Our results show that our adaptive system reaches 98% of performance efficiency of manually-tuned applications.","1550-6533","978-1-4799-6905","10.1109/SBAC-PAD.2014.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6970681","","Monitoring;Runtime;Temperature sensors;Power demand;Hardware;Temperature measurement","parallel processing;power aware computing;self-adjusting systems;sensors;software fault tolerance;software performance evaluation","manually-tuned applications;complex hierarchical architectures;system power consumption;parallel applications;execution environment;user-level controller;sensors;kernel-level monitoring;adaptation mechanisms;online feedback;introspective system software;parallel application;reliability issues;performance demand optimization;exascale systems;cross-layer self-aware system software;cross-layer self-adaptive system software","","2","39","","","","","","IEEE","IEEE Conferences"
"Leveraging Optimization Methods for Dynamically Assisted Control-Flow Integrity Mechanisms","J. Moreira; L. Teixeira; E. Borin; S. Rigo","NA; NA; NA; NA","2014 IEEE 26th International Symposium on Computer Architecture and High Performance Computing","","2014","","","49","56","Dynamic Binary Modification (DBM) tools are useful for cross-platform execution of binaries and are powerful run time environments that allow execution optimizations, instrumentation and profiling. These tools have also been used as enablers for control-flow integrity verification, a process that consists in the observation and analysis of a program's execution path focusing on the detection of anomalies, such as those arising from flow corruption based software attacks. Even though this class of tools helps us in identifying a myriad of attacks, it is typically expensive at run time and introduce significant overhead to the program execution. Considering their inherent high cost, further expanding the capabilities of such tools for detection of program flow anomalies can slow down the analysis to the point that it is unfeasible to run it in real world workflows. In this paper we present a mechanism for including program flow verification in DBMs that uses asynchronous analysis and applies different parallel-programming techniques that leverage current multi-core systems to control the overhead of our analysis. Our mechanism was tested against synthetic program flow corruption use cases and correctly detected all detours. With our new optimizations, we show that our system achieves an slowdown of only 1.46x, while a naively implemented verification system face 4.22x of overhead.","1550-6533","978-1-4799-6905","10.1109/SBAC-PAD.2014.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6970646","","Instruments;Computer architecture;Optimization;Monitoring;Software;Security;Benchmark testing","multiprocessing systems;parallel programming;program control structures;program diagnostics;program verification;security of data;software tools","verification system;synthetic program flow corruption;multicore systems;parallel-programming techniques;asynchronous analysis;program flow verification;program flow anomaly detection;flow corruption based software attacks;program execution path;control-flow integrity verification;run time environments;cross-platform execution;DBM tools;dynamic binary modification tools;dynamically assisted control-flow integrity mechanisms;optimization methods","","","32","","","","","","IEEE","IEEE Conferences"
"Optimum D-STATCOM placement using firefly algorithm for power quality enhancement","M. Farhoodnea; A. Mohamed; H. Shareef; H. Zayandehroodi","Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan, Malaysia; Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan, Malaysia; Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan, Malaysia; Department of Electrical, Electronic and Systems Engineering, Universiti Kebangsaan, Malaysia","2013 IEEE 7th International Power Engineering and Optimization Conference (PEOCO)","","2013","","","98","102","This paper presents a novel method to optimally place the Distribution Static Synchronous Compensator (DSTATCOM) in a distribution system using the firefly algorithm (FA) for enhancing power quality. In the proposed method, the average voltage total harmonic distortion (THDV), average voltage deviation and total investment cost are considered as the objective functions, where voltage and power limits for individual buses are chosen as the optimization constraints. The performance of the proposed FA is investigated using the Matlab software on the radial IEEE 16bus test system. The obtained results are also compared with the particle swarm optimization and genetic algorithm. The simulation results verify the ability of FA in accurately determining the optimal location and size of the D-STATCOM in radial distribution systems.","","978-1-4673-5074-7978-1-4673-5072-3978-1-4673-5073","10.1109/PEOCO.2013.6564523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564523","D-STATCOM;optimal placement;firefly algorithm;voltage sag","Automatic voltage control;Optimization;Voltage fluctuations;Power quality;Linear programming;Genetic algorithms;Harmonic analysis","genetic algorithms;harmonic distortion;investment;particle swarm optimisation;power distribution economics;power supply quality;static VAr compensators","optimum D-STATCOM placement;firefly algorithm;power quality enhancement;distribution static synchronous compensator;FA;average voltage total harmonic distortion;THDV;average voltage deviation;total investment cost;objective functions;optimization constraints;Matlab software;radial IEEE 16 bus test system;particle swarm optimization;genetic algorithm;radial distribution systems","","9","14","","","","","","IEEE","IEEE Conferences"
"Evolution-in-materio: Solving function optimization problems using materials","M. Mohid; J. F. Miller; S. L. Harding; G. Tufte; O. R. Lykkebø; M. K. Massey; M. C. Petty","Department of Electronics, University of York, York, UK; Department of Electronics, University of York, York, UK; Department of Electronics, University of York, York, UK; Department of Computer and Information Science, Norwegian University of Science and Technology, 7491, Trondheim, Norway; Department of Computer and Information Science, Norwegian University of Science and Technology, 7491, Trondheim, Norway; School of Engineering and Computing Sciences and Centre for Molecular and Nanoscale Electronics, Durham University, UK; School of Engineering and Computing Sciences and Centre for Molecular and Nanoscale Electronics, Durham University, UK","2014 14th UK Workshop on Computational Intelligence (UKCI)","","2014","","","1","8","Evolution-in-materio (EIM) is a method that uses artificial evolution to exploit properties of materials to solve computational problems without requiring a detailed understanding of such properties. In this paper, we show that using a purpose-built hardware platform called Mecobo, it is possible to evolve voltages and signals applied to physical materials to solve computational problems. We demonstrate for the first time that this methodology can be applied to function optimization. We evaluate the approach on 23 function optimization benchmarks and in some cases results come very close to the global optimum or even surpass those provided by a well-known software-based evolutionary approach. This indicates that EIM has promise and further investigations would be fruitful.","2162-7657","978-1-4799-5538","10.1109/UKCI.2014.6930152","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930152","evolutionary algorithm;evolution-in-materio;material computation;evolvable hardware;function optimization","Materials;Electrodes;Optimization;Hardware;Biological cells;Benchmark testing;Arrays","evolutionary computation;optimisation","evolution-in-materio;function optimization problem solving;EIM;artificial evolution;purpose-built hardware platform;Mecobo;physical materials;software-based evolutionary approach","","8","20","","","","","","IEEE","IEEE Conferences"
"Method to determine optimal hardware platforms in Human Centered Computing based on non functional requirements analysis","M. González; L. González; J. Echeverri; M. Aristizábal; G. Urrego; A. L. Pérez","Telecommunications Engineering Department, University of Medellin, Medellin, Colombia; System Engineering Department, University of Medellin, Medellin Colombia; System Engineering Department, University of Medellin, Medellin Colombia; Commercialization Department, UNE Telecommunications, Medellin, Colombia; Software Engineering Department, University of Antioquia, Medellin Colombia; Software Engineering Department, University of Antioquia, Medellin Colombia","2014 9th Iberian Conference on Information Systems and Technologies (CISTI)","","2014","","","1","6","Human Centered Computing is a novel paradigm to process context information of human being's environment in which computers are invisible to the subject, providing tools and services depending on the context of each individual. An increasing interest is growing regarding embedded computers since they offer advantages related to portability, dedicated tasks, invisibility, amongst others. However, a plenty of hardware platforms are in the market, so it is complicated to determine which the best for a particular need. Benchmarks make those tasks easier by performing measurements in hardware by running applications and performing comparisons. Nevertheless, they are thought to meet some quite particular kinds of applications. Moreover, if some benchmark applications have to be merged, i.e. voice or images processing, there are not schemes to correctly measure hardware platforms. On the other hand, requirements such as reliability and availability are not commonly assessed as a dependant set in hardware platforms. In this work, we propose a novel method to select hardware architectures for Human Centered Computing based on benchmarking, genetic algorithms, weighted sums and statistical distances in order to consider non-functional requirements.","2166-0727","978-9-8998-4343","10.1109/CISTI.2014.6876901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876901","HCC;Non Functional Requirements;Hardware Platforms;Optimization","Benchmark testing;Hardware;Vectors;Availability;Optimization;Algorithm design and analysis","benchmark testing;embedded systems;genetic algorithms;human factors;reliability;software portability","optimal hardware platforms;human centered computing;nonfunctional requirement analysis;process context information;human being environment;embedded computers;hardware platforms;benchmark applications;image processing;voice processing;genetic algorithms;weighted sums;statistical distances;nonfunctional requirements","","1","36","","","","","","IEEE","IEEE Conferences"
"Impact of refactoring on external code quality improvement: An empirical evaluation","S. H. Kannangara; W. M. J. I. Wijayanayake","Department of Industrial Management, Faculty of Science, University of Kelaniya, Sri Lanka; Department of Industrial Management, Faculty of Science, University of Kelaniya, Sri Lanka","2013 International Conference on Advances in ICT for Emerging Regions (ICTer)","","2013","","","60","67","Refactoring is the process of improving the design of the existing code by changing its internal structure without affecting its external behaviour, with the main aims of improving the quality of software product. Therefore, there is belief that refactoring improves quality factors such as understandability, flexibility, and reusability. Moreover, there are also claims that refactoring yields higher development productivity. However, there is limited empirical evidence to support such assumptions. The objective of this study is to validate/invalidate the claims that refactoring improves software quality. Experimental research approach was used to achieve the objective and ten selected refactoring techniques were used for the analysis. The impact of each refactoring technique was assessed based on external measures namely; analysability, changeability, time behaviour and resource utilization. After analysing the experimental results, among the tested ten refactoring techniques, “Replace Conditional with Polymorphism” ranked in the highest as having high percentage of improvement in code quality. “Introduce Null Object” was ranked as worst which is having highest percentage of deteriorate of code quality.","","978-1-4799-1276-6978-1-4799-1275","10.1109/ICTer.2013.6761156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6761156","Refactoring;Software maintenance;Analysability;Changeability;Time behaviour;Resource Utilization;ISO 9126","Q-factor;Software quality;Time measurement;Data mining;Software maintenance;Software measurement","software maintenance;software quality","refactoring process;code quality improvement;software product quality;quality factors;understandability factor;flexibility factor;reusability factor;development productivity;experimental research approach;analysability measure;changeability measure;time behaviour measure;resource utilization measure;replace conditional with polymorphism;introduce null object","","5","20","","","","","","IEEE","IEEE Conferences"
"Fast Numerical Evaluation for Symbolic Expressions in Java","Y. Liu; P. Zhang; M. Qiu","NA; NA; NA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","599","604","The symbolic-numeric computation has been extensively developed in scientific computing for experimenting mathematics in numerical programs, like in optimization problems and finite element methods. Many software and libraries have been developed to support symbolic-numeric computation especially in the recent years. However, most of the implementations are cumbersome and inefficient for numerically evaluating symbolic expressions. The popular implementation chooses the way that generates C/C++/FORTRAN source codes for symbolic expressions and compiles the source files using the external compilers. The compiled machine codes are then linked back to the symbolic manipulation language environment. Thi sprocess suffers from slow compilation and significant overhead of external function calls. To address this problem, this paper presents a handy approach that provides fast numerical evaluation for symbolic expressions in Java. In our approach, Java bytecode is generated in memory for symbolic expressions and further Just-In-Time (JIT) compiled to machine codes onJava Virtual Machine (JVM) at runtime. We have developedSymJava (https://github.com/yuemingl/SymJava) to implement our approach and tested a range of benchmark problems. The results show that SymJava is 1~3 orders of magnitude faster than the existing implementations including Matlab, Mathematica, Sage, Theano and SymPy. Additionally, SymJava offers a human friendly programming style for symbolic expressions by overloading operators in Java. Our approach opens up a new avenue for the development of next generation symbolic-numeric software.","","978-1-4799-8937-9978-1-4799-8936","10.1109/HPCC-CSS-ICESS.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336223","symbolic;numeric;java;bytecode;compile;JIT","Java;Libraries;MATLAB;Benchmark testing;Mathematics;Runtime","C++ language;finite element analysis;human factors;Java;program compilers;software libraries;source code (software);virtual machines","symbolic-numeric computation;optimization;software libraries;C source code;FORTRAN source code;C++ source code;compiled machine code;symbolic manipulation language environment;Java bytecode;just-in-time;JIT;Java virtual machine;JVM;SymJava;Matlab;Mathematica;Fast;Sage;Theano;SymPy;human friendly programming style;next generation symbolic-numeric software;compilers","","","26","","","","","","IEEE","IEEE Conferences"
"Design, analysis and improvement of a disc motor","J. Dai; S. Chang","School of Mechanical Engineering, Nanjing University of Science and Technology, 210094, China; School of Mechanical Engineering, Nanjing University of Science and Technology, 210094, China","2013 IEEE International Conference on Mechatronics and Automation","","2013","","","75","80","Aiming at motor application problems in small space, this paper puts forward a new design scheme of Axial-Flux Brushless Permanent Magnetism Disc Motor (AFBPMDM) with coreless and slotless stator structure. Through 3D modeling of the disc motor with Ansoft Maxwell software and simulation of electromagnetic field, we can get distribution of disc motor magnetic induction intensity, air-gap flux density and torque waveforms. Parametric analysis on unilateral air-gap thickness is adopted, and then optimal value for length of unilateral air-gap is gained. Simulation results show that air-gap flux density is higher and torque increases around 38% after unilateral air-gap thickness optimization. Finally, the torque testing experiment verifies accuracy of the simulation and feasibility of the scheme.","2152-7431;2152-744X","978-1-4673-5560-5978-1-4673-5557-5978-1-4673-5558","10.1109/ICMA.2013.6617896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617896","Disc motor;Coreless and slotless;Parametric analysis;Ansoft Maxwell 3D","Induction motors;Permanent magnet motors;Torque;Coils;Brushless DC motors","brushless machines;electromagnetic induction;machine insulation;machine testing;machine theory;magnetic flux;optimisation;permanent magnet motors;stators","axial-flux brushless permanent magnetism disc motor;AFBPM-DM;slotless stator structure;coreless stator structure;3D modeling;Ansoft Maxwell software;electromagnetic field simulation;disc motor magnetic induction intensity;air-gap flux density;torque waveform;parametric analysis;unilateral air-gap thickness optimization;torque testing experiment","","","14","","","","","","IEEE","IEEE Conferences"
"Tutorial: Digital microfluidic biochips: Towards hardware/software co-design and cyber-physical system integration","T. Ho; J. Huang; P. Pop","Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan, ROC; Department of Electronics Engineering and the Institute of Electronics, National Chiao Tung University, Taiwan; DTU Compute, Technical University of Denmark (DTU), Denmark","2013 IEEE International SOC Conference","","2013","","","316","317","This tutorial will first provide an overview of typical bio-molecular applications (market drivers) such as immunoassays, DNA sequencing, clinical chemistry, etc. Next, microarrays and various microfluidic platforms will be discussed. The next part of the tutorial will focus on electro-wetting-based digital micro-fluidic biochips. The key idea here is to manipulate liquids as discrete droplets. A number of case studies based on representative assays and laboratory procedures will be interspersed in appropriate places throughout the tutorial. Basic concepts in micro-fabrication techniques will also be discussed. Attendees will next learn about CAD and reconfiguration aspects of digital microfluidic biochips. Synthesis tools will be described to map assay protocols from the lab bench to a droplet-based microfluidic platform and generate an optimized schedule of bioassay operations, the binding of assay operations to functional units, and the layout and droplet-flow paths for the biochip. The role of the digital microfluidic platform as a “programmable and reconfigurable processor” for biochemical applications will be highlighted. Cyber-physical integration using low-cost sensors and adaptive control, software will be highlighted. Cost-effective testing techniques will be described to detect faults after manufacture and during field operation. On-line and off-line reconfiguration techniques will be presented to easily bypass faults once they are detected. The problem of mapping a small number of chip pins to a large number of array electrodes will also be covered. With the availability of these tools, chip users and chip designers will be able to concentrate on the development and chip-level adaptation of nano-scale bioassays (higher productivity), leaving implementation details to CAD tools.","2164-1706;2164-1676","978-1-4799-1166","10.1109/SOCC.2013.6749708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6749708","","Tutorials;Immune system","adaptive control;biology computing;CAD;drops;fault diagnosis;hardware-software codesign;lab-on-a-chip;microelectrodes;microfabrication;microfluidics;microsensors;molecular biophysics;wetting","low-cost sensors;adaptive control;cost-effective testing techniques;fault detection;off-line reconfiguration techniques;online reconfiguration techniques;chip pins;array electrodes;chip-level adaptation;nanoscale bioassays;CAD tools;programmable-reconfigurable processor platform;droplet-flow paths;assay protocols;droplet-based microfluidic platform;synthesis tools;microfabrication techniques;discrete droplets;electrowetting;microarrays;clinical chemistry;DNA sequencing;immunoassays;biomolecular applications;cyber-physical system integration;hardware-software codesign;digital microfluidic biochips","","","","","","","","","IEEE","IEEE Conferences"
"Emulation Performance Study of Traffic-Aware Policy Enforcement in Software Defined Networks","I. Vawter; D. Pan; W. Ma","NA; NA; NA","2014 IEEE 11th International Conference on Mobile Ad Hoc and Sensor Systems","","2014","","","775","780","Modern networks require robust traffic handling policies in order to minimize unwanted traffic and maximize performance. These policies are enforced by the placement of rules on network devices such as routers and switches. For high-speed processing, the rules are stored in Ternary Content Addressable Memory (TCAM) [3]. Because it is expensive, there is only a limited amount of TCAM on each network device. Software Defined Networking (SDN) [2] and the OpenFlow [1] protocol provide the capability to control the way rules are generated and placed within a network. This allows researchers to design algorithms that control TCAM usage while optimizing performance. The goal of this project is to develop a test bed for researchers to easily implement and evaluate their performance-optimizing algorithms. In this paper, we describe a test bed that emulates SDN activity and captures network performance data for rule placement algorithm evaluation.","2155-6806;2155-6814","978-1-4799-6036-1978-1-4799-6035","10.1109/MASS.2014.82","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7035782","Software Defined Networking;OpenFlow;Network Emulation;Performance Optimization;Link Load","Ports (Computers);Network topology;IP networks;Routing;Bandwidth;Switches;Emulation","content-addressable storage;protocols;software defined networking;telecommunication network routing;telecommunication switching;telecommunication traffic","emulation performance;traffic-aware policy enforcement;software defined networks;traffic handling policies;unwanted traffic minimisation;performance maximisation;routers;switches;ternary content addressable memory;TCAM;SDN;OpenFlow protocol;performance-optimizing algorithms;rule placement algorithm evaluation","","1","17","","","","","","IEEE","IEEE Conferences"
"Vibration-Based Diagnostics for Rotary MEMS","J. Feldman; B. M. Hanrahan; S. Misra; X. Z. Fan; C. M. Waits; P. D. Mitcheson; R. Ghodssi","Department of Electrical and Computer Engineering, Institute for Systems Research, University of Maryland, College Park, MD, USA; Department of Materials Science and Engineering, University of Maryland, College Park, MD, USA; Department of Electrical and Computer Engineering, Institute for Systems Research, University of Maryland, College Park, MD, USA; Department of Electrical and Computer Engineering, Institute for Systems Research, University of Maryland, College Park, MD, USA; U.S. Army Research Laboratory, Adelphi, MD, USA; Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Electrical and Computer Engineering, Institute for Systems Research, University of Maryland, College Park, MD, USA","Journal of Microelectromechanical Systems","","2015","24","2","289","299","This paper demonstrates the use of low-cost off-the-shelf (OTS) microelectromechanical system (MEMS) technology to perform vibration-based in situ monitoring, diagnostics, and characterization of a MEMS microball bearing supported radial air turbine platform. A multimodal software suite for platform automation and sensor monitoring is demonstrated using a three-level heuristic software suite and sensor network. The vibration diagnostic methods used in the platform have applications in rotary microsystems for the early detection of failure, fault diagnosis, and integrated diagnostic systems for feedback-based optimization to increase device performance, reliability, and operational lifetimes. The studied rotary microdevice used a dual OTS accelerometer configuration for dual range parallel redundant vibration analysis. The sensor suite has been used to monitor and detect multiple operational parameters measured optimally in time or frequency domains such as rotor instability, imbalance, wobble, and system resonance. This paper will lay the framework for active diagnostics in future MEMS devices through integrated systems.","1057-7157;1941-0158","","10.1109/JMEMS.2014.2383171","U.S. National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008455","Non-destructive testing;rotating machine measurement;rotating machine stability;vibration measurement;Non-destructive testing;rotating machine measurement;rotating machine stability;vibration measurement","Rotors;Vibrations;Force;Micromechanical devices;Accelerometers;Turbines;Software","accelerometers;fault diagnosis;microsensors;nondestructive testing;optimisation;reliability;vibration measurement","vibration-based diagnostics;rotary MEMS;low-cost off-the-shelf MEMS;microelectromechanical system;MEMS microball bearing;three-level heuristic software;sensor network;failure detection;fault diagnosis;integrated diagnostic systems;feedback based optimization;reliability;device performance;operational lifetimes;rotary microdevice;dual OTS accelerometer configuration;frequency domain;time domain;rotor instability","","1","33","","","","","","IEEE","IEEE Journals & Magazines"
"Development of system code of CFETR design","M. Y. Ye; S. J. Wang; Z. W. Wang; G. L. Xu; X. F. Liu; J. W. Zhang; S. F. Mao; V. S. Chan","School of Nuclear Science and Technology, University of Science and Technology of China, Hefei, China; School of Nuclear Science and Technology, University of Science and Technology of China, Hefei, China; Institute of Plasma Physics, Chinese Academy of Sciences, Hefei, China; School of Nuclear Science and Technology, University of Science and Technology of China, Hefei, China; Institute of Plasma Physics, Chinese Academy of Sciences, Hefei, China; Institute of Plasma Physics, Chinese Academy of Sciences, Hefei, China; School of Nuclear Science and Technology, University of Science and Technology of China, Hefei, China; School of Nuclear Science and Technology, University of Science and Technology of China, Hefei, China","2015 IEEE 26th Symposium on Fusion Engineering (SOFE)","","2015","","","1","4","China Fusion Engineering Test Reactor (CFETR) is now in the conceptual design phase. The main objective of CFETR is demonstration of fusion energy with 50 ~ 200 MW fusion power, fuel cycle of T self-sustained and steady-state operation with duty cycle of 0.3 ~ 0.5. The design of CFETR involves complex system structure, and there are complex constrains between physics and engineering. Between optimization of performance parameters and design of main structure and key components, numerous data exchange and iterative optimization are necessary for optimal design of sub-systems. To do the optimization design, a CFETR system code is under development. The main technical schemes for system code include: a physical design platform and various engineering design modules are developed, then a global framework integrates them by standard interfaces and communication technologies; and a standard material and design criterion database unifies the reference data for the system code. The detailed study will be presented in this conference.","2155-9953","978-1-4799-8264","10.1109/SOFE.2015.7482353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482353","System code;CFETR;Fusion reactor","Plasmas;Physics;Databases;Optimization;Software;Physical design;Steady-state","fusion reactor design;fusion reactor fuel;iterative methods;optimisation","CFETR design;China Fusion Engineering Test Reactor;fusion energy;fuel cycle;data exchange;iterative optimization;optimization design;CFETR system code;physical design platform;standard interfaces;power 50 MW to 200 MW","","3","7","","","","","","IEEE","IEEE Conferences"
"Automating Repetitive Tasks on Web-Based IDEs via an Editable and Reusable Capture-Replay Technique","Y. Sun; D. Chen; C. Xin; W. Jiao","NA; NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","2","","666","675","Web-based IDEs are more and more popular because developers can create or modify software artifacts in the browser without need to install any local development tool and spend valuable development time on system setup and maintenance. For those development tasks using a Web-based IDE, such as configuring programming context and batch test etc., some are frequent and repetitive because they are similar from project to project. Automating the repetitive tasks on Web-based IDEs, regardless of their complexity, would reduce the amount of work that developers must perform to complete the tasks, which would improve the development efficiency of Web applications. In this paper, we put forward a user-friendly approach to automating repetitive tasks on existing Web-based IDEs. The key to the approach is to extend the basic Web-based capture-replay technique with editable and reusable features, which are necessary for automation because some operations are redundant, as well as developers should recognize and define repetitive tasks. Moreover, we develop a supporting tool for the approach. In the case study, we introduce how the approach is used to support automating repetitive tasks on Web-based IDEs. Case studies verify that the approach can improve the development efficiency very well.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273682","Automating repetitive tasks;Web-based IDE;Capture-Replay technique","Graphical user interfaces;Context;Mice;Browsers;Optimization;Software;Automation","Internet;program testing;software reusability","repetitive tasks automation;Web-based IDE;editable capture-replay technique;reusable capture-replay technique;software artifacts;local development tool;development time;system setup;programming context;batch test;Web applications;user-friendly approach;Web-based capture-replay technique;development efficiency","","1","18","","","","","","IEEE","IEEE Conferences"
"Challenges and Issues of Mining Crash Reports","L. An; F. Khomh","SWAT, Polytechnique Montreal, Quebec, Canada; SWAT, Polytechnique Montreal, Quebec, Canada","2015 IEEE 1st International Workshop on Software Analytics (SWAN)","","2015","","","5","8","Automatic crash reporting tools built in many software systems allow software practitioners to understand the origin of field crashes and help them prioritise field crashes or bugs, locate erroneous files, and/or predict bugs and crash occurrences in subsequent versions of the software systems. In this paper, after illustrating the structure of crash reports in Mozilla, we discuss some techniques for mining information from crash reports, and highlight the challenges and issues of these techniques. Our aim is to raise the awareness of the research community about issues that may bias research results obtained from crash reports and provide some guidelines to address certain challenges related to mining crash reports.","","978-1-4673-6923","10.1109/SWAN.2015.7070480","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070480","Crash report;bug report;mining softwarerepositories.","Computer bugs;Data mining;Databases;Algorithm design and analysis;Software systems","data mining;program debugging;program testing","crash report mining;crash reporting tool;software systems","","3","13","","","","","","IEEE","IEEE Conferences"
"Combinatorial Validation Testing of Java Card Byte Code Verifiers","A. Calvagna; E. Tramontana","NA; NA","2013 Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises","","2013","","","347","352","We present a technique to fully automate validation of a byte code verifier (BCV) implementation by testing. The technique is based on the use of a finite state machine model of the JVM specifications to systematically explore the set of legal JVM states from which one or more illegal states are immediately reachable. All possible sequences of instructions bringing to illegal states reachable from each legal state are combinatorially enumerated to generate a suite of tests, consisting of valid Java programs. For tests to stress the BCV type inference algorithm, each test program has been purposely designed with a large number of intertwined execution flows. The illegal state in the instruction sequence is hidden inside one of those paths.","1524-4547","978-1-4799-0405","10.1109/WETICE.2013.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570641","Software engineering;software testing;byte code verification","Law;Testing;Registers;Optimized production technology;Load modeling","","","","5","13","","","","","","IEEE","IEEE Conferences"
"Detecting hardware Trojans in unspecified functionality using mutation testing","N. Fern; K. Cheng","University of California, Santa Barbara, ECE Department, USA; University of California, Santa Barbara, ECE Department, USA","2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","","2015","","","560","566","Existing functional Trojan detection methodologies assume Trojans violate the design specification under carefully crafted rare triggering conditions. We present a new type of Trojan that leaks secret information from the design by only modifying unspecified functionality, meaning the Trojan is no longer restricted to being active only under rare conditions. We provide a method based on mutation testing for detecting this new Trojan type along with mutant ranking heuristics to prioritize analysis of the most dangerous functionality. Applying our method to a UART controller design, we discover unspecified and untested bus functionality with the potential to leak 32 bits of information during hundreds of cycles without being detected! Our method also reveals poorly tested interrupt functionality with information leakage potential. After modifying the specification and test bench to remove the discovered vulnerabilities, we close the verification loop by re-analyzing the design using our methodology and observe the functionality is no longer flagged as dangerous.","","978-1-4673-8388-2978-1-4673-8389","10.1109/ICCAD.2015.7372619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372619","","Trojan horses;Testing;Hardware;Logic gates;Payloads;Logic functions;Optimization","computer interfaces;invasive software","hardware Trojans;mutation testing;functional Trojan detection methodologies;rare triggering conditions;unspecified functionality;mutant ranking heuristics;UART controller design;bus functionality;information leakage potential;verification loop","","6","19","","","","","","IEEE","IEEE Conferences"
"Characterization of Shared Library Access Patterns of Android Applications","X. Dong; S. Dwarkadas; A. L. Cox","NA; NA; NA","2015 IEEE International Symposium on Workload Characterization","","2015","","","112","113","We analyze the instruction access patterns of Android applications. Although Android applications are ordinarily written in Java, we find that native-code shared libraries play a large role in their instruction footprint. Specifically, averaging over a wide range of applications, we find that 60% of the instruction pages accessed belong to native-code shared libraries and 72% of the instruction fetches are from these same pages. Moreover, given the extensive use of native-code shared libraries, we find that, for any pair of applications, on average 28% of the overall instruction pages accessed by one of the applications are also accessed by the other. These results suggest the possibility of optimizations targeting shared libraries in order to improve instruction access efficiency and overall performance.","","978-1-5090-0088-3978-1-5090-0087","10.1109/IISWC.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314154","mobile platforms;shared libraries;Android process creation model","Libraries;Androids;Humanoid robots;Browsers;Benchmark testing;Computer science;Optimization","Android (operating system);Java;optimisation;software libraries","shared library access patterns characterization;Android applications;Java;native-code shared libraries;instruction footprint;instruction fetches;native-code shared libraries;instruction pages;optimizations;instruction access efficiency","","1","4","","","","","","IEEE","IEEE Conferences"
"Design and Implementation of a Service-Oriented Architecture for the Optimization of Industrial Applications","A. Girbea; C. Suciu; S. Nechifor; F. Sisak","Transilvania University of Brasov, Brasov, Romania; Transilvania University of Brasov, Brasov, Romania; Siemens Corporation, Corporate Technology, Brasov, Romania; Transilvania University of Brasov, Brasov, Romania","IEEE Transactions on Industrial Informatics","","2014","10","1","185","196","A novel architecture for the field of industrial automation is described, the goals of which are: 1) computation of optimal production plans; 2) automated usage of the optimized plans; 3) flexibility and reusability at development and maintenance; and 4) seamless transition from current practice to the approach introduced herein. The architecture consists of three main components: 1) a set of OPC unified architecture (UA) servers, which are used to model the information from the device level; 2) a set of services organized into two layers (basic and complex services), which act as a link between the first and the third layer; and 3) a constraint satisfaction problem (CSP) layer for the computation of production plans. Extensive performance tests motivate the choice of the service development framework, and prove the effectiveness of the special adapter software solution for the integration of current devices and the ability of the UA server to manage a high number of UA connections. As a proof-of-concept, the architecture has been tested for a real manufacturing problem composed of four flexible manufacturing systems. The results show that the architecture is able to efficiently control and monitor a real manufacturing process according to an optimized schedule with over 99% of the time spent on the manufacturing.","1551-3203;1941-0050","","10.1109/TII.2013.2253112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481444","Industrial automation;OPC unified architecture (UA);optimization;service-oriented architecture (SOA)","Computer architecture;Servers;Manufacturing;Production;Web services;Performance evaluation;Maintenance engineering","flexible manufacturing systems;manufacturing processes;production engineering computing;production planning;service-oriented architecture;software maintenance;software reusability","service-oriented architecture;industrial application optimization;industrial automation;optimal production plan computation;automated optimized plan usage;development flexibility;development reusability;maintenance flexibility;maintenance reusability;OPC unified architecture servers;constraint satisfaction problem layer;UA connections;flexible manufacturing systems;manufacturing problem;manufacturing process","","49","32","","","","","","IEEE","IEEE Journals & Magazines"
"Beam Search Combined With MAX-MIN Ant Systems and Benchmarking Data Tests for Weighted Vehicle Routing Problem","J. Tang; J. Guan; Y. Yu; J. Chen","College of Management Science and Engineering, Dongbei University of Finance and Economics, Shahekou, Dalian, P. R. China; Software Development Center, Agricultural Bank of China, Beijing, P. R. China; Department of System Engineering, Northeastern University, Shenyang, P. R. China; Department of Information and Control Engineering, Liaoning Shihua University, Fushun, P. R. China","IEEE Transactions on Automation Science and Engineering","","2014","11","4","1097","1109","In real-world cargo transportation, there are charges associated with both the traveling distance and the loading quantity. Cargo trucks must comply with a mandatory lower carbon emissions policy: the emissions of large-volume cargo truck/containers depend greatly on the cargo loading and the traveling distance. To address this issue, instead of assuming a constant vehicle loading from one customer to another, a variable vehicle loading should be used in optimizing the vehicle routine, which is known as a weighted vehicle routing problem (WVRP) model. The WVRP is an NP-hard problem; thus, the purpose of this paper is to develop a BEAM-MMAS algorithm that combines a MAX-MIN ant system with beam search to show that the WVRP is more effective than the VRP and to determine the types of VRP instances for which the WVRP has more cost-savings than the VRP. To this end, computational experiments are carried out on benchmark problems of the capacitated VRP for seven types of distributions, and the effectiveness of the BEAM-MMAS algorithm is compared with that of general ACO and MMAS algorithms for large-size benchmarking instances. The benchmarking tests show that lower operation costs are produced using the WVRP than using the optimal or best known paths of the CVRP and that the WVRP can increase cost savings for the instances with a dispersed customer distribution and a large weight.","1545-5955;1558-3783","","10.1109/TASE.2013.2295092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6705646","Ant colony algorithm;beam search;benchmarking data testing;meta-heuristics;weighted vehicle routing problem","Vehicle routing;Benchmark testing;Ant colony optimization;NP-hard problem","ant colony optimisation;computational complexity;goods distribution;minimax techniques;search problems;vehicle routing","beam search;max-min ant systems;weighted vehicle routing problem;cargo transportation;traveling distance;cargo loading quantity;cargo trucks;carbon emissions policy;variable vehicle loading;WVRP model;NP-hard problem;BEAM-MMAS algorithm;cost savings","","5","27","","","","","","IEEE","IEEE Journals & Magazines"
"Towards scalable symbolic routing for multi-objective networked embedded system design and optimization","S. Graf; F. Reimann; M. Glaß; J. Teich","Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Germany; Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Germany; Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Germany; Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU), Germany","2014 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2014","","","1","10","Symbolic encoding for resource allocation, task binding, and message routing during multi-objective design space exploration (DSE) has gained significant attention in recent years. To determine the message routing, existing symbolic approaches typically rely on an explicit encoding of routing hops which results in a huge number of required variables and/or constraints. As a result, these approaches fail in case of large network diameters and/or a huge number of messages or resources and even for smaller problems, the convergence of the involved optimization process suffers. To tackle this shortcomings, this work proposes three novel symbolic routing encoding strategies that all avoid to encode hops explicitly, but are based on an encoding of individual links or complete sender-receiver paths, but still cover the same design space. The result is a more compact problem representation with less constraints and, in particular, less variables; the latter eliminates ineffective degrees of freedom from the search space and significantly enhances the optimization quality of a multi-objective optimization with even non-linear objectives. In an extensive test-suite, three major classes of wired networked embedded systems are considered: (a) hierarchical stars as in MPSoCs or automotive, (b) redundant backbone buses as common in rail systems or avionics, and (c) mesh-based architectures that often occur in NoC-based MPSoCs. For all three classes, the proposed approaches significantly outperform existing techniques in both scalability and optimization quality and, thus, considerably enlarge the field of application of a multi-objective DSE for the networked embedded system design.","","978-1-4503-3051","10.1145/2656075.2656102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971818","","Routing;Encoding;Optimization;Receivers;Resource management;Embedded systems;Scalability","embedded systems;encoding;optimisation;resource allocation;routing protocols;system-on-chip","NoC-based MPSoCs;mesh-based architectures;avionics;rail systems;MPSoCs;multi-objective optimization;search space;complete sender-receiver paths;symbolic routing encoding strategies;message routing;(DSE);design space explo- ration;resource allocation;Symbolic encoding;Optimization;Multi-Objective Networked Embedded System;Scalable Symbolic Routing","","2","10","","","","","","IEEE","IEEE Conferences"
"Cognitive test-bed for wireless sensor networks","E. Romero; J. Blesa; A. Tena; G. Jara; J. Domingo; A. Araujo","Universidad Polit&#x00E8;cnica de Madrid, ETSI Telecomunicaci&#x00F3;n, 28040 Madrid, Spain; Universidad Polit&#x00E8;cnica de Madrid, ETSI Telecomunicaci&#x00F3;n, 28040 Madrid, Spain; Universidad Polit&#x00E8;cnica de Madrid, ETSI Telecomunicaci&#x00F3;n, 28040 Madrid, Spain; Universidad Polit&#x00E8;cnica de Madrid, ETSI Telecomunicaci&#x00F3;n, 28040 Madrid, Spain; Universidad Polit&#x00E8;cnica de Madrid, ETSI Telecomunicaci&#x00F3;n, 28040 Madrid, Spain; Universidad Polit&#x00E8;cnica de Madrid, ETSI Telecomunicaci&#x00F3;n, 28040 Madrid, Spain","2014 IEEE International Symposium on Dynamic Spectrum Access Networks (DYSPAN)","","2014","","","346","349","Cognitive Wireless Sensor Networks are an emerging technology with a vast potential to avoid traditional wireless problems such as reliability, interferences and spectrum scarcity in Wireless Sensor Networks. Cognitive Wireless Sensor Networks test-beds are an important tool for future developments, protocol strategy testing and algorithm optimization in real scenarios. A new cognitive test-bed for Cognitive Wireless Sensor Networks is presented in this paper. This work in progress includes both the design of a cognitive simulator for networks with a high number of nodes and the implementation of a new platform with three wireless interfaces and a cognitive software for extracting real data. Finally, as a future work, a remote programmable system and the planning for the physical deployment of the nodes at the university building is presented.","","978-1-4799-2661","10.1109/DySPAN.2014.6817811","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6817811","Test-bed;Cognitive Wireless Sensor Networks;Cognitive platform;ISM bands;Simulator","Wireless sensor networks;Software;Cognitive radio;Programming;Radio frequency;Dynamic spectrum access","cognitive radio;protocols;radiofrequency interference;telecommunication computing;telecommunication network planning;telecommunication network reliability;wireless sensor networks","reliability problem;interference problem;spectrum scarcity problem;cognitive wireless sensor network test-beds;protocol strategy testing;algorithm optimization;cognitive simulator design;wireless interfaces;cognitive software;remote programmable system;node physical deployment planning;university building","","2","14","","","","","","IEEE","IEEE Conferences"
"Dependability Issues of Android Games: A First Look via Software Analysis","J. Fu; Y. Zhang; Y. Kang","NA; NA; NA","2015 IEEE Symposium on Service-Oriented System Engineering","","2015","","","291","296","Smart phones have surged into popularity in recent years, which has dramatically changed the way people live, work, and have fun. Smart phone games are an important type of Smart phone applications, which attract many software developers. However, they still have not caught much research attention in the software dependability community. In this paper, we study the characteristics of over 2000 Android games with software analysis techniques, with a focus on their dependability issues. Our study suggests that a new security paradigm is of great importance to Smart phone games to prevent potential privilege abuse. Moreover, a new set of testing and debugging approaches should be specifically tailored for Smart phone games, since games are becoming more complicated. Our study also reveals that most games are not specifically optimized according to the user pattern of Smart phones. We expect these open problems can bring more attentions to the software dependability community.","","978-1-4799-8356","10.1109/SOSE.2015.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7133543","","Games;Internet;Androids;Humanoid robots;Libraries;Java;Software","Android (operating system);computer games;program debugging;program diagnostics;program testing;smart phones","testing approach;debugging approach;security paradigm;software analysis techniques;software dependability community;software developers;smartphone applications;smartphone games;Android games;dependability issues","","1","22","","","","","","IEEE","IEEE Conferences"
"Preemptive Regression Testingof Workflow-Based Web Services","L. Mei; W. K. Chan; T. H. Tse; B. Jiang; K. Zhai","NA; NA; NA; NA; NA","IEEE Transactions on Services Computing","","2015","8","5","740","754","An external web service may evolve without prior notification. In the course of the regression testing of a workflow-based web service, existing test case prioritization techniques may only verify the latest service composition using the not-yet-executed test cases, overlooking high-priority test cases that have already been applied to the service composition before the evolution. In this paper, we propose Preemptive Regression Testing (PRT), an adaptive testing approach to addressing this challenge. Whenever a change in the coverage of any service artifact is detected, PRT recursively preempts the current session of regression test and creates a sub-session of the current test session to assure such lately identified changes in coverage by adjusting the execution priority of the test cases in the test suite. Then, the sub-session will resume the execution from the suspended position. PRT terminates only when each test case in the test suite has been executed at least once without any preemption activated in between any test case executions. The experimental result confirms that testing workflow-based web service in the face of such changes is very challenging; and one of the PRT-enriched techniques shows its potential to overcome the challenge.","1939-1374;2372-0204","","10.1109/TSC.2014.2322621","National Natural Science Foundation of China; CCF-Tencent Open Research Fund; National High Technology Research and Development Program of China; National Science and Technology Infrastructure Program; Early Career Scheme and the General Research Fund of the Research Grants Council of Hong Kong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6812226","Evolving service composition;adaptive regression testing","Testing;Web services;Electronic mail;Educational institutions;Context;Maintenance engineering","program testing;regression analysis;Web services;workflow management software","preemptive regression testing;PRT;workflow-based Web service;adaptive testing approach;test case execution","","5","47","","","","","","IEEE","IEEE Journals & Magazines"
"Modification of MELD score by including Serum Albumin to improve prediction of mortality outcome of cirrhotic patient based on Thai cirrhotic patients","M. Duangkrut; Y. Temtanapat; P. Komolmit","Department of Computer Science, Thammasat University, Pathum Thani, Thailand; Department of Computer Science, Thammasat University, Pathum Thani, Thailand; Department of Gastroenterology, Chulalongkorn University, Bangkok, Thailand","2014 11th International Joint Conference on Computer Science and Software Engineering (JCSSE)","","2014","","","100","105","Nowadays, the Model for End-stage Liver Disease (MELD) has become a popular model and replaced the Child-Pugh score for the assessment of the mortality opportunity of patients with cirrhosis in 3-month period. The model predicts the severity of the disease based on 3 biochemical parameters: serum creatinine, serum total bilirubin, and INR. However, in the past, the first model like Child-Pugh score signified the importance of Serum Albumin, a protein producing in a liver. It is, thus, expected that the Serum Albumin has an effect on patients' mortality prediction. In this research, our main focus is to refine and evaluate the effect of Serum Albumin to mortality of Thai cirrhotic patients if included into the MELD model. We use the data collection from 158 Thai cirrhotic patients with different degrees of severity. They were treated at the Liver Unit and Clinic, King Chulalongkorn Memorial Hospital, The Thai Red Cross Society. The collected data were divided into the periods of 3 months, 6 months, 1 year and 2 years respectively[1]. The Kaplan-Meier statistic was used to analyze the survival opportunity of each period. Also, the Cox-Regression was utilized to evaluate the relationship and the statistical significance of the substance in each period in order to find the connection between the Serum Albumin and mortality opportunities. Results of the study show that of all the data from 158 patients, with the Serum Albumin level between 1.0 and 3.5 g/dL, when tested by Pearson's Chi-squared[2], Log Rank Test and Wilcoxon rank-sum (Mann-Whitney)[3] has the statistical significance at the 1% level of confidence (p &lt;; 0.001). Moreover, the correlation of the results using Cox Regression demonstrated also that Serum Albumin influenced the mortality opportunity at the hazard ratio of 5.14 (95%CI:2.971-8.920) with level of confidence p-value &lt;; 0.0001. Thus, we believe that the Serum Albumin affected the mortality prediction model. We also propose two refined MELD models[4], ThaiMELD-Albumin and ThaiMELD-CTP[5]. For the efficiency assessment of the models, we compare our models to others using the ROC. We found that ThaiMELD-Albumin had 0.85 (95% CI: 0.68-1.00) and it is better than MELD, MELD-Albumin and 5vMELD, while ThaiMELD-CTP is just better than MELD. Consequently, ThaiMELD-Albumin is better for prediction of the mortality opportunity for Thai patients than the MELD, MELD-Albumin or 5vMELD. While ThaiMELD-CTP which just added a scale value to MELD could give a better assessment than MELD itself. Therefore, our model could benefit to Thai patients for the assessment of mortality opportunity as well as symptoms' severity. It could, perhaps, be further used for the consideration of liver transplantation in Thailand.","","978-1-4799-5822-1978-1-4799-5821","10.1109/JCSSE.2014.6841850","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6841850","Child-Pugh score;MELD;MELDNa;MELD Albumin;Survival Analysis;Cox Regression;ROC","","diseases;liver;patient treatment;regression analysis;statistical testing","MELD score;serum albumin;Thai cirrhotic patients;model for end-stage liver disease;Child-Pugh score;biochemical parameter;serum creatinine;serum total bilirubin;INR;child-Pugh score;patient mortality prediction;MELD model;Liver Unit and Clinic;King Chulalongkorn Memorial Hospital;The Thai Red Cross Society;Kaplan-Meier statistic;survival opportunity;Cox-regression;statistical significance;Pearson's Chi-squared test;log rank test;Wilcoxon rank-sum test;hazard ratio;mortality prediction model;ThaiMELD-albumin;ThaiMELD-CTP;efficiency assessment;mortality opportunity assessment;liver transplantation;Thailand","","","15","","","","","","IEEE","IEEE Conferences"
"RCopterX - Experimental validation of a distributed leader-follower MPC approach on a miniature helicopter test bed","S. M. Huck; M. Rueppel; T. H. Summers; J. Lygeros","Automatic Control Laboratory, Department of Information Technology and Electrical Engineering, ETH Zurich, 8092, Switzerland; Automatic Control Laboratory, Department of Information Technology and Electrical Engineering, ETH Zurich, 8092, Switzerland; Automatic Control Laboratory, Department of Information Technology and Electrical Engineering, ETH Zurich, 8092, Switzerland; Automatic Control Laboratory, Department of Information Technology and Electrical Engineering, ETH Zurich, 8092, Switzerland","2014 European Control Conference (ECC)","","2014","","","802","807","RCopterX is an indoor testbed for miniature remote controlled helicopters build at the Automatic Control Laboratory at ETH Zurich. The experimental setup includes modular custom control software, a hardware interface between a PC and radio-control signal transmitter, off-the-shelf miniature helicopters and an infrared vision system for global positioning indoors. Its purpose is to experimentally validate problems of UAVs performing multi-level controls, including stabilizing low level controls as well as testing of high level algorithms. The main features are the modular architecture allowing for multi-agent support and interchangeability of the vehicle hardware and control algorithms. For illustration, a reliable fast model predictive control (MPC) scheme for coaxial helicopters as well as the implementation of a formation control algorithm is described. The presented distributed leader follower MPC approach achieves reference tracking of the whole formation along with maintenance of the formation shape, by having the individual agents solve their own optimization problem based on information from their leader. Results of the experimental validation demonstrate the successful implementation and the multi-agent capabilities of the setup.","","978-3-9524269-1","10.1109/ECC.2014.6862458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6862458","","Helicopters;Vehicles;Optimization;Trajectory;Lead;Mathematical model;Software","aerospace computing;autonomous aerial vehicles;control engineering computing;helicopters;multi-robot systems;optimisation;predictive control;telerobotics","optimization problem;reference tracking;formation control algorithm;coaxial helicopters;model predictive control scheme;multiagent support;multilevel control;UAV;global positioning indoors;infrared vision system;off-the-shelf miniature helicopters;PC-radio-control signal transmitter hardware interface;modular custom control software;ETH Zurich;Automatic Control Laboratory;miniature remote controlled helicopters;miniature helicopter test bed;distributed leader-follower MPC approach;RCopterX","","1","19","","","","","","IEEE","IEEE Conferences"
"Mining Sequential Patterns of Predicates for Fault Localization and Understanding","Z. Gao; Z. Chen; Y. Feng; B. Luo","NA; NA; NA; NA","2013 IEEE 7th International Conference on Software Security and Reliability","","2013","","","109","118","Fault localization has been widely recognized as one of the most costly activities in software engineering. Most of existing techniques target a single faulty entity as the root cause of a failure. However these techniques often fail to reveal the context of a failure which can be valuable for the developers and testers to understand and correct faults. Thus some tentative solutions have been proposed to localize faults as sequences of software entities. However, as far as we know, none of these pioneering works consistently handles execution data in a sequence-oriented way, i.e., they analyze suspiciousness of software entities separately before or after the construction of a faulty sequence. In this paper, we establish a systematic framework based on sequential-pattern mining to assist fault localization. We model the executions of test cases as sequences of predicates. Our framework outputs sequential patterns which are more likely related to the actual faults based on a 3-stage procedure: a preprocessing stage to prune sequences of predicates, a mining stage to discover candidate sequential patterns based on the revised SPADE mining algorithm, and a ranking stage to obtain top K results according to our novel metrics. The obtained sequential patterns of predicates can not only provide information about the locations of faults, but also convey valuable context information for understanding the root causes of software failures. A preliminary experiment on some widely used benchmarks was conducted to evaluate the performance of our framework. The experimental results show that our technique is effective and efficient in revealing causes of failures.","","978-1-4799-0406-8978-0-7695-5021","10.1109/SERE.2013.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571701","","Data mining;Measurement;Context;Runtime;Instruments;Software systems","data mining;software fault tolerance","sequential pattern mining;fault localization;fault understanding;software engineering;failure cause;single faulty entity;software entity sequence;execution data handling;software entity suspiciousness;systematic framework;preprocessing stage;mining stage;pattern discovery;SPADE mining algorithm;ranking stage","","1","19","","","","","","IEEE","IEEE Conferences"
"Multi agent based railway scheduling and optimization","P. Dalapati; A. J. Singh; A. Dutta; S. Bhattacharya","Dept. of IT, NIT Durgapur India; Dept. of IT, NIT Durgapur India; Dept. of IT, NIT Durgapur India; Dept. of CSE, NIT Surathkal India","TENCON 2014 - 2014 IEEE Region 10 Conference","","2014","","","1","6","This paper proposes a multi agent based timetable scheduling algorithm for railway system which handles the in-between time delay of the newly introduced train. The delay management indeed optimizes the total journey time, hence increases the total utility of the whole railway system as well. Here we show that schedule generated by our proposed algorithm is the most optimized schedule. It is done by using the notion of DCOP(Distributed Constraint Optimization Problem), where we define some metric to analyze the system to achieve our goals. We use JADE(Java Agent DEvelopment Framework) platforms to simulate our work and test it using a small network. We also take a small case study to compare our proposed work with the existing one and the results are therefore presented.","2159-3450;2159-3442","978-1-4799-4075-2978-1-4799-4076","10.1109/TENCON.2014.7022389","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7022389","Multi agent system;Railway Timetable;Railway scheduling;Distributed Constraint Optimization","Silicon;Delays;Rail transportation;Schedules;Optimization;Scheduling;Joining processes","delays;Java;multi-agent systems;optimisation;railways;scheduling;software agents","JADE;Java agent development framework;railway system;multiagent based timetable scheduling algorithm;time delay management;DCOP;distributed constraint optimization problem","","2","23","","","","","","IEEE","IEEE Conferences"
"RAVAGE: Post-silicon validation of mixed signal systems using genetic stimulus evolution and model tuning","B. Muldrey; S. Deyati; M. Giardino; A. Chatterjee","School of Electrical and Computer Engineering Georgia Institute of Technology Atlanta, Georgia 30332-0250; School of Electrical and Computer Engineering Georgia Institute of Technology Atlanta, Georgia 30332-0250; School of Electrical and Computer Engineering Georgia Institute of Technology Atlanta, Georgia 30332-0250; School of Electrical and Computer Engineering Georgia Institute of Technology Atlanta, Georgia 30332-0250","2013 IEEE 31st VLSI Test Symposium (VTS)","","2013","","","1","6","With trends in mixed-signal systems-on-chip indicating increasingly extreme scaling of device dimensions and higher levels of integration, the tasks of both design and device validation is becoming increasingly complex. Post-silicon validation of mixed-signal/RF systems provides assurances of functionality of complex systems that cannot be asserted by even some of the most advanced simulators. We introduce RAVAGE (from “random;” “validation;” and “generation”), an algorithm for generating stimuli for post-silicon validation of mixed-signal systems. The approach of RAVAGE is new in that no assumption is made about any design anomaly present in the DDT; but rather, the stimulus is generated using the DUT itself with the objective of maximizing the effects of any behavioral differences between the DUT (hardware) and its behavioral model (software) as can be seen in the differences of their response to the same stimulus. Stochastic test generation is used since the exact nature of any behavioral anomaly in the DUT cannot be known a priori. Once a difference is observed, the model parameters are tuned using nonlinear optimization algorithms to remove the difference between its and the DUT's responses and the process (test generation→tuning) is repeated. If a residual error remains at the end of this process that is larger than a predetermined threshold, then it is concluded that the DUT contains unknown and possibly malicious behaviors that need further investigation. Experimental results on an RF system (hardware) are presented to prove feasibility of the proposed technique.","1093-0167;1093-0167","978-1-4673-5543-8978-1-4673-5542-1978-1-4673-5541","10.1109/VTS.2013.6548917","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548917","automatic test pattern generation;integrated circuit testing;radiofrequency integrated circuits;post-silicon validation;hardware trojan detection","Sociology;Statistics;Mathematical model;Integrated circuit modeling;Genetic algorithms;Tuning;Radio frequency","circuit tuning;elemental semiconductors;mixed analogue-digital integrated circuits;optimisation;silicon;system-on-chip","RAVAGE;post-silicon validation;mixed signal systems;genetic stimulus evolution;model tuning;mixed-signal systems-on-chip;mixed-signal-RF systems;behavioral model;stochastic test generation;nonlinear optimization;residual error;malicious behaviors;Si","","5","","","","","","","IEEE","IEEE Conferences"
"Checking correctness of code generator architecture specifications","N. Hasabnis; R. Qiao; R. Sekar","Stony Brook University, NY; Stony Brook University, NY; Stony Brook University, NY","2015 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)","","2015","","","167","178","Modern instruction sets are complex, and extensions are proposed to them frequently. This makes the task of modelling architecture specifications used by the code generators of modern compilers complex and error-prone. Given the important role played by the compilers, it is necessary that they are tested thoroughly, so that most of the bugs are detected early on. Unfortunately, modern compilers such as GCC do not target testing of individual components of a compiler, but instead perform end-to-end testing. In this paper, we target the problem of checking correctness of the architecture specifications used by code generators of modern compilers. Our solution leverages the architecture of modern compilers where a language-specific front-end compiles source-code into an intermediate representation (IR), which is then translated by the compiler's code generator into assembly code. Hence our approach is to test code generators by testing the equivalence of IR snippets and the corresponding assembly code generated. For this purpose, we have developed an efficient, architecture-neutral test case generation strategy. Using our prototype implementation, we performed correctness checking of 140 assembly instructions (80 general-purpose and 60 SSE out of around 600×86 instructions) of GCC's ×86 code generator, and found semantic differences in 39 of them, at least one of which has already been fixed by the GCC community in response to our report. We believe that our approach can be invaluable when developing support for a new architecture, as well as during frequent updates made to existing architectures such as ×86 for the purpose of supporting new instructions.","","978-1-4799-8161","10.1109/CGO.2015.7054197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7054197","","Generators;Testing;Assembly;Semantics;Silicon;Computer bugs;Registers","formal specification;instruction sets;program compilers;program debugging;program testing;source code (software)","checking correctness;code generator architecture specifications;instruction sets;compilers;bugs;end-to-end testing;language-specific front-end;source-code;intermediate representation;assembly code;architecture-neutral test case generation strategy;GCC community;IR snippets","","5","21","","","","","","IEEE","IEEE Conferences"
"Design and research of three-layers open architecture model for industrial robot software system","J. Fang; J. Zhao; F. He; X. Lin","School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui Province, 230027, China; Institute of Advanced Manufacturing Technology, Institute of Hefei Institutes of Physical Science, Chinese Academy of Sciences, 230031, China; Institute of Advanced Manufacturing Technology, Institute of Hefei Institutes of Physical Science, Chinese Academy of Sciences, 230031, China; Institute of Advanced Manufacturing Technology, Institute of Hefei Institutes of Physical Science, Chinese Academy of Sciences, 230031, China","2013 IEEE International Conference on Mechatronics and Automation","","2013","","","104","109","A novel software architecture model for the industrial robot is proposed in this paper to slove the problem of information redundancy and logic complexity which exist in the traditional software architecture designed by the up to down theory. In this model, the industrial robot software system is divided into three-layers: interaction layer, decision layer and physical layer based on the software layering theory. In this way, we improve the industrial robot system's data processing ability and response speed effectively. In order to improve the performance of software scalability and practicability, an open interface is set on each layer based on open software theory. To modified the system's ability of data processing ,we optimize the algorithm of kinematics and inverse kinematics. At last, We also implement this architecture model in the programming environment of Visual C++ 6.0 with MFC's Dialog Template and demonstrate its application in 4 axises industrial transfer robot. The result shows that this model can get a superior performance on the aspect of the robot's stability and improve the efficiency of production at the same time.","2152-7431;2152-744X","978-1-4673-5560-5978-1-4673-5557-5978-1-4673-5558","10.1109/ICMA.2013.6617901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617901","Three-layers open architecture model;Industrial robot;Industrial application","Robots;Programming;Robustness;Testing;Kinematics","control engineering computing;industrial robots;object-oriented programming;programming environments;robot kinematics;robot programming;software architecture;stability;visual programming","robot stability;industrial transfer robot;MFC dialog template;Visual C++ 6.0;programming environment;inverse kinematics;open software theory;open interface;software scalability;response speed;data processing ability;software layering theory;physical layer;decision layer;interaction layer;up-to-down theory;logic complexity;information redundancy;software architecture model;industrial robot software system;three-layers open architecture model","","3","17","","","","","","IEEE","IEEE Conferences"
"RF measurement and analysis of 2G GSM network performance case study: Yogyakarta Indonesia","T. Yuwono; F. Ferdiyanto","Department of Electrical Engineering, Islamic University of Indonesia Yogyakarta, Kaliurang Street KM 14 Yogyakarta, Indonesia; Department of Electrical Engineering, Islamic University of Indonesia Yogyakarta, Kaliurang Street KM 14 Yogyakarta, Indonesia","2015 IEEE 3rd International Conference on Smart Instrumentation, Measurement and Applications (ICSIMA)","","2015","","","1","5","The development of telecommunication is very rapid. Although it reach the fourth generation, GSM is still have the most subscribers. To improve the performance of the GSM system, the system must be optimized periodically. The first stage of optimization process is measuring the RF parameters in the GSM system. The process of measurement is called drive test. The objective of drive test is to get the data from network. In the GSM network, The data include Received signal level (RxLev), Received Signal quality (RxQual), SQI (Speech Quality Index), The equipments in this research are GPS, mobile phone, notebook, and drive test software. This paper will discuss step by step in the RF measurement in 2G GSM network. The location of research are in Yogyakarta Indonesia. We take the sample in urban area, sub urban area, and rural area. The result of measurement as below: the average value of Rx Lev, respectively -80.78 dBm, -76.70 dBm, and -69.68 dBm for rural, sub urban, and urban. The average of Rx Quals are 1.68, 0.84, and 1.05, and the average of SQI values are 23.65, 23.73, and 24.43 for rural, sub urban, and urban. So by these results, the generaly, the performance of GSM services is good. But for few areas, they have bad service, so operator should improve and give solution for these areas like increase the transmission power and antenna position.","","978-1-4673-7255-8978-1-4673-7254-1978-1-4673-7256","10.1109/ICSIMA.2015.7559010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559010","2G Performance;RxLev;Rx Qual;SQI","GSM;Urban areas;Optimization;Radio frequency;Mobile communication;Quality of service;Speech","4G mobile communication;cellular radio;Global Positioning System;mobile handsets;notebook computers;optimisation","rural area;sub urban area;urban area;drive test software;notebook;GPS;mobile phone;speech quality index;SQI;RxLev;RxQual;received signal quality;received signal level;RF parameter measurement;optimization process;fourth generation network;Yogyakarta Indonesia;2G GSM network performance analysis","","1","13","","","","","","IEEE","IEEE Conferences"
"Using static analysis data for performance modeling and prediction","J. Noudohouenou; W. Jalby","Exascale Computing Research, University of Versailles Saint-Quentin-en-Yvelines, France; Exascale Computing Research, University of Versailles Saint-Quentin-en-Yvelines, France","2014 International Conference on High Performance Computing & Simulation (HPCS)","","2014","","","933","942","In general, high performance computing applications have large codebases composed of various scientific algorithms which must be tuned to achieve optimal speed. Therefore, a programmer extracts pieces of code from large programs, as candidates for the performance tuning. Maximizing such code performance requires measurement, analysis, and optimization strategies, targeting hardware components. Furthermore, computer architecture improvement raises hardware co-design issues such as measuring detailed computer performance. Currently, code execution time is well measured, but it is much harder to break out the performance contributory details per hardware resource in order to predict a code performance. This paper presents the Ubenchface tool, a framework for performance prediction and knowledge discovery. Inversely to traditional measurement methods and modeling, the proposed tool considers static metrics to analyze and tune application performance. This framework is more informative than simple benchmarking, or microbenchmarking. It is useful for performance investigations in similarity and redundancy study concerning benchmark suites, predicting, understanding scaling, and tuning.","","978-1-4799-5313-4978-1-4799-5312-7978-1-4799-5311","10.1109/HPCSim.2014.6903789","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903789","Modeling;Simulation and Evaluation Techniques of HPC Systems;Benchmarking and Assessment;Software Monitoring and Measurement;Data Mining and Searching","Hardware;Benchmark testing;Redundancy;Software;Current measurement;Arrays;Random access memory","benchmark testing;data mining;natural sciences computing;parallel processing;program diagnostics;software metrics;software performance evaluation","static analysis data;performance modeling;high performance computing applications;codebase;scientific algorithms;optimal speed;code extraction;code performance maximization;optimization strategies;hardware components;computer architecture improvement;hardware codesign issues;computer performance measurement;code execution time;hardware resource;code performance prediction;Ubenchface tool;knowledge discovery;static metrics;application performance analysis;application performance tuning;microbenchmarking","","","21","","","","","","IEEE","IEEE Conferences"
"Vectorization past dependent branches through speculation","M. H. Sujon; R. C. Whaley; Q. Yi","Dept. of Computer Science, Univ. of TX at San Antonio, 78249, USA; School of EE &amp; CS / CCT, Louisiana State University, Baton Rouge, 70803, USA; Dept. of Computer Science, Univ. of Colorado Colorado Springs, 80918, USA","Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques","","2013","","","353","362","Modern architectures increasingly rely on SIMD vectorization to improve performance for floating point intensive scientific applications. However, existing compiler optimization techniques for automatic vectorization are inhibited by the presence of unknown control flow surrounding partially vectorizable computations. In this paper, we present a new approach, speculative vectorization, which speculates past dependent branches to aggressively vectorize computational paths that are expected to be taken frequently at runtime, while simply restarting the calculation using scalar instructions when the speculation fails. We have integrated our technique in an iterative optimizing compiler and have employed empirical tuning to select the profitable paths for speculation. When applied to optimize 9 floating-point benchmarks, our optimizing compiler has achieved up to 6.8X speedup for single precision and 3.4X for double precision kernels using AVX, while vectorizing some operations considered not vectorizable by prior techniques.","1089-795X","978-1-4799-1021-2978-1-4799-1018","10.1109/PACT.2013.6618831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618831","SIMD Vectorization;speculation;compiler optimization;iterative compilation;ATLAS;iFKO","Vectors;Kernel;Optimization;Algorithm design and analysis;Optimizing compilers;Benchmark testing;Safety","optimisation;parallel processing;program compilers;software architecture;vectors","modern architectures;SIMD vectorization;floating point intensive scientific applications;compiler optimization techniques;floating-point benchmarks","","","29","","","","","","IEEE","IEEE Conferences"
"Integrating electric vehicles into smart grid infrastructures a simulation-based approach that became reality","M. Lützenberger; T. Küster; S. Albayrak","Technische Universität Berlin, Distributed Artificial Intelligence Laboratory, Ernst-Reuter-Platz 7, 10587, GERMANY; Technische Universität Berlin, Distributed Artificial Intelligence Laboratory, Ernst-Reuter-Platz 7, 10587, GERMANY; Technische Universität Berlin, Distributed Artificial Intelligence Laboratory, Ernst-Reuter-Platz 7, 10587, GERMANY","Proceedings of the Winter Simulation Conference 2014","","2014","","","1061","1072","The development of software that controls real life processes can be highly difficult and error prone. In the case that the destination test-bed does not fully exist, the situation becomes significantly more challenging. We developed a control software for charging processes of an electric vehicle fleet in a smart grid architecture. To accelerate the development and to ease the integration process, we used an agent-based approach and embedded the optimization software within a simulation environment. Later we enhanced this simulation environment to a consultant tool which can be used to assess the impact of structural extensions. In this paper we present both, the optimization mechanism as well as the simulation environment.","0891-7736;1558-4305","978-1-4799-7486-3978-1-4799-7484","10.1109/WSC.2014.7019965","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7019965","","Optimization;Schedules;Smart grids;Software;Electric vehicles;Computer architecture","electric vehicles;optimisation;power engineering computing;smart power grids;software engineering","electric vehicles;smart grid infrastructures;simulation-based approach;software development;smart grid architecture;agent-based approach;optimization software","","4","9","","","","","","IEEE","IEEE Conferences"
"Design and Analysis of HTS Cable Termination Stress Cones","J. Fang; Z. Shen; X. H. Huang; Y. X. Li; D. L. Dong; H. F. Li; T. Yu; H. J. Zhang; M. Qiu","School of Electrical Engineering, Beijing Jiaotong University, Beijing, China; School of Electrical Engineering, Beijing Jiaotong University, Beijing, China; China Electric Power Research Institute, Beijing, China; School of Electrical Engineering, Beijing Jiaotong University, Beijing, China; School of Electrical Engineering, Beijing Jiaotong University, Beijing, China; School of Electrical Engineering, Beijing Jiaotong University, Beijing, China; School of Electrical Engineering, Beijing Jiaotong University, Beijing, China; China Electric Power Research Institute, Beijing, China; China Electric Power Research Institute, Beijing, China","IEEE Transactions on Applied Superconductivity","","2013","23","6","90","98","High-temperature superconducting (HTS) cable terminal is an important part of the HTS cable system, which functions to improve the internal electric field distribution and the electrical insulation strength of the cable termination. In this paper, the particle swarm optimization method is adopted to the 35-kV HTS cable terminal stress cone. The capacitance, radial electric field, axial electric field, voltage, and other parameters of the stress cone were compared before and after the optimization of the stress cone. The simulations of the stress cone with the optimized structure were made using finite-element analysis software ANSYS. Finally, a capacitor voltage test verifies the equal capacitance and gradient design of stress cone.","1051-8223;1558-2515;2378-7074","","10.1109/TASC.2013.2275754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6587756","Electrical insulation strength;electric field distribution;high-temperature superconducting (HTS) cable termination;particle swarm optimization (PSO)","Stress;Optimization;Superconducting cables;Capacitance;High-temperature superconductors;Power cables;Cable insulation","cable insulation;capacitance;finite element analysis;high-temperature superconductors;particle swarm optimisation;superconducting cables;superconducting device testing","HTS cable termination stress cones;high-temperature superconducting cable terminal;HTS cable system;internal electric field distribution;electrical insulation strength;particle swarm optimization method;capacitance;radial electric field;axial electric field;finite-element analysis software ANSYS;capacitor voltage test;gradient design","","1","13","","","","","","IEEE","IEEE Journals & Magazines"
"A scalable method to measure similarity between two EDA-generated timing graphs","J. L. M. Lee","Software Engineering, Altera Corporation Malaysia, Penang, Malaysia 11900","2015 International Conference on Computer, Communications, and Control Technology (I4CT)","","2015","","","44","48","This is a case study of the use of graph similarity to correlate the timing models obtained from two electronic design automation (EDA) static timing analysis tools (Altera's Quartus II software versus Synopsys PrimeTime). Timing models are data modelled from the post-layout netlist and parasitics extraction. The field programmable gate array's (FPGA) timing model is used by customers to optimize their designs. As mask designs are constantly revisioned, new timing models are generated and thus the Quartus II software must constantly be updated with the new models. A verification needs to be carried out at every new iteration of timing models to ensure the regression testing of timing models is stable and reliable. This case study discusses one such verification methodology. A timing graph consists of nodes and edges. Edges have weights attached to them that can denote some characteristic, such as timing arc delays in this case. Out of the many graph similarity algorithms in the field, the most cited are edit distance similarity, neighbourhood matching, spectral matching and belief propagation. Neighbourhood matching, which was used in this study, is a point-to-point matching of a node's similarity score based on its neighbourhood's similarity score. The timing graph from the Quartus II software was generated with an in-house Tcl scripting language applications programming interface. The timing graph from PrimeTime was generated from its timing reports. An algorithm was postulated to calculate graph similarity based on edge weights of the graphs. The algorithm compared both graphs and produced a matrix of graph similarity scores for all paired nodes. The algorithm was tested on five data paths taken from the two EDA tools under evaluation. Our results showed good correlation between intuitive similarity measure and our algorithmic calculation.","","978-1-4799-7952-3978-1-4799-7951","10.1109/I4CT.2015.7219534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219534","timing graph;point-to-point matching;graph similarity;FPGA timing model","Delays;Field programmable gate arrays;Software;Data models;Symmetric matrices;Software algorithms","electronic design automation;field programmable gate arrays;graph theory;program testing;program verification;timing circuits","scalable method;similarity measure;EDA-generated timing graphs;electronic design automation static timing analysis tools;Altera Quartus II software;Synopsys PrimeTime;postlayout netlist;parasitics extraction;field programmable gate array timing model;FPGA timing model;design optimization;regression testing;verification methodology;graph nodes;graph edges;timing arc delays;edit distance similarity;neighbourhood matching;spectral matching;belief propagation;point-to-point matching;node similarity score;neighbourhood similarity score;Tcl scripting language application programming interface;timing reports;graph similarity score matrix;data paths;intuitive similarity measure","","","10","","","","","","IEEE","IEEE Conferences"
"Joint mutual coupling characterization and swarm optimization for efficient base station antenna beamforming","D. Samb; Z. Wu; M. Liu; S. Lei","Dept. of BSA, Tongyu Communication Inc., Zhongshan 528437, China; Dept. of BSA, Tongyu Communication Inc., Zhongshan 528437, China; Dept. of BSA, Tongyu Communication Inc., Zhongshan 528437, China; Dept. of BSA, Tongyu Communication Inc., Zhongshan 528437, China","2015 International Symposium on Antennas and Propagation (ISAP)","","2015","","","1","4","In this work, practical beamforming technique is proposed for improving directional radiation properties of base station antennas operating at 1710-2690MHz. Firstly, we characterize the mutual coupling between antenna elements using Rohde &amp; Schwartz multi-ports network analyzer. And with the active radiation parameters (from High Frequency Simulation Software), the real excitation coefficients of the different array elements' field are outputted by developing a MATLAB algorithm. Next, we use these excitation coefficients as inputs and then develop a swarm algorithm to compute the optimum phases of the array elements for a given sidelobes level requirement and beam steered angle. Experimental results are provided to confirm the analytical approach and the advantage of using mutual coupling characterization and swarm technique in base station antenna design process. Interestingly, results show that the analytical model approach can help to predict even the statistical performances of the antenna before testing it as minimum gap between analytical and experiment results has been observed.","","978-4-8855-2302-1978-4-8855-2303","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447416","Mutual Coupling Characterization;Particle swarm;Base station antenna","Mutual coupling;Antenna arrays;Array signal processing;Algorithm design and analysis;Particle swarm optimization;Scattering parameters","antenna arrays;antenna radiation patterns;array signal processing;beam steering;directive antennas;network analysers;particle swarm optimisation;UHF antennas","joint mutual coupling characterization;particle swarm optimization algorithm;efficient base station antenna beamforming;directional radiation properties;Rohde & Schwartz multiport network analyzer;active radiation parameters;array element field excitation coefficients;Matlab algorithm;beam steered angle;base station antenna design process;analytical model approach;statistical performances;frequency 1710 MHz to 2690 MHz","","","15","","","","","","IEEE","IEEE Conferences"
"Partitioning of ECE schemes components based on modified graph coloring algorithm","V. V. Kureichik; V. V. Kureichik; D. V. Zaruba","Southern Federal University, College of Automation and Computer Science, Russia; Southern Federal University, College of Automation and Computer Science, Russia; Southern Federal University, College of Automation and Computer Science, Russia","Proceedings of IEEE East-West Design & Test Symposium (EWDTS 2014)","","2014","","","1","4","One of the most important design problems - electronic computing equipment (ECE) schemes components partitioning problem is considered in the article. It belongs to the class of NP-hard problems. Statement of a partitioning problem and an optimization criterion are defined. A new approach for solving partitioning problem based on the modified graph coloring algorithm is suggested. The modified partitioning algorithm which provides obtained solutions of a specified accuracy in polynomial time is developed. A modified graph coloring heuristic is described. Authors suggested a procedure for the transition from colored subsets to specify parts of the partition. The program environment and computing experiment are implemented. The series of tests and experiments have allowed specifying theoretical estimations of partitioning algorithms running time. The running time of the algorithm is represented as O (n<sup>2</sup>).","","978-1-4799-7630","10.1109/EWDTS.2014.7027062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027062","","Partitioning algorithms;Algorithm design and analysis;Color;Educational institutions;Merging;Polynomials;Software algorithms","computational complexity;electronic engineering computing;graph colouring;optimisation;polynomials","partitioning algorithms running time;theoretical estimations;program environment;colored subsets;polynomial time;optimization criterion;NP-hard problems;component partitioning problem;electronic computing equipment schemes;design problems;modified graph coloring heuristic algorithm;ECE scheme components","","5","7","","","","","","IEEE","IEEE Conferences"
"Correlation-based re-ranking for semantic concept detection","H. Ha; F. C. Fleites; S. Chen; M. Chen","School of Computing and Information Sciences, Florida International University, Miami, FL 33199, USA; School of Computing and Information Sciences, Florida International University, Miami, FL 33199, USA; School of Computing and Information Sciences, Florida International University, Miami, FL 33199, USA; Computing and Software Systems School of STEM University of Washington Bothell, WA 98011, USA","Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014)","","2014","","","765","770","Semantic concept detection is among the most important and challenging topics in multimedia research. Its objective is to effectively identify high-level semantic concepts from low-level features for multimedia data analysis and management. In this paper, a novel re-ranking method is proposed based on correlation among concepts to automatically refine detection results and improve detection accuracy. Specifically, multiple correspondence analysis (MCA) is utilized to capture the relationship between a targeted concept and all other semantic concepts. Such relationship is then used as a transaction weight to refine detection ranking scores. To demonstrate its effectiveness in refining semantic concept detection, the proposed re-ranking method is applied to the detection scores of TRECVID 2011 benchmark data set, and its performance is compared with other state-of-the-art re-ranking approaches.","","978-1-4799-5880","10.1109/IRI.2014.7051966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051966","","Semantics;Correlation;Multimedia communication;Equations;Testing;Data mining;Educational institutions","information retrieval;multimedia systems;pattern classification","TRECVID 2011 benchmark data set;detection ranking score;transaction weight;MCA;multiple correspondence analysis;multimedia data management;multimedia data analysis;multimedia research;semantic concept detection;correlation-based re-ranking","","4","19","","","","","","IEEE","IEEE Conferences"
"Artificial bee colony algorithm with comprehensive search mechanism for numerical optimization","M. Li; H. Zhao; X. Weng; H. Huang","Department of Aeronautics and Astronautics Engineering, Air Force Engineering University, Xi'an 710038, China; Department of Aeronautics and Astronautics Engineering, Air Force Engineering University, Xi'an 710038, China; Department of Aeronautics and Astronautics Engineering, Air Force Engineering University, Xi'an 710038, China; Department of Aeronautics and Astronautics Engineering, Air Force Engineering University, Xi'an 710038, China","Journal of Systems Engineering and Electronics","","2015","26","3","603","617","The artificial bee colony (ABC) algorithm is a simple and effective global optimization algorithm which has been successfully applied in practical optimization problems of various fields. However, the algorithm is still insufficient in balancing exploration and exploitation. To solve this problem, we put forward an improved algorithm with a comprehensive search mechanism. The search mechanism contains three main strategies. Firstly, the heuristic Gaussian search strategy composed of three different search equations is proposed for the employed bees, which fully utilizes and balances the exploration and exploitation of the three different search equations by introducing the selectivity probability Ps. Secondly, in order to improve the search accuracy, we propose the Gbest-guided neighborhood search strategy for onlooker bees to improve the exploitation performance of ABC. Thirdly, the self-adaptive population perturbation strategy for the current colony is used by random perturbation or Gaussian perturbation to enhance the diversity of the population. In addition, to improve the quality of the initial population, we introduce the chaotic opposition-based learning method for initialization. The experimental results and Wilcoxon signed ranks test based on 27 benchmark functions show that the proposed algorithm, especially for solving high dimensional and complex function optimization problems, has a higher convergence speed and search precision than ABC and three other current ABC-based algorithms.","1004-4132","","10.1109/JSEE.2015.00068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170019","artificial bee colony (ABC);function optimization;search strategy;population initialization;Wilcoxon signed ranks test","Search problems;Tin;Optimization;Sociology;Statistics;Convergence;Mathematical model","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"Notice of Removal<BR>Remarkable researches in SICE-JSAE benchmark problem</BR>","Y. Yasui; S. Komori; T. Kawabe; Y. Hirano; A. Ohata; S. Watanabe","Automotive R&D Center, Honda R&D Co., Ltd., Tochigi, Japan; Integrated Control System Development Div., Mazda Motor Corporation, Hiroshima, Japan; Faculty of Information Science and Electrical Engineering, Kyushu university, Fukuoka, Japan; Future Project Division, Toyota Motor Corporation, Shizuoka, Japan; Higashifuji Technical Center, Toyota Motor Corporation, Shizuoka, Japan; Higashifuji Technical Center, Toyota Motor Corporation, Shizuoka, Japan","2015 54th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)","","2015","","","1347","1348","The environment surrounding the automotive industry is becoming increasingly severe, but automotive companies will secure competitiveness and continue to develop attractive products. In the meantime, to create new value, the integration information system, social systems, and transportation system is progressing remarkably. At the same time, the responsibility for safety and reliability of control software, which becomes complicated, must not be avoided.","","978-4-9077-6448","10.1109/SICE.2015.7285578","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7285578","Automotive control;Optimization;Identification;Modeling;Model predictive control;Robust control","Benchmark testing;Automotive engineering;Atmospheric modeling;Industries;Centralized control;Optimization;Predictive models","automobile industry;optimisation;predictive control;robust control","SICE-JSAE benchmark problem;automotive control;optimization;model predictive control;robust control;automotive industry;integration information system;social systems;transportation system;reliability","","","","","","","","","IEEE","IEEE Conferences"
"Effective verification of low-level software with nested interrupts","D. Kroening; L. Liang; T. Melham; P. Schrammel; M. Tautschnig","University of Oxford, UK; University of Oxford, UK; University of Oxford, UK; University of Oxford, UK; Queen Mary, University of London, UK","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","229","234","Interrupt-driven software is difficult to test and debug, especially when interrupts can be nested and subject to priorities. Interrupts can arrive at arbitrary times, leading to an explosion in the number of cases to be considered. We present a new formal approach to verifying interrupt-driven software based on symbolic execution. The approach leverages recent advances in the encoding of the execution traces of interacting, concurrent threads. We assess the performance of our method on benchmarks drawn from embedded systems code and device drivers, and experimentally compare it to conventional formal approaches that use source-to-source transformations. Our experimental results show that our method significantly outperforms conventional techniques. To the best of our knowledge, our technique is the first to demonstrate effective formal verification of low-level embedded software with nested interrupts.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0360","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092387","","Encoding;Benchmark testing;Instruments;Radio frequency;Instruction sets;Semantics","embedded systems;formal verification;interrupts;symbol manipulation","effective low-level software verification;nested interrupts;formal approach;interrupt-driven software;symbolic execution;embedded systems code;device drivers;source-to-source transformations;formal verification;low-level embedded software","","2","15","","","","","","IEEE","IEEE Conferences"
"GPU Computing Pipeline Inefficiencies and Optimization Opportunities in Heterogeneous CPU-GPU Processors","J. Hestness; S. W. Keckler; D. A. Wood","NA; NA; NA","2015 IEEE International Symposium on Workload Characterization","","2015","","","87","97","Emerging heterogeneous CPU-GPU processors have introduced unified memory spaces and cache coherence. CPU and GPU cores will be able to concurrently access the same memories, eliminating memory copy overheads and potentially changing the application-level optimization targets. To date, little is known about how developers may organize new applications to leverage the available, finer-grained communication in these processors. However, understanding potential application optimizations and adaptations is critical for directing heterogeneous processor programming model and architectural development. This paper quantifies opportunities for applications and architectures to evolve to leverage the new capabilities of heterogeneous processors. To identify these opportunities, we ported and simulated a broad set of benchmarks originally developed for discrete GPUs to remove memory copies, and applied analytical models to quantify their application-level pipeline inefficiencies. For existing benchmarks, GPU bulk-synchronous software pipelines result in considerable core and cache utilization inefficiency. For heterogeneous processors, the results indicate increased opportunity for techniques that provide flexible compute and data granularities, and support for efficient producer-consumer data handling and synchronization within caches.","","978-1-5090-0088-3978-1-5090-0087","10.1109/IISWC.2015.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314150","GPGPU;GPU computing;heterogeneous processors;benchmarking","Graphics processing units;Benchmark testing;Kernel;Optimization;Pipelines;Synchronization","graphics processing units;memory architecture;optimisation;pipeline processing","GPU computing pipeline inefficiency;optimization opportunity;heterogeneous CPU-GPU processor;unified memory space;cache coherence;CPU core;GPU core;memory copy overhead;application-level optimization;finer-grained communication;heterogeneous processor programming model;architectural development;application-level pipeline inefficiency;GPU bulk-synchronous software pipeline","","10","38","","","","","","IEEE","IEEE Conferences"
"A new software for power flow solution computing using LabVIEW","S. Souag; F. Benhamida; F. Z. Gherbi; A. Graa","Irecom laboratory, dept. of electrotechnics, UDL university of Sidi Bel Abbes, Algeria; Irecom laboratory, dept. of electrotechnics, UDL university of Sidi Bel Abbes, Algeria; Iceps laboratory, dept. of electrotechnics, UDL university of Sidi Bel Abbes, Algeria; Iceps laboratory, dept. of electrotechnics, UDL university of Sidi Bel Abbes, Algeria","2013 5th International Conference on Modeling, Simulation and Applied Optimization (ICMSAO)","","2013","","","1","6","In this paper we present a new program for solving the power flow problem using the graphical programming environment of LabVIEW as a virtual instrument. This program is used for computing power flow using Newton-Raphson algorithm and DC power flow. Or this latter greatly simplifies the power flow by making a number of approximations. Hence the DC power flow reduces the power flow problem to a set of linear equations. The effectiveness of the method developed is identified through its application to a 6 buses test system. The calculation results show excellent performance of the proposed method, in regard to computation time and quality of results.","","978-1-4673-5814-9978-1-4673-5812-5978-1-4673-5813","10.1109/ICMSAO.2013.6552631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552631","power flow;DC power flow;LabVIEW;virtual instrument","Load flow;Reactive power;Equations;Mathematical model;Newton method;Power system stability","load flow;Newton-Raphson method;power engineering computing;virtual instrumentation;visual programming","power flow solution computing;Labview;graphical programming environment;virtual instrument;Newton-Raphson algorithm;DC power flow;linear equations;6 buses test system","","","12","","","","","","IEEE","IEEE Conferences"
"Patching a patch — Software updates using horizontal patching","M. Stolikj; P. J. L. Cuijpers; J. J. Lukkien","Dept. of Mathematics and Computer Science, Eindhoven University of Technology, P.O. Box 513, 5600 MB, Eindhoven, The Netherlands; Dept. of Mathematics and Computer Science, Eindhoven University of Technology, P.O. Box 513, 5600 MB, Eindhoven, The Netherlands; Dept. of Mathematics and Computer Science, Eindhoven University of Technology, P.O. Box 513, 5600 MB, Eindhoven, The Netherlands","2013 IEEE International Conference on Consumer Electronics (ICCE)","","2013","","","647","648","This paper presents a method for optimizing incremental updates of consumer electronic devices running multiple applications, called horizontal patching. Instead of using separate deltas for patching different applications, the method generates one delta from the other. Due to the large similarities between the deltas, this horizontal delta is small in size. In all test cases horizontal patching produced smaller deltas, with compression ratios between 8.02% and 43.38%.","2158-3994;2158-3994;2158-4001","978-1-4673-1363-6978-1-4673-1361-2978-1-4673-1362","10.1109/ICCE.2013.6487054","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6487054","","Operating systems;Consumer electronics;Educational institutions;Lighting;Protocols;Encoding","software engineering;ubiquitous computing","software update;horizontal patching method;consumer electronic device;horizontal delta;delta similarity;compression ratio;pervasive system;incremental update optimization","","","7","","","","","","IEEE","IEEE Conferences"
"UnetStack: An agent-based software stack and simulator for underwater networks","M. Chitre; R. Bhatnagar; W. Soh","Department of Electrical and Computer Engineering, and ARL, Tropical Marine Science Institute, National University of Singapore; ARL, Tropical Marine Science Institute, National University of Singapore; Department of Electrical and Computer Engineering, National University of Singapore","2014 Oceans - St. John's","","2014","","","1","10","To deploy successful underwater networks in the face of challenges such as low bandwidth, long propagation delay, half-duplex nature of links, high packet loss and time variability, we require highly optimized network protocols with low overhead and significant cross-layer information sharing. UnetStack is a network stack designed to provide a good balance between separation of concern, and information sharing. By replacing a traditional layered stack architecture by an agent-based architecture, we provide additional flexibility that allows novel protocols to be easily implemented, deployed and tested. In discrete-event simulation mode, UnetStack can be used on desktop/laptop computers or computing clusters to simulate underwater networks and test protocol performance. In real-time simulation mode, it can be used to interactively debug protocol implementations, and test deployment scenarios prior to an experiment. Once tested, the protocols can simply be copied to an underwater modem with UnetStack support, and deployed in the field. The stack implementation has been extensively tested, not only through carefully calibrated simulations, but also in several field experiments. We provide an overview of UnetStack and briefly discuss a few deployments to illustrate some of its key features.","0197-7385","978-1-4799-4918-2978-1-4799-4920-5978-1-4799-4919","10.1109/OCEANS.2014.7003044","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7003044","","Protocols;Modems;Reliability;Peer-to-peer computing;Waste materials;Distance measurement;Computer architecture","discrete event simulation;protocols;software agents;software architecture;telecommunication computing;underwater acoustic communication","UnetStack;agent-based software stack;agent-based software simulator;underwater networks;network protocols;cross-layer information sharing;network stack;layered stack architecture;agent-based architecture;discrete-event simulation mode;desktop-laptop computers;test protocol performance;real-time simulation mode;underwater modem","","20","24","","","","","","IEEE","IEEE Conferences"
"Optimized Simulation Acceleration with Partial Testbench Evaluation","S. Banerjee; T. Gupta","NA; NA","2014 15th International Microprocessor Test and Verification Workshop","","2014","","","22","27","With growing adoption of the simulation acceleration technology for early functional verification of complex System-on-chip (SoC) designs, high level test benches contain substantial number of calls for debug and design state manipulation, to increase functional coverage. Ever increasing size and complexity of digital designs make these calls compute and IO intensive, requiring large software pre-processing time to prepare data suitable for feeding into the hardware accelerator. On the hardware side, growing design sizes are easily handled by parallelism inherently present in hardware accelerators with reconfigurable architecture, but it is often not trivial to speed up complex software test benches via parallel processing. In this paper, we present a novel methodology which enhances the performance of the system significantly by moving the software pre-processing time to an offline step named Partial Test bench Evaluation (PTE), which ""partially evaluates"" a test bench by running a pure simulation phase without requiring the hardware accelerator to be connected and generates optimized data to be directly fed into the hardware accelerator. Actual simulation acceleration with a partially evaluated test bench executes much faster than the same with the original test bench.","1550-4093;2332-5674","978-1-4673-6858","10.1109/MTV.2014.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7087228","Partial evaluation;simulation acceleration","Hardware;Acceleration;Software;Computational modeling;Databases;Life estimation;Heuristic algorithms","digital simulation;logic design;parallel processing;system-on-chip","partial testbench evaluation;simulation acceleration technology;SoC design;system-on-chip design;functional coverage;digital design;software preprocessing time;hardware accelerator;reconfigurable architecture;parallel processing;software PTE","","","11","","","","","","IEEE","IEEE Conferences"
"Issues and improvements of hardware/software co-design sensorless implementation in a permanent magnet synchronous motor using Veristand","V. Miñambres-Marcos; M. A. Guerrero-Martínez; E. Romero-Cadaval; J. Gutiérrez","Power Electrical & Electronic Systems Research Group (PE&ES). Universidad de Extremadura: Escuela de Ingenierías Industriales. Avda. de Elvas s/n, 06006, Badajoz, Spain; Power Electrical & Electronic Systems Research Group (PE&ES). Universidad de Extremadura: Escuela de Ingenierías Industriales. Avda. de Elvas s/n, 06006, Badajoz, Spain; Power Electrical & Electronic Systems Research Group (PE&ES). Universidad de Extremadura: Escuela de Ingenierías Industriales. Avda. de Elvas s/n, 06006, Badajoz, Spain; National Instruments: BDM Embedded Design and Test. C/ Rozabella, 2 - Berlin, 1st Pl. 28230 Las Rozas, Madrid, Spain","2013 IEEE International Symposium on Sensorless Control for Electrical Drives and Predictive Control of Electrical Drives and Power Electronics (SLED/PRECEDE)","","2013","","","1","7","This paper is focused on the implementation of a field oriented control of a permanent magnet synchronous in the emerging rapid control prototyping tool Veristand through a National Instruments PXI device. The device is composed of a FPGA and a controller so the hardware/software co-design is discussed in order to optimize the control system. The algorithm is solved by calculating the position of the rotor in two ways: firstly with the resolver sensor and secondly with a signal injection sensorless method to compare the position results. Both configurations are simulated to tune the system properly, and finally they are tested experimentally with the mentioned control platform. Taking advantage of the Veristand hardware/software co-design flexibility, the sensorless method was implemented in two ways in order to demonstrate the flexibility of the control system and compare the results of both solutions.","2166-6725;2166-6733","978-1-4799-0681-9978-1-4799-0680","10.1109/SLED-PRECEDE.2013.6684523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684523","Hardware/Software Co-design;Rapid Prototyping;Permanent Magnet Synchronous Motor;Resolver;Signal Injection Sensorless;Field Oriented Control","Field programmable gate arrays;Rotors;Signal resolution;Mathematical model;Hardware;Torque;Control systems","field programmable gate arrays;hardware-software codesign;machine vector control;permanent magnet motors;rapid prototyping (industrial);sensorless machine control;synchronous motors","hardware software codesign sensorless implementation;permanent magnet synchronous motor;Veristand;field oriented control;rapid control prototyping;FPGA;resolver sensor;signal injection sensorless method","","2","17","","","","","","IEEE","IEEE Conferences"
"A Sampling Diagnostics Model for Neural System Training Optimization","D. A. Montini; G. R. Matuck; A. M. Da Cunha; L. A. V. Dias; A. L. P. Ribeiro; A. A. Montini","NA; NA; NA; NA; NA; NA","2013 10th International Conference on Information Technology: New Generations","","2013","","","510","517","This paper describes a hybrid-sampling model for bank fraud diagnosis, including those for multiple frauds in a banking system. The Multi-Layer Perceptron (MLP) network was used to analyze similarity, together with a statistical optimization model for sampling, to reduce the volume of used data in the diagnostics phase. The created MLP was utilized for banking transactions learning, in order to detect frauds. This neural network was tested with different configurations to improve diagnosis. The hybrid-sampling model was also employed to improve training results. The results have shown that the optimization strategy reduced the database volume and improved the learning process, presenting similar precisions to diagnose frauds detection, within this hybrid-sampling model.","","978-0-7695-4967-5978-0-7695-4967","10.1109/ITNG.2013.138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614358","Model-Based Software Development; Statistical Sampling Model; Neural Networks; MLP; Banking Frauds Detection","Sociology;Statistics;Banking;Probabilistic logic;Computational modeling;Optimization;Data models","bank data processing;data reduction;fraud;learning (artificial intelligence);multilayer perceptrons;optimisation;sampling methods","sampling diagnostics model;neural system training optimization;hybrid-sampling model;bank fraud diagnosis;banking system;multilayer perceptron;MLP network;similarity analysis;statistical optimization model;data volume reduction;banking transactions learning;fraud detection;neural network;optimization strategy;database volume reduction;learning process improvement","","1","24","","","","","","IEEE","IEEE Conferences"
"Reconfiguration and DG placement considering critical system condition","S. J. Mirazimi; M. Nematollahi; M. H. Ashourian; S. Mirahmadi","Faculty of Engineering and Surveying, University of Southern Queensland (USQ), Australia; Department of Electrical Najafabad Branch, Islamic Azad University of Iran, Iran; Behrad Consulting Engineers Co. Isfahan, Iran; Department of Electrical Engineering, Isfahan Higher Education and Research Integrated Electricity &amp; Water Industry, Iran","2013 IEEE 7th International Power Engineering and Optimization Conference (PEOCO)","","2013","","","676","679","This paper offers a method to reconfiguration and DG placement simultaneously considering critical system condition in distribution systems. The critical system conditions like tripping a 63/20kv distribution transformer and adding an external maneuver loud. Additional finding place and power of DG in this research, optimal power factor is obtained by the given algorithm. Reconfiguration of distribution system is implemented by adaptive genetic algorithm and graph theory to find an optimal structure system with place and power of distributed generators. The offered algorithm is effectively implemented on a 33-bus IEEE test system and a real life distribution system in Iran by Digsilent and. Matlab software.","","978-1-4673-5074-7978-1-4673-5072-3978-1-4673-5073","10.1109/PEOCO.2013.6564632","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564632","","Optimization;Genetic algorithms;Reactive power;Linear programming;Actuators;Power system stability;Biological cells","distributed power generation;genetic algorithms;graph theory;power factor;transformers","DG placement;critical system condition;distribution transformer;external maneuver loud;optimal power factor;adaptive genetic algorithm;distributed generator;33-bus IEEE test system;Iran;Digsilent software;graph theory","","3","19","","","","","","IEEE","IEEE Conferences"
"Using TTCN-3 to Test SPDY Protocol Interaction Property","S. Zhang; H. Li; Y. Xue; X. Wang","NA; NA; NA; NA","2014 IEEE 38th International Computer Software and Applications Conference Workshops","","2014","","","541","546","SPDY protocol is a new application-layer communication protocol which was proposed by Google in order to overcome the defects of HTTP. On the basis of HTTP protocol, SPDY offered four improvements to shorten the page loading time such as multiplexed requests, prioritized requests, server pushed streams and compressed headers. However, there is no much test work of the SPDY especially treating it as a black box, focusing on its interaction property, or testing it by using TTCN-3. In this paper, the interaction property of SPDY protocol is analyzed according to the SPDY protocol draft specification, and a novel test work designed in view of SPDY interaction property is implemented. During the test work, the interaction granularity of the SPDY peers is divided into three kinds of granularity from different levels, meanwhile the test cases were generated in accordance with the draft specification and encoded by utilizing TTCN-3 language for executing on the TT work bench Professional software. Above all we must ensure the SPDY server-side can support the SPDY protocol and the test host installs TT work bench Professional software to complete TTCN-3 testing. Finally, the interaction property of SPDY protocol was tested and the test results were reported. Moreover, some failed test cases were analyzed in detail.","","978-1-4799-3578","10.1109/COMPSACW.2014.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903186","SPDY;Interaction Property;TTCN-3","Protocols;Servers;Testing;Decoding;Loading;Web pages;Google","protocols","SPDY protocol interaction property;application layer communication protocol;Google;HTTP protocol;page loading time;black box;SPDY protocol draft specification;TTCN-3 language;SPDY server-side;professional software;TTCN-3 testing","","1","13","","","","","","IEEE","IEEE Conferences"
"Renumber strategy enhanced particle swarm optimization for cloud computing resource scheduling","H. Li; Y. Fu; Z. Zhan; J. Li","Department of Computer Science, Sun Yat-Sen University, Guangzhou, 510275, China; Department of Computer Science, Sun Yat-Sen University, Guangzhou, 510275, China; Department of Computer Science, Sun Yat-Sen University, Guangzhou, 510275, China; School of Computer Science, South China Normal University, China","2015 IEEE Congress on Evolutionary Computation (CEC)","","2015","","","870","876","Cloud computing offers unprecedented capacity to execute large-scale workflows in the &#x201C;era of big data&#x201D;. In 2014, a cost-minimization and deadline-constrained workflow scheduling (CMDCWS) model is firstly proposed by Rodriguez and Buyya, which is applicable for the business need of cloud computing that a workflow task should be finished by minimizing the execute cost within a deadline constraint. As scheduling cloud computing resources for workflow is an NP-hard problem, Rodriguez and Buyya proposed to use particle swarm optimization (PSO) to solve the CMDCWS problem. In traditional PSO for CMDCWS, each dimension in the particle position stands for each task and the value of the corresponding dimension stands for the index of the cloud resource that executes this task. However, this may have drawback because the value of each dimension does not relate to the resource characteristic but is only a meaningless index number. Therefore the learning behaviors among the particles do not make sense because learning from index number may not lead to better position. In this paper, we present a resource renumber strategy to encode the particle position and design a renumber PSO (RNPSO) for CMDCWS. In RNPSO, all the resources are re-ordered and re-numbered according to their computational ability, i.e., the cost per unit time. By this, the values of particle position can make sense and the positions difference between the well-performed and poorly-performed particles can guide poorly-performed particle to promising region. We conduct experiments on test cases with small, middle, and large scales to compare the performance of PSO and RNPSO. The results show that the resource renumber strategy is promising for enhancing the PSO performance.","1089-778X;1941-0026","978-1-4799-7492-4978-1-4799-7491","10.1109/CEC.2015.7256982","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7256982","cloud computing;resource;scheduling;renumber;particle swarm optimization","Optimization;Flowcharts","Big Data;cloud computing;computational complexity;minimisation;particle swarm optimisation;scheduling;workflow management software","renumber strategy enhanced particle swarm optimization;cloud computing resource scheduling;Big Data;large-scale workflows task;deadline-constrained workflow scheduling model;CMDCWS model;cost minimization;NP-hard problem;index number;resource renumber strategy;renumber PSO;RNPSO;learning behaviors;particle position encoding","","10","23","","","","","","IEEE","IEEE Conferences"
"The LUT-SR Family of Uniform Random Number Generators for FPGA Architectures","D. B. Thomas; W. Luk","Department of Electrical and Electronic Engineering, Imperial College London, London, U.K.; Department of Computing, Imperial College London, London, U.K.","IEEE Transactions on Very Large Scale Integration (VLSI) Systems","","2013","21","4","761","770","Field-programmable gate array (FPGA) optimized random number generators (RNGs) are more resource-efficient than software-optimized RNGs because they can take advantage of bitwise operations and FPGA-specific features. However, it is difficult to concisely describe FPGA-optimized RNGs, so they are not commonly used in real-world designs. This paper describes a type of FPGA RNG called a LUT-SR RNG, which takes advantage of bitwise xor operations and the ability to turn lookup tables (LUTs) into shift registers of varying lengths. This provides a good resource-quality balance compared to previous FPGA-optimized generators, between the previous high-resource high-period LUT-FIFO RNGs and low-resource low-quality LUT-OPT RNGs, with quality comparable to the best software generators. The LUT-SR generators can also be expressed using a simple C++ algorithm contained within this paper, allowing 60 fully-specified LUT-SR RNGs with different characteristics to be embedded in this paper, backed up by an online set of very high speed integrated circuit hardware description language (VHDL) generators and test benches.","1063-8210;1557-9999","","10.1109/TVLSI.2012.2194171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6190771","Equidistribution;field-programmable gate array (FPGA);uniform random number generator (RNG)","Generators;Table lookup;Shift registers;Field programmable gate arrays;Software;Logic gates;Random access memory","benchmark testing;C++ language;circuit optimisation;field programmable gate arrays;hardware description languages;logic testing;random number generation;resource allocation;shift registers","test bench;VHDL generators;very high speed integrated circuit hardware description language;C++ algorithm;LUT-SR generators;low-resource low-quality LUT-OPT RNG;high-resource high-period LUT-FIFO RNG;resource-quality balancing;varying length shift registers;lookup tables;bitwise XOR operations;LUT-SR RNG;FPGA-optimized RNG;resource efficiency;field programmable gate arrays;FPGA architectures;uniform random number generators","","15","16","","","","","","IEEE","IEEE Journals & Magazines"
"Implementation and optimization of group-based signal control in traffic simulation","J. Jin; X. Ma","ITS Lab, Traffic and Logistics, KTH Royal Institute of Technology, Teknikringen 72, 10044, Stockholm, Sweden; ITS Lab, Traffic and Logistics, KTH Royal Institute of Technology, Teknikringen 72, 10044, Stockholm, Sweden","17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","","2014","","","2517","2522","Over the past decades, group-based control has become one of the most popular signal technologies being applied in many cities around the world. LHOVRA control is one of such group-based controls widely employed in Scandinavian countries. While several previous studies showed that group-based control outperforms stage-based control in many aspects, implementation and evaluation of signal controllers are complicated in a real application. In addition, little effort has been put in optimizing such group-based controllers in traffic management practice. This study implements generic group-based control in an object-oriented software framework, while a software-in-the-loop simulation is developed to integrate the signal controller with an open-source traffic simulator, SUMO. Also, stochastic optimization is applied to generate optimal signal parameters according to different settings of objective. In particular, part of the study is to improve the computational performance of the optimization process by parallelized simulation runs. Test-based experiments are finally carried out to evaluate traffic and optimize its impact on a small traffic network in Stockholm.","2153-0009;2153-0017","978-1-4799-6078","10.1109/ITSC.2014.6958093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6958093","","Optimization;Computational modeling;Object oriented modeling;Delays;Vehicles;Control systems;Unified modeling language","control engineering computing;digital simulation;object-oriented programming;public domain software;road traffic control;signalling;stochastic programming;traffic engineering computing","group-based signal control;traffic simulation;signal technologies;LHOVRA control;Scandinavian countries;stage-based control;traffic management practice;object-oriented software framework;software-in-the-loop simulation;open-source traffic simulator;SUMO;stochastic optimization;optimal signal parameters;parallelized simulation;small traffic network;Stockholm","","2","11","","","","","","IEEE","IEEE Conferences"
"Optimizing Google's warehouse scale computers: The NUMA experience","L. Tang; J. Mars; X. Zhang; R. Hagmann; R. Hundt; E. Tune","University of California, San Diego, USA; University of California, San Diego, USA; Google, USA; Google, USA; Google, USA; Google, USA","2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)","","2013","","","188","197","Due to the complexity and the massive scale of modern warehouse scale computers (WSCs), it is challenging to quantify the performance impact of individual microarchitectural properties and the potential optimization benefits in the production environment. As a result of these challenges, there is currently a lack of understanding of the microarchitecture-workload interaction, leaving potentially significant performance on the table. This paper argues for a two-phase performance analysis methodology for optimizing WSCs that combines both an in-production investigation and an experimental load-testing study. To demonstrate the effectiveness of this two-phase approach, and to illustrate the challenges, methodologies and opportunities in optimizing modern WSCs, this paper investigates the impact of non-uniform memory access (NUMA) for several Google's key web-service workloads in large-scale production WSCs. Leveraging a newly-designed metric and continuous large-scale profiling in live datacenters, our production analysis demonstrates that NUMA has a significant impact (10-20%) on two important web-services: Gmail backend and web-search frontend. Our carefully designed load-test further reveals surprising tradeoffs between optimizing for NUMA performance and reducing cache contention.","1530-0897;1530-0897","978-1-4673-5587-2978-1-4673-5585-8978-1-4673-5586","10.1109/HPCA.2013.6522318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522318","","Production;Servers;Microarchitecture;Measurement;Google;Memory management;Correlation","cache storage;computational complexity;computer centres;data warehouses;optimisation;software architecture;Web services;Web sites","Google warehouse scale computer optimization;modern warehouse scale computers;individual microarchitectural properties;potential optimization benefits;production environment;microarchitecture-workload interaction;two-phase performance analysis methodology;in-production investigation;experimental load-testing study;nonuniform memory access;Google key Web service workloads;large-scale production WSC;continuous large-scale profiling;live datacenters;Gmail;Web-search frontend;NUMA performance;cache contention reduction","","11","35","","","","","","IEEE","IEEE Conferences"
"On the performance of SDN controllers: A reality check","Y. Zhao; L. Iannone; M. Riguidel","INFRES, Telecom Paristech, Paris; INFRES, Telecom Paristech, Paris; INFRES, Telecom Paristech, Paris","2015 IEEE Conference on Network Function Virtualization and Software Defined Network (NFV-SDN)","","2015","","","79","85","In the Software Defined Network (SDN) ecosystem, the controller remains the cornerstone of the architecture and the critical point of its success. That is why performance concerns have existed throughout the history of SDN and controller development. This paper aims at making a reality check on the current performance achieved by mainstream open source controllers. The measurements are carried out in a controlled environment, where each controller is tested with its own optimized configuration, on the one hand allowing measuring peak performance, while, on the other hand allowing obtaining fair and reproducible results. Furthermore, besides optimizing the controller configuration, the system wide settings have been also tuned so as to maximize performance. From this comprehensive evaluation, advice on selecting and deploying controllers in real scenarios is derived.","","978-1-4673-6884","10.1109/NFV-SDN.2015.7387410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387410","Software Defined Network;Controller;Benchmark;Measurement","Control systems;Benchmark testing;Performance evaluation;Software;Servers;Throughput","controllers;software defined networking","SDN controllers;software defined network ecosystem;open source controllers","","14","26","","","","","","IEEE","IEEE Conferences"
"Optimization analysis of moving magnet motor for stirling cryocooler","Huo Yingjie; Wang Tiangang; Zhang An; Zhang Xuelin","Lanzhou Institute of Physics &amp; National Key Lab of Vacuum &amp; Cryogenic Technology and Physics, China; Lanzhou Institute of Physics &amp; National Key Lab of Vacuum &amp; Cryogenic Technology and Physics, China; Lanzhou Institute of Physics &amp; National Key Lab of Vacuum &amp; Cryogenic Technology and Physics, China; Lanzhou Institute of Physics &amp; National Key Lab of Vacuum &amp; Cryogenic Technology and Physics, China","2015 IEEE NW Russia Young Researchers in Electrical and Electronic Engineering Conference (EIConRusNW)","","2015","","","203","207","The LIP has developed space cryogenics since 2000, and has taken great progress in Stirling cryocooler. For domestic and potential application in future in space mission, a large cooling capacity Stirling cryocooler with a heat lift of 10W at 260W electric power input has been developed. In order to meet the rigor demand for large impulse, high efficient and tight compactness, the moving coil motor and moving magnet motor has been evaluated through analysis. The moving magnet motor performs high efficient and better compactness. A moving magnet motor is new to LIP. The magnet circuit for elements tests was evaluated by a basic equation and computed by multi-purpose FEM software. The results from the element tests and calculation data were used for improving design. This paper describes the development the motor design and analysis results.","","978-1-4799-7306-4978-1-4799-7305","10.1109/EIConRusNW.2015.7102263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102263","moving magnet motor;linear;drive","Switches;Heating;Footwear;Atmospheric modeling","coils;cryogenics;linear motors;magnetic circuits;optimisation","moving magnet motor;optimization analysis;space cryogenics;Stirling cryocooler;space mission;cooling capacity;moving coil motor;magnet circuit;multipurpose FEM software;power 260 W","","","14","","","","","","IEEE","IEEE Conferences"
"A Distribution Network Reconfiguration based on PSO: Considering DGs sizing and allocation evaluation for voltage profile improvement","M. N. M. Nasir; N. M. Shahrin; Z. H. Bohari; M. F. Sulaima; M. Y. Hassan","Faculty of Electrical Engineering, Universiti Teknikal Malaysia Melaka, Hang Tuah Jaya, 76100, Durian Tunggal, Malaysia; Faculty of Electrical Engineering, Universiti Teknikal Malaysia Melaka, Hang Tuah Jaya, 76100, Durian Tunggal, Malaysia; Faculty of Electrical Engineering, Universiti Teknikal Malaysia Melaka, Hang Tuah Jaya, 76100, Durian Tunggal, Malaysia; Faculty of Electrical Engineering, Universiti Teknikal Malaysia Melaka, Hang Tuah Jaya, 76100, Durian Tunggal, Malaysia; Centre of Electrical Energy Systems (CEES), Faculty of Electrical Engineering, Universiti Teknologi Malaysia (UTM), 81310 Johor Bahru, Malaysia","2014 IEEE Student Conference on Research and Development","","2014","","","1","6","The optimized network reconfiguration and Distributed Generations (DG) sizing with allocation instantaneously via Particle Swarm Optimization (PSO) proposed a new way of allocation DG based on low voltage profile. This method consists of three steps. It started with categorized the switching sequences for radial network configuration while observe the P losses and the profile of voltage without DG. The second step is reconfiguration feeder for reduce losses via DGs allocation based on substations geographical location. The final step is sizing and allocation DGs at each bus with low voltage profile produced from the first step, used to mend the voltage profile and minimize the P<sub>losses</sub> also compared the result with the geographical based allocation results. The objective of this study is to mend the voltage profile while decreasing the P<sub>losses</sub> by using optimization technique considering network reconfiguration, DGs Sizing and allocation concurrently. Four cases are compared which is case 1 is the initial case and taken as a reference. All three stages are tested on standards IEEE 33 bus system by using Particle Swarm Optimization (PSO) technique in MATLAB software. This method proved that improvement of Plosses and voltage profile has been made by change of the switching topology with DGs sizing and allocation technique respectively.","","978-1-4799-6428-4978-1-4799-6427","10.1109/SCORED.2014.7072981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7072981","Distributed Generation (DG);Particle Swarm Optimization (PSO);Distribution Network Reconfiguration (DNR);voltage profile;Plosses","Resource management;Genetic algorithms;Optimization;Particle swarm optimization;Low voltage;Sociology;Statistics","distributed power generation;distribution networks;particle swarm optimisation;substations","distribution network reconfiguration;PSO;DG sizing;voltage profile improvement;distributed generations;particle swarm optimization;radial network configuration;substations geographical location;voltage profile;optimization technique;IEEE 33 bus system;PSO technique;MATLAB software;switching topology","","7","26","","","","","","IEEE","IEEE Conferences"
"Testing of transport system management strategy","S. Lupin; T. Shein; K. K. Lin; A. Davydova","Department of Computer Science, National Research University of Electronic Technology, Zelenograd, Moscow, Russia; Department of Computer Science, National Research University of Electronic Technology, Zelenograd, Moscow, Russia; Department of Computer Science, National Research University of Electronic Technology, Zelenograd, Moscow, Russia; Department of Computer Science, National Research University of Electronic Technology, Zelenograd, Moscow, Russia","Proceedings of IEEE East-West Design & Test Symposium (EWDTS 2014)","","2014","","","1","4","The paper discusses modelling of transport company management systems in a competitive environment. In order to analyse behaviour of the competing transport companies were modelled using an object-oriented structure. The criterion function applied for optimisation and implemented in the model was defined. The performance algorithm of a competitive environment was developed. To verify the proposed approach the model of transport company operating in a completive environment was simulated using AnyLogic software platform.","","978-1-4799-7630","10.1109/EWDTS.2014.7027050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027050","","Load modeling;Object oriented modeling;Companies;Analytical models;Control systems;Vehicles;Cities and towns","object-oriented methods;traffic engineering computing;transportation","transport system management strategy;transport company management systems;competitive environment;object-oriented structure;criterion function;AnyLogic software platform","","","5","","","","","","IEEE","IEEE Conferences"
"Empirically Identifying the Best Greedy Algorithm for Covering Array Generation","C. Nie; J. Jiang; H. Wu; H. Leung; C. J. Colbourn","NA; NA; NA; NA; NA","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops","","2013","","","239","248","Covering array generation is a key issue in combinatorial testing. A number of researchers have been applying greedy algorithms for covering array construction. A greedy framework has been built to integrate most greedy algorithms and evaluate new approaches derived from this framework. However, this framework is affected by multiple factors, which makes its deployment and optimization very challenging. In order to identify the best configuration, we propose a search method that combines pairwise coverage with either base choice or hill climbing techniques. We conduct three different groups of experiments based on six decisions of the greedy framework. The influence of these decisions and their interactions are studied systematically, and the selected greedy algorithm for covering array generation is shown to be better than the existing greedy algorithms.","","978-0-7695-4993-4978-1-4799-1324","10.1109/ICSTW.2013.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6571639","combinatorial testing;greedy methods;covering array","Arrays;Greedy algorithms;Testing;Algorithm design and analysis;Local area networks;Linux;ISDN","combinatorial mathematics;greedy algorithms","greedy algorithm;covering array generation;combinatorial testing;covering array construction;greedy framework;optimization;pairwise coverage;hill climbing technique","","1","14","","","","","","IEEE","IEEE Conferences"
"Power quality enhancement in distribution systems by optimal placement of dynamic voltage restorer","M. Farhoodnea; A. Mohamed; H. Shareef","Department of Electrical, Electronic, and Systems Engineering, University Kebangsaan, Malaysia; Department of Electrical, Electronic, and Systems Engineering, University Kebangsaan, Malaysia; Department of Electrical, Electronic, and Systems Engineering, University Kebangsaan, Malaysia","2013 IEEE Student Conference on Research and Developement","","2013","","","111","115","This paper presents an optimization method based on the firefly algorithm (FA) to address the optimal location and sizing problem of Dynamic Voltage Restorers (DVRs) for enhancing power quality in distribution systems. The objective function is formulated to minimize the voltage total harmonic distortion and total investment cost as well as to improve voltage profile of the system. To test the performance of the proposed FA, the algorithm is implemented on the modified IEEE 16-bus radial distribution systems using the Matlab software. The results are then compared with Particle Swarm Optimization (PSO) and Genetic Algorithm (GA). The simulation results indicate that the proposed FA gives better performance and accuracy compared to PSO and GA in determining the optimum location and size of DVRs in a distribution system.","","978-1-4799-2656","10.1109/SCOReD.2013.7002553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7002553","Active voltage conditioner;optimal placement;firefly algorithm;power quality;optimization","Optimization;Linear programming;Genetic algorithms;Voltage measurement;Harmonic analysis;Power quality;Voltage fluctuations","genetic algorithms;harmonic distortion;minimisation;particle swarm optimisation;power distribution economics;power supply quality;power system restoration","dynamic voltage restorer optimal placement;firefly algorithm;DVR optimal location;power quality enhancement;objective function optimization method;voltage total harmonic distortion minimization;FA;modified IEEE 16-bus radial distribution system;particle swarm optimization;genetic algorithm;PSO;GA","","","14","","","","","","IEEE","IEEE Conferences"
"An Improvement to Fault Localization Technique Based on Branch-Coverage Spectra","S. Xu; J. Xu; H. Yang; J. Yang; C. Guo; L. Yuan; W. Song; G. Si","NA; NA; NA; NA; NA; NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","2","","282","287","For trust in software, developers spend much effort debugging to ensure that software behaviors as expected. Spectrum-based fault localization techniques (SFL) make use of runtime coverage of program elements, like statements, branches and du-pairs, and then check codes in the order of the rank of suspiciousness. So, correct elements with higher suspiciousness than faulty elements cause the loss of precision. In this paper, we focus on a situation where suspiciousness calculated according to coverage and outcome, i.e. Successful or failing, is higher than it should be. It is found that when a branch structure is repeatedly executed, which is normal in real-life programs, and all of its branches are covered within a run, a branch related to faults could lead other branches to be doubted. To reduce effects between branches, we do the following things: First, we utilize branches to monitor program behaviors, second, we take test cases with high similarities as triggered by the same fault, third, for branches mentioned, we propose an algorithm to infer which branch is more likely to be faulty in the failure, finally, experiments based on Siemens benchmark set and flex show that our approach is useful to heighten the ranking of faulty elements by reducing suspiciousness of correct branches.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273630","trust;fault localization;program spectra;branch coverage","Flexible printed circuits;Benchmark testing;Software;Measurement;Debugging;Probability;Instruments","program debugging;software fault tolerance;system recovery;trusted computing","branch-coverage spectra;trusted computing;software development;debugging;software behaviors;spectrum-based fault localization techniques;failure;Siemens benchmark set","","1","18","","","","","","IEEE","IEEE Conferences"
"Input-Sensitive Profiling","E. Coppa; C. Demetrescu; I. Finocchi","Department of Computer Science, Sapienza University of Rome, Italy; Department of Computer, Control, and Management Engineering, Sapienza University of Rome, Italy; Department of Computer Science, Sapienza University of Rome, Italy","IEEE Transactions on Software Engineering","","2014","40","12","1185","1205","In this article we present a building block technique and a toolkit towards automatic discovery of workload-dependentperformance bottlenecks. From one or more runs of a program, our profiler automatically measures how the performance of individual routines scales as a function of the input size, yielding clues to their growth rate. The output of the profiler is, for each executed routine of the program, a set of tuples that aggregate performance costs by input size. The collected profiles can be used to produceperformance plots and derive trend functions by statistical curve fitting techniques. A key feature of our method is the ability toautomatically measure the size of the input given to a generic code fragment: to this aim, we propose an effective metric for estimating the input size of a routine and show how to compute it efficiently. We discuss several examples, showing that our approach can reveal asymptotic bottlenecks that other profilers may fail to detect and can provide useful characterizations of the workload and behavior of individual routines in the context of mainstream applications, yielding several code optimizations as well as algorithmic improvements. To prove the feasibility of our techniques, we implemented a Valgrind tool called aprof and performed an extensive experimentalevaluation on the SPEC CPU2006 benchmarks. Our experiments show that aprof delivers comparable performance to otherprominent Valgrind tools, and can generate informative plots even from single runs on typical workloads for mostalgorithmically-critical routines.","0098-5589;1939-3520;2326-3881","","10.1109/TSE.2014.2339825","Italian Ministry of Education, University, and Research (MIUR); “AMANDA-Algorithmics for MAssive and Networked DAta”; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6858059","Performance profiling;asymptotic analysis;dynamic program analysis;instrumentation","Context modeling;Algorithm design and analysis;Market research;Benchmark testing","curve fitting;program diagnostics;software performance evaluation;software tools;statistical analysis","input-sensitive profiling;building block technique;automatic workload-dependent performance bottleneck discovery;growth rate;program executed routine;tuples;performance plots;statistical curve fitting techniques;generic code fragment;code optimizations;aprof;experimental evaluation;SPEC CPU2006 benchmarks;Valgrind tools","","5","60","","","","","","IEEE","IEEE Journals & Magazines"
"Non-invasive method for differentiating malignant and benign tumours, by the optimization of visual demonstration of MRI scans, using virtual instrument","A. P. Lakshmi","VNR VJIET Hyderabad, India","2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","","2014","","","317","320","This paper presents a non-invasive method to differentiate malignant and benign brain tumours using Lab VIEW Software. Brain tumours are atypical growth of cells in the brain[l]. It is important to diagnose these tumours as malignant or benign as early as possible after detection. Benign tumours are encapsulated and have a controlled growth whereas malignant tumours are not encapsulated and lead to necrosis and pushing aside of neighbouring tissue which causes the build-up of cerebral fluids and oedema in the brain compared to benign tumours. Hence, malignant tumours have a much more severe impact on the brain tissue than benign tumours. So far in clinical practice, only pathology tests have been used to diagnose tumours. This paper is a step towards non-invasive identification of malignant and benign tumours. Here a script was developed in N1 Vision Assistant software which identifies tumours based on the amount of oedema, fluids and infection in the brain which is reflected in MRI scans[2].","","978-1-4799-7088","10.1109/GlobalSIP.2014.7032130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7032130","","Tumors;Magnetic resonance imaging;Cancer;Software;Instruments;Fluids;Biomedical imaging","biomedical MRI;medical image processing;tumours;virtual instrumentation","malignant brain tumour;benign brain tumour;tumour differentiation;visual demonstration;MRI scan;magnetic resonance imaging;virtual instrument;LabVIEW software;tumour diagnosis;pathology tests;noninvasive tumour identification;N1 Vision Assistant software","","","8","","","","","","IEEE","IEEE Conferences"
"A Unified Approach to Use of Coprocessors of Various Types for Solving Global Optimization Problems","V. Gergel","NA","2015 Second International Conference on Mathematics and Computers in Sciences and in Industry (MCSI)","","2015","","","13","18","The paper offers a unified approach to use of accelerators of various types by the example of parallel global search algorithms. In the developed general scheme, the implementation elements depending on specific types of accelerators are encapsulated in a limited number of procedures, which allows to largely reduce expenses and time for software development. The offered scheme was tested on Intel Xeon Phi accelerators and general purpose GPUs. Speedup of the global optimization algorithm using accelerators compared to using CPU only is experimentally confirmed. Computational experiments are carried out using a set of a several hundred of multidimensional multiextremal problems.","","978-1-4799-8673-6978-1-4799-8672","10.1109/MCSI.2015.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7423935","global optimization;dimension reduction;parallel algorithms;speedup;Intel Xeon Phi;NVIDIA Tesla","Optimization;Parallel algorithms;Graphics processing units;Algorithm design and analysis;Search problems;Software algorithms","graphics processing units;mathematics computing;optimisation;parallel algorithms;search problems","parallel global search algorithms;Intel Xeon Phi accelerators;general purpose GPU;global optimization algorithm;coprocessors","","3","16","","","","","","IEEE","IEEE Conferences"
"Bench level automotive EMC validation test laboratory challenges and preferences","L. Banasky","Electromagnetic Compatibility, Ford Motor Company, Dearborn, MI, USA","2014 IEEE International Symposium on Electromagnetic Compatibility (EMC)","","2014","","","307","315","Automotive original equipment manufacturers (OEMs) and suppliers of electrical and electronic subsystems are required to perform bench level electrical and electromagnetic compatibility (EMC) validation testing. This is an important process that requires a significant investment in time and money. In an effort to improve the efficiency of testing, a survey was developed to gain an understanding of the challenges faced by test laboratories and also their preferences. This paper summarizes the results of the survey. Given the amount of time and money spent annually for the type of testing considered, the results suggest that pursuing improvements will result in a long term savings for the original equipment manufacturers (OEMs), suppliers, and labs involved. In order to increase test efficiency, the OEMs, suppliers, and laboratories will need to work together to better prepare test plans and test setups. It is suggested that the results of this survey are used to prioritize the improvement activities.","2158-1118;2158-110X","978-1-4799-5545-9978-1-4799-5544-2978-1-4799-5543","10.1109/ISEMC.2014.6898989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898989","Test;Laboratories;Bench Level;Subsystem;Automotive;Electromagnetic Compatibility (EMC);Validation","Laboratories;Electromagnetic compatibility;Time-frequency analysis;Software;Standards;Automotive engineering","automotive electronics;electromagnetic compatibility","bench level automotive EMC validation test;automotive original equipment manufacturers;OEM;electronic subsystems;electrical subsystems;electromagnetic compatibility;testing;original equipment manufacturers;test plans;test setups","","","11","","","","","","IEEE","IEEE Conferences"
"The Runtime Performance of invokedynamic: An Evaluation with a Java Library","F. Ortin; P. Conde; D. Fernandez-Lanvin; R. Izquierdo","University of Oviedo; University of Oviedo; University of Oviedo; University of Oviedo","IEEE Software","","2014","31","4","82","90","The Java 7 platform includes the invokedynamic opcode in its virtual machine, a feature that lets programmers define-and dynamically change-the linkage of method call sites, thereby maintaining platform optimizations. A comprehensive evaluation of a new library's performance includes a description of how to optimize real Java applications.","0740-7459;1937-4194","","10.1109/MS.2013.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6493308","invokedynamic;Java Virtual Machine;runtime performance;dynamic languages;reflection;software engineering","Java;Runtime;Programming;Benchmark testing;Performance evaluation","Java;software libraries;source code (software);virtual machines","runtime performance;Java library performance;Java 7 platform;invokedynamic opcode;virtual machine;method call sites;platform optimizations;source code","","10","9","","","","","","IEEE","IEEE Journals & Magazines"
"An Intelligent Genetic Algorithm for Effective Grid Resource Utilization","P. D. Babu; T. Amudha","NA; NA","2014 International Conference on Intelligent Computing Applications","","2014","","","64","68","Scientific world requires high performance computing to solve complex problems. Grid technologies emerged for satisfying scientific computing with more computing power. Grid is a set of resources distributed over wide area networks to support large scale distributed applications. A software agent is software that acts or has power or authority to act or represent another by which something is done or caused. Intelligent agent is software entity which functions continuously and autonomously in a particular environment. A Genetic algorithm is a search heuristic that memics the process of natural evolution. The heuristic is routinely used to generate useful solutions to optimization and search problems. Genetic algorithms belong to larger class of evolutionary algorithms. The paper proposes intelligent agent based on Genetic algorithm to achieve better resource utilization in Grid environment, and tested with different sizes of Job requests and analysis of results has shown better resource utilization.","","978-1-4799-3966","10.1109/ICICA.2014.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965012","Grid Computing;Load balancing;Software agents;Evolutionary Computing;Genetic Algorithm","Genetic algorithms;Resource management;Scheduling;Computer architecture;Grid computing;Load management;Software agents","genetic algorithms;grid computing;search problems;software agents","intelligent genetic algorithm;grid resource utilization;high performance computing;grid technology;large scale distributed application;software agent;intelligent agent;search heuristic;evolutionary algorithm","","","16","","","","","","IEEE","IEEE Conferences"
"Upsampled-view distortion optimization for mixed resolution 3D Video Coding","M. Joachimiak; M. M. Hannuksela; P. Aflaki; M. Gabbouj","Department of Signal Processing, Tampere University of Technology, Tampere Finland; Nokia Research Center, Tampere, Finland; Nokia Research Center, Tampere, Finland; Department of Signal Processing, Tampere University of Technology, Tampere Finland","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","1056","1060","The MVC+D extension of the Advanced Video Coding (H.264/AVC) standard enables multiview-and-depth 3D video coding but specifies that all views are coded at equal spatial resolution. In mixed resolution 3D video coding some of the views are coded at reduced resolution. This paper proposes an improvement for the mode decisions in depth encoding in the mixed resolution scenario. We modify the distortion calculation for rate-distortion optimized depth coding. The proposed solution optimizes the depth data compression with assumption that it will be used not only for view synthesis but also for depth-based super resolution in the post-processing stage. The algorithm is implemented on top of the mixed resolution 3D video encoder based on the 3DV-ATM reference software. Evaluation of the proposed solution, tested under the JCT-3V common test conditions, is done against the mixed resolution MVC+D coding with the view synthesis distortion enabled. The results show 2.64%dBR gain for coded views and 0.64% gain for synthesized views.","","978-1-4799-8339-1978-1-4799-8338","10.1109/ICIP.2015.7350961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350961","3D video coding;depth coding;mixed resolution;rate-distortion optimization;upsampled-view distortion","","distortion;image resolution;optimisation;video coding","JCT-3V common test conditions;3DV-ATM reference software;rate-distortion;spatial resolution;H.264/AVC standard;advanced video coding;MVC+D extension;3D video coding;mixed resolution;upsampled-view distortion optimization","","","27","","","","","","IEEE","IEEE Conferences"
"Optimization Techniques for Dimensionally Truncated Sparse Grids on Heterogeneous Systems","A. Deftu; A. Murarasu","NA; NA","2013 21st Euromicro International Conference on Parallel, Distributed, and Network-Based Processing","","2013","","","351","358","Given the existing heterogeneous processor landscape dominated by CPUs and GPUs, topics such as programming productivity and performance portability have become increasingly important. In this context, an important question refers to how can we develop optimization strategies that cover both CPUs and GPUs. We answer this for fastsg, a library that provides functionality for handling efficiently high-dimensional functions. As it can be employed for compressing and decompressing large-scale simulation data, it finds itself at the core of a computational steering application which serves us as test case. We describe our experience with implementing fastsg's time critical routines for Intel CPUs and Nvidia Fermi GPUs. We show the differences and especially the similarities between our optimization strategies for the two architectures. With regard to our test case for which achieving high speedups is a ""must'"" for real-time visualization, we report a speedup of up to 6.2x times compared to the state-of-the-art implementation of the sparse grid technique for GPUs.","1066-6192;1066-6192;2377-5750","978-1-4673-5321-2978-1-4673-5321-2978-0-7695-4939","10.1109/PDP.2013.57","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6498575","sparse grids;GPU;library;optimizations;CUDA","Optimization;Instruction sets;Graphics processing units;Computer architecture;Vectors;Programming;Computational modeling","data compression;data visualisation;grid computing;parallel architectures;real-time systems;software libraries","sparse grid technique;real-time visualization;Nvidia Fermi GPU;Intel CPU;time critical routines;computational steering application;large-scale simulation data compression;large-scale simulation data decompression;high-dimensional function handling;fastsg library;optimization strategies;performance portability;programming productivity;heterogeneous processor;dimensionally truncated sparse grids;optimization techniques","","","19","","","","","","IEEE","IEEE Conferences"
"A Separability Prototype for Automatic Memes with Adaptive Operator Selection","M. G. Epitropakis; F. Caraffini; F. Neri; E. K. Burke","Computing Science and Mathematics, School of Natural Sciences, University of Stirling, FK9 4LA, Scotland, United Kingdom; Centre for Computational Intelligence, School of Computer Science and Informatics, De Montfort University, The Gateway, Leicester LE1 9BH, England, United Kingdom; Centre for Computational Intelligence, School of Computer Science and Informatics, De Montfort University, The Gateway, Leicester LE1 9BH, England, United Kingdom; Computing Science and Mathematics, School of Natural Sciences, University of Stirling, FK9 4LA, Scotland, United Kingdom","2014 IEEE Symposium on Foundations of Computational Intelligence (FOCI)","","2014","","","70","77","One of the main challenges in algorithmics in general, and in Memetic Computing, in particular, is the automatic design of search algorithms. A recent advance in this direction (in terms of continuous problems) is the development of a software prototype that builds up an algorithm based upon a problem analysis of its separability. This prototype has been called the Separability Prototype for Automatic Memes (SPAM). This article modifies the SPAM by incorporating within it an adaptive model used in hyper-heuristics for tackling optimization problems. This model, namely Adaptive Operator Selection (AOS), rewards at run time the most promising heuristics/memes so that they are more likely to be used in the following stages of the search process. The resulting framework, here referred to as SPAM-AOS, has been tested on various benchmark problems and compared with modern algorithms representing the-state-of-the-art of search for continuous problems. Numerical results show that the proposed SPAM-AOS is a promising framework that outperforms the original SPAM and other modern algorithms. Most importantly, this study shows how certain areas of Memetic Computing and Hyper-heuristics are very closely related topics and it also shows that their combination can lead to the development of powerful algorithmic frameworks.","","978-1-4799-4491","10.1109/FOCI.2014.7007809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7007809","","Algorithm design and analysis;Unsolicited electronic mail;Adaptation models;Benchmark testing;Software algorithms;Prototypes;Optimization","optimisation;search problems;software prototyping","SPAM-AOS;search process;optimization problems;hyper-heuristics;adaptive model;separability prototype for automatic memes;software prototype;search algorithms;automatic design;memetic computing;algorithmics;adaptive operator selection","","5","59","","","","","","IEEE","IEEE Conferences"
"Analysing the influence of InfiniBand choice on OpenMPI memory consumption","O. Perks; D. A. Beckingsale; A. S. Dawes; J. A. Herdman; C. Mazauric; S. A. Jarvis","Performance Computing and Visualisation, Department of Computer Science, University of Warwick, UK; Performance Computing and Visualisation, Department of Computer Science, University of Warwick, UK; AWE pic, Aldermaston, Reading, UK; AWE pic, Aldermaston, Reading, UK; Application and Performance Team, Bull Information Systems, Grenoble, France; Performance Computing and Visualisation, Department of Computer Science, University of Warwick, UK","2013 International Conference on High Performance Computing & Simulation (HPCS)","","2013","","","186","193","The ever increasing scale of modern high performance computing platforms poses challenges for system architects and code developers alike. The increase in core count densities and associated cost of components is having a dramatic effect on the viability of high memory-per-core ratios. Whilst the available memory per core is decreasing, the increased scale of parallel jobs is testing the efficiency of MPI implementations with respect to memory overhead. Scalability issues have always plagued both hardware manufacturers and software developers, and the combined effects can be disabling. In this paper we address the issue of MPI memory consumption with regard to InfiniBand network communications. We reaffirm some widely held beliefs regarding the existence of scalability problems under certain conditions. Additionally, we present results testing memory-optimised runtime configurations and vendor provided optimisation libraries. Using Orthrus, a linear solver benchmark developed by AWE, we demonstrate these memory-centric optimisations and their performance implications. We show the growth of OpenMPI memory consumption (demonstrating poor scalability) on both Mellanox and QLogic InfiniBand platforms. We demonstrate a 616× increase in MPI memory consumption for a 64× increase in core count, with a default OpenMPI configuration on Mellanox. Through the use of the Mellanox MXM and QLogic PSM optimisation libraries we are able to observe a 117× and 115× reduction in MPI memory at application memory high water mark. This significantly improves the potential scalability of the code.","","978-1-4799-0838-7978-1-4799-0836-3978-1-4799-0837","10.1109/HPCSim.2013.6641412","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6641412","Memory;MPI;InfiniBand;HWM;Parallel;Tools","Memory management;Libraries;Benchmark testing;Scalability;Optimization;Runtime;Hardware","message passing;optimisation;parallel processing;software libraries;storage management","high performance computing platforms;core count densities;high memory-per-core ratios;memory per core;parallel jobs;memory overhead;scalability issues;QLogic PSM optimisation libraries;Mellanox MXM;QLogic InfiniBand platforms;OpenMPI memory consumption;memory-centric optimisations;AWE;linear solver benchmark;Orthrus;memory-optimised runtime configurations;scalability problems;InfiniBand network communications","","","13","","","","","","IEEE","IEEE Conferences"
"Fwrite: An Approach to Optimize Write Performance of ZFS on Linux","Y. Shen; L. Luo","NA; NA","2014 Seventh International Symposium on Computational Intelligence and Design","","2014","2","","57","60","ZFS file system has been widely used as it provides a file system interface and a block device interface. ZFS ignores the workload features of sequential writes. Writes to a block device with an unaligned I/O pattern will cause inefficient ""read-modify-write"" operations. In this paper, we propose an approach, called Fwrite, to optimize write performance of ZFS on Linux. This approach merges sequential write requests into fewer and larger I/O requests, which reduces the number of reads performed by ""read-modify-write"" operations. Experimental results demonstrate that this approach improves the original ZFS on Linux by 41.1-65.5% in sequential writes.","","978-1-4799-7005-6978-1-4799-7004","10.1109/ISCID.2014.119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081936","block device;sequential write;ZFS file system","Linux;Performance evaluation;Bandwidth;Electronic mail;Sun;Kernel;Benchmark testing","Linux;software performance evaluation;storage management","Fwrite;write performance optimization;ZFS file system;Linux;file system interface;block device interface;sequential write request merging;I/O requests;read-modify-write operations","","","8","","","","","","IEEE","IEEE Conferences"
"Multi objective optimization of energy production of distributed generation in distribution feeder","M. Barukčić; Ž. Hederić; K. Miklošević","Faculty of Electrical Engineering of Osijek, J. J. Strossmayer University of Osijek, KnezaTrpimira 2B, 31 000, Croatia; Faculty of Electrical Engineering of Osijek, J. J. Strossmayer University of Osijek, KnezaTrpimira 2B, 31 000, Croatia; Faculty of Electrical Engineering of Osijek, J. J. Strossmayer University of Osijek, KnezaTrpimira 2B, 31 000, Croatia","2014 IEEE International Energy Conference (ENERGYCON)","","2014","","","1325","1333","Distributed generation (DG) based on renewable sources becomes more and more implemented in distribution networks. Some of these sources have non constant (wind power plants, photovoltaic planes) and some have a constant power production (biogas plants). Also, loads on a feeder change during the day. There is a need for different optimizations because a number of variable changes over time. The total daily active energy losses, daily financial profit, and total daily active energy production of DG are considered for optimization in the paper. The load changes are considered on day level and for each load separately (not on the feeder level, but on node level). The multiobjective approach is applied in the paper for optimizing. The evolutionary strategy is utilized as the optimization method. The two and three objective optimization problems are presented and solved in the paper. The IEEE 13 node unbalanced distribution test feeder is used for presentation of multiobjective optimization. The proposed procedure is performed using OpenDSS and MATLAB software.","","978-1-4799-2449","10.1109/ENERGYCON.2014.6850595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850595","Distributed generation;Distribution feeder;Evolutionary strategy;Multiobjective optimization","Optimization;Sociology;Statistics;Production;Reactive power;Load flow;Linear programming","distributed power generation;evolutionary computation;power distribution economics;power generation economics","multiobjective optimization approach;distributed generation;DG;renewable sources;distribution networks;constant power production;total daily active energy losses;daily financial profit;total daily active energy production;load changes;evolutionary strategy;OpenDSS;Matlab software;IEEE 13 node unbalanced distribution test feeder","","3","17","","","","","","IEEE","IEEE Conferences"
"Hybrid PET/MRI Insert:<formula formulatype=""inline""><tex Notation=""TeX"">${B_0}$</tex></formula>Field Optimization by Applying Active and Passive Shimming on PET Detector Level","J. Wehner; V. Schulz","Department for Physics of Molecular Imaging Systems, Institute for Experimental Molecular Imaging, RWTH Aachen University, Aachen, Germany; Department for Physics of Molecular Imaging Systems, Institute for Experimental Molecular Imaging, RWTH Aachen University, Aachen, Germany","IEEE Transactions on Nuclear Science","","2015","62","3","644","649","The design of an MRI (Magnetic Resonance Imaging) compatible PET (Positron Emission Tomography) detector is regarded to be a challenging task since both imaging devices are likely interacting with each other potentially leading to performance degradation. A typically expected consequence of this interaction is the distortion of the MRI system's B0 field homogeneity. The B0 field gets distorted by any material with non-zero susceptibility brought into the MRI system. Typically, MRI machines have a so-called active shimming system available which allows the field optimization by using additional dedicated shim coils. However, this active shimming mechanism is limited and might not be capable to compensate localized higher order field patterns caused by e.g. a PET system. Since the high B0 field quality is needed for an undisturbed MRI acquisition in general and especially for more advanced MR sequences (spectroscopic studies, sequences which utilizes spectral selective pre-pulses), the PET system's hardware needs to be designed carefully. Consequently, the typical design paradigm regarding this B0 field distortion is the careful selection of all components according to their susceptibility (as low as possible). This design paradigm certainly limits the flexibility of the system design since worse performing components might be chosen over better alternatives because of their higher susceptibility. To overcome this limitation and to retain the MRI capabilities, we propose the application of localized shimming on PET detector level, meaning that the distortion profile caused by PET modules is compensated using either additional components such as magnetic materials (passive shimming) or DC coils (active shimming) on the PET modules or by intelligently arranging the hardware components of the PET detector. We have implemented a software framework which covers three parts: firstly, it calculates the B0 distortion of various objects (susceptibility objects, conductor configuration). Secondly, it allows the characterization of magnetic objects by fitting the implemented models to data and thirdly, it performs a B0 field homogenization of measured distortion maps by superimposing field disturbances of additional simulated objects. We tested this software framework in a first attempt with a single PET module of the Hyperion-II<sup>D</sup>scanner. The measured distortion map of the single PET module shows a strong localized B0 distortion with a volume RMS value of about 0.6 ppm. After optimization, the homogeneity of the simulated field distribution is strongly improved by a factor of 8 (volume RMS ≈ 0.075 ppm).","0018-9499;1558-1578","","10.1109/TNS.2015.2394787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7047239","Active and passive shimming;${B_0}$distortion;field optimization;field simulation;simultaneous PET/MRI","Positron emission tomography;Magnetic resonance imaging;Optimization;Distortion measurement;Image analysis;Software;Detectors","magnetic resonance imaging;positron emission tomography","hybrid PET/MRI insert;B0field optimization;PET detector level;MRI system;active shimming system;PET system hardware;hardware components;software framework;distortion maps;PET detector level","","1","14","","","","","","IEEE","IEEE Journals & Magazines"
"Optimized fuel consumption trajectories of intelligent vehicles in a segment of road for any lane configuration","R. Reghelin; L. V. R. de Arruda","Electrical Engineering Department, CPGEI, Universidade Tecnológica Federal do Paraná (UTFPR), Av. Sete de Setembro, 3165, Curitiba, PR, Brazil; Electrical Engineering Department, CPGEI, UTFPR","2013 IEEE Intelligent Vehicles Symposium (IV)","","2013","","","876","881","This paper presents a simulation algorithm to calculate optimized coordinated trajectories of intelligent vehicles in a road. The first objective is to reduce travel time for each vehicle and second is to have the best fuel consumption trajectory. The configuration of the road can be modelled regarding the number of lanes, direction preference and exclusivity. The calculation considers the main elements of the traffic system, such as topography of the lane, traffic rules and individual capacity of acceleration. It can deals with most traffic situations such as overtaking, obstacles, slopes, and speed reducers. The fuel consumption optimization is obtained by reducing the unnecessary picks of velocity. This will lower the necessity of braking and consequently save fuel. Experimental tests to evaluate the algorithm are presented.","1931-0587","978-1-4673-2755-8978-1-4673-2754","10.1109/IVS.2013.6629577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6629577","","Vehicles;Roads;Acceleration;Trajectory;Fuels;Computer aided software engineering;Optimization","fuel economy;optimisation;road vehicles;trajectory control","optimized fuel consumption trajectories;intelligent vehicles;road segment;lane configuration;optimized coordinated trajectories;lane topography;traffic situations;speed reducers;fuel consumption optimization","","1","10","","","","","","IEEE","IEEE Conferences"
"Hybrid pareto-evolutionary algorithm for solving mathematical models of high dimensional electronic circuits (HPEA)","V. Beglyarov; A. Bereza; L. Blanco","Dept. of Information Systems and Radio Engineering, Don State Technical University, Russia; Dept. of Information Systems and Radio Engineering, Don State Technical University, Russia; Dept. of Information Systems and Radio Engineering, Don State Technical University, Russia","East-West Design & Test Symposium (EWDTS 2013)","","2013","","","1","4","The article considers primary problems of circuitry modeling. The problem of solving ill-conditioned linear systems of algebraic equations (LSAE) is considered. Modified method and HPEA are described. An example of practical application of HPEA software implementation is proposed. The experimental studies and comparison to other LSAE-solving algorithms results are given, proving the advantages of modified method compared to traditional LSAE-solving methods.","","978-1-4799-2096-9978-1-4799-2095","10.1109/EWDTS.2013.6673135","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6673135","","Mathematical model;Equations;Algorithm design and analysis;Vectors;Sociology;Statistics;Biological cells","algebra;electronic engineering computing;evolutionary computation;Pareto optimisation;VLSI","hybrid Pareto-evolutionary algorithm;mathematical models;high dimensional electronic circuits;linear systems of algebraic equations;LSAE-solving algorithms;modified method;HPEA software implementation;very large-scale integration circuits integration;VLSI integration","","1","12","","","","","","IEEE","IEEE Conferences"
"Feature Selections for Effectively Localizing Faulty Events in GUI Applications","X. Xue; Y. Pang; A. S. Namin","NA; NA; NA","2014 13th International Conference on Machine Learning and Applications","","2014","","","306","311","Due to the complex causality of failure and the special characteristics of test cases, the faults in GUI (Graphic User Interface) applications are difficult to localize. This paper adapts feature selection algorithms to localize GUI-related faults in a given program. Features are defined as the subsequences of events executed. By employing statistical feature ranking techniques, the events can be ranked by the suspiciousness of events being responsible to exhibit faulty behavior. The features defined in a given source code implementing (event handle) the underlying event are then ranked in suspiciousness order. The evaluation of the proposed technique based on some open source Java projects verified the effectiveness of this feature selection based fault localization technique for GUI applications.","","978-1-4799-7415","10.1109/ICMLA.2014.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033132","GUI;feature selection;faults localization","Graphical user interfaces;Software;Testing;Java;Vectors;Feature extraction;Software algorithms","causality;feature selection;graphical user interfaces;Java;project management;public domain software;source code (software);statistical analysis","faulty event localization;GUI application;complex failure causality;graphic user interface applications;feature selection algorithms;statistical feature ranking techniques;source code;open source Java projects;feature selection based fault localization technique","","7","25","","","","","","IEEE","IEEE Conferences"
"Performance optimization of 3D applications by OpenGL ES library hooking in mobile devices","C. Cho; C. Hong; J. Piao; Y. Lim; S. Kim","Computer Science, University of Yonsei, Seoul, Korea; Mobile Communications, LG Electronics, Seoul, Korea; Computer Science, University of Yonsei, Seoul, Korea; Mobile Communications, LG Electronics, Seoul, Korea; Computer Science, University of Yonsei, Seoul, Korea","2014 IEEE/ACIS 13th International Conference on Computer and Information Science (ICIS)","","2014","","","471","476","The mobile GPU (Graphic Processing Unit) market has grown steadily due to expansion of the mobile game industry. Despite the rapid computation capability of mobile devices, handling a large amount of high-quality graphics in real-time is difficult. Therefore, effective technologies for improving mobile GPU in smartphones are required. In this thesis, we examine the trade-off between quality and performance, and address the benefits of graphic performance improvement by degrading quality. To implement this idea, we propose performance optimization methodologies for 3D applications using an OpenGL ES library hooking method. Our methodologies do not require any source code from 3D applications, and can be applied to any Android phones that use OpenGL ES in real-time. To demonstrate the benefits of our methodology, we conducted performance verifications of five well-known benchmarks using a smartphone, and measured the quality in accordance with each methodology. In addition, we showed the optimal trade-offs between quality and performance. By using the proposed technique, the performance of mobile GPU can be significantly improved to achieve a better trade-off between quality and performance.","","978-1-4799-4860","10.1109/ICIS.2014.6912179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912179","Embedded;3D Graphics;Optimization","Benchmark testing;Graphics processing units;Hardware;Three-dimensional displays;Graphical user interfaces;Degradation","application program interfaces;computer games;graphics processing units;mobile computing;performance evaluation;smart phones;software libraries;software performance evaluation","3D applications;mobile devices;mobile GPU market;graphic processing unit;mobile game industry;high-quality graphics;smartphones;graphic performance improvement;performance optimization methodologies;OpenGL ES library hooking method;performance verifications;Android phones","","2","27","","","","","","IEEE","IEEE Conferences"
"Prediction Models for Performance, Power, and Energy Efficiency of Software Executed on Heterogeneous Hardware","D. Bán; R. Ferenc; I. Siket; Á. Kiss","NA; NA; NA; NA","2015 IEEE Trustcom/BigDataSE/ISPA","","2015","3","","178","183","Heterogeneous environments are becoming commonplace so it is increasingly important to understand how and where we could execute a given algorithm the most efficiently. In this paper we propose a methodology that uses both static source code metrics and dynamic execution time, power and energy measurements to build configuration prediction models. These models are trained on special benchmarks that have both sequential and parallel implementations and can be executed on various computing elements, e.g., on CPUs or GPUs. After they are built, however, they can be applied to a new system using only the system's static metrics which are much more easily computable than any dynamic measurement. We found that we could predict the optimal execution configuration fairly accurately using static information alone.","","978-1-4673-7952-6978-1-4673-7951","10.1109/Trustcom.2015.629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345645","Green computing;heterogeneous architecture;performance optimization;power-aware execution;configuration selection","Measurement;Predictive models;Benchmark testing;Computational modeling;Machine learning algorithms;Prediction algorithms;Algorithm design and analysis","graphics processing units;parallel programming;prediction theory;sequential estimation;software metrics;source code (software)","heterogeneous hardware;static source code metrics;dynamic execution time;energy measurement;power measurement;parallel implementation;sequential implementation;GPU;CPU;optimal execution configuration;prediction model;energy efficiency;software execution","","1","16","","","","","","IEEE","IEEE Conferences"
"Feign: In-Silico Laboratory for Researching I/O Strategies","J. Lüttgau; J. M. Kunkel","NA; NA","2014 9th Parallel Data Storage Workshop","","2014","","","43","48","Evaluating I/O performance of an application across different systems is a daunting task because it requires preparation of the software dependencies and required input data. Feign aims to be an extensible trace replay solution for parallel applications that supports arbitrary software and library layers. The tool abstracts and streamlines the replay process while allowing plug-ins to provide, manipulate and interpret trace data. Therewith, the application's behavior can be evaluated without potentially proprietary or confidential software and input data.Even more interesting is the potential of Feign as a virtual laboratory for I/O research: by manipulating trace data, experiments can be conducted; for example, it becomes possible to evaluate the benefit of optimization strategies. Since a plug-in could determine ""future"" activities, this enables us to develop optimal strategies as baselines for any run-time heuristics, but also eases testing of a developed strategy on many applications without modifying them.The paper proposes and evaluates a workflow to automatically apply optimization candidates to application traces and approximate potential performance gains. By using Feign's reporting facilities, an automatic optimization engine can then independently conduct experiments by feeding traces and strategies to compare the results.","","978-1-4799-7025","10.1109/PDSW.2014.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016282","","Optimization;Benchmark testing;Engines;Pipelines;Laboratories;Kernel","input-output programs;parallel processing;software performance evaluation;virtual instrumentation","application I/O;I/O performance evaluation;software dependencies;input data;trace replay solution;parallel applications;arbitrary software layers;arbitrary library layers;replay process streamlining;replay process abstraction;plug-ins;trace data provision;trace data manipulation;trace data interpretation;application behavior evaluation;virtual laboratory;I/O research;optimization strategies;optimal strategy development;run-time heuristics;workflow evaluation;application traces;performance gain approximation;Feign reporting facilities;automatic optimization engine;flexible-event imitation engine;I/O strategies;in-silico laboratory","","","17","","","","","","IEEE","IEEE Conferences"
"Volt/VAR Optimization function with load uncertainty for planning of MV distribution networks","S. Rahimi; S. Massucco; F. Silvestro; M. R. Hesamzadeh; Y. Tohidi","ABB Enterprise Software, Västerås, Sweden; UNIGE, University of Genova, Italy; UNIGE, University of Genova, Italy; KTH, Royal Institute of Technology, Stockholm, Sweden; KTH, Royal Institute of Technology, Stockholm, Sweden","2015 IEEE Eindhoven PowerTech","","2015","","","1","6","Volt/VAR Optimization (VVO) function is an important element in real time operation of distribution networks and major part of advanced Distribution Management Systems (DMS). From planning prospective, VVO function can be used to optimize reactive power flow in distribution network to recommend the best operating condition for the control equipment in a predefined period of time in future (i.e. 24 hour). The typical objective function of VVO functions are minimizing the total system loss for a certain system load level. VVO function computes the optimized setting for transformer on-load tap changers (OLTC), Voltage Regulators (VR), and Capacitor Banks, while system voltage profile is maintained within its limits. In this paper the objective is to develop a planning VVO engine which can calculate the most probable expected loss of the network for the next 24 hours, and can recommend the best expected operating condition for the control equipment. For the VVO algorithm a full mixed integer linear programming (MILP) model is used to solve the loss objective of VVO problem for a planning application. The load uncertainty is modeled by an ARMA model which can create any arbitrary number of forecasted load scenarios to be used by VVO engine (implemented in a commercial solver GAMS, “General Algebraic Modeling System”). The implemented models have been tested on a real distribution network in southern Sweden and the results are presented.","","978-1-4799-7693-5978-1-4799-7692","10.1109/PTC.2015.7232790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7232790","Distribution systems;Mixed Integer Linear Programming (MILP);Volt/VAR Optimization (VVO)","Load modeling;Mathematical model;Planning;Capacitors;Optimization;Reactive power;Uncertainty","autoregressive moving average processes;integer programming;linear programming;load distribution;load flow control;on load tap changers;power distribution control;power distribution planning;reactive power control;voltage regulators","VVO function;Volt-VAR optimization function;load uncertainty;MV distribution network planning;advanced DMS;advanced distribution management system;reactive power flow optimization;transformer on-load tap changer;voltage regulator;OLTC transformer;VR;capacitor bank;VVO engine planning;control equipment;full mixed integer linear programming model;full MILP model;ARMA model;southern Sweden","","1","19","","","","","","IEEE","IEEE Conferences"
"Feature Mining for Machine Learning Based Compilation Optimization","F. Li; F. Tang; Y. Shen","NA; NA; NA","2014 Eighth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing","","2014","","","207","214","Compilation optimization is critical for software performance. Before a product releases, the most effective algorithm combination should be chosen to minimize the object file size or to maximize the running speed. Compilers like GCC usually have hundreds of optimization algorithms, in which they have complex relationships. Different combinations of algorithms will lead to object files with different performance. Usually developers select the combination manually, but it's unpractical since a combination for one project can't be reused for another one. In order to conquer this difficulty some approaches like iterative search, heuristic search and machine learning based optimization have been proposed. However these methods still need improvements at different aspects like speed and precision. This paper researches machine learning based compilation optimization especially on feature processing which is important for machine learning methods. Program features can be divided into static features and dynamic features. Apart from user defined static features, we design a method to generate lots of static features by template and select best ones from them. Furthermore, we observe that feature value changes during different optimization phases and implement a feature extractor to extract feature values at specific phases and predict optimization plan dynamically. Finally, we implement the prototype on GCC version 4.6 with GCC plug in system and evaluate it with benchmarks. The results show that our system has a 5% average speed up for object file running speed than GCC O3 optimization level.","","978-1-4799-4331-9978-1-4799-4333","10.1109/IMIS.2014.26","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975465","Compiler optimization;Machine learning;Feature mining","Feature extraction;Optimization;Training;Vectors;Predictive models;Benchmark testing;Computational modeling","data mining;learning (artificial intelligence);program compilers","feature mining;machine learning;compilation optimization;software performance;GCC compiler;iterative search;heuristic search;feature processing;static features;dynamic features;GCC plug in system","","3","22","","","","","","IEEE","IEEE Conferences"
"EcoDroid: An Approach for Energy-Based Ranking of Android Apps","R. J. Behrouz; A. Sadeghi; J. Garcia; S. Malek; P. Ammann","NA; NA; NA; NA; NA","2015 IEEE/ACM 4th International Workshop on Green and Sustainable Software","","2015","","","8","14","The ever increasing complexity of mobile apps comes with a higher energy cost, creating an inconvenience for users on batter-constrained mobile devices. At the same time, due to the meteoric rise of the numbers apps provisioned on app repositories, there are often multiple apps from the same category (e.g., Weather, dictionary) that provide similar features. In spite of similar functionality, the apps may present very different energy costs, due to the choices made in their design and construction. Given apps with similar features, users would prefer an app with the least energy cost. However, app repositories are currently lacking information about relative energy cost of apps in a given category, forcing the users to blindly choose an app for installation without a clear understanding of its energy implications. To address this issue, we have developed Eco Droid, an approach that ranks apps from the same category based on their energy consumption. To that end, Eco Droid uses both static and dynamic analyses to estimate energy consumption of apps in the same category and rank them accordingly. Our initial experiments have demonstrated the ability of Eco Droid in accurately ranking the energy cost of multiple apps from a particular category.","","978-1-4673-7049","10.1109/GREENS.2015.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167752","Android;Energy;Mobile App;Mobile Testing;Program Analysis","Energy consumption;Androids;Humanoid robots;Energy measurement;Dictionaries;Google;Estimation","Android (operating system);energy consumption;mobile computing;mobile handsets;power aware computing","energy-based ranking;Android apps;mobile apps;energy cost;battery-constrained mobile devices;least energy cost;app repositories;EcoDroid;energy consumption;static analyses;dynamic analyses","","3","16","","","","","","IEEE","IEEE Conferences"
"Towards purity-guided refactoring in Java","J. Yang; K. Hotta; Y. Higo; S. Kusumoto","Graduate School of Information Science and Technology, Osaka University, 1-5 Yamadaoka, Suita, 565-0871, Japan; Graduate School of Information Science and Technology, Osaka University, 1-5 Yamadaoka, Suita, 565-0871, Japan; Graduate School of Information Science and Technology, Osaka University, 1-5 Yamadaoka, Suita, 565-0871, Japan; Graduate School of Information Science and Technology, Osaka University, 1-5 Yamadaoka, Suita, 565-0871, Japan","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","2015","","","521","525","Refactoring source code requires preserving a certain level of semantic behaviors, which are difficult to be checked by IDEs. Therefore, IDEs generally check syntactic pre-conditions instead before applying refactoring, which are often too restrictive than checking semantic behaviors. On the other hand, there are pure functions in the source code that do not have observable side-effects, of which semantic behaviors are more easily to be checked. In this research, we propose purity-guided refactoring, which applies high-level refactoring such as memoization on pure functions that can be detected statically. By combining our purity analyzing tool purano with refactoring, we can ensure the preservation of semantic behaviors on these detected pure functions, which is impossible through previous refactoring operations provided by IDEs. As a case study of our approach, we applied memorization refactoring on several open-source software in Java. We observed improvements of the performance and preservation of semantics by profiling their bundled test cases.","","978-1-4673-7532-0978-1-4673-7531","10.1109/ICSM.2015.7332506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332506","refactoring;static analysis;pure function;side effect analysis","Semantics;Java;Syntactics;Software;Libraries;Optimization;Maintenance engineering","Java;program diagnostics;programming environments;software maintenance;source code (software)","purity-guided refactoring source code;Java;IDE;checking semantic behavior;source code;memorization refactoring;open-source software","","1","11","","","","","","IEEE","IEEE Conferences"
"Patching a patch - software updates using horizontal patching","M. Stolikj; P. J. L. Cuijpers; J. J. Lukkien","Department of Mathematics and Computer Science, Eindhoven University of Technology, den Dolech 2, 5612 AZ Eindhoven, The Netherlands; Department of Mathematics and Computer Science, Eindhoven University of Technology, den Dolech 2, 5612 AZ Eindhoven, The Netherlands; Department of Mathematics and Computer Science, Eindhoven University of Technology, den Dolech 2, 5612 AZ Eindhoven, The Netherlands","IEEE Transactions on Consumer Electronics","","2013","59","2","435","441","This paper presents a method for optimizing software updates of consumer electronic devices running multiple applications with a common software component, called horizontal patching. Instead of using separate deltas for patching different applications, the method generates one delta from the other. Due to the large similarities between the deltas, this horizontal delta is small in size. Experimental results on two test sets, consisting of software updates for sensor networks and smart phones, show that significant improvements can be achieved. Between 27% and 84% data can be saved from transmission, depending on the type of applications and shared components.","0098-3063;1558-4127","","10.1109/TCE.2013.6531128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531128","software update;remote reprogramming;horizontal patching;heterogeneous networks","Software algorithms;Operating systems;Smart phones;Microprogramming;Classification algorithms;Encoding","consumer electronics;mobile computing;smart phones;wireless sensor networks","software update optimisation;horizontal patching;consumer electronic devices;common software component;horizontal delta;sensor network;smart phone","","3","20","","","","","","IEEE","IEEE Journals & Magazines"
"Net2plan: an open source network planning tool for bridging the gap between academia and industry","P. Pavon-Marino; J. Izquierdo-Zaragoza","Universidad Politecnica de Cartagena; Universidad Politecnica de Cartagena","IEEE Network","","2015","29","5","90","96","The plethora of network planning results published in top-ranked journals is a good sign of the success of the network planning research field. Unfortunately, it is often difficult for network carriers and ISPs to reproduce these investigations on their networks. This is partially because of the absence of a software planning tool, meeting the requirements of industry and academia, which can make the adaptation and validation of planning algorithms less time consuming. We describe how a paradigm shift to an open source view of the network planning field emphasizes the power of distributed peer review and transparency to create high-quality software at an accelerated pace and lower cost. Then we present Net2Plan, an open source Java-based software tool. Built on top of a technology-agnostic network representation, it automates the elaboration of performance evaluation tests for userdefined or built-in network design algorithms, network recovery schemes, connection-admission-control systems, or dynamic provisioning algorithms for timevarying traffic. The Net2Plan philosophy enforces code reutilization as an open repository of network planning resources. In this article, a case study in a multilayer IP-over-WDM network is presented to illustrate the potential of Net2Plan. We cover standard CAPEX studies, and more advanced aspects such as a resilience analysis of the network under random independent failures and disaster scenarios, and an energy efficiency assessment of “green” schemes that switch off parts of the network during low load periods. All the planning algorithms in this article are publicly available on the Net2Plan website.","0890-8044;1558-156X","","10.1109/MNET.2015.7293311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293311","","Telecommunication network planning;Algorithm design and analysis;Optical fiber networks;IP networks;Nonhomogeneous media;Industries;Optical fiber devices","computer network performance evaluation;IP networks;Java;public domain software;software tools;telecommunication computing;telecommunication network planning;telecommunication traffic;wavelength division multiplexing","Net2plan;open source network planning tool;academia;industry;top-ranked journal;network planning research field;network carrier;ISP;software planning tool;planning algorithm;open source view;distributed peer review;high-quality software;open source Java-based software tool;technology-agnostic network representation;performance evaluation test;userdefined design algorithm;built-in network design algorithm;network recovery scheme;connection-admission-control system;dynamic provisioning algorithm;timevarying traffic;Net2Plan philosophy;code reutilization;open repository;network planning resource;multilayer IP-over-WDM network;standard CAPEX study;resilience analysis;random independent failure;disaster scenario;energy efficiency assessment","","35","15","","","","","","IEEE","IEEE Journals & Magazines"
"Code compilation exploration for thermal dissipation reduction in SoC","M. B. Saad; A. Jedidi; S. Niar; M. Abid","CES laboratory, Sfax University Tunisia; Technical College Dammam KSA; LAMIH, Universit de Valenciennes, France; CES laboratory, Sfax university Tunisia","2014 26th International Conference on Microelectronics (ICM)","","2014","","","108","111","The ever increasing transistor integration density has allowed the design of complex and powerful system-on-chips (SoC). As consequence, the power consumption density increased significantly, which is directly converted into heat. These two results have a negative impact on the performance and reliability of these SoC. Thermal dissipation is an important factor that might significantly degrade the reliability and lifetime of the SoC. Traditionally, thermal problems are solved by employing advanced packaging and cooling solutions. However, the modern high-performance of the SoC is already pushing the limits of what the cooling solutions can provide. In this way, software solution can be the key of this problem. In this paper, we propose a new method to control and reduce thermal dissipation by profiling and optimizing the application source code.","2159-1660;2159-1679","978-1-4799-8153","10.1109/ICM.2014.7071818","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7071818","Thermal dissipation;reliability;EV6 processor;HotSpot simulator and profiling","System-on-chip;Benchmark testing;Optimization;Reliability;Thermal analysis;Power demand;Temperature measurement","cooling;integrated circuit design;integrated circuit packaging;integrated circuit reliability;optimising compilers;source code (software);system-on-chip;thermal engineering","code compilation exploration;thermal dissipation reduction;SoC;transistor integration density;system-on-chips;power consumption density;reliability;advanced packaging;cooling solutions;application source code","","","15","","","","","","IEEE","IEEE Conferences"
"Development of High Power LED driver using LTspice software","M. I. M. Rashid; S. Ab Ghani; M. F. S. Mustahim","Sustainable Energy &amp; Power Electronics Research Group(SuPER), Faculty of Electrical and Electronics Engineering, Universiti Malaysia Pahang, Malaysia; Sustainable Energy &amp; Power Electronics Research Group(SuPER), Faculty of Electrical and Electronics Engineering, Universiti Malaysia Pahang, Malaysia; Sustainable Energy &amp; Power Electronics Research Group(SuPER), Faculty of Electrical and Electronics Engineering, Universiti Malaysia Pahang, Malaysia","2013 IEEE 7th International Power Engineering and Optimization Conference (PEOCO)","","2013","","","92","97","LED technology has been used widely in various applications due to its advantage in term of functionality, high efficiency, low cost, small size and high reliability. In order to improve LED performance and prolong LED's life, High Power LED driver is been developed. LED driver is used to control output voltage by using current mode controlled method. This development of LED driver can increase the LED operation efficiency, provide high voltage protection, decreases the driver size and lastly make possible of user friendly installation. Voltage source is step up by using boost converter as main circuit. In control circuit, current mode controlled is used to regulate output characteristic of High Power LED. LTC 3783 is used as PWM controller to drive the gate and provide pulse signal to the MOSFET. This driver is designed to operate load that consists of 6 units LED each rating of 5 Watt connected in series. Theoretical calculation is made to obtain component specification. The LTspice software is used to simulate the driver circuit by using calculated parameters before move on to hardware implementation. Eagle software is used to design the Printed Circuit Board (PCB). Then, all components are mounted on the PCB which is made of FR4 material. The hardware built is tested experimentally and the output waveform is recorded.","","978-1-4673-5074-7978-1-4673-5072-3978-1-4673-5073","10.1109/PEOCO.2013.6564522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564522","","Light emitting diodes;Voltage control;Pulse width modulation;Software;Hardware;Integrated circuit modeling;Inductors","driver circuits;electric current control;electrical installation;light emitting diodes;MOSFET;printed circuit design;PWM power convertors;reliability;voltage control","high power LED driver;LED technology;LED performance;prolong LED life;voltage control;current mode controlled method;LED operation efficiency;high voltage protection;user friendly installation;voltage source;boost converter;control circuit;PWM controller;pulse signal;MOSFET;LTspice software;Eagle software;printed circuit board design;FR4 material","","2","8","","","","","","IEEE","IEEE Conferences"
"An overview of cuckoo-inspired intelligent algorithms and their applications","B. Xing; Wen-Jing Gao; T. Marwla","Faculty of Engineering, University of Johannesburg, Auckland Park, 2006, South Africa; Faculty of Engineering, University of Johannesburg, Auckland Park, 2006, South Africa; Faculty of Engineering, University of Johannesburg, Auckland Park, 2006, South Africa","2013 IEEE Symposium on Swarm Intelligence (SIS)","","2013","","","85","89","A number of cuckoo-inspired algorithms have been introduced recently in the literature. These procedures are based on the parasitic behavior observed in some cuckoo species, in combination with the Lévy flight behavior discovered in some birds and fruit flies. Thus far, many different applications have been reported. In this paper we provide a quick overview of the current stage of cuckoo-inspired algorithms and their various applications.","","978-1-4673-6004","10.1109/SIS.2013.6615163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615163","cuckoo search (CS);modified cuckoo search (MFC);multi-objective cuckoo search (MOCS);cuckoo optimization algorithm (COA);optimization","Optimization;Algorithm design and analysis;Search problems;Software;Particle swarm optimization;Benchmark testing;Job shop scheduling","artificial intelligence;optimisation","cuckoo-inspired intelligent algorithms;parasitic behavior;cuckoo species;Levy flight behavior;birds;fruit flies","","3","31","","","","","","IEEE","IEEE Conferences"
"Engineering Quality while Embracing Change: Lessons Learned","M. C. Marinovici; H. Kirkham; K. A. Glass; L. C. Carlsen","NA; NA; NA; NA","2013 46th Hawaii International Conference on System Sciences","","2013","","","4810","4816","In an increasingly complex technical environment, failure is accepted as a way of maximizing potential, a way of growing up. Experience can be utilized to improve designs, advance product maturity, and at the same time, can increase team's training and education. It is not enough to understand the development tools to ensure a project's success. Understanding how to plan, measure, communicate, interact, and work in teams is mandatory to make a project successful. A manager cannot enforce a process of good communication between team members. Project teams have to work together in supporting each other and establish a constant communication environment. This paper presents lessons learned during the development process of operations research software. The team members have matured and learned during the process to plan successfully, adapt to changes, use Agile methodologies, and embrace a new attitude towards failures and communication.","1530-1605;1530-1605","978-1-4673-5933-7978-0-7695-4892","10.1109/HICSS.2013.193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6480423","","Software;Testing;Software engineering;Mathematical model;Optimization;Power systems;Programming","failure analysis;industrial training;project engineering;project management;quality control;software development management;software prototyping;team working","engineering quality;complex technical environment;technical failure;product maturity;design improvement;team training;team education;development tools;team member communication;project teams;constant communication environment;agile methodologies;operation research software development process","","2","17","","","","","","IEEE","IEEE Conferences"
"Simulated Raindrop algorithm for global optimization","A. Ibrahim; S. Rahnamayan; M. V. Martin","Department of Electrical, Computer, and Software Engineering, University of Ontario Institute of Technology, Oshawa, Canada; Department of Electrical, Computer, and Software Engineering, University of Ontario Institute of Technology, Oshawa, Canada; Faculty of Business and IT, University of Ontario Institute of Technology, Oshawa, Canada","2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE)","","2014","","","1","8","In this paper, we propose a novel single-solution based metaheuristic algorithm called Simulated Raindrop (SRD). The SRD algorithm is inspired by the principles of raindrops. When rain falls on the land, it normally flows from higher altitude to a lower due to gravity, while choosing the optimum path towards the lowest point on the landscape. We compared the performance of simulated annealing (SA) against the proposed SRD method on 8 commonly utilized benchmark functions. Experimental results confirm that SRD outperforms SA on all test problems in terms of variant performance measures, such as convergence speed, accuracy of the solution, and robustness.","0840-7789","978-1-4799-3101-9978-1-4799-3099","10.1109/CCECE.2014.6901103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901103","Nature-inspired algorithms;S-metaheuristic;raindrop;global optimization;simulated annealing","Benchmark testing;Simulated annealing;Heuristic algorithms;Genetic algorithms;Computers;Educational institutions","evolutionary computation;simulated annealing","simulated raindrop algorithm;global optimization;single-solution based metaheuristic algorithm;SRD algorithm;raindrops principle;simulated annealing;SA","","5","17","","","","","","IEEE","IEEE Conferences"
"Optimization for alkali/polymer flooding based on parallel self-regulation differential evolution with maximum average entropy","Y. Ge; S. Li; P. Chang; S. Lu; Y. Lei","School of information and control engineering, China University of Petroleum, Qingdao 266580; School of information and control engineering, China University of Petroleum, Qingdao 266580; School of information and control engineering, China University of Petroleum, Qingdao 266580; School of information and control engineering, China University of Petroleum, Qingdao 266580; Fujian Metrology Institute, Fuzhou 350003, China","2015 34th Chinese Control Conference (CCC)","","2015","","","8416","8421","The alkali/polymer (AP) flooding is a complex distributed parameter system (DPS). In this paper, an optimization model of AP flooding is developed, which takes net present value (NPV) as the performance index, oil/water seepage continuity equations and adsorption diffusion equations of displacing agents as the governing equations, physicochemical algebraic equations and boundary conditions of displacing agents as the constraint equations. To get the optimal injection-production strategy, a parallel self-regulation differential evolution algorithm with maximum average entropy (PSEDE) is proposed. In PSEDE, the maximum average entropy initialization strategy and multi-population parallel strategy are introduced. A new self-regulation principle containing the global population information and individual information is adopted to improve the performance of differential evolution (DE). After tested by three benchmark functions, the PSEDE is applied to optimize an optimization model of AP flooding. The solving effect is good by the comparison with trial and error solutions.","1934-1768","978-9-8815-6389","10.1109/ChiCC.2015.7260974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7260974","AP flooding;optimization;maximum average entropy;multi-population parallel strategy;self-regulation strategy;differential evolution","Optical character recognition software;Decision support systems","distributed parameter systems;entropy;evolutionary computation;large-scale systems;oil technology;optimisation;polymers","alkali-polymer flooding optimization;parallel self-regulation differential evolution;complex distributed parameter system;AP flooding optimization model;net present value;performance index;oil-water seepage continuity equations;adsorption diffusion equations;displacing agents;governing equations;physicochemical algebraic equations;boundary conditions;constraint equations;optimal injection-production strategy;maximum average entropy initialization strategy;multipopulation parallel strategy;self-regulation principle;global population information;individual information;PSEDE","","","21","","","","","","IEEE","IEEE Conferences"
"The optimization of process parameters based on the orthogonal experiments in wire bonding","L. Song; S. Bao; Y. Hu; W. Jiang; Q. Li","State Key Laboratory of Electronic Thin Films and Integrated Devices, University of Electronic Science and Technology of China, Chengdu, China; State Key Laboratory of Electronic Thin Films and Integrated Devices, University of Electronic Science and Technology of China, Chengdu, China; State Key Laboratory of Electronic Thin Films and Integrated Devices, University of Electronic Science and Technology of China, Chengdu, China; State Key Laboratory of Electronic Thin Films and Integrated Devices, University of Electronic Science and Technology of China, Chengdu, China; State Key Laboratory of Electronic Thin Films and Integrated Devices, University of Electronic Science and Technology of China, Chengdu, China","2015 16th International Conference on Electronic Packaging Technology (ICEPT)","","2015","","","1180","1183","Regarding the effects caused by power and time of 7476D semi-automatic bonding wire on surface of ceramic substrate wedge bonding wire performances. Orthogonal experiment named L<sub>9</sub>(34) had been designed using bonding power and bonding time to get results of pull test as the test index. After processing and analysis of test data using Statistical Package for Social Science 16.0 (SPSS16.0) software, influence degree of different factors is obtained, and the optimal technological conditions of bonding wire is obtained too, which can be further provided for the problems, causes and quality assessment of wire bonding analysis' data support. From the analysis results, influence degree of different factors is obtained, and the optimum formula for bonding wire is obtained too. Bonding output power of the first bond is the key factor that affects the results of pull test. The optimal technological conditions are obtained as follows: The influence degree of each factor is not the same: A&gt;D&gt;B&gt;C; the output power of the first bond is 0.32 watt; the output power of the first bond is 0.44 watt; bonding time of the first is bond 70 millisecond; bonding time of the second bond is 100 millisecond. Development of this work is of benefit to raising quality of gold wire wedge bonding. Through the above analysis, aiming at the problem of multiple factors and multiple levels, we can use the method of orthogonal test to solve test index, so not only reduced the number of test, and obtained the optimum formula.","","978-1-4673-7999-1978-1-4673-7998","10.1109/ICEPT.2015.7236790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7236790","Orthogonal experiment;Wire bonding;SPSS16.0","Bonding;Wires;Power generation;Metals;Welding;Substrates;Indexes","lead bonding;optimisation","process parameters;wire bonding;semi-automatic bonding wire;ceramic substrate wedge bonding wire;gold wire wedge bonding;orthogonal test;test index","","","6","","","","","","IEEE","IEEE Conferences"
"Compiler-aided methodology for low overhead on-line testing","G. Nazarian; R. M. Seepers; C. Strydis; G. N. Gaydadjiev","Faculty of Electrical Engineering, Mathematics and Computer Science, Delft University of Technology, Mekelweg 4, 2628 CD Delft, The Netherlands; Dept. of Neuroscience, Erasmus MC, Dr. Molewaterplein 50, 3015 GE Rotterdam, The Netherlands; Dept. of Neuroscience, Erasmus MC, Dr. Molewaterplein 50, 3015 GE Rotterdam, The Netherlands; Dept. of Computer Science and Engineering, Chalmers University of Technology, Rannvagen 6, Goteburg, Sweden","2013 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS)","","2013","","","219","226","Reliability is emerging as an important design criterion in modern systems due to increasing transient fault rates. Hardware fault-tolerance techniques, commonly used to address this, introduce high design costs. As alternative, software Signature-Monitoring (SM) schemes based on compiler assertions are an efficient method for control-flow-error detection. Existing SM techniques do not consider application-specific-information causing unnecessary overheads. In this paper, compile-time Control-Flow-Graph (CFG) topology analysis is used to place best-suited assertions at optimal locations of the assembly code to reduce overheads. Our evaluation with representative workloads shows fault-coverage increase with overheads close to Assertion-based Control-Flow Correction (ACFC), the method with lowest overhead. Compared to ACFC, our technique improves (on average) fault coverage by 17%, performance overhead by 5% and power-consumption by 3% with equal code-size overhead.","","978-1-4799-0103","10.1109/SAMOS.2013.6621126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621126","","Topology;Program processors;Optimization;Circuit faults;Hardware;Instruments;Runtime","graph theory;program compilers;program testing;software fault tolerance","compiler-aided methodology;low overhead on-line testing;important design criterion;transient fault rates;software signature-monitoring schemes;SM techniques;control-flow-error detection;compile-time control-flow-graph topology analysis;assertion-based control-flow correction;ACFC;CFG","","1","21","","","","","","IEEE","IEEE Conferences"
"Mobile Networks Optimization Using Open-Source GRASS-RaPlaT Tool and Evolutionary Algorithm","D. Šekuljica; A. Vilhar; M. Depolli; A. Hrovat; I. Ozimek; T. Javornik","Department of Communication Systems, Jožef Stefan Institute, Ljubljana, Slovenia; Department of Communication Systems, Jožef Stefan Institute, Ljubljana, Slovenia; Department of Communication Systems, Jožef Stefan Institute, Ljubljana, Slovenia; Department of Communication Systems, Jožef Stefan Institute, Ljubljana, Slovenia; Department of Communication Systems, Jožef Stefan Institute, Ljubljana, Slovenia; Department of Communication Systems, Jožef Stefan Institute, Ljubljana, Slovenia","2015 9th European Conference on Antennas and Propagation (EuCAP)","","2015","","","1","5","The process of LTE and LTE-A mobile networks optimization for 900 MHz radio signal using the open-source GRASS-RaPlaT tool and evolutionary algorithm is presented. The aim of the proposed solution is to reduce operational costs and to maximize network efficiency. Based on the terrain profile maps and construction locations, the tool calculates path loss by applying state-of-the-art statistical models and determines the optimal base station locations and parameters by maximizing defined criteria functions. The optimization has been tested on a selected area in Ljubljana, Slovenia. The obtained results show that the key network characteristics such as signal coverage, carrier-to-interference ratio and network capacity can be increased by tuning various base station parameters such as transmission power, antenna direction and location.","2164-3342","978-8-8907-0185-6978-8-8907-0182","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7228488","capacity;coverage;evolutionary algorithm;LTE;LTE-A;multiobjective optimization","Optimization;Mobile communication;Mobile computing;Interference;Evolutionary computation;Transmitting antennas;Base stations","evolutionary computation;Long Term Evolution;public domain software;statistical analysis;terrain mapping","evolutionary algorithm;open-source GRASS-RaPlaT tool;LTE-A mobile network optimization;LTE mobile network optimization;operational cost reduction;terrain profile map;state-of-the-art statistical model;optimal base station location;Ljubljana;Slovenia;Long Term Evolution","","","7","","","","","","IEEE","IEEE Conferences"
"Position Paper: Software-Defined Network Service Chaining","J. Blendin; J. Rückert; N. Leymann; G. Schyguda; D. Hausheer","NA; NA; NA; NA; NA","2014 Third European Workshop on Software Defined Networks","","2014","","","109","114","Network service chaining allows composing services out of multiple service functions. Traditional network service functions include, e.g., firewalls, TCP optimizers, web proxies, or higher layer applications. Network service chaining requires flexible service function deployment models. Related work facilitating service chaining include, e.g., the network service header proposal discussed at the IETF or the complementary work on network function virtualization (NFV) at ETSI. This position paper presents a high-level concept and architecture to enable service chaining using Software Defined Networking (SDN), specifically OpenFlow in a telecommunication environment. The paper discusses required functionalities, challenges, and testbed aspects to implement and test such an approach. Finally, the set of implemented service functions and management interfaces are highlighted to demonstrate the approach as a proof of concept for a selection of relevant use cases.","2379-0350;2379-0369","978-1-4799-6919","10.1109/EWSDN.2014.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984061","Network Service Chaining;Network Function Virtualization","Switches;IP networks;Ports (Computers);Software;Linux;Routing;Mobile communication","software defined networking;virtualisation","software-defined network service chaining;service function deployment model;network service header;network function virtualization;SDN;OpenFlow","","25","14","","","","","","IEEE","IEEE Conferences"
"Dynamic load modelling based on power quality recorder data","A. A. Rahim; M. F. Hashim; M. F. Mohd Siam","TNB Research, Malaysia; TNB Research, Malaysia; TNB Research, Malaysia","2013 IEEE 7th International Power Engineering and Optimization Conference (PEOCO)","","2013","","","119","123","The objective of the study is to derive, test and verify the dynamic load model parameters in TNB power system. Measurement tools used including online power quality monitoring system which record the voltage and current during voltage sag and other type of disturbances on the upstream networks. The disturbance data for the same location at different date is recorded and stored in centre server for reporting and analysis. The measurement point is at the distribution 11kV feeder with sampling resolution at 32 samples per cycle. Data processing using sample to RMS voltage and current and calculated Power (P) and Reactive power (Q). Load parameter derivation software (LMPD) from Electric Power Research Institute (EPRI) USA was used to derive the load model parameters. For model testing and validation, PSSE software was used to test the model and verify the load response by simulating the similar disturbance in the system.","","978-1-4673-5074-7978-1-4673-5072-3978-1-4673-5073","10.1109/PEOCO.2013.6564527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564527","Dynamic load model;PQ recorder;induction motor;ZIP model","Load modeling;Induction motors;Data models;Reactive power;Power system dynamics;Power measurement;Voltage fluctuations","computerised monitoring;electric current measurement;power distribution faults;power supply quality;power system measurement;power system simulation;reactive power;recorders;voltage measurement","power quality recorder data;TNB power system;dynamic load model parameters;measurement tools;online power quality monitoring system;current recording;voltage recording;voltage sag;disturbance data simulation;centre server;distribution feeder;measurement point;data processing;RMS voltage;calculated power;reactive power;RMS current;load parameter derivation software;LMPD;model testing;PSSE software;load response;power system software simulation;voltage 11 kV","","1","6","","","","","","IEEE","IEEE Conferences"
"A comprehensive energy saving potential evaluation method for the distribution networks","B. Chen; T. Cui; L. Ye; C. Li","Guangxi Key Laboratory of Power System Optimization and Energy Technology (Guangxi University) Nanning, China; Guangxi Key Laboratory of Power System Optimization and Energy Technology (Guangxi University) Nanning, China; Guangxi Key Laboratory of Power System Optimization and Energy Technology (Guangxi University) Nanning, China; Guangxi Key Laboratory of Power System Optimization and Energy Technology (Guangxi University) Nanning, China","2015 5th International Conference on Electric Utility Deregulation and Restructuring and Power Technologies (DRPT)","","2015","","","478","481","The construction projects for distribution networks require good economy besides energy saving and voltage quality improvement. Thus the line loss rate and voltage qualification rate are the main assessment indices of construction projects for distribution networks. This paper proposes a comprehensive energy saving potential (ESP) evaluation system for distribution networks. The above basic indices and comprehensive ESP indices for each selected feeder are calculated under different improvement measures. Firstly, typical feeders are modeled using Power Factory software. Then power flow for distribution networks is calculated and weak networks can be found. Secondly, improvement measures for typical feeders can be chosen based on weak networks analysis to achieve the purpose of reducing the power loss rate. Thirdly, the paper gives the definition of loss reduction rate, voltage improvement rate and economic benefits. Furthermore, the implementation order of the measures is optimized according to the value of comprehensive ESP indices and the difficulty extent of implementation. The investment cost and the transformation effect are considered simultaneously for planning. Four measures which include replacing high-loss transformers and worn conductors, adding reactive power compensation devices and adjusting bus voltage are carried out on the IEEE 33-bus test system. The results of simulation calculation demonstrate the validity and superiority of the proposed system.","","978-1-4673-7106-3978-1-4673-7105","10.1109/DRPT.2015.7432308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7432308","distribution network;loss reduction;voltage quality;cost-effectiveness;energy saving potential","Voltage measurement;Power measurement;Loss measurement;Q measurement;Electric potential;Economics;Investment","distribution networks;energy conservation;load flow;reactive power","comprehensive energy saving potential evaluation method;distribution networks;line loss rate;voltage qualification rate;power flow;weak networks;loss reduction rate;voltage improvement rate;reactive power compensation devices;bus voltage;IEEE 33-bus test system","","1","14","","","","","","IEEE","IEEE Conferences"
"Testing the capacity of off-the-shelf systems to store 10GbE traffic","V. Moreno; J. Ramos; J. L. Garcia-dorado; I. Gonzalez; F. J. Gomez-arribas; J. Aracil","Universidad Autónoma de Madrid; Universidad Autónoma de Madrid; Universidad Autónoma de Madrid; Universidad Autónoma de Madrid; Universidad Autónoma de Madrid; Universidad Autónoma de Madrid","IEEE Communications Magazine","","2015","53","9","118","125","The maturity of the telecommunications market and the fact that user demands increase every day leaves network operators no option but to deploy high-speed infrastructures and test them in an efficient and economical manner. A common approach to this problem has been the storage of network traffic samples for analysis and replay using different versions of what we have named NTSS. This type of task is particularly demanding in 10 Gb Ethernet links and has traditionally been addressed by closed solutions or NTSS built on top of high-end hardware. However, these approaches lack flexibility and extensibility, which typically translates into higher cost. This work studies how NTSS can be built using COTS: a combination of commodity hardware and open source software. To this end, we present the current limitations of COTS systems and focus on low-level optimization techniques at several levels: the NIC driver, hard drives, and the software interaction between them. The application of these techniques has proven crucial for reaching 10 Gb/s rates, as different state-of-the-art systems have shown after an extensive performance test.","0163-6804;1558-1896","","10.1109/MCOM.2015.7263355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7263355","","Throughput;File systems;Telecommunication traffic;Oscillators;Telecommunication network management","local area networks;public domain software;telecommunication links;telecommunication traffic","off-the-shelf system capacity testing;telecommunication market;high-speed infrastructure deployment;network traffic storage;NTSS;Ethernet link;high-end hardware;COTS system;open source software;commodity hardware;low-level optimization technique;NIC driver;hard drive;software interaction;commercial off-the-shelf solution","","5","15","","","","","","IEEE","IEEE Journals & Magazines"
"RFID system optimization for item-level pharmaceutical serialization","M. Laniel; I. Uysal","RFID Lab for Applied Research, University of South Florida, Tampa, 33620, USA; RFID Lab for Applied Research, University of South Florida, Tampa, 33620, USA","2013 IEEE International Conference on RFID-Technologies and Applications (RFID-TA)","","2013","","","1","6","A pharmaceutical company facilitated a program to serialize some of their products through the use of RFID tags at the unit-level. However, the system implemented on one of their packaging lines was unreliable. We have developed a laboratory test system that physically simulates the packaging line in order to find a working solution without any interruption to ongoing production. In this paper we discuss the methodical and analytical optimization steps we followed to find a combination of RFID hardware and software parameters which led to acceptable and repeatable performance (&gt;99.5% write/lock/read success rate for a high speed packaging lane) within company requirements and physical limitations. The solution involved modifications of the tag placement, antenna type, reader output power level and low level reader protocol for different Miller modulations compared to the original implementation. Although the solution was first found and tested in a laboratory setting, it was later validated on the actual packaging line in the factory.","","978-1-4799-2114","10.1109/RFID-TA.2013.6694507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6694507","radio frequency identification;item-level;pharmaceuticals;serialization","Radiofrequency identification;Satellites;Packaging;Software;Testing;Satellite antennas","optimisation;pharmaceutical industry;radiofrequency identification","RFID system optimization;item-level pharmaceutical serialization;pharmaceutical company;laboratory test system;tag placement;antenna type;reader output power level;low level reader protocol;Miller modulations","","","8","","","","","","IEEE","IEEE Conferences"
"Hard test generation for augmenting path maximum flow algorithms using genetic algorithms: Revisited","M. Buzdalov; A. Shalyto","ITMO University, 49 Kronverkskiy av., Saint-Petersburg, Russia, 197101; ITMO University, 49 Kronverkskiy av., Saint-Petersburg, Russia, 197101","2015 IEEE Congress on Evolutionary Computation (CEC)","","2015","","","2121","2128","To estimate performance of computer science algorithms reliably, one has to create worst-case execution time tests. For certain algorithms this task can be difficult. To reduce the amount of human effort, authors attempt using search-based optimization techniques, such as genetic algorithms. Our previous paper addressed test generation for several maximum flow algorithms. Genetic algorithms were applied for test generation and showed promising results. However, one of the aspects of maximum flow algorithm implementation was missing in that paper: parallel edges (edges which share source and target vertices) were not merged into one single edge (which is allowed in solving maximum flow problems). In this paper, parallel edge merging is implemented and new results are reported. A surprising fact is shown that fitness functions and choices of genetic operators which were the most efficient in the previous paper are much less efficient in the new setup and vice versa. What is more, the set of maximum flow algorithms, for which significantly better tests are generated, changed completely as well.","1089-778X;1941-0026","978-1-4799-7492-4978-1-4799-7491","10.1109/CEC.2015.7257146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7257146","","Generators;Genetic algorithms;Sociology;Statistics;Software algorithms;Extraterrestrial measurements;Genetics","genetic algorithms;program testing;search problems","hard test generation;path maximum flow algorithms;computer science algorithms;worst-case execution time tests;search-based optimization techniques;genetic algorithms;parallel edge merging;fitness functions;genetic operators","","","15","","","","","","IEEE","IEEE Conferences"
"Potential distribution computation and structure optimization for composite cross-arms in 750 kV AC transmission line","X. Yang; N. Li; Z. Peng; J. Liao; Q. Wang","State Key Laboratory of Electrical Insulation and Power Equipment Xi'an Jiaotong University Xi'an, Shaanxi, 710049, P.R.China; State Key Laboratory of Electrical Insulation and Power Equipment Xi'an Jiaotong University Xi'an, Shaanxi, 710049, P.R.China; State Key Laboratory of Electrical Insulation and Power Equipment Xi'an Jiaotong University Xi'an, Shaanxi, 710049, P.R.China; State Key Laboratory of Electrical Insulation and Power Equipment Xi'an Jiaotong University Xi'an, Shaanxi, 710049, P.R.China; State Key Laboratory of Electrical Insulation and Power Equipment Xi'an Jiaotong University Xi'an, Shaanxi, 710049, P.R.China","IEEE Transactions on Dielectrics and Electrical Insulation","","2014","21","4","1660","1669","As composite cross-arm will be used in domestic 750 kV AC transmission line for the first time, careful and meticulous preparatory work is highly needed. Using a three-dimension (3-D) finite-element method (FEM) software, this paper presents computation results of the potential and electric field distribution for composite crossarms in the process of structure optimization. Several optimization programs for composite cross-arm have been studied and the final design is made to omit the suspension insulator string, to increase the length of post insulators and to configure suitable grading and shielding rings based on preliminary design. A set of grading and shielding rings configuration program has been proposed. The comparison between optimized composite cross-arms and 750 kV line composite insulators indicates that composite cross-arms have better potential distribution. The electric tests demonstrate that optimized composite cross-arms have good electrical performance and can meet operation requirements.","1070-9878;1558-4135","","10.1109/TDEI.2014.004130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877993","Composite cross-arms;potential distribution;structure optimization;electric field;750 kV AC;finite element method (FEM);grading and shielding rings.","Insulators;Suspensions;Electric potential;Finite element analysis;Flanges;Poles and towers;Optimization","cable shielding;composite insulators;finite element analysis;power overhead lines","potential distribution computation;structure optimization;composite cross-arms;AC transmission line;3D finite-element method software;FEM software;electric field distribution;suspension insulator string;shielding rings;composite insulators;potential distribution","","9","17","","","","","","IEEE","IEEE Journals & Magazines"
"Enhancing students' geometrical thinking levels through Van Hiele's phase-based Geometer's Sketchpad-aided learning","A. H. Abdullah; J. Surif; L. M. Tahir; N. H. Ibrahim; E. Zakaria","Faculty of Education, Universiti Teknologi Malaysia, Skudai, Malaysia; Faculty of Education, Universiti Teknologi Malaysia, Skudai, Malaysia; Faculty of Education, Universiti Teknologi Malaysia, Skudai, Malaysia; Faculty of Education, Universiti Teknologi Malaysia, Skudai, Malaysia; Faculty of Education, Universiti Kebangsaan Malaysia, Bangi, Malaysia","2015 IEEE 7th International Conference on Engineering Education (ICEED)","","2015","","","106","111","The content order of geometric components in the mathematics curriculum and the teaching process in a geometry class are said to be in contrast with the existing level of geometric thinking among secondary school students in Malaysia. Van Hiele (1986) proposed that directions should be given at the same level as the students' ability and therefore recommended phases of learning geometry in order to address this issue. The present study investigates the effectiveness of activities based on Van Hiele's phases of learning geometry by using the Geometer's Sketchpad (GSP) software which researchers claim is in line with the approach of Van Hiele's model. A quasi-experimental design was used in this study. The study was conducted for six weeks and involved 94 students and two teachers from a secondary school. The students were divided into control and treatment groups, with each group consisting of 47 students. Van Hiele's geometry test was given to both groups before and after the treatment. Five students from each group were randomly selected to identify the early and final levels of their geometric thinking in greater depth. Wilcoxon signed-rank test analysis showed that, although both groups showed significant differences between the early and final levels of their geometric thinking, the students' final thinking levels in both groups were significantly different. The findings from the analysis of the interview data also support the findings of the statistical analysis. Therefore, it is demonstrated that the Van Hiele phases of learning geometry should be used as a reference in arranging the geometric curriculum and content. In addition, the Van Hiele model and the GSP software can be used as an approach to teaching and learning geometry in the classroom to help students improve their level of geometric thinking.","","978-1-4799-8810-5978-1-4799-8809","10.1109/ICEED.2015.7451502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7451502","Geometrical thinking;Van Hiele Model;Geometry;Geometer's Sketchpad;Dynamic Geometry Software","Geometry;Software;Mathematical model;Shape;Conferences;Engineering education","computer aided instruction;design of experiments;mathematics computing;statistical testing","student geometrical thinking levels;Van Hiele phase-based geometer sketchpad-aided learning;geometric components;mathematics curriculum;teaching process;geometry class;secondary school students;Malaysia;student ability;GSP software;quasiexperimental design;Wilcoxon signed-rank test analysis;geometric curriculum;geometric content","","","42","","","","","","IEEE","IEEE Conferences"
"Automated diagnosis of software configuration errors","S. Zhang; M. D. Ernst","Computer Science & Engineering, University of Washington, USA; Computer Science & Engineering, University of Washington, USA","2013 35th International Conference on Software Engineering (ICSE)","","2013","","","312","321","The behavior of a software system often depends on how that system is configured. Small configuration errors can lead to hard-to-diagnose undesired behaviors. We present a technique (and its tool implementation, called ConfDiagnoser) to identify the root cause of a configuration error - a single configuration option that can be changed to produce desired behavior. Our technique uses static analysis, dynamic profiling, and statistical analysis to link the undesired behavior to specific configuration options. It differs from existing approaches in two key aspects: it does not require users to provide a testing oracle (to check whether the software functions correctly) and thus is fully automated; and it can diagnose both crashing and non-crashing errors. We evaluated ConfDiagnoser on 5 non-crashing configuration errors and 9 crashing configuration errors from 5 configurable software systems written in Java. On average, the root cause was ConfDiagnoser's fifth-ranked suggestion; in 10 out of 14 errors, the root cause was one of the top 3 suggestions; and more than half of the time, the root cause was the first suggestion.","0270-5257;1558-1225","978-1-4673-3076-3978-1-4673-3073","10.1109/ICSE.2013.6606577","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606577","","Databases;Computer crashes;Software;Java;Instruments;Debugging;Libraries","configuration management;Java;program diagnostics;statistical analysis","software configuration error diagnosis;ConfDiagnoser;configuration error identification;static analysis;dynamic profiling;statistical analysis;noncrashing configuration errors;configurable software system;Java","","35","36","","","","","","IEEE","IEEE Conferences"
"Gain-Scheduled<inline-formula><tex-math notation=""LaTeX"">$\ell _{1}$</tex-math></inline-formula>-Optimal Control of Variable-Speed-Variable-Pitch Wind Turbines","H. Jafarnejadsani; J. Pieper","Department of Mechanical Science and Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, USA; Department of Mechanical Engineering, University of Calgary, Calgary, AB, Canada","IEEE Transactions on Control Systems Technology","","2015","23","1","372","379","The fast-growing technology of large scale wind turbines demands control systems capable of enhancing both the efficiency of capturing wind power, and the useful life of the turbines themselves. l<sub>1</sub>-Optimal control is an approach to deal with persistent exogenous disturbances which have bounded magnitude (l<sub>∞</sub>-norm) such as realistic wind disturbances and turbulence profiles. In this brief, we develop an efficient method to compute the l<sub>1</sub>-norm of a system. As the control synthesis problem is nonconvex, we use the proposed method to design the optimal output feedback controllers for a linear model of a wind turbine at different operating points using genetic algorithm optimization. The locally optimized controllers are interpolated using a gain-scheduled technique with guaranteed stability. The controller is tested with comprehensive simulation studies on a 5 MW wind turbine using fatigue, aerodynamics, structures, and turbulence (FAST) software. The proposed controller is compared with a well-tuned proportional-integral (PI) controller. The results show improved power quality, and decrease in the fluctuations of generator torque and rotor speed.","1063-6536;1558-0865;2374-0159","","10.1109/TCST.2014.2320675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6817567","Computation speed;discrete-time system;fatigue;aerodynamics;structures;and turbulence (FAST) software;gain-scheduling;genetic algorithm (GA);ℓ₁-optimal control;variable-speed--variable-pitch (VS--VP) wind turbine.;Computation speed;discrete-time system;fatigue;aerodynamics;structures;and turbulence (FAST) software;gain-scheduling;genetic algorithm (GA);$\ell_{1}$ -optimal control;variable-speed–variable-pitch (VS–VP) wind turbine","Wind turbines;Generators;Torque;Rotors;Wind speed;Optimization;Discrete-time systems","aerodynamics;control system synthesis;fatigue;gain control;genetic algorithms;optimal control;stability;turbulence;wind power plants;wind turbines","gain-scheduled l1-optimal control;variable-speed-variable-pitch wind turbines;large scale wind turbine demand control systems;wind power capturing efficiency;realistic wind disturbances;turbulence profiles;control synthesis problem;optimal output feedback controllers;linear model;genetic algorithm optimization;locally optimized controllers;gain-scheduled technique;guaranteed stability;fatigue-aerodynamics-structures and turbulence software;FAST software;proportional-integral controller;PI controller;power quality;generator torque;generator rotor speed","","20","24","","","","","","IEEE","IEEE Journals & Magazines"
"Green Networking With Packet Processing Engines: Modeling and Optimization","R. Bolla; R. Bruschi; A. Carrega; F. Davoli","Department of Electrical, Electronic and Telecommunication Engineering, and Naval Architecture (DITEN), University of Genoa; National Inter-University Consortium for Telecommunications (CNIT), University of Genoa Research Unit, Genoa, Italy; Department of Electrical, Electronic and Telecommunication Engineering, and Naval Architecture (DITEN), University of Genoa; Department of Electrical, Electronic and Telecommunication Engineering, and Naval Architecture (DITEN), University of Genoa","IEEE/ACM Transactions on Networking","","2014","22","1","110","123","With the aim of controlling power consumption in metro/transport and core networks, we consider energy-aware devices able to reduce their energy requirements by adapting their performance. In particular, we focus on state-of-the-art packet processing engines, which generally represent the most energy-consuming components of network devices, and which are often composed of a number of parallel pipelines to “divide and conquer” the incoming traffic load. Our goal is to control both the power configuration of pipelines and the way to distribute traffic flows among them. We propose an analytical model to accurately represent the impact of green network technologies (i.e., low power idle and adaptive rate) on network- and energy-aware performance indexes. The model has been validated with experimental results, performed by using energy-aware software routers loaded by real-world traffic traces. The achieved results demonstrate how the proposed model can effectively represent energy- and network-aware performance indexes. On this basis, we propose a constrained optimization policy, which seeks the best tradeoff between power consumption and packet latency times. The procedure aims at dynamically adapting the energy-aware device configuration to minimize energy consumption while coping with incoming traffic volumes and meeting network performance constraints. In order to deeply understand the impact of such policy, a number of tests have been performed by using experimental data from software router architectures and real-world traffic traces.","1063-6692;1558-2566","","10.1109/TNET.2013.2242485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6462001","Adaptive rate;forwarding engine;green networking;low power idle;multipipeline","Pipelines;Engines;Performance evaluation;Power demand;Green products;Servers;Optimization","computer networks;divide and conquer methods;green computing;optimisation;telecommunication computing;telecommunication traffic","real-world traffic traces;software router architectures;energy-aware device configuration;constrained optimization policy;energy-aware software routers;divide and conquer method;state-of-the-art packet processing engines;metro-transport networks;core networks;power consumption;packet processing engines;green networking","","21","41","","","","","","IEEE","IEEE Journals & Magazines"
"Design and Analysis of a 32-bit Embedded High-Performance Cluster Optimized for Energy and Performance","M. F. Cloutier; C. Paradis; V. M. Weaver","NA; NA; NA","2014 Hardware-Software Co-Design for High Performance Computing","","2014","","","1","8","A growing number of supercomputers are being built using processors with low-power embedded ancestry, rather than traditional high-performance cores. In order to evaluate this approach we investigate the energy and performance tradeoffs found with ten different 32-bit ARM development boards while running the HPL Linpack and STREAM benchmarks.Based on these results (and other practical concerns) we chose the Raspberry Pi as a basis for a power-aware embedded cluster computing testbed. Each node of the cluster is instrumented with power measurement circuitry so that detailed cluster-wide power measurements can be obtained, enabling power / performance co-design experiments.While our cluster lags recent x86 machines in performance, the power, visualization, and thermal features make it an excellent low-cost platform for education and experimentation.","","978-1-4799-7564","10.1109/Co-HPC.2014.7","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017957","","Power measurement;Program processors;Benchmark testing;Supercomputers;Universal Serial Bus;Random access memory;Servers","benchmark testing;embedded systems;microcontrollers;parallel processing;power aware computing","embedded high-performance cluster analysis;embedded high-performance cluster design;energy optimization;performance optimization;supercomputers;low-power embedded system;ARM development boards;HPL Linpack benchmark;STREAM benchmark;Raspberry Pi;power-aware embedded cluster computing testbed;instrumented cluster node;power measurement circuitry;cluster-wide power measurements;power-performance co-design experiments;visualization feature;thermal feature;performance feature;power feature","","10","36","","","","","","IEEE","IEEE Conferences"
"Optimization of P2P-TV traffic by means of header compression and multiplexing","I. Quintana-Ramirez; J. Saldana; J. Ruiz-Mas; L. Sequeira; J. Fernandez-Navajas; L. Casadesus","Communications Technology Group (GTC), Aragon Inst. of Engeneering Research (I3A), Dpt. IEC. Ada Byron Building, EINA University of Zaragoza, 50018, Zaragoza, Spain; Communications Technology Group (GTC), Aragon Inst. of Engeneering Research (I3A), Dpt. IEC. Ada Byron Building, EINA University of Zaragoza, 50018, Zaragoza, Spain; Communications Technology Group (GTC), Aragon Inst. of Engeneering Research (I3A), Dpt. IEC. Ada Byron Building, EINA University of Zaragoza, 50018, Zaragoza, Spain; Communications Technology Group (GTC), Aragon Inst. of Engeneering Research (I3A), Dpt. IEC. Ada Byron Building, EINA University of Zaragoza, 50018, Zaragoza, Spain; Communications Technology Group (GTC), Aragon Inst. of Engeneering Research (I3A), Dpt. IEC. Ada Byron Building, EINA University of Zaragoza, 50018, Zaragoza, Spain; Communications Technology Group (GTC), Aragon Inst. of Engeneering Research (I3A), Dpt. IEC. Ada Byron Building, EINA University of Zaragoza, 50018, Zaragoza, Spain","2013 21st International Conference on Software, Telecommunications and Computer Networks - (SoftCOM 2013)","","2013","","","1","5","This paper studies the optimization of the traffic of a P2P-TV application (SOPCast). First, a traffic characterization is deployed, and it is observed that the service generates a high rate of small UDP packet bursts between peers. Then, an optimization method based on header compression and multiplexing is used for sending together the packets with the same destination. Two multiplexing policies are defined and tested. The first one is based on a fixed multiplexing period, and the other one defines an inter-packet time threshold, with the aim of multiplexing together a whole traffic burst. Simulations using real traffic traces of SOPCast are performed in order to estimate the expected savings for both policies. The results show that the efficiency is improved, achieving uplink bandwidth savings between 26% and 33% for the period-based policy, and roughly 35% for the policy based on a threshold. The amount of packets per second is also reduced by a factor of 10 in both cases. As a counterpart, the addition of a small retention delay is necessary, but the tests show that it does not impair user's experience.","","978-953-290-040","10.1109/SoftCOM.2013.6671891","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671891","P2P-TV;SOPCast;header compression;multiplexing;traffic optimization","Multiplexing;Streaming media;Bandwidth;Delays;Optimization;Peer-to-peer computing;Games","IPTV;peer-to-peer computing;telecommunication traffic;transport protocols","P2P-TV traffic optimization;UDP packet;header compression;header multiplexing;interpacket time threshold;SOPCast;uplink bandwidth saving;retention delay","","","22","","","","","","IEEE","IEEE Conferences"
"Component-based approach for intelligent evaluation of complex algorithms","C. Mueller; A. Hofmeister; M. Breckner","Faculty of Informatics and Statistics, University of Economics Nám. W. Churchilla 4, 130 67 Prague; Department of Applied Informatics, Baden-Wuerttemberg Cooperative State University, Lohrtalweg 10, 74821 Mosbach; Department of Applied Informatics, Baden-Wuerttemberg Cooperative State University, Lohrtalweg 10, 74821 Mosbach","2014 IEEE 5th International Conference on Software Engineering and Service Science","","2014","","","808","811","In this paper a new approach for handling the problem of comparing different complex algorithms is proposed. The main idea behind this research is to set up a framework that can be applied to the most well-known and widely used complex algorithms. A component-based framework, IEOCA (Intelligent Evaluation of Complex Algorithms) written in Java for building and studying complex algorithms like genetic algorithms and ant colony optimization is developed. The evaluation framework provides a standard testing methodology for comparing the accuracy and performance of selected algorithms.","2327-0594;2327-0586","978-1-4799-3279-5978-1-4799-3278-8978-1-4799-3277","10.1109/ICSESS.2014.6933689","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933689","framework;evaluation;benchmark;algorithms;component-based software engineering","Software algorithms;Software;Computer architecture;Object oriented modeling;Runtime;Algorithm design and analysis;Heuristic algorithms","ant colony optimisation;genetic algorithms;Java;mathematics computing;object-oriented programming","component-based framework;intelligent evaluation of complex algorithms;IEOCA;Java;genetic algorithms;ant colony optimization","","","14","","","","","","IEEE","IEEE Conferences"
"Model-based cyber-physical system integration in the process industry","Z. Song; P. Labalette; R. Burger; W. Klein; S. Nair; S. Suresh; L. Shen; A. Canedo","Siemens Corporation, Corporate Technology; Siemens Process Industries and Drives; Siemens Process Industries and Drives; Siemens Corporation, Corporate Technology; Siemens Corporation, Corporate Technology; Siemens Corporation, Corporate Technology; Siemens Corporation, Corporate Technology; Siemens Corporation, Corporate Technology","2015 IEEE International Conference on Automation Science and Engineering (CASE)","","2015","","","1012","1017","Process industry's modern process control valves are complex Cyber-Physical Systems (CPS). A valve positioner may be connected to thousands of different types of process valves, and operate under a large variety of environmental conditions; therefore it is unfeasible to conduct exhaustive physical tests. The end user expects the system to work under any conditions and configurations and deliver high performance whenever required. This paper presents a Virtual Testing Environment (VTE) for process industry CPS performance assessment based on Amesim simulation software. A set of innovative techniques are used to effectively search for the best and worst system performances within the unlimited solution space. In order to speed up the simulation, we combine model order reduction techniques with an extensible optimizer using with different solvers as Amesim plug-ins. A Scheduler was designed to execute simulation tasks on multiple servers in parallel. This paper presents the preliminary results based on Particle Swarm Optimization (PSO) and over 5 billion configurations.","2161-8070;2161-8089","978-1-4673-8183-3978-1-4673-8182","10.1109/CoASE.2015.7294231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294231","","Mathematical model;Valves;Software;Solid modeling;Process control;Optimization;Data models","control engineering computing;large-scale systems;particle swarm optimisation;position control;process control;production engineering computing;reduced order systems;valves","model-based cyber-physical system integration;modern process control valves;complex CPS;valve positioner;environmental condition;virtual testing environment;VTE;process industry CPS performance assessment;Amesim simulation software;system performance;model order reduction technique;extensible optimizer;Amesim plug-in;scheduler design;parallel simulation task execution;particle swarm optimization;PSO","","1","21","","","","","","IEEE","IEEE Conferences"
"Multiple Bug Spectral Fault Localization Using Genetic Programming","L. Naish; Neelofar; K. Ramamohanarao","NA; NA; NA","2015 24th Australasian Software Engineering Conference","","2015","","","11","17","Debugging is crucial for producing reliable software. One of the effective bug localization techniques is Spectral-Based Fault Localization (SBFL). It locates a buggy statement by applying an evaluation metric to program spectra and ranking program components on the basis of the score it computes. Recently, genetic programming has been proposed as a way to find good metrics. We have found that the huge search space for metrics can cause this approach to be slow and unreliable, even for relatively simple data sets. Here we propose a restricted class of ""hyperbolic"" metrics, with a small number of numeric parameters. This class of functions is based on past theoretical and empirical results. We show that genetic programming can reliably discover effective metrics over a wide range of data sets of program spectra. We evaluate the performance for both real programs and model programs with single bugs, multiple bugs, ""deterministic"" bugs and nondeterministic bugs.","1530-0803","978-1-4673-9390","10.1109/ASWEC.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365789","","Measurement;Computer bugs;Genetic programming;Reliability;Software;Debugging;Benchmark testing","genetic algorithms;program debugging;software metrics","software debugging;bug localization techniques;spectral-based fault localization;SBFL;evaluation metric;program spectra;program components ranking;genetic programming;hyperbolic metrics;deterministic bugs;nondeterministic bugs","","2","29","","","","","","IEEE","IEEE Conferences"
"Solving reliability redundancy allocation problems with orthogonal simplified swarm optimization","W. Yeh; V. Y. Y. Chung; Yun-Zhi Jiang; Xiangjian He","Department of Industrial Engineering and Engineering Management, National Tsing Hua University, Hsinchu 300, Taiwan; School of Information Technology, University of Sydney, NSW 2006, Australia; School of Software, Jiangxi Agricultural University, Nanchang, China 330045; Computer Vision and Recognition Laboratory, Research Centre for Innovation in IT Services and Applications (iNEXT), University of Technology, Sydney (UTS), PO Box 123, Broadway NSW 2007, Australia","2015 International Joint Conference on Neural Networks (IJCNN)","","2015","","","1","7","This study applies a penalty guided strategy and the orthogonal array test (OA) based on the Simplified Swarm Optimization algorithm (SSO) to solve the reliability redundancy allocation problems (RRAP) in the series system, the series-parallel system, the complex (bridge) system, and the overspeed protection of gas turbine system. For several decades, the RRAP has been one of the most well known techniques. The maximization of system reliability, the number of redundant components, and the reliability of corresponding components in each subsystem have to be decided simultaneously with nonlinear constraints, acting as one difficulty for the use of the RRAP. In other words, the objective function of the RRAP is the mixed-integer programming problem with the nonlinear constraints. The RRAP is of the class of NP-hard. Hence, in this paper, the SSO algorithm is proposed to solve the RRAP and improve computation efficiency for these NP-hard problems. There are four RRAP problems used to illustrate the applicability and the effectiveness of the SSO. The experimental results are compared with previously developed algorithms in literature. Moreover, the maximum-possible-improvement (MPI) is used to measure the amount of improvement of the solution found by the SSO to the previous solutions. According to the results, the system reliabilities obtained by the proposed SSO for the four RRAP problems are as well as or better than the previously best-known solutions.","2161-4407;2161-4393","978-1-4799-1960-4978-1-4799-1959","10.1109/IJCNN.2015.7280420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280420","Reliability;Redundancy allocation problem;RRAP;Simplified Swarm Optimization algorithm;SSO;Mixed-integer nonlinear programming","Rails;Reliability;Optimization","computational complexity;gas turbines;integer programming;large-scale systems;redundancy;swarm intelligence","reliability redundancy allocation problems;RRAP;orthogonal simplified swarm optimization;SSO;penalty guided strategy;orthogonal array test;series system;series-parallel system;complex system;bridge system;gas turbine system overspeed protection;system reliability maximization;mixed-integer programming problem;NP-hard problems;maximum-possible-improvement;MPI","","","46","","","","","","IEEE","IEEE Conferences"
"Library functions identification in binary code by using graph isomorphism testings","Jing Qiu; Xiaohong Su; Peijun Ma","School of Computer Science and Technology, Harbin Institute of Technology, China; School of Computer Science and Technology, Harbin Institute of Technology, China; School of Computer Science and Technology, Harbin Institute of Technology, China","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","","2015","","","261","270","Library functions identification is a key technique in reverse engineering. Discontinuity and polymorphism of inline and optimized library functions in binary code create a difficult challenge for library functions identification. To solve this problem, a novel approach is developed to identify library functions. First, we introduce execution dependence graphs (EDGs) to describe the behavior characteristics of binary code. Then, by finding similar EDG subgraphs in target functions, we identify both full and inline library functions. Experimental results from the prototype tool show that the proposed method is not only capable of identifying inline functions but is also more efficient and precise than the current methods for identifying full library functions.","1534-5351","978-1-4799-8469","10.1109/SANER.2015.7081836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081836","Binary code analysis;library functions identification;graph isomorphism","Libraries;Binary codes;Registers;Semantics;Reverse engineering;Optimization;Testing","graph theory;reverse engineering;software libraries;source code (software)","library function identification;binary code;graph isomorphism testing;reverse engineering;execution dependence graph;EDG subgraph","","","27","","","","","","IEEE","IEEE Conferences"
"Bambu: A modular framework for the high level synthesis of memory-intensive applications","C. Pilato; F. Ferrandi","Politecnico di Milano - DEIB, P.zza Leonardo da Vinci, 32, 20133 - Milano (Italy); Politecnico di Milano - DEIB, P.zza Leonardo da Vinci, 32, 20133 - Milano (Italy)","2013 23rd International Conference on Field programmable Logic and Applications","","2013","","","1","4","This paper presents bambu, a modular framework for research on high-level synthesis currently under development at Politecnico di Milano. It can accept most of C constructs without requiring any three-state for their implementations by exploiting a novel and efficient memory architecture. It also allows the integration of floating-point units and thus it can deal with a wide range of data types. Finally, it allows to easily customize the synthesis flow (e.g., transformation passes, constraints, options, synthesis scripts) through an XML file and it automatically generates test-benches and validates the results against the corresponding software execution, supporting both ASIC and FPGA technologies.","1946-147X;1946-1488","978-1-4799-0004","10.1109/FPL.2013.6645550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6645550","","Software;XML;Resource management;Optimization;Benchmark testing;Hardware;Clocks","application specific integrated circuits;C language;field programmable gate arrays;floating point arithmetic;high level synthesis;memory architecture;XML","FPGA technology;ASIC technology;software execution;test-benches;XML file;synthesis flow;data types;floating-point units;memory architecture;C constructs;Politecnico di Milano;memory-intensive applications;high level synthesis;Bambu","","18","12","","","","","","IEEE","IEEE Conferences"
"Measuring security for cloud service provider: A Third Party approach","M. Whaiduzzaman; A. Gani","Mobile Cloud Computing Research Lab, Faculty of Computer Science and Information Technology, University of Malaya, Malaysia; Mobile Cloud Computing Research Lab, Faculty of Computer Science and Information Technology, University of Malaya, Malaysia","2013 International Conference on Electrical Information and Communication Technology (EICT)","","2014","","","1","6","Cloud Computing (CC) is a new paradigm of utility computing and enormously growing phenomenon in the present IT industry hype. CC leverages low cost investment opportunity for the new business entrepreneur as well as business avenues for cloud service providers. As the number of the new Cloud Service Customer (CSC) increases, users require a secure, reliable and trustworthy Cloud Service Provider (CSP) from the market to store confidential data. However, a number of shortcomings in reliable monitoring and identifying security risks, threats are an immense concern in choosing the highly secure CSP for the wider cloud community. The secure CSP ranking system is currently a challenging aspect to gauge trust, privacy and security. In this paper, a Trusted Third Party (TTP) like credit rating agency is introduced for security ranking by identifying current assessable security risks. We propose an automated software scripting model by penetration testing for TTP to run on CSP side and identify the vulnerability and check security strength and fault tolerance capacity of the CSP. Using the results, several non-measurable metrics are added and provide the ranking system of secured trustworthy CSP ranking systems. Moreover, we propose a conceptual model for monitoring and maintaining such TTP cloud ranking providers worldwide called federated third party approach. Hence the model of federated third party cloud ranking and monitoring system assures and boosts up the confidence to make a feasible secure and trustworthy market of CSPs.","","978-1-4799-2299-4978-1-4799-2297","10.1109/EICT.2014.6777855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777855","Cloud computing;cloud service provider;trusted third party;cloud security ranking","Security;Monitoring;Measurement;Business;Cloud computing;Mobile communication","cloud computing;program testing;trusted computing","security measurement;cloud service provider;cloud computing;CC;utility computing;IT industry;information technology;business entrepreneur;business avenues;cloud service customer;CSC;confidential data storage;security risks monitoring;security risks identification;CSP ranking system;trusted third party;TTP;credit rating agency;automated software scripting model;penetration testing;CSP security strength;CSP fault tolerance capacity;federated third party approach","","3","15","","","","","","IEEE","IEEE Conferences"
"The optimization of transducer layout in rail flaw detection","L. Zhang; X. Gao; W. Wang; C. Peng","Intelligent Transportation System Research Center, Southeast University, Nanjing 210096, China; School of Physical Science and Technology, Southwest Jiaotong University, Chengdu 610031, China; School of Transportation, Southeast University, Nanjing 210096, China; School of Physical Science and Technology, Southwest Jiaotong University, Chengdu 610031, China","2015 IEEE Far East NDT New Technology & Application Forum (FENDT)","","2015","","","120","123","The railway is a kind of large volume public transportation, and more and more attention is paid to the safety of rails nowadays. As the rail is subject to heavy cyclic loads of trains, it is easily broken in the long-term operations, and this will seriously affect the safety of the trains. As an important rail flaw detection method, ultrasonic testing has been widely utilized to ensure the safety of the rail. However, the existing programs of this method normally have problems such as the deflection angle of transducer, number of transducers and frequent replacement of transducer layout. A new method was proposed in this paper, which is based on both the conventional transducer and the phased array transducer. Matlab software and acoustics approach were employed to optimize the parameters and the deflection angle of transducers. Results show that this method is experimentally reasonable and it can meet requirements of rail flaw detection.","","978-1-4673-7001-1978-1-4673-7000-4978-1-4673-6999","10.1109/FENDT.2015.7398323","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398323","rail flaw detection;phased array ultrasonic;transducer layout","Rail transportation;Inspection","flaw detection;rails;railway safety;ultrasonic applications;ultrasonic transducers","transducer layout optimization;large volume public transportation;railway;heavy cyclic loads;long-term operations;rail flaw detection method;ultrasonic testing;transducer deflection angle;phased array transducer;conventional transducer;Matlab software;acoustics approach","","","12","","","","","","IEEE","IEEE Conferences"
"Risks in design, construction and testing of grounding system: A recent CEATI project by GLTF (Grounding and Lightning Task Force)","B. Jamali; S. Otal; B. Ma; A. Mogilevsky","Metsco Energy Solutions Inc.; Metsco Energy Solutions Inc., Mississauga, Ontario, Canada; Engineer in Training, Metsco Energy Solutions Inc.; T&amp;D, CEATI International","2014 IEEE PES T&D Conference and Exposition","","2014","","","1","5","This paper describes the key findings of a recently completed research project. The primary objective of this project is to develop a guide for use by electric utilities through comprehensive review of the processes and tools employed in design, construction and testing of grounding systems with the objective of identifying potential sources of errors that can result in either overly pessimistic (non-optimal) designs or overly optimistic (with unsatisfactory performance) design of the grounding system, introducing unintended risk in achieving the safe grounding conditions in most cost efficient manner. The guide identifies design software limitations, design assumptions, errors and omissions during construction in not following the design specifications and measurement errors during the commissioning phase to be the key sources of error that can introduce significant risks affecting ground grid performance or their cost efficiency. The guide also provides recommendations to control and mitigate such risks.","2160-8563;2160-8555","978-1-4799-3656","10.1109/TDC.2014.6863456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6863456","Construction;design engineering;grounding;Risk analysis;testing","Soil;Conductivity;Grounding;Conductors;Circuit faults;Testing;Electric potential","","","","","","","","","","","IEEE","IEEE Conferences"
"Constrained feature selection for localizing faults","T. B. Le; D. Lo; M. Li","School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; National Key Laboratory for Novel Software Technology, Nanjing University, China","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","2015","","","501","505","Developers often take much time and effort to find buggy program elements. To help developers debug, many past studies have proposed spectrum-based fault localization techniques. These techniques compare and contrast correct and faulty execution traces and highlight suspicious program elements. In this work, we propose constrained feature selection algorithms that we use to localize faults. Feature selection algorithms are commonly used to identify important features that are helpful for a classification task. By mapping an execution trace to a classification instance and a program element to a feature, we can transform fault localization to the feature selection problem. Unfortunately, existing feature selection algorithms do not perform too well, and we extend its performance by adding a constraint to the feature selection formulation based on a specific characteristic of the fault localization problem. We have performed experiments on a popular benchmark containing 154 faulty versions from 8 programs and demonstrate that several variants of our approach can outperform many fault localization techniques proposed in the literature. Using Wilcoxon rank-sum test and Cliff's d effect size, we also show that the improvements are both statistically significant and substantial.","","978-1-4673-7532-0978-1-4673-7531","10.1109/ICSM.2015.7332502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332502","","Standards;Feature extraction;Software;Machine learning algorithms;Benchmark testing;Computer bugs;Information systems","feature selection;program debugging","constrained feature selection algorithms;execution trace mapping;classification instance;fault localization;programs;Wilcoxon rank-sum test;Cliffs d effect size","","1","14","","","","","","IEEE","IEEE Conferences"
"Cassandra: Proactive conflict minimization through optimized task scheduling","B. K. Kasi; A. Sarma","Computer Science and Engineering Department, University of Nebraska-Lincoln, Lincoln, NE, USA; Computer Science and Engineering Department, University of Nebraska-Lincoln, Lincoln, NE, USA","2013 35th International Conference on Software Engineering (ICSE)","","2013","","","732","741","Software conflicts arising because of conflicting changes are a regular occurrence and delay projects. The main precept of workspace awareness tools has been to identify potential conflicts early, while changes are still small and easier to resolve. However, in this approach conflicts still occur and require developer time and effort to resolve. We present a novel conflict minimization technique that proactively identifies potential conflicts, encodes them as constraints, and solves the constraint space to recommend a set of conflict-minimal development paths for the team. Here we present a study of four open source projects to characterize the distribution of conflicts and their resolution efforts. We then explain our conflict minimization technique and the design and implementation of this technique in our prototype, Cassandra. We show that Cassandra would have successfully avoided a majority of conflicts in the four open source test subjects. We demonstrate the efficiency of our approach by applying the technique to a simulated set of scenarios with higher than normal incidence of conflicts.","0270-5257;1558-1225","978-1-4673-3076-3978-1-4673-3073","10.1109/ICSE.2013.6606619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606619","Collaborative development;coordination;collaboration conflicts;task scheduling","Context;Shape;Iron;Minimization;Software;Syntactics;Control systems","minimisation;scheduling;task analysis","Cassandra;proactive conflict minimization;optimized task scheduling;software conflicts;workspace awareness tools;constraint space;conflict minimal development paths;open source projects;open source test subjects","","11","44","","","","","","IEEE","IEEE Conferences"
"Component-based visual evolutionary computation","L. Garzón; N. Rodriguez; H. Orjuela; S. Rojas-Galeano","DreamGis SAS, Bogotá, Colombia; CHECK24 Vergleichsportal GmbH, Munich Area, Germany; School of Engineering, Universidad Distrital, Bogotá, Colombia; School of Engineering, Universidad Distrital, Bogotá, Colombia","2015 10th Computing Colombian Conference (10CCC)","","2015","","","392","399","Component-based computation advocates for building systems by assembling prefabricated self-contained software modules within a formally specified composition model. Furthermore, visual components allow users to perform such assemblage in a friendly graphical board were components are connected visually with virtually no coding needed. Recently, an Evolutionary Computation (EC) suite of components called Goldenberry complying with such premises has been released. In this paper we demonstrate its feasibility in several EC tasks: optimisation of continuous-valued and discrete-valued costs functions using Genetic Algorithms and Estimation of Distribution Algorithms as well as feature selection algorithms and relevance estimation in data mining tasks. We discuss the advantages and weaknesses of the approach and the toolbox, its differences with other existing software tools and proposing avenues of future development. Goldenberry is open-source under the New BSD License and it is available at.","","978-1-4673-9464-2978-1-4673-9463","10.1109/ColumbianCC.2015.7333451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333451","","Cost function;Visualization;Genetic algorithms;Software;Benchmark testing;Sociology","data mining;distributed algorithms;feature selection;formal specification;genetic algorithms;object-oriented programming;public domain software","component-based visual evolutionary computation;prefabricated self-contained software module assembling;formally specified composition model;visual components;friendly-graphical board;EC suite;Goldenberry;continuous-valued cost function optimisation;discrete-valued cost function optimisation;genetic algorithms;distribution algorithm estimation;feature selection algorithms;relevance estimation;data mining tasks;software tools;open-source software;BSD license","","","26","","","","","","IEEE","IEEE Conferences"
"Properties estimation of inhomogeneous dispersive materials by electromagnetic modeling and dielectric spectroscopy","F. Ciuprina; B. Ghiliftoiu","ELMAT Laboratory, Faculty of Electrical Engineering, University Politehnica of Bucharest, Bucharest, 060042, Romania; ELMAT Laboratory, Faculty of Electrical Engineering, University Politehnica of Bucharest, Bucharest, 060042, Romania","2015 IEEE MTT-S International Conference on Numerical Electromagnetic and Multiphysics Modeling and Optimization (NEMO)","","2015","","","1","4","An electroquasistatic numerical model of polyethylene insulation affected by water trees is proposed in this study. The model includes dispersive materials (polyethylene and water trees) having the properties set after processing experimental results obtained by dielectric spectroscopy in the frequency range 10<sup>-2</sup>-10<sup>6</sup> Hz, combined with the usage of two semi-analytical models for effective permittivity of dielectric mixtures. The numerical model is implemented in the finite element method of the software package COMSOL Multiphysics. The solution of the proposed model was computed for all frequencies tested in dielectric spectroscopy measurements, and the effective permittivity (real part) and loss tangent as resulting from the numerical model were compared with the experimental ones. The modeling results are close to the experimental data, thus showing that the inverse problem approach proposed in this work leads to a good estimation of the frequency dependent behavior of dielectric inclusions, such as the water trees grown in polyethylene insulation.","","978-1-4799-6811-4978-1-4799-6810","10.1109/NEMO.2015.7415068","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415068","Cable insulation;water trees;dielectric spectroscopy;electromagnetic modeling;finite element method;effective permittivity","Permittivity;Numerical models;Dielectrics;Trees - insulation;Dispersion;Mathematical model;Spectroscopy","computational electromagnetics;dispersive media;electromagnetic waves;finite element analysis;inverse problems;permittivity;software packages","inhomogeneous dispersive materials;electromagnetic modeling;dielectric spectroscopy;electroquasistatic numerical model;polyethylene insulation;water trees;dielectric mixtures permittivity;finite element method;software package;COMSOL Multiphysics;inverse problem approach;dielectric inclusions","","","8","","","","","","IEEE","IEEE Conferences"
"Design techniques of future reliable SoCs","I. Sourdis; C. Strydis","Chalmers University of Technology, Sweden; Neurasmus BV, The Netherlands","2013 2nd Mediterranean Conference on Embedded Computing (MECO)","","2013","","","1","2","Summary form only given. As a side effect of semiconductor technology scaling, chips are becoming ever less reliable. Prominent reasons for this phenomenon are the increasing sheer number of transistors on a given silicon area and their shrinking device features. As a consequence, fault tolerance has to be more often applied, e.g. provided through various redundancy schemes, but at the same time, the fault tolerance has more and more disproportionally increasing power and performance costs. To make the matters even worse, the chip power density is becoming a significant limiting factor for performance and SoC design in general, as it bars the maximum allowed number of online transistors per chip unit area. In the face of such changes in the technological landscape, current solutions for fault tolerance are expected to introduce an excessive overhead in future SoCs. Attempting to design and manufacture a totally faultfree system, would heavily or even prohibitively impact the design, manufacturing, and testing costs, as well as, the performance and power consumption of a system. In this tutorial, we will address the above challenges and discuss new design techniques for more efficient, adaptive fault-tolerant SoCs. We will describe a SoC architecture which uses a small, guaranteed-by-manufacturing reliable fraction of the chip to manage the remaining unreliable SoC resources. We will further discuss how the flexibility of reconfigurable hardware can be used to tolerate (permanent) chip defects and ageing faults. We will also present novel approaches for dealing with transient and intermittent faults, as well as (software) runtime optimization mechanisms to adapt the system to various fault types and density on demand, in order to improve the system efficiency and facilitate graceful system degradation. Finally, as an interesting case study, we will discuss the particular safety and system requirements of two cutting-edge, medical devices and explain how the above design techniques can be applied to such highly-demanding systems.","2377-5475","978-9940-9436-1","10.1109/MECO.2013.6601392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601392","","Abstracts","circuit optimisation;fault tolerant computing;integrated circuit design;integrated circuit manufacture;integrated circuit reliability;integrated circuit testing;performance evaluation;reconfigurable architectures;system-on-chip","reliable SoC design techniques;semiconductor technology scaling;transistor sheer number;fault tolerance;redundancy schemes;chip power density;faultfree system;testing costs;power consumption;manufacturing costs;adaptive fault-tolerant SoCs;SoC architecture;SoC resources;reconfigurable hardware flexibility;chip defects;ageing faults;runtime optimization mechanisms;density on demand;system efficiency;system requirements;safety requirements","","","","","","","","","IEEE","IEEE Conferences"
"An opposition-based repair operator for multi-objective evolutionary algorithm in constrained optimization problems","Zhun Fan; Han Huang; Wenji Li; Shuxiang Xie; Xinye Cai; E. Goodman","Department of Electronic Engineering, Shantou University, Guangdong, 515063, China; School of Software Engineering, South China University of Technology, Guangdong, Guangzhou 510006, China; Department of Electronic Engineering, Shantou University, Guangdong, 515063, China; Department of Electronic Engineering, Shantou University, Guangdong, 515063, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Jiangsu, 210016, China; BEACON Centre for the Study of Evolution in Action, Michigan State University, East Lansing, 48824, USA","2015 11th International Conference on Natural Computation (ICNC)","","2015","","","330","336","In this paper, we design a set of multi-objective constrained optimization problems (MCOPs) and propose a new repair operator to address them. The proposed repair operator is used to fix the solutions that violate the box constraints. More specifically, it employs a reversed correction strategy that can effectively avoid the population falling into local optimum. In addition, we integrate the proposed repair operator into two classical multi-objective evolutionary algorithms MOEA/D and NSGA-II. The proposed repair operator is compared with other two kinds of commonly used repair operators on benchmark problems CTPs and MCOPs. The experiment results demonstrate that our proposed approach is very effective in terms of convergence and diversity.","2157-9563","978-1-4673-7679-2978-1-4673-7678","10.1109/ICNC.2015.7378012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378012","Multi-objective Evolutionary Algorithm;Repair Operator;Constrained Optimization","Maintenance engineering;Optimization;Evolutionary computation;Sociology;Statistics;Linear programming;Benchmark testing","evolutionary computation","opposition-based repair operator;multiobjective evolutionary algorithm;constrained optimization problems;MCOP;reversed correction strategy;MOEA/D algorithm;NSGA-II algorithm","","","31","","","","","","IEEE","IEEE Conferences"
"A 77-GHz software defined OFDM radar","C. Pfeffer; R. Feger; M. Jahn; A. Stelzer","Christian Doppler Laboratory for Integrated Radar Sensors, Institute for Communications Engineering and RF-Systems, Johannes Kepler University, 4040 Linz, Austria; Christian Doppler Laboratory for Integrated Radar Sensors, Institute for Communications Engineering and RF-Systems, Johannes Kepler University, 4040 Linz, Austria; Corporate Technology RFT-AT, Siemens AG Austria, Siemensstr. 90, 1210 Wien, Austria; Christian Doppler Laboratory for Integrated Radar Sensors, Institute for Communications Engineering and RF-Systems, Johannes Kepler University, 4040 Linz, Austria","2014 15th International Radar Symposium (IRS)","","2014","","","1","5","In this paper we present a 77-GHz software defined orthogonal frequency-division multiplexing (OFDM) radar. A single channel RF frontend equipped with a 77-GHz SiGe chip which includes an integrated IQ modulator in the transmit path and an IQ receive mixer is used for range and range/Doppler measurements. Digital-to-analog converters (DACs) with a sampling rate of 500MSPS and 14 bit resolution are used to generate OFDM symbols with 201 sub-carriers and a bandwidth of 200 MHz. Sampling of the received signal is realized by analog-to-digital converters which, like the DACs used to generate the TX signal, run at 500MSPS and deliver a resolution of 14 bit. The performance of the overall system was optimized by a peak-toaverage power-ratio reduction technique. A standard deviation of 12.4mm was achieved for range measurements which were carried out in an anechoic chamber. Furthermore, first test measurements demonstrate that the system can be applied to moving target scenarios.","2155-5745;2155-5753","978-617-607-470-0978-617-607-552-3978-617-607-554","10.1109/IRS.2014.6869235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6869235","OFDM radar;Radar applications","Peak to average power ratio;Doppler radar;Doppler effect;Modulation;Gain","digital-analogue conversion;millimetre wave integrated circuits;millimetre wave mixers;millimetre wave radar;OFDM modulation;software radio","software defined OFDM radar;orthogonal frequency division multiplexing radar;single channel RF front end;integrated IQ modulator;transmit path;IQ receive mixer;Doppler measurements;digital-analog converter;received signal sampling;moving target scenario;frequency 77 GHz;bandwidth 200 MHz;SiGe","","4","17","","","","","","IEEE","IEEE Conferences"
"Intelligent prediction of execution times","D. Tetzlaff; S. Glesner","Technische Universit&#x00E4;t Berlin, Chair Software Engineering for Embedded Systems; Technische Universit&#x00E4;t Berlin, Chair Software Engineering for Embedded Systems","2013 Second International Conference on Informatics & Applications (ICIA)","","2013","","","234","239","It is a major challenge in software engineering to statically analyze in advance the expectable run-time behavior of applications. The most needed information is the expected execution time of a function to determine its computational cost. In this paper, we present a sophisticated approach that solves this problem by utilizing Machine Learning (ML) techniques based on regression modeling to automatically derive precise predictions for this information. This enables to focus optimization efforts on the parts that are relevant for the resulting performance and to predict execution times of functions on different processing elements of a heterogeneous architecture. Among others, our approach eliminates the need for manual annotations of run-time information, which automates and facilitates the development of complex software, thus improving the software engineering process. For our experiments that demonstrate the accuracy of our approach, we have used a considerable number of programs from various benchmark suites which encompass different realworld application domains. This shows on the one hand the general applicability and on the other hand the high scalability of our ML techniques.","","978-1-4673-5256-7978-1-4673-5255","10.1109/ICoIA.2013.6650262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650262","","Benchmark testing;Prediction algorithms;Predictive models;Vectors;Training;Support vector machines;Feature extraction","learning (artificial intelligence);regression analysis;software engineering","intelligent execution times prediction;software engineering;applications expectable run-time behavior;machine learning techniques;ML techniques;regression modeling;heterogeneous architecture;run-time information annotation;software development;application domains","","3","19","","","","","","IEEE","IEEE Conferences"
"Modified Condition Decision Coverage: A Hardware Verification Perspective","M. A. Salem; K. I. Eder","NA; NA","2013 14th International Workshop on Microprocessor Test and Verification","","2013","","","8","13","Verification is a critical phase of the development cycle. It confirms the compliance of a design implementation with its functional specification. Coverage measures the progress of the verification plan. Structural coverage determines the code exercised by the functional tests. Modified Condition Decision Coverage (MC/DC) is a structural coverage type. This paper compiles a comprehensive overview of established MC/DC conventions, and develops novel MC/DC insights through conduction of experimental study for MC/DC in hardware verification. It provides a generic MC/DC overview while explaining MC/DC types, and criteria of MC/DC validation in the software domain. It introduces the motivation for adoption of MC/DC as a potential structural coverage type for hardware verification. The paper presents the experimental evaluation conducted over a diverse base of logic combinations. The introduced experimental results inferred distinct MC/DC insights. These insights present novel MC/DC aspects that optimize the minimal MC/DC coverage requirements, defines MC/DC compositionality concepts, and provide RTL design guidelines for MC/DC fulfillment.","1550-4093;2332-5674","978-1-4799-3246","10.1109/MTV.2013.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926093","","Logic gates;Observability;Hardware;Software;Vectors;Encoding;Controllability","hardware description languages;program testing;program verification","modified condition decision coverage;hardware verification;design implementation;functional specification;structural coverage types;functional tests;MC/DC validation criteria;software domain;logic combinations;minimal MC/DC coverage requirement optimization;MC/DC compositionality concepts;RTL design guidelines","","1","11","","","","","","IEEE","IEEE Conferences"
"Sensitivity analysis of a magnetic circuit for non-destructive testing by the magnetic flux leakage technique","C. F. Jaimes Saavedra; S. Roa Prada","Programa de Ingeniería Mecatrónica, Universidad Autónoma de Bucaramanga, Colombia; Programa de Ingeniería Mecatrónica, Universidad Autónoma de Bucaramanga, Colombia","2014 III International Congress of Engineering Mechatronics and Automation (CIIMA)","","2014","","","1","5","Monitoring the integrity of structures in the oil and gas industry is a mandatory task that helps preventing both natural disasters and economical losses. This paper presents the optimization of the geometry of a magnetic circuit for the detection of faults in a pipeline by the magnetic flux leakage method. The main goal of these efforts is to perform a sensitivity analysis on the geometrical parameters of the circuit to find configurations that improve the performance of the fault searching tool. The sensitivity analysis of the design parameters of the magnetic circuit is based on numerical evaluations of the performance of the tool using the finite element method. The commercial software selected for this analysis is COMSOL Multiphysics®. The results obtained in this investigation will identify the geometrical configurations that provide better performance, with respect to other configurations, for the detection of the same fault. The findings of this investigation can be utilized as guidelines for the design of magnetic circuits for non-destructive testing using the magnetic flux leakage technique.","","978-1-4799-7932-5978-1-4799-7931","10.1109/CIIMA.2014.6983447","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983447","Magnetic flux leakage;magnetic circuit;sensitivity analysis;finite elements","Magnetic circuits;Magnetic flux leakage;Geometry;Materials;Finite element analysis;Permanent magnets","finite element analysis;gas industry;magnetic flux;magnetic leakage;nondestructive testing;petroleum industry;production engineering computing","sensitivity analysis;magnetic circuit;nondestructive testing;magnetic flux leakage technique;oil industry;gas industry;natural disasters;economical losses;geometrical parameter;finite element method;COMSOL Multiphysics","","","9","","","","","","IEEE","IEEE Conferences"
"Optimization and Evaluation of an Arterial Thrombus Retrieval Device","A. Pollano; M. Suarez; A. Rivarola; A. Dominguez","NA; NA; NA; NA","IEEE Latin America Transactions","","2013","11","1","281","286","The aim of this work was the optimization of an arterial clot retrieval device made of Ni-Ti in the shape of a helical spring. Wire was obtained from GAC International Inc., a company dedicated to the development of products vinculated to orthodontic and orthopedic, and then employed for its development. Finite Element (FE) simulation was used to compare the effects in the clot when different shapes of springs, cylindrical and conical, are used. Abaqus 6.9 was the software chosen to develop those simulations. The assessment of the device using an in vivo model was attempted; an experimental protocol was developed, using the abdominal aorta of the rat for effective in vivo testing of the endovascular catheter-delivered device. Taking into account the results obtained with the FE analysis it was concluded that the utilization of conical shaped springs is better than cilindrical due to the lower stress caused in the clot in this case, that avoids its rupture. The in vivo intervention was successful, the generation of the clot in the aorthic artery of a rat was achieved and so was the extraction of it.","1548-0992","","10.1109/TLA.2013.6502817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6502817","finite element method;mechanical embolectomy;nitinol","In vivo;Materials;Optimization;Clamps;Springs;Shape;Finite element analysis","biomedical materials;blood;blood vessels;catheters;finite element analysis;mechanoception;medical computing;medical supplies;software tools;springs (mechanical);wires","aorthic artery;clot generation;conical shaped springs;FE analysis;endovascular catheter device;in vivo testing;rat abdominal aorta;in vivo model;Abaqus 6.9 software;conical wires;blood clot;finite element simulation;orthopedic;orthodontic;GAC International Inc;helical spring;Ni-Ti devices;arterial clot retrieval device optimization;Ni-Ti","","","","","","","","","IEEE","IEEE Journals & Magazines"
"MOPSO approach to solve profit based unit commitment problem (PBUCP)","C. Dhifaoui; T. Guesmi; H. H. Abdallah","Control &amp; Energies Management (CEM-Lab); National Engineering School of Sfax Tunisia; Control &amp; Energies Management (CEM-Lab), National Engineering School of Sfax Tunisia; Control &amp; Energies Management (CEM-Lab), National Engineering School of Sfax Tunisia","2015 4th International Conference on Systems and Control (ICSC)","","2015","","","175","182","In this paper a new intelligent technique named multi-objective particle swarm optimization (MOPSO) algorithm used to solve profit based unit commitment (PBUCP). The Profit Based Unit Commitment problem is a nonlinear multi-objective optimization problem which involves the simultaneous optimization to maximize the generation companies (GENCOs) profit. The first function is the revenue while the second is the total cost. This optimization involves many constraints such as system power and reserve, unit generation limit, unit minimum ON OFF duration and ramping constraints. The used technique has been tested on IEEE-39 bus system with ten generating units over 24-h time horizon. The simulation results obtained are compared without another technique. The algorithm and simulation are realized with MATLAB 7.4 software.","2379-0059;2379-0067","978-1-4799-8318-6978-1-4673-7108","10.1109/ICoSC.2015.7153301","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7153301","Price Based Unit Commitment Problem (PBUCP);Market price;Deregulated market;Economic dispatch;GENCO;multi-objective particle swarm optimization","Optimization;Linear programming;Schedules;Production;Simulation;Power systems;Energy management","particle swarm optimisation;power generation scheduling;power markets","MOPSO approach;profit based unit commitment problem;PBUCP;intelligent technique;multiobjective particle swarm optimization;nonlinear multiobjective optimization problem;simultaneous optimization;generation companies;GENCO;unit generation limit;constraints;IEEE-39 bus system;MATLAB 7.4 software","","","15","","","","","","IEEE","IEEE Conferences"
"Design and optimization of a low-voltage shunt capacitive RF-MEMS switch","M. L. Ya; N. Soin; A. N. Nordin","Department of Electrical Engineering, Faculty of Engineering, University of Malaya, 50603 Kuala Lumpur, Malaysia; Department of Electrical Engineering, Faculty of Engineering, University of Malaya, 50603 Kuala Lumpur, Malaysia; Department of Electrical and Computer Engineering, International Islamic University Malaysia, 53100 Kuala Lumpur, Malaysia","2014 Symposium on Design, Test, Integration and Packaging of MEMS/MOEMS (DTIP)","","2014","","","1","6","This paper presents the design, optimization and simulation of a radio frequency (RF) micro-electromechanical system (MEMS) switch. The device is a capacitive shunt-connection switch, which uses four folded beams to support a big membrane above the signal transmission line. Another four straight beams provide the bias voltage. The switch is designed in 0.35μm complementary metal oxide semiconductor (CMOS) process and is electrostatically actuated by a low pull-in voltage of 2.9V. Taguchi Method is employed to optimize the geometric parameters of the beams, in order to obtain a low spring constant and a robust design. The pull-in voltage, vertical displacement, and maximum von Mises stress distribution was simulated using finite element modeling (FEM) simulation - IntelliSuite v8.7<sup>®</sup>software. With Pareto ANOVA technique, the percentage contribution of each geometric parameter to the spring constant and stress distribution was calculated; and then the optimized parameters were got as t=0.877μm, w=4μm, L1=40μm, L2=50μm and L3=70μm. RF performance of the switch was simulated by AWR Design Environment 10<sup>®</sup>and yielded isolation and insertion loss of -23dB and -9.2dB respectively at 55GHz.","","978-2-35500-027-0978-2-35500-028-7978-2-35500-029","10.1109/DTIP.2014.7056645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056645","RF switch;low-voltage;MEMS;capacitive;Taguchi Method","Switches;Springs;Stress;Radio frequency;Switching circuits;Optimization;Micromechanical devices","beams (structures);CMOS integrated circuits;finite element analysis;microswitches;Pareto distribution;Taguchi methods;yield stress","low-voltage shunt capacitive RF-MEMS switch design;low-voltage shunt capacitive RF-MEMS switch optimization;radiofrequency microelectromechanical system switch;capacitive shunt-connection switch;four-folded beams;signal transmission line;bias voltage;complementary metal oxide semiconductor process;CMOS process;electrostatic actuation;pull-in voltage;Taguchi method;beam geometric parameters;low-spring constant;robust design;vertical displacement;maximum von Mises stress distribution;finite element modeling simulation;FEM simulation;IntelliSuite v8.7 software;Pareto ANOVA technique;percentage contribution;geometric parameter;stress distribution;AWR Design Environment 10;yielded isolation;insertion loss;size 0.35 mum;voltage 2.9 V;frequency 55 GHz;loss -23 dB;loss -9.2 dB","","1","12","","","","","","IEEE","IEEE Conferences"
"Remote Invalidation: Optimizing the Critical Path of Memory Transactions","A. Hassan; R. Palmieri; B. Ravindran","NA; NA; NA","2014 IEEE 28th International Parallel and Distributed Processing Symposium","","2014","","","187","197","Software Transactional Memory (STM) systems are increasingly emerging as a promising alternative to traditional locking algorithms for implementing generic concurrent applications. To achieve generality, STM systems incur overheads to the normal sequential execution path, including those due to spin locking, validation (or invalidation), and commit/abort routines. We propose a new STM algorithm called Remote Invalidation (or RInval) that reduces these overheads and improves STM performance. RInval's main idea is to execute commit and invalidation routines on remote server threads that run on dedicated cores, and use cache-aligned communication between application's transactional threads and the server routines. By remote execution of commit and invalidation routines and cache-aligned communication, RInval reduces the overhead of spin locking and cache misses on shared locks. By running commit and invalidation on separate cores, they become independent of each other, increasing commit concurrency. We implemented RInval in the Rochester STM framework. Our experimental studies on micro-benchmarks and the STAMP benchmark reveal that RInval outperforms InvalSTM, the corresponding non-remote invalidation algorithm, by as much as an order of magnitude. Additionally, RInval obtains competitive performance to validation-based STM algorithms such as NOrec, yielding up to 2x performance improvement.","1530-2075","978-1-4799-3800-1978-1-4799-3799","10.1109/IPDPS.2014.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877254","Software Transactional Memory;Remote Invalidation;Synchronization","Servers;Synchronization;Benchmark testing;Instruction sets;Concurrent computing;Scalability;Hardware","cache storage;concurrency control;multi-threading;software performance evaluation","remote invalidation;critical path optimization;memory transactions;software transactional memory systems;concurrent applications;sequential execution path;spin locking;overhead reduction;STM performance improvement;invalidation routines;remote server threads;cache-aligned communication;transactional threads;remote execution;commit routines;commit concurrency;RInval;Rochester STM framework;STAMP benchmark;microbenchmarks;validation-based STM algorithms","","2","23","","","","","","IEEE","IEEE Conferences"
"Automatic optimization of thread-coarsening for graphics processors","A. Magni; C. Dubach; M. O'Boyle","School of Informatics, University of Edinburgh, United Kingdom; School of Informatics, University of Edinburgh, United Kingdom; School of Informatics, University of Edinburgh, United Kingdom","2014 23rd International Conference on Parallel Architecture and Compilation Techniques (PACT)","","2014","","","455","466","OpenCL has been designed to achieve functional portability across multi-core devices from different vendors. However, the lack of a single cross-target optimizing compiler severely limits performance portability of OpenCL programs. Programmers need to manually tune applications for each specific device, preventing effective portability. We target a compiler transformation specific for data-parallel languages: thread-coarsening and show it can improve performance across different GPU devices. We then address the problem of selecting the best value for the coarsening factor parameter, i.e., deciding how many threads to merge together. We experimentally show that this is a hard problem to solve: good configurations are difficult to find and naive coarsening in fact leads to substantial slowdowns. We propose a solution based on a machine-learning model that predicts the best coarsening factor using kernel-function static features. The model automatically specializes to the different architectures considered. We evaluate our approach on 17 benchmarks on four devices: two Nvidia GPUs and two different generations of AMD GPUs. Using our technique, we achieve speedups between 1.11× and 1.33× on average.","","978-1-4503-2809-8978-1-5090-6607","10.1145/2628071.2628087","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7855921","","Benchmark testing;Instruction sets;Graphics processing units;Kernel;Performance evaluation;Hardware","graphics processing units;learning (artificial intelligence);multiprocessing programs;optimisation;parallel languages;program compilers;software portability","automatic optimization;thread-coarsening;graphics processors;functional portability;multicore devices;cross-target optimizing compiler;OpenCL programs;data-parallel languages;machine learning;kernel-function static features;Nvidia GPU;AMD GPU","","6","35","","","","","","IEEE","IEEE Conferences"
"C++ library for fuzzy type-2 controller design with particle swarm optimization tuning","F. E. Serrano; M. A. Flores","Energy Division, Physics Department, Honduras National Autonomous University, Tegucigalpa, Honduras, Central America USA; Energy Division, Physics Department, Honduras National Autonomous University, Tegucigalpa, Honduras, Central America","2015 IEEE Thirty Fifth Central American and Panama Convention (CONCAPAN XXXV)","","2015","","","1","7","In this article a C++ library for fuzzy type-2 controller with particle swarm optimization tuning is proposed. The main objective of this library is to provide a tool for researchers, professors and students in the automatic control field to design fuzzy type-2 controllers for nonlinear systems. This library contains functions for standard fuzzy type-2 controllers and PID fuzzy type-2 controllers and their variations (PI and PD) with a main class that defines the fuzzy type-2 membership functions shapes and operations along with the ordinary differential equations solver ODE and particle swarm optimization technique routines for the tuning of the fuzzy type-2 PID, PD and PI controllers respectively. Due to the standard form of these controllers is not necessary to test the stability of the closed loop system, but the stability is assured as long as the automatic control designer can establishes the right membership function parameters and rules or use the automatic tuning by particle swarm optimization. The automatic tuning by particle swarm optimization (PSO) is designed to tune the membership function parameters so this project can be extended to tune the rules of the fuzzy type-2 controller. This library was programmed in the C++ compiler GNU GCC along with the mathematical programming language GNU OCTAVE for GNU LINUX, this library is completely open source and it can be modified, distributed and improved by anyone.","","978-1-4673-7872-7978-1-4673-7871","10.1109/CONCAPAN.2015.7428449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7428449","type-2 fuzzy logic systems. type reduction;type-2 fuzzy sets;uncertainties;type-2 fuzzy sets","Libraries;Particle swarm optimization;Tuning;PD control;Nonlinear systems;Differential equations","C++ language;control engineering computing;control system synthesis;differential equations;fuzzy control;nonlinear control systems;particle swarm optimisation;PD control;software libraries;stability;three-term control","C++ library;fuzzy type-2 controller design;particle swarm optimization tuning;automatic control field;nonlinear systems;standard fuzzy type-2 controllers;PID fuzzy type-2 controllers;fuzzy type-2 membership functions;ordinary differential equations solver;ODE;fuzzy type-2 PID;PD controllers;PI controllers;stability;automatic tuning;PSO;C++ compiler GNU GCC;mathematical programming language;GNU OCTAVE;GNU LINUX","","1","14","","","","","","IEEE","IEEE Conferences"
"Loop nests parallelization for digital system synthesis","A. Chemeris; J. Gorunova; D. Lazorenko","Pukhov Institute for Modelling in Energy Engineering NASU, Kiev, Ukraine; Pukhov Institute for Modelling in Energy Engineering NASU, Kiev, Ukraine; Pukhov Institute for Modelling in Energy Engineering NASU, Kiev, Ukraine","East-West Design & Test Symposium (EWDTS 2013)","","2013","","","1","4","A modified method of affine transformations of nested loops is offered. The point of modification is in processing of group of loop operators without data dependencies. The method offered allows to formalize the parallelization process and increase quality of parallelization. As a result efficiency of synthesis of digital circuits increased.","","978-1-4799-2096-9978-1-4799-2095","10.1109/EWDTS.2013.6673180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6673180","","Optimization;Vectors;Equations;Digital circuits;Linear programming;Software packages;Calculators","affine transforms;digital circuits;electronic design automation;network synthesis","loop nest parallelization;affine transformation;digital circuit synthesis","","","5","","","","","","IEEE","IEEE Conferences"
"Polyhedral Optimizations of Explicitly Parallel Programs","P. Chatarasi; J. Shirako; V. Sarkar","NA; NA; NA","2015 International Conference on Parallel Architecture and Compilation (PACT)","","2015","","","213","226","The polyhedral model is a powerful algebraic framework that has enabled significant advances to analysis and transformation of sequential affine (sub)programs, relative to traditional AST-based approaches. However, given the rapid growth of parallel software, there is a need for increased attention to using polyhedral frameworks to optimize explicitly parallel programs. An interesting side effect of supporting explicitly parallel programs is that doing so can also enable optimization of programs with unanalyzable data accesses within a polyhedral framework. In this paper, we address the problem of extending polyhedral frameworks to enable analysis and transformation of programs that contain both explicit parallelism and unanalyzable data accesses. As a first step, we focus on OpenMP loop parallelism and task parallelism, including task dependences from OpenMP 4.0. Our approach first enables conservative dependence analysis of a given region of code. Next, we identify happens-before relations from the explicitly parallel constructs, such as tasks and parallel loops, and intersect them with the conservative dependences. Finally, the resulting set of dependences is passed on to a polyhedral optimizer, such as PLuTo and PolyAST, to enable transformation of explicitly parallel programs with unanalyzable data accesses. We evaluate our approach using eleven OpenMP benchmark programs from the KASTORS and Rodinia benchmark suites. We show that 1) these benchmarks contain unanalyzable data accesses that prevent polyhedral frameworks from performing exact dependence analysis, 2) explicit parallelism can help mitigate the imprecision, and 3) polyhedral transformations with the resulting dependences can further improve the performance of the manually-parallelized OpenMP benchmarks. Our experimental results show geometric mean performance improvements of 1.62x and 2.75x on the Intel Westmere and IBM Power8 platforms respectively (relative to the original OpenMP versions).","1089-795X","978-1-4673-9524","10.1109/PACT.2015.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429307","Explicit parallelism;Polyhedral transformations;Task parallelism;OpenMP;Happens-before relations","Parallel processing;Benchmark testing;Arrays;Schedules;Kernel;Jacobian matrices;Pluto","message passing;optimising compilers;parallel programming;program diagnostics","polyhedral optimizations;parallel programs;polyhedral model;algebraic framework;sequential affine subprograms;parallel software;polyhedral frameworks;program analysis;program transformation;explicit parallelism;OpenMP loop parallelism;task parallelism;task dependences;OpenMP 4.0;parallel loops;PLuTo;PolyAST;OpenMP benchmark programs;KASTORS;Rodinia benchmark;dependence analysis;polyhedral transformations;happens-before relations;optimizing compilers","","5","36","","","","","","IEEE","IEEE Conferences"
"Flow-based feasibility test of linear interference alignment with arbitrary interference topology","P. Wan; F. Al-Dhelaan; S. Ji; L. Wang; O. Frieder","Department of Computer Science, Illinois Institute of Technology; Department of Computer Science, Illinois Institute of Technology; College of Computer And Software, Nanjing University of Information Science And Technology; School of Software, Dalian University of Technology; Department of Computer Science, Georgetown University","2015 IEEE Conference on Computer Communications (INFOCOM)","","2015","","","1526","1534","Linear interference alignment (LIA) is one of the key interference mitigation techniques to enhance the wireless MIMO network capacity. The generic LIA feasibility amounts to whether or not a well-structured random matrix with entries drawn from a continuous distribution has full row-rank almost surely. Recently, a randomized algebraic test of feasibility was proposed in the literature. It is a pseudo-polynomial bounded-error probabilistic algorithm in nature, and has intrinsic limitations of requiring an inordinate amount of running time and memory even for a moderate sized input and being prone to round-off errors in floating-point computations. This paper presents necessary conditions and sufficient conditions of the generic LIA feasibility and develops fast and robust tests of them based on network flow. In certain settings, these conditions are both necessary and sufficient, and their flow-based tests yield efficient algorithm for feasibility test.","0743-166X","978-1-4799-8381","10.1109/INFOCOM.2015.7218531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7218531","","Interference;Topology;Network topology;MIMO;Computers;Conferences;Wireless communication","MIMO communication;polynomials;radiofrequency interference;telecommunication network topology","flow based feasibility test;linear interference alignment;arbitrary interference topology;LIA;key interference mitigation techniques;wireless MIMO network capacity;randomized algebraic test;pseudo polynomial bounded error probabilistic algorithm;floating-point computations","","","29","","","","","","IEEE","IEEE Conferences"
"Integrated cross-layer solutions for enabling silicon photonics into future chip multiprocessors","P. Grani; S. Bartolini; E. Furdiani; L. Ramini; D. Bertozzi","Department of Information Engineering and Mathematical Sciences, University of Siena, via Roma 56, 53100, Italy; Department of Information Engineering and Mathematical Sciences, University of Siena, via Roma 56, 53100, Italy; Engineering Department, University of Ferrara, Via Saragat 1, 44122, Italy; Engineering Department, University of Ferrara, Via Saragat 1, 44122, Italy; Engineering Department, University of Ferrara, Via Saragat 1, 44122, Italy","19th Annual International Mixed-Signals, Sensors, and Systems Test Workshop Proceedings","","2014","","","1","8","Nanophotonic is a promising solution for interconnections in future chip multiprocessors (CMPs) due to its intrinsic low-latency and low-power features. This paper proposes an integrated approach with physical level design choices to select the most suitable optical network topology, and an adhoc software strategy to improve performance and reduce energy consumption of a tiled CMP architecture. We adopt an all-optical reconfigurable network which has been designed to significantly reduce path-setup latency and energy consumption. Specifically the optimization aims at distributing the traffic into the Network on Chip (NoC) in such a way to limit resurce usage conflicts (during path-setups) and have a more uniform utilization of the fast optical resources. On-chip photonics indeed is the key enabler for such a strategy permitting to reach even far destinations with a reduced latency, the same as the closest ones. We investigate performance/power consumption effects on a CMP system and we compare against both a high-performance electronic folded Torus NoC and the standard optical reconfigurable architecture. The optical network improves 7% on average over the electronic counterpart and, especially when using the dedicated software optimization for matching application locality and network features, it reaches about 26% average execution time improvement.","","978-1-4799-6540","10.1109/IMS3TW.2014.6997403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6997403","","Optical switches;Optimization;Optical fiber networks;Optical losses;Propagation losses;Topology;Software","elemental semiconductors;low-power electronics;multiprocessor interconnection networks;nanophotonics;network topology;network-on-chip;reconfigurable architectures;silicon","integrated cross-layer solutions;silicon photonics;chip multiprocessor interconnections;nanophotonics;low-latency features;low-power features;physical level design;optical network topology;adhoc software;energy consumption;tiled CMP architecture;all-optical reconfigurable network;path-setup latency;network on chip;optical resources;on-chip photonics;high-performance electronic folded Torus NoC;optical reconfigurable architecture;software optimization;Si","","1","23","","","","","","IEEE","IEEE Conferences"
"High Resolution Pulse Propagation Driven Trojan Detection in Digital Logic: Optimization Algorithms and Infrastructure","S. Deyati; B. J. Muldrey; A. Singh; A. Chatterjee","NA; NA; NA; NA","2014 IEEE 23rd Asian Test Symposium","","2014","","","200","205","Insertion of malicious Trojans into outsourced chip manufacturing generally results in increased capacitances of internal circuit nodes that have been tapped for node controllability and observability by malicious circuitry. Current path delay measurement and side channel Trojan detection techniques are unable to detect Trojans that present low loading to such tapped circuit nodes, especially in the presence of large manufacturing process variations. In this paper, a high-resolution Trojan detection method for digital logic based on pulse propagation is developed. The method exhibits 25X -- 30X higher diagnostic resolution (ability to measure small capacitive loads on internal circuit nodes) as compared to current path delay based Trojan detection techniques in the presence of significant manufacturing process variations. Further, a key benefit is that theoretically, as opposed to path delay measurement based methods, the diagnostic resolution of the test approach is independent of circuit logic depth over and above the benefits already mentioned above. Test methods and test infrastructure compatible with existing scan based techniques are described. Simulation results are presented to prove the viability and effectiveness of the proposed Trojan detection scheme and especially for circuits with large logic depths (35-70 gates) suffering from worst case process variation effects.","1081-7735;2377-5386","978-1-4799-6030","10.1109/ATS.2014.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6979100","Hardware Security;Hardware Trojan Detection;Hardware Intrusion Detection","Trojan horses;Logic gates;Delays;Capacitance;Transistors;Vectors;Flip-flops","invasive software;logic design;logic testing","worst case process variation effects;diagnostic resolution;manufacturing process variations;capacitive loads;pulse propagation;digital logic;high-resolution Trojan detection method;side channel Trojan detection techniques;path delay measurement;malicious circuitry;observability;node controllability;internal circuit nodes;outsourced chip manufacturing;malicious Trojans insertion","","5","19","","","","","","IEEE","IEEE Conferences"
"A new method of solving the unit commitment problem","X. Liu","Department of Systems Engineering, University of Arkansas at Little Rock, 72204, USA","2013 IEEE Power & Energy Society General Meeting","","2013","","","1","5","Unit commitment (UC) is a fundamental problem in power systems and has been extensively investigated by many researchers. However, almost all existing approaches are based on integer decision variables. In this paper, we propose an alternative approach, in which the continuous variables (real output power) are the only decision variables. This approach is based on a simple theoretical result, for which a proof is given. Then a new model is presented, followed by several 24-dimensional and 96-dimensional test cases. The results are consistent with those solved by conventional approaches reported in the literature.","1932-5517","978-1-4799-1303","10.1109/PESMG.2013.6672177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6672177","Generator scheduling;integer programming;optimization;unit commitment","Optimization;Mathematical model;Power systems;Economics;Computer aided software engineering;Numerical models;Particle swarm optimization","dynamic programming;integer programming;power generation scheduling","unit commitment problem;power systems;integer decision variables;continuous variables;decision variables;96-dimensional test cases;24-dimensional test cases;dynamic programming;Lagrangian relaxation;integer programming","","","11","","","","","","IEEE","IEEE Conferences"
"Optimal power flow using group search optimizer with intraspecific competition and lévy walk","Y. Z. Li; M. S. Li; Z. Ji; Q. H. Wu","School of Electrical Engineering, South China University of Technology (SCUT), Guangzhou 510641, China; College of Computer Science and Software Engineering, Shenzhen University, 518060, China; School of Electrical Engineering, South China University of Technology (SCUT), Guangzhou 510641, China; Department Electrical Engineering and Electronics, University of Liverpool, Liverpool, L69 3GJ, U.K.","2013 IEEE Symposium on Swarm Intelligence (SIS)","","2013","","","256","262","This paper presents an enhanced group search optimizer (GSO), group search optimizer with intraspecific competition and lévy walk (GSOICLW), to solve the optimal power flow (OPF) problem. GSOICLW s a more biologically realistic algorithm and performs better balance between global and local searching than GSO n hat intraspecific competition IC) and lévy walk (LW) are introduced o GSO. GSOICLW is tested or the OPF problem on the IEEE 30-bus power system, with green house gases emission constraint considered. Simulation results demonstrate the accuracy and reliability of the proposed algorithm, compared with other evolutionary algorithms EAs).","","978-1-4673-6004","10.1109/SIS.2013.6615187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615187","Optimal power flow;group search optimizer intraspecific competition;lévy walk;evolutionary algorithms","IEEE 802.3 Standards;EPON","air pollution;evolutionary computation;optimisation;power system analysis computing","optimal power flow;group search optimizer;intraspecific competition;Lévy walk;GSOICLW;OPF;IEEE 30-bus power system;green house gases emission constraint;evolutionary algorithms","","","27","","","","","","IEEE","IEEE Conferences"
"Reliable Spectrum Sensing and Opportunistic Access in Network-Coded Communications","A. Fanous; Y. E. Sagduyu; A. Ephremides","Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, 20742, USA; Intelligent Automation Inc., Rockville, MD, 20855, USA; Department of Electrical and Computer Engineering, University of Maryland, College Park, MD, 20742, USA","IEEE Journal on Selected Areas in Communications","","2014","32","3","400","410","We consider the problem of reliable spectrum sensing and opportunistic access on channels with stochastic traffic in batch processing systems such as network coding (NC). We show how a secondary user (SU) can leverage the structure induced by block-based NC on primary users' (PUs) channels to mitigate the effects of channel sensing errors and improve the detection of idle PU spectrum and the throughput. NC is known to improve the transmission efficiency and therefore when applied on a PU channel, it can extend the spectrum availability for the SUs. We refer to the additional gain of spectrum predictability from NC and show that under possible sensing errors the SU can more reliably detect the idle spectrum if the PUs' channels carry network-coded transmissions even when the channel utilization is fixed. We consider two different objectives at the SU. For quickest detection, the SU applies the Cumulative Summation (CUSUM) algorithm to detect idle slots on a PU channel and further improves the detection capability with the Viterbi algorithm, if the PU spectrum dynamics are known. For throughput maximization, the SU tracks the PU spectrum with the Partially Observable Markov Decision Process (POMDP) approach. Our results show that NC renders the spectrum more predictable, which can be used by the SUs to mitigate the effects of sensing errors and improve the throughput. We validate these results with real radio measurements taken in software-defined radio based wireless network tests.","0733-8716;1558-0008","","10.1109/JSAC.201.031403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678825","Spectrum access;spectrum sensing;network coding;throughput optimization;quickest detection;POMDP;cognitive radio network;software-defined radio","Sensors;Throughput;Heuristic algorithms;Availability;Viterbi algorithm;Correlation","cognitive radio;network coding;optimisation;radio spectrum management;software radio;telecommunication network reliability;telecommunication traffic;wireless channels","software defined radio based wireless network tests;radio measurements;partially observable Markov decision process;throughput maximization;Viterbi algorithm;detection capability;cumulative summation algorithm;channel utilization;spectrum predictability;transmission efficiency;channel sensing errors;primary users channels;block based NC;secondary user;network coding;batch processing systems;stochastic traffic;network coded communications;opportunistic access;reliable spectrum sensing","","14","25","","","","","","IEEE","IEEE Journals & Magazines"
"Remote engineering solutions for industrial maintenance","A. Kolesnikow; R. Behrens; C. Westerkamp; H. Kremer; M. Rafrafi; N. Colin","University of Applied Sciences Osnabrueck, 49076 Osnabr&#x00FC;ck, Germany; University of Applied Sciences Osnabrueck, 49076 Osnabr&#x00FC;ck, Germany; University of Applied Sciences Osnabrueck, 49076 Osnabr&#x00FC;ck, Germany; University of Applied Sciences Osnabrueck, 49076 Osnabr&#x00FC;ck, Germany; Testia / NDT EXPERT, 31025 Toulouse, France; EADS Innovation Works, 31025 Toulouse, France","2013 11th IEEE International Conference on Industrial Informatics (INDIN)","","2013","","","488","493","This paper presents the NDT (Non Destructive Testing) concept and process in combination with the OMA system (Online Maintenance Assistance) for remote service support. The system is used to support complex maintenance processes by assistance from remote experts in an online video conference with integration to inspection workflow and devices. The system operates on Commercial of the Shelf (COTS) hardware and in standard Web Browsers. It allows flexible integration into the enterprise IT e.g. in existing role and authentication concepts and is compliant to standard enterprise security regulations.","1935-4576;2378-363X","978-1-4799-0752","10.1109/INDIN.2013.6622933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622933","Non Destructive Testing;NDT;Online Maintenance Assistance;Remote Control;Workflow Support;Ultrasonic Testing;Maintenance;Audio;Video;Remote Service;Expert Guidance;Workflow Optimisation","Maintenance engineering;Inspection;Acoustics;Software;Aircraft;Streaming media;Context","inspection;maintenance engineering;nondestructive testing;production engineering computing;teleconferencing;video communication;Web services","remote engineering solutions;industrial maintenance;NDT;nondestructive testing;OMA system;online maintenance assistance;remote service support;online video conference;inspection workflow;commercial of the shelf hardware;Web browsers;security regulations;authentication concepts;COTS","","3","25","","","","","","IEEE","IEEE Conferences"
"Unlocking the buried margins entrenched in test data","L. V. Kirkland","WesTest Engineering Corp., A RPS HOLDINGS company, 810 Shepard Lane, Farmington, Utah 84025, United States of America","2015 IEEE AUTOTESTCON","","2015","","","433","439","What information is critical in test and diagnosis? Limits for measurements are certainly a critical aspect of the diagnosis of a Unit Under Test (UUT) fault. Specific measurement deviations can be extremely telling when it comes to potential Unit Under Test (UUT) problems which can have an adverse consequence of lowering the UUT Mean Time Between Failure (MTBF). Even though the measurement might be within tolerance, borderline or out-of-the-normal median range readings tend to be an indication of a future unit problem. A potential future problem in the UUT can be evident by unlocking specific measurement data and making the appropriate information known to the user. Limit parameters are always critical during the test and diagnosis process. Any UUT will fail or show a weakness at some point. There is usually a point between a hard functional failure and a degraded function or pending failure. The UUT will fail, it is just a question of when. The effect of the failure or change in performance interferes in different degrees with missions that depend on that system. In critical missions a system failure can lead to catastrophic consequences. Significant changes in the performance of equipment may occur as a result of aging and other factors within systems and their associated support equipment. Current methods do not allow for the effects of these factors in the determination of equipment performance tolerances or test limits, resulting in apparent and actual decreases in equipment readiness and test program precision. The overall cost of a failure or malfunction, measured in any standard, is always higher than the preventive action. Therefore, system users are interested in knowing when a system or part is about to fail in order to take preemptive actions. Diagnoses in general are complicated, but the test results coming from Fiddly, Fuzzy, Finicky (F<sup>3</sup>) failure events offer an extra layer of confusion. F<sup>3</sup> failures could be classified as a natural sequence of events that are eventually going to happen. On the other hand, it should be stated that some failures never occur even though there is a potential they might occur. Moreover, the test program set (TPS) or built-in test procedures anticipate only catastrophic and stable faults that will always result in a predictable manner<sup>2</sup>. It would not be feasible for TPS developers to plan for exotic failure modes that would result from F<sup>3</sup> events. As a result repair technicians are often left in a state of confusion about test results in general and diagnostic help in particular. It is important to know that test technology is not limited in its extent and growth. New hardware and software techniques are continually evolving and we must pursue optimal repair by utilizing saved test data to increase the MTBF and the overall reliability of the UUT. This paper will be a discussion of evaluating test data with the intent to add additional sets of limits so as to provide the repair technician with varied choices depending on the actual measurement for specific UUT tests. The System Test Specifications provide the Upper Limit and the Lower Limit for each measurement. These limits must be followed, but additional limits can be employed which provide the repairer with additional information which can optimize the repair process.","","978-1-4799-8190-8978-1-4799-8189","10.1109/AUTEST.2015.7356529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7356529","","Measurement units;Visualization;TV","fault diagnosis;fault tolerant computing;preventive maintenance","UUT fault diagnosis;unit under test;mean time between failure;MTBF;limit parameter;equipment performance tolerance;test program precision;preemptive action;test program set;TPS;hardware technique;software technique;repair process optimization","","","2","","","","","","IEEE","IEEE Conferences"
"Enabling Prioritized Cloud I/O Service in Hadoop Distributed File System","T. Yeh; Y. Sun","NA; NA","2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)","","2014","","","256","259","Cloud computing has become more and more popular nowadays. Both governments and enterprises provide service through the construction of public and private clouds accordingly. Among the platforms used in cloud computing, Hadoop is considered one of the most practical and stable systems. Nevertheless, as with other regular software, Hadoop still needs to rely on the underlying operating system to communicate with hardware to function appropriately. For modern computer systems, CPUs excessively outrun hard drives (hard disks). The computer hard disk has become a major bottleneck to the overall system performance. Consequently, computer programs can execute faster if their corresponding I/O operation can be completed sooner. This is important in particular when we want to expedite the execution of urgent programs in a busy system. Unfortunately, under the current Hadoop environment, users cannot prioritize operation of disk and memory for programs which they would like them to run faster. With the help of prioritized I/O service we developed earlier, we proposed and implemented a Hadoop environment with the ability of providing prioritized I/O service. Our Hadoop environment could accelerate the execution of programs with high priority assigned by users. We evaluated our design by executing prioritized programs in environments with different busy levels. Experimental results show that programs can improve their performance by up to 33.79% if executed with high priority.","","978-1-4799-6123-8978-1-4799-6122","10.1109/HPCC.2014.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056748","cloud computing;Hadoop;HDFS","Cloud computing;Computers;Writing;Operating systems;Benchmark testing;Conferences","cloud computing;data handling;distributed databases;network operating systems;parallel processing","Hadoop distributed file system;cloud computing;CPU;computer hard disk;operating system","","","17","","","","","","IEEE","IEEE Conferences"
"Fast energy evaluation of embedded applications for many-core systems","F. Rosa; L. Ost; T. Raupp; F. Moraes; R. Reis","UFRGS - Instituto de Inform&#x00E1;tica - PGMicro/PPGC, Av. Bento Gon&#x00E7;alves 9500 Porto Alegre, RS - Brazil; LIRMM - 161 rue Ada, Cedex 05 - 34095 Montpellier, France; FACIN-PUCRS - Av. Ipiranga 6681- 90619-900, Porto Alegre, Brazil; FACIN-PUCRS - Av. Ipiranga 6681- 90619-900, Porto Alegre, Brazil; UFRGS - Instituto de Inform&#x00E1;tica - PGMicro/PPGC, Av. Bento Gon&#x00E7;alves 9500 Porto Alegre, RS - Brazil","2014 24th International Workshop on Power and Timing Modeling, Optimization and Simulation (PATMOS)","","2014","","","1","6","The growing concerns of energy efficiency and performance scalability motivate research in the area of many-core embedded systems. The software development of such systems plays an important role on the system performance, while accounting for a significant part of the total energy consumption. Thus, it becomes imperative to consider the software energy consumption at early stages of the software development. This paper proposes an instruction-driven energy analysis approach that provides an accurate and practical way of evaluating software energy cost at the speed of up to 1.8 MIPS. Results show that the accuracy of our approach varies from 0.06% to 8.05% when compared to a gate-level implementation.","","978-1-4799-5412","10.1109/PATMOS.2014.6951893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6951893","Instruction-driven energy model;fast and accurate energy evaluation;JIT-based simulation;OVP","Software;Logic gates;Computational modeling;Benchmark testing;Energy consumption;Analytical models;Accuracy","embedded systems;energy consumption;multiprocessing systems;power aware computing;software engineering","many-core embedded systems;instruction-driven energy analysis;software energy cost;gate-level implementation","","9","16","","","","","","IEEE","IEEE Conferences"
"An Integrated Hardware-Software Approach to Task Graph Management","N. Engelhardt; T. Dallou; A. Elhossini; B. Juurlink","NA; NA; NA; NA","2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)","","2014","","","392","399","Task-based parallel programming models with explicit data dependencies, such as OmpSs, are gaining popularity, due to the ease of describing parallel algorithms with complex and irregular dependency patterns. These advantages, however, come at a steep cost of runtime overhead incurred by dynamic dependency resolution. Hardware support for task management has been proposed in previous work as a possible solution. We present VSs, a runtime library for the OmpSs programming model that integrates the Nexus++ hardware task manager, and evaluate the performance of the VSs-Nexus++ system. Experimental results show that applications with fine-grain tasks can achieve speedups of up to 3.4×, while applications optimized for current runtimes attain 1.3×. Providing support for hardware task managers in runtime libraries is therefore a viable approach to improve the performance of OmpSs applications.","","978-1-4799-6123-8978-1-4799-6122","10.1109/HPCC.2014.66","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056770","OmpSs;parallel programming models;task dataflow;hardware task scheduler;runtime library","Runtime;Hardware;Benchmark testing;Runtime library;Multicore processing;Programming;Field programmable gate arrays","graph theory;parallel algorithms;parallel programming;software performance evaluation","integrated hardware-software approach;task graph management;task-based parallel programming model;data dependency;parallel algorithm;irregular dependency pattern;runtime overhead;dynamic dependency resolution;task management;OmpSs programming model;Nexus++ hardware task manager;performance evaluation;VSs-Nexus++ system;runtime library","","5","17","","","","","","IEEE","IEEE Conferences"
"A basic linear algebra compiler for embedded processors","N. Kyrtatas; D. G. Spampinato; M. Püschel","Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","1054","1059","Many applications in signal processing, control, and graphics on embedded devices require efficient linear algebra computations. On general-purpose computers, program generators have proven useful to produce such code, or important building blocks, automatically. An example is LGen, a compiler for basic linear algebra computations of fixed size. In this work, we extend LGen towards the embedded domain using as example targets Intel Atom, ARM Cortex-A8, ARM Cortex-A9, and ARM1176 (Raspberry Pi). To efficiently support these processors we introduce support for the NEON vector ISA and a methodology for domain-specific load/store optimizations. Our experimental evaluation shows that the new version of LGen produces code that performs in many cases considerably better than well-established, commercial and non-commercial libraries (Intel MKL and IPP), software generators (Eigen and ATLAS), and compilers (icc, gcc, and clang).","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092545","","Program processors;Linear algebra;Optimization;Linux;Libraries;Arrays","embedded systems;linear algebra;program compilers;software libraries","linear algebra compiler;embedded processors;LGen;Intel Atom;ARM Cortex-A8;ARM Cortex-A9;ARM1176;domain-specific load optimizations;domain-specific store optimizations;noncommercial libraries;software generators","","1","21","","","","","","IEEE","IEEE Conferences"
"The evaluation of the results of an eye tracking based usability tests of the so called Instructor's Portal framework (http://tanitlap.ektf.hu/csernaiz)","C. K. Prantner","Department of Human Informatics, Eszterházy Károly College, Eger, Hungary","2015 6th IEEE International Conference on Cognitive Infocommunications (CogInfoCom)","","2015","","","459","465","The research discussed in this paper can be positioned at the cross-section of Applied Computer Science, Didactics and Human-Computer Interaction. Accordingly, an Instructor's Portal (IPo) framework system was developed at the Department of Human Informatics of the Eszterházy Károly College (EKC) in 2015. The aim of the framework system is to enable instructors working in higher education institutions to establish, customize, and update their own webpages independently without any help from informatics professionals. Said system not only fulfills a gap filling function in the higher education sphere, but performs complex tasks while serving a wide range of users. In order to establish a logically arranged content structure and user interface the respective development process observed several developmental principles, methods, and web-ergonomic rules. This paper introduces the results of usability tests and examinations pertaining to the system. The examination utilized, an eye-tracking hardware device and a mouse movement recording software along with a special software facilitating the evaluation and presentation of the respective results. This essay introduces and highlight how the respective apparatus complements the traditional human observation-based usability tests while identifying the cognitive skills to be ascertained with such devices, one of the main priorities of Cognitive Infocommunications (CogInfoCom).","","978-1-4673-8129-1978-1-4673-8128","10.1109/CogInfoCom.2015.7390637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390637","eye tracking;usability test;ergonomics;instruction portal;higher education","Usability;Testing;Gaze tracking;Portals;Informatics;Hardware","computer aided instruction;educational institutions;further education;interactive devices;portals;user interfaces","eye tracking based usability tests;instructors portal framework;applied computer science;didactics;human computer interaction;IPo framework system;higher education institutions;Web page;gap filling function;logically arranged content structure;user interface;eye-tracking hardware device;mouse movement recording software;cognitive infocommunications;CogInfoCom","","","20","","","","","","IEEE","IEEE Conferences"
"Coordinated closed-loop voltage control by using a real-time Volt/VAR Optimization function for MV distribution Networks","S. Rahimi; S. Massucco; F. Silvestro","ABB Enterprise Software, Vasteras, Sweden; UNIGE, University of Genova, Italy; UNIGE, University of Genova, Italy","2015 IEEE 15th International Conference on Environment and Electrical Engineering (EEEIC)","","2015","","","1222","1228","Advanced Distribution Management Systems (DMS) functions in general and Volt/Var Optimization (VVO) function in particular are forefront tools for reliable and economic operation of distribution Networks. By using real time calculation results from VVO, there is a potential for closed-loop voltage control and sending set-points to tap changers and capacitor banks. The major challenge for such closed-loop voltage control is the possibility of larger computation time for optimization problem and also delays associated to the two-way communication platform between the VVO engine and field through the SCADA system. The objective of this paper is to develop the idea of closed-loop voltage control by utilizing a VVO engine with a full mixed integer linear programming (MILP) model for solving optimization problem for real-time and planning applications. Optimization problems are implemented in GAMS and have been tested on a real small MV distribution network.","","978-1-4799-7993-6978-1-4799-7992","10.1109/EEEIC.2015.7165343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165343","Voltage Control;Distribution systems;Mixed Integer Linear Programming (MILP);Volt/VAR Optimization (VVO)","Voltage control;Optimization;Loading;Real-time systems;Reactive power;Planning;Load modeling","capacitor storage;closed loop systems;integer programming;linear programming;power distribution control;SCADA systems;voltage control","GAMS;general algebraic modeling system;MILP model;mixed integer linear programming;SCADA system;VVO engine;two-way communication platform;capacitor banks;tap changers;economic operation;DMS functions;distribution management systems;MV distribution networks;VVO function;volt-VAR optimization function;coordinated closed-loop voltage control","","1","21","","","","","","IEEE","IEEE Conferences"
"Knowledge Based Engineering to support electric and electronic system design and automatic control software development","F. Tian; M. Voskuijl","Faculty of Aerospace Engineering, Delft University of Technology, the Netherlands; Faculty of Aerospace Engineering, Delft University of Technology, the Netherlands","2013 IEEE/AIAA 32nd Digital Avionics Systems Conference (DASC)","","2013","","","7A4-1","7A4-9","The level of software integration in vehicles, such as electric cars and aircraft, is rapidly increasing. Due to the increasing complexity of the embedded control software, significant delays can occur in development programs or even errors can be present in the control software of the final product. In the development of the electric and electronic system (E/E system), the analysis and specification for the architecture of the logical system, technical system and software itself includes many repetitive (manual) processes. Those repetitive processes are time consuming and are prone to errors. This research proposes new methods and tools that allow the designer to take electronic components, including control software, into account already in the conceptual design stage of complex systems. These new methods are based upon the principles of Knowledge Based Engineering (KBE), which is essentially a combination of computer aided design (CAD) and artificial intelligence (AI). The proposed methods can establish the relationship from the logical system architecture to the technical system architecture and finally the software components. Moreover, the proposed tools can model the logical, technical architecture and automatically generate the software components. The software language GDL, which is particularly suitable for the representation of complex systems and the development of KBE applications, has been used to develop the tools. The development of an Anti-lock Braking System (ABS) for a novel electric vehicle configuration has been chosen as test case. Based on a single intelligent product model, that contains the main design parameters of the vehicle specified by the designer, two models are generated automatically; (1) the simulation model of the physical plant and the associated control system, and (2) the control software. For a specific vehicle configuration, the simulation model can be used to test the control system and to optimize the parameters of the control system. In the case of an ABS, a braking maneuver is simulated. Next, the software components are generated automatically. The simulation model is used to test the software components for a range of conditions. The results show that the of the software components are automatically updated when the physical plant of the E/E systems or top level overall design changes. The final source code is well-structured and easy to understand due to the fact that there is a direct relation between the vehicle design parameters specified in the original product model and the variables and their values in the data model of the software components. The proposed design methods and tools can in principle be applied to any dynamic system with a high level of software integration, such as e.g. unmanned aerial vehicles.","2155-7195;2155-7209","978-1-4799-1538-5978-1-4799-1536","10.1109/DASC.2013.6712633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6712633","","Software;Data models;Vehicles;Wheels;Solid modeling;Control systems;Computer architecture","braking;CAD;electric vehicles;electrical engineering computing;knowledge engineering","knowledge based engineering;electric and electronic system design;automatic control software development;software integration;electric cars;aircraft;E/E system;electronic components;complex systems;KBE;computer aided design;CAD;artificial intelligence;AI;logical system architecture;technical system architecture;software language GDL;antilock braking system;ABS;electric vehicle configuration;single intelligent product model;associated control system;braking maneuver;source code;unmanned aerial vehicles","","1","10","","","","","","IEEE","IEEE Conferences"
"Improving the life cycle management of power transformers transforming data to life","N. T. Waugh; D. D. Muir","Faculty of Engineering and Computing, University of Technology, Jamaica; Faculty of Engineering and Computing, University of Technology, Jamaica","SoutheastCon 2015","","2015","","","1","7","Power transformers are essential in the transmission of alternating current and are the most costly equipment in the substation. Although their failure rate is relatively low (less than two percent), the in-service failure of a power transformer can be very catastrophic. The restoration cost of in-service power transformer failures increases by up to seventy-five percent and loss of revenue by sixty percent. It is therefore prudent for utility companies to optimize the life of power transformers and mitigate in-service failures. “Early Detection Saves Lives”, a slogan associated with the fight against cancer is also very relevant to the life cycle management of power transformers. The health of the power transformer is reflected through its insulation strength. There are several on-line and offline tests used to assess the health of power transformer. These give rise to an enormous amount of data to be analyzed. Many utilities manually analyze these results and only focus on the most recent test results. This method of analysis is time consuming and is subject to human error. It abets the back-log of testing and not correcting developing faults in a timely manner. There are benefits that can be gained from using a computer program to store, analyze and trend power transformer data, as well as predict and monitor developing power transformer faults. Software are available to test and monitor power transformer oil via online devices; while other maintenance software analyze single electrical field tests. There is the potential to gain greater benefits from having one computer program that incorporates oil and electrical field tests results, as well as the age, loading and inspection data of the power transformers to determine their health status or condition index. This paper outlines the proposed architecture and features of a custom built Power Transformer Maintenance Software (PTMS). PTMS facilitates a paradigm shift from reactive maintenance (such as practiced at the Jamaica Public Service Company Limited) to a predictive maintenance approach. This study proposes the use of a customized integrated data management system to improve the life cycle management of power transformers.","1558-058X;1091-0050","978-1-4673-7300","10.1109/SECON.2015.7132977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7132977","Power transformers;life cycle management;diagnostic software;failure;oil tests;electrical field tests","Power transformers;Market research;Computers;Oil insulation;Propagation losses;Maintenance engineering;Companies","maintenance engineering;power system faults;power transformer insulation;product life cycle management;substations;transformer oil","life cycle management;power transformers;substation equipment;power transformer in-service failure;insulation strength;power transformer faults;electrical field tests;power transformer maintenance software;PTMS;Jamaica Public Service Company Limited;integrated data management system","","3","9","","","","","","IEEE","IEEE Conferences"
"Threshold-free code clone detection for a large-scale heterogeneous Java repository","I. Keivanloo; F. Zhang; Y. Zou","Department of Electrical and Computer Engineering, Queen's University, Kingston, Ontario, Canada; School of Computing, Queen's University, Kingston, Ontario, Canada; Department of Electrical and Computer Engineering, Queen's University, Kingston, Ontario, Canada","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","","2015","","","201","210","Code clones are unavoidable entities in software ecosystems. A variety of clone-detection algorithms are available for finding code clones. For Type-3 clone detection at method granularity (i.e., similar methods with changes in statements), dissimilarity threshold is one of the possible configuration parameters. Existing approaches use a single threshold to detect Type-3 clones across a repository. However, our study shows that to detect Type-3 clones at method granularity on a large-scale heterogeneous repository, multiple thresholds are often required. We find that the performance of clone detection improves if selecting different thresholds for various groups of clones in a heterogeneous repository (i.e., various applications). In this paper, we propose a threshold-free approach to detect Type-3 clones at method granularity across a large number of applications. Our approach uses an unsupervised learning algorithm, i.e., k-means, to determine true and false clones. We use a clone benchmark with 330,840 tagged clones from 24,824 open source Java projects for our study. We observe that our approach improves the performance significantly by 12% in terms of F-measure. Furthermore, our threshold-free approach eliminates the concern of practitioners about possible misconfiguration of Type-3 clone detection tools.","1534-5351","978-1-4799-8469","10.1109/SANER.2015.7081830","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081830","clone detection;clone search;clustering;unsupervised learning;large-scale repository;threshold-free","Cloning;Benchmark testing;Clustering algorithms;Java;Software systems;Google;Optimization methods","Java;public domain software","threshold-free code clone detection algorithms;heterogeneous Java repository;software ecosystems;method granularity;dissimilarity threshold;clone benchmark;open source Java projects;F-measure;type-3 clone detection tools","","8","37","","","","","","IEEE","IEEE Conferences"
"Improved batch elimination: A fast algorithm to identify and remove harmful compiler optimizations","E. Daniel de Lima; A. Faustino da Silva","Departament of Informatic, State University of Maringá, Maringá, Paraná, Brazil; Departament of Informatic, State University of Maringá, Maringá, Parana, Brazil","2015 Latin American Computing Conference (CLEI)","","2015","","","1","8","Modern compilers provide several optimizations that can be applied to the source code, in order to increase its performance. Due to the complex relationship between various optimizations, discovering harmful compiler optimizations is a problem in the context of compilers. Strategies based on iterative compilation try to solve this problem evaluating the performance of the compiled program using different sets. In this context, Combined Elimination is an efficient iterative compilation strategy. The purpose of Combined Elimination is to identify the harmful optimizations and remove them in an iterative compilation process. Combined Elimination provides good results, which are close to those founded by an exhaustive search approach. However, its drawback is the number of program runs. In this paper, we proposed an iterative compilation algorithm, named Improved Batch Elimination. This algorithm is based on the first step towards Combined Elimination, the Batch Elimination algorithm. The goal of Improved Batch Elimination is to produce results similar to Combined Elimination, with a complexity similar to Batch Elimination. In other words, the goal is to produce good results and to be faster than Combined Elimination. We evaluate our algorithm by measuring the performance of Spec Cpu2006, Polybench and cBench benchmarks under a set of LLVM compiler optimizations. The results indicate that Improved Batch Elimination is a good strategy to remove harmful compiler optimizations, using few program runs.","","978-1-4673-9143-6978-1-4673-9142","10.1109/CLEI.2015.7360010","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360010","","","benchmark testing;computational complexity;iterative methods;program compilers;search problems;source code (software)","improved batch elimination;harmful compiler optimization;source code;performance evaluation;iterative compilation strategy;iterative compilation process;exhaustive search approach;iterative compilation algorithm;Batch Elimination algorithm;CBENCH benchmarks;POLYBENCH benchmarks;SPEC CPU2006 benchmarks;LLVM compiler optimizations","","","33","","","","","","IEEE","IEEE Conferences"
"New electro-thermally actuated micromanipulator with optimized design and FEM simulations analyses","R. Voicu; R. Muller","Simulation, Modelling and Computer-Aided Design Laboratory National Institute for R&amp;D in Microtechnologies, IMT-Bucharest Bucharest, Romania; Simulation, Modelling and Computer-Aided Design Laboratory National Institute for R&amp;D in Microtechnologies, IMT-Bucharest Bucharest, Romania","2013 Symposium on Design, Test, Integration and Packaging of MEMS/MOEMS (DTIP)","","2013","","","1","6","This paper presents the designs for a new configuration of an electro-thermally actuated SU-8 polymeric micro-manipulator. The electro-thermally driven micromanipulators were studied using computer simulations, based on finite element method (FEM) performed with CoventorWare software tool. Numerical coupled electro-thermomechanical and transient electro-thermal simulations were performed. The simulation results were used to analyze the statically and dynamical behaviour of the micromanipulator. We investigated the influence of the temperature achieved in the arms and the `in plane' deflections of the tips, as function of the applied voltage. The numerical investigations show a good mechanical behaviour of the micromanipulators, so that they are suitable to operate in air and in liquid. We realize a comparison between the results and found the optimal configuration. Preliminary results of the fabrication of a SU-8 micromanipulator structure are presented, using surface micromachining technique.","","978-2-35500-026-3978-1-4673-4477-7978-2-35500-024","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6559451","Key words;micromanipulator;electro-thermal actuator;FEM simulation;SU-8 polymer","Micromanipulators;Polymers;Finite element analysis;Computational modeling;Atmospheric modeling;Gold;Analytical models","control engineering computing;finite element analysis;mechanical engineering computing;micromachining;micromanipulators;software tools","electro-thermally actuated micromanipulator;optimized design;FEM simulations analyses;electro-thermally actuated SU-8 polymeric micro-manipulator;electro-thermally driven micromanipulators;computer simulations;finite element method;CoventorWare software tool;numerical coupled electro-thermomechanical simulation;transient electro-thermal simulations;numerical investigations;mechanical behaviour;optimal configuration;SU-8 micromanipulator structure;surface micromachining technique","","","10","","","","","","IEEE","IEEE Conferences"
"Fundamental study of outer-rotor hybrid excitation flux switching generator for grid connected wind turbine applications","E. B. Sulaiman; A. M. Arab","Dept. of Electrical Power Engineering, Faculty of Electrical and Electronic Engineering, UniversitiTun Hussein Onn Malaysia, Johor, Malaysia; Dept. of Electrical Power Engineering, Faculty of Electrical and Electronic Engineering, UniversitiTun Hussein Onn Malaysia, Johor, Malaysia","2015 IEEE Student Conference on Research and Development (SCOReD)","","2015","","","716","720","This paper presents an operating principle of a new proposed outer-rotor hybrid excitation flux switching generator. In this Generator a combination of a permanent magnet (PM) and field excitation coil (FEC) are used as the main flux sources. Additional FEC can be used to control the flux so that constant voltage can be produced at various wind conditions. Moreover, twelve coil test analysis, Three Phase coil test flux excited by PM only, Back-EMF at various speed and stack-length conditions, magnetic flux strengthening at various current densities and flux distributions are investigated by using JMAG software. The result shows that the generated voltage is directly proportional with the change of speed and stack-length. Also parametric optimization was conducted using deterministic optimization method (DOM) and the improved design indicated a higher output voltage.","","978-1-4673-9572-4978-1-4673-9571","10.1109/SCORED.2015.7449431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7449431","flux switching generator;permenant magnet;field excitation coil;electromotive force;outer-rotor","Generators;Rotors;Current density;Magnetic flux;Stators;Couplings;Wind speed","magnetic flux;optimisation;permanent magnet generators;power grids;rotors;wind turbines","outer-rotor hybrid excitation flux switching generator;grid connected wind turbine;permanent magnet;field excitation coil;coil test analysis;back-EMF;magnetic flux strengthening;JMAG software;deterministic optimization method;parametric optimization","","","11","","","","","","IEEE","IEEE Conferences"
"Understanding Performance Portability of OpenACC for Supercomputers","S. Sawadsitang; J. Lin; S. See; F. Bodin; S. Matsuoka","NA; NA; NA; NA; NA","2015 IEEE International Parallel and Distributed Processing Symposium Workshop","","2015","","","699","707","Scientific applications need to be moved among supercomputers, such as Tianhe-2 and TSUBAME 2.5. OpenACC provides a directive-based approach for a single source code base with function portability across different accelerators used in the supercomputers. However, the performance portability is not guaranteed by the OpenACC standard. Therefore, we propose a systematic optimization method, instead of auto-tuning by compliers, to achieve reasonable portable performance with minor code modifications. With this method, we evaluate the four kernels from Rodin a benchmark suite and one mini-application Hydro on our hybrid ""CPU+GPU+MIC"" supercomputer À with the CAPS and PGI compilers. We analyze Parallel Thread Execution (PTX) codes to further understand the performance portability, and find CAPS adopts a different strategy from PGI in thread distribution. The evaluation results show the optimized OpenACC versions can archive a better performance portability ratio than the OpenCL version in some cases. The understanding and the method are valuable for OpenACC application developers to efficiently and correctly use the available OpenACC compilers.","","978-1-4673-7684","10.1109/IPDPSW.2015.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284377","Performance Portability;OpenACC;OpenCL;GPU;MIC","Graphics processing units;Microwave integrated circuits;Optimization;Kernel;Supercomputers;Instruction sets;Standards","application program interfaces;benchmark testing;multi-threading;parallel machines;program compilers;software performance evaluation;software portability;source code (software)","scientific applications;Tianhe-2;TSUBAME 2.5;directive-based approach;function portability;OpenACC standard;systematic optimization method;Rodinia benchmark suite;miniapplication Hydro;CPU+GPU+MIC supercomputer π;PGI compilers;CAPS compilers;parallel thread execution codes;PTX codes;thread distribution;performance portability ratio;OpenACC compilers","","1","13","","","","","","IEEE","IEEE Conferences"
"Numerical analysis and optimization of divertor cooling system","A. Khodak; M. A. Jaworski","Princeton Plasma Physics Laboratory, Princeton, New Jersey, USA; Princeton Plasma Physics Laboratory, Princeton, New Jersey, USA","2013 IEEE 25th Symposium on Fusion Engineering (SOFE)","","2013","","","1","6","Novel divertor cooling system concept is currently under development at Princeton Plasma Physics Laboratory (PPPL). This concept utilizes supercritical carbon dioxide as a coolant for the liquid lithium filled porous divertor front plate. Coolant is flowing in closed loop in the T tube type channel. Application of CO<sub>2</sub> eliminates safety concerns associated with water cooling of liquid lithium systems, and promises higher overall efficiency compared to systems using He as a coolant Numerical analysis of divertor system initial configuration was performed using ANSYS software. Initially conjugated heat transfer problem was solved involving computational fluid dynamics (CFD) simulation of the coolant flow, and heat transfer in the coolant and solid regions of the cooling system. Redlich Kwong real gas model was used for equation of state of supercritical CO<sub>2</sub> together with temperature and pressure dependent transport properties. Porous region filled with liquid lithium was modeled as a solid body with liquid lithium properties. Evaporation of liquid lithium from the front face was included via special temperature dependent boundary condition. Results of CFD and heat transfer analysis were used as external conditions for structural analysis of the system components. Simulations were performed within ANSYS Workbench framework using ANSYS CFX for conjugated heat transfer and CFD analysis, and ANSYS Mechanical for structural analysis. Initial results were obtained using simplified 2D model of the cooling system. 2D model allowed direct comparison with previous cooling concepts which use He as a coolant. Optimization of the channel geometry in 2D allowed increase in efficiency of the cooling system by reducing the total pressure drop in the coolant flow. Optimized geometrical parameters were used to create a 3D model of the cooling system which eventually can be implemented and tested experimentally. 3D numerical simulation will be used to validate design variants of the divertor cooling system.","1078-8891;2155-9953","978-1-4799-0171-5978-1-4799-0169","10.1109/SOFE.2013.6635485","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6635485","divertor;cooling system;lithium;numerical simulations computational fluid dynamics","Numerical models;Lithium;Coolants;Mathematical model;Heat transfer;Temperature","computational fluid dynamics;fusion reactor divertors;fusion reactor safety;numerical analysis;plasma pressure;plasma temperature;plasma transport processes","supercritical CO<sub>2</sub>;temperature dependent transport property;pressure dependent transport property;porous region filled liquid lithium;solid body;liquid lithium properties;liquid lithium evaporation;special temperature dependent boundary condition;heat transfer analysis;external conditions;structural analysis;system components;ANSYS workbench framework;ANSYS CFX;conjugated heat transfer;CFD analysis;structural analysis;simplified 2D model;He coolant;2D channel geometry optimization;total pressure drop;coolant flow;optimized geometrical parameters;3D model;3D numerical simulation;design variant validation;equation-of-state;real gas model;solid region;coolant region;heat transfer;coolant flow;CFD simulation;computational fluid dynamic simulation;initially conjugated heat transfer problem;ANSYS software;divertor system initial configuration;He coolant systems;liquid lithium systems;water cooling;safety concern elimination;CO<sub>2</sub> application;T tube type channel;liquid lithium filled porous divertor front plate coolant;supercritical carbon dioxide;PPPL development;Princeton Plasma Physics Laboratory development;divertor cooling system concept;numerical optimization;numerical analysis","","1","8","","","","","","IEEE","IEEE Conferences"
"Coordinated voltage control algorithms tested in real time digital simulator","J. Tuominen; S. Repo; A. Kulmala","Department of Electrical Engineering, Tampere University of Technology, Tampere, Finland; Department of Electrical Engineering, Tampere University of Technology, Tampere, Finland; Department of Electrical Engineering, Tampere University of Technology, Tampere, Finland","2014 Power Systems Computation Conference","","2014","","","1","7","To restrict voltage rise problems and to increase the hosting capacity of distribution network for distributed generation, active voltage control methods have been developed. Two coordinated voltage control algorithms previously created at Tampere University of Technology have been tested and compared by the real time software-in-the-loop tests. The paper also proposes test sequences and metrics to be used in the functional and performance testing of coordinated voltage control algorithms. In this article results from real time digital simulator tests of these algorithms are presented and discussed and suggestions for the future development of the algorithms are also given.","","978-83-935801-3","10.1109/PSCC.2014.7038471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7038471","Active Voltage Control;Coordinated Voltage Control;Optimization;RTDS;Software-in-the-loop","Measurement;Production;Reactive power;Automatic voltage control;Algorithm design and analysis;Real-time systems","digital simulation;distributed power generation;power distribution control;power generation control;voltage control","coordinated active voltage control algorithm;real time digital simulator;distribution network;distributed generation;Tampere University of Technology;real time software-in-the-loop test","","1","13","","","","","","IEEE","IEEE Conferences"
"Programming Support for Speculative Execution with Software Transactional Memory","M. Feng; R. Gupta; I. Neamtiu","NA; NA; NA","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum","","2013","","","394","403","In this paper, we identify the practical issues that deter the adoption of software transactional memory (STM) for speculation in real applications. These issues include dealing with excessive instrumentation added by naive identification of potential shared accesses, functions that may be called from both transactional and non-transactional contexts, and calls to functions for which the source is unavailable. We address these challenges and provide an approach for developing speculatively-executed code in C/C++ that offers superior programmability and performance. Our contributions consist of a set of programming constructs for writing speculatively-executed code and a compiler that translates code annotated with these constructs into speculatively-executable code that uses STM runtime libraries. Our approach uses annotations that simply mark the boundaries of the code that is to be executed speculatively and supports calling precompiled (e.g., C standard library) and irreversible (e.g., I/O operations and system calls) functions from within transactions. We employ a series of important optimizations for reducing the overhead of speculative execution, including: placement of read/write barriers only for accesses that actually can cause a data race; elimination of redundant read/write barriers by caching shared variables; and eliminating unnecessary search in the write buffer. We evaluate the programmability of our constructs and the performance of our compiler implementation using eight STAMP benchmarks and two real applications-the Velvet genomic assembler and the ITI decision tree constructor. Compared to Intel's STM compiler, our approach requires 91% fewer constructs to be inserted by the programmer, yet it achieves 20.8% better performance.","","978-0-7695-4979-8978-0-7695-4979","10.1109/IPDPSW.2013.194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650911","parallel programming;speculation;compiler","Libraries;Programming;Program processors;Instruments;Benchmark testing;Cloning;Writing","C++ language;cache storage;concurrency control;decision trees;parallel programming;program assemblers;program compilers;software libraries;software performance evaluation","Intel's STM compiler;ITI decision tree constructor;Velvet genomic assembler;STAMP benchmarks;compiler implementation performance;write buffer;shared variable caching;redundant read-write barrier elimination;data race;speculative execution overhead;precompiled functions;irreversible functions;STM runtime libraries;annotated code translation;speculatively-executed code writing;programming constructs;programmability;C-C++;speculatively-executed code;software transactional memory","","","26","","","","","","IEEE","IEEE Conferences"
"Determination of cyclic mechanical properties of thin copper layers for PCB applications","K. Fellner; P. F. Fuchs; T. Antretter; G. Pinter; R. Schöngrundner","Polymer Competence Center Leoben GmbH, 8700, Austria; Polymer Competence Center Leoben GmbH, 8700, Austria; Institute of Mechanics, Montanuniversitaet Leoben, Austria; Institute of Materials Science and Testing of Polymers, Montanuniversitaet Leoben, Austria; Materials Center Leoben Forschung GmbH, 8700, Austria","2014 15th International Conference on Thermal, Mechanical and Mulit-Physics Simulation and Experiments in Microelectronics and Microsystems (EuroSimE)","","2014","","","1","7","The overall objective of this research work is the characterization of the mechanical behavior of Printed Circuit Boards (PCBs) under cyclic thermal loads. The conducting traces in PCBs are made from thin copper layers in an etching process. Hence, thin copper layers are characterized experimentally and subsequently cyclic material parameters are determined. The experimental characterization is conducted using cyclic tensile-compression tests at different temperatures and loading conditions. For these tests composite specimens made of five layers of copper and four layers of glass fiber reinforced epoxy resin are used. The obtained material response is modeled using the “Nonlinear isotropic/kinematic hardening model” built-in in the Finite Element Analysis-software Abaqus. For every loading case the optimal set of parameters is determined using an optimization procedure. Based on the known parameter sets of the individual loading cases the calibration of a “Nonlinear isotropic/kinematic hardening model” for all R-ratios and temperatures is attempted and the findings are discussed.","","978-1-4799-4790-4978-1-4799-4791","10.1109/EuroSimE.2014.6813788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6813788","","Load modeling;Abstracts;Strain;ISO standards","copper alloys;etching;finite element analysis;glass fibre reinforced composites;printed circuits;tensile testing;thermal engineering","cyclic mechanical property determination;thin copper layers;PCB applications;printed circuit boards;cyclic thermal loads;etching process;cyclic material parameters;cyclic tensile-compression tests;loading conditions;test composite specimens;glass fiber reinforced epoxy resin;nonlinear isotropic-kinematic hardening model;finite element analysis-software Abaqus;optimization procedure;R-ratios;Cu","","4","16","","","","","","IEEE","IEEE Conferences"
"Computing opposition by involving entire population","S. Rahnamayan; J. Jesuthasan; F. Bourennani; H. Salehinejad; G. F. Naterer","Department of Electrical, Computer, and Software Engineering, University of Ontario Institute of Technology, 2000 Simcoe Street North, Oshawa, ON L1H 7K4, Canada; Electrical and Computer Engineering Department, University of Waterloo, 200 University Avenue West, Waterloo, ON N2L 3G1, Canada; Department of Electrical, Computer, and Software Engineering, University of Ontario Institute of Technology, 2000 Simcoe Street North, Oshawa, ON L1H 7K4, Canada; Department of Electrical, Computer, and Software Engineering, University of Ontario Institute of Technology, 2000 Simcoe Street North, Oshawa, ON L1H 7K4, Canada; Faculty of Engineering and Applied Science, Memorial University of Newfoundland, St. John's, Canada","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","1800","1807","The capabilities of evolutionary algorithms (EAs) in solving nonlinear and non-convex optimization problems are significant. Among the many types of methods, differential evolution (DE) is an effective population-based stochastic algorithm, which has emerged as very competitive. Since its inception in 1995, many variants of DE to improve the performance of its predecessor have been introduced. In this context, opposition-based differential evolution (ODE) established a novel concept in which, each individual must compete with its opposite in terms of the fitness value in order to make an entry in the next generation. The generation of opposite points is based on the population's current extreme points (i.e., maximum and minimum) in the search space; these extreme points are not proper representatives for whole population, compared to centroid point which is inclusive regarding all individuals in the population. This paper develops a new scheme that utilizes the centroid point of a population to calculate opposite individuals. Therefore, the classical scheme of an opposite point is modified accordingly. Incorporating this new scheme into ODE leads to an enhanced ODE that is identified as centroid opposition-based differential evolution (CODE). The performance of the CODE algorithm is comprehensively evaluated on well-known complex benchmark functions and compared with the performance of conventional DE, ODE, and some other state-of-the-art algorithms (such as SaDE, ADE, SDE, and jDE) in terms of solution accuracy. The results for CODE are promising.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900329","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900329","","Sociology;Statistics;Vectors;Optimization;Benchmark testing;Accuracy;Convergence","concave programming;evolutionary computation;nonlinear programming","CODE algorithm;opposition-based differential evolution;fitness value;population-based stochastic algorithm;DE;differential evolution;nonconvex optimization problems;nonlinear optimization problems;EA;evolutionary algorithms","","7","33","","","","","","IEEE","IEEE Conferences"
"Fault Localization for Make-Based Build Crashes","J. Al-Kofahi; H. V. Nguyen; T. N. Nguyen","NA; NA; NA","2014 IEEE International Conference on Software Maintenance and Evolution","","2014","","","526","530","In large-scale software projects, build code has a high level of complexity, churn rate, and defect proneness. While it is desirable to have automated tools to help developers in localizing faults in build code, it is challenging to build such tools due to the dynamic nature of build code. Existing automatic fault localization methods focus on traditional code and none of them has such support for build code. This paper introduces MkFault, a novel automatic tool/method to localize faults in build code that cause run-time build failures. Given a test case that causes a run-time crash in the execution of a Make file, it returns a ranked list of statements in the Make file with their suspiciousness scores. MkFault records the evaluation traces from Make code to identify the corresponding concrete build rules and the execution traces of those rules. It then uses those traces and its novel Bayesian-like rating algorithm to give suspiciousness scores to the original statements in the Make file. Our empirical evaluation on real faults in several open-source projects has shown that MkFault can achieve high accuracy and help reduce a large percentage of the lines of code that developers need to examine.","1063-6773","978-1-4799-6146","10.1109/ICSME.2014.87","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976131","fault localization;build crashes;build code;build failure","Computer crashes;Concrete;Accuracy;Java;Complexity theory;Software;Instruments","Bayes methods;fault diagnosis;program debugging;project management;public domain software;software development management","make-based build crashes;large-scale software projects;churn rate;defect proneness;automated tools;automatic fault localization methods;MkFault;run-time build failures;Makefile;suspiciousness scores;Make code;Bayesian-like rating algorithm;open-source projects","","2","29","","","","","","IEEE","IEEE Conferences"
"Performance and Power Estimation for Mobile-Cloud Applications on Virtualized Platforms","S. Hung; F. Liang; C. Tu; N. Chang","NA; NA; NA; NA","2013 Seventh International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing","","2013","","","260","267","Performance optimization and energy-saving have become major challenges for both hardware designers and software developers of mobile systems due to limited battery capacities. Analysis of the performance and power consumption in such a system and employment of energy-saving techniques as early as possible are keys to competitive products designs. With virtual platforms powered by simulation/emulation engines to help hardware designers in the early design stages for verifying the functional correctness of a hardware design or testing software applications before the hardware is ready, we propose a framework to further support hardware/software co-design with advanced performance and power analysis facilities. Our case studies and experimental results show that our framework can reflect the performance and power behavior of mobile-cloud applications and help hardware/software designers in reducing the energy consumption on these systems.","","978-0-7695-4974","10.1109/IMIS.2013.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603683","","Hardware;Software;Computational modeling;Mathematical model;Power demand;Timing;Performance evaluation","cloud computing;energy conservation;hardware-software codesign;mobile computing;power consumption;virtual reality","power estimation;performance optimization;hardware designers;software developers;mobile systems;limited battery capacities;power consumption;energy-saving techniques;competitive products designs;virtual platforms;simulation/emulation engines;hardware design functional correctness;software applications testing;hardware/software codesign;power analysis facilities;power behavior;mobile-cloud applications;energy consumption","","1","20","","","","","","IEEE","IEEE Conferences"
"Hardware/Software Codesign Guidelines for System on Chip FPGA-Based Sensorless AC Drive Applications","I. Bahri; L. Idkhajine; E. Monmasson; M. El Amine Benkhelifa","Laboratoire de G&#x00E9;nie &#x00E9;lectrique de Paris (LGEP), University of Paris-Sud XI, Orsay, France; Syst&#x00E8;mes et Applications des Technologies de l'Information et de l'Energie (SATIE), University of Cergy-Pontoise, Cergy-Pontoise, France; Syst&#x00E8;mes et Applications des Technologies de l'Information et de l'Energie (SATIE), University of Cergy-Pontoise, Cergy-Pontoise, France; Equipes Traitement de l'Information et Syst&#x00E8;mes (ETIS), University of Cergy-Pontoise, Cergy-Pontoise, France benkhelifa@ensea.fr","IEEE Transactions on Industrial Informatics","","2013","9","4","2165","2176","This paper aims to provide Hardware/Software (Hw/Sw) codesign guidelines for system-on-chip field-programmable gate array-based sensorless ac drive applications. Among these guidelines, an efficient Hw/Sw partitioning procedure is presented. This Hw/Sw partitioning is performed taking into account both the control requirements (bandwidth and stability margin) and the architectural constraints (e.g., available area, memory, and hardware multipliers). A nondominated sorting genetic algorithm (NSGA-II) is used to solve the corresponding multi-objective optimization problem. The proposed Hw/Sw partitioning approach is then validated on a sensorless control algorithm for a synchronous motor based on an extended Kalman filter. Among the nondominated implementation solutions supplied by the NSGA-II, those that are considered as the most interesting are synthesized. Their time/area performances after synthesis are compared with success to their predictions. In addition, one of these optimal solutions is also tested on an experimental setup.","1551-3203;1941-0050","","10.1109/TII.2013.2245908","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459009","Codesign guidelines;extended Kalman filter (EKF);field-programmable gate array (FPGA);hardware/software (Hw/Sw) partitioning;nondominated sorting genetic algorithm (NSGA-II);soft-core processor;system-on-chip (SoC)","Algorithm design and analysis;Sensorless control;Genetic algorithms;System-on-a-chip;Field programmable gate arrays;Kalman filters","control engineering computing;field programmable gate arrays;genetic algorithms;hardware-software codesign;Kalman filters;power engineering computing;sensorless machine control;synchronous motor drives;system-on-chip","hardware-software codesign guidelines;system-on-chip FPGA;FPGA-based sensorless AC drive applications;field programmable gate array;HW-SW partitioning procedure;control requirements;bandwidth requirement;stability margin requirement;architectural constraints;available area constraint;memory constraint;hardware multipliers constraint;nondominated sorting genetic algorithm;NSGA-II;multiobjective optimization problem;sensorless control algorithm;synchronous motor;extended Kalman filter;time-area performance","","36","20","","","","","","IEEE","IEEE Journals & Magazines"
"Optimization of the stop-and-go- and dual-mode-algorithms for real-time baseband transmission","S. Meyer","Communication Systems, Software Defined Radio, Fraunhofer Institute for Communication, Information Processing and Ergonomics, Wachtberg, Germany","2014 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)","","2014","","","137","142","This paper presents two optimized blind equalization algorithms, developed on a real-time digital signal processor (DSP) test bed. The most commonly used group of blind channel equalizers are those based on the Bussgang algorithm. By combining a joint algorithm (JA) with a dual-mode (DM) algorithm or a stop-and-go (SAG) algorithm exists the option for optimal matching to the channel behavior. The performance of a single algorithm like modified decision directed modulus (MDDMA), modified constant modulus (MCMA) or the modified decision directed (MDDA) algorithm can only be adapted by adjusting the step size for updating the equalizer tap vector or by changing the number of equalizer taps. Using SAG or DM algorithm introduces a flag (SAG) or a confidence zone (DM) to optimize the algorithm performance further. Combining the joint algorithm within the SAG or DM algorithm yields an optimized algorithm which offers the advantage of the different algorithms. The new algorithms lead to a decrease of the required transmit power by about 22 percent while achieving the same BER. Additional to this, a 6 percent reduction of the processing time is possible relative to MDDMA due to the possibility to reduce the equalizer length.","2326-0319;2326-0262","978-8-3620-6520-2978-8-3620-6518-9978-8-3620-6519","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7067285","","Convolution;Clocks;Phase shift keying;Transmitters;Receivers;Equalizers","blind equalisers;digital signal processing chips;intersymbol interference;least mean squares methods;optimisation;telecommunication channels","stop-and-go-algorithms;SAG algorithm;dual-mode-algorithms;baseband transmission;blind equalization algorithms;digital signal processor test bed;DSP test bed;blind channel equalizers;Bussgang algorithm;joint algorithm;modified decision directed modulus algorithm;MDDMA;modified constant modulus algorithm;MCMA;modified decision directed algorithm;MDDA;equalizer tap vector;BER","","","24","","","","","","IEEE","IEEE Conferences"
"Diverse Compiling for Microprocessor Fault Detection in Temporal Redundant Systems","A. Höller; T. Rauter; J. Iber; C. Kreiner","NA; NA; NA; NA","2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing","","2015","","","1928","1935","As hardware components are expected to become ever more unreliable due to the technology scaling, hardware errors have become unavoidable. Dependable systems that rely on a correct functionality often use redundancy to detect such hardware faults during operation. However, to design costefficient reliable systems, it is crucial to effectively exploit the available redundancy. Thus, researchers have investigated ways to automatically add cost-efficient diversity to software to increase the efficiency of redundancy strategies. One of these automated software diversification methods is diverse compiling, which exploits the diversity introduced by different compilers and different optimization flags. However, there is a lack of statistics regarding the efficiency of the approach for detecting certain types of faults. In this paper, we contribute towards filling this gap by evaluating the diverse compiling approach regarding its ability to detect faults in the microprocessor. We experimentally quantify the efficiency of diverse compiling for three benchmarks regarding the detection of register and instruction decoder faults for an ARM9 processor. For these applications, our fault injection campaigns show that about 90% register faults and 70% instruction decoder faults can be detected with diverse compiling. These results indicate that this approach is quite promising for improving the efficiency of redundancy techniques.","","978-1-5090-0154-5978-1-5090-0153","10.1109/CIT/IUCC/DASC/PICOM.2015.285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363331","diverse compiling;fault tolerance;redundancy;microprocessor faults;hardware faults;automated software diversity","Redundancy;Hardware;Optimization;Fault tolerant systems;Software;Registers","benchmark testing;fault diagnosis;microprocessor chips;program compilers;redundancy","microprocessor fault detection;temporal redundant systems;hardware components;technology scaling;hardware errors;redundancy strategies;automated software diversification methods;compiler diversity;optimization flags;instruction decoder fault detection;register fault detection;ARM9 processor;fault injection campaigns;redundancy techniques","","","32","","","","","","IEEE","IEEE Conferences"
"Securing smart maintenance services: Hardware-security and TLS for MQTT","C. Lesjak; D. Hein; M. Hofmann; M. Maritsch; A. Aldrian; P. Priller; T. Ebner; T. Ruprechter; G. Pregartner","Design Center Graz, Infineon Technologies Austria AG, Graz, Austria; Institute for Applied Information Processing and Communications, Graz University of Technology, Graz, Austria; GUEP Software GmbH, Graz, Austria; evolaris next level GmbH, Graz, Austria; Instrumentation and Test Systems, AVL List GmbH, Graz, Austria; Instrumentation and Test Systems, AVL List GmbH, Graz, Austria; evolaris next level GmbH, Graz, Austria; Design Center Graz, Infineon Technologies Austria AG, Graz, Austria; GUEP Software GmbH, Graz, Austria","2015 IEEE 13th International Conference on Industrial Informatics (INDIN)","","2015","","","1243","1250","Increasing the efficiency of production and manufacturing processes is a key goal of initiatives like Industry 4.0. Within the context of the European research project ARROWHEAD, we enable and secure smart maintenance services. An overall goal is to proactively predict and optimize the Maintenance, Repair and Operations (MRO) processes carried out by a device maintainer, for industrial devices deployed at the customer. Therefore it is necessary to centrally acquire maintenance relevant equipment status data from remotely located devices over the Internet. Consequently, security and privacy issues arise from connecting devices to the Internet, and sending data from customer sites to the maintainer's back-end. In this paper we consider an exemplary automotive use case with an AVL Particle Counter (APC) as device. The APC transmits its status information by means of a fingerprint via the publish-subscribe protocol Message Queue Telemetry Transport (MQTT) to an MQTT Information Broker in the remotely located AVL back-end. In a threat analysis we focus on the MQTT routing information asset and identify two elementary security goals in regard to client authentication. Consequently we propose a system architecture incorporating a hardware security controller that processes the Transport Layer Security (TLS) client authentication step. We validate the feasibility of the concept by means of a prototype implementation. Experimental results indicate that no significant performance impact is imposed by the hardware security element. The security evaluation confirms the advanced security of our system, which we believe lays the foundation for security and privacy in future smart service infrastructures.","1935-4576;2378-363X","978-1-4799-6649-3978-1-4799-6648","10.1109/INDIN.2015.7281913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7281913","","Maintenance engineering;Authentication;Protocols;Internet;Cryptography;Production","automotive engineering;cryptographic protocols;maintenance engineering;message passing;middleware;routing protocols;transport protocols","smart maintenance service security;TLS;MQTT;production manufacturing process;Industry 4.0;European research project;ARROWHEAD;maintenance-repair-and-operations process;MRO process optimization;MRO process prediction;industrial devices;remotely located devices;customer sites;maintainer back-end;automotive use case;AVL-Particle Counter;APC device;information transmission;publish-subscribe protocol;Message Queue Telemetry Transport;MQTT Information Broker;remotely located AVL back-end;threat analysis;MQTT routing information asset;security goals;client authentication;hardware security controller;Transport Layer Security;TLS client authentication;APC fingerprint","","15","20","","","","","","IEEE","IEEE Conferences"
"Rapid Prototyping of Digital Controllers for Microgrid Inverters","S. Buso; T. Caldognetto","Department of Information Engineering, University of Padova, Padova, Italy; Department of Information Engineering, University of Padova, Padova, Italy","IEEE Journal of Emerging and Selected Topics in Power Electronics","","2015","3","2","440","450","A rapid prototyping methodology for digital controllers is presented in this paper. Its main application is in the development, debugging, and test of microgrid inverter controllers. To fulfill the application requirements, these systems are characterized by complex multilayer architectures, extending from pulse width modulation (PWM) and current control loops up to global optimization and high level communication functions. The complexity and the wide variability of the different layer implementations make digital control mandatory. However, developing so complex digital controllers on conventional hardware platforms, like digital signal processors (DSPs) or even FPGAs, is not the most practical choice. This paper shows how multiplatform control devices, where software configurable DSP functions and programmable logic circuits are efficiently combined, represent the optimal solution for this field of application. Furthermore, this paper proposes hardware-in-the-loop real-time simulation as an effective means of developing and debugging complex hardware and software codesigned controllers. A case study is presented and used to illustrate the different design and test phases, from initial concept and numerical simulation to final experimental verification.","2168-6777;2168-6785","","10.1109/JESTPE.2014.2327064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6823626","Digital control platforms;hardware-in-the-loop (HIL) real-time (RT) simulation;power converters;rapid prototyping","Field programmable gate arrays;Inverters;Microgrids;Density estimation robust algorithm;Hardware;Digital signal processing;Couplings","control engineering computing;digital control;distributed power generation;electric current control;logic circuits;power engineering computing;programmable logic devices;PWM invertors;real-time systems;software architecture;software prototyping","rapid prototyping methodology;microgrid inverters;application requirements;complex multilayer architectures;pulse width modulation;PWM;current control loops;global optimization;high level communication functions;digital controllers;hardware platforms;digital signal processors;software configurable DSP functions;multiplatform control devices;programmable logic circuits;multiplatform control devices;hardware-in-the-loop real-time simulation;complex hardware controllers;software codesigned controllers;test phases;initial concept;numerical simulation","","12","29","","","","","","IEEE","IEEE Journals & Magazines"
"Shared memory aware MPSoC software deployment","T. Schönwald; A. Viehl; O. Bringmann; W. Rosenstiel","FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","1771","1776","In this paper we present a novel approach for mapping interconnected software components onto cores of homogenous MPSoC architectures. The analytic mapping process considers shared memory communication as well as the routing algorithm controlling packet-based communication. The software components are mapped with the constraints of avoiding communication conflicts as well as access conflicts to shared memory resources. The core of the elaborated approach consists of an algorithm for software mapping which is inspired by force-directed scheduling from high-level synthesis. Experimental results show that the presented approach increases the overall system performance by 22% while reducing the average communication latency by 35%. For presenting the major advantages of the developed solution, we optimized an advanced driver assistance system on the Tilera TILEPro64 processor.","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.356","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513802","","Computer architecture;Force;Software;Routing;Software algorithms;Data communication;Matched filters","","","","","24","","","","","","IEEE","IEEE Conferences"
"Improving fault tolerance utilizing hardware-software-co-synthesis","H. Riener; S. Frehse; G. Fey","Institute of Computer Science, University of Bremen, Germany; Institute of Computer Science, University of Bremen, Germany; Institute of Computer Science, University of Bremen, Germany","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","939","942","Embedded systems consist of hardware and software and are ubiquitous in safety-critical and mission-critical fields. The increasing integration density of modern, digital circuits causes an increasing vulnerability of embedded systems to transient faults. Techniques to improve the fault tolerance are often either implemented in hardware or in software. In this paper, we focus on synthesis techniques to improve the fault tolerance of embedded systems considering hardware and software. A greedy algorithm is presented which iteratively assesses the fault tolerance of a processor-based system and decides which components of the system have to be hardened choosing from a set of existing techniques. We evaluate the algorithm in a simple case study using a Traffic Collision Avoidance System (TCAS).","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513643","Fault tolerance;Formal methods;Synthesis;Optimization","Fault tolerance;Fault tolerant systems;Hardware;Software;Transient analysis;Greedy algorithms;Circuit faults","","","","1","16","","","","","","IEEE","IEEE Conferences"
"Survey of Covering Arrays","J. Torres-Jimenez; I. Izquierdo-Marquez","NA; NA","2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing","","2013","","","20","27","Covering Arrays(CA) are combinatorial objects that have been used succesfully to automate the generation of test cases for software testing. The CAs have the features of being of minimal cardinality (i.e. minimize the number of test cases), and maximum coverage (i.e. they guarantee to cover all combinations of certain size between the input parameters). Only in few cases there is known an optimal solution to construct CAs, but in general the problem of constructing optimal CAs is a hard combinatorial optimization problem. For this reason, a number of methods to solve the construction of covering arrays have been developed. This paper gives a survey of the state of the art of the methods to construct covering arrays. The methods analyzed were grouped in four categories: exact methods (Section II), greedy methods (Section III), metaheuristic methods (Section IV), and algebraic methods (Section V). The paper ends with a summary of the methods analyzed.","","978-1-4799-3036-4978-1-4799-3035","10.1109/SYNASC.2013.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821126","covering arrays; methods to construct covering arrays","Encoding;Generators;Simulated annealing;Markov processes;Genetic algorithms;Metals","arrays;automatic test pattern generation;combinatorial mathematics;greedy algorithms;program testing","covering arrays;combinatorial objects;automatic test case generation;software testing;minimal cardinality;optimal CAs;hard combinatorial optimization problem;exact methods;greedy methods;metaheuristic methods;algebraic methods","","8","38","","","","","","IEEE","IEEE Conferences"
"NIRMAL: Automatic identification of software relevant tweets leveraging language model","A. Sharma; Y. Tian; D. Lo","School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore; School of Information Systems, Singapore Management University, Singapore","2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER)","","2015","","","449","458","Twitter is one of the most widely used social media platforms today. It enables users to share and view short 140-character messages called “tweets”. About 284 million active users generate close to 500 million tweets per day. Such rapid generation of user generated content in large magnitudes results in the problem of information overload. Users who are interested in information related to a particular domain have limited means to filter out irrelevant tweets and tend to get lost in the huge amount of data they encounter. A recent study by Singer et al. found that software developers use Twitter to stay aware of industry trends, to learn from others, and to network with other developers. However, Singer et al. also reported that developers often find Twitter streams to contain too much noise which is a barrier to the adoption of Twitter. In this paper, to help developers cope with noise, we propose a novel approach named NIRMAL, which automatically identifies software relevant tweets from a collection or stream of tweets. Our approach is based on language modeling which learns a statistical model based on a training corpus (i.e., set of documents). We make use of a subset of posts from StackOverflow, a programming question and answer site, as a training corpus to learn a language model. A corpus of tweets was then used to test the effectiveness of the trained language model. The tweets were sorted based on the rank the model assigned to each of the individual tweets. The top 200 tweets were then manually analyzed to verify whether they are software related or not, and then an accuracy score was calculated. The results show that decent accuracy scores can be achieved by various variants of NIRMAL, which indicates that NIRMAL can effectively identify software related tweets from a huge corpus of tweets.","1534-5351","978-1-4799-8469","10.1109/SANER.2015.7081855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7081855","","Software;Twitter;Support vector machines;Training;Computational modeling;Mathematical model;Market research","information filtering;natural language processing;question answering (information retrieval);social networking (online);statistical analysis","NIRMAL framework;automatic software relevant tweet identification;language model leveraging;social media platforms;short-character message sharing;short-character message view;user generated content generation;information overload;irrelevant tweet filtering;Twitter streams;Twitter adoption barrier;tweet stream;tweet collection;statistical model;training corpus;StackOverflow;programming question-and-answer site;language model learning;tweet sorting;tweet analysis;accuracy scores","","11","30","","","","","","IEEE","IEEE Conferences"
"Performance research and optimization on CPython's interpreter","H. Cao; N. Gu; K. Ren; Y. Li","NA; Department of Computer Science and Technology, University of Science and Technology of China, Hefei, China, 230027; NA; NA","2015 Federated Conference on Computer Science and Information Systems (FedCSIS)","","2015","","","435","441","In this paper, the performance research on CPython's latest interpreter is presented, concluding that bytecode dispatching takes about 25 percent of total execution time on average. Based on this observation, a novel bytecode dispatching mechanism is proposed to reduce the time spent on this phase to a minimum. With this mechanism, the blocks associated with each kind of bytecodes are rewritten in hand-tuned assembly, their opcodes are renumbered, and their memory spaces are rescheduled. With these preparations, this new bytecode dispatching mechanism replaces the time-consuming memory reading operations with rapid operations on registers. This mechanism is implemented in CPython-3.3.0. Experiments on lots of benchmarks demonstrate its correctness and efficiency. The comparison between original CPython and optimized CPython shows that this new mechanism achieves about 8.5 percent performance improvement on average. For some particular benchmarks, the maximum improvement is up to 18 percentages.","","978-8-3608-1065-1978-8-3608-1067","10.15439/2015F139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7321477","","Dispatching;Benchmark testing;Binary codes;Switches;Optimization;Registers;Instruction sets","C language;optimisation;software performance evaluation","CPython interpreter;bytecode dispatching mechanism;time-consuming memory;CPython-3.3.0","","1","25","","","","","","IEEE","IEEE Conferences"
"An activity based learning: C programming","V. S. Handur; P. D. Kalwad; N. Yaligar; V. G. Garagad; M. K. Pawar","Dept. of Computer Science and Engineering, B.V. Bhoomaraddi College of Engineering and Technology, Hubballi, India; Dept. of Computer Science and Engineering, B.V. Bhoomaraddi College of Engineering and Technology, Hubballi, India; Dept. of Computer Science and Engineering, B.V. Bhoomaraddi College of Engineering and Technology, Hubballi, India; Dept. of Computer Science and Engineering, B.V. Bhoomaraddi College of Engineering and Technology, Hubballi, India; Dept. of Computer Science and Engineering, B.V. Bhoomaraddi College of Engineering and Technology, Hubballi, India","2015 IEEE 3rd International Conference on MOOCs, Innovation and Technology in Education (MITE)","","2015","","","310","314","Computers are the leading technology of the 21st century. Programming, the development of software is thus a fundamental activity in which many people are engaged worldwide. Therefore programming courses are included as a part of the curriculum. In these courses, students are primarily introduced to language features. Traditionally, the students practice by applying the acquired knowledge to solve some logically straightforward problems giving less scope for the programming skills. This paper focuses on application of coding standards, coding techniques, debugging, code review, refactoring, code optimization, test driven programming and pair programming based learning to master not only programming language features, but also an integrated approach to gain problem solving and programming skills. The subject is introduced as a first year course where the students are without any or with smaller amount of background or experience in computer programming. Taking this into consideration, activities like programming were designed. These activities enhanced the learning ability, problem solving skills programming skills and debugging skills.","","978-1-4673-6747-9978-1-4673-6746","10.1109/MITE.2015.7375336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375336","problem solving;programming skills;code optimization;Test driven programming;debugging;coding standards","Programming profession;Education;Encoding;Standards;Problem-solving;Debugging","C language;computer science education;educational courses;program debugging;software maintenance","activity-based learning;C programming course;software development;programming skills;coding standards;coding techniques;code review;refactoring;code optimization;test driven programming;pair programming based learning;programming language features;integrated approach;problem solving skills;computer programming;learning ability enhancement;debugging skills","","1","6","","","","","","IEEE","IEEE Conferences"
"A novel Bi-Ant colony optimization algorithm for solving multi-objective service selection problem","Liping Huang; B. Zhang; Xun Yuan; Changsheng Zhang; Anxiang Ma","Software College, Northeastern University, Shen Yang, China; College of Information Science & Engineering, Northeastern University, Shen Yang, China; Institute of Information Science & Engineering, Shenyang Ligong University, China; College of Information Science & Engineering, Northeastern University, Shen Yang, China; College of Information Science & Engineering, Northeastern University, Shen Yang, China","2015 11th International Conference on Natural Computation (ICNC)","","2015","","","1080","1084","The multi-objective service selection problem is a basic problem in Service Computing and is NP-Hard. This paper proposes a novel Bi-Ant colony optimization multi-objective service selection algorithm for this problem. Two objective functions related with response time and cost attributes are considered. For each objective, a heuristic function and a pheromone updating policy are defined against the characteristics of this problem. Then, a combined state transition rule is designed based on them. The algorithm has been tested in nine cases and compared with the related MOACO algorithm for this problem. The result demonstrates that our approach is effective and better than MOACO.","2157-9563","978-1-4673-7679-2978-1-4673-7678","10.1109/ICNC.2015.7378142","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378142","multi-objective;service selection;Bi-Ant;MOACO","Time factors;Quality of service;Algorithm design and analysis;Optimization;Silicon;Concrete;Approximation algorithms","ant colony optimisation","novel biant colony optimization algorithm;multiobjective service selection problem;service computing;NP-hard problem;objective functions;response time;cost attributes;heuristic function;pheromone updating policy;combined state transition rule;MOACO algorithm;NBACO algorithm","","","12","","","","","","IEEE","IEEE Conferences"
"Research on key technology on designing high-frequency resistive voltage divider design","H. Jin; J. Wu; L. You; Z. Hu","Beijing Orient Institute of Measurement &amp; Test (BOIMT), Department 12, Mailbox 9628, 100086, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing 100191, China; Beijing Orient Institute of Measurement &amp; Test (BOIMT), Department 12, Mailbox 9628, 100086, China; Beijing Orient Institute of Measurement &amp; Test (BOIMT), Department 12, Mailbox 9628, 100086, China","29th Conference on Precision Electromagnetic Measurements (CPEM 2014)","","2014","","","628","629","For high-frequency resistive voltage divider, the distributed parameter model was studied and the key factors affecting its performance were discussed. The high-frequency characteristics of typical outer structure of voltage divider were simulated by ANSYS software. According to the reflection coefficient, VSWR and return loss, an optimized structure of divider for high frequency is presented.","2160-0171;0589-1485","978-1-4799-2479-0978-1-4799-5205-2978-1-4799-2478","10.1109/CPEM.2014.6898542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898542","High-frequency voltage divider;circuit model;simulation;structure","Voltage measurement;Integrated circuit modeling;Frequency conversion;Analytical models;Capacitance;Transmission line measurements;Impedance","circuit simulation;distributed parameter networks;network synthesis;voltage dividers","VSWR;return loss;reflection coefficient;ANSYS software;distributed parameter model;high-frequency resistive voltage divider design","","","4","","","","","","IEEE","IEEE Conferences"
"Data - Based scheduling production with simulation and optimization methods","T. Witkowski","Faculty of Production Engineering, Warsaw University Technology, Poland","2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS)","","2013","01","","259","264","This paper shows the application of statistics techniques, simulation and optimization methods for data - based scheduling production. The software of this methods, allows us to analyze the process construction rules for many variants reflecting a variety of combination other factors. For the analysis of the production process the greedy randomized adaptive search procedure (GRASP) and simulation were used. Experiments with different levels of factors have been considered and compared. The GRASP algorithm has been tested and illustrated with results for the serial and parallel route.","","978-1-4799-1429-6978-1-4799-1426-5978-1-4799-1427","10.1109/IDAACS.2013.6662685","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662685","data mining;simulation;optimization methods;scheduling production","Job shop scheduling;Data models;Object oriented modeling;Testing;Schedules;Dispatching","data analysis;data mining;greedy algorithms;optimisation;scheduling;search problems;statistical analysis","data mining;serial route;parallel route;GRASP algorithm;greedy randomized adaptive search procedure;statistics techniques;optimization methods;data-based scheduling production","","","28","","","","","","IEEE","IEEE Conferences"
"Development of Bat Algorithm toolkit in LabVIEW<sup>™</sup>","K. S. Thakur; V. Kumar; K. P. S. Rana; P. Mishra; J. Kumar; S. S. Nair","Division of Instrumentation and Control Engineering, Netaji Subhas Institute of Technology, New Delhi, India; Division of Instrumentation and Control Engineering, Netaji Subhas Institute of Technology, New Delhi, India; Division of Instrumentation and Control Engineering, Netaji Subhas Institute of Technology, New Delhi, India; Division of Instrumentation and Control Engineering, Netaji Subhas Institute of Technology, New Delhi, India; Division of Instrumentation and Control Engineering, Netaji Subhas Institute of Technology, New Delhi, India; Division of Instrumentation and Control Engineering, Netaji Subhas Institute of Technology, New Delhi, India","International Conference on Computing, Communication & Automation","","2015","","","5","10","Meta-heuristic optimization algorithms are very useful tool for obtaining an optimal solution of engineering optimization problems. Bat Algorithm (BA) is one of the recent meta-heuristic algorithms. It has been claimed to be superior to its counter parts. On the other hand LabVIEW<sup>™</sup>is a versatile software tool being utilized for measurement and control applications in various engineering domains worldwide. The standard LabVIEW<sup>™</sup>package is only supplied with the Differential Evolution (DE) Toolkit for optimization. At times need was felt to develop a better optimization support in the LabVIEW<sup>™</sup>package. In this work a genuine effort has been made for this task and a BA Toolkit has been developed in LabVIEW<sup>™</sup>. The detailed development strategy, component descriptions and their interfaces and a comparison of the obtained results with the DE optimization toolkit has been presented in this paper for a set of classical benchmark functions.","","978-1-4799-8890-7978-1-4799-8889","10.1109/CCAA.2015.7148362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7148362","bat algorithm;optimization;meta-heuristic;virtual instrument","Optimization;Sociology;Statistics;Benchmark testing;Upper bound;Automation;Instruments","evolutionary computation;mathematics computing;virtual instrumentation","bat algorithm toolkit;metaheuristic optimization algorithms;engineering optimization problems;software tool;LabVIEW package;differential evolution toolkit;DE toolkit;DE optimization toolkit","","2","27","","","","","","IEEE","IEEE Conferences"
"Understanding the power-performance tradeoff through Pareto analysis of live performance data","J. Michanan; R. Dewri; M. J. Rutherford","Department of Computer Science, University of Denver, Denver, Colorado U.S.A; Department of Computer Science, University of Denver, Denver, Colorado U.S.A.; Department of Computer Science, University of Denver, Denver, Colorado U.S.A.","International Green Computing Conference","","2014","","","1","8","Optimizing the power-performance tradeoff of a software system is challenging as the design space is large and live data is difficult to obtain. As a result, many power reduction techniques are based on power models which may not represent the full complexity of the system being analyzed. In this paper, in contrast, we propose a process for performing a tradeoff analysis using live power/performance data. As a case study, we conduct an empirical evaluation of the power/performance impact of cache configuration on embedded systems. We gather live power consumption and execution time data for the programs in the CHStone benchmark suite on an embedded processor with configurable cache parameters and perform a Pareto analysis on these data to identify the optimal cache configurations. We observe that the optimal configurations are sparse in the design space, are inconsistent across the benchmark, and are counterintuitive in some cases. Our results reveal interesting, unexpected insights motivating the need for tools and methodologies that automate this process and operate directly on data gathered from the systems.","","978-1-4799-6177","10.1109/IGCC.2014.7039164","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7039164","Energy;Power;Efficiency;Performance;Tradeoff;Optimization;Cache;FPGA;Pareto","Benchmark testing;Power demand;Pareto optimization;Hardware;Software","benchmark testing;cache storage;embedded systems;Pareto analysis;power aware computing;program processors","power-performance tradeoff;live performance data;software system;power reduction techniques;cache configuration;embedded systems;CHStone benchmark suite;embedded processor;configurable cache parameters;Pareto analysis;optimal cache configurations","","6","17","","","","","","IEEE","IEEE Conferences"
"Duals in Spectral Fault Localization","L. Naish; H. J. Lee","NA; NA","2013 22nd Australian Software Engineering Conference","","2013","","","51","59","Numerous set similarity metrics have been used for ranking ""suspiciousness"" of code in spectral fault localization, which uses execution profiles of passed and failed test cases to help locate bugs. Research in data mining has identified several forms of possibly desirable symmetry in similarity metrics. Here we define several forms of ""duals"" of metrics, based on these forms of symmetries. Use of these duals, plus some other slight modifications, leads to several new similarity metrics. We show that versions of several previously proposed metrics are optimal, or nearly optimal, for locating single bugs. We also show that a form of duality exists between locating single bugs and locating ""deterministic"" bugs (execution of which always results in test case failure). Duals of the various single bug optimal metrics are optimal for locating such bugs. This more theoretical work leads to a conjecture about how different metrics could be chosen for different stages of software development.","1530-0803;2377-5408","978-0-7695-4995","10.1109/ASWEC.2013.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6601292","program spectra;fault localization;debugging;set similarity","Measurement;Computer bugs;Software;Data mining;Vectors;Benchmark testing;Australia","program debugging;software metrics","set similarity metrics;spectral fault localization;execution profiles;metrics duals;similarity metrics;duality form;deterministic bugs;single bug optimal metrics;software development","","6","26","","","","","","IEEE","IEEE Conferences"
"Software enabled wear-leveling for hybrid PCM main memory on embedded systems","J. Hu; Q. Zhuge; C. J. Xue; W. Tseng; E. H. -. Sha","Dept. of Computer Science, University of Texas at Dallas, Richardson, 75080, USA; College of Computer Science, Chongqing University, China; Dept. of Computer Science, City University of Hong Kong, Tat Chee Ave, Kowloon, Hong Kong; Dept. of Computer Science, University of Texas at Dallas, Richardson, 75080, USA; Dept. of Computer Science, University of Texas at Dallas, Richardson, 75080, USA","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","599","602","Phase Change Memory (PCM) is a promising DRAM replacement in embedded systems due to its attractive characteristics. However, relatively low endurance has limited its practical applications. In this paper, in additional to existing hardware level optimizations, we propose software enabled wear-leveling techniques to further extend PCM's lifetime when it is adopted in embedded systems. A polynomial-time algorithm, the Software Wear-Leveling (SWL) algorithm, is proposed in this paper to achieve wear-leveling without hardware overhead. According to the experimental results, the proposed technique can reduce the number of writes on the most-written bits by more than 80% when compared with a greedy algorithm, and by around 60% when compared with the existing Optimal Data Allocation (ODA) algorithm with under 6% memory access overhead.","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.131","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513577","","Phase change materials;Software;Software algorithms;Random access memory;Hardware;Resource management;Nonvolatile memory","","","","29","18","","","","","","IEEE","IEEE Conferences"
"Multi-objective parameter optimization of S-MAC protocol","E. Kaljic; S. Konjicija","Department of Telecommunication, Faculty of Electrical Engineering at the University of Sarajevo, Zmaja od Bosne, 71000 Sarajevo, Bosnia and Herzegovina; Department of Automatic Control and Electronics, Faculty of Electrical Engineering at the University of Sarajevo, Zmaja od Bosne, 71000 Sarajevo, Bosnia and Herzegovina","2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","","2014","","","545","550","In the process of design of protocols for medium access control in wireless sensor networks, it is necessary to find a compromise between energy saving, time sensitivity, scalability, adaptability and complexity. In this research, as an object of optimization S-MAC protocol is chosen. The main advantage of the S-MAC protocol is a state of sleep, which significantly reduces power consumption. Although the S-MAC protocol is described with a large number of parameters, in this paper we have selected three parameters that directly affect the time frame protocol: the duty cycle, the duration of the synchronization interval, and duration of the data interval. To assess the quality of the selected parameters, three measures were selected: total electricity consumption in the network, delay in the delivery of packets to the application layer, and the total achieved bandwidth. Using ns-2 network simulator, a model for testing the influence of parameters of S-MAC protocol on network performance has been developed. The parameters values are selected using the NSGA-II algorithm implemented in MATLAB software package, and the whole process of finding of non-dominated solution set is fully automated. In addition to multi-objective optimization of parameters, an experiment with single-objective optimization using aggregated utility function and the standard GA is performed. Finally, the analysis of the obtained set of non-dominated solutions is performed and a comparison with the results of the single-objective optimization is done.","","978-953-233-077-9978-953-233-081","10.1109/MIPRO.2014.6859628","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859628","S-MAC;WSN;multi-objective;single-objective;NSGA;optimization;ns-2;protocol","Optimization;Wireless sensor networks;Delays;Media Access Protocol;Bandwidth;Energy consumption","access protocols;energy consumption;genetic algorithms;wireless sensor networks","medium access control;wireless sensor networks;multiobjective parameter optimization;S-MAC protocol;power consumption;time frame protocol;duty cycle;synchronization interval duration;data interval duration;total electricity consumption;ns-2 network simulator;network performance;parameters values;NSGA-II algorithm;MATLAB software package;nondominated solution set;aggregated utility function","","","10","","","","","","IEEE","IEEE Conferences"
"Operating system support to an online hardware-software co-design scheduler for heterogeneous multicore architectures","M. A. F. Bueno; J. A. M. de Holanda; E. Pereira; E. Marques","USP - University of São Paulo, Institute of Mathematics and Computer Science, São Carlos, Brazil; USP - University of São Paulo, Institute of Mathematics and Computer Science, São Carlos, Brazil; USP - University of São Paulo, Institute of Mathematics and Computer Science, São Carlos, Brazil; USP - University of São Paulo, Institute of Mathematics and Computer Science, São Carlos, Brazil","2014 IEEE 20th International Conference on Embedded and Real-Time Computing Systems and Applications","","2014","","","1","10","This paper aims at designing and implementing a scheduler model for heterogeneous multiprocessor architectures based on software and hardware. As a proof of concept, the scheduler model was applied to the Linux operating system running on the SPARC Leon3 processor. In this sense, performance monitors have been implemented within the processors, which identify demands of processes in real-time. For each process, its demand is projected for the other processors in the architecture and then, it is performed a balancing to maximize the total system performance by distributing processes among processors. The Hungarian maximization algorithm, used in balancing scheduler was developed in hardware, and provides greater parallelism and performance in the execution of the algorithm. The scheduler has been validated through the parallel execution of several benchmarks, resulting in decreased execution times compared to the scheduler without the heterogeneity support.","2325-1271;2325-1301","978-1-4799-3953","10.1109/RTCSA.2014.6910514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6910514","","Monitoring;Benchmark testing;Integrated circuits","computer architecture;hardware-software codesign;Linux;microprocessor chips;multiprocessing systems;operating systems (computers);optimisation;processor scheduling","operating system support;online hardware-software codesign scheduler;heterogeneous multicore architectures;multiprocessor architecture;Linux operating system;SPARC Leon3 processor;Hungarian maximization algorithm","","1","23","","","","","","IEEE","IEEE Conferences"
"Plugin for concept-assisted search and navigation on PUBMED","T. Joseph; V. G. Saipradeep; S. Kotte; A. Rao; R. Srinivasan","TCS Innovation Labs, Tata Consultancy Services Limited 1, Software Units Layout, Madhapur, Hyderabad, India; TCS Innovation Labs, Tata Consultancy Services Limited 1, Software Units Layout, Madhapur, Hyderabad, India; TCS Innovation Labs, Tata Consultancy Services Limited 1, Software Units Layout, Madhapur, Hyderabad, India; TCS Innovation Labs, Tata Consultancy Services Limited 1, Software Units Layout, Madhapur, Hyderabad, India; TCS Innovation Labs, Tata Consultancy Services Limited 1, Software Units Layout, Madhapur, Hyderabad, India","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","1712","1714","Introduction: Search of MEDLINE using the PUBMED interface still remains a preferred approach for many scientists, even in the presence of alternatives. While PUBMED provides a rich interface for querying, it does not provide deep analysis of the query results to refine queries and build hypotheses. To provide an improved search experience in PubMed, we have developed a Firefox browser plugin called “TPX plus” that enables concept-assisted search and navigation of the MEDLINE database while remaining in the PubMed website. Some of the features of the plugin include identification and highlighting of biomedical concept types like genes, processes, diseases etc. in the PubMed results page; link-outs to dictionary sources for identified terms; a Concept Explorer to view ranked automatically identified concepts in the results as well as use them for filtering/refining the search; deriving related concepts using a statistical concept-associations network; bookmarking and managing articles, storing notes and sharing comments on articles. Implementation: The core of the plugin is written in JavaScript that uses Web 2.0 technologies such as jQuery, AJAXlJSON and XUL for handling and manipulating content of web pages within the PubMed domain. 'TPX plus' uses the DOM structure of PubMed's search interface for a) building input to Annotation server by extracting appropriate content such as title, abstract text and PubMed ID from the appropriate DOM elements and b) presenting the top ranked biomedical concepts based on their relevance to the search query under concept explorer and c) highlight biomedical concepts for each abstract in the search result under article view in the PubMed's search interface. Conclusion: Its three-pronged nature of relying on PubMed as the basic indexing engine, enhancing PubMed by performing customized offline and on-the-fly analysis of the search results and most importantly being available as a browser plugin allowing use of the PubMed user-interface makes this plugin an ideal PubMed search assistant for scientists.","","978-1-4673-6799-8978-1-4673-6798","10.1109/BIBM.2015.7359934","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359934","PubMed;search;plugin;TPX;biomedical text;information retrieval","Navigation;Browsers;Buildings;Benchmark testing;Cryptography;Bioinformatics","indexing;information filtering;medical computing;query processing;user interfaces;Web sites","concept-assisted search and navigation;PUBMED interface;Firefox browser plugin;TPX plus;MEDLINE database;PubMed Web site;biomedical concept type identification;biomedical concept type highlighting;Concept Explorer;search filtering;search refining;statistical concept-associations network;article bookmarking;article management;JavaScript;Web 2.0 technologies;jQuery;AJAXlJSON;XUL;content handling;content manipulation;Web pages;Annotation server;indexing engine;offline analysis;on-the-fly analysis;PubMed user-interface","","2","11","","","","","","IEEE","IEEE Conferences"
"Interaction driven composition of student groups for optimal groupwork learning performance","L. Cen; D. Ruta; L. Powell; J. Ng","Etisalat British Telecom Innovation Centre, Khalifa University of Science, Technology and Research, Abu Dhabi, United Arab Emirates; Etisalat British Telecom Innovation Centre, Khalifa University of Science, Technology and Research, Abu Dhabi, United Arab Emirates; Etisalat British Telecom Innovation Centre, Khalifa University of Science, Technology and Research, Abu Dhabi, United Arab Emirates; Etisalat British Telecom Innovation Centre, Khalifa University of Science, Technology and Research, Abu Dhabi, United Arab Emirates","2015 IEEE Frontiers in Education Conference (FIE)","","2015","","","1","6","Collaborative Learning (CL) has been considered as an effective way to improve the learning outcomes of students in contrast to individual learning. However, assigning a groupwork task to a team of students does not guarantee a successful performance, and in fact could hinder the benefits of group learning if the members do not interact as expected. Indeed, group learning performance is largely dependent on group composition. In this work we address the challenge of identifying the characteristics of the individual group members that bare the significant impact on the performance of the groupwork. Specifically we investigate the impact that a combination of individual student performances and gender have on the group performance and intend to find generic segmentation guidelines that would map smoothly onto the groupwork performance. A novel grouping method is proposed, which splits the set of students into groups that maximize one of the two desired criteria: the expected average groupwork performance or the average improvement achieved by a student as a result of synergic group learning and interaction effects. The model uses global optimization approach to identify optimal students allocation into the groups that best satisfy the optimization criteria. We illustrate our findings on the data obtained from the trial of the Collaborative Learning Environment (CLE) software. The CLE was developed at Etisalat British Telecom Innovation Centre (EBTIC) and tested over one semester with a sample of 122 students working in different groups in the Engineering and Molecular Biology courses at Khalifa University. The results of our method can not only help to understand the significant factors impacting group performance in group-based learning, but can also provide practical strategies on optimal group composition for collaborative learning activities.","","978-1-4799-8454","10.1109/FIE.2015.7344266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344266","Collaborative Learning;learning performance evaluation;group composition;global optimization;Genetic Algorithm","Collaborative work;Optimization;Biological cells;Genetic algorithms;Sociology;Statistics;Collaboration","courseware;groupware;optimisation","interaction-driven composition;student group;groupwork learning performance;collaborative learning software;CL software;grouping method;optimization criteria;group-based learning","","","21","","","","","","IEEE","IEEE Conferences"
"Solving the Multidimensional Knapsack Problem using a CUDA accelerated PSO","D. Zan; J. Jaros","Department of Computer Systems, Brno University of Technology, Bozetechova 2, 612 00 Brno, Czech Republic; Department of Computer Systems, Brno University of Technology, Bozetechova 2, 612 00 Brno, Czech Republic","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","2933","2939","The Multidimensional Knapsack Problem (MKP) represents an important model having numerous applications in combinatorial optimisation, decision-making and scheduling processes, cryptography, etc. Although the MKP is easy to define and implement, the time complexity of finding a good solution grows exponentially with the problem size. Therefore, novel software techniques and hardware platforms are being developed and employed to reduce the computation time. This paper addresses the possibility of solving the MKP using a GPU accelerated Particle Swarm Optimisation (PSO). The goal is to evaluate the attainable performance benefit when using a highly optimised GPU code instead of an efficient multi-core CPU implementation, while preserving the quality of the search process. The paper shows that a single Nvidia GTX 580 graphics card can outperform a quad-core CPU by a factor of 3.5 to 9.6, depending on the problem size. As both implementations are memory bound, these speed-ups directly correspond to the memory bandwidth ratio between the investigated GPU and CPU.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900534","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900534","","Graphics processing units;Bandwidth;Arrays;Kernel;Optimization;Benchmark testing;Instruction sets","computational complexity;graphics processing units;knapsack problems;parallel architectures;particle swarm optimisation","memory bandwidth ratio;quad-core CPU;Nvidia GTX 580 graphics card;multicore CPU;optimised GPU code;particle swarm optimisation;software techniques;time complexity;MKP;cryptography;scheduling processes;decision-making;combinatorial optimisation;PSO;CUDA;multidimensional knapsack problem","","2","17","","","","","","IEEE","IEEE Conferences"
"Program-Adaptive Mutational Fuzzing","S. K. Cha; M. Woo; D. Brumley","NA; NA; NA","2015 IEEE Symposium on Security and Privacy","","2015","","","725","741","We present the design of an algorithm to maximize the number of bugs found for black-box mutational fuzzing given a program and a seed input. The major intuition is to leverage white-box symbolic analysis on an execution trace for a given program-seed pair to detect dependencies among the bit positions of an input, and then use this dependency relation to compute a probabilistically optimal mutation ratio for this program-seed pair. Our result is promising: we found an average of 38.6% more bugs than three previous fuzzers over 8 applications using the same amount of fuzzing time.","1081-6011;2375-1207","978-1-4673-6949","10.1109/SP.2015.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163057","fuzzing;mutation ratio optimization;mutational fuzzing;software testing","Computer bugs;Optimization;Security;Testing;Hamming distance;Software","fuzzy set theory;probability;program debugging","program-adaptive mutational fuzzing;black-box mutational fuzzing;bugs;white-box symbolic analysis;execution trace;program-seed pair;bit positions;dependency relation;probabilistically optimal mutation ratio;fuzzing time","","21","59","","","","","","IEEE","IEEE Conferences"
"Optimized architecture of high order CIC filters","M. Pristach; M. Pavlik; J. Haze; L. Fujcik","Department of Microelectronics, Faculty of Electrical Engineering and Communication, Brno University of Technology, Czech Republic; Department of Microelectronics, Faculty of Electrical Engineering and Communication, Brno University of Technology, Czech Republic; Department of Microelectronics, Faculty of Electrical Engineering and Communication, Brno University of Technology, Czech Republic; Department of Microelectronics, Faculty of Electrical Engineering and Communication, Brno University of Technology, Czech Republic","Proceedings of the 20th International Conference Mixed Design of Integrated Circuits and Systems - MIXDES 2013","","2013","","","263","266","The paper presents an optimized architecture of cascaded integrator-comb (CIC) digital filter structure. The structure is suitable for implementation in application specific integration circuits (ASICs) or field programmable gate arrays (FPGAs). The main advantages of the architecture are higher working frequency, smaller area size and lower power consumption. Software in C++ language was written for automatic filter generation. The software generates fully synthesizable VHDL description of filter, batch file for simulator and test-bench file for automatic filter verification from the filter specification file.","","978-83-63578-02-2978-83-63578-00","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613354","application specific integration circuits;cascaded integrator-comb filters;hardware description language;Hogenauer filter","Computer architecture;Field programmable gate arrays;Power demand;Software;Interpolation;Application specific integrated circuits;Clocks","application specific integrated circuits;C++ language;digital filters;field programmable gate arrays","optimized architecture;high order CIC filters;cascaded integrator-comb digital filter structure;application specific integration circuits;field programmable gate arrays;lower power consumption;C++ language;automatic filter generation;VHDL description","","","7","","","","","","IEEE","IEEE Conferences"
"Profiling Patterns of Bit Flipping for Software Transactional Memories","F. L. Teixeira; M. L. Pilla; A. R. D. Bois; D. Mossé","NA; NA; NA; NA","2014 IEEE 26th International Symposium on Computer Architecture and High Performance Computing","","2014","","","136","143","Software Transactional Memory (STM) is a synchronization method proposed as an alternative to lockbased synchronization. It provides a higher-level abstraction that is easier to program, and that enables software composition. Transactions are defined by programmers, but the runtime system is responsible for detecting conflicts and avoiding race conditions. Phase Change Memory (PCM) is a new technology that is being developed to replace Dynamic Random Access Memories (DRAMs) in large datacenters. PCM write operations are much more expensive than reads in both energy and time. In this paper, we analyze performance, energy consumption, and write patterns in software transactional memories (STMs) to determine the potential of optimization for PCM scenarios. As the write operations are more expensive both in time and energy in PCMs, benchmarks from the STAMP suite were instrumented to count bits swapped due to store instructions, and experiments were executed using TinySTM. Our results showed a pattern of few bits being flipped for each memory write, and performance was inversely proportional to the number of writes. For most benchmarks, there was a small increase in energy consumption with more threads, which may be explained by the timid contention manager used by TinySTM.","1550-6533","978-1-4799-6905","10.1109/SBAC-PAD.2014.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6970657","Software trasactional memory;transactional memory;phase change memory;parallel programming","Benchmark testing;Phase change materials;Instruction sets;Genomics;Bioinformatics;Random access memory","computer centres;phase change memories;power aware computing;synchronisation","bit flipping profiling patterns;software transactional memories;synchronization method;lock-based synchronization;software composition;phase change memory;PCM;datacenters;STAMP suite;TinySTM;memory write;energy consumption","","2","27","","","","","","IEEE","IEEE Conferences"
"Practical and accurate pinpointing of configuration errors using static analysis","Z. Dong; A. Andrzejak; K. Shao","Institute of Computer Science, Heidelberg University, Germany; Institute of Computer Science, Heidelberg University, Germany; School of Computer and Information, Hefei University of Technology, China","2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)","","2015","","","171","180","Software misconfigurations are responsible for a substantial part of today's system failures, causing about one-quarter of all customer-reported issues. Identifying their root causes can be costly in terms of time and human resources. We present an approach to automatically pinpoint such defects without error reproduction. It uses static analysis to infer the correlation degree between each configuration option and program sites affected by an exception. The only run-time information required by our approach is the stack trace of a failure. This is an essential advantage compared to existing approaches which require to reproduce errors or to provide testing oracles. We evaluate our approach on 29 errors from 4 configurable software programs, namely JChord, Randoop, Hadoop, and Hbase. Our approach can successfully diagnose 27 out of 29 errors. For 20 errors, the failure-inducing configuration option is ranked first.","","978-1-4673-7532-0978-1-4673-7531","10.1109/ICSM.2015.7332463","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7332463","Configuration errors;Static analysis;Automated debugging","Software;Correlation;Indexes;Accuracy;Computer science;Debugging;Computer crashes","program diagnostics;program testing;system recovery","configuration error pinpointing;static analysis;software misconfiguration;system failures;correlation degree;program sites;failure stack trace;testing oracles;JChord;Randoop;Hadoop;Hbase;failure-inducing configuration option","","6","38","","","","","","IEEE","IEEE Conferences"
"SDN dual-optimization application for EDFAs and WSS-based ROADMs","H. Carvalho; E. Magalhães; M. Garrich; N. Gonzalez; M. Nascimento; F. Margarido; L. Mariote; A. Bordonalli; J. Oliveira","CPqD Foundation, Rod. Campinas/Mogi Mirim, km 118.5, SP, Brazil; CPqD Foundation, Rod. Campinas/Mogi Mirim, km 118.5, SP, Brazil; CPqD Foundation, Rod. Campinas/Mogi Mirim, km 118.5, SP, Brazil; CPqD Foundation, Rod. Campinas/Mogi Mirim, km 118.5, SP, Brazil; CPqD Foundation, Rod. Campinas/Mogi Mirim, km 118.5, SP, Brazil; CPqD Foundation, Rod. Campinas/Mogi Mirim, km 118.5, SP, Brazil; CPqD Foundation, Rod. Campinas/Mogi Mirim, km 118.5, SP, Brazil; School of Electrical and Computer Engineering, UNICAMP, Campinas, SP, Brazil; CPqD Foundation, Rod. Campinas/Mogi Mirim, km 118.5, SP, Brazil","2015 Optical Fiber Communications Conference and Exhibition (OFC)","","2015","","","1","3","A novel SDN dual-optimization application based on EDFA adaptive gain control and WSS-based spectrum equalization is proposed. OSNR 5.7 dB gains for 80 DWDM 128-Gb/s channels are experimentally demonstrated in our metropolitan optical network test-bed.","","978-1-5575-2937","10.1364/OFC.2015.Th3J.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7121736","","Decision support systems;Erbium-doped fiber amplifiers;Attenuation;Optical fiber networks;Integrated optics;Stimulated emission","adaptive control;equalisers;gain control;metropolitan area networks;optical fibre networks;software defined networking;telecommunication channels","metropolitan optical network;DWDM channels;OSNR;WSS-based spectrum equalization;EDFA adaptive gain control;SDN dual-optimization application;WSS-based ROADM;EDFA;bit rate 128 Gbit/s;gain 5.7 dB","","2","5","","","","","","IEEE","IEEE Conferences"
"Separation of contactless captured high-resolution overlapped latent fingerprints: Parameter optimisation and evaluation","K. Qian; M. Schott; J. Dittmann","Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany; Faculty of Computer Science, Otto von Guericke University Magdeburg, Germany","2013 International Workshop on Biometrics and Forensics (IWBF)","","2013","","","1","4","Although overlapped latent fingerprints often occur in crime scenes, they usually can not directly be used as forensic evidence as it is difficult for the state-of-the-art fingerprint matchers to separate and match them. There exist separation techniques based on chemical imaging, but they require invasive fingerprint acquisition (e.g. applying chemicals) methods which not only compromise the integrity of potential evidence but also eliminate the possibility of analysing the fingerprints using other means (e.g. spectroscopy). Addressing this issue, in this work we use scanned samples of two overlapped latent fingerprints acquired noninvasively by a Chromatic White Light (CWL) sensor. To properly apply our previous separation approach adapted from Chen et al. on those samples, in this paper we conduct a thorough parameter optimisation. A test set of 20 authentic overlapped fingerprints is used. With a cross examination with 4 templates of 2 different kinds using NIST Biometric Image Software (NBIS), we obtain 160 samples from the matching test, which shows a clear increasing number of positive matching after the separation of the fingerprints and leads to an overall Equal Error Rate (EER) of 8.3%, which is lower than the results from Chen et al. where simulated samples are used.","","978-1-4673-4989-5978-1-4673-4987-1978-1-4673-4988","10.1109/IWBF.2013.6547316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6547316","latent fingerprints;fingerprint separation;biometrics;relaxation labelling","Abstracts;Labeling;Optimization;Visualization;Lead;Forensics","error statistics;fingerprint identification;image forensics;image matching;image sensors;optimisation","parameter optimisation;parameter evaluation;overlapped latent fingerprint separation;crime scenes;forensic evidence;chemical imaging;invasive fingerprint acquisition;fingerprint analysis;scanned sample;chromatic white light;CWL sensor;overlapped fingerprint authentication;NIST biometric image software;NBIS;fingerprint matching;equal error rate;EER","","","8","","","","","","IEEE","IEEE Conferences"
"Implementation and testing of optimal design of RTU hardware for Wireless SCADA","M. Aamir; M. A. Uqaili; J. Poncela; N. A. Khan; B. S. Chowdhry","Sir Syed University of Engineering &amp; Technology, Karachi. Pakistan and Mehran University of Engineering &amp; Technology, Jamshoro, Pakistan; Mehran University of Engineering &amp; Technology, Jamshoro, Pakistan; ETSI Telecommunication Engineering, University of Malaga, Spain; Advanced Engineering &amp; Research Organization, Karachi, Pakistan; Mehran University of Engineering &amp; Technology, Jamshoro, Pakistan","2014 4th International Conference on Wireless Communications, Vehicular Technology, Information Theory and Aerospace & Electronic Systems (VITAE)","","2014","","","1","5","Design of an optimal Remote Terminal Unit (RTU) is a key step in implementation of Wireless SCADA. This paper presents Implementation and Testing of an RTU design which is suitable for wide area operation essential for controlling and monitoring oil and gas sector, water and power industries. This particular implementation is based on FPGA which has resulted in reliable and reconfigurable RTU. It has significant advantages resulting in a more powerful and optimized solution for execution of wireless based SCADA. The main objective of this research work is to implement and verify a design considering performance parameters which assists in optimized development and inexpensive implementation of an RTU, also featured with wireless communication.","","978-1-4799-4624-2978-1-4799-4626","10.1109/VITAE.2014.6934498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6934498","Supervisory Control and Data Acquisition (SCADA);Remote Terminal Unit (RTU);Field Programmable Gate Array (FPGA);Communication Interface Module;hardware description language","Field programmable gate arrays;Hardware;Testing;SCADA systems;Registers;Wireless communication;Software","field programmable gate arrays;SCADA systems;telecontrol","optimal design;remote terminal unit;RTU hardware design;wireless SCADA;FPGA;wireless communication","","","10","","","","","","IEEE","IEEE Conferences"
"A novel educational platform, based on the Raspberry-Pi: Optimised to assist the teaching and learning of younger students","N. K. Ioannou; G. S. Ioannidis; G. D. Papadopoulos; A. E. Tapeinos","The Science Laboratory, School of Education, University of Patras, Greece; The Science Laboratory, School of Education, University of Patras, Greece; The Science Laboratory, School of Education, University of Patras, Greece; The Science Laboratory, School of Education, University of Patras, Greece","2014 International Conference on Interactive Collaborative Learning (ICL)","","2014","","","517","524","A novel 2-section educational learning environment was developed and is presented herein, together with the results of the first educational trial teaching Physics to 118, primary school, 10 year-olds. The present study is the proof-of-concept for a novel software platform, which has been designed and implemented for use with the Raspberry Pi hardware. The purpose of this platform is ""education using ICT"", as opposed to computer education, on which most other Pi-based projects concentrate. The Pi hardware is very small and of sufficiently low cost facilitating students to own for home use, in addition to its obvious use in schools, for which it is primarily intended. While the first section of the educational learning environment produced operates on the tiny Pi, the second section of the platform operates on an online server, and provides software support while allowing for administered exchange of educational material amongst educators - a novelty on its own. It incorporates rich text, multimedia, and custom applications. The first educational test presented herein proves the high suitability of the concept in the first place, and of the novel platform in particular. Although the target group of the present trial consists of primary school children, the ultimate target group of the platform (comprising the main research product of the present educational study) includes both primary and secondary education children. The platform would seem usable for teaching other subjects also, in addition to Science.","","978-1-4799-4437","10.1109/ICL.2014.7017826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017826","","Educational institutions;Materials;Software;Computers;Hardware;Collaborative work","computer aided instruction;educational institutions;physics education;teaching","educational platform;optimised Raspberry-Pi;younger students;teaching assistance;learning assistance;2-section educational learning environment;physics teaching;primary school;software platform;ICT;online server;software support;educational material;text applications;multimedia applications;custom applications;educational test;target group;primary school children;primary education children;secondary education children","","3","7","","","","","","IEEE","IEEE Conferences"
"Automating data reuse in High-Level Synthesis","W. Meeus; D. Stroobandt","Imec and Ghent University, Gent, Belgium; Ghent University, Gent, Belgium","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","4","Current High-Level Synthesis (HLS) tools perform excellently for the synthesis of computation kernels, but they often don't optimize memory bandwidth. As memory access is a bottleneck in many algorithms, the performance of the generated circuit will benefit substantially from memory access optimization. In this paper we present an automated method and a toolchain to detect reuse of array data in loop nests and to build hardware that exploits this data reuse. This saves memory bandwidth and improves circuit performance. We make use of the polyhedral representation of the source program, which makes our method computationally easy. Our software complements the existing HLS flows. Starting from a loop nest written in C, our tool generates a reuse buffer and a loop controller, and preprocesses the loop body for synthesis with an existing HLS tool. Our automated tool produces designs from unoptimized source code that are as efficient as those generated by a commercial HLS tool from manually-optimized source code.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800512","","Arrays;Buffer storage;Optimization;Hardware;Image edge detection;Algorithm design and analysis;Indexes","buffer storage;data handling;high level synthesis;source code (software);storage management","data reuse buffer generation;loop nests;array data;computation kernel synthesis;source program;manually-optimized source code;loop controller;reuse buffer;polyhedral representation;circuit performance;memory access optimization;memory access;HLS tools;high-level synthesis;data reuse automation","","","12","","","","","","IEEE","IEEE Conferences"
"ArchRanker: A ranking approach to design space exploration","T. Chen; Q. Guo; K. Tang; O. Temam; Z. Xu; Z. Zhou; Y. Chen","State Key Laboratory of Computer Architecture, Institute of Computing Technology (ICT), CAS, China; Carnegie Mellon University, United States; University of Science and Technology of China, China; Inria, France; State Key Laboratory of Computer Architecture, Institute of Computing Technology (ICT), CAS, China; National Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology (ICT), CAS, China","2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)","","2014","","","85","96","Architectural Design Space Exploration (DSE) is a notoriously difficult problem due to the exponentially large size of the design space and long simulation times. Previously, many studies proposed to formulate DSE as a regression problem which predicts architecture responses (e.g., time, power) of a given architectural configuration. Several of these techniques achieve high accuracy, though often at the cost of significant simulation time for training the regression models.We argue that the information the architect mostly needs during the DSEprocess is whether a given configuration will perform better than another one in the presences ofdesign constraints, or better than any other one seen so far, rather than precisely estimating the performance of that configuration. Based on this observation, we propose a novel rankingbased approach to DSE where we train a model to predict which of two architecture configurations will perform best. We show that, not only this ranking model more accurately predicts the relative merit of two architecture configurations than an ANN-based state-of-the-art regression model, but also that it requires much fewer training simulations to achieve the same accuracy, or that it can be used for and is even better at quantifying the performance gap between two configurations. We implement the framework for training and using this model, called ArchRanker, and we evaluate it on several DSE scenarios (unicore/multicore design spaces, and both time and power performance metrics). We try to emulate as closely as possible the DSE process by creating constraint-based scenarios, or an iterative DSEprocess. We find that ArchRanker makes 29.68% to 54.43% fewer incorrect predictions on pairwise relative merit of configurations (tested with 79,800 configuration pairs) than an ANN-based regression model across all DSE scenarios considered (values averaged over all benchmarks for each scenario). We also find that, to achieve the same accuracy as ArchRanker, the ANN often requires three times more training simulations.","1063-6897","978-1-4799-4394-4978-1-4799-4396","10.1109/ISCA.2014.6853198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853198","","Training;Predictive models;Multicore processing;Accuracy;Benchmark testing;Hidden Markov models","multiprocessing systems;neural net architecture;performance evaluation;regression analysis","ArchRanker;ranking approach;architectural design space exploration;regression problem;architecture response;architectural configuration;simulation time;fdesign constraints;architecture configuration;ANN-based state-of-the-art regression model;training simulation;unicore design space;multicore design space;power performance metrics;constraint-based scenario;iterative DSE process;ANN-based regression model","","3","45","","","","","","IEEE","IEEE Conferences"
"Time overcurrent relay coordination using the Levy flight Cuckoo search algorithm","S. S. Gokhale; V. S. Kale","Y.C.C.E, Nagpur, India; V.N.I.T, Nagpur, India","TENCON 2015 - 2015 IEEE Region 10 Conference","","2015","","","1","6","Over-current relays provide primary as well as backup protection to electrical distribution systems. These relays should be coordinated and set at the near optimum values, to minimize the total operating time and hence ensure that least damage is caused when a fault occurs. While they provide backup protection, it is also imperative to ensure that their settings should not cause their inadvertent operation and subsequent sympathy trips. This paper describes a Cuckoo Search algorithm for optimal time coordination of these relays. The algorithm has been implemented in MATLAB software and tested on several systems, out of which two have been illustrated in this paper. The results are verified by comparing it with the linear programming method. The novel feature of this paper is the application of the Cuckoo algorithm to the problem of over-current relay coordination.","2159-3450;2159-3442","978-1-4799-8641-5978-1-4799-8639-2978-1-4799-8640","10.1109/TENCON.2015.7372879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372879","Power System Protection;Over-current Relay coordination;Optimization;Cuckoo Search algorithm","Relays;Linear programming;Circuit faults;Plugs;Optimization;Birds;Software algorithms","overcurrent protection;relay protection;search problems","time overcurrent relay coordination;Levy flight Cuckoo search algorithm;optimal time coordination;MATLAB software;linear programming method","","1","22","","","","","","IEEE","IEEE Conferences"
"An improved bug localization using structured information retrieval and version history","S. Rahman; K. K. Ganguly; K. Sakib","Institute of Information Techology, University of Dhaka, Dhaka, Bangladesh; Institute of Information Techology, University of Dhaka, Dhaka, Bangladesh; Institute of Information Techology, University of Dhaka, Dhaka, Bangladesh","2015 18th International Conference on Computer and Information Technology (ICCIT)","","2015","","","190","195","Locating buggy files is a time consuming and challenging task because defects can deflate from a large variety of sources. So, researchers proposed several automated bug localization techniques where the accuracy can be improved. In this paper, an information retrieval based bug localization technique has been proposed, where buggy files are identified by measuring the similarity between bug report and source code. Besides this, source code structure and frequently changed files are also incorporated to produce a better rank for buggy files. To evaluate the proposed approach, a large-scale experiment on three open source projects, namely SWT, ZXing and Guava has been conducted. The result shows that the proposed approach improves 7% in terms of Mean Reciprocal Rank (MRR) and about 8% for Mean Average Precision (MAP) compared to existing techniques.","","978-1-4673-9930","10.1109/ICCITechn.2015.7488066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488066","Accuracy;Bug Localization;Information Retrieval;Vector Space Model","Computer bugs;Mathematical model;Software;Information retrieval;History;Semantics;Electronic mail","configuration management;information retrieval;program debugging;program testing;source code (software)","improved bug localization;structured information retrieval;version history;automated bug file localization techniques;similarity measurement;bug report;source code structure;buggy file ranking;open source projects;SWT project;ZXing project;Guava project;mean reciprocal rank;MRR;mean average precision;MAP","","3","12","","","","","","IEEE","IEEE Conferences"
"Hardware Trojans hidden in RTL don't cares — Automated insertion and prevention methodologies","N. Fern; S. Kulkarni; K. T. Cheng","University of California - Santa Barbara, ECE Department; University of California - Santa Barbara, ECE Department; University of California - Santa Barbara, ECE Department","2015 IEEE International Test Conference (ITC)","","2015","","","1","8","Don't cares in RTL code have long plagued chip verification due to hard-to-diagnose “X-bugs” resulting from ambiguous X simulation semantics, yet prevail in modern designs because of enormous opportunities for area/performance/power optimization during synthesis. We analyze don't cares specified at the RTL level from a security perspective and propose a novel class of Hardware Trojans which leak internal circuit node values using only existing design don't cares. Detection of this Trojan class is impossible using either functional simulation/verification or a perfect sequential equivalence checker. We then provide a formal automated X-analysis technique which both prevents the insertion of this new Trojan type and also has the potential to uncover accidental X-bugs as well. We provide several examples, including an Elliptic Curve Processor, illustrating both Trojan insertion and our prevention technique.","","978-1-4673-6578","10.1109/TEST.2015.7342387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7342387","","Trojan horses;Logic gates;Hardware;Hardware design languages;Solid modeling;Semantics;Payloads","invasive software;microprocessor chips;program verification;public key cryptography","elliptic curve processor;formal automated X-analysis;perfect sequential equivalence checker;functional simulation-verification;Trojan class;internal circuit node values;security perspective;ambiguous X simulation semantics;hard-to-diagnose X-bugs;chip verification;RTL code;prevention methodology;automated insertion;RTL don't cares;hardware Trojans","","10","29","","","","","","IEEE","IEEE Conferences"
"Hardware-in-the-loop simulation and energy optimization of cardiac pacemakers","C. Barker; M. Kwiatkowska; A. Mereacre; N. Paoletti; A. Patanè","University of Southampton, UK; Department of Computer Science, University of Oxford, UK; Department of Computer Science, University of Oxford, UK; Department of Computer Science, University of Oxford, UK; University of Catania, IT","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","7188","7191","Implantable cardiac pacemakers are medical devices that can monitor and correct abnormal heart rhythms. To provide the necessary safety assurance for pacemaker software, both testing and verification of the code, as well as testing the entire pacemaker hardware in the loop, is necessary. In this paper, we present a hardware testbed that enables detailed hardware-in-the-loop simulation and energy optimisation of pacemaker algorithms with respect to a heart model. Both the heart and the pacemaker models are encoded in Simulink/Stateflow™ and translated into executable code, with the pacemaker executed directly on the microcontroller. We evaluate the usefulness of the testbed by developing a parameter synthesis algorithm which optimises the timing parameters based on power measurements acquired in real-time. The experiments performed on real measurements successfully demonstrate that the testbed is capable of energy minimisation in real-time and obtains safe pacemaker timing parameters.","1094-687X;1558-4615","978-1-4244-9271-8978-1-4244-9270","10.1109/EMBC.2015.7320050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320050","","Pacemakers;Hardware;Optimization;Heart beat;Energy consumption","bioelectric potentials;cardiology;diseases;microcontrollers;minimisation;pacemakers;patient monitoring;physiological models","hardware-in-the-loop simulation;energy optimization;implantable cardiac pacemakers;medical devices;heart model;abnormal heart rhythm monitoring;microcontroller;parameter synthesis algorithm;energy minimisation","Arrhythmias, Cardiac;Computer Simulation;Humans;Models, Theoretical;Monitoring, Physiologic;Pacemaker, Artificial;Software","","13","","","","","","IEEE","IEEE Conferences"
"Robust, fast and optimal solution of practical economic dispatch by a new enhanced gradient-based simplified swarm optimisation algorithm","R. Azizipanah-Abarghooee; T. Niknam; M. Gharibzadeh; F. Golestaneh","Department of Electronic and Electrical Engineering, Shiraz University of Technology, Modares Boulevard, Shiraz, Iran; Department of Electronic and Electrical Engineering, Shiraz University of Technology, Modares Boulevard, Shiraz, Iran; Department of Electronic and Electrical Engineering, Shiraz University of Technology, Modares Boulevard, Shiraz, Iran; Department of Electronic and Electrical Engineering, Shiraz University of Technology, Modares Boulevard, Shiraz, Iran","IET Generation, Transmission & Distribution","","2013","7","6","620","635","Nowadays, the rising concern about the costs of fuel and operation of generating units in the energy control centre deserve progress of solution methodologies for the practical economic dispatch (ED) problems. ED aims to schedule the committed units' output power while satisfying practical constraints and load demand. The generator ramp rate limits, non-convex and discontinuous nature of prohibited operating zones, non-smooth characteristic of valve-point effects, multi-fuel type of generation units, transmission line losses and the large number of units in practical power plants make this problem very hard to solve. In this study, a new solution method of integrating the classical gradient-based optimisation technique and a new enhanced simplified swarm optimisation algorithm is comprehensively presented and successfully applied to determine the feasible, robust, fast and globally or near-globally optimal solution within a rapid timeframe for the ED problems. The simulations are carried out on four-test systems, including 10-, 15-, 40- and 80-units using the proposed optimisation technique in the Fortran Power Station 4.0 software. The current proposal outperforms other methods showcased in the recent state-of-the art literature in the area.","1751-8687;1751-8695","","10.1049/iet-gtd.2012.0616","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6542291","","","gradient methods;particle swarm optimisation;power generation dispatch;power generation economics;power plants","economic dispatch;enhanced gradient-based simplified swarm optimisation algorithm;energy control centre;ED problems;load demand;generator ramp rate limits;nonconvex nature;discontinuous nature;operating zones;nonsmooth characteristic;valve-point effects;multifuel type;generation units;transmission line losses;power plants;four-test systems;Fortran Power Station 4.0 software;80-units;40-units;15-units;10-units","","9","36","","","","","","IET","IET Journals & Magazines"
"New possibilities of industrial programming software","P. Seman; M. Juhás; L. Tkáč; M. Honek","Institute of Automation Measurement and Applied Informatics, Faculty of Mechanical Engineering, Slovak University of Technology in Bratislava, Nám. Slobody 17, 812 31, Slovakia; Institute of Automation Measurement and Applied Informatics, Faculty of Mechanical Engineering, Slovak University of Technology in Bratislava, Nám. Slobody 17, 812 31, Slovakia; Institute of Automation Measurement and Applied Informatics, Faculty of Mechanical Engineering, Slovak University of Technology in Bratislava, Nám. Slobody 17, 812 31, Slovakia; Institute of Automation Measurement and Applied Informatics, Faculty of Mechanical Engineering, Slovak University of Technology in Bratislava, Nám. Slobody 17, 812 31, Slovakia","2013 International Conference on Process Control (PC)","","2013","","","474","479","Recent developments in the area of industrial programing software expand the potential of the automation programs considerably. In addition to IEC 61131-3 languages, programming languages like C and C++ are getting available for industrial controllers, which opens new possibilities of reusing existing programs for real time processing in these controllers. Paper deals with swing-up and online model predictive control of mechatronic device - reaction wheel inverted pendulum. Swing-up control uses convenient shape of exponential function over pendulum position for determination of required energy to be delivered to the system. For stabilization of pendulum in the upright position a predictive controller based on optimal control law with perturbation was used. An important part of the solution is PLC implementation of qpOASES package for numerical solution of quadratic optimization problem. Application is implemented and tested using industrial PLC connected to a laboratory pendulum.","","978-1-4799-0927-8978-1-4799-0926","10.1109/PC.2013.6581456","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6581456","industrial software;reaction wheel pendulum;predictive control;PLC","Wheels;Mathematical model;Automation;Process control;Cost function;Software packages","control engineering computing;industrial control;nonlinear control systems;optimal control;pendulums;predictive control;production engineering computing;programmable controllers;programming languages;real-time systems;stability;wheels","laboratory pendulum;industrial PLC;quadratic optimization problem;qpOASES package;optimal control law;pendulum stabilization;exponential function;swing-up control;mechatronic device;online model predictive control;real time processing;industrial controllers;programming languages;IEC 61131-3 languages;automation programs;reaction wheel inverted pendulum;industrial programming software","","1","12","","","","","","IEEE","IEEE Conferences"
"Occlusion handling in depth estimation from multiview video","K. Wegner; O. Stankiewicz; M. Domański","Chair of Multimedia Telecommunication and Microelectronics Poznan University of Technology Poznań, Poland; Chair of Multimedia Telecommunication and Microelectronics Poznan University of Technology Poznań, Poland; Chair of Multimedia Telecommunication and Microelectronics Poznan University of Technology Poznań, Poland","2014 International Conference on Signals and Electronic Systems (ICSES)","","2014","","","1","4","This paper presents a novel approach to occlusion handling problem in depth estimation using three views. A solution based on modification of similarity cost function in optimization algorithm (on example of graph cuts) is proposed. During depth estimation via optimization algorithms like graph cuts similarity cost function is constantly updated so that only non-occluded pixels in the side views are considered. For the side views, virtual depth maps are synthesized and occluded regions are detected. Basing on that, similarity cost function is updated for correspondence search only in non-occluded regions of the side views. The experimental results, performed with use of a well-known 3D video test sequences, show that the proposed approach in application for virtual view synthesis for next generation of 3D-television, provides gains of about 1.25 dB of PSNR related to the state-of-the-art technique implemented in MPEG Depth Estimation Reference Software.","","978-1-4799-7009","10.1109/ICSES.2014.6948712","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6948712","depth estimation;occlusion handling;MVD;graph cuts","Estimation;Density estimation robust algorithm;Three-dimensional displays;Software;Optimization;Transform coding;Measurement","graph theory;image sequences;optimisation;video signal processing","depth estimation;multiview video;occlusion handling problem;optimization algorithm;graph cuts similarity cost function;nonoccluded pixels;occluded regions;video test sequences;PSNR;state-of-the-art technique;MPEG depth estimation reference software","","1","15","","","","","","IEEE","IEEE Conferences"
"Performance optimization of Hadoop cluster using linux services","H. Ahmed; M. A. Ismail; M. F. Hyder","Department of Computer &amp; Information Systems Engineering, NED University of Engineering &amp; Technology, University Road, Karachi-75270, Pakistan; Department of Computer &amp; Information Systems Engineering, NED University of Engineering &amp; Technology, University Road, Karachi-75270, Pakistan; Department of Computer &amp; Information Systems Engineering, NED University of Engineering &amp; Technology, University Road, Karachi-75270, Pakistan","17th IEEE International Multi Topic Conference 2014","","2014","","","167","172","Hadoop is an open source tool. It enables the processing and distributed storage of big data sets using commodity cluster computing. With Hadoop occupying a core status in the current processing era, its performance optimization is also being heavily studied. This paper introduces one such method to improve Hadoop cluster performance by using a Remote Procedure Call (RPC), rpcbind service of the Linux system. The comparison is done by executing multiple Hadoop benchmarks on a configured multi-node Hadoop cluster. The final outcome turns in rpcbind favor depicting how the service improves the cluster performance by reducing the elapsed time of the benchmark executed.","","978-1-4799-5755-2978-1-4799-5754-5978-1-4799-5753","10.1109/INMIC.2014.7097331","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7097331","","Benchmark testing;Linux;Servers;File systems;Measurement;Java;Distributed databases","Big Data;Linux;optimisation;parallel processing;public domain software;remote procedure calls;workstation clusters","Linux services;open source tool;distributed storage;Big Data sets;commodity cluster computing;performance optimization;Hadoop cluster performance;remote procedure call;RPC;rpcbind service;Linux system;multinode Hadoop cluster","","4","16","","","","","","IEEE","IEEE Conferences"
"Evaluation of the Huffman Encoding for Memory Optimization on Hardware Network Intrusion Detection","E. Freire; L. Schnitman; W. Oliveira; A. Duarte","NA; NA; NA; NA","2013 III Brazilian Symposium on Computing Systems Engineering","","2013","","","131","136","The design of specialized hardware for Network Intrusion Detection has been subject of intense research over the last decade due to its considerably higher performance compared to software implementations. In this context, one of the limiting factors is the finite amount of memory resources versus the increasing number of threat patterns to be analyzed. This paper proposes an architecture based on the Huffman algorithm for encoding, storage and decoding of these patterns in order to optimize such resources. We have made tests with simulation and synthesis in FPGA of rule subsets of the Snort software, and analysis indicate a saving of up to 73 percent of the embedded memory resources of the chip.","2324-7894;2324-7886","978-1-4799-3890","10.1109/SBESC.2013.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825355","network intrusion detection;FPGA;memory optimization;Huffman encoding","Encoding;Decoding;Hardware;Memory management;Intrusion detection;Software","computer network security;decoding;field programmable gate arrays;Huffman codes;network coding","Huffman encoding evaluation;memory optimization;hardware network intrusion detection;pattern decoding;FPGA;Snort software;embedded memory resources;computer network security","","2","17","","","","","","IEEE","IEEE Conferences"
"Shared storage performance of cloud computing: OpenNebula case","A. A. Russkov","Scientific Center Chernogolovka RAS, 142432, Russia","2014 IEEE 3rd International Conference on Cloud Networking (CloudNet)","","2014","","","275","277","We report on the testing of write operation in cloud for OpenNebula package. Results obtained for writing small blocks with zero delay show stochastic behaviour, which could be explained by compound influence of optimization technologies, such as caching, used in hypervisor.","","978-1-4799-2730","10.1109/CloudNet.2014.6969007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6969007","OpenNebula;storage performance;hypervisor;stochastic behaviour;statistical analysis","Delays;Stochastic processes;Virtual machine monitors;Standards;Conferences;Optimization;Reliability","cloud computing;software packages;stochastic processes;virtual machines","shared storage performance;cloud computing;write operation testing;OpenNebula package;stochastic behaviour;optimization technologies;caching;hypervisor","","","4","","","","","","IEEE","IEEE Conferences"
"New Insights Into Diversification of Hyper-Heuristics","Z. Ren; H. Jiang; J. Xuan; Y. Hu; Z. Luo","School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; INRIA Lille–Nord Europe, Lille, France; School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China","IEEE Transactions on Cybernetics","","2014","44","10","1747","1761","There has been a growing research trend of applying hyper-heuristics for problem solving, due to their ability of balancing the intensification and the diversification with low level heuristics. Traditionally, the diversification mechanism is mostly realized by perturbing the incumbent solutions to escape from local optima. In this paper, we report our attempt toward providing a new diversification mechanism, which is based on the concept of instance perturbation. In contrast to existing approaches, the proposed mechanism achieves the diversification by perturbing the instance under solving, rather than the solutions. To tackle the challenge of incorporating instance perturbation into hyper-heuristics, we also design a new hyper-heuristic framework HIP-HOP (recursive acronym of HIP-HOP is an instance perturbation-based hyper-heuristic optimization procedure), which employs a grammar guided high level strategy to manipulate the low level heuristics. With the expressive power of the grammar, the constraints, such as the feasibility of the output solution could be easily satisfied. Numerical results and statistical tests over both the Ising spin glass problem and the p-median problem instances show that HIP-HOP is able to achieve promising performances. Furthermore, runtime distribution analysis reveals that, although being relatively slow at the beginning, HIP-HOP is able to achieve competitive solutions once given sufficient time.","2168-2267;2168-2275","","10.1109/TCYB.2013.2294185","Fundamental Research Funds for the Central Universities; New Century Excellent Talents in University; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690192","Hyper-heuristics;instance perturbation;Ising spin glass;linear genetic programming;p-median;Hyper-heuristics;instance perturbation;Ising spin glass;linear genetic programming;$p$ -median","Glass;Linear programming;Search problems;Vectors;Grammar;Problem-solving;Genetic programming","heuristic programming;optimisation","hyper-heuristics diversification;problem solving;diversification mechanism;instance perturbation concept;HIP-HOP framework;instance perturbation-based hyper-heuristic optimization procedure;Ising spin glass problem;p-median problem","","8","54","","","","","","IEEE","IEEE Journals & Magazines"
"Electrical design analysis and breakdown voltage test aspects of indigenously developed electrical breaks at cryo temperatures","R. Sharma; V. L. Tanna; A. Amardas; S. Pradhan; S. Chandramouli","Institute for Plasma Research, Bhat, Near Indira Bridge, Gandhinagar-382428, India; Institute for Plasma Research, Bhat, Near Indira Bridge, Gandhinagar-382428, India; Institute for Plasma Research, Bhat, Near Indira Bridge, Gandhinagar-382428, India; Institute for Plasma Research, Bhat, Near Indira Bridge, Gandhinagar-382428, India; M/s Uniglass Industries Pvt. Ltd., Bangalore, India","2014 International Symposium on Discharges and Electrical Insulation in Vacuum (ISDEIV)","","2014","","","61","64","Electrical insulation breaks are very critical component of large-scale fusion devices employing superconducting magnets and are used to insulate electrically the coil and the cryogenic supply line (kept at ground potential). The electrical insulation of breaks consists of the composites insulations, G-10 GFRP (Glass fiber reinforced plastic), cryogenic compatible epoxy and S-glass boron free fiber in air and helium environment under vacuum. To optimize the design parameters, the detailed electrical analysis have been carried out using the electrostatic solid axi-symmetric model by COMSOL 4.1 multiphysics software for electrodes separation, electrical field strength as well as flashover phenomenon between two electrodes with composite insulator material have been analyzed at different surfaces and contours along with by varying gaps between SS electrodes. We have carried out the high voltage breakdown test (0-35) kV, insulation resistance measurement at 0-5 kV test voltage in Paschen condition of (10<sup>3</sup>-10<sup>-5</sup>) mbar in helium gas and air environment at 300 K and 77 K experimentally on in-house developed insulation breaks.","1093-2941","978-1-4799-6752-0978-1-4799-6750","10.1109/DEIV.2014.6961619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6961619","","Insulation;Electric fields;Helium;Electrodes;Electron tubes;Cryogenics","cryogenics;electric breakdown;glass fibre reinforced plastics;insulating materials;polymer insulators","air environment;helium gas;insulation resistance measurement;insulator material;flashover phenomenon;electrical field strength;electrodes separation;COMSOL 4.1 multiphysics software;electrostatic solid axi-symmetric model;S-glass boron free fiber;cryogenic compatible epoxy;glass fiber reinforced plastic;, G-10 GFRP;composites insulations;cryogenic supply line;superconducting magnets;large-scale fusion devices;electrical insulation breaks;cryo temperatures;indigenously developed electrical breaks;breakdown voltage test aspects;electrical design analysis","","","5","","","","","","IEEE","IEEE Conferences"
"Application of phased array ultrasonic inspection for EMUs wheel-set online","Y. Zhang; J. Peng; X. Gao; C. Peng","NDT Research Center, College of Physical Science and Technology, Southwest Jiaotong University, Chengdu, China; NDT Research Center, College of Physical Science and Technology, Southwest Jiaotong University, Chengdu, China; NDT Research Center, College of Physical Science and Technology, Southwest Jiaotong University, Chengdu, China; NDT Research Center, College of Physical Science and Technology, Southwest Jiaotong University, Chengdu, China","2014 IEEE Far East Forum on Nondestructive Evaluation/Testing","","2014","","","168","171","Based on application conditions of CRH EMUs wheel-set, a more strictly ultrasonic inspection for online wheel-set is required. To find an optimized solution, phased array ultrasonic testing technology mixed with conventional ultrasonic testing technology is studied. In this article, multi probes arrangement on wheel tread and inner wheel rim are designed, and their ultrasonic fields in wheel rim &amp; wheel disk and defect response are simulated based on CIVA software. Concerning on the special structure of CRH wheel, defect location recognition algorithms in depth and angle are deduced, and they are verified by reference wheel. The inspection feasibility and ability of those designs are proved by huge amount of real CRH wheel-set tests in field application.","","978-1-4799-4730-0978-1-4799-4731-7978-1-4799-6286","10.1109/FENDT.2014.6928255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928255","phased array;ultrasonic inspection;CRH wheel;probe arrangement;defect recognition","Wheels;Acoustics;Probes;Inspection;Arrays;Mathematical model;Equations","antenna phased arrays;automatic optical inspection;wheels","defect location recognition algorithm;CIVA software;wheel disk;wheel rim;wheel tread;optimized solution;CRH EMU wheel-set online;phased array ultrasonic inspection","","","12","","","","","","IEEE","IEEE Conferences"
"Secure cloud storage and search scheme for mobile devices","A. Awad; A. Matthews; B. Lee","Irish Centre for Cloud Computing and Commerce, Software Research Institute, Athlone Institute of Technology, Athlone, Ireland; Irish Centre for Cloud Computing and Commerce, Software Research Institute, Athlone Institute of Technology, Athlone, Ireland; Irish Centre for Cloud Computing and Commerce, Software Research Institute, Athlone Institute of Technology, Athlone, Ireland","MELECON 2014 - 2014 17th IEEE Mediterranean Electrotechnical Conference","","2014","","","144","150","In this paper, we consider the problem of securing mobile-cloud storage services. We design and develop a novel and secure searchable encryption scheme for mobile devices. In the proposed algorithm, a content based index is constructed and a locality sensitive hashing is used to allow the user to store his files on the cloud in a secure way and then perform a secure query and a fuzzy search. Moreover, the scheme allows to rank the results while maintaining the privacy and confidentiality of the user and to save storage space and computation resources of his mobile phone. To quantify the performance of the proposed scheme, a range of tests are performed and experimental results demonstrate that the proposed system is suitable for a secure, fast and searchable storage mobile-cloud system.","2158-8473;2158-8481","978-1-4799-2337","10.1109/MELCON.2014.6820522","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820522","Cloud;Security;Searchable encryption;Locality sensitive hashing;Fuzzy;Ranking","Indexes;Encryption;Mobile handsets;Keyword search;Servers;Software","cloud computing;cryptography;data privacy;fuzzy set theory;mobile computing;query processing;smart phones;storage management","secure cloud search scheme;mobile devices;mobile-cloud storage service security;secure searchable encryption scheme;content based index;locality sensitive hashing;file storage;secure query;fuzzy search;user privacy;user confidentiality;computation resources;secure-searchable storage mobile-cloud system;secure cloud storage scheme","","1","17","","","","","","IEEE","IEEE Conferences"
"Optimization of Vivaldi antenna for microwave imaging applications","B. N. Abhijith; M. J. Akhtar","Department of Electronics Engineering, Rajiv Gandhi University of Knowledge Technology, Andhra Pradesh, India; Department of Electrical Engineering, Indian Institute of Technology Kanpur, Uttar Pradesh, India","2014 IEEE Antennas and Propagation Society International Symposium (APSURSI)","","2014","","","1596","1597","Microwave imaging of dielectric media plays a major role in non-destructive testing, remote sensing, medical imaging etc. The procedure usually requires the measurement of reflection coefficient data of the object under test in free space over a wide frequency band which demands an ultra-wide band antenna. The design of a UWB antipodal Vivaldi antenna through optimization working in a frequency band of 1-18 GHz is discussed in this paper. The design and optimization process is carried out using the full wave EM simulation software, the CST microwave studio and various parameters such as return loss, radiation pattern, directivity and input impedance are computed. The simulated design is validated by fabricating a number of antennas on FR-4 substrate and measuring their return loss as well as radiation characteristics.","1522-3965;1947-1491","978-1-4799-3540-6978-1-4799-3538-3978-1-4799-3537","10.1109/APS.2014.6905124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6905124","","Microwave antennas;Antenna measurements;Microwave measurement;Vivaldi antennas;Microwave imaging;Antenna radiation patterns","antenna radiation patterns;dielectric materials;electromagnetic wave reflection;microwave antennas;microwave imaging;UHF antennas;ultra wideband antennas","microwave imaging;dielectric media;nondestructive testing;remote sensing;medical imaging;reflection coefficient data;ultrawide band antenna;UWB antipodal Vivaldi antenna;full wave EM simulation software;CST microwave studio;return loss;radiation pattern;directivity;input impedance;FR-4 substrate;radiation characteristics;frequency 1 GHz to 18 GHz","","2","6","","","","","","IEEE","IEEE Conferences"
"Efficient Work-Stealing with Blocking Deques","L. Chi; S. Ping; L. Yi; H. Qinfen","NA; NA; NA; NA","2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)","","2014","","","149","152","Work stealing is a popular and effective approach to implement load balancing in modern multi-/many-core systems, where each parallel thread has its local deque to maintain its own work-set of tasks and performs load balancing by stealing tasks from other deques. Unfortunately, the existing concurrent deques have two limitations. Firstly, these algorithms require memory fences in the owner's critical path operations to ensure correctness, which is expensive in modern weak-memory architectures. Secondly, the concurrent deques are difficult to extend to support various flexible forms of task distribution strategies, which can be more sufficient to optimize computation in some special applications, such as steal-half strategy in solving large, irregular graph problems. This paper proposes a blocking work-stealing deque. We optimize work stealing task deques through effective ways of accessing the deques to decrease the synchronization overhead. These ways can reduce the frequency of races when different threads need to operate on the same deque, especially using massive threads. We present implementation of the algorithm as a C++ library and the experiment results show that it behaves well to Cilk plus on a series of benchmarks. Since our approach relies on blocking deques, it is easy to extend to support flexible task creation and distribution strategies and also reduces the memory fences impact on performance.","","978-1-4799-6123-8978-1-4799-6122","10.1109/HPCC.2014.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056731","work stealing;load balancing;deque;scheduling strategies","Algorithm design and analysis;Instruction sets;Synchronization;Benchmark testing;Processor scheduling;Containers","C++ language;multi-threading;resource allocation;software libraries","blocking work-stealing deque;synchronization overhead;C++ library;Cilk plus;memory fences impact reduction;load balancing;multicore systems;parallel thread;many-core systems","","","10","","","","","","IEEE","IEEE Conferences"
"Implementation of subset query form scalar in collections searching feature inside digital library system (Case study library of research center for biology cibinong bogor)","A. Hisyam; Arini; I. M. Shofi","Informatics Engineering, Faculty of Science and Technology, Islamic State University (UIN) Syarif Hidayatullah Jakarta, Jl. Ir. H. JuandaNo. 95, Ciputat 15412; Informatics Engineering, Faculty of Science and Technology, Islamic State University (UIN) Syarif Hidayatullah Jakarta, Jl. Ir. H. JuandaNo. 95, Ciputat 15412; Informatics Engineering, Faculty of Science and Technology, Islamic State University (UIN) Syarif Hidayatullah Jakarta, Jl. Ir. H. JuandaNo. 95, Ciputat 15412","2014 International Conference on Cyber and IT Service Management (CITSM)","","2014","","","48","53","Research Center for Biology LIPI as research institutions have made use of ICT in improving the function of libraries with digital library system named BIOLIB. But because it was his application biolib many features that are not working properly. Therefore, Research Center for Biology require a new digital library system that can search a collection, booking reference, report the results of a reservation, keyword search statistics, data migration feature to the benefit plan using the new system and the system is integrated with library management information system. Data collection Puslit biology library had about 80,000 data records with big data, it's require to optimization. One of methods optimization is Cost Based models subset query. Methods subset of queries in the form of scalar successfully implemented into the search feature collections in the digital library system.","","978-1-4799-7975-2978-1-4799-7973","10.1109/CITSM.2014.7042174","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042174","","Libraries;Software;Testing;Biology;Databases;Unified modeling language;Bibliographies","Big Data;bioinformatics;digital libraries;optimisation;query processing","subset query form;digital library system;BIOLIB;library management information system;Puslit biology library;Big Data;optimization method","","","6","","","","","","IEEE","IEEE Conferences"
"Actor classification using actor machines","G. Cedersj&#x00F6;; J. W. Janneck","Department of Computer Science, Lund University, Sweden; Department of Computer Science, Lund University, Sweden","2013 Asilomar Conference on Signals, Systems and Computers","","2013","","","1801","1804","Program analysis is an important tool in software development, both for verifying desired properties and for enabling optimizations. For dataflow programs, properties such as determinacy and static schedulability are important for verifying correctness and creating efficient implementations. In this paper we develop an analyzer for dataflow actors, the computational units of a dataflow program, that classifies actors based on these properties. The analysis is performed on a language independent model for dataflow actors called actor machine.","1058-6393","978-1-4799-2390-8978-1-4799-2388","10.1109/ACSSC.2013.6810612","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6810612","","Ports (Computers);Computational modeling;Educational institutions;Testing;Computers;Abstracts;Schedules","data flow analysis;pattern classification;software engineering","actor classification;actor machines;program analysis;software development;optimizations;dataflow programs;determinacy;static schedulability;dataflow actors;language independent model","","1","11","","","","","","IEEE","IEEE Conferences"
"Spectrum-Based Fault Localization for Spreadsheets: Influence of Correct Output Cells on the Fault Localization Quality","B. Hofer","NA","2014 IEEE International Symposium on Software Reliability Engineering Workshops","","2014","","","263","268","Spreadsheets used in companies often contain several thousand formulas. The localization of faulty cells in such large spreadsheets could be time-consuming and frustrating. Spectrum-based fault localization (SFL) supports users in faster locating the faulty cell (s). However, SFL depends on the information the user provides. In this paper, we address three research questions in this context: (RQ1) Do spreadsheets contain correct output cells that positively or negatively influence the ranking of the faulty cells? (RQ2) If yes, is it possible to a-priori determine which correct output cells would positively influence the ranking? (RQ3) Is it possible to avoid a decreasing fault localization quality when adding more correct output cells? This paper shows that there exist correct output cells which positively or negatively influence the ranking. In particular, correct output cells with the largest cones positively influence the ranking of the faulty cell. Balancing the relation of correct and erroneous output cells by duplicating the cones of erroneous output cells improves the fault localization quality.","","978-1-4799-7377","10.1109/ISSREW.2014.100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983850","Spreadsheets;Spectrum-based Fault Localizatio;Test Case Selection Strategies;Debugging","Circuit faults;Debugging;Measurement;Vectors;Companies;Computer languages;Software reliability","fault tolerant computing;program debugging;program testing;spreadsheet programs","spectrum-based fault localization;spreadsheets;correct output cells;fault localization quality","","","21","","","","","","IEEE","IEEE Conferences"
"Multifrequency Excitation and Support Vector Machine Regressor for ECT Defect Characterization","A. Bernieri; G. Betta; L. Ferrigno; M. Laracca; S. Mastrostefano","Department of Electrical and Information Engineering, Maurizio Scarano University of Cassino and Southern Lazio, Cassino, Italy; Department of Electrical and Information Engineering, Maurizio Scarano University of Cassino and Southern Lazio, Cassino, Italy; Department of Electrical and Information Engineering, Maurizio Scarano University of Cassino and Southern Lazio, Cassino, Italy; Department of Electrical and Information Engineering, Maurizio Scarano University of Cassino and Southern Lazio, Cassino, Italy; Department of Electrical and Information Engineering, Maurizio Scarano University of Cassino and Southern Lazio, Cassino, Italy","IEEE Transactions on Instrumentation and Measurement","","2014","63","5","1272","1280","Eddy current testing (ECT) has three main tasks: detection, location, and characterization of defects. The characterization task, which means the ability to find the geometrical characteristics of the defect, is still in the research domain, although in many industrial applications this task has to be carried out with good accuracy to allow reliable acceptance or rejection decision indispensable to save costs and even human lives. This paper proposes an ECT measurement method that allows the reliable estimation of the geometrical characteristics of thin defects (i.e., length, height, and depth) by using a combination of a multifrequency excitation and an optimized support vector machine for regression. The proposed solution is tested on real specimens with known cracks by using a suitable measurement setup comprising a giant magnetoresistance-based biaxial ECT probe.","0018-9456;1557-9662","","10.1109/TIM.2013.2292326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6679272","Defect characterization;giant magnetoresistance (GMR) sensor;multifrequency eddy current testing (ECT);nondestructive testing (NDT);signal processing;support vector machine","Probes;Coils;Software;Data mining;Feature extraction;Reliability;Materials","cracks;eddy current testing;giant magnetoresistance;magnetoresistive devices;regression analysis;support vector machines","multifrequency excitation;support vector machine regressor;ECT defect characterization;eddy current testing;geometrical characteristics;thin defects;cracks;giant magnetoresistance-based biaxial ECT probe","","19","36","","","","","","IEEE","IEEE Journals & Magazines"
"RSVM: A Region-based Software Virtual Memory for GPU","F. Ji; H. Lin; X. Ma","Department of Computer Science, North Carolina State University, Raleigh, USA; Department of Computer Science, Virginia Tech, Blacksburg, USA; Department of Computer Science, North Carolina State University, Raleigh, USA","Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques","","2013","","","269","278","While Graphics Processing Units (GPU) have gained much success in general purpose computing in recent years, their programming is still difficult, due to, particularly, explicitly managed GPU memory and manual CPU-GPU data transfer. Despite recent calls for managing GPU resources as first-class citizens in the operating system, a mature GPU memory management mechanism is still missing, which leads to reinventing the wheels in various GPU system software. Meanwhile, due to ever enlarging problem sizes, we urgently need a system-level mechanism for unified CPU-GPU memory management. In this work, we present the design of Region-based Software Virtual Memory (RSVM), a software virtual memory running on both CPU and GPU in a distributed and cooperative way. In addition to automatic GPU memory management and GPU-CPU data transfer, RSVM offers two novel features: 1) GPU kernel-issued on-demand data fetching from the host into the GPU memory, and 2) intra-kernel transparent GPU memory swapping into the main memory. Our study reveals important insights on the challenges and opportunities of building unified virtual memory systems for heterogeneous computing. Experimental results on real GPU benchmarks demonstrate that, though it incurs a small overhead, RSVM can transparently scale GPU kernels to large problem sizes exceeding the device memory size limit; developers write the same code for different problem sizes, but still can optimize on data layout definition accordingly. Our evaluation also identifies missing GPU architecture features for better system software efficiency.","1089-795X","978-1-4799-1021-2978-1-4799-1018","10.1109/PACT.2013.6618823","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618823","GPGPU;Heterogeneous System;GPU Memory Mangement","Graphics processing units;Memory management;Instruction sets;Synchronization;Data transfer;Kernel","benchmark testing;graphics processing units;memory architecture;multiprocessing systems;operating systems (computers);resource allocation;virtual storage","system software efficiency;missing GPU architecture;data layout;device memory size limit;real GPU benchmarks;heterogeneous computing;on-demand data fetching;GPU kernel;main memory;intrakernel transparent GPU memory swapping;automatic GPU memory management;unified CPU-GPU memory management;GPU system software;operating system;GPU resource management;GPU memory management mechanism;manual CPU-GPU data transfer;general purpose computing;graphics processing units;region-based software virtual memory;RSVM","","3","32","","","","","","IEEE","IEEE Conferences"
"Benchmarking Internet of things devices","C. P. Kruger; G. P. Hancke","Advanced Sensor Networks Research Group, Counsil for Scientific and Industrial Research, South Africa; Department of Electrical, Electronic and Computer Engineering, University of Pretoria, South Africa","2014 12th IEEE International Conference on Industrial Informatics (INDIN)","","2014","","","611","616","The use of commercial off-the-shelf components for implementing Internet of Things devices has become a common practice amongst researchers and solution providers. IOT solutions, based on the Raspberry Pi, BeagleBone and BeagleBone Black, offer cost effective, versatile and uncomplicated platforms for rapid application development. The devices are treated as black box devices and little work has been done to quantify the performance of these devices when the system architecture, software components or communication channels are varied. This paper introduces micro- and macro-benchmarking methods for these devices; quantifying the performance of each device for the varying hardware architectures. Micro-benchmarking was performed using lmbench - a cross platform benchmarking framework for UNIX devices. The macro-benchmarking was implemented using a custom developed CoAP benchmarking utility created using the libCoAP library. The results showed that the selection of the platform processor is a key design requirement and has the most potential to optimise CoAP server performance. The latency associated with the communication channels was found to be a dominating factor for round-trip times associated with CoAP requests.","1935-4576;2378-363X","978-1-4799-4905","10.1109/INDIN.2014.6945583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945583","Internet of Things;Wireless Sensor Networks;Commercial off-the-shelf;Benchmarking;CoAP","Logic gates;Performance evaluation;Wireless sensor networks;Linux;Graphical user interfaces;Computer architecture;Software","benchmark testing;computer architecture;Internet of Things;internetworking;software libraries;telecommunication equipment testing;Unix;wireless sensor networks","Internet of Things devices;commercial off-the-shelf components;IOT solutions;Raspberry Pi;BeagleBone Black;black box devices;devices performance;system architecture;software components;communication channels;microbenchmarking methods;macrobenchmarking methods;hardware architectures;lmbench;UNIX devices;CoAP benchmarking utility;libCoAP library;platform processor;CoAP server performance;CoAP requests;wireless sensor networks;IOT gateway device","","40","10","","","","","","IEEE","IEEE Conferences"
"Automated Classification of Static Code Analysis Alerts: A Case Study","U. Yüksel; H. Sözer","NA; NA","2013 IEEE International Conference on Software Maintenance","","2013","","","532","535","Static code analysis tools automatically generate alerts for potential software faults that can lead to failures. However, developers are usually exposed to a large number of alerts. Moreover, some of these alerts are subject to false positives and there is a lack of resources to inspect all the alerts manually. To address this problem, numerous approaches have been proposed for automatically ranking or classifying the alerts based on their likelihood of reporting a critical fault. One of the promising approaches is the application of machine learning techniques to classify alerts based on a set of artifact characteristics. In this work, we evaluate this approach in the context of an industrial case study to classify the alerts generated for a digital TV software. First, we created a benchmark based on this code base by manually analyzing thousands of alerts. Then, we evaluated 34 machine learning algorithms using 10 different artifact characteristics and identified characteristics that have a significant impact. We obtained promising results with respect to the precision of classification.","1063-6773","978-0-7695-4981","10.1109/ICSM.2013.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676950","alert classification;industrial case study;static code analysis","Machine learning algorithms;Inspection;Accuracy;Benchmark testing;Middleware;History","learning (artificial intelligence);pattern classification;program diagnostics;software reliability","static code analysis tools;digital TV software;artifact characteristics;machine learning techniques;potential software faults;static code analysis alerts;automated classification","","5","17","","","","","","IEEE","IEEE Conferences"
"Decision support system prototype on obstetrics ultrasonography for primary service physicians","B. S. Sabarguna; S. K. Wijaya; F. Sakinah; A. Maryati","Biomedical Engineering Study Program, Universitas Indonesia; Biomedical Engineering Study Program, Universitas Indonesia; Biomedical Engineering Study Program, Universitas Indonesia; Biomedical Engineering Study Program, Universitas Indonesia","2015 4th International Conference on Instrumentation, Communications, Information Technology, and Biomedical Engineering (ICICI-BME)","","2015","","","226","231","Introduction. The National Social Security System (SJSN) prioritizes primary service as a spearhead to assist Primary Care Physicians to make medical decisions. The purpose of this research is to develop computer software that will assist primary care physicians in the fields of Obstetrics Ultrasonography, related to referral decision-making abilities. Methods. A quasi-experimental post-test only design without a control group. The stages of the research process: Systems Analysis and Design, Prototyping and Testing by Lecture, Students, Programmers and Doctors. Results. From Analysis and Systems design document, has been produced prototype of software, and a test run has been proven successful Decision Support System software helping doctors develop diagnosis and specialty referrals. Conclusion: The Decision Support System software can be used in Obstetrics Ultrasonography by Primary Care Physicians to provide aid in their diagnosis and referrals. Before it is used, it is recommended for trainings and application tests.","","978-1-4673-7800-0978-1-4673-7799","10.1109/ICICI-BME.2015.7401367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7401367","","Software;Medical services;Decision support systems;Prototypes;Ultrasonography;System analysis and design","biomedical ultrasonics;decision support systems;medical image processing;obstetrics;software prototyping;ultrasonic imaging","decision support system prototype;obstetrics ultrasonography;primary service physicians;primary care physicians;computer software;decision-making;quasiexperimental post-testing;system analysis;decision support system software","","","26","","","","","","IEEE","IEEE Conferences"
"New tests for a two variable search algorithm implemented in a hysteresis model","O. Tabara","University &#x201C;Politehnica&#x201D; of Bucharest","2014 International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)","","2014","","","89","94","Hysteresis modeling in magnetic materials is an important subject in current scientific literature. This paper presents a 2 variable search algorithm implemented in Preisach model. Results obtained with this algorithm are comparing with those from identification search algorithm from Matlab software.","1842-0133","978-1-4799-5183","10.1109/OPTIM.2014.6850992","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850992","","Mathematical model;Computational modeling;MATLAB;Magnetization;Magnetic hysteresis;Probability density function;Software algorithms","magnetic hysteresis;magnetic materials;search problems","variable search algorithm;hysteresis model;magnetic materials;Preisach model;Matlab software","","","23","","","","","","IEEE","IEEE Conferences"
"Finite element analysis of residual stresses and thin plate distortion after face milling","Y. Ma; S. Liu; P. F. Feng; D. W. Yu","Department of Mechanical Engineering, Tsinghua University, Beijing, China; AVIC Xi'an Flight Automatic Control Research Institute, Xi'an, China; Department of Mechanical Engineering, Tsinghua University, Beijing, China; Department of Mechanical Engineering, Tsinghua University, Beijing, China","2015 12th International Bhurban Conference on Applied Sciences and Technology (IBCAST)","","2015","","","67","71","Thin-walled structures are now widely used in aerospace structural components, turbines and many other applications, while the control of machining distortion has become one of the most difficult problems influencing their machining quality. Machining distortion is the result of many factors, among which the machining induced residual stress is an important issue. In this paper, a finite element analysis (FEA) approach of predicting machining induced residual stresses and the consequent distortion is presented, which includes the simulation of machining process, processing of the extracted residual stress data, loading of the fitted stress data and distortion analysis. Professional metal cutting simulation and optimization software, TWS AdvantEdge, was used for 3D machining process simulation. The constitutive model of work piece material was obtained from pressure bar tests and orthogonal cutting tests, and has been proved of good accuracy. General FEM software ABAQUS was used to create and mesh the thin-plate work piece. The meshing data outputted from ABAQUS and stress data calculated by AdvantEdge were processed with MATLAB, obtaining the stress distribution of each element, and then the stress distribution was input back into ABAQUS to calculate the distortion. Experiments were carried out, measuring the residual stress and distortion, to verify the FEM simulation. Under the designed simulation and experiment conditions, the machining induced residual stresses in feeding and peripheral directions are both tensile at the machined surface and fall off to a maximum compressive state with the increase in depth, after which the compressive stresses decrease and approach a steady value next to zero in near work piece substrate. Both experimental and FEA simulation results show that the release of the residual stresses make the thin plate to become convex. Yet there are still differences in conditions between simulation and experiment, and further will be carried out to improve the accuracy.","2151-1403;2151-1411","978-1-4799-6369","10.1109/IBCAST.2015.7058481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7058481","residual stress;machining distortion;finite element analysis;thin-walled structure","Stress measurement;Stress;Finite element analysis;Silicon;Manganese;Nickel;Strain","aerospace components;cutting;digital simulation;finite element analysis;internal stresses;mechanical testing;milling;plates (structures);production engineering computing;quality control;solid modelling;thin wall structures;turbines","finite element analysis;thin plate distortion;face milling;thin-walled structures;aerospace structural components;turbines;machining distortion;machining quality;FEA;machining induced residual stresses;fitted stress data;professional metal cutting simulation;optimization software;TWS AdvantEdge;3D machining process simulation;workpiece material;pressure bar tests;orthogonal cutting tests;ABAQUS;FEM software;MATLAB;stress distribution;workpiece substrate","","1","14","","","","","","IEEE","IEEE Conferences"
"Optimized Indoor Positioning based on WIFI in Mobile Classroom Project","Jianhui Guo; Xianzhong Liu; Zhongzhu Wang","Software Engineering Institute, East China Normal University, Shanghai, China; Software Engineering Institute, East China Normal University, Software/Hardware Co-design Engineering Research Center, MoE, Shanghai, China; Software Engineering Institute, East China Normal University, Shanghai, China","2015 11th International Conference on Natural Computation (ICNC)","","2015","","","1208","1212","The Mobile Classroom Project aims at building an interactive platform between students and teachers. The LBS(Location Based Services) Teaching Platform is an important part of it. With its help, teachers can send homework, in-class exercises or related learning materials to students, which take mobile teaching equipment in the classroom. Students can submit their homework and in-class test in specified location quickly. The indoor positioning system is the foundation of the LBS teaching platform. However, the current indoor localization based on WIFI(Wireless Fidelity) fingerprint cannot consider both effects of environment to AP(Access Point) and complexity of localization algorithm, leading to the result of low location accuracy or consume more resources of mobile equipment. This paper proposes an optimized Indoor Positioning algorithm based on WIFI. It uses a reliable AP algorithm as AP selection algorithm to reduce the effects of the indoor environment, and takes reliable AP algorithm combining hybrid of KNN(K-Nearest Neighbor) and Bayesian decision into account to obtain better location accuracy and to reduce complexity of positioning computation possibly. In this paper, we realized the indoor positioning system and gave the experiments. We can see the location accuracy is improved and the complexity of the algorithm is reduced.","2157-9563","978-1-4673-7679-2978-1-4673-7678","10.1109/ICNC.2015.7378163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7378163","Mobile Classroom;Indoor positioning;WIFI fingerprint;reliable AP","Reliability;IEEE 802.11 Standard;Mobile communication;Bayes methods;Databases;Education;Complexity theory","Bayes methods;indoor environment;indoor navigation;indoor radio;mobile computing;mobility management (mobile radio);telecommunication network reliability;wireless LAN","indoor positioning system;wireless fidelity;WiFi;mobile classroom project;location based services;LBS;mobile teaching equipment;indoor localization;access point;AP selection algorithm;localization algorithm;mobile equipment;indoor environment;K-nearest neighbor;KNN;Bayesian decision","","","19","","","","","","IEEE","IEEE Conferences"
"Terabyte-sized image computations on Hadoop cluster platforms","P. Bajcsy; A. Vandecreme; J. Amelot; P. Nguyen; J. Chalfoun; M. Brady","Software and Systems Division, Information Technology Laboratory, National Institute of Standards and Technology, Gaithersburg, MD; Software and Systems Division, Information Technology Laboratory, National Institute of Standards and Technology, Gaithersburg, MD; Software and Systems Division, Information Technology Laboratory, National Institute of Standards and Technology, Gaithersburg, MD; Software and Systems Division, Information Technology Laboratory, National Institute of Standards and Technology, Gaithersburg, MD; Software and Systems Division, Information Technology Laboratory, National Institute of Standards and Technology, Gaithersburg, MD; Software and Systems Division, Information Technology Laboratory, National Institute of Standards and Technology, Gaithersburg, MD","2013 IEEE International Conference on Big Data","","2013","","","729","737","We present a characterization of four basic Terabyte-sized image computations on a Hadoop cluster in terms of their relative efficiency according to the modified Amdahl's law. The work is motivated by the lack of standard benchmarks and stress tests for big image processing operations on a Hadoop computer cluster platform. Our benchmark design and evaluations were performed on one of the three microscopy image sets, each consisting of over one half Terabyte. All image processing benchmarks executed on the NIST Raritan cluster with Hadoop were compared against baseline measurements, such as the Terasort/Teragen designed for Hadoop testing previously, image processing executions on a multiprocessor desktop and on NIST Raritan cluster using Java Remote Method Invocation (RMI) with multiple configurations. By applying our methodology to assessing efficiencies of computations on computer cluster configurations, we could rank computation configurations and aid scientists in measuring the benefits of running image processing on a Hadoop cluster.","","978-1-4799-1293","10.1109/BigData.2013.6691645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691645","Big Data Industry Standards;Big Data Open Platform;Big Data Applications and Infrastructure","Benchmark testing;Feature extraction;Java;Computers;Image segmentation;Random access memory","image processing;Java;microscopy;multiprocessing systems","terabyte-sized image computations;relative efficiency;Amdahl law;image processing;Hadoop computer cluster platform;benchmark design;microscopy image sets;NIST Raritan cluster;multiprocessor desktop;Java remote method invocation;RMI","","12","37","","","","","","IEEE","IEEE Conferences"
"Getting more for less in optimized MapReduce workflows","Z. Zhang; L. Cherkasova; B. T. Loo","University of Pennsylvania; Hewlett-Packard Labs; University of Pennsylvania","2013 IFIP/IEEE International Symposium on Integrated Network Management (IM 2013)","","2013","","","93","100","Many companies are piloting the use of Hadoop for advanced data analytics over large datasets. Typically, such MapReduce programs represent workflows of MapReduce jobs. Currently, a user must specify the number of reduce tasks for each MapReduce job. The choice of the right number of reduce tasks is non-trivial and depends on the cluster size, input dataset of the job, and the amount of resources available for processing this job. In the workflow of MapReduce jobs, the output of one job becomes the input of the next job, and therefore the number of reduce tasks in the previous job may impact the performance and processing efficiency of the next job. In this work,1 we offer a novel performance evaluation framework for easing the user efforts of tuning the reduce task settings while achieving performance objectives. The proposed framework is based on two performance models: a platform performance model and a workflow performance model. A platform performance model characterizes the execution time of each generic phase in the MapReduce processing pipeline as a function of processed data. The complementary workflow performance model evaluates the completion time of a given workflow as a function of i) input dataset size(s) and ii) the reduce tasks' settings in the jobs that comprise a given workflow. We validate the accuracy, effectiveness, and performance benefits of the proposed framework using a set of realistic MapReduce applications and queries from the TPC-H benchmark.","1573-0077","978-3-901882-50-0978-1-4673-5229","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572974","","Benchmark testing;Phase measurement;Computational modeling;Time measurement;Data models;Tuning;Production","data analysis;parallel programming;pipeline processing;software performance evaluation;task analysis;workflow management software","optimized MapReduce workflows;Hadoop;advanced data analytics;MapReduce programs;MapReduce job workflow;task reduction;cluster size;input dataset;job processing;performance impact;processing efficiency;performance evaluation framework;platform performance model;workflow performance model;generic phase;MapReduce processing pipeline;complementary workflow performance model;workflow completion time evaluation;input dataset size;TPC-H benchmark;MapReduce queries","","","21","","","","","","IEEE","IEEE Conferences"
"Data Centers as Software Defined Networks: Traffic Redundancy Elimination with Wireless Cards at Routers","Y. Cui; S. Xiao; C. Liao; I. Stojmenovic; M. Li","Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, P.R.China; Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, P.R.China; Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, P.R.China; SEECS, University of Ottawa, Ottawa, Canada, and School of Software, Tsinghua University, Beijing, 100084, P.R.China; Department of Computer Science, City University of Hong Kong, Hong Kong, China","IEEE Journal on Selected Areas in Communications","","2013","31","12","2658","2672","We propose a novel architecture of data center networks (DCN), which adds wireless network card to both servers and routers. Existing traffic redundancy elimination (TRE) mechanisms reduce link loads and increase network capacity in several environments by removing strings that have appeared in earlier packets through encoding and decoding them several hops downstream. This article is the first to explore TRE mechanisms in large-scale DCNs and the first to exploit cooperative TRE among servers. Moreover, it also achieves the `logically centralized' control over the physically distributed states in emerging software defined networks (SDN) paradigm, by sharing information among servers and routers in data centers with wireless cards. We first formulate the TREDaCeN (TRE in Data Center Networks) problem and reduce the cycle cover problem to prove that finding an optimal caching task assignment for TREDaCeN problem is NP-hard. We further describe an offline TREDaCeN algorithm which is proved to have good approximation ratio. We then discuss efficient online zero-delay and semi-distributed implementations of TREDaCeN supported by physical proximity of servers and routers, enabling status updates in a single wireless transmission, using an efficient prioritized schedule. We also address online cache replacement and consistency of information in servers and routers with and without delay. Our framework is tested on different parameters and shows superior performance in comparison to other mechanisms (imported directly to this setting). Our results show the robustness and the trade-off between the `logically centralized' implementation and the overhead on handling inconsistency of distributed information in DCN.","0733-8716;1558-0008","","10.1109/JSAC.2013.131207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678112","Data Center Network;Redundancy Elimination;Wireless Link;Software Defined Networks","Wireless networks;Servers;Decoding;Routing;Database management","approximation theory;cache storage;computational complexity;computer centres;computer networks;network interfaces;telecommunication network routing;telecommunication traffic","data center network architecture;software defined networks;traffic redundancy elimination mechanism;wireless network card;routers;link load reduction;increase network capacity;string removal;hops downstream;TRE mechanisms;logical centralized control;SDN;information sharing;TREDaCeN problem;cycle cover problem reduction;optimal caching task assignment;NP-hard problem;online zero-delay;approximation ratio;semi-distributed implementations;status updates;wireless transmission;online cache replacement;information consistency;inconsistency handling","","22","30","","","","","","IEEE","IEEE Journals & Magazines"
"A Compiler Design Technique for Impulsive VDD Current Minimization","S. Yuan; W. Su; G. Ni; T. Chi; S. Kuo","Department of Communication Engineering, Fang Chia University, Taichung, Taiwan; Department of Electrical Engineering , National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering , National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering , National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering , National Taiwan University, Taipei, Taiwan","IEEE Transactions on Electromagnetic Compatibility","","2013","55","5","855","866","For electromagnet interference (EMI) optimization issues, different hardware-level techniques have been proposed. This paper focuses on an EMI optimization technique via a software-level technique. We propose a novel estimation and optimization tool for reducing conducted EMI at specified frequency by compiler technology. This study is not a research on compiler technique but an adaption of computer science-domain technology to EMI optimization research. The proposed tool can accept C language syntax and generate many versions of assembly programs. These assembly programs perform the same functionality defined by the input C program but with different conducted EMI behaviors when they are executed. The proposed tool can estimate, select, and generate the assembly program with the least amount of conducted EMI released during its execution. The experiment results show that the proposed tool can analyze test C-programs and generate lower EMI assembly programs. Currently, compared to a commercial compiler, the proposed technique can decrease conducted EMI by 2-5 dB at any specified frequency.","0018-9375;1558-187X","","10.1109/TEMC.2013.2240459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6449312","Electromagnetic analysis;electromagnetic interference (EMI);electromagnetic measurements","Grammar;Electromagnetic interference;Assembly;Databases;Optimization;Generators;Microcontrollers","C language;electromagnetic compatibility;electromagnetic interference;optimisation;optimising compilers;program assemblers","electromagnet interference;EMI optimization issues;hardware-level techniques;software-level technique;compiler design technique;computer science-domain technology;C language syntax;assembly programs;EMI behaviors;conducted EMI;impulsive VDD current minimization","","1","13","","","","","","IEEE","IEEE Journals & Magazines"
"CUDA vs OpenACC: Performance Case Studies with Kernel Benchmarks and a Memory-Bound CFD Application","T. Hoshino; N. Maruyama; S. Matsuoka; R. Takaki","NA; NA; NA; NA","2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing","","2013","","","136","143","OpenACC is a new accelerator programming interface that provides a set of OpenMP-like loop directives for the programming of accelerators in an implicit and portable way. It allows the programmer to express the offloading of data and computations to accelerators, such that the porting process for legacy CPU-based applications can be significantly simplified. This paper focuses on the performance aspects of OpenACC using two micro benchmarks and one real-world computational fluid dynamics application. Both evaluations show that in general OpenACC performance is approximately 50\% lower than CUDA. However, for some applications it can reach up to 98\% with careful manual optimizations. The results also indicate several limitations of the OpenACC specification that hamper full use of the GPU hardware resources, resulting in a significant performance gap when compared to a fully tuned CUDA code. The lack of a programming interface for the shared memory in particular results in as much as three times lower performance.","","978-0-7695-4996-5978-1-4673-6465","10.1109/CCGrid.2013.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6546071","GPU;OpenACC;CUDA","Graphics processing units;Kernel;Optimization;Arrays;Programming;Instruction sets;Benchmark testing","application program interfaces;benchmark testing;computational fluid dynamics;graphics processing units;message passing;parallel architectures;shared memory systems;software maintenance;software performance evaluation","CUDA;kernel benchmark;memory-bound CFD;accelerator programming interface;OpenMP-like loop directive;porting process;legacy CPU-based application;performance aspect;computational fluid dynamics;OpenACC specification;GPU hardware resource;shared memory","","30","12","","","","","","IEEE","IEEE Conferences"
"Transparent offloading of computational hotspots from binary code to Xeon Phi","M. Damschen; H. Riebler; G. Vaz; C. Plessl","Karlsruhe Institute of Technology; University of Paderborn; University of Paderborn; University of Paderborn","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","1078","1083","In this paper, we study how binary applications can be transparently accelerated with novel heterogeneous computing resources without requiring any manual porting or developer-provided hints. Our work is based on Binary Acceleration At Runtime (BAAR), our previously introduced binary acceleration mechanism that uses the LLVM Compiler Infrastructure. BAAR is designed as a client-server architecture. The client runs the program to be accelerated in an environment, which allows program analysis and profiling and identifies and extracts suitable program parts to be off-loaded. The server compiles and optimizes these off-loaded program parts for the accelerator and offers access to these functions to the client with a remote procedure call (RPC) interface. Our previous work proved the feasibility of our approach, but also showed that communication time and overheads limit the granularity of functions that can be meaningfully off-loaded. In this work, we motivate the importance of a lightweight, high-performance communication between server and client and present a communication mechanism based on the Message Passing Interface (MPI). We evaluate our approach by using an Intel Xeon Phi 5110P as the acceleration target and show that the communication overhead can be reduced from 40% to 10%, thus enabling even small hotspots to benefit from off-loading to an accelerator.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.1124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092549","","Servers;Runtime;Acceleration;Computer architecture;IP networks;Software;Optimization","binary codes;client-server systems;graphics processing units;message passing;optimising compilers;program diagnostics;remote procedure calls","transparent computational hotspot off-loading;binary code;Xeon Phi;binary acceleration at runtime;BAAR;client-server architecture;program analysis;program profiling;off-loaded program compilation;off-loaded program optimization;remote procedure call;RPC interface;high-performance communication;message passing interface;MPI;Intel Xeon Phi 5110P;accelerator","","2","13","","","","","","IEEE","IEEE Conferences"
"Development of distribution networks with low carbon technologies","N. N. Mansor; V. Levi","Dept. of Electrical & Electronic Engineering, The University of Manchester, Manchester, United Kingdom; Dept. of Electrical & Electronic Engineering, The University of Manchester, Manchester, United Kingdom","2015 IEEE Innovative Smart Grid Technologies - Asia (ISGT ASIA)","","2015","","","1","6","A new methodology for optimal development of distribution networks with low carbon technologies is proposed in this paper. The complete planning model is developed as a two-stage optimization process, where the first stage deals with the investment problem and the second stage focuses on the operational aspects. The novelty of the proposed investment model is explicit incorporation of network security constraints in the optimization formulation in line with the UK planning standards, while taking into consideration different operating regimes with changing load and generation profiles. Connection of new distributed generation units and new types of demands, construction of circuits on new corridors and optimal network reconfiguration, as well as real-life restoration rules are also included in the model. The investment problem is defined as the complex mixed-integer nonlinear optimization model and solved using a dedicated commercial software package. It is applied to a few practical medium-voltage test systems. Additionally, sensitivity analyses are done to determine critical parameters affecting the development and performance of medium voltage distribution networks.","2378-8542","978-1-5090-1238-1978-1-5090-1237","10.1109/ISGT-Asia.2015.7387169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387169","Planning of distribution networks;mixed-integer nonlinear optimization;UK security criteria;sensitivity studies","Mathematical model;Planning;Load modeling;Optimization;Integrated circuit modeling;Investment;Load flow","distributed power generation;integer programming;nonlinear programming;power distribution planning;power system security;sensitivity analysis","distribution network optimal development;low-carbon technology;planning model;two-stage optimization process;investment problem;investment model;network security constraint;UK planning standards;distributed generation units;optimal network reconfiguration;real-life restoration rules;complex mixed-integer nonlinear optimization model;dedicated commercial software package;medium-voltage test systems;sensitivity analysis;medium-voltage distribution networks","","","17","","","","","","IEEE","IEEE Conferences"
"Design-Space Reduction for Architectural Optimization of Automotive Embedded Systems","X. Zhang; L. Feng; D. Chen; M. Törngren","NA; NA; NA; NA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","1103","1109","A key decision for the synthesis of automotive embedded systems is the allocation of application software components to ECUs. Design Space Exploration (DSE) supports the decision by automatically characterizing and evaluating a large number of possible design alternatives, and thereby suggesting the optimal ones. A primary challenge for applying DSE methods to support this decision is to reduce the computation time of the DSE process while maintaining the generality and optimality. This paper exploits legacy system architectures and the AUTOSAR standard to preemptively reduce the design space, because both artifacts limit the flexibility of certain design variables. A new DES formulation incorporating the constraints of the legacy system architectures and the AUTOSAR standard is proposed in this paper. Computation result shows a large reduction of the computation time comparing to traditional modeling and formulations. The scalability of our method is also analyzed by testing it on a set of random problem instances.","","978-1-4799-8937-9978-1-4799-8936","10.1109/HPCC-CSS-ICESS.2015.298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336316","","Computer architecture;Automotive engineering;Resource management;Embedded systems;Standards;Systems architecture;Aging","automotive engineering;object-oriented programming;software maintenance","design-space reduction;automotive embedded systems;software components;design space exploration;DSE methods;AUTOSAR standard;legacy system architectures","","2","19","","","","","","IEEE","IEEE Conferences"
"Optimizing Live Migration for Virtual Desktop Clouds","C. Jo; B. Egger","NA; NA","2013 IEEE 5th International Conference on Cloud Computing Technology and Science","","2013","1","","104","111","Live migration of virtual machines (VM) from one physical host to another is a key enabler for virtual desktop clouds (VDC). The prevalent algorithm, pre-copy, suffers from long migration times and a high data transfer volume for non-idle VMs which hinders effective use of live migration in VDC environments. In this paper, we present an optimization to the pre-copy method which is able to cut the total migration time in half. The key idea is to load memory pages duplicated on non-volatile storage directly and in parallel from the attached storage device. To keep the downtime short, outstanding data is fetched by a background process after the VM has been restarted on the target host. The proposed method has been implemented in the Xen hyper visor. A thorough performance analysis of the technique demonstrates that the proposed method significantly improves the performance of live migration: the total migration time is reduced up to 90% for certain benchmarks and by 50% on average at an equal or shorter downtime of the migrated VM with no or only minimal side-effects on co-located VMs.","","978-0-7695-5095","10.1109/CloudCom.2013.21","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6753784","live migration;virtual desktop cloud;shared storage","Benchmark testing;Optimized production technology;Synchronization;Virtual machine monitors;Memory management","cloud computing;software performance evaluation;storage management;virtual machines","live migration optimization;virtual desktop clouds;virtual machines;VM;physical host;VDC;precopy method;memory pages;nonvolatile storage;Xen hypervisor;performance analysis","","3","29","","","","","","IEEE","IEEE Conferences"
"Process optimum design","A. Kader Mazouz","NA","2013 5th International Conference on Modeling, Simulation and Applied Optimization (ICMSAO)","","2013","","","1","8","The objective of this article is to optimize the screen printing process. A comparison is made between the full factorial ANOVA, a one half-fraction design and the Taguchi technique to optimize the screen printing process. The Taguchi technique was the most powerful for this particular experiment. The significant were determined with only half of the runs needed on a full factorial and another advantage is that you do not require the use of any statistical software to get the optimal factor levels. Too little will cause a faulty joint or an unreliable joint with an electrical continuity failure potential. Too much will create solder short problems or worse yet, solder balls. Solder paste can become detached after the product has been tested, causing potential field failures.","","978-1-4673-5814-9978-1-4673-5812-5978-1-4673-5813","10.1109/ICMSAO.2013.6552541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552541","Design of Experiments (DOE);Tagushi Method;Factorial Design;Fractional Design;Orthogonal Array;Two Step Optimization","Cameras;Optimization;Indium;Clamps;Microprogramming;Measurement uncertainty","design;printing industry;process design;statistical analysis;Taguchi methods","process optimum design;screen printing process;ANOVA;half-fraction design;Taguchi technique;statistical software","","","11","","","","","","IEEE","IEEE Conferences"
"Cache locking optimization in java virtual machine","Chuanwen Lin; Naijie Gu; Songsong Cai","School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","IEEE Conference Anthology","","2013","","","1","4","Cache plays an important role in multilevel storage system. It can greatly reduce the memory access latency. So the cache hit rate has a significant impact on the performance of the application. Many processors provide cache locking mechanism, which can lock the certain lines in cache. It enables an application to affect the cache replacement decisions under software control. This paper presents a novel method, which uses cache locking mechanism to reduce the run-time of java virtual machine (JVM). JVM often uses just-in-time compiler (JIT) to improve the performance. JIT compiles the method that has been invoked certain times, and then JVM executes the compiled method when invoking this method the next time. This paper analyzes the calling situation of the compiled method in JVM, and then proposes a heuristic approach to lock the compiled method in cache for an appropriate period. It can reduce cache miss rate when JVM executes the compiled method. The algorithm has been implemented in HotSpot based on Loongson-3A. Also, it can be implemented in other run-time systems. Experiment results show that the cache locking heuristic algorithm averagely reduces the cache miss rate by 8.5%, and improves the performance by 4% on the benchmark SPECjvm2008.","","978-1-4799-1660","10.1109/ANTHOLOGY.2013.6784761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6784761","java virtual machine;cache;cache locking;just-in-time compiler","Real-time systems;Java;Program processors;Optimization;Virtual machining;Heuristic algorithms;Benchmark testing","","","","","8","","","","","","IEEE","IEEE Conferences"
"Design and implementation of automatic verification for PLC systems","M. Xia; M. Sun; G. Luo; X. Zhao","School of Software, Tsinghua University, Tsinghua National Laboratory for Information Science and Technology, Beijing 10084, China; School of Software, Tsinghua University, Tsinghua National Laboratory for Information Science and Technology, Beijing 10084, China; School of Software, Tsinghua University, Tsinghua National Laboratory for Information Science and Technology, Beijing 10084, China; School of Software, Tsinghua University, Tsinghua National Laboratory for Information Science and Technology, Beijing 10084, China","2013 IEEE 12th International Conference on Cognitive Informatics and Cognitive Computing","","2013","","","374","379","Programmable Logic Controller (PLC) have been widely used in industries, and safety and reliability of them has been urgently concerned. However, it's hard to verify all the cases to discover the logical flaws of complex systems by traditional testing. Formal verification methods introduce mathematical rigor in their analysis thereby guaranteeing exhaustive state space coverage. But there is not an effective and efficient tool for PLC verification, and the general-purpose formal tools need lots of relevant knowledge. This paper proposes an automatic verification tool for PLC systems. It includes graphical modeling, syntax check, code generation, code optimization and representation of the counter-examples which violate some system properties.","","978-1-4799-0783-0978-1-4799-0781","10.1109/ICCI-CC.2013.6622270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622270","","Model checking;Syntactics;Routing;Optimization;Computer languages;Automata;User interfaces","control engineering computing;formal verification;programmable controllers","PLC systems automatic verification;programmable logic controller;PLC safety;PLC reliability;formal verification methods;exhaustive state space coverage;general-purpose formal tools;graphical modeling;syntax check;code generation;code optimization;code representation","","1","15","","","","","","IEEE","IEEE Conferences"
"Stiction Conformity Test for Electrostatic MEMS Device in Design and Simulation Process","M. A. Johar; A. Koenig","NA; NA","Sensors and Measuring Systems 2014; 17. ITG/GMA Symposium","","2014","","","1","6","The software advancement for CAD/CAE tools in MEMS design has greatly reduced the cost of new product development. Most design parameters now can be analyzed and optimized during design and simulation stages. Finite element method (FEM) simulation based software [1-2] normally consumes a lot of time and computing resources with results closer to reality. On the other hand, a simplified pre-defined finite element model based simulation software produces less accurate simulation result but with faster and less computing power. Our group has been working on design and development of DC-MEMS switches with self-x features using MEMS+ software. Electrostatic and heat actuators were embedded in the switch to provide micro movement. In our first prototype, we are facing stiction problem in electrostatic actuator fingers during switch operations. Simulation results recorded no failures in the first prototype model but the actual devices have shown a high tendency of stiction occurrences. Investigation has been done and the finding showed that low stiffness in finger structures has lead to this problem. A new stiction conformity test at design and simulation stages has been proposed. In this test, the stiffness integrity of the structure is tested to ensure the ability to withstand the electrostatic force produced by the actuator. A safety design factor S with various values of electrostatic actuator design has been introduced and implemented in the second MEMS prototype.","","978-3-8007-3622","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6856705","","","","","","","","","","","","","VDE","VDE Conferences"
"Evaluation of a Server-Grade Software-Only ARM Hypervisor","A. Smirnov; M. Zhidko; Y. Pan; P. Tsao; K. Liu; T. Chiueh","NA; NA; NA; NA; NA; NA","2013 IEEE Sixth International Conference on Cloud Computing","","2013","","","855","862","Because of its enormous popularity in embedded systems and mobile devices, ARM CPU is arguably the most used CPU in the world. The resulting economies of scale benefit entices system architects to ponder the feasibility of building lower-cost and lower-power-consumption servers using ARM CPU. In modern data centers, especially those built to host cloud applications, virtualization is a must. So how to support virtualization on ARM CPUs becomes a major issue for constructing ARM-based servers. Although the latest versions of ARM architecture (Cortex-A15 and beyond) provide hardware support for virtualization, the majority of ARM-based SOCs (system-on-chip) currently available on the market do not. This paper presents results of an evaluation study of a fully operational hypervisor that successfully runs multiple VMs on an ARM Cortex A9-based server, which is architecturally non-virtualizable, and supports VM migration. This hypervisor features several optimizations that significantly reduce the performance overhead of virtualization, including physical memory remapping, and batching of sensitive/privileged instruction emulation.","2159-6190;2159-6182","978-0-7695-5028-2978-0-7695-5028","10.1109/CLOUD.2013.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6740234","kvm-arm;kvm;arm;virtualization;cortex-a9","Virtual machine monitors;Virtualization;Kernel;Servers;Emulation;Benchmark testing;Context","cloud computing;embedded systems;microprocessor chips;optimisation;system-on-chip;virtual machines;virtualisation","server-grade software-only ARM hypervisor;embedded systems;mobile devices;ARM CPU;economies of scale benefit;lower-cost servers;lower-power-consumption servers;data centers;cloud applications;virtualization;ARM-based servers;ARM architecture;Cortex-A15;hardware support;ARM-based SOC;system-on-chip;ARM Cortex A9-based server;VM migration;optimizations;physical memory remapping;sensitive/privileged instruction emulation","","2","32","","","","","","IEEE","IEEE Conferences"
"Leveraging variable function resilience for selective software reliability on unreliable hardware","S. Rehman; M. Shafique; P. V. Aceituno; F. Kriebel; J. Chen; J. Henkel","Karlsruhe Institute of Technology (KIT), Germany; Karlsruhe Institute of Technology (KIT), Germany; Karlsruhe Institute of Technology (KIT), Germany; Karlsruhe Institute of Technology (KIT), Germany; Karlsruhe Institute of Technology (KIT), Germany; Karlsruhe Institute of Technology (KIT), Germany","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","1759","1764","State-of-the-art reliability optimizing schemes deploy spatial or temporal redundancy for the complete functionality. This introduces significant performance/area overhead which is often prohibitive within the stringent design constraints of embedded systems. This paper presents a novel scheme for selective software reliability optimization constraint under user-provided tolerable performance overhead constraint. To enable this scheme, statistical models for quantifying software resilience and error masking properties at function and instruction level are proposed. These models leverage a whole new range of reliability optimization. Given a tolerable performance overhead, our scheme selectively protects the reliability-wise most important instructions based on their masking probability, vulnerability, and redundancy overhead. Compared to state-of-the-art [7], our scheme provides a 4.84X improved reliability at 50% tolerable performance overhead constraint.","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513800","","Resilience;Software reliability;Redundancy;Analytical models;Computational modeling;Hidden Markov models","","","","13","30","","","","","","IEEE","IEEE Conferences"
"A Hybrid Harmony Search Algorithm for the Job Shop Scheduling Problems","H. Piroozfard; K. Y. Wong; A. D. Asl","NA; NA; NA","2015 8th International Conference on Advanced Software Engineering & Its Applications (ASEA)","","2015","","","48","52","Machine scheduling is assigning a set of operations of jobs on machines during a time period, taking into account the time, capability, and capacity constraints. In machine scheduling and management science, job shop scheduling is considered as an important problem due to many real-world applications. The job shop scheduling problems are numerically intractable that cannot be solved in polynomial time, unless P = NP, and they are classified as NP-hard. Harmony search algorithm has been successfully implemented in many optimization problems, particularly in scheduling problems, and hybridization is an effective approach for improving the solution quality of the algorithm. This paper proposes an effective hybrid harmony search algorithm for solving the job shop scheduling problems with the objective of minimizing makespan. A set of well-studied benchmarked problems is used to prove the effectiveness and efficiency of the proposed algorithm. The results indicate that the proposed hybrid harmony search algorithm improves the efficiency.","","978-1-4673-9837-4978-1-4673-9836","10.1109/ASEA.2015.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7433068","job shop scheduling problems;harmony search algorithm;combinatorial optimization;hybridization strategy","Job shop scheduling;Algorithm design and analysis;Genetic algorithms;Search problems;Benchmark testing;Schedules","computational complexity;job shop scheduling;minimisation;search problems","makespan minimization;NP-hard problem;polynomial time;machine scheduling;job shop scheduling;hybrid harmony search algorithm","","","25","","","","","","IEEE","IEEE Conferences"
"Estimating the Odds for Texas Hold'em Poker Agents","L. F. Teófilo; L. P. Reis; H. L. Cardoso","NA; NA; NA","2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)","","2013","2","","369","374","Developing software agents that play incomplete information games is a demanding task: it is required they incorporate strategies capable of dealing with hidden information and deception and risk management. In Poker, these issues are commonly addressed by estimating opponents' game play using a variety of techniques such as Expected Hand Strength (E[HS]) or Hand Potential. In this paper, we propose criteria which can be applied when assessing such techniques, and we have also run benchmark tests which demonstrate their pertinence. We have, however, been faced with a clear gap in terms of the methods' efficiency. While this is not a problem in theoretical models, when implementing such methods in real world applications, they can prove to be painfully slow. In order to address this issue, we propose the Average Rank Strength (ARS) method. It can calculate the strength of a hand of any size through the hand's rank width negligible error, when compared to the original method. Still, the greatest contribution of this method lies in the speed-up factor of about 1000 times over E[HS]. Since most successful agents in the literature use their game abstraction based on E[HS], this breakthrough will significantly contribute towards a much lighter strategy computation, since this routine must be called billions of times. By saving computation time, we believe that future integration of ARS with current game playing algorithms will allow for creating agents with smaller abstraction levels, thus making room for improvement in their overall performance.","","978-0-7695-5145-6978-1-4799-2902","10.1109/WI-IAT.2013.134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6690813","computer poker;hand probabilities;abstraction;hand strength;average hand strength","Games;Indexes;Table lookup;Heating;History;Artificial intelligence;Software agents","computer games;game theory;risk management;software agents","Texas Hold'em Poker agents;software agents;incomplete information games;hidden information;risk management;opponent gameplay estimation;expected hand strength;hand potential;benchmark tests;average rank strength method;ARS method;game abstraction;strategy computation;game playing algorithms","","","13","","","","","","IEEE","IEEE Conferences"
"PCERE: Fine-Grained Parallel Benchmark Decomposition for Scalability Prediction","M. Popov; C. Akel; F. Conti; W. Jalby; P. d. O. Castro","NA; NA; NA; NA; NA","2015 IEEE International Parallel and Distributed Processing Symposium","","2015","","","1151","1160","Evaluating the strong scalability of OpenMP applications is a costly and time-consuming process. It traditionally requires executing the whole application multiple times with different number of threads. We propose the Parallel Codelet Extractor and REplayer (PCERE), a tool to reduce the cost of scalability evaluation. PCERE decomposes applications into small pieces called codelets: each codelet maps to an OpenMP parallel region and can be replayed as a standalone program. To accelerate scalability prediction, PCERE replays codelets while varying the number of threads. Prediction speedup comes from two key ideas. First, the number of invocations during replay can be significantly reduced. Invocations that have the same performance are grouped together and a single representative is replayed. Second, sequential parts of the programs do not need to be replayed for each different thread configuration. PCERE codelets can be captured once and replayed accurately on multiple architectures, enabling cross-architecture parallel performance prediction. We evaluate PCERE on a C version of the NAS 3.0 Parallel Benchmarks (NPB). We achieve an average speed-up of 25 × on evaluating OpenMP applications scalability with an average error of 4.9% (median error of 1.7%).","1530-2075","978-1-4799-8649","10.1109/IPDPS.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161599","OpenMP applications;program replay;checkpoint restart;parallel code isolation;scalability prediction;cross-architecture performance prediction","Benchmark testing;Instruction sets;Scalability;Accuracy;Optimization;In vivo;Context","benchmark testing;parallel processing;software architecture;software performance evaluation","PCERE;fine-grained parallel benchmark decomposition;scalability prediction;OpenMP applications;parallel codelet extractor and replayer;thread configuration;cross-architecture parallel performance prediction;NAS 3.0 parallel benchmarks;NPB","","3","37","","","","","","IEEE","IEEE Conferences"
"Trivial Compiler Equivalence: A Large Scale Empirical Study of a Simple, Fast and Effective Equivalent Mutant Detection Technique","M. Papadakis; Y. Jia; M. Harman; Y. Le Traon","NA; NA; NA; NA","2015 IEEE/ACM 37th IEEE International Conference on Software Engineering","","2015","1","","936","946","Identifying equivalent mutants remains the largest impediment to the widespread uptake of mutation testing. Despite being researched for more than three decades, the problem remains. We propose Trivial Compiler Equivalence (TCE) a technique that exploits the use of readily available compiler technology to address this long-standing challenge. TCE is directly applicable to real-world programs and can imbue existing tools with the ability to detect equivalent mutants and a special form of useless mutants called duplicated mutants. We present a thorough empirical study using 6 large open source programs, several orders of magnitude larger than those used in previous work, and 18 benchmark programs with hand-analysis equivalent mutants. Our results reveal that, on large real-world programs, TCE can discard more than 7% and 21% of all the mutants as being equivalent and duplicated mutants respectively. A human- based equivalence verification reveals that TCE has the ability to detect approximately 30% of all the existing equivalent mutants.","0270-5257;1558-1225","978-1-4799-1934","10.1109/ICSE.2015.103","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194639","","Optimization;Java;Scalability;Benchmark testing;Syntactics","formal verification;program compilers;program testing","trivial compiler equivalence technology;mutant detection technique;mutation testing;TCE technique;duplicated mutants;human-based equivalence verification","","48","58","","","","","","IEEE","IEEE Conferences"
"A novel residual stress test structure for MEMs thin films","Y. Zhou; Z. Zhou; N. Wang","School of Electronic and Engineering, Southeast University, Nanjing 210096, China; Key Laboratory of MEMS of the Ministry of Education, Southeast University, Nanjing 210096, China; School of Electronic and Engineering, Southeast University, Nanjing 210096, China","2014 12th IEEE International Conference on Solid-State and Integrated Circuit Technology (ICSICT)","","2014","","","1","3","Residual stress has a great impact on the feature of MEMS thin films, and the structural parameter and manufacturing process of MEMS thin films determine the magnitude of residual stress. Based on the conventional single-cantilever-beam micro-rotating structure, this paper presents a new structure and optimizes the corresponding parameters. The ANSYS software is adopted to conduct finite element analysis. In addition, we figure out the relationship between the new structure's residual stress and the deflection angle according to elasticity formulas and geometric principles. The analysis demonstrates that the original structure has several advantages in aspects of measuring residual stress such as low deviation, high accuracy and simplified process.","","978-1-4799-3282-5978-1-4799-3296-2978-1-4799-3281","10.1109/ICSICT.2014.7021635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7021635","","Gallium arsenide;Residual stresses;Micromechanical devices;Films;Abstracts;Manufacturing processes","cantilevers;elasticity;finite element analysis;internal stresses;manufacturing processes;micromechanical devices;thin film devices","simplified process;high accuracy process;low deviation process;geometric principles;elasticity formulas;deflection angle;finite element analysis;ANSYS software;single-cantilever-beam microrotating structure;manufacturing process;structural parameter;MEMS thin film feature;residual stress test structure","","1","9","","","","","","IEEE","IEEE Conferences"
"A VMM/FPGA co-verification method for “Longtium Stream” processor","L. Bai; X. Fan; M. Zhang; L. Sun","School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China","2013 IEEE International Conference on Signal Processing, Communication and Computing (ICSPCC 2013)","","2013","","","1","6","The growth of chip complexity increases the workload of verification and new advanced verification methodology is needed to face this challenge. We adopt the verification methodology based Verification Methodology Manual (VMM) to build an automatic coverage-driven verification platform for functional verification of the “Longtium Stream” processor instruction set, which improves the efficiency and functional coverage of verification and enhances the reusability and portability of the verification platform. The stream processor was implemented in a FPGA prototype system for evaluation of the functionality and performance. After analyzing the result obtained by running test program set and the key bottleneck that restrict acceleration performance, we optimized the performance of stream processor by improving the architecture.","","978-1-4799-1027","10.1109/ICSPCC.2013.6664138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6664138","stream processor;instruction set verification;VMM;FPGA prototype verification","Vectors;Registers;Prototypes;Acceleration;Field programmable gate arrays;System-on-chip;Generators","field programmable gate arrays;instruction sets;performance evaluation;program testing;program verification;software portability;software reusability","VMM-FPGA coverification method;Longtium stream processor instruction set;chip complexity;verification methodology based verification methodology manual;automatic coverage-driven verification platform;functional verification;verification platform portability;verification platform reusability;FPGA prototype system;functionality evaluation;performance evaluation;test program set;stream processor performance optimization","","","15","","","","","","IEEE","IEEE Conferences"
"Alternative parameter determination methods for a PMSG","T. Kalogiannis; E. Malz; E. M. Llano; L. Petersen; M. Schimmelmann; K. Leban; E. Ritchie","Department of Energy Technology, Aalborg University, Denmark; Department of Energy Technology, Aalborg University, Denmark; Department of Energy Technology, Aalborg University, Denmark; Department of Energy Technology, Aalborg University, Denmark; Department of Energy Technology, Aalborg University, Denmark; Department of Energy Technology, Aalborg University, Denmark; Department of Energy Technology, Aalborg University, Denmark","2014 International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)","","2014","","","483","488","One of the fundamental requirements for testing and analysing a Permanent Magnet Synchronous Generator (PMSG) is to obtain its electrical and mechanical parameters. This paper describes the test set up and the procedure for obtaining them. Stator resistance and flux linkage measurements follow IEEE standards. In the other hand a new approach for an alternative stator inductance and inertia measurement is analysed. More precisely, the former is obtained through laboratory work based on the locked rotor test, and the latter through a CAD software based on a 3D model. In order to assess and validate the obtained values, an electromechanical model is derived for validation and contribution of the whole project. Finally, small relative errors between measured and simulated values indicate the functionality of the used methods and of the machine.","1842-0133","978-1-4799-5183","10.1109/OPTIM.2014.6850897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850897","Electrical parameters;mechanical parameters;permanent magnet synchronous generator;stator inductance","Rotors;Torque;Stators;Permanent magnets;Resistance;Couplings;Friction","electric machine analysis computing;electric machine CAD;inductance measurement;machine testing;magnetic flux;mechanical variables measurement;permanent magnet generators;rotors;solid modelling;stators;synchronous generators","alternative parameter determination methods;PMSG;permanent magnet synchronous generator;electrical parameters;mechanical parameters;stator resistance measurement;flux linkage measurement;alternative stator inductance measurement;inertia measurement;locked rotor test;CAD software;3D model;electromechanical model","","","17","","","","","","IEEE","IEEE Conferences"
"Overhead power line design in market conditions","S. Beryozkina; L. Petrichenko; A. Sauhats; N. Jankovskis","Institute of Power Engineering, Riga Technical University (RTU) Riga, Latvia; Institute of Power Engineering, Riga Technical University (RTU) Riga, Latvia; Institute of Power Engineering, Riga Technical University (RTU) Riga, Latvia; Department of New Connections Function, JSC &#x201C;Latvenergo&#x201D;, Riga, Latvia","2015 IEEE 5th International Conference on Power Engineering, Energy and Electrical Drives (POWERENG)","","2015","","","278","282","This paper deals with the concept of the stochastic optimization methodology for power line designing, which allows optimizing transmission network planning. The paper presents a comparison of both methods applied for selecting the best line design alternative - the deterministic economic intervals method and the stochastic approach based method, taking into account market conditions. Both methods are utilized for designing overhead power lines, including the choice of main line parameters such as tower height, type and coordinates, conductor type and cross-section, line fittings, etc. Moreover, the use of High Temperature Low Sag conductors - a part of the advanced technology - was evaluated alongside the use of conductors of the traditional type. The optimization problem is formulated as minimization of the total annual costs. The proposed methodology is tested in the developed tool, which is realized in MATLAB software by using the Monte Carlo method, and in an overhead power line designing program - PLS-CADD. Two basic case studies for verifying the proposed power line planning solution are presented in the paper.","2155-5532;2155-5516","978-1-4799-9978-1978-1-4673-7203-9978-1-4799-9977","10.1109/PowerEng.2015.7266333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7266333","design optimization;power systems;process planning;transmission lines","Conductors;Planning;Investment;Optimization;Economics;Power systems;Stochastic processes","Monte Carlo methods;optimisation;power overhead lines;power transmission planning","stochastic optimization methodology;transmission network planning;line design alternative;deterministic economic intervals method;stochastic approach based method;market conditions;main line parameters;high temperature low sag conductors;total annual costs minimization;MATLAB software;Monte Carlo method;PLS-CADD;overhead power line designing program","","1","32","","","","","","IEEE","IEEE Conferences"
"Obstacle avoidance in real time with Nonlinear Model Predictive Control of autonomous vehicles","M. A. Abbas; R. Milman; J. M. Eklund","Department of Electrical, Computer Engineering and Software Engineering, University of Ontario Institute of Technology, Oshawa, L1H 7K4, Canada; Department of Electrical, Computer Engineering and Software Engineering, University of Ontario Institute of Technology, Oshawa, L1H 7K4, Canada; Department of Electrical, Computer Engineering and Software Engineering, University of Ontario Institute of Technology, Oshawa, L1H 7K4, Canada","2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE)","","2014","","","1","6","A Nonlinear Model Predictive Controller (NMPC) for trajectory tracking of autonomous vehicles is presented in this paper. This controller is tested under several constrained scenarios including static obstacle avoidance and avoidance of obstacles with more complex constraints. In the latter case the real life necessary constraint of remaining on the road while performing the obstacle avoidance manoeuvers is implemented. The resulting controllers are applied and tested in a simulation environment and the required CPU time is analyzed to evaluate the ability to implement these schemes in real-time using both cold and warm starts for the embedded optimization problem. In order to simplify the vehicle dynamics, a bicycle model is used for the prediction of future vehicle states in the NMPC framework. A fully nonlinear CarSim vehicle model is used to evaluate the vehicle performance in the simulations. Results show that the NMPC controller provides satisfactory online tracking performance in a realistic scenario at normal road speeds while still satisfying the real-time constraints.","0840-7789","978-1-4799-3101-9978-1-4799-3099","10.1109/CCECE.2014.6901109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901109","","Vehicles;Trajectory;Roads;Mathematical model;Real-time systems;Computational modeling;Collision avoidance","bicycles;collision avoidance;mobile robots;nonlinear control systems;optimisation;predictive control;road vehicles;vehicle dynamics","static obstacle avoidance;nonlinear model predictive control;autonomous vehicles;NMPC controller;trajectory tracking;road vehicles;embedded optimization problem;vehicle dynamics;bicycle model;vehicle state prediction;fully nonlinear CarSim vehicle model;online tracking performance","","8","15","","","","","","IEEE","IEEE Conferences"
"MDDM: A Method to Improve Multiple Dimension Data Management Performance in HBase","Z. Wei; Q. JunMei; L. Liang; Z. ChaoQiang; Y. WenJun","NA; NA; NA; NA; NA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","102","109","Big data is the term applied to a new generation of software, applications and storage system, designed to derive business values. The big data phenomenon requires a revolutionary approach to the technologies deployed to ensure that timely results are delivered to create value. However, the state-of-the-art techniques for multiple dimensions big data query are facing problems as the data expand and user access pattern changes. In this paper, we will propose an optimized storage model and index scheme to provide efficient query over big multiple dimension data and multiple query patterns. We implement our scheme on HBase by introducing four components in its master node. Taking pollutant concentration data in ""Green Horizon"" project as the test data, we conduct numerous experiments. Experiment results show that our proposed storage model and index can help provide obvious performance improvement on multiple different queries patterns over big multiple dimension data and also has good scalability as data expand.","","978-1-4799-8937-9978-1-4799-8936","10.1109/HPCC-CSS-ICESS.2015.102","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336151","Multiple dimension data;multiple query patterns;HBase;column;column family;storage optimization model","Time series analysis;Big data;Data models;Monitoring;Software;Diamonds;Buildings","Big Data;green computing;query processing","MDDM;multiple dimension data management performance method;HBase;multiple dimension Big Data query patterns;optimized storage model;index scheme;master node;pollutant concentration data;Green-Horizon project;test data;performance improvement;data expansion","","1","17","","","","","","IEEE","IEEE Conferences"
"A low cost 3D markerless system for the reconstruction of athletic techniques","A. El-Sallam; M. Bennamoun; F. Sohel; J. Alderson; A. Lyttle; M. Rossi","School of Computer Science &amp; Software Engineering University of Western Australia; School of Computer Science &amp; Software Engineering University of Western Australia; School of Computer Science &amp; Software Engineering University of Western Australia; School of Sport Science, Exercise and Health University of Western Australia; School of Sport Science, Exercise and Health University of Western Australia; School of Sport Science, Exercise and Health University of Western Australia","2013 IEEE Workshop on Applications of Computer Vision (WACV)","","2013","","","222","229","We present a low cost markerless system for the optimization of athlete performance in sports such as pole vault, jumping and javelin throw. The system uses a number of calibrated cameras to capture a video of an athlete from different viewpoints. The athlete's body is then segmented from the background in each video frame. The silhouettes of the segmented body are then reprojected to reconstruct an estimate of the 3D body shape of the athlete, known as the visual hull (VH). The VH is tracked over a number of frames in real testing trials. A template combining a high resolution 3D scan and a 2D mass scan is then aligned with the VH in each frame. A set of motion analysis parameters such as the take-off data are finally estimated from the aligned template and compared with the ones obtained using a gold standard marker-based system, namely the Vicon. The proposed system was tested in real-time trials and was able to provide comparable results to the Vicon system.","1550-5790;1550-5790","978-1-4673-5054-9978-1-4673-5053-2978-1-4673-5052","10.1109/WACV.2013.6475022","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6475022","","Cameras;Kernel;Motion segmentation;Joints;Shape;Image reconstruction;Calibration","image reconstruction;image resolution;image segmentation;optimisation;sport;video signal processing","low cost 3D markerless system;athletic technique reconstruction;athlete performance optimization;pole vault;javelin throw;jumping;calibrated cameras;video capture;video frame;segmented body;visual hull;VH;high resolution 3D scan;2D mass scan;take-off data;gold standard marker-based system;Vicon system","","3","21","","","","","","IEEE","IEEE Conferences"
"Highly distributed state estimation for a DC spacecraft power system","R. D. May; C. Beierle; Mingguo Hong; K. A. Loparo","Vantage Partners, LLC., Cleveland, Ohio, USA; NASA Glenn Research Center, Cleveland, Ohio, USA; Case Western Reserve University, Cleveland, Ohio, USA; Case Western Reserve University, Cleveland, Ohio, USA","2015 IEEE Power & Energy Society General Meeting","","2015","","","1","5","To enable the development of an autonomous power control system comprised of highly-distributed software agents, a distributed implementation of a power system state estimator must be developed. The estimator uses agents located at each bus to estimate the local state and boundary conditions that are then communicated to neighboring agents. Using an iterative approach, the agents can compute a near-optimal state estimate using only local information and the communicated boundary conditions. The algorithm is implemented and tested against a subset of the International Space Station's power system. The results are compared and validated with those of a centralized state estimator.","1932-5517","978-1-4673-8040","10.1109/PESGM.2015.7285698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7285698","state estimation;software agents;microgrids;power system control;distributed optimization","Sensors;Power systems;Boundary conditions;State estimation;Distributed algorithms;Software agents;Distributed databases","power system analysis computing;power system state estimation;software agents;space vehicle power plants","autonomous power control system;highly-distributed software agents;power system state estimator;local state;boundary conditions;neighboring agents;near-optimal state estimate;International Space Station power system;DC spacecraft power system","","","11","","","","","","IEEE","IEEE Conferences"
"Optimized selection of reliable and cost-effective cyber-physical system architectures","N. Bajaj; P. Nuzzo; M. Masin; A. Sangiovanni-Vincentelli","EECS Department, University of California at Berkeley; EECS Department, University of California at Berkeley; IBM Haifa Research Lab, Haifa, Israel; EECS Department, University of California at Berkeley","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","561","566","We address the problem of synthesizing safety-critical cyber-physical system architectures to minimize a cost function while guaranteeing the desired reliability. We cast the problem as an integer linear program on a reconfigurable graph which models the architecture. Since generating symbolic probability constraints by exhaustive enumeration of failure cases on all possible graph configurations takes exponential time, we propose two algorithms to decrease the problem complexity, i.e. Integer-Linear Programming Modulo Reliability (ILP-MR) and Integer-Linear Programming with Approximate Reliability (ILP-AR). We compare the two approaches and demonstrate their effectiveness on the design of aircraft electric power system architectures.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092450","","Computer architecture;Power system reliability;Reliability theory;Algorithm design and analysis;Complexity theory;Reliability engineering","graph theory;integer programming;linear programming;probability;reliability;safety-critical software","optimized cyber-physical system architecture selection;safety-critical cyber-physical system architecture synthesis;cost function minimization;integer linear program;reconfigurable graph;symbolic probability constraints;graph configurations;problem complexity;integer-linear programming modulo reliability;ILP-MR;integer-linear programming with approximate reliability;ILP-AR;aircraft electric power system architectures","","5","9","","","","","","IEEE","IEEE Conferences"
"Towards cryptographic function distinguishers with evolutionary circuits","P. Svenda; M. Ukrop; V. Matyáš","Masaryk University, Botanicka 68a, Brno, Czech Republic; Masaryk University, Botanicka 68a, Brno, Czech Republic; Masaryk University, Botanicka 68a, Brno, Czech Republic","2013 International Conference on Security and Cryptography (SECRYPT)","","2013","","","1","12","Cryptanalysis of a cryptographic function usually requires advanced cryptanalytical skills and extensive amount of human labour. However, some automation is possible, e.g., by using randomness testing suites like STS NIST (Rukhin, 2010) or Dieharder (Brown, 2004). These can be applied to test statistical properties of cryptographic function outputs. Yet such testing suites are limited only to predefined patterns testing particular statistical defects. We propose more open approach based on a combination of software circuits and evolutionary algorithms to search for unwanted statistical properties like next bit predictability, random data non-distinguishability or strict avalanche criterion. Software circuit that acts as a testing function is automatically evolved by a stochastic optimization algorithm and uses information leaked during cryptographic function evaluation. We tested this general approach on problem of finding a distinguisher (Englund et al., 2007) of outputs produced by several candidate algorithms for eStream competition from truly random sequences. We obtained similar results (with some exceptions) as those produced by STS NIST and Dieharder tests w.r.t. the number of rounds of the inspected algorithm. This paper focuses on providing solid assessment of the proposed approach w.r.t. STS NIST and Dieharder when applied over multiple different algorithms rather than obtaining best possible result for a particular one. Additionally, proposed approach is able to provide random distinguisher even when presented with very short sequence like 16 bytes only.","","978-9-8975-8131","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223162","eStream;Genetic Programming;Random Distinguisher;Randomness Statistical Testing;Software Circuit","Cryptography;Software;NIST;Testing;Batteries;Connectors;Evolutionary computation","","","","","28","","","","","","IEEE","IEEE Conferences"
"Software Defined Networking for HTTP video quality optimization","Hui Liu; Yihong Hu; Guochu Shou; Zhigang Guo","Beijing University of Posts and Telecommunications, 100876, China, Beijing Laboratory of Network System Architecture and Convergence; Beijing University of Posts and Telecommunications, 100876, China, Beijing Laboratory of Network System Architecture and Convergence; Beijing University of Posts and Telecommunications, 100876, China, Beijing Laboratory of Network System Architecture and Convergence; Beijing University of Posts and Telecommunications, 100876, China, Beijing Laboratory of Network System Architecture and Convergence","2013 15th IEEE International Conference on Communication Technology","","2013","","","413","417","In this paper, the scheme to improve HTTP video quality using Software Defined Networking (SDN) is explored. First, by analyzing the characteristics of current HTTP video and the defects of the widely used Content Delivery Network (CDN), a HTTP video content delivery scheme is proposed in SDN network. Then, the authors build SDN experimental platform, and present five experiments to measure video quality in the presence of different round-trip delay according to round-trip delay test results of HTTP video access in both SDN and actual network environment (including CDN). Finally, the user QOE of the video measured in each experiment is calculated and compared. For all experimental results examined, SDN scheme yields a substantial improvement in HTTP video quality and user QOE, and achieves more than Good level of video quality and MOS&gt;4 user QOE even in poor network environment with high pocket loss rate.","","978-1-4799-0077","10.1109/ICCT.2013.6820411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6820411","SDN;HTTP video;Programmable Storage Router;CDN;QOE;RTT","Streaming media;Quality assessment;Video recording;Delays;Servers;Loss measurement","hypermedia;quality of experience;transport protocols;video signal processing","HTTP video quality optimization;software defined networking;content delivery network;CDN;HTTP video content delivery scheme;round-trip delay;HTTP video access;user QOE;SDN scheme","","1","16","","","","","","IEEE","IEEE Conferences"
"New clustering algorithm for identification of a nonlinear stochastic model","T. Ahmed; H. Lassad; B. Mohamed; C. Abdelkader","Higher School of Sciences and Techniques of Tunis (ESSTT) Research unit (C3S) Tunisia; Higher School of Sciences and Techniques of Tunis (ESSTT) Research unit (C3S) Tunisia; Higher School of Sciences and Techniques of Tunis (ESSTT) Research unit (C3S) Tunisia; Higher School of Sciences and Techniques of Tunis (ESSTT) Research unit (C3S) Tunisia","2013 International Conference on Electrical Engineering and Software Applications","","2013","","","1","6","Many clustering algorithms have been proposed in literature to identify the premise and consequence parameters involved in the TS fuzzy model. In this paper this parameters are estimated at the same time and this from the minimization of four optimization criteria. The proposed algorithm constitutes an extension of the algorithm proposed by J.Q. Chen in 1998. However, in this paper we introduced some modification on the optimization criteria and especially the last two criteria, thus we replaced the Euclidean distance by another non-Euclidean distance when calculating the fuzzy partition matrix. The purpose of these modifications is to introduce more robustness with the algorithm especially for highly nonlinear systems and those operating in a stochastic environment. The efficiency of the algorithm is tested on an electro-hydraulic system.","","978-1-4673-6301-3978-1-4673-6302-0978-1-4673-6300","10.1109/ICEESA.2013.6578495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578495","Nonlinear system;fuzzy clustering;TS fuzzy model;stochastic environment;linguistic modeling;fuzzy identification;non-Euclidean distance","Clustering algorithms;Minimization;Optimization;Partitioning algorithms;Euclidean distance;Algorithm design and analysis;Simulation","fuzzy control;fuzzy set theory;matrix algebra;minimisation;nonlinear control systems;parameter estimation;pattern clustering;stochastic systems","stochastic environment;nonlinear system;fuzzy partition matrix;Euclidean distance;optimization criteria;minimization;parameter estimation;Takagi-Sugeno fuzzy model;TS fuzzy model;nonlinear stochastic model;clustering algorithm","","","10","","","","","","IEEE","IEEE Conferences"
"Simulation platform of atmosphere based on GIS","H. Xu; G. Li; X. Xu; S. Yang","College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China","2015 IEEE International Conference on Information and Automation","","2015","","","1723","1726","Currently, the research of GIS and environmental models has made great progress, but the combination of both is still a difficulty. Aiming at the integration of GIS and environment model, the paper drew some functional components from open source geographic information system, and embedded the atmosphere environment model into GIS as application program interface by hybrid programming, achieved the effective integration of GIS and models, constructed the simulation platform. During the designing process, we used a number of key technologies to optimize the platform, such as design of SQLite-based database systems, Gaussian coordinate conversion, design of simulation run management software based on multi-threaded, improved the efficiency of simulation. Finally, take Beijing haze for example to verify the effectiveness of the platform, tests show that the simulation result is reasonable and effective, and the platform runs more efficiently.","","978-1-4673-9104-7978-1-4673-9103","10.1109/ICInfA.2015.7279565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279565","GIS;Simulation;Environment Model;Beijing Haze","Atmospheric modeling;Data models;Geographic information systems;Analytical models;Simulation;Pollution;MATLAB","digital simulation;geographic information systems;geophysics computing;multi-threading;public domain software;software engineering;SQL","simulation platform;atmosphere environment model;open source geographic information system;GIS;platform optimization;SQLite-based database system;Gaussian coordinate conversion;simulation run management software;multithreading","","","7","","","","","","IEEE","IEEE Conferences"
"Differential evolution with nonlinear simplex method and dynamic neighborhood search","D. C. Tran; Z. Wu; H. Wang; V. H. Tran","State Key Lab of Software Engineering, Wuhan University, Wuhan 430072, PR China; State Key Lab of Software Engineering, Wuhan University, Wuhan 430072, PR China; State Key Lab of Software Engineering, Wuhan University, Wuhan 430072, PR China; University of Transport and Communications, Hanoi, Vietnam","2013 International Conference on Soft Computing and Pattern Recognition (SoCPaR)","","2013","","","37","43","In this paper, by combination of some approaches we propose a new approach of Differential Evolution (DE) algorithm, called DE with nonlinear simplex method and dynamic neighborhood search (DENNS). In our approach the nonlinear simplex method (NSM) is used for population initialization and local neighborhood search. Moreover, local and global neighborhood search operators are employed to generate high quality candidate solutions. During the search process, the population is periodically ranked to change the topology of neighbors. Experimental studies are conducted on a comprehensive set of benchmark functions. Simulation results show that DENNS achieves better results on the majority of test functions, when comparing with some other similar evolutionary algorithms.","","978-1-4799-3400-3978-1-4799-3399","10.1109/SOCPAR.2013.7054154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7054154","differential evolution;neighborhood search;local search;nonlinear simplex method;dynamic neighborhood;global optimization","Sociology;Statistics;Vectors;Heuristic algorithms;Benchmark testing;Convergence;Topology","evolutionary computation;mathematical operators;search problems","nonlinear simplex method;dynamic neighborhood search;differential evolution algorithm;DE algorithm;DENNS;NSM;global neighborhood search operator;local neighborhood search operator;population initialization","","","16","","","","","","IEEE","IEEE Conferences"
"PSO based OPF algorithm","D. P. Cristian; R. Teslovan; C. Barbulescu; S. Kilyeni; A. Simo","Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara 2, Bd. V. Parvan, Timisoara, Romania; Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara 2, Bd. V. Parvan, Timisoara, Romania; Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara 2, Bd. V. Parvan, Timisoara, Romania; Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara 2, Bd. V. Parvan, Timisoara, Romania; Power Systems Department, &#x201C;Politehnica&#x201D; University of Timisoara 2, Bd. V. Parvan, Timisoara, Romania","Eurocon 2013","","2013","","","1235","1243","Nowadays there is a huge interest in developing new approaches for power system related problem solving. Generally, (meta)heuristic search methods are considered a good alternative to conventional ones. This paper aims to elaborate an original mathematical model focused on particle swarm optimization (PSO) applied for optimal power flow computing (OPF). Several practical issues are presented. One test power system is used as case study. Additionally, the algorithm developed is also applied for a real power system. It refers to the Western and South-Western side of the Romanian Power System. Further, it is intended to be used for transmission expansion planning.","","978-1-4673-2232-4978-1-4673-2230","10.1109/EUROCON.2013.6625138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6625138","particle swarm optimization;optimal power flow;complex power systems;mathematical model;software","Load flow;Linear programming;Mathematical model;Optimization;Minimization;Integrated circuits","optimal systems;particle swarm optimisation;power engineering computing;power systems","transmission expansion planning;Romanian power system;optimal power flow computing;particle swarm optimization;mathematical model;metaheuristic search methods;power system related problem solving;PSO based OPF algorithm","","","28","","","","","","IEEE","IEEE Conferences"
"Modeling and parameter extraction methods of PV modules — Review","S. Pranith; T. S. Bhatti","Centre for Energy Studies, IIT Delhi, New Delhi, India; Centre for Energy Studies, IIT Delhi, New Delhi, India","2015 International Conference on Recent Developments in Control, Automation and Power Engineering (RDCAPE)","","2015","","","72","76","The main objective of this paper is to provide the researchers and developers with a review of the characteristics of PV modules and the different possible methods of extracting parameters for modeling a PV system. Modeling a Photovoltaic (PV) module is essential for both software and hardware implementations, for operating and testing the performance of PV system. PV characteristic curve is non linear and modeling it accurately needs determining the parameters associated with the panel accurately, which are not usually provided in manufacturer data sheets. To get the I-V Curve of a PV panel in single diode model, the characteristic parameters should be extracted from the manufacture's datasheets. This paper analyzes conventional methods and intelligent methods which are available for extraction of parameters for modeling PV modules, along with their respective advantages and limitations.","","978-1-4799-7247-0978-1-4799-7246","10.1109/RDCAPE.2015.7281372","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7281372","PV cell;I-V Curve;empirical;Newton-Rhapson method;Particle Swarm Optimization;Differential Evolution;Genetic Algorithms;curve fitting","Mathematical model;Integrated circuit modeling;Photovoltaic systems;Genetic algorithms;Optimization;Computational modeling","curve fitting;particle swarm optimisation;solar cells","parameter extraction methods;PV modules;parameter extraction;PV system modeling;photovoltaic module;PV characteristic curve;PV panel;single diode model","","4","24","","","","","","IEEE","IEEE Conferences"
"Concoction of UPFC for optimal reactive power dispatch using hybrid GAPSO approach for power loss minimisation","S. S. Shrawane; M. Diagavane; N. Bawane","Department of Electical Engg. GHRCE, Nagpur, India; VIT Nagpur, India; VIT Nagpur, India","2014 6th IEEE Power India International Conference (PIICON)","","2014","","","1","4","The main motive of OPRD is to minimise the transmission loss along with control of voltage profile so that the voltage deviations at the load buses for variations in the loading conditions. With the concoction of FACTS devices, the power control can be finely achieved. The paper describes ORPD for the power loss minimisation using optimal concoction of UPFC and the cost of UPFC for ORPD. The power flow is first solved by NR method without concocting UPFC and then with concoction of UPFC randomly placed in the IEEE-30 bus system. The UPFC allows control of real and reactive power both in addition to voltage magnitude control at various buses. In this paper, OPRD is applied using the hybrid GA-PSO. Simulations are performed on IEEE-30 bus test system using MATLAB software package to ensure efficacy of the proposed algorithm. This paper aims at to find the optimum usage of UPFC which means the finding of the optimal location where their influence would be more useful as well as to determine their cost.","","978-1-4799-6042-2978-1-4799-6041-5978-1-4799-6040","10.1109/POWERI.2014.7117658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7117658","Unified Power flow controller UPFC;Genetic algorithm GA;Particle swarm optimisation PSO;Flexible AC transmission devices FACTs","Propagation losses;Load flow;Reactive power;Optimization;Power system stability;Power transmission lines;Thermal stability","genetic algorithms;load flow control;particle swarm optimisation;power generation dispatch;reactive power control;voltage control","OPRD;transmission loss;voltage profile control;voltage deviations;load buses;loading conditions;FACTS devices;power control;power loss minimisation;UPFC;power flow;NR method;voltage magnitude control;hybrid GA-PSO;optimal reactive power dispatch","","","13","","","","","","IEEE","IEEE Conferences"
"CrowdAdaptor: A Crowd Sourcing Approach toward Adaptive Energy-Efficient Configurations of Virtual Machines Hosting Mobile Applications","E. Y. Y. Kan; W. K. Chan; T. H. Tse","NA; NA; NA","2014 IEEE 38th Annual Computer Software and Applications Conference","","2014","","","493","502","Applications written by end-user programmers are hardly energy-optimized by these programmers. The end users of such applications thus suffer significant energy issues. In this paper, we propose CrowdAdaptor, a novel approach toward locating energy-efficient configurations to execute the applications hosted in virtual machines on handheld devices. CrowdAdaptor innovatively makes use of the development artifacts (test cases) and the very large installation base of the same application to distribute the test executions and performance data collection of the whole test suites against many different virtual machine configurations among these installation bases. It synthesizes these data, continuously discovers better energy-efficient configurations, and makes them available to all the installations of the same applications. We report a multi-subject case study on the ability of the framework to discover energy-efficient configurations in three power models. The results show that Crowd Adaptor can achieve up to 50% of energy savings based on a conservative linear power model.","0730-3157","978-1-4799-3575","10.1109/COMPSAC.2014.72","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899253","Mobile energy consumption;Test harness;Postdeployment validation;Energy optimization;Energy saving","Vectors;Virtual machining;Energy consumption;Energy efficiency;Performance evaluation;Frequency control;Optimization","energy conservation;mobile computing;user interfaces;virtual machines","CrowdAdaptor;crowd sourcing approach;adaptive energy-efficient configurations;virtual machines;mobile applications;end-user programmers;handheld devices","","1","26","","","","","","IEEE","IEEE Conferences"
"The travelling thief problem: The first step in the transition from theoretical problems to realistic problems","M. R. Bonyadi; Z. Michalewicz; L. Barone","School of Computer Science, The University of Adelaide, Australia; SolveIT Software, 99 Frome St, Adelaide, SA 5000, Australia; SolveIT Software, 99 Frome St, Adelaide, SA 5000, Australia","2013 IEEE Congress on Evolutionary Computation","","2013","","","1037","1044","There are some questions concerning the applicability of meta-heuristic methods for real-world problems; further, some researchers claim there is a growing gap between research and practice in this area. The reason is that the complexity of real-world problems is growing very fast (e.g. due to globalisation), while researchers experiment with benchmark problems that are fundamentally the same as those of 50 years ago. Thus there is a need for a new class of benchmark problems that reflect the characteristics of real-world problems. In this paper, two main characteristics of real-world problems are introduced: combination and interdependence. We argue that real-world problems usually consist of two or more sub-problems that are interdependent (to each other). This interdependence is responsible for the complexity of the real-world problems, while the type of complexity in current benchmark problems is missing. A new problem, called the travelling thief problem, is introduced; it is a combination of two well-known problems, the knapsack problem and the travelling salesman problem. Some parameters which are responsible for the interdependence of these two sub-problems are defined. Two sets of parameters are introduced that result in generating two instances of the travelling thief problem. The complexities that are raised by interdependences for these two instances are discussed in detail. Finally, a procedure for generating these two instances is given.","1089-778X;1941-0026","978-1-4799-0454-9978-1-4799-0453-2978-1-4799-0451-8978-1-4799-0452","10.1109/CEC.2013.6557681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557681","Real-world problems;NP-hard problems;combinatorial benchmark problems;complexity","Cities and towns;Benchmark testing;NP-hard problem;Optimization;Storage tanks;Evolutionary computation","knapsack problems;optimisation;travelling salesman problems","travelling thief problem;meta-heuristic methods;real-world problem characteristics;combination characteristics;interdependence characteristics;real-world problem complexity;benchmark problem complexity;knapsack problem;travelling salesman problem","","27","23","","","","","","IEEE","IEEE Conferences"
"Empirical modelling of FDSOI CMOS inverter for signal/power integrity simulation","W. Dghais; J. Rodriguez","Institute of Telecommunications, Department of Electronics, Telecommunications, and Informatics, University of Aveiro, Portugal; Institute of Telecommunications, Department of Electronics, Telecommunications, and Informatics, University of Aveiro, Portugal","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","1555","1558","This paper presents a multiport empirical model based on artificial neural network for I/O memory interface (e.g. inverter) designed based on fully depleted silicon on isolator (FDSOI) CMOS 28 nm process for signal and power integrity assessments. The analog mixed-signal identification signals that carry the information about the I/O interface's nonlinear dynamic behavior are recorded from large signal simulation setup. The model's functions are extracted based on a nonlinear optimization algorithm and then implemented in Simulink software. The performance of the resulted model is validated in typical power and ground switching noise scenario. The developed empirical model accurately predicts the timing signal waveforms at the power, ground, and at the output port.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0212","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092637","FDSOI CMOS inverter;large signal multiport model;signal and power integrity;transient analysis","Semiconductor device modeling;Integrated circuit modeling;Mathematical model;Inverters;Predictive models;CMOS integrated circuits;Solid modeling","CMOS integrated circuits;integrated circuit design;integrated circuit modelling;invertors;mixed analogue-digital integrated circuits;silicon-on-insulator","FDSOI CMOS inverter;signal/power integrity simulation;multiport empirical model;artificial neural network;I/O memory interface;fully depleted silicon on isolator;analog mixed-signal identification signals;nonlinear optimization algorithm;Simulink software;size 28 nm","","3","10","","","","","","IEEE","IEEE Conferences"
"DSP based programmable FHD HEVC decoder","S. Lee; J. Song; W. Lee; D. Kim; J. Kim; S. Lee","DMC R&amp;D Center, Samsung Electronics, Suwon-City, South Korea; DMC R&amp;D Center, Samsung Electronics, Suwon-City, South Korea; DMC R&amp;D Center, Samsung Electronics, Suwon-City, South Korea; DMC R&amp;D Center, Samsung Electronics, Suwon-City, South Korea; DMC R&amp;D Center, Samsung Electronics, Suwon-City, South Korea; DMC R&amp;D Center, Samsung Electronics, Suwon-City, South Korea","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","972","973","A programmable video decoding system with multi-core DSP and co-processors is presented. This system is adopted by Digital TV System on Chip (SoC) and is used for FHD High Efficiency Video Coding (HEVC) decoder under 400MHz. Using the DSP based programmable solution, we can reduce commercialization period by one year because we can parallelize algorithm development, software optimization and hardware design. In addition to the HEVC decoding, the proposed system can be used for other application such as other video decoding standard for multi-format decoder or video quality enhancement.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092529","HEVC;FHD;DSP","Decoding;VLIW;Computer architecture;SDRAM;Hardware;Instruction sets;Standards","coprocessors;decoding;digital television;multiprocessing systems;system-on-chip;video coding","multicore DSP based programmable FHD HEVC decoder;programmable video decoding system;coprocessor system;digital TV system on chip;digital TV SoC;High Efficiency Video Coding decoder;parallelize algorithm development;software optimization;hardware design","","","4","","","","","","IEEE","IEEE Conferences"
"Harnessing Dynamic Interests of Crowd in Chinese Online Shopping Festivals","L. He; X. Xie; H. Jin; F. Liu; X. Ke","NA; NA; NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","2","","806","815","Online shopping is popular in promotion festivals like Singles' Day in China. With the presence of a flash crowd of customers during the limited time in E-commerce websites, the traditional system provides multi-dimensional product ranking lists including popularity based bought lists and viewed lists due to the customers' conformity to the popular products. However, the overloaded information may cause a long path to purchase for customers, who can not efficiently buy satisfied products within the time-limited festivals. To shorten customers' purchasing path, this paper designs an interests-based ranking method to automatically and efficiently aggregate an unified ranking list, by analyzing thousands of customers' shopping behavior to harness the dynamic and wide interests of crowd. Initially, we transform the local interaction behavior of flash crowd into the interests of crowd, which then is further personalized to an unified ranking for each customer. We combine user's temporal interests and current crowd wisdom to generate a new personalized unified ranking. The combined ranking is continuously adjusted with the recent fixed past time window data iteratively according to the evolution growth of crowd. An implementation test is carried out on a real world trace in large-scale interaction logs, which is from Alibaba Tmall with 12 million users and 29 thousand brands. It shows that combining the user's temporal interests and the aggregated crowd wisdom ranking improve the quality of item recommendations distinctly.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273703","E-commerce;user interests;product ranking;prediction","Aggregates;Bayes methods;Market research;Mathematical model;Bipartite graph;Time complexity;Predictive models","consumer behaviour;Internet;iterative methods;purchasing;recommender systems;retail data processing","crowd dynamic interest;Chinese online shopping festival;e-commerce Web site;customer purchasing path;customer shopping behavior;user temporal interest;crowd wisdom;personalized unified ranking;iterative method;item recommendation","","","22","","","","","","IEEE","IEEE Conferences"
"Optimizing Collective Communication in UPC","J. Jose; K. Hamidouche; J. Zhang; A. Venkatesh; D. K. Panda","NA; NA; NA; NA; NA","2014 IEEE International Parallel & Distributed Processing Symposium Workshops","","2014","","","361","370","Message Passing Interface (MPI) has been the defacto programming model for scientific parallel applications. However, data driven applications with irregular communication patterns are harder to implement using MPI. The Partitioned Global Address Space (PGAS) programming models present an alternative approach to improve programmability. PGAS languages like UPC are growing in popularity because of their ability to provide shared-memory programming model over distributed memory machines. However, since UPC is an emerging standard, it is unlikely that entire applications will be re-written with it. Instead, unified communication runtimes have paved the way for a new class of hybrid applications that can leverage the benefits of both MPI and PGAS models. Such unified runtimes need to be designed in a high performance, scalable manner to improve the performance of emerging hybrid applications. Collective communication primitives offer a flexible, portable way to implement group communication operations and are supported in both MPI and PGAS programming models. Owing to their advantages, they are also widely used across various scientific parallel applications. Over the years, MPI libraries have relied upon aggressive software- /hardware-based and kernel-assisted optimizations to deliver low communication latency for various collective operations. However, there is much room for improvement for collective operations in state-of-the-art, open-source implementations of UPC. In this paper, we address the challenges associated with improving the performance of collective primitives in UPC. Further, we also explore design alternatives to enable collective primitives in UPC to directly leverage the designs available in the MVAPICH2 MPI library. Our experimental evaluations show that our designs improve the performance of the UPC broadcast and all-gather operations, by 25X and 18X respectively for 128KB message at 2,048 processes. Our designs improve the performance of the UPC 2D-Heat kernel by up to 2X times at 2,048 processes, and NAS-FT benchmark by 12% at 256 processes.","","978-1-4799-4116-2978-1-4799-4117","10.1109/IPDPSW.2014.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6969411","UPC; Collectives; InfiniBand; Programming Models; PGAS","Electronics packaging;Runtime;Programming;Benchmark testing;Kernel;Libraries;Algorithm design and analysis","application program interfaces;message passing;parallel programming","UPC language;message passing interface;MPI programming model;scientific parallel application;PGAS programming model;partitioned global address space;software-based optimization;hardware-based optimization;kernel-assisted optimization;communication latency;MVAPICH2 MPI library;NAS-FT benchmark","","1","44","","","","","","IEEE","IEEE Conferences"
"Is Source-Code Isolation Viable for Performance Characterization?","C. Akel; Y. Kashnikov; P. d. O. Castro; W. Jalby","NA; NA; NA; NA","2013 42nd International Conference on Parallel Processing","","2013","","","977","984","Source-code isolation finds and extracts the hotspots of an application as independent isolated fragments of code, called codelets. Codelets can be modified, compiled, run, and measured independently from the original application. Source-code isolation reduces benchmarking cost and allows piece-wise optimization of an application. Source-code isolation is faster than whole-program benchmarking and optimization since the user can concentrate only on the bottlenecks. This paper examines the viability of using isolated codelets in place of the original application for performance characterization and optimization. On the NAS benchmarks, we show that codelets capture 92.3% of the original execution time. We present a set of techniques for keeping codelets as faithful as possible to the original hotspots: 63.6% of the codelets have the same assembly as the original hotspots and 81.6% of the codelets have the same run time performance as the original hotspots.","0190-3918;2332-5690","978-0-7695-5117","10.1109/ICPP.2013.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687441","Codelets;source code isolation;performance characterization","In vitro;Benchmark testing;Assembly;In vivo;Runtime;Optimization;Measurement","optimising compilers;source code (software)","source-code isolation;performance characterization;codelets;program benchmarking cost reduction;piecewise optimization;NAS benchmarks","","2","12","","","","","","IEEE","IEEE Conferences"
"Optimization weather parameters influencing rainfall prediction using Adaptive Network-Based Fuzzy Inference Systems (ANFIS) and linier regression","D. Munandar","Research Center for Informatics, Indonesian Institute of Sciences, Bandung, Indonesia","2015 International Conference on Data and Software Engineering (ICoDSE)","","2015","","","1","6","This paper conducted a study to investigate the ability of Adaptive Network-Based Fuzzy Inference System (ANFIS) in doing modeling to determine the weather parameters that influence the output parameters of rainfall (RF) and have good predictive ability. Plotting the data of the prediction is also made to the Linear Regression (LR). The data is tested daily at the weather station in Badau area, Belitung province, Indonesia. A total consisting of 433 pairs of data for 1 year containing seven weather parameters as input and one parameter as output. As for the performance evaluation criteria used indicator of the ability of ANFIS statistic model: Pearson correlation coefficient (r), coefficient of determination (R2) and root mean squared error (RMSE), from several input parameters in the analysis, 1-input RHmax most optimal influencing rainfall (RF) output, (RMSE = 1.8896 mm / day at the training phase and RMSE = 3.2370 mm / day at the checking phase). Plot the data ANFIS against Linear Regression, 1-input parameter RHmax has optimal value of the influence of rainfall (RF) output with optimal statistical indicator (R<sup>2</sup> = 0.7065, r = 0.8405, RMSE = 0.8732 mm / day).","","978-1-4673-8430-8978-1-4673-8428-5978-1-4673-8427-8978-1-4673-8429","10.1109/ICODSE.2015.7436990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436990","Ontology;ANFIS;Linear Regression;Parameter combinations;Plotting data","Meteorology;Predictive models;Linear regression;Mathematical model;Fuzzy logic;Adaptation models;Radio frequency","correlation methods;fuzzy neural nets;fuzzy reasoning;geophysics computing;mean square error methods;optimisation;rain;regression analysis;statistical analysis;weather forecasting","weather parameter optimization;rainfall prediction;adaptive network-based fuzzy inference systems;linear regression;ANFIS data plotting;weather parameters;rainfall output parameters;weather station;Badau area;Belitung province;Indonesia;performance evaluation criteria;ANFIS statistic model;Pearson correlation coefficient;coefficient of determination;root mean squared error;RMSE;optimal statistical indicator","","","17","","","","","","IEEE","IEEE Conferences"
"Game network traffic simulation by a custom bot","T. Alstad; J. Riley Dunkin; S. Detlor; B. French; H. Caswell; Z. Ouimet; Y. Khmelevsky; G. Hains","Computer Science Department, Okanagan College, Kelowna, BC, Canada; Computer Science Department, Okanagan College, Kelowna, BC, Canada; Computer Science Department, Okanagan College, Kelowna, BC, Canada; Computer Science Department, Okanagan College, Kelowna, BC, Canada; Computer Science Department, Okanagan College, Kelowna, BC, Canada; Computer Science Department, Okanagan College, Kelowna, BC, Canada; Computer Science Department, Okanagan College, Kelowna, BC, Canada; Lab. d'Algorithmique, Complexite et Logique, Universite Paris-Est Creteil, France","2015 Annual IEEE Systems Conference (SysCon) Proceedings","","2015","","","675","680","Minecraft is a popular video game played worldwide, and is built simply enough to be used for network analysis and research. This paper describes an automated software agent created to simulate player traffic within the game. Realistic network traffic simulation was the goal that inspired the creation of our “Minecraft bot”: an automatic program or bot that could act in similar ways to a real player, and be able to be mass produced to saturate a local area network. This will facilitate network research by allowing users to have a more scalable testing environment and thus enable controlled laboratory experiments that are impossible to set up in live online gaming environments. The basic commands in Minecraft consist of moving, placing and breaking blocks (pieces of environment) and a realistic bot needs to replicate these actions. Another important objective was to have the ability to create hundreds or thousands of bots doing the same actions, to be able to create artificial latency on the network. This paper will go through the entire lifecycle of our project, starting with some information on existing research about the subject, and how it relates to ours. Following that we describe our bot requirements, the work that was done to find a pre-built solution, the solution we ended up using and how it was modified to fit our requirements. We then have a section showing performance experiments we ran, which compared the packet count and traffic volume between players and bots, as well as cpu usage statistics as more connections were made to the server to ensure that our server hardware was not a factor in our network testing. The final section is the conclusion which talks about the outcome of our project in relation to our original goals, and how it will impact future research in this area.","","978-1-4799-5927","10.1109/SYSCON.2015.7116828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7116828","","Games;Servers;Optimization;Hardware;Testing;Software;Monitoring","computer games;computer network performance evaluation;local area networks;telecommunication traffic","game network traffic simulation;Minecraft video game;automated software agent;Minecraft bot;automatic program;local area network;network research;scalable testing environment;controlled laboratory experiments;live online gaming environments;moving command;placing command;block breaking command;artificial latency;project lifecycle;performance experiments;packet count;traffic volume;CPU usage statistics;server hardware;network testing","","3","15","","","","","","IEEE","IEEE Conferences"
"Access-averse framework for computing low-rank matrix approximations","I. Yamazaki; T. Mary; J. Kurzak; S. Tomov; J. Dongarra","Department of Computer Science, University of Tennessee, Knoxville, Tennessee, U.S.A.; Université de Toulouse, INPT(ENSEEIHT)-IRIT, France; Department of Computer Science, University of Tennessee, Knoxville, Tennessee, U.S.A.; Department of Computer Science, University of Tennessee, Knoxville, Tennessee, U.S.A.; Department of Computer Science, University of Tennessee, Knoxville, Tennessee, U.S.A.","2014 IEEE International Conference on Big Data (Big Data)","","2014","","","70","77","Low-rank matrix approximations play important roles in many statistical, scientific, and engineering applications. To compute such approximations, different algorithms have been developed by researchers from a wide range of areas including theoretical computer science, numerical linear algebra, statistics, applied mathematics, data analysis, machine learning, and physical and biological sciences. In this paper, to combine these efforts, we present an “access-averse” framework which encapsulates some of the existing algorithms for computing a truncated singular value decomposition (SVD). This framework not only allows us to develop software whose performance can be tuned based on domain specific knowledge, but it also allows a user from one discipline to test an algorithm from another, or to combine the techniques from different algorithms. To demonstrate this potential, we implement the framework on multicore CPUs with multiple GPUs and compare the performance of two representative algorithms, blocked variants of matrix power and Lanczos methods. Our performance studies with large-scale graphs from real applications demonstrate that, when combined with communication-avoiding and thick-restarting techniques, the Lanczos method can be competitive with the power method, which is one of the most popular methods currently used for these applications. InIn addition, though we only focus on the truncated SVDs, the two computational kernels used in our studies, the sparse-matrix dense-matrix multiply and tall-skinny QR factorization, are fundamental building blocks for computing low-rank approximations with other objectives. Hence, our studies may have a greater impact beyond the truncated SVDs.","","978-1-4799-5666","10.1109/BigData.2014.7004374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004374","","Vectors;Approximation methods;Approximation algorithms;Sparse matrices;Software algorithms;Kernel;Convergence","approximation theory;graphics processing units;mathematics computing;matrix multiplication;multiprocessing systems;singular value decomposition;sparse matrices","access-averse framework;low-rank matrix approximation computation;truncated singular value decomposition;SVD;domain specific knowledge;multicore CPU;GPU;matrix power blocked variants;Lanczos method;communication-avoiding techniques;thick-restarting techniques;power method;computational kernels;sparse-matrix dense-matrix multiplication;tall-skinny QR factorization","","2","22","","","","","","IEEE","IEEE Conferences"
"An Efficient and Experimentally Tuned Software-Based Hardening Strategy for Matrix Multiplication on GPUs","P. Rech; C. Aguiar; C. Frost; L. Carro","Instituto de Informática, Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, Brazil; Instituto de Informática, Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, Brazil; STFC, Rutherford Appleton Laboratories, Didcot, UK; Instituto de Informática, Federal University of Rio Grande do Sul (UFRGS), Porto Alegre, Brazil","IEEE Transactions on Nuclear Science","","2013","60","4","2797","2804","Neutron radiation experiment results on matrix multiplication on graphic processing units (GPUs) show that multiple errors are detected at the output in more than 50% of the cases. In the presence of multiple errors, the available hardening strategies may become ineffective or inefficient. Analyzing radiation-induced error distributions, we developed an optimized and experimentally tuned software-based hardening strategy for GPUs. With fault-injection simulations, we compare the performance and correcting capabilities of the proposed technique with the available ones.","0018-9499;1558-1578","","10.1109/TNS.2013.2252625","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6510503","Graphic processing unit (GPU);matrix multiplication;neutron radiation testing;software-based hardening","Graphics processing units;Instruction sets;Neutrons;Algorithm design and analysis;Vectors;Error analysis;Radiation hardening (electronics)","graphics processing units;matrix algebra;neutron effects;radiation hardening (electronics)","software-based hardening strategy;matrix multiplication;GPU;neutron radiation;graphic processing units;multiple errors;radiation-induced error distributions;fault-injection simulations","","31","14","","","","","","IEEE","IEEE Journals & Magazines"
"Hardware-assisted power estimation for design-stage processors using FPGA emulation","S. Hesselbarth; T. Baumgart; H. Blume","Institute of Microelectronic Systems, Leibniz Universit&#x00E4;t Hannover, Appelstra&#x00DF;e 4, 30167, Germany; Institute of Microelectronic Systems, Leibniz Universit&#x00E4;t Hannover, Appelstra&#x00DF;e 4, 30167, Germany; Institute of Microelectronic Systems, Leibniz Universit&#x00E4;t Hannover, Appelstra&#x00DF;e 4, 30167, Germany","2014 24th International Workshop on Power and Timing Modeling, Optimization and Simulation (PATMOS)","","2014","","","1","8","This paper presents the application of an accurate power estimation model for design-stage processors that can be mapped onto an FPGA together with the functional emulation. Based on a hybrid functional level power analysis (FLPA) and instruction level power analysis (ILPA) approach, the model enables the estimation of application-specific power consumption and energy per task at very early design stages of programmable embedded processors. The extremely short execution time of the emulated power model compared to gate-transfer level (GTL) power simulation allows both hardware and software designers to constantly optimize their implementations for low-power iteratively in different design stages. The power consumption modeling methodology used for this work and necessary considerations for FPGA implementation are described. The presented model is validated against GTL power simulation with respect to execution time and precision by benchmarking for an exemplary embedded RISC processor core, the LEON2. Benchmarking results yield a percentage mean absolute error (%MAE) of less than 9% and normalized root mean square error (NRMSE) of less than 6% while reducing power estimation time from several hours down to a few milliseconds. Finally, a case-study with varying real-world input data sizes has been performed on different software implementations of JPEG encoder and decoder applications and optimized processor core. With software and hardware optimizations applied, required energy per task has been reduced by up to 46% for the JPEG encoder and 39% for the JPEG decoder, demonstrating the advantage of the presented approach.","","978-1-4799-5412","10.1109/PATMOS.2014.6951877","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6951877","","Program processors;Estimation;Power demand;Field programmable gate arrays;Optimization;Emulation;Hardware","application specific integrated circuits;benchmark testing;circuit simulation;decoding;encoding;field programmable gate arrays;integrated circuit design;low-power electronics","JPEG decoder;energy per task;JPEG encoder;normalized root mean square error;percentage mean absolute error;benchmarking;LEON2;embedded RISC processor core;gate-transfer level power simulation;power consumption modeling;application-specific power consumption;instruction level power analysis;hybrid functional level power analysis;functional emulation;FPGA emulation;design-stage processors;hardware-assisted power estimation","","2","15","","","","","","IEEE","IEEE Conferences"
"Quantitative gamma-spectrometry of nuclear material under conditions of limited information about assayed objects","S. Yoon; A. Berlizov; A. Lebrun","Department of Safeguards, International Atomic Energy Agency, Vienna International Centre, PO Box 100, 1400 Vienna, Austria; Department of Safeguards, International Atomic Energy Agency, Vienna International Centre, PO Box 100, 1400 Vienna, Austria; Department of Safeguards, International Atomic Energy Agency, Vienna International Centre, PO Box 100, 1400 Vienna, Austria","2015 4th International Conference on Advancements in Nuclear Instrumentation Measurement Methods and their Applications (ANIMMA)","","2015","","","1","7","Canberra Industries has developed a specialized software package, Advanced In-Situ Object Counting System (A-ISOCS), which is based on absolute gamma-spectrometric measurements. It is intended for the verification of the quantity of nuclear materials under conditions where measurements are challenged by incomplete information on the properties of assayed objects which are inspected by the International Atomic Energy (IAEA)'s inspectors. A-ISOCS introduces a technique, which is called the automated optimization that allows the users to determine unknown sample parameters as well as the mass and isotopic composition of nuclear materials. The optimization process is carried out by using benchmarks referring the information included within the gamma-ray spectrum of the measured item. For each benchmark, Figure-of-Merit (FOM) is calculated and used to find the most consistent quantitative result (the best solution) corresponded to the minimized FOM. We have evaluated the capabilities and performance of A-ISOCS based on the measurements of uranium standard reference material samples, which have well-known geometry parameters and physical properties. In addition the applicability of the software to field measurements was assessed with an extensive set of gamma-ray spectra from holdup materials collected at uranium bulk handling facilities. This paper summarizes the results of our evaluation activity.","","978-1-4799-9918-7978-1-4799-9917","10.1109/ANIMMA.2015.7465574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7465574","","Optimization;Uranium;Benchmark testing;Fuels;Geometry;Standards;Atomic measurements","fission reactor instrumentation;fission reactor materials;nuclear engineering computing","automated optimization;clear material mass composition;clear material isotopic composition;optimization process;gamma-ray spectrum;figure-of-merit;FOM;uranium standard reference material;geometry parameters;physical property;software applicability;holdup materials;uranium bulk handling facility;IAEA inspectors;International Atomic Energy inspectors;assayed object property;absolute gamma-spectrometric measurements;A-ISOCS;advanced in-situ object counting system;specialized software package;Canberra Industries;limited assayed object information;nuclear material verification;quantitative gamma-spectrometry","","","6","","","","","","IEEE","IEEE Conferences"
"Bottleneck-centric pull and push allocation and sequencing of wet-bench and furnace tools","Y. Kao; S. Zhan; S. Chang","National Taiwan University, No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan (R.O.C); National Taiwan University, No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan (R.O.C); National Taiwan University, No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan (R.O.C)","2013 e-Manufacturing & Design Collaboration Symposium (eMDC)","","2013","","","1","4","Furnace tool group is often a bottleneck of semiconductor fabrication, which makes effective scheduling of furnace tools and its upstream tool group, wet-bench, important to operation efficiency. Major challenges come from not only the problem complexity such as batching requirements, internal tool sequence, and waiting time limitations but also a reasonable computation time for rescheduling. This paper develops a novel solution scheme, a push-and-then-pull bottleneck centric integrated allocation and sequencing method (P&P BCIASM), to integrate the sequencing and allocation for wafers processing by wet bench and furnace tools effectively. Furnace tool allocations, first derived from a coarse wet-fine furnace model, are viewed as pull demands for allocating and sequencing wet-bench tools. The allocations of wet tools thus obtained are considered as push supplies for refining the scheduling of furnace tools. A commonly available optimization tool is skillfully applied to solution finding. Test results over problem instances extracted from real fab data demonstrate that P&P BCIASM leads to near-optimal schedules, 2.56% higher utilization in average than a hybrid allocation scheme of optimization and manual adjustment, and 15.04% mean waiting time reductions. The violation of waiting time limitation is reduced to none. The computation time in average is within one minute, indicating a strong potential for applications to both large size problems and dynamic allocations.","","978-1-4799-4709","10.1109/eMDC.2013.6756048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6756048","bottleneck-centric pull and push solution scheme;tool allocation and sequencing;efficiency;optimization","Sequential analysis;Resource management;Furnaces;Lead;Optimization;Job shop scheduling;Computer aided software engineering","batch processing (industrial);furnaces;optimisation;push-pull production;resource allocation;semiconductor device manufacture","bottleneck-centric pull-push allocation;sequencing;wet bench tools;furnace tool scheduling;semiconductor fabrication;rescheduling;wafer processing;optimization;BClASM;near-optimal schedules;mean waiting time reductions;batching requirements;internal tool sequence;waiting time limitations","","","9","","","","","","IEEE","IEEE Conferences"
"On the effectiveness of the coupling FEM-BEM approach for solving the elasto-plastic problems","D. Boumaiza; B. Aour","Laboratoire de Biom&#x00E9;canique Appliqu&#x00E9;e et Biomat&#x00E9;riaux, Facult&#x00E9; de G&#x00E9;nie M&#x00E9;canique et G&#x00E9;nie des proc&#x00E9;d&#x00E9;s, BP 32 El Alia 16111 Bab Ezzouar Alger, Alg&#x00E9;rie; Laboratoire de Biom&#x00E9;canique Appliqu&#x00E9;e et Biomat&#x00E9;riaux, D&#x00E9;partement de G&#x00E9;nie M&#x00E9;canique, BP 1323 En Mnaour, ENP Oran, Alg&#x00E9;rie","2013 5th International Conference on Modeling, Simulation and Applied Optimization (ICMSAO)","","2013","","","1","6","In this paper an interface relaxation finite element-boundary element coupling method has been developed to analyse the non-linear elasto-plastic problems of solid mechanics. Indeed, this coupling approach preserves the nature of the finite element method (FEM) and the boundary element method (BEM) and it does not require any access to the matrices generated by FEM and BEM. Hence, it becomes easier to use different software packages without any difficulty. The successive computations of the displacements and forces or tractions on the interface of the finite element and boundary element sub-domains were performed through an iterative relaxation procedure. The coupling FEM-BEM approach was implemented in a computer code and was tested through several examples of elasto-plastic media. The obtained results are compared to the experimental data and analytical solutions to establish their accuracy. Conventional methods FEM and BEM were also exploited, and a critical comparison of the results was carried out.","","978-1-4673-5814-9978-1-4673-5812-5978-1-4673-5813","10.1109/ICMSAO.2013.6552721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552721","Coupling FEM-BEM;Finite element;Boundary element;Elasto-plasticity","Finite element analysis;Software;Vectors;Stress;Strain","boundary-elements methods;elastoplasticity;finite element analysis;iterative methods;nonlinear dynamical systems;physics computing;software packages","coupling FEM-BEM approach effectiveness;interface relaxation finite element-boundary element coupling method;nonlinear elastoplastic problems;solid mechanics;finite element method nature;boundary element method nature;software packages;displacement successive computation;force successive computation;traction successive computation;finite element subdomain interface;boundary element subdomain interface;iterative relaxation procedure;computer code;elastoplastic media;analytical solutions","","","18","","","","","","IEEE","IEEE Conferences"
"Explicating symbolic execution (xSymExe): An evidence-based verification framework","J. Hatcliff; Robby; P. Chalin; J. Belt","Department of Computing and Information Sciences, College of Engineering, Kansas State University, USA; Department of Computing and Information Sciences, College of Engineering, Kansas State University, USA; Department of Computing and Information Sciences, College of Engineering, Kansas State University, USA; Department of Computing and Information Sciences, College of Engineering, Kansas State University, USA","2013 35th International Conference on Software Engineering (ICSE)","","2013","","","222","231","Previous applications of symbolic execution (Sym-Exe) have focused on bug-finding and test-case generation. However, SymExe has the potential to significantly improve usability and automation when applied to verification of software contracts in safety-critical systems. Due to the lack of support for processing software contracts and ad hoc approaches for introducing a variety of over/under-approximations and optimizations, most SymExe implementations cannot precisely characterize the verification status of contracts. Moreover, these tools do not provide explicit justifications for their conclusions, and thus they are not aligned with trends toward evidence-based verification and certification. We introduce the concept of explicating symbolic execution (xSymExe) that builds on a strong semantic foundation, supports full verification of rich software contracts, explicitly tracks where over/under-approximations are introduced or avoided, precisely characterizes the verification status of each contractual claim, and associates each claim with explications for its reported verification status. We report on case studies in the use of Bakar Kiasan, our open source xSymExe tool for Spark Ada.","0270-5257;1558-1225","978-1-4673-3076-3978-1-4673-3073","10.1109/ICSE.2013.6606568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606568","","Contracts;Concrete;Indexes;Sparks;Arrays;Software;Semantics","formal verification;safety-critical software","symbolic execution;open source xSymExe tool;evidence-based verification framework;software contract;safety-critical system;evidence-based certification;semantic foundation;Bakar Kiasan;Spark Ada","","1","23","","","","","","IEEE","IEEE Conferences"
"Primary research on the system identification method of pitching and heave motion of composite trimaran","F. Zhu; S. Yang","China Ship Scientific Research Center, Wuxi 214000, China; Jiangsu University of Science and Technology, Zhenjiang 212000, China","The 26th Chinese Control and Decision Conference (2014 CCDC)","","2014","","","1172","1176","This article established the mathematical model of system identification of composite trimaran pitching and heave motion model based on the genetic optimization algorithm, using VB language to write the recognition software, Design and produce a composite trimaran test model, doing the free pitching and heaving decay test, then completing the pitching and heave motion system identification according to the test data, and got their motion equation which will be used to do the forecast, the results of forecast can well match with the test results, confirmed the feasibility of this set of identification method. Also using the simulation software called SIMULINK in MATLAB to simulate the vertical acceleration, getting the ideal forecasting result.","1948-9439;1948-9447","978-1-4799-3708-0978-1-4799-3707-3978-1-4799-3706","10.1109/CCDC.2014.6852343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6852343","Composite Trimaran;Ship Model Test;Seakeeping;System Identification","Mathematical model;Electronic mail;System identification;Predictive models;MATLAB;Marine vehicles;Data models","boats;genetic algorithms;naval engineering computing;Visual BASIC","system identification method;composite trimaran test model;pitching motion;heave motion;genetic optimization algorithm;VB language","","","1","","","","","","IEEE","IEEE Conferences"
"High speed real time data acquisition system using PXIe technology","S. Soni; A. Tandon; K. Mankadiya","Student of Control &amp; Automation, Nirma Institute of Technology, Project Trainee at Optimized Solutions Pvt. Ltd., Ahmedabad; Instrumentation &amp; Control Engineering Nirma Institute of Technology Ahmedabad; Optimized Solutions Pvt. Ltd, Ashram Road Ahmedabad","2013 Nirma University International Conference on Engineering (NUiCONE)","","2013","","","1","4","PCI eXtension for Instrumentation (PXI) is a rugged PC-based platform that offers a high-performance, low-cost deployment solution for measurement and automation systems. PXI combines the Peripheral Component Interconnect (PCI) electrical bus with the rugged, modular Euro card mechanical packaging of CompactPCI and adds specialized synchronization buses and key software features. PXI also adds mechanical, electrical, and software features that define complete systems for test and measurement, data acquisition Building on PXI capabilities, PXI Express provides the additional timing and synchronization features of a 100 MHz differential system clock, differential signaling, and differential star triggers. By using differential clocking and synchronization, PXI Express systems benefit from increased noise immunity for instrumentation clocks and the ability to transmit at higher-frequency rates. So the purpose of this paper is to use Ni PXIe to acquire real time system parameters and system interlocks using real time operation system for better deterministic approach.","2375-1282","978-1-4799-0727-4978-1-4799-0726","10.1109/NUiCONE.2013.6780169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6780169","Bandwidth;Data Acquisition;latency;PXIe;RTOS","Bandwidth;Synchronization;Real-time systems;Backplanes;Data acquisition;Software;Control systems","data acquisition;peripheral interfaces;real-time systems;synchronisation","high speed real time data acquisition system;PXIe technology;PCI eXtension for Instrumentation;rugged PC-based platform;peripheral component interconnect electrical bus;modular Euro card mechanical packaging;CompactPCI;differential system clock;differential signaling;differential star triggers;PXI express systems;instrumentation clocks;real time system parameters;system interlocks;real time operation system","","1","6","","","","","","IEEE","IEEE Conferences"
"Priority-based parameter performance optimization for EDCA","Z. Wang; X. Guo","School of Information Engineering Communication University of China, Beijing, China; Computer Nic Center, Communication University of China, Beijing, China","Proceedings of 2013 3rd International Conference on Computer Science and Network Technology","","2013","","","685","688","The IEEE 802.11 working group released the IEEE 802.11e protocol to provide QoS for different transmission business. The analysis of the EDCA mechanism in the IEEE 802.11e is made for the purpose of improving the network transmission performance and the QoS of the EDCA mechanism under different network load conditions. The focuses of this article are three priority parameters (AIFS, CW_min and CW_max, TXOP) of the EDCA mechanism. In this paper, with the help of the NS2 software, we build a simulation environment in which the simulation contrast tests of each priority parameter are made at first. Then we observe in different network load conditions that the effect of each priority parameter has on the network transmission performance and QoS. Finally, based on the analysis of simulation results, we conclude that the network performance of EDCA and QoS can be enhanced by two means: adjusting the parameters to reduce the waiting time in low-load circumstances while varying the parameters to minimize the conflicting time in heavy-load conditions.","","978-1-4799-0561","10.1109/ICCSNT.2013.6967204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6967204","IEEE 802.11e;EDCA;QoS;AIFS;CW_min and CW_max;TXOP;NS2","Business;IEEE 802.11 Standards;Real-time systems;Quality of service;Throughput;Load modeling;Simulation","data communication;protocols;quality of service;wireless LAN","priority-based parameter performance optimization;EDCA;IEEE 802.11e protocol;QoS;AIFS;CW_min;CW_max;TXOP;NS2 software","","1","12","","","","","","IEEE","IEEE Conferences"
"The optimized design and experimental analysis of CPC-PV/T system","C. Haiping; W. Chenhui; S. Jieling; A. Dengxin; Y. Chentao; L. Zhaohao; W. Jinjia","School of Energy, Power and Mechanical Engineering, National Thermal Power Engineering and Technology Research Center, North China Electric Power University, 102206, Beijing, China; School of Energy, Power and Mechanical Engineering, National Thermal Power Engineering and Technology Research Center, North China Electric Power University, 102206, Beijing, China; School of Energy, Power and Mechanical Engineering, National Thermal Power Engineering and Technology Research Center, North China Electric Power University, 102206, Beijing, China; School of Energy, Power and Mechanical Engineering, National Thermal Power Engineering and Technology Research Center, North China Electric Power University, 102206, Beijing, China; School of Energy, Power and Mechanical Engineering, National Thermal Power Engineering and Technology Research Center, North China Electric Power University, 102206, Beijing, China; School of Energy, Power and Mechanical Engineering, National Thermal Power Engineering and Technology Research Center, North China Electric Power University, 102206, Beijing, China; State Key Laboratory of Multiphase Flow in Power Engineering, Xi'an Jiaotong University, 710049, Shanxi Province, China","International Conference on Renewable Power Generation (RPG 2015)","","2015","","","1","5","With the CPC integrated photovoltaic / thermal (PV/T) system as the research object, this paper studies the design techniques, builds experimental platform of the system and analyzes its thermal and electrical performance. Firstly, the basic architecture parameters of CPC is designed and the key factor which influences the concentrating uniformity of CPC is found via the software named light tools simulation so that the light distribution uniformity of PV panel is improved. Secondly, the manufacture technique of PV/T collector is designed, including the determination of the size of PV module and the external and internal dimensions of the flat cassette collector. The entire PV/T collector is manufactured by means of direct lamination. Finally, according to the manufactured CPC and PV/T, the experimental platform of CPC integrated PV/T system, whose power is 1kW, is built. The thermal-electrical performance evaluation model of the experimental platform is established. The thermal-electrical performance of the experimental platform is tested and the results show that the water temperature of the outlet can be above 45°C and the total efficiency can reach 60%.C.","","978-1-78561-041-7978-1-78561-040","10.1049/cp.2015.0501","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446658","concentrating;photovoltaic/thermal;system design;thermal-electrical performance","","photovoltaic power systems;solar absorber-convertors;solar energy concentrators","thermal-electrical performance evaluation;flat cassette collector;manufacture technique;light distribution uniformity;software named lighttools simulation;integrated photovoltaic/thermal system;CPC-PV/T system;combined photovoltaic collector system;optimized design","","","","","","","","","IET","IET Conferences"
"ALLARM: Optimizing sparse directories for thread-local data","A. Roy; T. M. Jones","EPFL; University of Cambridge","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","6","Large-scale cache-coherent systems often impose unnecessary overhead on data that is thread-private for the whole of its lifetime. These include resources devoted to tracking the coherence state of the data, as well as unnecessary coherence messages sent out over the interconnect. In this paper we show how the memory allocation strategy for non-uniform memory access (NUMA) systems can be exploited to remove any coherence-related traffic for thread-local data, as well removing the need to track those cache lines in sparse directories. Our strategy is to allocate directory state only on a miss from a node in a different affinity domain from the directory. We call this ALLocAte on Remote Miss, or ALLARM. Our solution is entirely backward compatible with existing operating systems and software, and provides a means to scale cache coherence into the many-core era. On a mix of SPLASH2 and Parsec workloads, ALLARM is able to improve performance by 13% on average while reducing dynamic energy consumption by 9% in the on-chip network and 15% in the directory controller. This is achieved through a 46% reduction in the number of sparse directory entries evicted.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800292","","Probes;Benchmark testing;Coherence;Instruction sets;Resource management;Message systems;Random access memory","cache storage;network-on-chip;operating systems (computers)","ALLARM;sparse directory optimization;thread-local data;large-scale cache-coherent system;unnecessary coherence message;interconnection;memory allocation strategy;nonuniform memory access system;NUMA system;allocate on remote miss;operating system;SPLASH2;Parsec workload;energy consumption;on-chip network","","","21","","","","","","IEEE","IEEE Conferences"
"Augmenting JavaScript JIT with ahead-of-time compilation","R. Zhuykov; V. Vardanyan; D. Melnik; R. Buchatskiy; E. Sharygin","Institute for System Programming of Russian Academy of Sciences, Alexander Solzhenitsyn st., 25, Moscow, Russia; Institute for System Programming of Russian Academy of Sciences, Alexander Solzhenitsyn st., 25, Moscow, Russia; Institute for System Programming of Russian Academy of Sciences, Alexander Solzhenitsyn st., 25, Moscow, Russia; Institute for System Programming of Russian Academy of Sciences, Alexander Solzhenitsyn st., 25, Moscow, Russia; Institute for System Programming of Russian Academy of Sciences, Alexander Solzhenitsyn st., 25, Moscow, Russia","2015 Computer Science and Information Technologies (CSIT)","","2015","","","116","120","Modern JavaScript engines use just-in-time (JIT) compilation to produce a binary code. JIT compilers are limited in a complexity of optimizations they can perform at a runtime without delaying an execution. On the contrary, ahead-of-time (AOT) compilers don't have such limitations, but they are not well suited for compiling dynamic languages such as JavaScript. In the paper we discuss methods for augmenting multi-tiered JavaScript JIT with a capability for AOT compilation, so to reduce program startup time and to move complex optimizations to AOT phase. We have implemented saving of JavaScript programs as a binary package containing bytecode and native code in open-source WebKit library. Our implementation allows shipping of JavaScript programs not only as a source code, but also as application binary packages with a precompiled code. In addition, our approach does not require any language feature restrictions. This has resulted in performance gain for popular JavaScript benchmarks such as SunSpider and Kraken on ARM platform, however, at a cost of increased package size.","","978-1-4673-7562","10.1109/CSITechnol.2015.7358262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358262","JavaScript;JIT;Ahead-of-Time Compilation;JavaScriptCore;WebKit","Optimization;Engines;Benchmark testing;Loading;Binary codes;Dynamic compiler;Libraries","binary codes;Java;just-in-time;optimisation;program compilers;public domain software","JavaScript JIT augmentation;ahead-of-time compilation;JavaScript engines;just-in-time compilation;JIT compilation;binary code;JIT compilers;optimization complexity;ahead-of-time compilers;AOT compilers;dynamic languages;multitiered JavaScript JIT;JavaScript programs;binary package;open-source WebKit library;source code;binary packages;language feature restrictions;JavaScript benchmarks;ARM platform","","","11","","","","","","IEEE","IEEE Conferences"
"Analysis and optimization of the system-level simulator","L. Fang; Z. Shengbing; L. Yang; Z. Meng","School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an China 710072; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an China 710072; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an China 710072; School of Computer Science and Engineering, Northwestern Polytechnical University, Xi'an China 710072","2014 IEEE International Conference on Information and Automation (ICIA)","","2014","","","1020","1024","Simulator is a software system running on the host and the behaviour of the simulated target architecture machine. It can explain and execute the executable program on the target architecture machine, while providing run-time instruction and event-related records, as well as the performance of statistical parameters of the target architecture machine. Systemlevel architecture simulator can be used as a virtual target machine running the software system can achieve the functional simulation of the single (multi) processor, system memory, cache, and an external device subsystem. The simulator is an important means for the processor architecture. Accurate simulation of the processor architecture can access and analyze the architecture parameters on the performance of the processor, to provide effective support for the architecture design space exploration. Simulator development purposes and a different focus, vary widely in speed, uses simulation target, degree of simulation. This article is based on the characteristics of different workloads to study the performance of several state-of-art system-level architecture simulators and focuses on the method that can improve the system performance significantly. We take SPLASH-2 as the benchmark to test the performance of these representative system-level architecture simulators. Finally we give a summary on the optimization methods of different kinds of simulators.","","978-1-4799-4100","10.1109/ICInfA.2014.6932799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6932799","system-level architecture simulators;simulator performance;splash-2","Benchmark testing;Bandwidth;Educational institutions;Accuracy;Memory management;Prefetching","computer architecture;optimisation;performance evaluation;shared memory systems;simulation","system-level simulator optimization;target architecture machine;executable program;run-time instruction;event-related records;multiprocessor system;system memory;processor architecture;architecture parameter analysis;processor performance;SPLASH-2","","","11","","","","","","IEEE","IEEE Conferences"
"Minimizing sensors for system monitoring - a case study with EEG signals","G. Chakraborty; S. Horie; H. Yokoha; Z. Kokosiński","Department of Software & Information Science, Iwate Prefectural University, Japan; Graduate School of Software & Information Science, Iwate Prefectural University, Japan; Graduate School of Software & Information Science, Iwate Prefectural University, Japan; Cracow University of Technology, Faculty of ECE, Dept. of Automatic Control and Information Technologies, ul. Warszawska 24, 31-155 Kraków, Poland","2015 IEEE 2nd International Conference on Cybernetics (CYBCONF)","","2015","","","206","211","For monitoring any system, be it a chemical plant, a nuclear power station, or a human heart or brain, we need to attach sensors and analyze the multivariate time-series data collected by those sensors. If the system has two states, say state 1 (good) and state 2 (bad), we need to infer which state the system is, by classifying the collected time-series signals. To make this work efficiently, it is important to search the least number of probes that would give best classification result. It is a multi-objective optimization problem. The proposed approach works in two steps. We start with a large number of probes. As the first step, we cluster the time-series signals. and choose a representative one from each cluster. Next, we run pareto GA to select the smallest set of probes (from cluster representatives), that would give the highest classification result. Depending on the nature of the signals, and the target application, appropriate signal-features, clustering and classification algorithms will be different, but the basic principle is applicable to any system. In this paper, we tested the effectiveness of our algorithm with EEG signals, to detect the presence or absence of ERP 300. Improved results with less number of probes compared with previous works validated the approach.","","978-1-4799-8322-3978-1-4799-8320","10.1109/CYBConf.2015.7175933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7175933","Ward's Hierarchical Clustering;Dendrogram;Electroencephalogram (EEG);Brain Computer Interface (BCI);Pareto GA;Artificial Neural Network Classifier","Probes;Electroencephalography;Clustering algorithms;Ash;Sensors;Monitoring;Electrodes","electroencephalography;genetic algorithms;medical signal processing;Pareto optimisation;pattern clustering;signal classification;time series","system monitoring;EEG signals;multivariate time-series data;multiobjective optimization problem;time-series signal clustering;Pareto GA;signal classification;ERP 300","","2","12","","","","","","IEEE","IEEE Conferences"
"Performance analysis of parallel master-slave Evolutionary strategies (μ,λ) model python implementation for CPU and GPGPU","D. Zubanovic; A. Hidic; A. Hajdarevic; N. Nosovic; S. Konjicija","Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina; Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina","2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","","2014","","","1609","1613","Evolutionary strategies is a heuristic, guided-search based evolutionary algorithm, widely used as optimization technique for computationally intensive problems. Python is a high-level programming language known for code readability, reusability and the ease of use, making it preferable choice for quick and robust software development, although it is lacking in performance and concurrency area. Emerging technologies such as Anaconda Accelerate Python compiler attempt to combine Python's ease of use with both declarative and explicit parallelization and high performance in computationally intensive problems. In this paper an example of master - slave parallel Evolutionary strategy ES(μ,λ) implementation in Python is given, and its performance on CPU and GPU are analyzed.","","978-953-233-077-9978-953-233-081","10.1109/MIPRO.2014.6859822","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859822","Parallel computing;Python-Anaconda;GPU;Multithreading;Heuristic algorithms;Test functions for optimization","Sociology;Statistics;Graphics processing units;Acceleration;Master-slave;Computational modeling;Heuristic algorithms","evolutionary computation;graphics processing units;high level languages;mathematics computing;parallel processing;program compilers;search problems;software reusability","performance analysis;Python parallel master-slave evolutionary strategy (μ,λ) model;GPGPU;CPU;heuristic guided-search based evolutionary algorithm;optimization technique;high-level programming language;code readability;robust software development;concurrency area;Anaconda Accelerate Python compiler;ES(μ,λ);code reusability","","1","7","","","","","","IEEE","IEEE Conferences"
"Near-Real-Time Parameter Estimation of an Electrical Battery Model With Multiple Time Constants and SOC-Dependent Capacitance","W. Wang; H. Shu-Hung Chung; J. Zhang","Centre for Smart Energy Conversion and Utilization Research, City University of Hong Kong, Kowloon, Hong Kong; Centre for Smart Energy Conversion and Utilization Research, City University of Hong Kong, Kowloon, Hong Kong; Department of Computer Science , Key Laboratory of Digital Life, Ministry of Education, Key Laboratory of Software Technology, Education Department of Guangdong Province, Sun Yat-sun University, Guangzhou, China","IEEE Transactions on Power Electronics","","2014","29","11","5905","5920","A modified particle swarm optimization algorithm for conducting near-real-time parameter estimation of an electrical model for lithium batteries is presented. The model comprises a dynamic capacitance for characterizing the nonlinear relationship between the battery electromotive force and the state-of-charge, and a resistor-capacitor network for characterizing the static and transient responses. The algorithm is confirmed by successfully determining all parameters in a predefined simulation model. It is also evaluated on a hardware test bed with two samples of 3.3-V, 40-Ah, Lithium Iron Phosphate (LiFePO<sub>4</sub>) battery driven under six different loading patterns. The intrinsic parameters are estimated by first processing 15-min samples of the battery terminal voltage and current. The whole process takes 2 min. Then, the voltage-current characteristics in the following 15 min are predicted. Results show that the extracted parameters can fit the first 15-min voltage samples with a maximum error of 16 mV and an average error of 3.8 mV. With the extracted parameters, the electrical model can predict voltage-current characteristics in the following 15 min with a maximum error of 31 mV and an average error of 15 mV. The algorithm is further verified by successfully determining the emulated variation of the output resistance.","0885-8993;1941-0107","","10.1109/TPEL.2014.2300143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714474","Battery model;battery storage system;online parameter estimation;particle swarm optimization (PSO);state of charge (SOC)","Batteries;Integrated circuit modeling;Mathematical model;Computational modeling;Parameter estimation;System-on-chip;Estimation","capacitance;capacitors;electric potential;iron compounds;lithium compounds;particle swarm optimisation;resistors;secondary cells;transient response","real-time parameter estimation;electrical battery model;multiple time constants;SOC-dependent capacitance;particle swarm optimization algorithm;dynamic capacitance;battery electromotive force;resistor-capacitor network;transient response;hardware test;lithium iron phosphate battery;voltage-current characteristics;voltage 3.3 V;time 2 min;time 15 min;LiFePO<sub>4</sub>","","24","35","","","","","","IEEE","IEEE Journals & Magazines"
"Efficient hardware implementation of PSO-based object tracking system","N. N. Morsi; M. B. Abdelhalim; K. A. Shehata","Arab Academy for Science and Technology and Maritime Transport (AASTMT), Cairo, Egypt; Arab Academy for Science and Technology and Maritime Transport (AASTMT), Cairo, Egypt; Arab Academy for Science and Technology and Maritime Transport (AASTMT), Cairo, Egypt","2013 International Conference on Electronics, Computer and Computation (ICECCO)","","2013","","","155","158","This paper proposes hardware design, modelling and implementation for object tracking system targeting video sequence based on Particle Swarm Optimization (PSO). The system uses Structural SIMilarity index (SSIM) as PSO's fitness function. In this paper, we describe Particle Swarm Optimisation algorithm and Structural SIMilarity index, and then we show how it has been simplified for hardware implementation and achieved better results than its software implementation counterpart. Finally, we present the proposed algorithm's performance that has been tested and evaluated over different video sequences, showing its adaption for tracking in real-time.","","978-1-4799-3343","10.1109/ICECCO.2013.6718252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6718252","PSO;Structural SIMilarity;Object Tracking;FPGA","Object tracking;Particle swarm optimization;Indexes;Hardware;Sociology;Statistics;Video sequences","image sequences;object tracking;particle swarm optimisation;video signal processing","hardware implementation;PSO-based object tracking system;video sequence;particle swarm optimization;structural similarity index;SSIM;PSO fitness function","","2","15","","","","","","IEEE","IEEE Conferences"
"Design and test of a thermal measurement system prototype for SPIDER experiment","M. D. Palma; N. Pomaro; C. Taliercio; R. Pasqualotto","Consorzio RFX - Associazione Euratom-ENEA sulla Fusione, Padova, Italy; Consorzio RFX - Associazione Euratom-ENEA sulla Fusione, Padova, Italy; Consorzio RFX - Associazione Euratom-ENEA sulla Fusione, Padova, Italy; Consorzio RFX - Associazione Euratom-ENEA sulla Fusione, Padova, Italy","2013 IEEE 25th Symposium on Fusion Engineering (SOFE)","","2013","","","1","6","SPIDER, the full-size prototype of the ITER Heating Neutral Beams (HNBs) RF Ion Source, has been designed and is under construction at Consorzio RFX in Padua, Italy. Several thermocouples will be installed in both SPIDER and HNB Source to characterize and monitor the possibly severe thermal load on mechanical components. The presence of about 1 MW RF power ionizing the gas and frequent electrical breakdowns on the 100 kV beam accelerator represent a very harsh environment for thermal measurements, both in terms of possible sensors damage and of disturbances induced on the signals. A complete measurement chain prototype, composed of two sensors, conditioning electronics and data acquisition system, was realized in order to test and validate technologies and design solutions. The prototype was installed and tested inside the BATMAN RF Ion source at IPP-Garching, where measurement conditions are very similar to those expected in SPIDER, and in particular high RF power and high voltage breakdowns are present. A careful installation was carried out, with extensive optimization of grounding and shielding configuration, in order to achieve sufficient EMI immunity. Analog and digital filtering were also implemented on signals to eliminate the residual noise. Results produced during a dedicated experimental campaign on BATMAN have been very successful: 0.5 °C measurement global accuracy can be obtained during RF plasma pulses and no damage occurred on sensors or electronic equipments. The paper describes the prototype design detailing the technical solutions implemented, in particular regarding sensors design and installation, signal conditioning and noise reduction. Hardware and software characteristics of the data acquisition system are presented and discussed. Experimental results obtained are also described and their significance for future work is analyzed.","1078-8891;2155-9953","978-1-4799-0171-5978-1-4799-0169","10.1109/SOFE.2013.6635351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6635351","thermocouples;grounding;RF shielding;EMI immunity;RF ion source;noise reduction","Probes;Radio frequency;Plasma measurements;Assembly;Thermal sensors;Cable shielding","data acquisition;earthing;nuclear electronics;optimisation;plasma beam injection heating;plasma toroidal confinement;sensors;shielding;signal conditioning circuits;thermocouples;Tokamak devices","digital filtering;analog filtering;residual noise;experimental campaign;measurement global accuracy;RF plasma pulses;electronic equipments;prototype design;technical solutions;sensor design;signal conditioning;noise reduction;software characteristics;hardware characteristics;EMI immunity;grounding configuration;shielding configuration;extensive optimization;high voltage breakdowns;high RF power;measurement conditions;IPP-Garching;BATMAN RF ion source;design solutions;data acquisition system;measurement chain prototype;sensor damage;beam accelerator;electrical breakdowns;mechanical components;thermal load;thermocouples;Italy;Padua;Consorzio RFX;ITER heating neutral beam RF ion source;SPIDER experiment;thermal measurement system prototype","","1","12","","","","","","IEEE","IEEE Conferences"
"Computation of Mixed Strategy Non-dominated Nash Equilibria in Game Theory","C. A. O. Soares; L. S. Batista; F. Campelo; F. G. Guimarães","NA; NA; NA; NA","2013 BRICS Congress on Computational Intelligence and 11th Brazilian Congress on Computational Intelligence","","2013","","","242","247","Finding Nash equilibria has been one of the early objectives of research in game theory, and still represents a challenge to this day. We introduce a multiobjective formulation for computing Pareto-optimal sets of mixed Nash equilibria in normal form games. Computing these sets can be notably useful in decision making, because it focuses the analysis on solutions with greater outcome and hence more stable and desirable ones. While the formulation is suitable for any multiobjective optimization algorithm, we employ a method known as the cone-epsilon MOEA, due to its good convergence and diversity characteristics when solving multiobjective optimization problems. The adequacy of the proposed formulation is tested on most normal form games provided by the GAMBIT software test suite. The results show that the cone-epsilon MOEA working on the proposed formulation correctly finds the Pareto-optimal Nash equilibra in most games.","2377-0589;2377-0597","978-1-4799-3194","10.1109/BRICS-CCI-CBIC.2013.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6855856","Pareto;Nash;multiobjective;evolutionary algorithm","Games;Game theory;Silicon;Sociology;Statistics;Optimization;Vectors","game theory;Pareto optimisation","nondominated Nash equilibria;game theory;multiobjective formulation;Pareto-optimal sets;multiobjective optimization algorithm;cone-epsilon MOEA;diversity characteristic;GAMBIT software test suite","","","11","","","","","","IEEE","IEEE Conferences"
"An approach of processor core customization for stencil computation","Y. Li; Y. Zhang; J. Yang; W. Luk; G. Yang; W. Zheng","Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computing,Imperial College, London, UK; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China","2014 IEEE 25th International Conference on Application-Specific Systems, Architectures and Processors","","2014","","","182","183","Architecture customization is believed as one of the most promising methods to meet ever-increasing computing needs and power density limitations. This paper presents an approach to enhance a preliminary customizable core with some common architecture features, to adapt to the specific applications while keeping the programming flexibility. Those features include several effective software/hardware co-optimizing strategies, such as loop tiling, pre-fetching, cache customization, customized Single Instruction Multiple Data (SIMD) and Direct Memory Access (DMA), as well as the necessary ISA extensions. Currently we select stencil computation as the research target. Detailed tests of power-efficiency to evaluate the effect of all these optimizations comprehensively shows impressive performance speedup and power efficiency, even compared to X86, GPU and FPGA platforms. All these proposed customizations here could be applied to other computing applications.","2160-052X;1063-6862","978-1-4799-3609","10.1109/ASAP.2014.6868656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868656","customizable processor;stencil computation;software/hardware co-design;high-performance computing","Optimization;Hardware;Energy consumption;Computer architecture;Graphics processing units;Field programmable gate arrays","computer architecture;field programmable gate arrays;file organisation;graphics processing units;microprocessor chips;multiprocessing systems;parallel processing","processor core customization approach;stencil computation;architecture customization;power density limitations;architecture features;customizable core;programming flexibility;software/hardware cooptimizing strategies;loop tiling;prefetching;cache customization;customized single instruction multiple data;SIMD;direct memory access;DMA;FPGA platforms;GPU platforms;X86","","1","8","","","","","","IEEE","IEEE Conferences"
"Fusion of LIDAR and video cameras to augment medical training and assessment","B. R. VanVoorst; M. Hackett; C. Strayhorn; J. Norfleet; E. Honold; N. Walczak; J. Schewe","Raytheon BBN Technologies Corp., St. Louis Park, MN 55416 USA; Army Research Laboratory/HRED-STTC, Orlando, FL 32826 USA; Information Visualization and Innovative Research (IVIR) Inc., Sarasota, FL 34240, USA; Army Research Laboratory/HRED-STTC, USA; Information Visualization and Innovative Research (IVIR) Inc., USA; Raytheon BBN Technologies Corp., USA; Raytheon BBN Technologies Corp., USA","2015 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)","","2015","","","345","350","The Mobile Medical Lane Trainer (MMLT) is a multi-sensor rapidly deployed After-Action Review (AAR) system for Army medical lane training. Current AAR systems have two main drawbacks: 1) video does not provide a complete view of the medical and tactical situation, and 2) the video is not readily available for effective evaluation. The MMLT program is developing a “smarter” AAR system by using 3D LIDAR (LIght Detection And Ranging), a camera array, People Tracking software and Medical Training Evaluation and Review (MeTER) software. This system can be brought to the field and deployed in less than an hour to provide hands-off data collection for the exercise. MMLT supplements existing evaluation systems deployed at the Medical Simulation Training Centers (MSTCs) by providing a 3-D perspective of the training event for tactical evaluation with synchronized video technology to capture both tactical and clinical skills and instructor scoring. This capability is used in conjunction with the MeTER system's skill assessment checklists for automated performance review. An immediate synchronized playback capability has been developed, ultimately resulting in a rapid AAR for debriefing. This paper will discuss the technical components of the system, including hardware components, data fusion technique, tracking algorithms, and camera prioritization approaches, and will conclude with operational test results and lessons learned.","","978-1-4799-7772-7978-1-5090-0307","10.1109/MFI.2015.7295832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7295832","","Cameras;Laser radar;Training;Target tracking;Three-dimensional displays;Software;Sensors","biomedical education;computer software;medical computing;sensor fusion;video cameras","3D LIDAR fusion;video camera array;augment medical training and assessment;mobile medical lane trainer;multisensor rapidly deployed after-action review system;army medical lane training;medical-tactical situation;MMLT program;people tracking software;medical training evaluation and review software;hands-off data collection;exercise;tactical evaluation;synchronized video technology;tactical skills;clinical skills;skill assessment checklists;automated performance review;immediate synchronized playback capability;technical components;hardware components;data fusion technique;operational testing","","","16","","","","","","IEEE","IEEE Conferences"
"High-performance imaging subsystems and their integration in mobile devices","M. Lindwer; M. R. Pedersen","IAG/MCG/VIED, Intel Corporation, Eindhoven, The Netherlands; IAG/MCG/VIED, Intel Corporation, Eindhoven, The Netherlands","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","170","170","Within today's SoCs, functionality such as video, audio, graphics, and imaging is increasingly integrated through IP blocks, which are subsystems in their own right. Integration of IP blocks within SoCs always brought software integration aspects with it. However, since these subsystems increasingly consist of programmable processors, many more layers of firmware and software need to be integrated. In the imaging domain, this is particularly true. Imaging subsystems typically are highly heterogeneous, with high levels of parallelism. The construction of their firmware requires target-specific optimization, yet needs to take interoperability with sensor input systems and graphics/display subsystems into account. Hard real-time scheduling within the subsystem needs to cooperate with less stringent image analytics and SoC-level (OS) scheduling. In many of today's systems, the latter often only supports soft scheduling deadlines. At HW level, IP subsystems need to be integrated such that they can efficiently exchange both short-latency control signals and high-bandwidth data-plane blocks. Solutions exist, but need to be properly configured. However, at the SW level, currently no support exists that provides (i) efficient programmability, (ii) SW abstraction of all the different HW features of these blocks, and (iii) interoperability of these blocks. Starting points could be languages such as OpenCL and OpenCV, which do provide some abstractions, but are not yet sufficiently versatile.","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513494","MPSoC;ASIP;imaging;IP integration;Software","IP networks;System-on-chip;Program processors;Imaging;Graphics;Scheduling","","","","","3","","","","","","IEEE","IEEE Conferences"
"A Bandwidth-Saving Optimization for MPI Broadcast Collective Operation","H. Zhou; V. Marjanovic; C. Niethammer; J. Gracia","NA; NA; NA; NA","2015 44th International Conference on Parallel Processing Workshops","","2015","","","111","118","The efficiency and scalability of MPI collective operations, in particular the broadcast operation, plays an integral part in high performance computing applications. MPICH, as one of the contemporary widely-used MPI software stacks, implements the broadcast operation based on point-to-point operation. Depending on the parameters, such as message size and process count, the library chooses to use different algorithms, as for instance binomial dissemination, recursive-doubling exchange or ring all-to-all broadcast (all-gather). However, the existing broadcast design in latest release of MPICH does not provide good performance for large messages (lmsg) or medium messages with non-power-of-two process counts (mmsg-npof2) due to the inner suboptimal ring allgather algorithm. In this paper, based on the native broadcast design in MPICH, we propose a tuned broadcast approach with bandwidth-saving in mind catering to the case of lmsg and mmsg-npof2. Several comparisons of the native and tuned broadcast designs are made for different data sizes and program sizes on Cray XC40 cluster. The results show that the performance of the tuned broadcast design can get improved by a range from 2% to 54% for lmsg and mmsg-npof2 in terms of user-level testing.","1530-2016","978-1-4673-7589","10.1109/ICPPW.2015.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349902","MPICH;Broadcast;Bandwidth-saving","Algorithm design and analysis;Data communication;High performance computing;Bandwidth;Standards;Clustering algorithms;Optimized production technology","application program interfaces;message passing;parallel processing","user-level testing;Cray XC40 cluster;program sizes;data sizes;tuned broadcast design;native broadcast design;inner suboptimal ring allgather algorithm;ring all-to-all broadcast;recursive-doubling exchange;instance binomial dissemination;point-to-point operation;MPI software stacks;MPICH;high performance computing;broadcast operation;MPI broadcast collective operation;bandwidth-saving optimization","","2","14","","","","","","IEEE","IEEE Conferences"
"Traffic Optimization in Multi-layered WANs Using SDN","H. Rodrigues; I. Monga; A. Sadasivarao; S. Syed; C. Guok; E. Pouyoul; C. Liou; T. Rosing","NA; NA; NA; NA; NA; NA; NA; NA","2014 IEEE 22nd Annual Symposium on High-Performance Interconnects","","2014","","","71","78","Wide area networks (WAN) forward traffic through a mix of packet and optical data planes, composed by a variety of devices from different vendors. Multiple forwarding technologies and encapsulation methods are used for each data plane (e.g. IP, MPLS, ATM, SONET, Wavelength Switching). Despite standards defined, the control planes of these devices are usually not interoperable, and different technologies are used to manage each forwarding segment independently (e.g. Open Flow, TL-1, GMPLS). The result is lack of coordination between layers and inefficient resource usage. In this paper we discuss the design and implementation of a system that uses unmodified Open Flow to optimize network utilization across layers, enabling practical bandwidth virtualization. We discuss strategies for scalable traffic monitoring and to minimize losses on route updates across layers. A prototype of the system was built using a traditional circuit reservation application and an unmodified SDN controller, and its evaluation was performed on a multi-vendor test bed.","1550-4794;2332-5569","978-1-4799-5860","10.1109/HOTI.2014.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6925721","Wide Area Network;Software-defined networking;Open Flow;Multi-layer;Virtual Networks;Traffic Engineering","Optical switches;Optical packet switching;Bandwidth;Optical network units;Optical interconnections;Topology","optimisation;wide area networks","unmodified SDN controller;scalable traffic monitoring;unmodified Open Flow;encapsulation methods;multiple forwarding technologies;wide area networks;SDN;multilayered WAN;traffic optimization","","5","18","","","","","","IEEE","IEEE Conferences"
"Performance Enhancement for Network I/O Virtualization with Efficient Interrupt Coalescing and Virtual Receive-Side Scaling","H. Guan; Y. Dong; R. Ma; D. Xu; Y. Zhang; J. Li","Shanghai Jiao Tong University, Shanghai; Intel China Software Center, Shanghai; Shanghai Jiao Tong University, Shanghai; Intel China Software Center, Shanghai; Intel China Software Center, Shanghai; Shanghai Jiao Tong University, Shanghai","IEEE Transactions on Parallel and Distributed Systems","","2013","24","6","1118","1128","Virtualization is a key technology in cloud computing; it can accommodate numerous guest VMs to provide transparent services, such as live migration, high availability, and rapid checkpointing. Cloud computing using virtualization allows workloads to be deployed and scaled quickly through the rapid provisioning of virtual machines on physical machines. However, I/O virtualization, particularly for networking, suffers from significant performance degradation in the presence of high-speed networking connections. In this paper, we first analyze performance challenges in network I/O virtualization and identify two problems-conventional network I/O virtualization suffers from excessive virtual interrupts to guest VMs, and the back-end driver does not efficiently use the computing resources of underlying multicore processors. To address these challenges, we propose optimization methods for enhancing the networking performance: 1) Efficient interrupt coalescing for network I/O virtualization and 2) virtual receive-side scaling to effectively leverage multicore processors. These methods are implemented and evaluated with extensive performance tests on a Xen virtualization platform. Our experimental results confirm that the proposed optimizations can significantly improve network I/O virtualization performance and effectively solve the performance challenges.","1045-9219;1558-2183;2161-9883","","10.1109/TPDS.2012.339","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6392167","Network I/O virtualization;Xen;interrupt coalescing;receive-side scaling;multicore","Software;Virtual machine monitors;Hardware;Equations;Electronic mail;Mathematical model","cloud computing;interrupts;multiprocessing systems;virtual machines;virtualisation","performance enhancement;network I-O virtualization;interrupt coalescing;virtual receive-side scaling;cloud computing;guest VM;live migration;high availability;rapid checkpointing;virtual machines;physical machines;virtual interrupts;back-end driver;multicore processors;optimization methods;Xen virtualization platform","","5","17","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal design of the pipeline right-of-way nearby high voltage transmission lines using genetic algorithms","L. Czumbil; D. D. Micu; C. Munteanu; D. Stet; B. Tomoioaga","Depart. of Electrotechniques and Measurements, Technical University of Cluj-Napoca, Romania; Depart. of Electrotechniques and Measurements, Technical University of Cluj-Napoca, Romania; Depart. of Electrotechniques and Measurements, Technical University of Cluj-Napoca, Romania; Depart. of Electrotechniques and Measurements, Technical University of Cluj-Napoca, Romania; Romanian Power Grid Company - TRANSELECTRICA, Cluj-Napoca, Romania","2015 50th International Universities Power Engineering Conference (UPEC)","","2015","","","1","5","Underground or above ground metallic pipeline placed in the vicinity of high voltage transmission lines are exposed to AC electromagnetic interference effects. Therefore, at design stage one should determine the proper pipeline right-of-way in order to minimize the level of induced AC current and voltages due to electromagnetic coupling effects, and in the same time to reduce construction costs. A genetic algorithm based pipeline right-of-way optimization tool box has been implemented in the InterfStud EMI software application, developed by the authors. Two different, underground pipeline next to overhead power line, case studies are analysed to test the implemented optimization tool box.","","978-1-4673-9682-0978-1-4673-9683","10.1109/UPEC.2015.7339841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339841","electromagnetic interference;genetic algorithms;optimal design;integrated software package;high voltage power lines;metallic pipelines","Pipelines;Optimization;Genetic algorithms;Layout;Electromagnetic interference;Software;Power transmission lines","cost reduction;electromagnetic coupling;electromagnetic interference;genetic algorithms;high-voltage engineering;minimisation;pipelines;power engineering computing;power transmission lines","genetic algorithms;underground metallic pipeline;above ground metallic pipeline;AC electromagnetic interference effects;optimal pipeline right-of-way design;high voltage transmission lines;induced AC current level minimization;electromagnetic coupling effects;construction cost reduction;pipeline right-of-way optimization tool box;InterfStud EMI software application;overhead power line","","","11","","","","","","IEEE","IEEE Conferences"
"SI engine combustion wall thermal management potential without the presence of control limitation","R. I. Abdul Jalal; T. Steffen; A. Williams","Department of Aeronautical and Automotive Engineering Loughborough University Loughborough, United Kingdom; Department of Aeronautical and Automotive Engineering Loughborough University Loughborough, United Kingdom; Wolfson School of Mechanical and Manufacturing Engineering Loughborough University Loughborough, United Kingdom","2014 UKACC International Conference on Control (CONTROL)","","2014","","","307","312","Tight future CO<sub>2</sub> emission targets have encouraged extensive research in options for improving internal combustion engine efficiency. Amongst those, engine thermal management is a promising area to improve fuel economy, engine power and even reliability. Earlier studies have shown that engine thermal management was not just protecting engine from overheating but it also can improve engine performance, fuel consumption and even emissions. However, the effects and limits of thermal management are highly complex, and a better understanding is required to reach the full potential. The aim of this paper is to demonstrate the potential of manipulating combustion wall temperature for improving engine efficiency. A 1D numerical model of a 2.2L natural aspirated engine was developed using GT-Suite software for this purpose. The spark timing and fuelling in the engine model was also recalibrated to explore the indirect influence of thermal management influence on engine efficiency. The model assumes that the optimal temperature can be achieved at all times, ignoring some of the control implementation issues for now. The results show that optimized combustion wall temperature produces significant fuel consumption improvements at low to medium engine speed at both low and high load. The comparison with conventional temperature control was made using 7 legislated and academic test cycles. The highest fuel economy improvement of about 4% was recorded in urban test cycles. A smaller improvement of more than 2% was found for motorway driving. The results are due to improved combustion and lubrication only, not including reduced hydraulic losses.","","978-1-4799-5011","10.1109/CONTROL.2014.6915158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915158","thermal management;engine cooling;cooling system;engine efficiency;combustion wall","Engines;Combustion;Load modeling;Coolants;Friction;Fuels","environmental economics;fuel economy;internal combustion engines;lubrication;optimisation;reliability;sparks;temperature control;thermal management (packaging)","lubrication;motorway driving;urban test cycle;fuel economy improvement;academic test cycle;legislated test cycle;fuel consumption improvement;combustion wall temperature optimization;engine model;fuelling;spark timing;GT-Suite software;2.2L natural aspirated engine;1D numerical model;reliability;engine power;SI engine combustion wall thermal management potential;internal combustion engine efficiency","","","18","","","","","","IEEE","IEEE Conferences"
"Reverse Engineering Complex Feature Correlations for Product Line Configuration Improvement","B. Zhang; M. Becker","NA; NA","2014 40th EUROMICRO Conference on Software Engineering and Advanced Applications","","2014","","","320","327","As a Software Product Line (SPL) evolves with increasing number of variant features and feature values, the feature correlations become extremely intricate. However, these correlations are often incompletely documented (e.g., In feature models) so that most features can only be configured manually. In order to make product configuration processes more efficient, we present an approach to extracting complex feature correlations from existing product configurations using association mining techniques. Then these correlations are pruned and prioritized in order to minimize the effort of correlation validation. Our approach is conducted on an industrial SPL with 100 product configurations across 480 features. While 80 out of the 100 configurations are used as training data to automatically extract 4834 complex feature correlations, the rest 20 configurations are used as test data to evaluate the improvement potential of configuration efficiency. In the end, avg. 25% features in each of the 20 products can be configured automatically.","1089-6503;2376-9505","978-1-4799-5795","10.1109/SEAA.2014.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6928830","variability reverse engineering;feature correlation mining;product line configuration improvement","Feature extraction;Correlation;Association rules;Itemsets;Training data;Software engineering","data mining;software product lines","reverse engineering;product line configuration improvement;software product line;SPL;product configuration process;association mining techniques;correlation validation;configuration efficiency","","","23","","","","","","IEEE","IEEE Conferences"
"MVEM-Based Fault Diagnosis of Automotive Engines Using Dempster–Shafer Theory and Multiple Hypotheses Testing","J. Z. Vasu; A. K. Deb; S. Mukhopadhyay","Department of Electrical Engineering, Indian Institute of Technology, Kharagpur, Kharagpur, India; Department of Electrical Engineering, Indian Institute of Technology, Kharagpur, Kharagpur, India; Department of Electrical Engineering, Indian Institute of Technology, Kharagpur, Kharagpur, India","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2015","45","7","977","989","Internal combustion engines exhibit fast pulsating short-time dynamics due to the reciprocating cylinder motion, around mean operating points that change comparatively slow due to inputs such as throttle and load. Comparatively, simple mean value engine models (MVEM) describe the slow changes of the averaged states for automotive control and fault diagnosis. In this paper, a bank of state estimators based on MVEMs is used for fault residual generation. Three faults: 1) throttle mass air-flow sensor fault; 2) exhaust gas recirculation valve sensor fault; and 3) exhaust leak fault are considered here. These faults are significant as they affect emission levels. Optimized thresholds for residual classification are derived for minimizing false alarm rates and missed detection rates. The diagnosis logic, based on the principles of structured residuals proposed in literature, is extended here for multiple hypotheses testing. Furthermore, the Dempster-Shafer theory is used to associate a confidence measure with the decision conclusions and this is shown to improve isolation. Performance is demonstrated with automotive engine data obtained from a four-cylinder instantaneous spark-ignition engine (gasoline) system model, developed in the simulation software AMESim.","2168-2216;2168-2232","","10.1109/TSMC.2014.2384471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004877","Decision making;diagnosis;estimation;internal combustion engines;Kalman filters;modeling;Decision making;diagnosis;estimation;internal combustion engines;Kalman filters;modeling","Engines;Mathematical model;Circuit faults;Equations;Fault diagnosis;Valves;Automotive engineering","air pollution;automotive components;exhaust systems;fault diagnosis;flow sensors;inference mechanisms;internal combustion engines;mechanical engineering computing;mechanical testing;pattern classification;state estimation;valves","MVEM-based fault diagnosis;Dempster-Shafer theory;hypothesis testing;automotive engines;internal combustion engines;fast pulsating short-time dynamics;reciprocating cylinder motion;mean operating points;mean value engine models;automotive control;state estimators;fault residual generation;throttle mass air-flow sensor fault;exhaust gas recirculation valve sensor fault;exhaust leak fault;emission levels;residual classification;false alarm rates;missed detection rates;structured residual principles;confidence measure;four-cylinder instantaneous spark-ignition engine system model;gasoline system model;simulation software;AMESim","","8","36","","","","","","IEEE","IEEE Journals & Magazines"
"Study of a clinical analysis laboratory's lighting system design","F. M. M. Raminhos; M. M. T. Valdez; C. M. Ferreira","IPC - ISEC; IPC-ISEC; IPC-ISEC &amp; INESC Coimbra","2013 48th International Universities' Power Engineering Conference (UPEC)","","2013","","","1","5","The lighting integrates the key factors in creating a desirable environment to space. The hospital environments represent, in general, a type of buildings whose activity has a significant energy saving potential. The type of activity, combined with the specifics of the health sector, makes this type of buildings a target for analysis and energy optimization. This work will therefore focus on understanding the analysis of methods of lighting design from the point of view of hospital environments, using the various possibilities of use of lighting calculation RELUX PRO tools, which are tested in the program through interior simulations performed for a hospital environment, more specifically a clinical analysis laboratory using lighting technology such as light-emitting diodes (LED's) seeking to compare with the current installation taking into account the most relevant criteria.","","978-1-4799-3254","10.1109/UPEC.2013.6714974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714974","hospital environment;indoor lighting;Led;Relux software","Lighting;Light emitting diodes;Hospitals;Three-dimensional displays;Rendering (computer graphics);Investment;Laboratories","hospitals;light emitting diodes;lighting;optimisation","clinical analysis laboratory;lighting system design;hospital environments;energy saving potential;health sector;energy optimization;lighting calculation;RELUX PRO tools;lighting technology;light emitting diodes;LED","","","15","","","","","","IEEE","IEEE Conferences"
"Cloud Benchmarking for Performance","B. Varghese; O. Akgun; I. Miguel; L. Thai; A. Barker","NA; NA; NA; NA; NA","2014 IEEE 6th International Conference on Cloud Computing Technology and Science","","2014","","","535","540","How can applications be deployed on the cloud to achieve maximum performance? This question has become significant and challenging with the availability of a wide variety of Virtual Machines (VMs) with different performance capabilities in the cloud. The above question is addressed by proposing a six step benchmarking methodology in which a user provides a set of four weights that indicate how important each of the following groups: memory, processor, computation and storage are to the application that needs to be executed on the cloud. The weights along with cloud benchmarking data are used to generate a ranking of VMs that can maximise performance of the application. The rankings are validated through an empirical analysis using two case study applications, the first is a financial risk application and the second is a molecular dynamics simulation, which are both representative of workloads that can benefit from execution on the cloud. Both case studies validate the feasibility of the methodology and highlight that maximum performance can be achieved on the cloud by selecting the top ranked VMs produced by the methodology.","","978-1-4799-4093","10.1109/CloudCom.2014.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7037713","cloud benchmarking;cloud ranking;cloud performance","Benchmark testing;Cloud computing;Virtual machining;Aggregates;Risk analysis;Standards;Bandwidth","cloud computing;software performance evaluation;virtual machines","cloud benchmarking;virtual machines;financial risk application;molecular dynamics simulation;top ranked VM","","7","10","","","","","","IEEE","IEEE Conferences"
"OpenCL-based hardware-software co-design methodology for image processing implementation on heterogeneous FPGA platform","S. O. Ayat; M. Khalil-Hani; R. Bakhteri","VeCAD Research Laboratory, Faculty Of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Skudai, Johor, Malaysia; VeCAD Research Laboratory, Faculty Of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Skudai, Johor, Malaysia; VeCAD Research Laboratory, Faculty Of Electrical Engineering, Universiti Teknologi Malaysia, 81310 Skudai, Johor, Malaysia","2015 IEEE International Conference on Control System, Computing and Engineering (ICCSCE)","","2015","","","36","41","Recently, the OpenCL hardware-software co-design methodology has gained traction in realizing effective parallel architecture designs in heterogeneous FPGA platforms. In fact, the portability of OpenCL on hardware ready platforms such as GPU or multicore CPU enables ease of design verification. This is true especially for parallel algorithms before implementing them using cumbersome HDL-based RTL design. In this paper we employed OpenCL programming platform based on Altera SDK for OpenCL (AOCL) to implement a Sobel filter algorithm as an image processing test case on a Cyclone V FPGA board. Using the portability of this platform, the performance of the kernel code is benchmarked against that of the GPU and multicore CPU implementations for different image and kernel sizes. Different optimization strategies are also applied for each platform. We found that increasing the Sobel filter kernel size from 3×3 to 5×5 results in only 11.3% increase in computation time for FPGA, while the effect was much more significant where the execution time was as high as 23.6% and 85.7% for CPU and GPU, respectively.","","978-1-4799-8252-3978-1-4799-8251","10.1109/ICCSCE.2015.7482154","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7482154","parallel computing;image processing;multi-processor;FPGA;GPU;OpenCL","Field programmable gate arrays;Kernel;Graphics processing units;Multicore processing;Image processing;Convolution;Algorithm design and analysis","field programmable gate arrays;graphics processing units;hardware-software codesign;image processing;multiprocessing systems;parallel architectures","optimization strategies;kernel sizes;Cyclone V FPGA board;Sobel filter algorithm;Altera SDK;HDL-based RTL design;parallel algorithms;design verification;multicore CPU;GPU;parallel architecture designs;heterogeneous FPGA platform;image processing implementation;OpenCL-based hardware-software codesign methodology","","2","9","","","","","","IEEE","IEEE Conferences"
"Fault tolerance on multicore processors using deterministic multithreading","H. Mushtaq; Z. Al-Ars; K. Bertels","Computer Engineering Laboratory Delft University of Technology Delft, the Netherlands; Computer Engineering Laboratory Delft University of Technology Delft, the Netherlands; Computer Engineering Laboratory Delft University of Technology Delft, the Netherlands","2013 8th IEEE Design and Test Symposium","","2013","","","1","6","This paper describes a software based fault tolerance approach for multithreaded programs running on multicore processors. Redundant multithreaded processes are used to detect soft errors and recover from them. Our scheme makes sure that the execution of the redundant processes is identical even in the presence of non-determinism due to shared memory accesses. This is done by making sure that the redundant processes acquire the locks for accessing the shared memory in the same order. Instead of using record/replay technique to do that, our scheme is based on deterministic multithreading, meaning that for the same input, a multithreaded program always have the same lock interleaving. Unlike record/replay systems, this eliminates the requirement for communication between the redundant processes. Moreover, our scheme is implemented totally in software, requiring no special hardware, making it very portable. Furthermore, our scheme is totally implemented at user-level, requiring no modification of the kernel. For selected benchmarks, our scheme adds an average overhead of 49% for 4 threads.","2162-0601;2162-061X","978-1-4799-3525","10.1109/IDT.2013.6727107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727107","","Clocks;Optimization;Instruction sets;Fault tolerant systems;Fault tolerance;Memory management;Hardware","multiprocessing systems;multi-threading;software fault tolerance","multicore processors;deterministic multithreading;software-based fault tolerance approach;multithreaded programs;redundant multithreaded process;soft error detection;soft error recovery;redundant process execution;shared memory access;record-replay technique;multithreaded program;lock interleaving;user-level","","","15","","","","","","IEEE","IEEE Conferences"
"Algorithm analysis of PHM for sensitive cargo transportation","Lou Wenzhong; Liu Peng; Guo Mingru","State Key Laboratory of Mechatronics Engineering and Control, Beijing Institute of Technology, CHINA; State Key Laboratory of Mechatronics Engineering and Control, Beijing Institute of Technology, CHINA; State Key Laboratory of Mechatronics Engineering and Control, Beijing Institute of Technology, CHINA","The 8th Annual IEEE International Conference on Nano/Micro Engineered and Molecular Systems","","2013","","","751","754","Several Prognostics and Health Management (PHM) algorithms are to be analyzed in this paper, and the purpose is set to master the know-how of the optimization of the prognostic algorithms. To implement the PHM for sensitive cargo transportation, based on the original data collected during the dedicated tests, applying the microsystem hardware designed and assembled by the research team, as well as the embedded software. At the end, the framework of the system platform in the future is layouted.","","978-1-4673-6352-5978-1-4673-6351","10.1109/NEMS.2013.6559837","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6559837","PHM algorithms;MEMS signal processing & analysis;smart integrated system;intelligent transportation system","Prognostics and health management;Sensors;Roads;Software algorithms;Monitoring;Feature extraction","assembling;condition monitoring;data acquisition;embedded systems;freight handling;micromechanical devices;optimisation;transportation","PHM algorithm analysis;sensitive cargo transportation;prognostic and health management algorithms;microsystem hardware design;microsystem hardware assembling;embedded software","","","6","","","","","","IEEE","IEEE Conferences"
"Reliability evaluation of wind and PV energy penetrated power system","M. P. Lalitha; P. H. Reddy; P. J. Naidu","EEE Department, Annamacharya Institute of Technology &amp; Sciences, Rajampeta, India; EEE Department, Annamacharya Institute of Technology &amp; Sciences, Rajampeta, India; EEE Department, Annamacharya Institute of Technology &amp; Sciences, Rajampeta, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","5","This paper presents the reliability evaluation of high wind and photovoltaic (PV) energies penetrated power system at Hierarchical Level I (HL I). First, the renewable unit models are developed by considering the component failure rates and uncertain nature of renewable energy sources. The Data Synthesizer Software is used to extract the hourly wind speed and solar radiation from the weather statistical data. The Fuzzy C-means Clustering Method is used to obtain the required number of generation states from the hourly data. The Markov process is used to obtain both wind and PV models. Then, these renewable models are added to the Roy Billinton Test System (RBTS) by Recursive Algorithm approach and different reliability indices are calculated.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014675","Photovoltaic farm;Wind farm;Data Synthesizer Software;Markov models;Fuzzy C-means Clustering;Recursive Algorithm;Generation Reliability Evaluation","Power system reliability;Reliability;Mathematical model;Wind speed;Markov processes","fuzzy reasoning;Markov processes;pattern clustering;photovoltaic power systems;power system analysis computing;power system reliability;recursive estimation;wind power","reliability evaluation;high wind and photovoltaic energies penetrated power system;hierarchical level I;HL I;renewable unit models;component failure rates;renewable energy sources;data synthesizer software;hourly wind speed;solar radiation;weather statistical data;fuzzy c-means clustering method;Markov process;Roy Billinton Test System;RBTS;recursive algorithm approach;reliability indices","","1","16","","","","","","IEEE","IEEE Conferences"
"Solving examination timetabling problem using partial exam assignment with great deluge algorithm","A. K. Mandal; M. N. M. Kahar","Faculty of Computer Systems &amp; Software Engineering, University Malaysia Pahang, Kuantan, Malaysia; Faculty of Computer Systems &amp; Software Engineering, University Malaysia Pahang, Kuantan, Malaysia","2015 International Conference on Computer, Communications, and Control Technology (I4CT)","","2015","","","530","534","Constructing a quality solution for the examination timetable problem is a difficult task. This paper presents a partial exam assignment approach with great deluge algorithm as the improvement mechanism in order to generate good quality timetable. In this approach, exams are ordered based on graph heuristics and only selected exams (partial exams) are scheduled first and then improved using great deluge algorithm. The entire process continues until all of the exams have been scheduled. We implement the proposed technique on the Toronto benchmark datasets. Experimental results indicate that in all problem instances, this proposed method outperforms traditional great deluge algorithm and when comparing with the state-of-the-art approaches, our approach produces competitive solution for all instances, with some cases outperform other reported result.","","978-1-4799-7952-3978-1-4799-7951","10.1109/I4CT.2015.7219635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219635","graph heuristic;great deluge algorithm;timetable","Heuristic algorithms;Optimization;Software algorithms;Algorithm design and analysis;Europe;Benchmark testing;Search problems","education;graph theory","examination timetabling problem;partial exam assignment;great deluge algorithm;quality timetable;graph heuristics","","","37","","","","","","IEEE","IEEE Conferences"
"Optimal generation expansion planning with integration of variable renewables and bulk energy storage systems","Z. Hu; W. T. Jewell","Electrical Engineering and Computer Science, Wichita State University, Wichita, USA; Electrical Engineering and Computer Science, Wichita State University, Wichita, USA","2013 1st IEEE Conference on Technologies for Sustainability (SusTech)","","2013","","","1","8","The variable nature of wind and solar generation could exploit the economic potential of bulk electric energy storage (EES) systems. This paper presents an optimal planning model, a dc optimal power flow (OPF) based multi-period optimization formulation, and its software implementation. The model co-optimizes the investment in all resources including EES while taking their optimal operation into consideration. Condensed hourly profiles of wind and solar output are applied to preserve their variable nature in terms of production. In this study, a 3-bus test system is constructed to demonstrate and test the sensitivity of the planning model. The relevant economic data are compiled for different types of generators and EES technologies. The potential impact of CO<sub>2</sub>emission price, renewable incentive, natural gas price, and energy storage cost to the future grid are demonstrated by employing the proposed optimal planning model.","","978-1-4673-4630","10.1109/SusTech.2013.6617290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6617290","bulk energy storage;CO2 emissions;multi-period optimization;planning;power systems;renewables","Generators;Planning;Energy storage;Natural gas;Fuels;Investment;Optimization","energy storage;power generation planning;renewable energy sources","optimal generation expansion planning;variable renewables;wind generation;solar generation;economic potential;bulk electric energy storage systems;optimal planning model;dc optimal power flow;multiperiod optimization formulation;software implementation;3 bus test system;economic data;emission price;renewable incentive;natural gas price;energy storage cost","","5","24","","","","","","IEEE","IEEE Conferences"
"Autonomous and flexible multiagent systems enhance transport logistics","M. Gath; O. Herzog; S. Edelkamp","Institute for Artificial Intelligence, TZI - Center for Computing and Communication Technologies, Bremen, Germany; Institute for Artificial Intelligence, TZI - Center for Computing and Communication Technologies, Bremen, Germany; Institute for Artificial Intelligence, TZI - Center for Computing and Communication Technologies, Bremen, Germany","2014 11th International Conference & Expo on Emerging Technologies for a Smarter World (CEWIT)","","2014","","","1","6","This paper presents an autonomous multiagent system which optimizes the planning and scheduling of industrial processes using the example of courier and express services. In order to handle the rising demands and to capitalize on the increasing optimization potential in transport logistics, which both result from the consequent integration of industrial processes into the Internet of Things and Services, the presented dispAgent solution ensures a flexible, adaptive, and proactive system behavior. Intelligent, selfishly acting agents represent logistic entities, which communicate and negotiate with each other to optimize the allocation of orders to transport facilities. The system has been developed in cooperation with our industrial partner tiramizoo, which is an expert in courier and express services. In order to determine the quality of the computed solutions, we evaluated the system using an established benchmark set and compared the results to best-known solutions. In addition, we further validated the system's performance by multiple simulations of real-world scenarios relying on data which was provided by our industrial partner. The results show that the system achieves high quality solutions for the benchmark set and outperforms a standard dispatching software product in real-world scenarios.","","978-1-4799-6740","10.1109/CEWIT.2014.7021143","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7021143","Industry 4.0;Internet of Things and Services;Mulitagent Systems;Planning and Controll in Logistics;Decision-making Algorithms;Vehicle Routing Problem;Pickup and Delivery Problem","Vehicles;Logistics;Benchmark testing;Multi-agent systems;Vehicle dynamics;Proposals;Optimization","dispatching;goods distribution;Internet of Things;logistics;multi-agent systems;optimisation;planning (artificial intelligence);scheduling;traffic engineering computing;transportation","dispatching software product;tiramizoo;dispAgent solution;Internet of Things;optimization potential;express services;courier services;industrial process;scheduling;planning;transport logistics;flexible multiagent systems;autonomous multiagent systems","","2","22","","","","","","IEEE","IEEE Conferences"
"Software defined networking for distributed mobility management","Y. Li; H. Wang; M. Liu; B. Zhang; H. Mao","State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, 100876, China","2013 IEEE Globecom Workshops (GC Wkshps)","","2013","","","885","889","With the mobile network core evolving towards an unlayered and decentralized architecture, distributed mobility management appears to be more compatible and efficient with the flattened networks. In this paper, we propose a new approach to realize the distributed mobility management using the software defined networking techniques instead of the existing mobility management protocols. The mobility management functions are implemented with the help of distributed controllers. The controllers will update the involved forwarding tables directly in case of handover, which realizes the route optimization inherently. The test results show that the proposed SDN-aided approach is an efficient mechanism for distributed mobility management.","2166-0077","978-1-4799-2851","10.1109/GLOCOMW.2013.6825101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825101","Distributed Mobility Management;Software Defined Networking;Flat Network","Mobile radio mobility management;IP networks;Handover;Delays;Mobile computing","Internet;mobility management (mobile radio);protocols","software defined networking;distributed mobility management;mobile network core;mobility management protocols;distributed controllers;handover;route optimization","","1","21","","","","","","IEEE","IEEE Conferences"
"An open-source educational toolbox for power system frequency control tuning and optimization","I. M. Cecílio; A. M. Ersdal; D. Fabozzi; N. F. Thornhill","Department of Chemical Engineering, Imperial College London, UK; Department of Engineering Cybernetics, Norwegian University of Science and Technology, Trondheim, Norway; Department of Chemical Engineering, Imperial College London, UK; Department of Chemical Engineering, Imperial College London, UK","IEEE PES ISGT Europe 2013","","2013","","","1","5","This paper presents an open-source educational tool for power and control engineering students to practice frequency control and test tunings and control strategies. The disturbance scenarios are realistic and automatically generated by the tool. This feature facilitates statistical analyses of the behaviour of the system. The paper shows simulation results with a common disturbance scenario and with two more severe cases.","2165-4816;2165-4824","978-1-4799-2984","10.1109/ISGTEurope.2013.6695305","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6695305","","Frequency control;Turbines;Time-frequency analysis;Mathematical model;Power system dynamics;Power system stability","computer aided instruction;control engineering computing;control engineering education;frequency control;optimisation;power engineering computing;power engineering education;power system control;power system faults;public domain software;statistical analysis","open-source educational toolbox;power system frequency control tuning;optimization;control engineering student;disturbance scenario;statistical analyses","","2","13","","","","","","IEEE","IEEE Conferences"
"A Method for Deriving and Testing Malicious Behavior Detection Rules","R. Hilden; K. Hätönen","NA; NA","2015 IEEE Trustcom/BigDataSE/ISPA","","2015","1","","1337","1342","The internet is riddled with numerous malware and other threats. This puts the limited resources of network security devices, such as firewalls and intrusion detection systems, under growing stress. They have to cope with increasing network traffic and manage numerous detection rules for threatening traffic. Creating covering set of detection rules manually is a slow and tedious process. In this paper, we present a method to automatically create detection rules for an intrusion detection system from interaction signatures of known malware. Our method maintains information integrity and reports potential issues during the derivation process. The method was tested with HTTP traffic generated from known malware signatures using Snort as the IDS rule-engine.","","978-1-4673-7952-6978-1-4673-7951","10.1109/Trustcom.2015.527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7345435","IDS;rule generation;automation;pruning;expert system","Redundancy;Testing;Malware;Telecommunication traffic;Syntactics;Intrusion detection;Optimization","computer network security;Internet;invasive software;transport protocols","malicious behavior detection rule;Internet;malware detection;network security device;network traffic;threatening traffic;tedious process;intrusion detection system;interaction signature;HTTP traffic;malware signature;Snort;IDS rule-engine;denial-of-service attack;DoS attack","","","19","","","","","","IEEE","IEEE Conferences"
"Optimistic Shared Memory Dependence Tracing (T)","Y. Jiang; D. Li; C. Xu; X. Ma; J. Lu","NA; NA; NA; NA; NA","2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2015","","","524","534","Inter-thread shared memory dependences are crucial to understanding the behavior of concurrent systems, as such dependences are the cornerstone of time-travel debugging and further predictive trace analyses. To enable effective and efficient shared memory dependence tracing, we present an optimistic scheme addressing the challenge of capturing exact dependences between unsynchronized events to reduce the probe effect of program instrumentation. Specifically, our approach achieved a wait-free fast path for thread-local reads on x86-TSO relaxed memory systems, and simultaneously achieved precise tracing of exact read-after-write, write-after-write and write-after-read dependences on the fly. We implemented an open-source RWTrace tool, and evaluation results show that our approach not only achieves efficient shared memory dependence tracing, but also scales well on a multi-core computer system.","","978-1-5090-0025-8978-1-5090-0024","10.1109/ASE.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372040","concurrency;shared memory dependence;dynamic analysis","Instruction sets;Memory management;Synchronization;Algorithm design and analysis;Benchmark testing;Computer science;Debugging","program debugging;shared memory systems","optimistic shared memory dependence tracing;interthread shared memory dependence;time-travel debugging;predictive trace analysis;program instrumentation;x86-TSO relaxed memory system;open-source RWTrace tool;multicore computer system","","1","35","","","","","","IEEE","IEEE Conferences"
"Simulated, hands-on and remote laboratories for studying the solar cells","P. A. Cotfas; D. T. Cotfas; C. Gerigan","Department of Electronics and Computers, Faculty of Electrical Engeneering and Computer Science, Transilvania University of Brasov, Romania; Department of Electronics and Computers, Faculty of Electrical Engeneering and Computer Science, Transilvania University of Brasov, Romania; Department of Electronics and Computers, Faculty of Electrical Engeneering and Computer Science, Transilvania University of Brasov, Romania","2015 Intl Aegean Conference on Electrical Machines & Power Electronics (ACEMP), 2015 Intl Conference on Optimization of Electrical & Electronic Equipment (OPTIM) & 2015 Intl Symposium on Advanced Electromechanical Motion Systems (ELECTROMOTION)","","2015","","","206","211","There are many studies in the literature related to the introduction in the engineering curricula of simulated laboratories and, more recently, of remote laboratories to extend the area of applications, nowadays limited by the labs endowment and to use creatively the advantages of e-learning. The paper aims to present a solution for the experiments related to the solar cells field. The presented solution combines the three existing laboratories, namely simulated, hands-on and remote laboratories. The solution put together the software applications Multisim and LabVIEW, the hardware NI ELVIS and the SolarLab board developed by the authors. The choice was based on the easiness of exchanging data between software applications, the speed of creating electronic prototypes offered by NI ELVIS, the large number of lab works on testing and characterization of solar cells provided by the SolarLab board and the easiness of performing remote laboratories using the web publishing tool in LabVIEW. The study was conducted within the solar cells course held for students from Applied Electronics, Physics Engineering and for other courses that have as partial subject the solar cells. The results were obtained based on the feedback from students as well as the correctness in performing lab works by the students. During hands-on and remote labs, the students support the idea of combining classical instrumentation, virtual instrumentation and modern tools, such as SolarLab.","","978-1-4673-7239-8978-1-4673-7547","10.1109/OPTIM.2015.7426953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7426953","Solar cells;Simulated;hands-on and remote laboratories;LabVIEW and NI ELVIS","Photovoltaic cells;Nickel;Remote laboratories;Instruments;Current measurement;Software","educational computing;electrical engineering computing;electrical engineering education;laboratories;physics computing;physics education;solar cells;virtual instrumentation","simulated laboratory;hands-on laboratory;remote laboratory;solar cells;Multisim software;LabVIEW software;NI ELVIS hardware;SolarLab board","","1","28","","","","","","IEEE","IEEE Conferences"
"Comparison of hardware and software based encryption for secure communication in wireless sensor networks","M. Botta; M. Simek; N. Mitton","Brno University of Technology, Faculty of Electrical Engineering and Communication, Department of Telecommunications, Technicka 12, 61600 Brno, Czech Republic; Brno University of Technology, Faculty of Electrical Engineering and Communication, Department of Telecommunications, Technicka 12, 61600 Brno, Czech Republic; Inria Lille-Nord Europe research center, Btiment A - Parc scientifique de la haute borne 40, avenue Halley 59650 Villeneuve d'Ascq, France","2013 36th International Conference on Telecommunications and Signal Processing (TSP)","","2013","","","6","10","This paper deals with the energy efficient issue of cryptographic mechanisms used for secure communication between devices in wireless sensor networks. Since these devices are mainly targeted for low power consumption appliances, there is an effort for optimization of any aspects needed for regular sensor operation. On a basis of utilization of hardware cryptographic accelerators integrated in microcontrollers, this article provides the comparison between software and hardware solutions. Proposed work examines the problems and solutions for implementation of security algorithms for WSN devices. Because the speed of hardware accelerator should be much higher than the software implementation, there are examination tests of energy consumption and validation of performance of this feature. Main contribution of the article is real testbed evaluation of the time latency and energy requirements needed for securing the communication. In addition, global evaluation for all important network communication parameters like throughput, delay and delivery ratio are also provided.","","978-1-4799-0404-4978-1-4799-0402-0978-1-4799-0403","10.1109/TSP.2013.6613880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613880","AES;encryption;Lightweight MESH;WSN;XTEA","Encryption;Hardware;Payloads;Software;Wireless sensor networks","cryptography;microcontrollers;power consumption;telecommunication security;wireless sensor networks","encryption;secure communication;wireless sensor networks;energy efficient issue;cryptographic mechanisms;power consumption;hardware cryptographic accelerators;software cryptographic accelerators;security algorithms;WSN devices;hardware accelerator;software implementation;energy consumption;global evaluation;network communication parameters;delay","","5","8","","","","","","IEEE","IEEE Conferences"
"Pipelets: Self-organizing software Pipelines for many-core architectures","J. Jahn; J. Henkel","Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Germany; Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Germany","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","1516","1521","We present the novel concept of Pipelets: self-organizing stages of software pipelines that monitor their computational demands and communication patterns and interact to optimize the performance of the application they belong to. They enable dynamic task remapping and exploit application-specific properties. Our experiments show that they improve performance by up to 31.2% compared to state-of-the-art when resource demands of applications alter at runtime as is the case for many complex applications.","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513754","","Runtime;Throughput;Pipelines;Software;Complexity theory;Computer architecture;Bandwidth","","","","3","23","","","","","","IEEE","IEEE Conferences"
"High-frequency financial statistics with parallel R and Intel Xeon Phi coprocessor","J. Zou; H. Zhang","Department of Mathematical Sciences, Worcester Polytechnic Institute; Pervasive Technology Institute, Indiana University, Bloomington","2014 IEEE International Conference on Big Data (Big Data)","","2014","","","61","69","Financial statistics covers a wide array of applications in the financial world, such as (high frequency) trading, risk management, pricing and valuation of securities and derivatives, and various business and economic analytics. Portfolio allocation is one of the most important problems in financial risk management. One most challenging part in portfolio allocation is the tremendous amount of data and the optimization procedures that require computing power beyond the currently available desktop systems. In this article, we focus on the portfolio allocation problem using high-frequency financial data, and propose a hybrid parallelization solution to carry out efficient asset allocations in a large portfolio via intra-day high-frequency data. We exploit a variety of HPC techniques, including parallel R, Intel Math Kernel Library, and automatic offloading to Intel Xeon Phi coprocessor in particular to speed up the simulation and optimization procedures in our statistical investigations. Our numerical studies are based on high-frequency price data on stocks traded in New York Stock Exchange in 2011. The analysis results show that portfolios constructed using high-frequency approach generally perform well by pooling together the strengths of regularization and estimation from a risk management perspective. We also investigate the computation aspects of large-scale multiple hypothesis testing for time series data. Using a combination of software and hardware parallelism, we demonstrate a high level of performance on high-frequency financial statistics.","","978-1-4799-5666","10.1109/BigData.2014.7004414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7004414","high-frequency financial analysis;massive parallelism;parallel R;Intel Xeon Phi Coprocessor","Portfolios;Resource management;Optimization;Risk management;Vectors;Libraries;Parallel processing","asset management;coprocessors;financial data processing;investment;optimisation;parallel processing;pricing;risk management;stock markets;time series","financial world;high frequency trading;pricing;securities valuation;business analytics;economic analytics;financial risk management;optimization procedures;portfolio allocation problem;high-frequency financial data;hybrid parallelization solution;asset allocations;intra-day high-frequency data;HPC techniques;parallel R;Intel Math Kernel Library;automatic offloading;Intel Xeon Phi coprocessor;high-frequency price data;stocks trading;New York Stock Exchange;large-scale multiple hypothesis testing;time series data;software parallelism;hardware parallelism;high-frequency financial statistics","","1","44","","","","","","IEEE","IEEE Conferences"
"A novel method for distribution system feeder reconfiguration using black-box optimization","Lei Tang; Fang Yang; Xianyong Feng","Dept. of Electrical and Computing Engineering, Iowa State University, Ames, USA; U.S. Corporate Research Center, ABB Inc., Raleigh, NC, USA; U.S. Corporate Research Center, ABB Inc., Raleigh, NC, USA","2013 IEEE Power & Energy Society General Meeting","","2013","","","1","5","This paper presents a novel method for distribution system feeder reconfiguration. The problem is formulated as a black-box optimization problem such that it can be solved by Mesh Adaptive Direct Search (MADS) algorithm. Given feeder reconfiguration candidates, a distribution system software simulator- OpenDSS is used to calculate detailed power flow solutions to provide various operating outputs such as system losses, line currents, and bus voltages. The radial topology constraints are maintained based on graph theory analysis, and two versions of constraints are formulated for binary programming and integer programming. The proposed method is a general framework, which is not restricted by the type of objectives, and OpenDSS enables detailed modeling of any distribution system. Test of the method on the IEEE 123 node feeder with distributed generators shows promising results, and the two versions of radial topology constraints are compared.","1932-5517","978-1-4799-1303","10.1109/PESMG.2013.6672532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6672532","Black-box optimization;distribution system;feeder reconfiguration;MADS;OpenDSS;radial topology","Optimization;Topology;Switches;Network topology;Load flow analysis;Linear programming","distributed power generation;network topology;power convertors","distribution system feeder reconfiguration;black-box optimization problem;mesh adaptive direct search algorithm;given feeder reconfiguration candidates;power flow solutions;IEEE 123 node feeder;distributed generators;radial topology","","3","25","","","","","","IEEE","IEEE Conferences"
"YamiPred: A Novel Evolutionary Method for Predicting Pre-miRNAs and Selecting Relevant Features","D. Kleftogiannis; K. Theofilatos; S. Likothanassis; S. Mavroudi","King Abdullah University of Science and Technology (KAUST), Computer Science and Mathematical Sciences and Engineering Division, Thuwal, Saudi Arabia; Department of Computer Engineering and Informatics, University of Patras, Greece; Department of Computer Engineering and Informatics, University of Patras, Greece; Department of Computer Engineering and Informatics, University of Patras, Greece","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2015","12","5","1183","1192","MicroRNAs (miRNAs) are small non-coding RNAs, which play a significant role in gene regulation. Predicting miRNA genes is a challenging bioinformatics problem and existing experimental and computational methods fail to deal with it effectively. We developed YamiPred, an embedded classification method that combines the efficiency and robustness of support vector machines (SVM) with genetic algorithms (GA) for feature selection and parameters optimization. YamiPred was tested in a new and realistic human dataset and was compared with state-of-the-art computational intelligence approaches and the prevalent SVM-based tools for miRNA prediction. Experimental results indicate that YamiPred outperforms existing approaches in terms of accuracy and of geometric mean of sensitivity and specificity. The embedded feature selection component selects a compact feature subset that contributes to the performance optimization. Further experimentation with this minimal feature subset has achieved very high classification performance and revealed the minimum number of samples required for developing a robust predictor. YamiPred also confirmed the important role of commonly used features such as entropy and enthalpy, and uncovered the significance of newly introduced features, such as %A-U aggregate nucleotide frequency and positional entropy. The best model trained on human data has successfully predicted pre-miRNAs to other organisms including the category of viruses.","1545-5963;1557-9964;2374-0043","","10.1109/TCBB.2014.2388227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018958","Classifier design and evaluation;Evolutionary computing and genetic algorithms;Feature evaluation and selection;SVM;GA;pre-miRNA prediction;Classifier design and evaluation;evolutionary computing and genetic algorithms;feature evaluation and selection;SVM;GA;pre-miRNA prediction","Support vector machines;Sociology;Statistics;Bioinformatics;Genetic algorithms;Optimization;Computational biology","bioinformatics;enthalpy;entropy;evolution (biological);feature selection;genetics;genomics;microorganisms;molecular biophysics;optimisation;RNA;support vector machines","YamiPred;evolutionary method;microRNA;small noncoding RNA;miRNA gene prediction;bioinformatics problem;embedded classification method;support vector machines;genetic algorithms;feature selection;parameter optimization;computational intelligence approaches;enthalpy;A-U aggregate nucleotide frequency;positional entropy;viruses","Algorithms;Base Sequence;Computer Simulation;Evolution, Molecular;High-Throughput Nucleotide Sequencing;Humans;MicroRNAs;Models, Genetic;Molecular Sequence Data;Pattern Recognition, Automated;Sequence Alignment;Sequence Analysis, RNA;Software;Support Vector Machine","9","46","","","","","","IEEE","IEEE Journals & Magazines"
"Innovative practices session 5C: Cloud atlas — Unreliability through massive connectivity","H. Naeimi; S. Natarajan; K. Vaid; P. Kudva; M. Natu","Intel Corporation; Intel Corporation; Microsoft Corporation; IBM Corporation; Intel Corporation","2013 IEEE 31st VLSI Test Symposium (VTS)","","2013","","","1","1","The rapid pace of integration, emergence of low power, low cost computing elements, and ubiquitous and ever-increasing bandwidth of connectivity have given rise to data center and cloud infrastructures. These infrastructures are beginning to be used on a massive scale across vast geographic boundaries to provide commercial services to businesses such as banking, enterprise computing, online sales, and data mining and processing for targeted marketing to name a few. Such an infrastructure comprises of thousands of compute and storage nodes that are interconnected by massive network fabrics, each of them having their own hardware and firmware stacks, with layers of software stacks for operating systems, network protocols, schedulers and application programs. The scale of such an infrastructure has made possible service that has been unimaginable only a few years ago, but has the downside of severe losses in case of failure. A system of such scale and risk necessitates methods to (a) proactively anticipate and protect against impending failures, (b) efficiently, transparently and quickly detect, diagnose and correct failures in any software or hardware layer, and (c) be able to automatically adapt itself based on prior failures to prevent future occurrences. Addressing the above reliability challenges is inherently different from the traditional reliability techniques. First, there is a great amount of redundant resources available in the cloud from networking to computing and storage nodes, which opens up many reliability approaches by harvesting these available redundancies. Second, due to the large scale of the system, techniques with high overheads, especially in power, are not acceptable. Consequently, cross layer approaches to optimize the availability and power have gained traction recently. This session will address these challenges in maintaining reliable service with solutions across the hardware/software stacks. The currently available commercial data-center and cloud infrastructures will be reviewed and the relative occurrences of different causalities of failures, the level to which they are anticipated and diagnosed in practice, and their impact on the quality of service and infrastructure design will be discussed. A study on real-time analytics to proactively address failures in a private, secure cloud engaged in domain-specific computations, with streaming inputs received from embedded computing platforms (such as airborne image sources, data streams, or sensors) will be presented next. The session concludes with a discussion on the increased relevance of resiliency features built inside individual systems and components (private cloud) and how the macro public cloud absorbs innovations from this realm.","1093-0167;1093-0167","978-1-4673-5543-8978-1-4673-5542-1978-1-4673-5541","10.1109/VTS.2013.6548907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548907","","Hardware;Software reliability;Cloud computing;Streaming media;Very large scale integration","","","","","","","","","","","IEEE","IEEE Conferences"
"Linux malware detection using non-parametric statistical methods","A. KA; V. P","Department of Computer Science &amp; Engineering, SCMS School of Engineering &amp; Technology, Ernakulam, Kerala, India; Department of Computer Science &amp; Engineering, SCMS School of Engineering &amp; Technology, Ernakulam, Kerala, India","2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","","2014","","","356","361","Linux is the most renowned open source operating system. In recent years, the number of malware targeting Linux OS has been increased and the traditional defence mechanisms seems to be futile. We propose a novel non-parametric statistical approach using machine learning techniques for identifying previously unknown malicious Executable Linkable Files (ELF). The system calls employed as features extracted dynamically within a controlled environment. The proposed approach ranks and determine the prominent features by using non-parametric statistical methods like Kruskal-Wallis ranking test (KW), Deviation From Poisson (DFP). Three learning algorithms (J48, Adaboost and Random Forest) are applied to generate prediction model, from a minimal set of features extracted from the system call traces. Optimal feature vector resulted in over all classification accuracy of 97.30% to identify unknown malicious specimens.","","978-1-4799-3080-7978-1-4799-3078","10.1109/ICACCI.2014.6968611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6968611","non-parametric;feature selection;classifiers;system call analysis;dynamic analysis","Malware;Accuracy;Feature extraction;Linux;Training;Computational modeling;Predictive models","invasive software;learning (artificial intelligence);Linux;statistical testing","Linux malware detection;nonparametric statistical methods;open source operating system;Linux OS;executable linkable files;ELF;machine learning techniques;Kruskal-Wallis ranking test;KW;deviation from Poisson method;DFP method;J48 learning algorithm;Adaboost learning algorithm;random forest learning algorithm;system call traces","","","21","","","","","","IEEE","IEEE Conferences"
"Leveraging Markov chain and optimal mutation strategy for smart fuzzing","Yongji Ouyang; Zehui Wu; Qing Mu; Qingxian Wang","State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China","ICINS 2014 - 2014 International Conference on Information and Network Security","","2014","","","169","174","Fuzz testing is an important way of vulnerability discovery, however, the existing fuzzers based on symbolic execution and others have inherent shortcomings like needing more computing resource, in-depth analysis and so on. To solve above problems, this paper presents a smart fuzzing method based on Markov chain. Firstly, this method optimizes the testing input sample to get the minimal sample set. Secondly, this method records program execution information by using instrument, and makes a Markov model about state. Finally, this method uses Markov chain to detect the change of execution path, and leads tester to choose better samples to mutate. Meanwhile, we analyse mutation strategies in depth for better triggering exception. Experimental data shows that the presented method can help fuzzer to generate effective test samples. We discovers 51 vulnerabilities in software like WPS, along with the code coverage increases of nearly 49% comparing with zzuf and the average exception discovery rate increase nearly 9 times comparing with MiniFuzz.","","978-1-84919-909","10.1049/cp.2014.1282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7133813","Smart fuzzing;Vulnerability discovery;Markov chain;Mutation strategies","","fuzzy set theory;Markov processes;program testing;security of data","software security testing;vulnerability discovery;symbolic execution;smart fuzzing method;Markov chain;testing input sample;minimal sample set;program execution information;execution path change detection;WPS software;average exception discovery;optimal mutation strategy","","","","","","","","","IET","IET Conferences"
"A Prototype Software Package to Retrieve Soil Moisture From Sentinel-1 Data by Using a Bayesian Multitemporal Algorithm","N. Pierdicca; L. Pulvirenti; G. Pace","Dept. Information Engineering, Electronics, Telecommunications, Sapienza University of Rome, Rome, Italy; Dept. Information Engineering, Electronics, Telecommunications, Sapienza University of Rome, Rome, Italy; Environment Division, Advanced Computer Systems S.p.A., Rome, Italy","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2014","7","1","153","166","The Sentinel-1 mission will offer the opportunity to obtain C-band radar data characterized by short revisit time, thus allowing for the generation of frequent soil moisture maps. This work presents a prototype software implementing a multitemporal approach to the problem of soil moisture retrieval using Synthetic Aperture Radar (SAR) data. The approach exploits the short revisit time of Sentinel-1 data by assuming the availability of a time series of SAR images that is integrated within a retrieval algorithm based on the Bayesian maximum a posteriori probability statistical criterion. The paper focuses on the combination of on-line and off-line processing that has been designed in order to decrease the time necessary to produce a soil moisture map, which may be a critical aspect of multitemporal approaches. It describes also the optimization of the algorithm carried out to find the set of algorithm parameters that allow obtaining the best tradeoff between accuracy of the estimates and computational efficiency. A set of simulations of C-band SAR data, produced by applying a well-established radar-backscattering model, is used to perform the optimization. The designed system is tested on a series of ERS-1 SAR data acquired on February-April 1994 in Central Italy with a revisit time of three days. The results indicate that the temporal trend of estimated soil moisture is consistent with the succession of rain events occurred throughout the period of ERS-1 acquisitions over the observed geographic area.","1939-1404;2151-1535","","10.1109/JSTARS.2013.2257698","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515352","Bayesian estimation;ground segment;multitemporal algorithm;SAR;Sentinel-1;soil moisture","Soil moisture;Vegetation mapping;Synthetic aperture radar;Software;Prototypes","backscatter;Bayes methods;geophysics computing;hydrological techniques;maximum likelihood estimation;moisture measurement;radar signal processing;remote sensing by radar;soil;synthetic aperture radar","prototype software package;soil moisture retrieval;Sentinel-1 data;Bayesian multitemporal algorithm;Sentinel-1 mission;C-band radar data;soil moisture map;multitemporal approach;synthetic aperture radar data;time series;SAR images;retrieval algorithm;Bayesian maximum a posteriori probability statistical criterion;algorithm optimization;algorithm parameters;C-band SAR data;radar-backscattering model;ERS-1 SAR data;AD 1994 02 to 04;Central Italy;rain events;ERS-1 acquisition","","24","37","","","","","","IEEE","IEEE Journals & Magazines"
"Transparent acceleration of program execution using reconfigurable hardware","N. Paulino; J. C. Ferreira; J. Bispo; J. M. P. Cardoso","INESC TEC and Faculty of Engineering, University of Porto, Porto, Portugal; INESC TEC and Faculty of Engineering, University of Porto, Porto, Portugal; INESC TEC and Faculty of Engineering, University of Porto, Porto, Portugal; INESC TEC and Faculty of Engineering, University of Porto, Porto, Portugal","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","1066","1071","The acceleration of applications, running on a general purpose processor (GPP), by mapping parts of their execution to reconfigurable hardware is an approach which does not involve program's source code and still ensures program portability over different target reconfigurable fabrics. However, the problem is very challenging, as suitable sequences of GPP instructions need to be translated/mapped to hardware, possibly at runtime. Thus, all mapping steps, from compiler analysis and optimizations to hardware generation, need to be both efficient and fast. This paper introduces some of the most representative approaches for binary acceleration using reconfigurable hardware, and presents our binary acceleration approach and the latest results. Our approach extends a GPP with a Reconfigurable Processing Unit (RPU), both sharing the data memory. Repeating sequences of GPP instructions are migrated to an RPU composed of functional units and interconnect resources, and able to exploit instruction-level parallelism, e.g., via loop pipelining. Although we envision a fully dynamic system, currently the RPU resources are selected and organized offline using execution trace information. We present implementation prototypes of the system on a Spartan-6 FPGA with a MicroBlaze as GPP and the very encouraging results achieved with a number of benchmarks.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.1122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092547","","Hardware;Computer architecture;Acceleration;Benchmark testing;Runtime;Software;Registers","field programmable gate arrays;multiprocessing systems;program compilers;source code (software);supervisory programs","transparent acceleration;program execution;reconfigurable hardware;general purpose processor;GPP;compiler analysis;hardware generation;binary acceleration approach;reconfigurable processing unit;RPU;Spartan-6 FPGA;MicroBlaze;program source code","","4","29","","","","","","IEEE","IEEE Conferences"
"Evaluating emergency lateral transshipment policies using simulation-based approaches","W. Hachicha; A. Afli; F. Elleuch","Unit of Mechanic, Modeling and Production (U2MP), Engineering School of Sfax, Sfax, Tunisia; Unit of Logistic, Industrial and Quality Management, (LOGIQ), Higher Institute of Industrial Management, Sfax, Tunisia; Unit of Logistic, Industrial and Quality Management, (LOGIQ), Higher Institute of Industrial Management, Sfax, Tunisia","2013 International Conference on Advanced Logistics and Transport","","2013","","","470","475","This paper deals with a lateral transshipment models involving two-echelon supply chain network, with a single supplier at the higher echelon and two retail locations at the lower. Lateral transshipment is considered as an option at each reorder decision under the periodic review standard (R, s, S) replenishment policy. The purpose of this paper is twofold. Firstly, a metamodel-based simulation optimization approach is applied to find the optimal values of s and S, for each retailer. Secondly, a series of simulation experiments are performed to find the best transshipment policy, in terms of smallest total cost and disservice rate. The tested policies are no pooling, complete pooling and various partial pooling policies according to what the threshold levels of physical stock are selected. An important finding is that each tested transshipment policy is considerably superior to a policy of no such transshipments, although at the expense of increased transportation activity. The best transshipment policy is such partial pooling with exactly s as the well-chosen value of the threshold level. Partial pooling is very interesting transshipment policy and should be further addressed in future research.","","978-1-4799-0313-9978-1-4799-0314-6978-1-4799-0312","10.1109/ICAdLT.2013.6568504","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6568504","emergency transshipment;pooling;periodic (R;s;S) replenishment policy;discrete event simulation;metamodel-based simulation;Desirability function approach","Optimization;Computational modeling;Mathematical model;Supply chains;Analytical models;Stochastic processes;Software","costing;emergency services;goods distribution;optimisation;supply chain management","emergency lateral transshipment policies;simulation-based approaches;two-echelon supply chain network;single supplier;retail locations;reorder decision;periodic review standard replenishment policy;metamodel-based simulation optimization approach;total cost;disservice rate;no pooling policies;complete pooling policies;partial pooling policies;physical stock threshold levels;tested transshipment policy;transportation activity","","","20","","","","","","IEEE","IEEE Conferences"
"SAR and BER evaluation using a simulation test bench for in vivo communication at 2.4 GHz","T. P. Ketterl; G. E. Arrobo; R. D. Gitlin","Department of Electrical Engineering, University of South Florida, Tampa, USA; Department of Electrical Engineering, University of South Florida, Tampa, USA; Department of Electrical Engineering, University of South Florida, Tampa, USA","WAMICON 2013","","2013","","","1","4","We present a simulation method and results that utilizes accurate electromagnetic field simulations to study the maximum allowable transmitted power levels from in vivo devices to achieve a required bit error rates (BER) at the external node (receiver) while maintaining the specific absorption rate (SAR) under a required threshold. The BER of the communication can be calculated using the derived power threshold for a given modulation scheme. These results can be used to optimize the transmitted power levels while assuring that the safety guidelines in terms of the resulting SAR of transmitters placed in any location inside the human body are met. To evaluate the SAR and BER, a software-based test bench that allows an easy way to implement field solver solutions directly into system simulations was developed. To demonstrate the software-based test bench design, a complete OFDM-based communication (IEEE 802.11g) for the in vivo environment was simulated. Results showed that for cases when noise levels increase or the BER becomes more stringent, a relay network or the use of multiple receive antennas, such as in a MIMO system, will be become necessary to achieve high data rate communication.","","978-1-4673-5537-7978-1-4673-5536-0978-1-4673-5535","10.1109/WAMICON.2013.6572751","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572751","SAR;BER;in vivo propagation;medical implants;wireless communication","Bit error rate;Antennas;Demodulation;Generators;Antenna measurements;OFDM;Laparoscopes","antenna arrays;error statistics;MIMO communication;OFDM modulation;radio transmitters;wireless LAN","SAR;BER;simulation test bench;in vivo communication;electromagnetic field simulation;bit error rates;specific absorption rate;modulation scheme;transmitters;software-based test bench;IEEE 802.11g;OFDM-based communication;MIMO system;relay network;multiple receive antennas;high data rate communication;frequency 2.4 GHz","","8","18","","","","","","IEEE","IEEE Conferences"
"Improving Project Management Decision Making by Modeling Quality, Time, and Cost Continuously","M. J. Liberatore; B. Pollack-Johnson","Department of Management and Operations, Villanova University, Villanova, PA, USA; Department of Mathematics and Statistics, Villanova University, Villanova, PA, USA","IEEE Transactions on Engineering Management","","2013","60","3","518","528","The value of a project to a client can be measured in part by the level of quality associated with the completed project. Quality is acknowledged to be an important component of project management, but its joint relationship with time and cost previously has not been modeled. This paper introduces the notion of a quality function for individual tasks and uses the functional form of the bivariate normal (after also evaluating a bivariate logistic form), to model quality at the task level. Using real data from two case studies, a translation agency and a software development company, the quality function is specified and incorporated into a mathematical programming model that allows quality to be explicitly considered in project planning and scheduling. An alternative model formulation leads to the creation of quality level curves that enable managers to evaluate the nonlinear tradeoffs between quality, time, and cost for each of the example projects. The results of these analyses lead to specific decisions about the planned values for these three fundamental dimensions at the task level and provide insights for project planning and scheduling that can be gained through improved understanding of the choices and tradeoffs.","0018-9391;1558-0040","","10.1109/TEM.2012.2219586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6408001","Mathematical programming;nonlinear optimization;project management;project planning;project scheduling;quality management","Project management;Software;Planning;Time measurement;Standards;Testing;Logistics","costing;decision making;DP industry;language translation;normal distribution;planning;project management;quality function deployment;scheduling;software development management;time management","project management decision making improvement;quality modeling;time modeling;cost modeling;quality function;bivariate normal;bivariate logistic form;translation agency;software development company;mathematical programming model;project planning;scheduling;quality level curves","","10","19","","","","","","IEEE","IEEE Journals & Magazines"
"Technology-design co-optimization of resistive cross-point array for accelerating learning algorithms on chip","P. Chen; D. Kadetotad; Z. Xu; A. Mohanty; B. Lin; J. Ye; S. Vrudhula; J. Seo; Y. Cao; S. Yu","Arizona State University, Tempe, AZ 85281, USA; Arizona State University, Tempe, AZ 85281, USA; Arizona State University, Tempe, AZ 85281, USA; Arizona State University, Tempe, AZ 85281, USA; Arizona State University, Tempe, AZ 85281, USA; Arizona State University, Tempe, AZ 85281, USA; Arizona State University, Tempe, AZ 85281, USA; Arizona State University, Tempe, AZ 85281, USA; Arizona State University, Tempe, AZ 85281, USA; Arizona State University, Tempe, AZ 85281, USA","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","854","859","Technology-design co-optimization methodologies of the resistive cross-point array are proposed for implementing the machine learning algorithms on a chip. A novel read and write scheme is designed to accelerate the training process, which realizes fully parallel operations of the weighted sum and the weight update. Furthermore, technology and design parameters of the resistive cross-point array are co-optimized to enhance the learning accuracy, latency and energy consumption, etc. In contrast to the conventional memory design, a set of reverse scaling rules is proposed on the resistive cross-point array to achieve high learning accuracy. These include 1) larger wire width to reduce the IR drop on interconnects thereby increasing the learning accuracy; 2) use of multiple cells for each weight element to alleviate the impact of the device variations, at an affordable expense of area, energy and latency. The optimized resistive cross-point array with peripheral circuitry is implemented at the 65 nm node. Its performance is benchmarked for handwritten digit recognition on the MNIST database using gradient-based sparse coding. Compared to state-of-the-art software approach running on CPU, it achieves &gt;10<sup>3</sup> speed-up and &gt;10<sup>6</sup> energy efficiency improvement, enabling real-time image feature extraction and learning.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092504","machine learning;neuromorphic computing;cross-point array;resistive memory;synaptic device","Arrays;Accuracy;Resistance;Wires;Encoding;Machine learning algorithms","feature extraction;handwritten character recognition;learning (artificial intelligence)","technology-design cooptimization;machine learning algorithms;latency;energy consumption;memory design;reverse scaling rules;optimized resistive cross-point array;peripheral circuitry;handwritten digit recognition;MNIST database;gradient-based sparse coding;real-time image feature extraction","","","15","","","","","","IEEE","IEEE Conferences"
"How to generate worst-case scenarios when testing already deployed systems against unexpected situations","F. Zapata; R. Pineda; M. Ceberio","Research Institute for Manufacturing &amp; Engineering Systems RIMES, University of Texas at El Paso, 500 West University Ave., 79968, USA; Research Institute for Manufacturing &amp; Engineering Systems RIMES, University of Texas at El Paso, 500 West University Ave., 79968, USA; Department of Computer Science, University of Texas at El Paso, 500 West University Ave., 79968, USA","2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS)","","2013","","","617","622","Before a system of systems is deployed, it is tested - but it is tested against known operational mission, under several known operational scenarios. Once the system is deployed, new possible unexpected and/or uncertain operational scenarios emerge. It is desirable to develop methodologies to test the system against such scenarios. A possible methodology to test the system would be to generate the worst case scenario that we can think of - to understand, in principle, the behavior of the system. So, we face a question of generating such worst-case scenarios. In this paper, we provide some guidance on how to generate such worst-case scenarios.","","978-1-4799-0348","10.1109/IFSA-NAFIPS.2013.6608472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608472","","Linear programming;Optimization;Sensors;Systems engineering and theory;Standards;Computer aided software engineering;Vectors","system theory;systems engineering","worst-case scenario generation;unexpected situations;system of systems testing;operational mission","","1","12","","","","","","IEEE","IEEE Conferences"
"Reliability calculus for an experimental device performing quantitative analysis of alcohol vapors from exhaled air","D. Iudean; R. Munteanu; F. Cadar; P. Maier; A. Bogdan Amza","Department of Electrical Engineering and Measurements, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Department of Electrical Engineering and Measurements, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Department of Electrical Engineering and Measurements, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; Department of Electrical Engineering and Measurements, Technical University of Cluj-Napoca, Cluj-Napoca, Romania; "Iuliu Haţieganu" University of Medicine and Pharmacy, Cluj-Napoca, Romania","2015 E-Health and Bioengineering Conference (EHB)","","2015","","","1","4","The article is based on an experimental device performing quantitative analysis of alcohol vapors from exhaled air (breathalyzer), developed at the Technical University of Cluj-Napoca, called alcoholmeter. The reliability research is conducted over the nine components of the alcoholmeter, in terms of reliability indicators. The features selected and analyzed within the Relex Software Architect environment. Failure rates for each component and the reliability for the alcoholmeter prototype were determined. Also, the component failure rates were ranked and then a graphical representation of all components of the system has been built, based on the calculations results.","","978-1-4673-7545-0978-1-4673-7544-3978-1-4673-7543","10.1109/EHB.2015.7391388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391388","failure rate;reliability analysis;alcohol test;experimental device;breathalyzer","Prototypes;Reliability engineering;Software reliability;Materials reliability;Stress;Green products","biomedical equipment;biomedical measurement;gas sensors;organic compounds;reliability","graphical representation;component failure rates;Relex Software Architect environment;reliability indicators;alcoholmeter;breathalyzer;exhaled air;alcohol vapor quantitative analysis;experimental device;reliability calculus","","","12","","","","","","IEEE","IEEE Conferences"
"Design of Sobel operator based image edge detection algorithm on FPGA","G. Chaple; R. D. Daruwala","Department of Electrical Engineering, Veermata Jijabai Technological Institute, Mumbai, India; Electrical Engineering Department, Veermata Jijabai Technological Institute, Mumbai, India","2014 International Conference on Communication and Signal Processing","","2014","","","788","792","Real-time image processing applications requires processing on large data of pixels in a given timing constraints. Reconfigurable device like FPGAs have emerged as promising solutions for reducing execution times by deploying parallelism techniques in image processing algorithms. Implementation of highly parallel system architecture, parallel access of large internal memory banks and optimization of processing element for applications makes FPGA an ideal device for image processing system. Edge detection is basic tool used in many image processing applications for extracting information from image. Sobel edge detection is gradient based edge detection method used to find edge pixels in image. This paper presents a design of a Sobel edge detection algorithm to find edge pixels in gray scale image. Xilinx ISE Design Suite-14 software platforms is used to design a algorithm using VHDL language. MATLAB software platform is used for obtaining pixel data matrix from gray scale image and vice versa. Xilinx FPGAs of family Vertex-5 are more suitable for image processing work than Spartan-3 and Spartan-6.","","978-1-4799-3358-7978-1-4799-3357-0978-1-4799-3356","10.1109/ICCSP.2014.6949951","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949951","Edge detection;Sobel operator;FPGA;VHDL;test bench;real-time image processing","MATLAB;Image edge detection;Digital signal processing","edge detection;field programmable gate arrays;gradient methods;parallel architectures","Sobel operator based image edge detection algorithm;real-time image processing algorithm;timing constraints;reconfigurable device;parallelism techniques;parallel system architecture;internal memory banks;processing element optimization;gradient based edge detection method;edge pixels;gray scale image;Xilinx ISE design;Suite-14 software platforms;VHDL language;Matlab software platform;pixel data matrix;Xilinx FPGAs;family Vertex-5;Spartan-6;Spartan-3","","13","11","","","","","","IEEE","IEEE Conferences"
"A testability growth model and its application","C. Zhao; J. Qiu; G. Liu; K. Lv; K. Pattipati","Science and Technology on Integrated Logistics Support Laboratory, National University of Defense Technology, Changsha, 410073, China; Science and Technology on Integrated Logistics Support Laboratory, National University of Defense Technology, Changsha, 410073, China; Science and Technology on Integrated Logistics Support Laboratory, National University of Defense Technology, Changsha, 410073, China; Science and Technology on Integrated Logistics Support Laboratory, National University of Defense Technology, Changsha, 410073, China; Department of Electrical and Computer Engineering, University of Connecticut, 371 Fairfield Road, U-2157, Storrs, 06269, USA","2014 IEEE AUTOTEST","","2014","","","121","128","Testability growth is the enhancement in system testability through design modifications and/or other corrective actions performed throughout a system's life cycle. A testability growth model can help system designers to plan and execute a testability progression process, and to achieve the specified system testability metrics in minimum time and/or cost. A Markov chain-based testability growth model that tracks and projects a user-defined composite testability growth metric is proposed. A Bayesian approach and a hybrid genetic algorithm, coupled with a particle swarm optimization method, are used to learn the parameters of the testability growth model from evolving data, and use the estimated model to track and project the testability metric. Validation of the theory is provided via simulated data. Results show that the testability growth model is reasonable, and the accuracy of the method is quite good.","1088-7725;1558-4550","978-1-4799-3005-0978-1-4799-3389","10.1109/AUTEST.2014.6935132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935132","design for testability;testability growth;Markov chain;Baysesian;genetic algorithm;particle swarm optimization","Discrete Fourier transforms;Measurement;Markov processes;Bayes methods;Testing;Software reliability","Bayes methods;design for testability;failure analysis;genetic algorithms;Markov processes;particle swarm optimisation","design modifications;system life cycle;testability progression process;system testability metrics;Markov chain-based testability growth model;user defined composite testability growth metric;Bayesian approach;hybrid genetic algorithm;particle swarm optimization method","","3","25","","","","","","IEEE","IEEE Conferences"
"Self-adaptive learning based discrete differential evolution algorithm for solving CJWTA problem","Y. Xue; Y. Zhuang; T. Ni; S. Ni; X. Wen","College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing 210044, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; No.723 Research Institute of China Shipbuilding Industry Corporation, Yangzhou 225001, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing 210044, China","Journal of Systems Engineering and Electronics","","2014","25","1","59","68","Cooperative jamming weapon-target assignment (CJWTA) problem is a key issue in electronic countermeasures (ECM). Some symbols which relevant to the CJWTA are defined firstly. Then, a formulation of jamming fitness is presented. Finally, a model of the CJWTA problem is constructed. In order to solve the CJWTA problem efficiently, a self-adaptive learning based discrete differential evolution (SLDDE) algorithm is proposed by introducing a self-adaptive learning mechanism into the traditional discrete differential evolution algorithm. The SLDDE algorithm steers four candidate solution generation strategies simultaneously in the framework of the self-adaptive learning mechanism. Computational simulations are conducted on ten test instances of CJWTA problem. The experimental results demonstrate that the proposed SLDDE algorithm not only can generate better results than only one strategy based discrete differential algorithms, but also outperforms two algorithms which are proposed recently for the weapon-target assignment problems.","1004-4132","","10.1109/JSEE.2014.00007","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754228","global optimization;self-adaptive;discrete differential evolution;weapon-target assignment (WTA);cooperative jamming","Jamming;Radar;Weapons;Next generation networking;Military equipment;Algorithm design and analysis;Electronic countermeasures;Learning systems;Computational modeling","","","","1","","","","","","","BIAI","BIAI Journals & Magazines"
"Prototyping Control and Data Acquisition for the ITER Neutral Beam Test Facility","A. Luchetta; G. Manduchi; C. Taliercio; A. Soppelsa; F. Paolucci; F. Sartori; P. Barbato; M. Breda; R. Capobianco; F. Molon; M. Moressa; S. Polato; P. Simionato; E. Zampiva","Consorzio RFX, Padova, Italy; Consorzio RFX, Padova, Italy; Consorzio RFX, Padova, Italy; Consorzio RFX, Padova, Italy; Fusion for Energy, Barcelona, Spain; Fusion for Energy, Barcelona, Spain; Consorzio RFX, Padova, Italy; Consorzio RFX, Padova, Italy; Consorzio RFX, Padova, Italy; Consorzio RFX, Padova, Italy; Consorzio RFX, Padova, Italy; Consorzio RFX, Padova, Italy; Consorzio RFX, Padova, Italy; Consorzio RFX, Padova, Italy","IEEE Transactions on Nuclear Science","","2013","60","5","3454","3460","The ITER Neutral Beam Test Facility will be the project's R&D facility for heating neutral beam injectors (HNB) for fusion research operating with H/D negative ions. Its mission is to develop technology to build the HNB prototype injector meeting the stringent HNB requirements (16.5 MW injection power, -1 MeV acceleration energy, 40 A ion current and one hour continuous operation). Two test-beds will be built in sequence in the facility: first SPIDER, the ion source test-bed, to optimize the negative ion source performance, second MITICA, the actual prototype injector, to optimize ion beam acceleration and neutralization. The SPIDER control and data acquisition system is under design. To validate the main architectural choices, a system prototype has been assembled and performance tests have been executed to assess the prototype's capability to meet the control and data acquisition system requirements. The prototype is based on open-source software frameworks running under Linux. EPICS is the slow control engine, MDSplus is the data handler and MARTe is the fast control manager. The prototype addresses low and high-frequency data acquisition, 10 kS/s and 10 MS/s respectively, camera image acquisition, data archiving, data streaming, data retrieval and visualization, real time fast control with 100 μs control cycle and supervisory control.","0018-9499;1558-1578","","10.1109/TNS.2013.2281264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616620","ITER;neutral beam injector;neutral beam test facility","Prototypes;Data acquisition;Synchronization;Structural beams;Memory;Servers;Particle beams","data acquisition;fusion reactor instrumentation;plasma beam injection heating;plasma toroidal confinement;prototypes;Tokamak devices","prototyping control;data acquisition;ITER Neutral Beam Test Facility;R&D facility;heating neutral beam injectors;fusion research;HNB prototype injector;SPIDER ion source testbed;negative ion source performance;MITICA injector;actual prototype injector;ion beam acceleration;ion beam neutralization;Linux. EPICS;MARTe","","4","21","","","","","","IEEE","IEEE Journals & Magazines"
"Optimization-based visual sensor deployment algorithm in PTZ Wireless Visual Sensor Networks","H. Yen","Department of Information Management, Shih-Hsin University, Taiwan","2015 Seventh International Conference on Ubiquitous and Future Networks","","2015","","","734","739","By equipping the Pan-Tilt-Zoom (PTZ) capable visual sensor in Wireless Visual Sensor Networks (WVSN), the visual sensor could move the camera to increase its coverage area. Then, fewer visual sensors will be needed on deploying the WVSN. Because of the PTZ, the Field of View (FoV) of the sensor is divided into two regions, namely Direct Coverage Region (DCR) and PTZ Coverage Region (PTZCR). The DCR is determined by the camera's depth of field and DCR span angle. The PTZCR is determined by the camera's depth of field and PTZCR span angle. We propose a mathematical model to capture the above requirements in determining the DCR and PTZCR in WVSN. Then the optimization-based heuristics that based on Lagrangean relaxation, call LGR_PTZ, is proposed to solve this problem. From the computational experiments, the LGR_PTZ outperforms the other two heuristics under all tested cases.","2165-8528;2165-8536","978-1-4799-8993","10.1109/ICUFN.2015.7182640","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7182640","Pan-Tilt-Zoom Visual Sensor;Visual Sensor Deployment;Field of View;Wireless Visual Sensor Networks","Visualization;Optical character recognition software;IEL;Heuristic algorithms","optimisation;visual communication;wireless sensor networks","optimization-based visual sensor deployment algorithm;PTZ wireless visual sensor network;Pan-Tilt-Zoom capable visual sensor;WVSN;sensor field of view;direct coverage region;PTZ coverage region;Lagrangean relaxation;LGR PTZ","","1","7","","","","","","IEEE","IEEE Conferences"
"Hardware PCA for gas identification systems using high level synthesis on the Zynq SoC","A. A. S. Ali; A. Amira; F. Bensaali; M. Benammar","College of Engineering, Qatar University, Doha, Qatar, P. O. Box: 2713; School of Computing, University of the West of Scotland, Paisley, Scotland, PAI 2BE; College of Engineering, Qatar University, Doha, Qatar, P. O. Box: 2713; College of Engineering, Qatar University, Doha, Qatar, P. O. Box: 2713","2013 IEEE 20th International Conference on Electronics, Circuits, and Systems (ICECS)","","2013","","","707","710","One of the significant stages in a gas identification system is dimensionality reduction to speed up the processing part. This is even more important when the system is implemented on a hardware platform where the resources are limited. This paper presents the design and the implementation of the learning and testing phases of principal component analysis (PCA) that can be used in a gas identification system on the heterogeneous Zynq platform. All steps of PCA starting from the mean computation to the projection of data onto the new space, passing by the normalization process, covariance matrix and the eigenvectors computation are developed in C and synthesized using the new Xilinx VIVADO high level synthesis (HLS). The computation of the eigenvectors was based on the iterative Jacobi method. The designed hardware for computing the learning part of PCA on the Zynq system on chip showed that it can be faster than its 64-bit Intel i7-3770 processor counterpart with a speed up of 1.41. Optimization techniques using HLS directives were also utilised in the hardware implementation of the testing part of the PCA to speed up the design and reduce its latency.","","978-1-4799-2452","10.1109/ICECS.2013.6815512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6815512","","Principal component analysis;Testing;Hardware;Field programmable gate arrays;IP networks;Software;Covariance matrices","computerised instrumentation;covariance matrices;data reduction;eigenvalues and eigenfunctions;gas sensors;high level synthesis;iterative methods;Jacobian matrices;learning (artificial intelligence);microprocessor chips;optimisation;principal component analysis;system-on-chip","HLS;optimization techniques;Intel i7-3770 processor;system on chip;heterogeneous Zynq SoC;iterative Jacobi method;Xilinx VIVADO high level synthesis;C;eigenvectors computation;covariance matrix;normalization process;principal component analysis;testing phase;learning;dimensionality reduction;gas identification system;high level synthesis;hardware PCA","","2","15","","","","","","IEEE","IEEE Conferences"
"Adding context to fault localization with integration coverage","H. A. de Souza; M. L. Chaim","Institute of Mathematics and Statistics, University of Sao Paulo, Brazil; School of Arts, Sciences and Humanities, University of Sao Paulo, Brazil","2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2013","","","628","633","Fault localization is a costly task in the debugging process. Several techniques to automate fault localization have been proposed aiming at reducing effort and time spent. Some techniques use heuristics based on code coverage data. The goal is to indicate program code excerpts more likely to contain faults. The coverage data mostly used in automated debugging is based on white-box unit testing (e.g., statements, basic blocks, predicates). This paper presents a technique which uses integration coverage data to guide the fault localization process. By ranking most suspicious pairs of method invocations, roadmaps-sorted lists of methods to be investigated-are created. At each method, unit coverage (e.g., basic blocks) is used to locate the fault site. Fifty-five bugs of four programs containing 2K to 80K lines of code (LOC) were analyzed. The results indicate that, by using the roadmaps, the effectiveness of the fault localization process is improved: 78% of all the faults are reached within a fixed amount of basic blocks; 40% more than an approach based on the Tarantula technique. Furthermore, fewer blocks have to be investigated until reaching the fault.","","978-1-4799-0215","10.1109/ASE.2013.6693124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6693124","Coverage-based debugging;Fault localization;Integration coverage","Debugging;Computer bugs;Libraries;Educational institutions;Statistical analysis;Context;Testing","program debugging;program testing","integration coverage;debugging process;fault localization automation;program code excerpts;white-box unit testing;fault localization process;Tarantula technique","","1","20","","","","","","IEEE","IEEE Conferences"
"Performance prediction for performance-sensitive queries based on algorithmic complexity","C. Chi; Y. Zhou; X. Ye","School of Software, Tsinghua University, Beijing 100084, China; Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China; School of Software, Tsinghua University, Beijing 100084, China","Tsinghua Science and Technology","","2013","18","6","618","628","Performance predictions for database queries allow service providers to determine what resources are needed to ensure their performance. Cost-based or rule-based approaches have been proposed to optimize database query execution plans. However, Virtual Machine (VM)-based database services have little or no sharing of resources or interactions between applications hosted on shared infrastructures. Neither providers nor users have the right combination of visibility/access/expertise to perform proper tuning and provisioning. This paper presents a performance prediction model for query execution time estimates based on the query complexity for various data sizes. The user query execution time is a combination of five basic operator complexities: O(1), O(log(n)), O(n), O(n log(n)), and O(n<sub>2</sub>). Moreover, tests indicate that not all queries are equally important for performance prediction. As such, this paper illustrates a performance-sensitive query locating process on three benchmarks: RUBiS, RUBBoS, and TPC-W. A key observation is that performance-sensitive queries are only a small proportion (20%) of the application query set. Evaluation of the performance model on the TPC-W benchmark shows that the query complexity in a real life scenario has an average prediction error rate of less than 10% which demonstrates the effectiveness of this predictive model.","1007-0214","","10.1109/TST.2013.6678907","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6678907","query performance;data size;query complexity;performance-sensitive query","Time complexity;Indexes;Benchmark testing;Monitoring;Semantics","computational complexity;query processing;user interfaces;virtual machines","performance prediction;performance-sensitive queries;algorithmic complexity;database queries;cost-based approach;rule-based approach;virtual machine;database services;query complexity;user query execution","","","","","","","","","TUP","TUP Journals & Magazines"
"Properties of Dynamically Dead Instructions for Contemporary Architectures","M. J. Jantz; K. Wu; P. A. Kulkarni","NA; NA; NA","2014 17th Euromicro Conference on Digital System Design","","2014","","","341","348","Processor frequency scaling has greatly stagnated over the last few years, making it difficult to continue improving sequential or single-threaded program speed. Hardware and software system developers now need to devise innovative and aggressive schemes to grow sequential software performance. The goal of this work is to assess the potential and feasibility of eliminating dynamically dead instructions (DDI) -- where the results of executed instructions are not used by the program -- to benefit program speed. Specifically, we quantify the ratio of DDI in the dynamic instruction stream for different classes of contemporary programs (general-purpose vs. embedded) and architectures (CISC vs. RISC), and explore characteristics of DDI to assist the design of effective solution mechanisms. To achieve our goal, we develop a robust and portable compiler (GCC) based framework for DDI research, and target this investigation at contemporary x86 and ARM based machines. We find that while a substantial fraction of instructions executed by all classes of programs are dynamically dead, architectural features show a visible impact. Our experiments reveal that a handful of static program instructions contribute a majority of DDI. We further find that DDI are often highly predictable, can be detected within small instruction windows, and a small amount of static context information can significantly benefit DDI detection at run-time. Thus, our research can induce the development and adoption of practical DDI elimination techniques to scale sequential program performance in future processors.","","978-1-4799-5793","10.1109/DSD.2014.108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927263","Dynamically dead instructions;architecture;compiler","Benchmark testing;Context;Program processors;Registers;Hardware;Optimization;Computer architecture","program compilers;reduced instruction set computing;software performance evaluation","dead instructions;contemporary architectures;processor frequency scaling;sequential program speed;single-threaded program speed;hardware and software system developer;sequential software performance;dynamically dead instruction;executed instruction;dynamic instruction stream;contemporary programs;solution mechanism;robust compiler;portable compiler;GCC based framework;contemporary x86 based machine;ARM based machines;architectural feature;static program instruction;instruction window;static context information;DDI detection;DDI elimination techniques;sequential program performance","","","25","","","","","","IEEE","IEEE Conferences"
"Neural Network Based Short Term Forecasting Engine to Optimize Energy and Big Data Storage Resources of Wireless Sensor Networks","Y. R. V. Prasad; R. Pachamuthu","NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","3","","511","516","Energy efficient wireless networks is the primary research goal for evolving billion device applications like IoT, smart grids and CPS. Monitoring of multiple physical events using sensors and data collection at central gateways is the general architecture followed by most commercial, residential and test bed implementations. Most of the events monitored at regular intervals are largely redundant/minor variations leading to large wastage of data storage resources in Big data servers and communication energy at relay and sensor nodes. In this paper a novel architecture of Neural Network (NN) based day ahead steady state forecasting engine is implemented at the gateway using historical database. Gateway generates an optimal transmit schedules based on NN outputs thereby reducing the redundant sensor data when there is minor variations in the respective predicted sensor estimates. It is observed that NN based load forecasting for power monitoring system predicts load with less than 3% Mean Absolute Percentage Error (MAPE). Gateway forward transmit schedules to all power sensing nodes day ahead to reduce sensor and relay nodes communication energy. Mat lab based simulation for evaluating the benefits of proposed model for extending the wireless network life time is developed and confirmed with an emulation scenario of our testbed. Network life time is improved by 43% from the observed results using proposed model.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.264","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273414","Clustered multi-hop network;Short term load forecasting;Power monitoring;Neural Network model;Big Data","Sensors;Load modeling;Artificial neural networks;Mathematical model;Data models;Wireless sensor networks;Monitoring","Big Data;forecasting theory;neural net architecture;telecommunication computing;wireless sensor networks","short term forecasting engine;energy optimization;big data storage resource optimization;wireless sensor networks;energy efficient wireless networks;IoT;smart grids;CPS;multiple physical event monitoring;data storage resources;big data servers;neural network architecture;NN architecture;day ahead steady state forecasting engine;historical database;gateway;optimal transmit schedules;redundant sensor data reduction;load forecasting;power monitoring system;load prediction;mean absolute percentage error;MAPE;sensor energy reduction;relay nodes communication energy reduction;Matlab based simulation;wireless network life time;emulation scenario","","","23","","","","","","IEEE","IEEE Conferences"
"On efficient computation of time constrained optimal power flow in rectangular form","N. Meyer-Huebner; M. Suriyah; T. Leibfried","Institute of Electrical Energy Systems and High Voltage Technology, Karlsruhe Institute of Technology (KIT), Germany; Institute of Electrical Energy Systems and High Voltage Technology, Karlsruhe Institute of Technology (KIT), Germany; Institute of Electrical Energy Systems and High Voltage Technology, Karlsruhe Institute of Technology (KIT), Germany","2015 IEEE Eindhoven PowerTech","","2015","","","1","6","In this paper, an entire formulation of a multiperiod optimal power flow is presented. The standard timeindependent AC Optimal Power Flow (AC-OPF) includes voltage, line capacity and generator limits for each time step. Additionally, time-dependent constraints like generator rampings, stored energy or energy contracts are crucial for time constrained optimization. A comparison of methods for modeling the latter and its computation efficiency is shown. Rectangular coordinates are used to minimize the computation time of the Hessian. The composition and computation of Jacobian and Hessian is described in detail. There is no conditional software or toolboxes necessary than MATLAB, as it uses a compact open source interior-point-method for solving the nonlinear optimization problem. The functionality is shown with a modified IEEE14 test case.","","978-1-4799-7693-5978-1-4799-7692","10.1109/PTC.2015.7232378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7232378","Dynamic Optimal Power Flow;Optimization;Model Predictive Control;Energy Storage;Smart Grids;Wind generation","Generators;Energy storage;Optimization;Jacobian matrices;Time factors;Mathematical model;Load flow","Hessian matrices;IEEE standards;Jacobian matrices;load flow;minimisation;nonlinear programming","time constrained optimal power flow computation;rectangular form;multiperiod optimal power flow;AC optimal power flow;AC-OPF;time-dependent constraint optimization;computation efficiency;rectangular coordinates;Hessian computation time minimization;Jacobian computation;Jacobian composition;Hessian composition;compact open source interior-point method;nonlinear optimization problem;modified IEEE14 test case","","3","11","","","","","","IEEE","IEEE Conferences"
"Expansion planning of medium voltage distribution networks","Nurulafiqah Nadzirah Binti Mansor; V. Levi","The University of Manchester, United Kingdom; The University of Manchester, United Kingdom","2015 50th International Universities Power Engineering Conference (UPEC)","","2015","","","1","6","The paper proposes a new methodology for optimal planning of medium voltage distribution networks. The overall model is presented as a two-stage optimization model, where the first stage deals with the investment part and the second stage focuses on the operation part. The novelty of the proposed model is demonstrated through the explicit incorporation of network security constraints in the mathematical model, in line with the UK planning standards, while taking into consideration different operating regimes with changing load and generation profiles, integration of new distributed generation units, construction of circuits on new corridors and optimal network reconfiguration. The highly complex mixed-integer nonlinear optimization model is applied to a few practical medium-voltage test systems of up to 120 nodes using a dedicated commercial software package. Some of the characteristics results are examined and highlighted in this paper. Additionally, sensitivity analyses are done to determine critical parameters affecting the development and performance of medium voltage distribution networks.","","978-1-4673-9682-0978-1-4673-9683","10.1109/UPEC.2015.7339776","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339776","Mixed-integer nonlinear optimization;planning of distribution networks;sensitivity studies;UK security criteria","Planning;Mathematical model;Optimization;Load modeling;Integrated circuit modeling;Substations;Medium voltage","integer programming;nonlinear programming;power distribution planning;power system security","expansion planning;medium voltage distribution networks;optimal planning;two-stage optimization model;network security constraints;mathematical model;UK planning standards;load profiles;generation profiles;distributed generation units;optimal network reconfiguration;complex mixed-integer nonlinear optimization model;commercial software package;sensitivity analyses","","","20","","","","","","IEEE","IEEE Conferences"
"Implementing buck converter for battery charger using soft switching techniques","M. Salem; A. Jusoh; N. R. N. Idris","Department of power engineering, Faculty of Electrical Engineering, University Technology Malaysia, Johor, Malaysia; Department of power engineering, Faculty of Electrical Engineering, University Technology Malaysia, Johor, Malaysia; Department of power engineering, Faculty of Electrical Engineering, University Technology Malaysia, Johor, Malaysia","2013 IEEE 7th International Power Engineering and Optimization Conference (PEOCO)","","2013","","","188","192","This paper describes the performance of the resonant buck converter battery charger using soft switching, which has been seen as a good and convincing solution for switched mode electronic power converter. The switching frequency of the power semiconductor devices has been increased in order to decrease the losses and the stress of the switching. As a result of that, the efficiency and the reliability of the buck converter obviously increased. The proposed charger circuits with both zero voltage switching and zero current switching were analyzed and tested using MATLAB-SIMULINK software for a 12V 4 Ah lead acid battery. The structure of the proposed circuits does not require complex control mechanisms and a large number of components.","","978-1-4673-5074-7978-1-4673-5072-3978-1-4673-5073","10.1109/PEOCO.2013.6564540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564540","zero current switching;zero voltage switching;battery charging;buck converter;soft switching","Switches;Pulse width modulation;Partial discharges;Lead compounds;Discharges (electric);Lead;MATLAB","battery chargers;circuit reliability;power semiconductor devices;resonant power convertors;secondary cells;switched mode power supplies;switching convertors;zero current switching;zero voltage switching","charger circuits;zero voltage switching;zero current switching;Matlab-Simulink software;lead acid battery;buck converter reliability;power semiconductor devices;switching frequency;switched mode electronic power converter;resonant buck converter battery charger;soft switching techniques;voltage 12 V","","3","11","","","","","","IEEE","IEEE Conferences"
"Performance of Tunisian enterprises integrating quality system, ERP (enterprise resource planning) and SCM (Supply Chain Management)","I. Aouadni; A. Rebai","University of Sfax, MODILS, FSEG 3018, Tunisia; University of Sfax, MODILS, FSEG 3018, Tunisia","2013 5th International Conference on Modeling, Simulation and Applied Optimization (ICMSAO)","","2013","","","1","4","The main objective of this paper is to identify the determinants of performance of companies certified ISO 9001/2000 and integrating ERP and SCM. Can do this, we try as a first step explains the critical success factors of the alignment between the formalized process according to ISO 9001/2000 and the ERP and SCM software and its modules . Then, we study the impact of the integration of these three projects on organizational and financial performance. Finally, the proposed conceptual model was tested on 31 industrial companies in Tunisia. The results show that performance improvement depends on the change management.","","978-1-4673-5814-9978-1-4673-5812-5978-1-4673-5813","10.1109/ICMSAO.2013.6552576","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552576","Alignment;ISO 9001/2000;ERP;SCM;Performance","Companies;ISO standards;Total quality management;Qualifications;Project management","enterprise resource planning;ISO standards;management of change;manufacturing data processing;supply chain management","Tunisian enterprises;quality system;ERP;enterprise resource planning;SCM software;supply chain management;ISO 9001-2000 standard;critical success factors;organizational performance;financial performance;conceptual model;industrial companies;change management","","","8","","","","","","IEEE","IEEE Conferences"
"Performance and Energy Analysis of the Restricted Transactional Memory Implementation on Haswell","B. Goel; R. Titos-Gil; A. Negi; S. A. McKee; P. Stenstrom","NA; NA; NA; NA; NA","2014 IEEE 28th International Parallel and Distributed Processing Symposium","","2014","","","615","624","Hardware transactional memory implementations are becoming increasingly available. For instance, the Intel Core i7 4770 implements Restricted Transactional Memory (RTM) support for Intel Transactional Synchronization Extensions (TSX). In this paper, we present a detailed evaluation of RTM performance and energy expenditure. We compare RTM behavior to that of the TinySTM software transactional memory system, first by running micro benchmarks, and then by running the STAMP benchmark suite. We find that which system performs better depends heavily on the workload characteristics. We then conduct a case study of two STAMP applications to assess the impact of programming style on RTM performance and to investigate what kinds of software optimizations can help overcome RTM's hardware limitations.","1530-2075","978-1-4799-3800-1978-1-4799-3799","10.1109/IPDPS.2014.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877294","HTM;TSX;RTM;Performance evaluation;Energy evaluation","Hardware;Pollution;Instruction sets;Concurrent computing;Synchronization;Benchmark testing","benchmark testing;microprocessor chips;performance evaluation;power aware computing;synchronisation","energy analysis;performance analysis;restricted transactional memory implementation;Haswell;hardware transactional memory implementations;Intel Core i7 4770;Intel Transactional Synchronization Extension;TSX;RTM performance;energy expenditure;TinySTM software transactional memory system;microbenchmarks;STAMP benchmark suite;workload characteristics;programming style;software optimizations","","11","21","","","","","","IEEE","IEEE Conferences"
"Research on Optimization Segmentation Algorithm for Chinese/English Mixed Character Image in OCR","L. Mingzhu; S. Yuxiu; D. Yinan","NA; NA; NA","2014 Fourth International Conference on Instrumentation and Measurement, Computer, Communication and Control","","2014","","","764","769","In allusion to the problem of low accuracy rate recognition in Chinese/English mixed characters, the paper researches on optimization algorithm for segmentation in Chinese/English mixed characters based on OCR system. Rough segment for text images is based on vertical projection method, which follows to characters segmentation theory, extraction of Chinese character, Chinese character component and English number connectivity regional. In Chinese character component connectivity regional, traditional Chinese character component merging algorithms will cause some Chinese characters components are merged incompletely, therefore, an unit merging algorithm based on feedback recognition is presented to merge Chinese character component, in Chinese character, English and number connectivity regional, as adhesion character can lead to segmentation errors, achieving the detection of adhesion character and re-segmentation through the geometric features of character. The test of mixed character segmentation showes that: In the course of recognition on Chinese/English mixed character, the segmentation optimization algorithm have obvious advantage over the traditional algorithms on the accuracy rate of recognition, especially on Chinese characters those are composed of left and right components.","","978-1-4799-6575-5978-1-4799-6574","10.1109/IMCCC.2014.162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6995132","Chinese/English mixed character;unit merging;feedback recognition;OCR","Adhesives;Image segmentation;Character recognition;Merging;Accuracy;Optical character recognition software;Optimization","feedback;image segmentation;natural language processing;optical character recognition;text analysis","optimization segmentation algorithm;Chinese-English mixed character image;OCR system;rough segment;text image segmentation;vertical projection method;character segmentation theory;English number connectivity regional;unit merging algorithm;feedback recognition;adhesion character;Chinese character component connectivity regional component merging algorithms;traditional Chinese character component merging algorithms","","1","20","","","","","","IEEE","IEEE Conferences"
"Artificial intelligence based TNEP. Part 1: Mathematical models","D. Cristian; C. Barbulescu; S. Kilyeni; O. Pop; F. Solomonese","&#x201C;Politehnica&#x201D; University of Timisoara, Romania, Power Systems Department; &#x201C;Politehnica&#x201D; University of Timisoara, Romania, Power Systems Department; &#x201C;Politehnica&#x201D; University of Timisoara, Romania, Power Systems Department; &#x201C;Politehnica&#x201D; University of Timisoara, Romania, Power Systems Department; &#x201C;Politehnica&#x201D; University of Timisoara, Romania, Power Systems Department","2013 IEEE 8th International Symposium on Applied Computational Intelligence and Informatics (SACI)","","2013","","","391","394","The paper is focusing on transmission network expansion planning (TNEP) problem solved using artificial intelligence techniques. It is divided into two parts. The 1<sup>st</sup> part is dedicated to the particle swarm optimization (PSO) and genetic algorithm (GA) concepts and mechanisms. The mathematical models and the associated software tool are also presented. Practical considerations are discussed. The 2<sup>nd</sup> part is focusing on case studies. 13 buses test power system, developed by the authors and IEEE 24 RTS have been used. The research work is going to be used in case of the Romanian power system (over 1000 buses).","","978-1-4673-6400-3978-1-4673-6397","10.1109/SACI.2013.6609005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6609005","","Power systems;Sociology;Statistics;Planning;Particle swarm optimization;Genetic algorithms;Mathematical model","artificial intelligence;genetic algorithms;IEEE standards;mathematical analysis;particle swarm optimisation;power engineering computing;power transmission planning;power transmission reliability","GA;PSO;TNEP problem;Romanian power system;IEEE 24 RTS;buses test power system;software tool;genetic algorithm;particle swarm optimization;artificial intelligence techniques;transmission network expansion planning;mathematical models","","1","18","","","","","","IEEE","IEEE Conferences"
"Distribution network power loss by using Artificial Bee Colony","M. N. Bin Muhtazaruddin; G. Fujita","Shibaura Inst. of Technol., Koto, Japan; Shibaura Inst. of Technol., Koto, Japan","2013 48th International Universities' Power Engineering Conference (UPEC)","","2013","","","1","5","Distributed Generation (DG) offers many benefits to the distribution networks for instance increase reliability, power losses reduction and voltage improvement. Nevertheless, one of the issues still faced by the utilities is uncertainty to determine optimal output power and location of DG. Improper coordination of the DG can deteriorate the distribution performance. Therefore, many researchers have suggested various tools to compute optimally the DG coordination. Usually, calculation of location and size of the DG are determined separately, which means different method is applied for sizing and location. Thus, it may result the solution trap in local optimum. This manuscript provides a solution to the output power and location of multiple DG sources by using modified Artificial Bee Colony. Furthermore, in the analysis, several case studies with different load profile are presented. The solution is simulated by using MATLAB programming and tested on 33-bus distribution system.","","978-1-4799-3254","10.1109/UPEC.2013.6714964","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714964","Artificial Bee Colony;DG coordination;DG sizing;distributed generation;power losses reduction;radial distribution networks","Distributed power generation;Optimization;Linear programming;Computer aided software engineering;Power system stability;Resource management","distributed power generation;optimisation;power distribution reliability","distribution network power loss;artificial bee colony;distributed generation;distribution network reliability;power loss reduction;voltage improvement;multiple DG source;33-bus distribution system","","3","17","","","","","","IEEE","IEEE Conferences"
"Research on detection sensitivity based on wave structure of Lamb wave","X. Li; Y. Wang; G. Song; X. Sun; X. Sun","Guizhou Construction Science Research &amp; Design Institute Limited Company of CSCEC. Guiyang, China; College of Aerospace and Civil Engineering, Harbin Engineering University, Harbin, China; School of Computer Science and Software Engineering, Tianjin Polytechnic University Tianjin, China; College of Aerospace and Civil Engineering, Harbin Engineering University, Harbin, China; College of Aerospace and Civil Engineering, Harbin Engineering University, Harbin, China","2013 Far East Forum on Nondestructive Evaluation/Testing: New Technology and Application","","2013","","","172","176","Dispersion and multi-mode occurred when Lamb wave propagate in the lamina, the reflection principle of Lamb wave is complicated while it encounter defects. For diverse defects, different modes have different sensitivities. The theoretical of displacement and stress wave structure are analyzed. The dispersion curve and wave structure curve are presented. Piezoelectric (PZT) ceramic wafers are applied for excitation and reception of guided wave in the experiment and different frequencies under low FD (the product of frequency and thickness) are optimized for evaluate the defects. The sensitivity for diverse defects is derived according to the change of amplitude. Experimental results show that there are a good corresponding relationship between the wave structure and the detection sensitivity.","","978-1-4673-6020-3978-1-4673-6018-0978-1-4673-6019","10.1109/FENDT.2013.6635550","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6635550","wave structure;displacement component;stress component;change of amplitude","Stress;Dispersion;Sensitivity;Surface waves;Educational institutions;Steel;Acoustics","acoustic wave propagation;piezoceramics;surface acoustic waves","detection sensitivity;Lamb wave propagation;lamina;reflection principle;stress wave structure;dispersion curve;piezoelectric ceramic wafers","","","6","","","","","","IEEE","IEEE Conferences"
"Multiple Flow in Extended SDN Wireless Mobility","F. Meneses; D. Corujo; C. Guimarães; R. L. Aguiar","NA; NA; NA; NA","2015 Fourth European Workshop on Software Defined Networks","","2015","","","1","6","5G, as the next generation telecommunications architecture that aims to tackle the explosion of connected devices, services and access technologies, will heavily rely on Software Defined Mechanisms (SDN) to compose its underlying mechanisms. Notwithstanding, despite the need for novel control procedures to support and optimize increasingly challenging wireless mobile scenarios, SDN has been being deployed at the core and backhaul sections of the network and is not actively considering its impact directly over the wireless mobile terminals themselves. The challenges associated with the extension of SDN protocols, such as Open Flow, all the way to the terminal requires the design and evaluation of frameworks that not only provide such mechanisms, but actually evaluate them and their benefits. This paper explores a framework where SDN mechanisms are extended all the way to the mobile node, in heterogeneous wireless environments featuring different mobile nodes with multiple data flows, which act both as consumers and producers of information. In this way, flow-based mobility management becomes available to the network controller entity, through the Open Flow protocol, allowing as well the assistance of the mobile nodes in the execution of the mobility procedure. The concept framework was implemented over a physical wireless test bed, validating its contribution in a mobile source-mobility use case, with results highlighting the promising benefits of extending SDN approaches for end-to-end flow control in wireless environments.","2379-0369;2379-0350","978-1-5090-0180","10.1109/EWSDN.2015.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313587","Open Flow;WIreless;Mobile Node;SDN;Handover","Handover;Wireless communication;Throughput;Protocols;Mobile nodes","5G mobile communication;mobility management (mobile radio);protocols;software defined networking","5G;next generation telecommunications architecture;software defined mechanisms;wireless mobile scenarios;wireless mobile terminals;SDN protocols;heterogeneous wireless environments;mobile nodes;multiple data flows;flow-based mobility management;Open Flow protocol;physical wireless test bed;mobile source-mobility use case;end-to-end flow control","","4","12","","","","","","IEEE","IEEE Conferences"
"Proving termination of imperative programs using Max-SMT","D. Larraz; A. Oliveras; E. Rodríguez-Carbonell; A. Rubio","Universitat Politècnica de Catalunya, Barcelona, Spain; Universitat Politècnica de Catalunya, Barcelona, Spain; Universitat Politècnica de Catalunya, Barcelona, Spain; Universitat Politècnica de Catalunya, Barcelona, Spain","2013 Formal Methods in Computer-Aided Design","","2013","","","218","225","We show how Max-SMT can be exploited in constraint-based program termination proving. Thanks to expressing the generation of a ranking function as a Max-SMT optimization problem where constraints are assigned different weights, quasi-ranking functions -functions that almost satisfy all conditions for ensuring well-foundedness- are produced in a lack of ranking functions. By means of trace partitioning, this allows our method to progress in the termination analysis where other approaches would get stuck. Moreover, Max-SMT makes it easy to combine the process of building the termination argument with the usually necessary task of generating supporting invariants. The method has been implemented in a prototype that has successfully been tested on a wide set of programs.","","978-0-9835678-3","10.1109/FMCAD.2013.6679413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6679413","","Pressing;Optimization;Buildings;Prototypes;Computer bugs;Software;Generators","computability;constraint handling;optimisation;theorem proving","imperative programs;constraint-based program termination proving;max-SMT optimization problem;quasiranking functions;trace partitioning;termination analysis;termination argument","","13","25","","","","","","IEEE","IEEE Conferences"
"Profiling-driven multi-cycling in FPGA high-level synthesis","S. Hadjis; A. Canis; R. Sobue; Y. Hara-Azumi; H. Tomiyama; J. Anderson","Dept. of Electrical and Computer Engineering, University of Toronto, Toronto, Ontario, Canada; Dept. of Electrical and Computer Engineering, University of Toronto, Toronto, Ontario, Canada; Dept. of Electronic and Computer Engineering, Ritsumeikan University, Shiga, Japan; Dept. of Communications and Compuer Engineering, Tokyo Institute of Technology, Tokyo, Japan; Dept. of Electronic and Computer Engineering, Ritsumeikan University, Shiga, Japan; Dept. of Electrical and Computer Engineering, University of Toronto, Toronto, Ontario, Canada","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","31","36","Multi-cycling is a well-known strategy to improve performance in digital design, wherein the required time for selected combinational paths is lengthened to multiple clock cycles (rather than just one). The approach can be applied to paths associated with computations whose results are not needed immediately - such paths are allowed multiple clock cycles to “complete”, reducing the opportunity for them to form the critical path of the circuit. In this paper, we consider multi-cycling in the high-level synthesis context (HLS) and use software profiling to guide multi-cycling optimizations. Specifically, prior to HLS, we execute the program in software with typical datasets to gather data on the number of times each code segment executes. During HLS, we then extend the schedule for infrequently executed code segments and apply multi-cycling to the dilated schedules, which exhibit greater opportunities for multi-cycling. In essence, our approach ensures that non-frequently executed code segments will not form the critical path of the HLS-generated circuit. In an experimental study targeting the Altera Stratix IV FPGA, we evaluate the impact on speed performance and area for both traditional multi-cycling, as well as the proposed software profiling-driven multi-cycling, and show that profiling-driven multi-cycling leads to an average speedup of over 10% across 13 benchmark circuits, with some circuit speedups in excess of 30%. Circuit area is reduced by 11%, yielding a mean 20% improvement in area-delay product.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0512","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092354","","Clocks;Registers;Software;Hardware;Particle separators;Delays;Cutoff frequency","field programmable gate arrays;high level synthesis","multiple clock cycle;nonfrequently executed code segment;HLS-generated circuit;Altera Stratix IV FPGA high-level synthesis;software profiling-driven multicycling optimization;benchmark circuit","","2","11","","","","","","IEEE","IEEE Conferences"
"Optimal power flow for reactive power compensation increasing the cross-border transmission capacity","M. Farrokhseresht; M. R. Hesamzadeh; S. Rahimi; J. Lin","Electric Power Systems Dep., KTH Royal Institute of Technology, Stockholm, Sweden; Electric Power Systems Dep., KTH Royal Institute of Technology, Stockholm, Sweden; ABB Enterprise Software, V&#x00E4;ster&#x00E5;s, Sweden; PJM Interconnection, Philadelphia, USA","2015 IEEE 15th International Conference on Environment and Electrical Engineering (EEEIC)","","2015","","","2135","2140","Increasing the Net Transfer Capacity (NTC) of tie-lines between different grid areas may lead to a decrease in the cost to generate electricity in one area if this cost is more expensive than the cost of importing power from neighbouring areas over those tie-lines. An effective means to increase the cross-border transmission capacity is by installing reactive power compensation devices. In this paper, a multi-objective optimization is devised that optimally locates and sizes reactive compensation devices in a grid, so that the net benefit of increasing the NTC value of a tie-line is maximized and the maximum voltage stability indicator (L-index) in the grid area is minimized, indicating there is enough voltage stability margin. The genetic algorithm NSGA-II is implemented to perform the optimization. The IEEE 30-bus and 14-bus test grids are used as two interconnected areas.","","978-1-4799-7993-6978-1-4799-7992","10.1109/EEEIC.2015.7165508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165508","Net Transfer Capacity;reactive power compensation;Reactive Power Planning;voltage stability;L-indices;NSGA-II","Reactive power;Capacitors;Power system stability;Generators;Optimization;Genetic algorithms;Linear programming","genetic algorithms;IEEE standards;load flow control;power grids;power system control;reactive power control;voltage control","optimal power flow;reactive power compensation;cross-border transmission capacity;net transfer capacity;multiobjective optimization;reactive compensation device;power grid;maximum voltage stability indicator;genetic algorithm;NSGA-II;IEEE 30-bus test grids;IEEE 14-bus test grids","","1","16","","","","","","IEEE","IEEE Conferences"
"Optimizing GPU Virtualization with Address Mapping and Delayed Submission","X. Wang; H. Wang; Y. Sang; Z. Wang; Y. Luo","NA; NA; NA; NA; NA","2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)","","2014","","","413","416","The state-of-the-art GPU virtualization framework, gVirtuS, relies on an API remoting mechanism to set up a communication channel between a virtual machine and the host, so that a CUDA application in a virtual machine can be executed ""remotely"" in the host. We observe that this API remoting mechanism often involves large-volume and frequent data transmissions between the host OS and the guest OS, which lead to a significant performance degradation. We present an address mapping scheme so the host can directly access the machine memory space of the guest and thus avoid data copying between the guest and the host. To reduce the frequency of data transmissions, we introduce a delayed submission scheme. We implement both address mapping and delayed submission in KVM. Our evaluation on a set of CUDA benchmarks shows that address mapping can improve over the original gVirtuS by up to 6.5 times. Delayed submission is able to further reduce the virtualization overhead by half in a pathological case.","","978-1-4799-6123-8978-1-4799-6122","10.1109/HPCC.2014.70","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056774","GPU virtualization;address mapping;memory virtualization;delayed submission;performance improvement","Graphics processing units;Virtualization;Benchmark testing;Data communication;Virtual machining;Memory management;Resource management","application program interfaces;graphics processing units;operating systems (computers);parallel architectures;remote procedure calls;storage management;virtual machines;virtualisation","GPU virtualization optimization;GPU virtualization framework;gVirtuS;API remoting mechanism;communication channel;virtual machine;remote CUDA application execution;data transmission;host OS;guest OS;performance degradation;address mapping scheme;machine memory space access;data copying;delayed submission scheme;KVM;CUDA benchmark;virtualization overhead","","1","6","","","","","","IEEE","IEEE Conferences"
"A novel multi-resolution segmentation algorithm for highresolution remote sensing imagery based on minimum spanning tree and minimum heterogeneity criterion","H. Li; Y. Tang; Q. Liu; H. Ding; L. Jing; Q. Lin","Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese, Academy of Sciences, Beijing, 100094, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese, Academy of Sciences, Beijing, 100094, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese, Academy of Sciences, Beijing, 100094, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese, Academy of Sciences, Beijing, 100094, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese, Academy of Sciences, Beijing, 100094, China; Key Laboratory of Digital Earth Science, Institute of Remote Sensing and Digital Earth, Chinese, Academy of Sciences, Beijing, 100094, China","2014 IEEE Geoscience and Remote Sensing Symposium","","2014","","","2850","2853","Image segmentation is the basis of object-based information extraction from remote sensing imagery. Image segmentation based on multiple features, multi-resolution, and spatial context is one current research focus. Combining graph theory based optimization with the multi-scale image segmentation framework of the eCognition software, a multi-scale image segmentation method is proposed in this paper. In this method, a coherent enhancement anisotropic diffusion filtering approach and a minimum spanning tree segmentation algorithm are employed to initially segment the image. After that, the resulting segments are merged regarding minimum heterogeneity criteria, which are based on both the spectral characteristics and the shape parameters of segments. Two test images were used for visual and quantitative comparisons of the proposed method with the multi-scale segmentation method FNEA employed in the eCognition software. The results show that the proposed method is effective, and is more sensitive to subtle spectral differences than the FNEA.","2153-6996;2153-7003","978-1-4799-5775","10.1109/IGARSS.2014.6947070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6947070","Multi-scale segmentation;Minimum spanning tree;Minimum heterogeneity criteria;Remote sensing imagery","Image segmentation;Remote sensing;Software;Indexes;Image resolution;Optimization;Anisotropic magnetoresistance","feature extraction;geophysical image processing;image resolution;image segmentation;remote sensing;terrain mapping","FNEA multiscale segmentation method;spectral characteristics;shape parameters;minimum heterogeneity criteria;minimum spanning tree segmentation algorithm;coherent enhancement anisotropic diffusion filtering approach;eCognition software;spatial context;object-based information extraction;image segmentation;minimum heterogeneity criterion;high- resolution remote sensing imagery;multiresolution segmentation algorithm","","","10","","","","","","IEEE","IEEE Conferences"
"Athena: A Visual Tool to Support the Development of Computational Intelligence Systems","P. Oliveira; M. Souza; R. Braga; R. Britto; R. L. Rabêlo; P. S. Neto","NA; NA; NA; NA; NA; NA","2014 IEEE 26th International Conference on Tools with Artificial Intelligence","","2014","","","950","959","Computational Intelligence (CI) embraces techniques designed to address complex real-world problems in which traditional approaches are ineffective or infeasible. Some of these techniques are being used to solve several complex problems, such as the team allocation, building products portfolios in a software product line and test case selection/prioritization. However, despite the usefulness of these applications, the development of solutions based in CI techniques is not a trivial activity, since it involves the implementation/adaptation of algorithms to specific context and problems. This work presents Athena, a visual tool developed aiming at offering a simple approach to develop CI-based software systems. In order to do this, we proposed a drag-and-drop approach, which we called CI as a Service (CIaaS). Based on a preliminary study, we can state that Athena can help researchers to save time during the development of computational intelligence approaches.","1082-3409;2375-0197","978-1-4799-6572","10.1109/ICTAI.2014.144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984580","Computational Intelligence;Artificial Intelligence;Visual Programming;Tool;Service","Visualization;Productivity;Remuneration;Computational modeling;Algorithm design and analysis;Computational intelligence;Resource management","artificial intelligence;software product lines;visual programming","Athena;Computational Intelligence Systems;software product line;test case selection;CI techniques;trivial activity;drag-and-drop approach;CI-as-a-Service;CIaaS;visual programming;prioritization;visual tool","","1","30","","","","","","IEEE","IEEE Conferences"
"Phase-only adaptive spatial transmit nulling","T. Webster; T. Higgins; A. K. Shackelford; J. Jakabosky; P. McCormick","Radar Division, Naval Research Laboratory, Washington, DC 20375, USA; Radar Division, Naval Research Laboratory, Washington, DC 20375, USA; Radar Division, Naval Research Laboratory, Washington, DC 20375, USA; Radar Systems Lab, University of Kansas, Lawrence, 66045, USA; Radar Systems Lab, University of Kansas, Lawrence, 66045, USA","2015 IEEE Radar Conference (RadarCon)","","2015","","","0931","0936","Adaptive phase-only transmit nulling was previously demonstrated using an eight-channel radar test bed. An error in the calibration software that left one of the transmit elements uncalibrated has since been corrected. New experimental results are presented that demonstrate improved performance relative to previous experiments. In this paper the Reiterative Uniform Weight Optimization (RUWO) algorithm, which utilizes the maximum signal-to-interference plus noise ratio (SINR) framework in a reiterative fashion, is used to generate adaptive phase-only weights from received interference data. Additionally, this work investigates the impact of the bandwidth of the interference signal on the ability to adaptively produce spatial nulls using RUWO.","1097-5659;2375-5318","978-1-4799-8232-5978-1-4799-8231","10.1109/RADAR.2015.7131128","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131128","","Interference;Antenna measurements;Arrays;Azimuth;Frequency measurement;Receivers;Radar","adaptive antenna arrays;adaptive radar;antenna phased arrays;array signal processing;iterative methods;optimisation;radar antennas;radar computing;radar interference;radar signal processing","phase-only adaptive spatial transmit nulling;calibration software error;reiterative uniform weight optimization algorithm;RUWO algorithm;maximum signal-to-interference-plus-noise ratio framework;SINR framework;received interference data;interference signal bandwidth","","1","8","","","","","","IEEE","IEEE Conferences"
"Algorithm research on parallel topology of large-scale power system on-line simulation","Z. Min; X. Dechao; L. Yalou; A. Ning","China Electric Power Research Institute, Qinghe, Beijing 100192, P.R. China; China Electric Power Research Institute, Qinghe, Beijing 100192, P.R. China; China Electric Power Research Institute, Qinghe, Beijing 100192, P.R. China; China Electric Power Research Institute, Qinghe, Beijing 100192, P.R. China","2014 International Conference on Power System Technology","","2014","","","55","60","With the continuous expansion of calculation scale, on-line simulation analysis of large-scale power grid requires that network topology analysis is more efficient, while parallel topology is one good solution. Parallel topology of large scale power system on-line simulation calculation is studied in detail; this paper presented a parallel network topology algorithm, and this algorithm is successfully applied to large power electromechanical transient simulation in real time. According to the initial load flow, the algorithm firstly defines main power grid topology structure; the network partitioning algorithm based on optimization boundary surface is applied to put substations which include lots of switches into the corresponding sub network, then one parallel topology algorithm is realized based on in ill ti point interface. Based on parallel electromechanical transient simulation program of power system analysis software package, in order to achieve effective integration, switching logic between topology switch shift and disconnection fault is defined. For real-time electromechanical transient simulation of large-scale power grid, this paper does not increase the complexity of the contact system in original parallel electromechanical transient algorithms, plants and substations topology analysis is distributed in the sub network in parallel. At the same time, in order to speed up the solving efficiency and power grid topology analysis, the local topology changes, admittance matrix modification technology, etc. is used in this paper. Large power system examples are tested to show that the proposed parallel topology algorithm can obtain good acceleration effect, substations operation can be simulated fast without modifying the power grid model, and the overall efficiency is basically meet the requirement of real-time simulation.","","978-1-4799-5032","10.1109/POWERCON.2014.6993509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6993509","Topology;Parallel computing;Transient Stability;Online simulation;Network partitioning","Network topology;Topology;Transient analysis;Algorithm design and analysis;Power grids;Power system stability","EMTP;optimisation;power grids;power system analysis computing","network topology analysis;large scale power system online simulation calculation;parallel network topology algorithm;large power electromechanical transient simulation;power grid topology structure;network partitioning algorithm;optimization boundary surface;parallel electromechanical transient simulation program;power system analysis software package;substations topology analysis","","","18","","","","","","IEEE","IEEE Conferences"
"Shared autonomous vehicles: Model formulation, sub-problem definitions, implementation details, and anticipated impacts","D. J. Fagnant","University of Utah, United States","2015 American Control Conference (ACC)","","2015","","","2593","2593","The emergence of self-driving vehicles holds great promise for the future of transportation. While it will still be a number of years before fully self-driving vehicles can safely and legally drive unoccupied on U.S. streets, once this is possible, a new transportation mode for personal travel looks set to arrive. This new mode is the shared autonomous vehicle (SAV), combining features of short-term on-demand rentals with self-driving capabilities. This presentation seeks to demonstrate how SAVs' potential may be assessed through agent-based modeling, as applied in Austin, TX. The framework sheds SAVs' current speed limitations established in early pilot SAV demonstrations by CityMobil2 and Google. A 12-mile by 24-mile regional geofence is employed to limit service within Austin to the areas with the greatest demand intensity. The simulation uses a sample of trips from the region's planning model to generate demand across traffic analysis zones and a 32,272-link network. Trips call on the vehicles in 5-minute departure time windows, with link-level travel times varying by hour of day based on MATSim's dynamic traffic assignment simulation software. A sizable degree of market share is assumed, though not market dominance, with adoption levels ranging from 2.3-11.1 percent of regional personal trip-making within the geofenced area. This simulation work also assumes that individual travelers may share rides through dynamic ride-sharing (DRS), which may pool two or more travelers with similar origins, destinations and departure times in the same vehicle. The presentation focuses on problem formulation and solution implementation details regarding SAV-traveler assignment, unoccupied vehicle relocation, and dynamic ridesharing. Model objectives in these problems seek to balance competing goals of minimalized total miles driven, as well as minimalized traveler wait (particularly long waits) and in-vehicle travel times. Multiple scenario variations are also tested, as well as a fleet size optimization procedure that seeks to maximize return on investment by a private operator. Results show that each SAV is able to replace around 10 conventional vehicles within the 24 mi × 12 mi area while still maintaining a reasonable level of service (as proxied by user wait times, which average just 1.0 minutes), though up to 8 percent more vehicle-miles traveled (VMT) may be generated if DRS is not utilized, due to SAVs journeying unoccupied to the next traveler, or relocating to a more favorable position in anticipation of next-period demand. Simulation results also indicate that DRS reduces total service times (wait times plus in-vehicle travel times) and travel costs for SAV users, even after accounting for extra passenger pick-ups, drop-offs and non-direct routings. While the base-case scenario (serving 56,324 person-trips per day, on average) showed that a fleet of SAVs allowing for DRS may result in vehicle-miles traveled that exceed person-trip miles demanded (due to anticipatory relocations of empty vehicles, between trip calls), it is possible to reduce overall VMT as trip-making intensity (SAV membership) rises and/or DRS users become more flexible in their trip timing and routing. Finally, these simulation results suggest that a private fleet operator paying $70,000 per new SAV could earn a 19% annual (long-term) return on investment while offering SAV services at $1.00 per mile of a non-shared trip (which is less than a third of Austin's average taxi cab fares).","0743-1619;2378-5861","978-1-4799-8684-2978-1-4799-8685","10.1109/ACC.2015.7171124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7171124","","Vehicles;Vehicle dynamics;Mobile robots;Analytical models;Investment;Simulation","mobile robots;optimisation;road traffic;road vehicles;vehicle routing","shared autonomous vehicles;self-driving vehicles;short-term on-demand rentals;agent-based modeling;Austin;Texas;CityMobil2;Google;traffic analysis zones;MATSim dynamic traffic assignment simulation software;dynamic ride-sharing;DRS;SAV-traveler assignment;unoccupied vehicle relocation;in-vehicle travel times;fleet size optimization procedure;trip timing;vehicle routing","","","","","","","","","IEEE","IEEE Conferences"
"SSDM: Smart Stack Data Management for software managed multicores (SMMs)","Jing Lu; Ke Bai; A. Shrivastava","Compiler Microarchitecture Laboratory, Arizona State University, Tempe, 85287, USA; Compiler Microarchitecture Laboratory, Arizona State University, Tempe, 85287, USA; Compiler Microarchitecture Laboratory, Arizona State University, Tempe, 85287, USA","2013 50th ACM/EDAC/IEEE Design Automation Conference (DAC)","","2013","","","1","8","Software Managed Multicore (SMM) architectures have been proposed as a solution for scaling the memory architecture. In an SMM architecture, there are no caches, and each core has only a local scratchpad memory. If all the code and data of the task to be executed on an SMM core cannot fit on the local memory, then data must be managed explicitly in the program through DMA instructions. While all code and data need to be managed, an efficient technique to manage stack data is of utmost importance since an average of 64% of all accesses may be to stack variables [16]. In this paper, we formulate the problem of stack data management optimization on an SMM core. We then develop both an ILP and a heuristic - SSDM (Smart Stack Data Management) to find out where to insert stack data management calls in the program. Experimental results demonstrate SSDM can reduce the overhead by 13X over the state-of-the-art stack data management technique [10].","0738-100X;0738-100X","978-1-4503-2071","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6560742","Stack data;local memory;scratchpad memory;SPM;embedded systems;multi-core processor","Memory management;Multicore processing;Libraries;Benchmark testing;Software;Memory architecture","digital storage;microprocessor chips;multiprocessing systems","SSDM;smart stack data management;software managed multicores;DMA instructions","","","30","","","","","","IEEE","IEEE Conferences"
"Real-time control of a small urban stormwater network","R. Degrave; J. Schoorens; X. Litrico","LyRE, Research center of Lyonnaise des Eaux, Bordeaux, France; LyRE, Research center of Lyonnaise des Eaux, Bordeaux, France; LyRE, Research center of Lyonnaise des Eaux, Bordeaux, France","2013 10th IEEE INTERNATIONAL CONFERENCE ON NETWORKING, SENSING AND CONTROL (ICNSC)","","2013","","","526","531","Real-time control can be used to prevent combined sewer overflows, by maximizing the amount of water stored and minimizing the amount of water discharged to the natural environment. Existing methods use optimization as a control means, which may be too costly for small cities. We propose in this article a method to control urban water network using simple controllers. A model of a pilot site (Biarritz) has been built with the software SWMM5 (sewerage modeling software) which is linked to Scilab, to test various control strategies. We first consider two classical control policies for the sewerage system: the local upstream control and the distant downstream which have their advantages and drawbacks. In this study, we propose the mixed control which ensures the benefits of these two types of control.","","978-1-4673-5200-0978-1-4673-5198-0978-1-4673-5199","10.1109/ICNSC.2013.6548794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548794","Combined Sewer Overflow;real-time control;urban sewer network","Software;Real-time systems;Europe","control engineering computing;flow control;sewage treatment;water resources","realtime control;urban stormwater network;local upstream control structure;distant downstream control structure;sewer overflow;optimization;SWMM5 software;sewerage modeling software;control strategy;mixed control","","","5","","","","","","IEEE","IEEE Conferences"
"A domain-specific language to facilitate software defined radio parallel executable patterns deployment on heterogeneous architectures","L. J. Mohapi; S. Winberg; M. Inggs","Department of Electrical Engineering, Radar Remote Sensing Group, University of Cape Town, Cape Town, South Africa; Department of Electrical Engineering, Radar Remote Sensing Group, University of Cape Town, Cape Town, South Africa; Department of Electrical Engineering, Radar Remote Sensing Group, University of Cape Town, Cape Town, South Africa","2014 IEEE 33rd International Performance Computing and Communications Conference (IPCCC)","","2014","","","1","8","In this paper, we present a domain-specific language, referred to as OptiSDR, that matches high level digital signal processing (DSP) routines for software defined radio (SDR) to their generic parallel executable patterns targeted to heterogeneous computing architectures (HCAs). These HCAs includes a combination of hybrid GPU-CPU and DSP-FPGA architectures that are programmed using different programming paradigms such as C/C++, CUDA, OpenCL, and/or VHDL. OptiSDR presents an intuitive single high-level source code and near specification-level approach for optimization and facilitation of HCAs. OptiSDR uses an optimized embedded domain-specific language (DSL) compiler framework called Delite. Our focus is on the programming language expressiveness for parallel programming and optimization of typical DSP algorithms for deployment on SDR HCAs. We demonstrate the capability of OptiSDR to express the solution to the issues of parallel DSP low-level implementation complexities in the closest way to the original parallel programming of SDR systems. This paper will achieve these by focusing on three generic parallel executable patterns suitable for DSP routines such as cross-correlation, convolution in FIR filter based Hilbert transformers, and fast Fourier transforms for spectral analysis. This paper concludes with a performance analysis using DSP algorithms that tests automatically generated code against hand-crafted solutions.","1097-2641;2374-9628","978-1-4799-7575","10.1109/PCCC.2014.7017083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017083","","DSL;Digital signal processing;Graphics processing units;Finite impulse response filters;Computer architecture;Programming","convolution;fast Fourier transforms;FIR filters;Hilbert transforms;parallel programming;program compilers;programming languages;signal processing;software radio;spectral analysis;telecommunication computing","domain-specific language;software defined radio;OptiSDR;digital signal processing;parallel executable patterns deployment;heterogeneous computing architectures;hybrid GPU-CPU;DSP-FPGA architectures;programming paradigms;high-level source code;DSL compiler framework;Delite;programming language;parallel programming;SDR HCA;DSP routines;FIR filter;Hilbert transforms;fast Fourier transforms;spectral analysis;convolution;cross-correlation;DSP algorithms","","1","27","","","","","","IEEE","IEEE Conferences"
"Specification development of robotic system for pesticide spraying in greenhouse","V. Komasilovs; E. Stalidzans; V. Osadcuks; M. Mednis","Department of Computer Systems, Latvia University of Agriculture, Jelgava, Latvia; Department of Computer Systems, Latvia University of Agriculture, Jelgava, Latvia; Department of Computer Systems, Latvia University of Agriculture, Jelgava, Latvia; Department of Computer Systems, Latvia University of Agriculture, Jelgava, Latvia","2013 IEEE 14th International Symposium on Computational Intelligence and Informatics (CINTI)","","2013","","","453","457","Robotic systems are replacing humans in increasing number of dangerous tasks. Pesticide spraying in greenhouses is one of tasks where a number of robotic systems have been developed and tested. Therefore it is the right time to raise the question about optimisation of the costs efficiency of robot colony. In case of homogenous robotic system only the number of universal robots has to be determined. The drawback is that some subsystems (vision, spraying) might be utilised at a very low rate and the investment may be inefficient. To avoid that it is possible to design a heterogeneous robotic system where the solution space of a robotic system specification is large and optimisation task becomes very complex. Still a good solution can reduce costs. Optimisation of specification of heterogeneous robotic system for pesticide spraying (plant inspection, spraying and pesticide transport functions) in a greenhouse with rectangular layout is performed using genetic algorithms and corresponding open source GAMBot-Eva software. Optimisation results demonstrate three groups of solutions that are within 3% range of lowest possible costs: 1) homogeneous system of universal robots, 2) heterogeneous system with two types of robots (inspection-spraying robots and spraying-transportation robots) and 3) heterogeneous system with two types of robots without duplicating functions (inspection-spraying robots and transportation robots). Applied optimisation approach can be adapted for different robot missions.","","978-1-4799-0197-5978-1-4799-0194","10.1109/CINTI.2013.6705239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6705239","","Spraying;Optimization;Inspection;Green products;Robot sensing systems;Agriculture","cost reduction;genetic algorithms;greenhouses;inspection;mobile robots;pest control;spraying","specification development;pesticide spraying;greenhouse;costs efficiency optimisation;robot colony;homogenous robotic system;universal robots;investment;heterogeneous robotic system;robotic system specification;optimisation task;cost reduction;specification optimisation;plant inspection;pesticide transport functions;rectangular layout;genetic algorithms;open source GAMBot-Eva software;heterogeneous system;inspection-spraying robots;spraying-transportation robots;applied optimisation approach;robot missions","","1","20","","","","","","IEEE","IEEE Conferences"
"Incremental symbolic execution of evolving state machines","A. Khalil; J. Dingel","School of Computing, Queen's University, Kingston, Ontario, Canada; School of Computing, Queen's University, Kingston, Ontario, Canada","2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems (MODELS)","","2015","","","14","23","This paper introduces two complementary techniques, memoization-based and dependency-based incremental symbolic execution, that aim to optimize the analysis of state machine models that undergo change. We implement the two proposed techniques on IBM Rhapsody Statecharts and present some evaluation results.","","978-1-4673-6908-4978-1-4673-6907","10.1109/MODELS.2015.7338231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338231","","Optimization;Cost accounting;Analytical models;Electronic mail;Software;Testing;Engines","finite state machines;optimisation;trees (mathematics)","state machine model optimization;memoization-based incremental symbolic execution;dependency-based incremental symbolic execution;IBM Rhapsody statechart;symbolic execution tree;SET","","1","15","","","","","","IEEE","IEEE Conferences"
"Read-Write Lock Allocation in Software Transactional Memory","A. G. Bavarsad; E. Atoofian","NA; NA","2013 42nd International Conference on Parallel Processing","","2013","","","680","687","Transactional Memory (TM) is a promising programming model for managing concurrent accesses to the shared memory locations. Time-based Software Transactional Memories (STMs) exploit a global clock to maintain consistency of transactions and validate transactional data. One of the shortcomings of this technique is that the global clock becomes bottleneck as the number of transactions increases. In this paper, we introduce two optimization techniques to overcome the overhead of the global clock. The first technique is Read-Write Lock Allocation (RWLA) which does not exploit any central data structure to maintain consistency of transactions. This method improves performance of STMs only if transactions commit successfully. However, in the event of frequent conflicts, RWLA increases cost of abort and degrades performance. Our second optimization technique is an adaptive technique which dynamically selects either baseline scheme or RWLA. Our experimental results reveal that our adaptive technique is effective and is able to improve performance of transactional applications up to 66%.","0190-3918;2332-5690","978-0-7695-5117","10.1109/ICPP.2013.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6687406","Transactional Memory;Validation;Global Clock;Performance","Clocks;Benchmark testing;Vectors;Instruction sets;Synchronization;Genomics;Bioinformatics","concurrency control;data structures;parallel programming;transaction processing","read-write lock allocation;TM programming model;shared memory locations;time-based software transactional memory;transactional data validation;optimization techniques;RWLA;data structure;transaction consistency","","","17","","","","","","IEEE","IEEE Conferences"
"Low-cost sensor array design optimization based on planar electromagnetic sensor design for detecting nitrate and sulphate","A. S. M. Nor; M. A. M. Yunus; S. W. Nawawi; S. Ibrahim","Control and Mechatronics Engineering Department (CMED), Universiti Teknologi Malaysia, Skudai, Malaysia; Infocomm Research Alliance (RA - Infocomm), Universiti Teknologi Malaysia, Skudai, Malaysia; Infocomm Research Alliance (RA - Infocomm), Universiti Teknologi Malaysia, Skudai, Malaysia; Infocomm Research Alliance (RA - Infocomm), Universiti Teknologi Malaysia, Skudai, Malaysia","2013 Seventh International Conference on Sensing Technology (ICST)","","2013","","","693","698","In recent years, the tremendous development of agriculture sector has been driven by the growing human population. Therefore, the crop productions need to be increased to fulfill the food demands. In order to do it, the amount of fertilizer used, might have been more than required and unabsorbed fertilizer by plants usually washed into water sources like rivers, ponds, and wells. Hence, the water will be polluted and unsafe to consume. Therefore, it is important to determine the contamination level in natural water resources. This paper discusses the development of a low-cost sensor array based on planar electromagnetic sensors to determine the contamination levels of two common fertilizer components which are nitrate and sulphate in water sources. Three types of sensor array, were suggested: parallel, star, and delta. The modeling and simulation of all type of sensor array were carried out using COMSOL Multiphysics 4.2 software to calculate the sensors' impedance value. The contamination state was simulated by altering the electrical properties values of the environment domain of the model to represent water contamination. The simulations results show that all types of sensor array are sensitive to conductivity, σ and permittivity, ε. Furthermore, a set of experiments was conducted to determine the relationship between the sensor's impedance and the waters' nitrate and sulphate contamination. The performance of the system was observed where the sensors were tested with the additional of distilled water with different concentrations of amount of potassium nitrate and potassium sulphate. The sensitivity of the developed sensors was evaluated where the best sensor was selected. Based on the outcomes of the experiments, the star sensor array placement has the highest sensitivity and can be used to measure the water content in the water.","2156-8073;2156-8065","978-1-4673-5222-2978-1-4673-5220","10.1109/ICSensT.2013.6727742","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727742","Planar electromagnetic sensor;sensor array;impedance;nitrate contamination;sulphate contamination","Arrays;Impedance;Water pollution;Electromagnetics;Contamination;Sensitivity;Mathematical model","contamination;electrochemical sensors;electromagnetic devices;fertilisers;magnetic sensors;optimisation;sensor arrays;water pollution","potassium sulphate;potassium nitrate;water content measurement;distilled water;electrical property;COMSOL Multiphysics 4.2 software;delta. sensor array;star sensor array;parallel sensor array;fertilizer component;natural water resource;contamination level;water pollution;crop production;planar electromagnetic sensor design;low-cost sensor array design optimization;K2SO4;KNO3","","4","21","","","","","","IEEE","IEEE Conferences"
"Performance evaluation and improvement in cloud computing environment","O. Khedher; M. Jarraya","Innov'com Laboratory, E.N.I.T, Tunisia; Saudi Electronic University, Saudi Arabia","2015 International Conference on High Performance Computing & Simulation (HPCS)","","2015","","","650","652","Cloud computing covers a wide range of applications, from online services for the end user. It becomes the new trends for most organizations to handle their business IT units. Services provided are becoming flexible because the resources and processing power available to each can be adjusted on the fly to meet changes in need [6]. However, infrastructures deployed on a cloud computing environment may induce significant performance penalties for the demanding computing workload. In our doctoral research, we aim to study, analyze, evaluate and improve performance in cloud computing environment based on different criteria. To achieve the thesis objectives, a research performed is based on a quantitative analysis of repeatable empirical experiment.","","978-1-4673-7813-0978-1-4673-7812-3978-1-4673-7811","10.1109/HPCSim.2015.7237109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237109","cloud computing;high performance;scheduling;optimization","Cloud computing;Time factors;Benchmark testing;Processor scheduling;Virtual machining;Measurement","cloud computing;software performance evaluation","performance evaluation;cloud computing environment;online services;business IT units;performance improvement","","1","7","","","","","","IEEE","IEEE Conferences"
"A fast intra mode decision method based on reduction of the number of modes in HEVC standard","M. R. Fini; F. ZargariAsl","Department of Computer Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Information Technology Research Institute Iran Telecom Research Center (ITRC) Tehran, Iran","7'th International Symposium on Telecommunications (IST'2014)","","2014","","","839","843","High efficiency video coding (HEVC) standard, introduced by joint collaborative team on video coding (JCT-VC) is the newest international standard of video compression. This standard provides more compression and better video quality, compared to the previous standards such as H.264. The higher compression efficiency is obtained at the cost of an increase in the computational load. One of the portions which imposes high computational load to encoder is the intra prediction unit. In HEVC, 35 modes are proposed for intra prediction to improve the compression efficiency. To reduce the computational load of intra prediction, HEVC uses a pre-processing step, called Rough Mode Decision (RMD). It selects a number of best prediction modes. Then, using Rate Distortion Optimization (RDO) process, the encoder selects the best prediction mode. We reduced the number of the tested modes in RMD from 35 to 19 in our proposed method. Moreover, we reduced the number of the selected modes through the RMD step in order to decrease the encoding time. Simulation results indicate 14.2% reduction in the encoding time whereas the R-D specification in general remains unchanged.","","978-1-4799-5359-2978-1-4799-5358","10.1109/ISTEL.2014.7000820","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000820","video coding;HEVC;intra coding;mode decision","Encoding;Prediction algorithms;Video coding;Standards;Software;Algorithm design and analysis;Computational complexity","data compression;optimisation;video coding","fast intramode decision method;HEVC standard;high efficiency video coding standard;joint collaborative team on video coding;JCT-VC;international standard;video compression;video quality;H.264 standard;compression efficiency;computational load;intraprediction unit;preprocessing step;rough mode decision;RMD step;rate distortion optimization process;RDO process;encoding time;R-D specification","","5","16","","","","","","IEEE","IEEE Conferences"
"SentiMeter-Br: A Social Web Analysis Tool to Discover Consumers' Sentiment","R. L. Rosa; D. Z. Rodriguez; G. Bressan","NA; NA; NA","2013 IEEE 14th International Conference on Mobile Data Management","","2013","2","","122","124","This article analyzes Brazilian Consumers' Sentiments in a specific domain using a system, SentiMeter-Br. A Portuguese dictionary focused in a specific field of study was built, in which tenses and negative words are treated in a different way to measure the polarity, the strength of positive or negative sentiment, in short texts extracted from Twitter. For the Portuguese Dictionary performance validation, the results are compared with the SentiStrength tool and are evaluated by three Specialists in the field of study; each one analyzed 2000 texts captured from Twitter. Comparing the efficiency of the SentiMeter-Br and the SentiStrength against the Specialists' opinion, a Pearson correlation factor of 0.89 and 0.75 was reached, respectively, proving that the metric used in the Sentimeter-Br is better than the one used in the SentiStrength. The polarity of the short texts were also tested through machine learning, with correctly classified instances of 71.79% by Sequential Minimal Optimization algorithm and F-Measure of 0.87 for positive and 0.91 for negative phrases. Another contribution is a Twitter and Facebook search framework that extracts online tweets and Facebook posts, the latter with geographic location, gender and birth date of the user who posted the comments, and can be accessed by mobile phones.","1551-6245;2375-0324","978-0-7695-4973-6978-1-4673-6068","10.1109/MDM.2013.80","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569076","consumer sentiment;Twitter;Facebook;machine learning;social web analysis tool;support vector machines","Dictionaries;Twitter;Facebook;Software;Decision trees;Hair;Classification algorithms","consumer behaviour;data mining;dictionaries;information retrieval;learning (artificial intelligence);natural language processing;optimisation;performance evaluation;social networking (online);text analysis","SentiMeter-Br;social Web analysis tool;consumer sentiment discovery;Brazilian consumer sentiment analysis;negative words;tenses;negative sentiment;positive sentiment;short texts;Twitter;Portuguese dictionary performance validation;Pearson correlation factor;specialist opinion;machine learning;sequential minimal optimization algorithm;F-Measure;negative phrases;positive phrases;Facebook search framework;Facebook posts;online tweet extraction;geographic location;gender;birth date;mobile phones","","5","3","","","","","","IEEE","IEEE Conferences"
"Chaotic self adaptive particle swarm approach for solving economic dispatch problem with valve-point effect","C. Rani; D. P. Kothari; K. Busawon","Vellore Institute of Technology, VIT University, Vellore, India; J B Group of Institutions, Hyderabad, India; Northumbria University, Newcastle Upon Tyne, UK","2013 International Conference on Power, Energy and Control (ICPEC)","","2013","","","405","410","This research work presents a Chaotic Self Adaptive Particle Swarm Optimization (CSAPSO) algorithm in order to solve the Economic Dispatch (ED) problem. The main purpose of the work is to derive a simple and effective method for optimum generation dispatch to minimize the generation cost power networks by considering several non-linear characteristics of the generator such as valve point effect, prohibited operating zones and ramp rate limits. A chaotic local search operator is introduced in the proposed algorithm to avoid premature convergence. Simulation studies are carried out, using MATLAB software, to show the effectiveness of the proposed optimization method. The applicability and high feasibility of the proposed method is validated on three different test systems. Results show that the CSAPSO is more powerful than other algorithms.","","978-1-4673-6030-2978-1-4673-6027-2978-1-4673-6029","10.1109/ICPEC.2013.6527690","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6527690","Economic Dispatch (ED);Chaotic Self Adaptive Particle Swarm Optimization (CSAPSO);ramp rate limits;valve point effect","Generators;Equations;Sociology;Statistics;Fuels;Heuristic algorithms;Optimization","chaos;particle swarm optimisation;power generation dispatch;power system economics;search problems","chaotic self adaptive particle swarm algorithm;economic dispatch problem;valve-point effect;CSAPSO algorithm;optimum generation dispatch;generation cost power network;nonlinear characteristics;chaotic local search operator;MATLAB software","","","22","","","","","","IEEE","IEEE Conferences"
"Modelling of boiler fireside control in flownex software environment","A. Kellerman; I. A. Gorlach","Department of Mechatronic Engineering, Nelson Mandela Metropolitan University, Port Elizabeth, South Africa; Department of Mechatronic Engineering, Nelson Mandela Metropolitan University, Port Elizabeth, South Africa","2015 Pattern Recognition Association of South Africa and Robotics and Mechatronics International Conference (PRASA-RobMech)","","2015","","","1","6","With the influx of renewable generation technology and the constraints placed on the grid due to unplanned maintenance on aging power stations, coal fired stations designed for continuous base load production have to fulfil the role of frequency support. With the high inertia of the coal fired power stations, the dynamic operating philosophy of frequency support will impact negatively on the overall efficiency and life expectancy of these plants. The solution proposed is to develop a computational model of the boiler controllers capable of optimising boiler fireside control loops and control philosophies. The model will enable engineers to test and optimize various control parameters. The model will also be a safe environment for testing transient scenarios, such as the loss of a draught group or a multiple mill trip.","","978-1-4673-7450-7978-1-4673-7449","10.1109/RoboMech.2015.7359489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359489","Flownex;Boiler Fireside Control;Engineering Simulator;Boiler Control Modelling","Boilers;Coal;Combustion;Computational modeling;Furnaces;Process control;Load modeling","boilers;control engineering computing;power generation control;renewable energy sources;steam power stations","flownex software environment;boiler fireside control modelling;renewable generation technology;boiler fireside control loop;coal fired power stations;continuous-base load production;aging power station","","","9","","","","","","IEEE","IEEE Conferences"
"Synergistic use of multiple on-chip networks for ultra-low latency and scalable distributed routing reconfiguration","M. Balboni; J. Flich; D. Bertozzi","ENDIF - MPSoC Research Group, University of Ferrara, Italy; GAP - Parallel Architectures Group, Universidad Politécnica de Valencia, Spain; ENDIF - MPSoC Research Group, University of Ferrara, Italy","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","806","811","Extending the principle of partially good die allowance to manycore processors, and testing them over time to detect the onset of permanent faults, are only feasible through proper support in the on-chip interconnection network. In fact, this implies the ability to reconfigure the routing algorithm at runtime to reflect changes in network topologies. Current literature cannot avoid a large hardware and/or software overhead when tackling this challenge. This paper exploits the existence of multiple physical networks in industry-relevant manycore processors in a synergistic way, for the sake of fast and scalable distributed reconfiguration of the routing function at runtime.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0392","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092496","","Switches;Routing;Ports (Computers);System recovery;Runtime;Testing;Optimization","network routing;network-on-chip","synergistic use;multiple on-chip networks;ultra-low latency;scalable distributed routing reconfiguration;die allowance;on-chip interconnection network;hardware overhead;software overhead;multiple physical networks;industry-relevant manycore processors;network-on-chip;NoC;permanent faults","","3","26","","","","","","IEEE","IEEE Conferences"
"Lemma localization: A practical method for downsizing SMT-interpolants","F. Pigorsch; C. Scholl","University of Freiburg, Department of Computer Science, 79110 Breisgau, Germany; University of Freiburg, Department of Computer Science, 79110 Breisgau, Germany","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","1405","1410","Craig interpolation has become a powerful and universal tool in the formal verification domain, where it is used not only for Boolean systems, but also for timed systems, hybrid systems, and software programs. The latter systems demand interpolation for fragments of first-order logic. When it comes to model checking, the structural compactness of interpolants is necessary for efficient algorithms. In this paper, we present a method to reduce the size of interpolants derived from proofs of unsatisfiability produced by SMT (Satisfiability Modulo Theory) solvers. Our novel method uses structural arguments to modify the proof in a way, that the resulting interpolant is guaranteed to have smaller size. To show the effectiveness of our approach, we apply it to an extensive set of formulas from symbolic hybrid model checking.","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513733","","Interpolation;Benchmark testing;Model checking;Partitioning algorithms;Buildings;Software;Optimization","","","","1","28","","","","","","IEEE","IEEE Conferences"
"A coevolving memetic algorithm for simultaneous partitional clustering and feature weighting","Y. Sun; Z. Zhu; S. He; Z. Ji","Department of Biomedical Engineering, School of Medicine, Shenzhen University, China 518060; Shenzhen City Key Laboratory of Embedded System Design, College of Computer Science and Software Engineering, Shenzhen University, China 518060; School of Computer Science, University of Birmingham, B15 2TT, UK; Shenzhen City Key Laboratory of Embedded System Design, College of Computer Science and Software Engineering, Shenzhen University, China 518060","2013 IEEE Workshop on Memetic Computing (MC)","","2013","","","9","15","This paper proposes a coevolving Memetic clustering algorithm namely CoMCA for simultaneous partitional clustering and feature weighting. Particularly, CoMCA uses a coevolving particle swarm optimization (PSO) with two swarms for the global search of optimal combination of cluster centroids and feature weights. In each iteration of PSO, a local search based on K-means and gradient descent is introduced to fine-tune the best solution. Comparison study of CoMCA to K-means, PSO clustering, Fuzzy C-means, and WK-Means on test data demonstrates that CoMCA is robust in highlighting relevant features and attaining better (or competitive) performance than the other counterpart algorithms in terms of inter-cluster variance and Rand Index.","","978-1-4673-5891","10.1109/MC.2013.6608201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608201","","Iris","gradient methods;particle swarm optimisation;pattern clustering;search problems","coevolving memetic clustering algorithm;simultaneous partitional clustering;feature weighting;CoMCA;coevolving particle swarm optimization;global search;cluster centroids;gradient descent;PSO clustering;fuzzy C-means;WK-means;inter-cluster variance;Rand Index","","","29","","","","","","IEEE","IEEE Conferences"
"The berth and quay cranes integrated scheduling based on redundancy policy","X. Zhang; B. Sun; J. Sun; Z. Gou","Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, 300384, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, 300384, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, 300384, China; Tianjin Key Laboratory of Intelligence Computing and Novel Software Technology, Tianjin University of Technology, 300384, China","Proceedings of the 33rd Chinese Control Conference","","2014","","","7595","7600","The arrival time of each container vessel berthing at a container terminal and the numbers of the containers loaded on and off the vessel are disturbed stochastically since their being affected by many random factors. In order to make the plans of berth allocation and quay cranes scheduling implemented successfully, this paper proposes a mathematical model of the robustly planning for berth allocation and quay cranes integrated scheduling. Based on the trade-off between service level and robustness of the plans, the mathematical model optimizes the weighted sum of the service level and the robustness indicator by making decision on berthing position on the terminal, berthing time and the buffer time for each vessel. Through analyzing the properties of the model it is found that there must be a “lower-left tight” plan which is optimal. Then an improved genetic algorithm is designed to solve the problem according to the properties. The simulation tests show that the optimal solution or the approximately optimal solution of the model can absorb the perturbation, avoid the interim adjustment plan and reduce the delay time of each vessel. Especially, it remarkably reduces the number of the vessels with more containers loaded on and off them and enhances the robustness of the terminal operations plan.","1934-1768","978-9-8815-6387","10.1109/ChiCC.2014.6896265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6896265","Container terminal;Robust planning;Berth allocation;Quay cranes scheduling;Genetic algorithm","Cranes;Planning;Containers;Robustness;Delays;Uncertainty","containers;cranes;decision making;genetic algorithms;redundancy;scheduling;sea ports;ships;stability;transportation","perturbation;interim adjustment plan;vessel delay time reduction;terminal operations plan;optimal solution;genetic algorithm;lower-left tight plan;vessel buffer time;berthing time;berthing position;decision making;robustness indicator;weighted sum optimization;plan robustness;service level;robust planning;mathematical model;quay crane scheduling;berth allocation;random factors;stochastic disturbance;container loading;container off-loading;container terminal;container vessel berthing;container vessel arrival time;redundancy policy;berth scheduling","","","12","","","","","","IEEE","IEEE Conferences"
"Developing a pieces of data allocation method in distributed databases using Bayesian networks","S. Pourhaji; M. H. Moattar","Department of Software Engineering, Neyshabur Branch, Islamic Azad University, Neyshabur, Iran; Department of Software Engineering, Mashhad Branch, Islamic Azad University, Mashhad, Iran","2015 International Congress on Technology, Communication and Knowledge (ICTCK)","","2015","","","117","122","Determining the location of the storage of data on different nodes of a distributed system is called allocation of pieces of data and is considered as the most important factor influencing the cost of the implementation of distributed database. Allocation of piece of data is a NP-hard problem that needs heuristic solutions. The main purpose of the process is to allocate parts with the lowest cost to the node that has the most access to the data segment. In this article, in terms of a structural oriented learning method, using analysis of the factors influencing the process of allocating a piece of data to a node, (i.e. distance from apiece of data, cost, node loading, availability of data, etc), the dependence relations between adjacent nodes to target nodes was determined and the structure of Bayesian network was obtained. In the second stage, the obtained Bayesian network is trained using gradient method and tested using the data collected from the data collection x. The proposed model efficiency is compared with the results of Huang and Chen's method which have used a heuristic approach using the imperialist competitive algorithm. The results indicate that this model is an efficient tool in the optimization process of allocating Pieces of Data in the distributed systems.","","978-1-4673-9762-9978-1-4673-9763","10.1109/ICTCK.2015.7582656","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582656","Distributed Databases;Allocation of Pieces of Data;Bayesian Networks;Structured Learning","Bayes methods;Distributed databases;Resource management;Heuristic algorithms;Bandwidth;Maximum likelihood estimation;Data collection","belief networks;distributed databases;gradient methods;learning (artificial intelligence);optimisation;storage allocation","data allocation method;distributed databases;Bayesian networks;data storage;distributed system;NP-hard problem;data segment access;structural oriented learning method;data availability;gradient method;data collection;optimization","","1","11","","","","","","IEEE","IEEE Conferences"
"Energy-based fair queuing scheduling implementation for battery-limited mobile systems","J. Wei; R. Ren; E. Juárez; F. Pescador","Centra de Investigación en Tecnologías del Software y Sistemas Multimedia para la Sostenibilidad (CITSEM) Universidad Politécnica de Madrid, Spain; Centra de Investigación en Tecnologías del Software y Sistemas Multimedia para la Sostenibilidad (CITSEM) Universidad Politécnica de Madrid, Spain; Centra de Investigación en Tecnologías del Software y Sistemas Multimedia para la Sostenibilidad (CITSEM) Universidad Politécnica de Madrid, Spain; Centra de Investigación en Tecnologías del Software y Sistemas Multimedia para la Sostenibilidad (CITSEM) Universidad Politécnica de Madrid, Spain","Design of Circuits and Integrated Systems","","2014","","","1","6","Energy-based fair queuing (EFQ) is an implementation of the classical fair queuing scheduling algorithm in the energy domain. It is an energy-centric scheduling algorithm that manages energy as the first-class resource that is globally shared by different devices in the system. Recent high-level simulation and Linux-based implementation and experiment have proven that EFQ can achieve proportional power sharing by accounting the energy consumption on both CPU and I/O operations and scheduling tasks based on their normalized energy consumptions. It has also been demonstrated that EFQ can achieve effective and flexible time-constraint compliance. In this paper, the core Linux implementation of EFQ is introduced in detail, and the experimental platform as well as the benchmarks are presented for the experiments. Experimental results on EFQ scheduling show that it is more effective and flexible than the default Linux scheduler in optimizing the user experience of battery-limited systems<sup>1</sup>.","","978-1-4799-5743","10.1109/DCIS.2014.7035594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7035594","energy-centric scheduling;Linux scheduler;battery-limited mobile devices","Benchmark testing;Linux;Resource management;Instruction sets;Time factors;Scheduling;Kernel","Linux;mobile computing;mobile radio;queueing theory;scheduling;telecommunication computing;telecommunication power management;telecommunication power supplies","energy-based fair queuing scheduling implementation;battery-limited mobile systems;EFQ;energy domain;energy-centric scheduling algorithm;energy management;high-level simulation;Linux-based implementation;proportional power sharing;energy consumption;CPU operation;I-O operation;task scheduling;flexible time-constraint compliance;Linux scheduler","","","5","","","","","","IEEE","IEEE Conferences"
"An Accurate Instruction-Level Energy Estimation Model and Tool for Embedded Systems","M. Bazzaz; M. Salehi; A. Ejlali","Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","IEEE Transactions on Instrumentation and Measurement","","2013","62","7","1927","1934","Estimating the energy consumption of applications is a key aspect in optimizing embedded systems energy consumption. This paper proposes a simple yet accurate instruction-level energy estimation model for embedded systems. As a case study, the model parameters were determined for a commonly used ARM7TDMI-based microcontroller. The total energy includes the energy consumption of the processor core, Flash memory, memory controller, and SRAM. The model parameters are instructions opcode, number of shift operations, register bank bit flips, instructions weight and their Hamming distance, and different types of memory accesses. Also, the effect of pipeline stalls have been considered. In order to validate the proposed model, a physical hardware platform equipped with energy measurement capabilities was developed. We have conducted experiments on several embedded applications from MiBench benchmark suite and the results show less than 6% error in the energy consumption estimation. We have also developed an energy profiler tool for the systems that use ARM7TDMI processors by embedding the model parameters in an instruction-level profiler from the SimpleScalar toolset which provides valuable information and guidelines for software energy optimization.","0018-9456;1557-9662","","10.1109/TIM.2013.2248288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6478810","Embedded software;energy estimation;energy measurement;memory;regression analysis","Energy consumption;Benchmark testing;Random access memory;Estimation;Flash memory;Registers;Memory management","electronic engineering computing;embedded systems;energy consumption;flash memories;microcontrollers;SRAM chips","software energy optimization;SimpleScalar toolset;instruction-level profiler;MiBench benchmark suite;energy measurement capabilities;physical hardware platform;memory accesses;Hamming distance;instruction weight;register bank bit flips;shift operations;instructions opcode;SRAM energy consumption;memory controller energy consumption;flash memory energy consumption;processor core energy consumption;ARM7TDMI-based microcontroller;embedded systems energy consumption optimization;energy consumption estimation;embedded systems;accurate instruction-level energy estimation model","","25","18","","","","","","IEEE","IEEE Journals & Magazines"
"Bayesian optimal design of step stress accelerated degradation testing","X. Li; M. Rezvanizaniani; Z. Ge; M. Abuali; J. Lee","Science and Technology on Reliability and Environmental Engineering Laboratory, Beihang University, Beijing 100191, China; NSF Industry/University Cooperative Research Center on Intelligent Maintenance Systems, University of Cincinnati, OH 45221, USA; Beijing Institute of Electronic System Engineering, Beijng 100854, China; NSF Industry/University Cooperative Research Center on Intelligent Maintenance Systems, University of Cincinnati, OH 45221, USA; NSF Industry/University Cooperative Research Center on Intelligent Maintenance Systems, University of Cincinnati, OH 45221, USA","Journal of Systems Engineering and Electronics","","2015","26","3","502","513","This study presents a Bayesian methodology for designing step stress accelerated degradation testing (SSADT) and its application to batteries. First, the simulation-based Bayesian design framework for SSADT is presented. Then, by considering historical data, specific optimal objectives oriented Kullback-Leibler (KL) divergence is established. A numerical example is discussed to illustrate the design approach. It is assumed that the degradation model (or process) follows a drift Brownian motion; the acceleration model follows Arrhenius equation; and the corresponding parameters follow normal and Gamma prior distributions. Using the Markov Chain Monte Carlo (MCMC) method and WinBUGS software, the comparison shows that KL divergence is better than quadratic loss for optimal criteria. Further, the effect of simulation outliers on the optimization plan is analyzed and the preferred surface fitting algorithm is chosen. At the end of the paper, a NASA lithium-ion battery dataset is used as historical information and the KL divergence oriented Bayesian design is compared with maximum likelihood theory oriented locally optimal design. The results show that the proposed method can provide a much better testing plan for this engineering application.","1004-4132","","10.1109/JSEE.2015.00058","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170009","accelerated testing;Bayesian theory;KL divergence;degradation;optimal design;battery","Stress;Bayes methods;Testing;Degradation;Life estimation;Modeling;Mathematical model","","","","3","","","","","","","BIAI","BIAI Journals & Magazines"
"Design and investigation of dielectric resonator antenna oscillator (DRAO) in electronic toll collection (ETC) system","J. Sun; Q. Hao; Z. Feng; W. Jiang","Science and Technology on Electronic Test &amp; Measurement Laboratory, The 41st Institute of China Electronics Technology Group Corporation, Qingdao, Shandong, 266555, P. R. China; Department of Electronic Engineering, Tsinghua University, Beijing, 100084, P. R. China; Department of Electronic Engineering, Tsinghua University, Beijing, 100084, P. R. China; Science and Technology on Electronic Test &amp; Measurement Laboratory, The 41st Institute of China Electronics Technology Group Corporation, Qingdao, Shandong, 266555, P. R. China","2013 IEEE International Wireless Symposium (IWS)","","2013","","","1","3","A novel dielectric resonator antenna oscillator (DRAO), which realizes dielectric resonator antenna (DRA) and dielectric resonator oscillator (DRO) simultaneously, is presented in this paper. The design procedure of the parallel-feedback type DRAO at dedicated short range communications band is discussed. Firstly, the DRA was three-dimensional simulated by HFSS. Then the DRO was designed based on the negative resistance theory, using the ADS software for optimization and nonlinear analysis. Finally, it was integrated in the on-board unit (OBU) of electronic toll collection (ETC) system. The experimental results of the OBU show that the performance of the DROA is well and it can be used in real ETC system.","","978-1-4673-2141","10.1109/IEEE-IWS.2013.6616747","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616747","Dielectric resonator antenna(DRA);dielectric resonator oscillator (DRO);negative resistance","Dielectrics;Dielectric resonator antennas;Oscillators;Antenna measurements;Microwave filters;Microwave antennas;Microwave circuits","dielectric resonator antennas;dielectric resonator oscillators;road pricing (tolls)","dielectric resonator antenna oscillator;electronic toll collection system;parallel-feedback type;dedicated short range communications band;negative resistance theory;ADS software;nonlinear analysis;on-board unit","","1","10","","","","","","IEEE","IEEE Conferences"
"A low cost acceleration method for hardware trojan detection based on fan-out cone analysis","Bin Zhou; Wei Zhang; S. Thambipillai; J. K. J. Teo","School of Computer Engineering, Nanyang Technological University, Singapore; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, China; School of Computer Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore","2014 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2014","","","1","10","Fabless semiconductor industry and government agencies have raised serious concerns about tampering with inserting hardware Trojans in an integrated circuit supply chain in recent years. In this paper, a low hardware overhead acceleration method of the detection of HTs based on the insertion of 2-to-1 MUXs as test points is proposed. In the proposed method, the fact that one logical gate has a significant impact on the transition probability of the logical gates in its logical fan-out cone is utilized to optimize the number of the insertion MUXs. The nets which have smaller transition probability than the threshold value set by the user and minimal logical depth from the primary inputs are selected as the candidate nets. As for each candidate net, only its input net with smallest signal probability is required to be inserted the MUXs based test points until the minimal transition probability of the entire circuit is no smaller than the threshold value. Experiment results on ISCAS'89 benchmark circuits show that our proposed method can achieve remarkable improvement of transition probability with small overhead penalty.","","978-1-4503-3051","10.1145/2656075.2656077","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6971844","Hardware Trojan;Fan-out cone;Low cost;Signal probability;Transition probability","Logic gates;Trojan horses;Delays;Hardware;Vectors;Integrated circuits;Flip-flops","integrated circuit testing;integrated logic circuits;invasive software;logic gates","low cost acceleration method;hardware Trojan detection;fan-out cone analysis;MUX insertion;transition probability;logical gates","","4","16","","","","","","IEEE","IEEE Conferences"
"Inputs of aspect oriented programming for the profiling of C++ parallel applications on manycore platforms","P. Schweitzer; C. Mazel; D. R. C. Hill; C. Cârloganu","LIMOS UMR CNRS 6158 - Université Blaise Pascal, 24 avenue des Landais, 63173 Aubière cedex - France; LIMOS UMR CNRS 6158 - Université Blaise Pascal, 24 avenue des Landais, 63173 Aubière cedex - France; LIMOS UMR CNRS 6158 - Université Blaise Pascal, 24 avenue des Landais, 63173 Aubière cedex - France; LPC, Université Blaise Pascal, CNRS/IN2P3 BP 10448 63000 Clermont-Ferrand - France","2014 International Conference on High Performance Computing & Simulation (HPCS)","","2014","","","793","802","High Performance Computing systems expect applications to leverage the most of their processing power. This need is even more present for applications such as Monte Carlo simulations that require noteworthy CPU time and memory footprint. Optimizing applications is one approach to reduce the consumption of these resources. Before optimizing, it is mandatory to profile the application in order to pinpoint bottlenecks and hot spots. In this paper, we propose an approach to applications profiling based on Aspect Oriented Programming (AOP). We introduce a profiling approach for C++ codes with the pthread library based on free open source software with low overhead, multicore awareness, multi-threading handling, ease of use and quality outputs compared to established profilers. We will present how our prototype, based on an AOP approach proved to be useful and efficient on a test application.","","978-1-4799-5313-4978-1-4799-5312-7978-1-4799-5311","10.1109/HPCSim.2014.6903769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903769","profiling;callgrind;parallel programming;manycores;Aspect Oriented Programming;C++","Radiation detectors;Context;Multicore processing;Programming;Kernel;Libraries","aspect-oriented programming;C++ language;Monte Carlo methods;multiprocessing systems;multi-threading;parallel processing;public domain software;software libraries","C++ parallel applications;manycore platforms;high performance computing systems;processing power;Monte Carlo simulations;CPU time;memory footprint;optimizing applications;consumption reduction;applications profiling;aspect oriented programming;AOP;profiling approach;C++ codes;pthread library;free open source software;multicore awareness;multithreading handling;quality outputs","","","22","","","","","","IEEE","IEEE Conferences"
"A call for cross-layer and cross-domain reliability analysis and management","D. Alexandrescu; A. Evans; E. Costenaro; M. Glorieux","iRoC Technologies, France; iRoC Technologies, France; iRoC Technologies, France; iRoC Technologies, France","2015 IEEE 21st International On-Line Testing Symposium (IOLTS)","","2015","","","19","22","For many applications, reliability, availability and trustability are key factors, requiring careful design to meet the end users expectations. The complex ASICs, which are now ubiquitous, often embed tens of millions of flip-flops, hundreds of megabits of embedded SRAM, and hundreds of millions of combinatorial cells. These designs integrate IP from multiple providers and are implemented in advanced process technologies, making it challenging to evaluate their reliability. Initiatives such as RIIF (Reliability Information Interchange Format) allow the formalization, specification and modeling of extra-functional, reliability properties for technology, circuits and systems. Continuing these efforts, we propose RAFT (Reliability Architect Framework and Toolset) - a reliability-centric framework including reliability data and models, methodologies and tools allowing system reliability exploration and optimization using mathematical models and high-level tools. The proposed approach can be combined with performance management methodologies aiming at reducing the engineering effort devoted to reliability analysis and improvement.","1942-9398;1942-9401","978-1-4673-7905-2978-1-4673-7904","10.1109/IOLTS.2015.7229821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7229821","Cross-layer optimization;reliability analysis;RIIF","Reliability engineering;Databases;Integrated circuit reliability;Software reliability;Data models;Application specific integrated circuits","fault tolerant computing","cross-layer cross-domain reliability analysis;cross-layer cross-domain reliability management;ASIC;application-specific integrated circuits;embedded SRAM;static random access memory;RIIF;reliability information interchange format;RAFT;reliability architect framework and toolset;reliability-centric framework;reliability data;reliability models;reliability exploration;reliability optimization;performance management methodologies;reliability improvement","","","5","","","","","","IEEE","IEEE Conferences"
"Optimized Multipactor-Resistant Wedge-Shaped Waveguide Bandpass Filters","J. H. González; D. R. García-Baquero; C. Ernst; D. Schmitt; V. E. B. Esbert; B. G. Martínez; M. T. Calduch; C. V. Quiles","European Patent Office, Rijswijk ZH, The Netherlands; European Space Agency, Noordwijk, The Netherlands; European Space Agency, Noordwijk, The Netherlands; European Space Agency, Noordwijk, The Netherlands; Departamento de Comunica—ciones—iTEAM, Universidad Politécnica de Valencia, Valencia, Spain; Departamento de Física Aplicada—ICMUV, Universidad de Valencia, Valencia, Spain; Departamento de Comunica—ciones—iTEAM, Universidad Politécnica de Valencia, Valencia, Spain; Aurora Software and Testing S.L., Universidad Politécnica de Valencia, Valencia, Spain","IEEE Transactions on Plasma Science","","2013","41","8","2135","2144","Wedge-shaped waveguides present a certain advantage with respect to rectangular waveguides regarding their resistance to multipactor discharges. In this paper, the optimal configuration for the wedge geometry is investigated based on theoretical results, on a precise multipactor prediction tool, and on previous experience. In addition, design rules are presented, which allow us to achieve for wedge-shaped filters electrical performances comparable to the ones of rectangular waveguide filters, while at the same time improving the multipactor-free power range. As a proof of concept, two three-pole bandpass filters with equivalent electrical characteristic of 150-MHz bandwidth, centered at 12 GHz (Ku band), and the same<formula formulatype=""inline""><tex Notation=""TeX"">$Q$</tex></formula>factor have been designed, manufactured, and tested. The first design is based on conventional rectangular waveguide technology, while the second one has non-parallel broadside walls (wedge-shaped cross section). The multipactor power threshold and RF performance of the filters have been measured in order to validate the improvements achievable employing wedge-shaped resonators.","0093-3813;1939-9375","","10.1109/TPS.2013.2253134","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6576256","Bandpass filters;circuit synthesis;microwave filters;multipactor (MP);space technology;vacuum breakdown","Rectangular waveguides;Cutoff frequency;Microwave filters;Discharges (electric);Circuit synthesis;Band-pass filters;Space technology","","","","2","20","","","","","","IEEE","IEEE Journals & Magazines"
"An algorithm to calculate optimized coordinated trajectories of intelligent vehicles in a segment of road for any lane configuration","R. Reghelin; L. V. R. de Arruda","Electrical Engineering Department, CPGEI, Universidade Tecnol&#x00F3;gica Federal do Paran&#x00E1; (UTFPR), Av. Sete de Setembro, 3165, Curitiba, PR, Brazil; Electrical Engineering Department, CPGEI, UTFPR","2013 IEEE Intelligent Vehicles Symposium (IV)","","2013","","","250","255","This paper presents a simulation algorithm to calculate optimized coordinated trajectories of intelligent vehicles in a road. The objective is to reduce travel time for each vehicle. The configuration of the road can be modelled regarding the number of lanes, direction preference and exclusivity. The calculation considers the main elements of the traffic system, such as topography of the lane, traffic rules and individual capacity of acceleration. It can deals with most traffic situations such as overtaking, obstacles, slopes, and speed reducers. An extension of the algorithm is also proposed. It improves results by providing ideal space for acceleration within a platoon, maintaining the order of priority of the vehicles in a queue. Experimental tests to evaluate the algorithm are presented.","1931-0587","978-1-4673-2755-8978-1-4673-2754","10.1109/IVS.2013.6629478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6629478","","Vehicles;Roads;Acceleration;Computer aided software engineering;Trajectory;Optimization;Mathematical model","automated highways;automobiles;road traffic;traffic engineering computing","speed reducers;slopes;obstacles;overtaking;individual acceleration capacity;traffic rules;lane topography;traffic system;road exclusivity;road direction preference;road lane number;road segment;intelligent vehicles;optimized coordinated trajectories calculation","","","12","","","","","","IEEE","IEEE Conferences"
"HOMER: A valuable tool to facilitate the financing process of photovoltaic systems in Puerto Rico","A. Pérez-Santiago; R. Ortiz-Dejesus; E. I. Ortiz-Rivera","University of Puerto Rico, Mayagüez Campus, Mayaguez, Puerto Rico 00682, Electrical and Computer Engineering Department; University of Puerto Rico, Mayagüez Campus, Mayaguez, Puerto Rico 00682, Electrical and Computer Engineering Department; University of Puerto Rico, Mayagüez Campus, Mayaguez, Puerto Rico 00682, Electrical and Computer Engineering Department","2014 IEEE 40th Photovoltaic Specialist Conference (PVSC)","","2014","","","1467","1470","This paper is about how the HOMER software facilitates the analysis of the financing process and the viability of photovoltaic systems in Puerto Rico (PR). Photovoltaic (PV) systems are becoming a very popular way to produce energy in PR since the cost of electricity is around 0.30 $/KWh. Unfortunately there are some facts that are affecting the financial process of PV systems, for example there are not enough tools available in the market that could help users obtain a realistic projection of their capital and operating expenses. One solution for this problem is using a software developed by the Department of Energy, named HOMER. HOMER (Hybrid Optimization Modeling for Electric Renewable) software has the necessary tools to make this work since the users do not need to be an expert in the energy area. By using HOMER software it is possible to improve the criteria of the financing analysis and also to speed up the process of financing photovoltaic systems for different facilities in PR. As part of this paper two case of study for different regions in PR were developed. Firstly, it is considered a residential case covering the residential area and secondly an important establishment case like a hospital is tested, covering the commercial area. Finally, with the outcome of the two different cases, costumers and financiers will be able to analyze how convenient or not will be spending on the implementation of photovoltaic technologies in PR.","0160-8371","978-1-4799-4398-2978-1-4799-4399","10.1109/PVSC.2014.6925192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6925192","economical analysis;financing photovoltaic systems;solar energy","Photovoltaic systems;Arrays;Software;Hospitals;Inverters;Cities and towns","costing;financial management;photovoltaic power systems;power engineering computing;power generation economics","financing process;photovoltaic systems;Puerto Rico;HOMER software;PV systems;electricity cost;Department of Energy;hybrid optimization modeling for electric renewable;financing analysis;photovoltaic technologies","","2","3","","","","","","IEEE","IEEE Conferences"
"Georegistration of Earth Observing-1 (EO-1) data using Global Land Survey (GLS) maps","J. Le Moigne; P. Sazama; S. Swanson; V. Ly; D. Mandl","NASA Goddard Space Flight Center, Software Engineering Division, Greenbelt, MD 20771; University of Maryland, Computer Science Department, College Park, MD 20742; Princeton University, Computer Science Department, Princeton, NJ, 08544; NASA Goddard Space Flight Center, Software Engineering Division, Greenbelt, MD 20771; NASA Goddard Space Flight Center, Software Engineering Division, Greenbelt, MD 20771","2014 IEEE Geoscience and Remote Sensing Symposium","","2014","","","2518","2521","The method presented in this paper utilizes Global Land Survey (GLS) maps to register Earth Observing-1 (EO-1) data, either using entire scenes or utilizing chips extracted from the GLS maps. The automated registration algorithm is based on the optimization of wavelet or wavelet-like features extracted from both reference and input image data. After testing the method on several ALI scenes, results and conclusions are presented.","2153-6996;2153-7003","978-1-4799-5775","10.1109/IGARSS.2014.6946985","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6946985","Georegistration;Earth Observing-1;Global Land Survey (GLS);wavelet features","Earth;Feature extraction;Satellites;Remote sensing;Databases;Image registration;Accuracy","feature extraction;geophysical image processing;image registration;terrain mapping;wavelet transforms","Earth Observing-1;EO-1 data georegistration;Global Land Survey maps;GLS maps;chip extraction;automated registration algorithm;wavelet-like feature extraction;feature optimization;reference image data;input image data;ALI scene","","","7","","","","","","IEEE","IEEE Conferences"
"A discrete artificial bee colony algorithm for the team orienteering problem with time windows","K. Karabulut; M. F. Tasgetiren","Software Engineering Department Yasar University Selcuk Yasar Campus, Izmir, Turkey; Industrial Engineering Department Yasar University Selcuk Yasar Campus, Izmir, Turkey","2013 IEEE Symposium on Computational Intelligence in Production and Logistics Systems (CIPLS)","","2013","","","99","106","This paper presents a discrete artificial bee colony algorithm (DABC) for solving the team orienteering problem with time windows (TOPTW). The proposed algorithm employs a destruction and construction procedure to generate neighboring food sources in the framework of the DABC algorithm. In addition, a variable neighborhood descent (VND) algorithm is developed to enhance the solution quality. The performance of the algorithm was tested on a benchmark set from the literature. Experimental results show that the proposed DABC algorithm is competitive to the best performing algorithms from the literature. Ultimately, 11 instances are further improved by the proposed DABC algorithm.","","978-1-4673-5905","10.1109/CIPLS.2013.6595206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595206","team orienteering problem with time windows;artificial bee colony algorithm;iterated greedy algorithm;swarm intelligence;heuristic optimization","Algorithm design and analysis;Sociology;Statistics;Optimization;Computational intelligence;Logistics;Computers","computational complexity;optimisation","discrete artificial bee colony algorithm;team orienteering problem with time windows;DABC algorithm;TOPTW;destruction procedure;construction procedure;neighboring food source generation;variable neighborhood descent algorithm;VND algorithm;NP-hard problem","","5","19","","","","","","IEEE","IEEE Conferences"
"Cuckoo search algorithm based two degree of freedom controller for multi-area thermal system","S. Debbarma; A. Nath; U. Sarma; L. C. Saikia","National Institute of Technology Meghalaya, Shillong, India; SONY India software centre Pvt. Ltd; National Institute of Technology Silchar, Assam, India; National Institute of Technology Silchar, Assam, India","2015 International Conference on Energy, Power and Environment: Towards Sustainable Growth (ICEPE)","","2015","","","1","6","Two-degree-of-freedom based Proportional plus Integral plus Double Derivative (2-DOF-PIDD) controller is designed and presented to solve the load frequency control (LFC) problem of a three unequal area thermal system. All the areas are equipped with single stage of reheat turbines and appropriate generation rate constraints (GRC). The performances of the proposed 2-DOF-PIDD controller are compared with the commonly used classical controllers such as I, PI and PID controller to assess the best supplementary controller for LFC. Selection of the optimum parameter of the several controllers and speed regulation (R) parameters are done by a new metaheuristic optimization algorithm called Cuckoo Search algorithm (CSA). The main advantage associated with CSA is that it has only two controllable parameters that make the algorithm less complex and thus potentially more generic. Numerous simulation results clearly show that proposed controller guarantees better performance over others in terms of settling time, peak overshoots and reduced oscillations. Further, robustness of CSA optimized 2-DOF-PIDD controller is tested against different position of perturbation. Proposed 2-DOF-PIDD controller also provides better performances when subjected to higher degree of perturbation.","","978-1-4673-6503-1978-1-4673-6504","10.1109/EPETSG.2015.7510113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7510113","Load Frequency Control;2-DOF-PIDD Controller;Cuckoo Search Algorithm","Frequency control;Optimization;Birds;Turbines;Mathematical model;Automatic generation control;Sociology","control system synthesis;frequency control;load regulation;optimisation;perturbation techniques;power generation control;search problems;steam turbines;three-term control","Cuckoo search algorithm;two degree of freedom controller;multiarea thermal system;proportional plus integral plus double derivative controller;load frequency control problem;LFC problem;unequal area thermal system;reheat turbines;generation rate constraints;GRC;2-DOF-PIDD controller design;optimum parameter selection;speed regulation parameters;metaheuristic optimization algorithm;CSA;settling time;peak overshoots;reduced oscillations;perturbation position","","1","14","","","","","","IEEE","IEEE Conferences"
"Cross-correlation of electrical measurements via physics-based device simulations: Linking electrical and structural characteristics","A. Padovani; L. Larcher; L. Vandelli; M. Bertocchi; R. Cavicchioli; D. Veksler; G. Bersuker","DISMI, University of Modena and Reggio Emilia, Italy; DISMI, University of Modena and Reggio Emilia, Italy; DISMI, University of Modena and Reggio Emilia, Italy; DISMI, University of Modena and Reggio Emilia, Italy; DISMI, University of Modena and Reggio Emilia, Italy; SEMATECH Albany, NY 12203 USA; NA","Proceedings of the 2015 International Conference on Microelectronic Test Structures","","2015","","","100","102","We present a comprehensive simulation framework to interpret electrical characteristics (I-V, C-V, G-V, Charge-Pumping, BTI, CVS, RVS, ...) commonly used for material characterization and reliability analysis of gate dielectric stacks in modern semiconductor devices. By accounting for the physical processes controlling charge transport through the dielectric (e.g. carrier trapping/de-trapping at the defect sites, defect generation, etc.), which is modeled using a novel approach based of material characteristics [1], [2], the simulations provide a unique link between the electrical measurements data and specific atomic defects in the dielectric stack. Within this methodology, the software allows an accurate defect spectroscopy by cross-correlating measurements of pre-stress electrical parameters (IV, CV, BTI). These data are then used to project the stack reliability through the simulations of stress-induced leakage current (SILC) and time-dependent dielectric degradation trends, demonstrating the tool capabilities as a technology characterization/optimization benchmark.","1071-9032;2158-1029","978-1-4799-8304-9978-1-4799-8302","10.1109/ICMTS.2015.7106117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7106117","","Logic gates;Charge pumps;Temperature measurement;Materials reliability;Benchmark testing;Capacitance-voltage characteristics","dielectric materials;semiconductor device measurement;semiconductor device reliability","electrical measurement cross-correlation;physics-based device simulations;structural characteristics;electrical characteristics;dielectric stack reliability analysis;semiconductor devices;charge transport control;material characteristics;specific atomic defects;defect spectroscopy;pre-stress electrical parameters;stress-induced leakage current;SILC;time-dependent dielectric degradation","","","8","","","","","","IEEE","IEEE Conferences"
"Implementation of EKF for Vehicle Velocities Estimation on FPGA","H. Guo; H. Chen; F. Xu; F. Wang; G. Lu","State Key Laboratory of Automotive Simulation and Control and the Department of Control Science and Engineering, Jilin University (Campus NanLing), Changchun, China; State Key Laboratory of Automotive Simulation and Control and the Department of Control Science and Engineering, Jilin University (Campus NanLing), Changchun, China; Department of Control Science and Engineering, Jilin University (Campus NanLing) , Changchun, China; Department of Control Science and Engineering, Jilin University (Campus NanLing), Changchun, China; State Key Laboratory on Integrated Optoelectronics, College of Electronic Science and Engineering , Jilin University, Changchun, China","IEEE Transactions on Industrial Electronics","","2013","60","9","3823","3835","In order to improve the computational performance of the extended Kalman filter (EKF) for longitudinal and lateral vehicle velocities estimation, a novel scheme for the EKF implementation is proposed based on field programmable gate array (FPGA) and System on Programmable Chip (SoPC). A Nios II processor clocked at 100 MHz is embedded into the FPGA chip. The EKF is created by C/C++ program and runs in the Nios II processor. The main procedure for the EKF implementation using FPGA/SoPC technique is decomposed into three parts: system requirements analysis, hardware design, and software design. The proposed architecture offers favorable flexibility since it supports the reconfigurable hardware and reprogramming software. For the sake of increasing the computational efficiency, the single precision floating-point customized instructions and algorithm optimization are adopted. A testing platform is introduced to evaluate the functionality and the computational performance of the EKF, which includes an FPGA prototyping board and an xPC-Target system. Simulation results of standard double lane change, slalom test, and hard accelerating and braking test show that the proposed EKF implementation scheme has acceptable precision and computational efficiency.","0278-0046;1557-9948","","10.1109/TIE.2012.2208436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6266733","Extended Kalman filter (EKF);field programmable gate array (FPGA);real-time systems;vehicle velocities estimation","Vehicles;Field programmable gate arrays;Wheels;Tires;Roads;Estimation;Hardware","automotive electronics;field programmable gate arrays;Kalman filters;nonlinear filters;road vehicles;system-on-chip","EKF;extended Kalman filter;lateral vehicle velocity estimation;longitudinal vehicle velocity estimation;field programmable gate array;system on programmable chip;Nios II processor;FPGA chip;SoPC technique;software design;hardware design;system requirements analysis;single precision floating-point customized instructions;algorithm optimization;testing platform;FPGA prototyping board;xPC-target system;hard accelerating test;slalom test;braking test;frequency 100 MHz","","45","36","","","","","","IEEE","IEEE Journals & Magazines"
"Improved differential evolution with adaptive opposition strategy","H. Liu; Z. Wu; H. Wang; S. Rahnamayan; C. Deng","Computer School of Wuhan University, Wuhan 430072, China; Computer School of Wuhan University, Wuhan 430072, China; State Key Lab of Software Engineering, Wuhan University, Wuhan 430072, China; Department of Electrical, Computer and Software Engineering, University of Ontario Institute of Technology, Oshawa, ONL1H7K4, Canada; School of Information Science and Technology, Jiujiang University, Jiujiang 332005, China","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","1776","1783","Generalized opposition-based differential evolution (GODE) is an effective algorithm for global optimization over continuous search space. However, the performance of GODE highly depends on its control parameters. To improve the performance of GODE, this paper proposes an enhanced GODE algorithm called AGODE, which employs an adaptive generalized opposition-based learning (GOBL) mechanism to automatically adjust the probability of opposition during the evolution. Experimental study is conducted on a set of 19 well-known benchmark functions. Computational results show that the proposed approach AGODE outperforms some state-of-the-art DE variants on the majority of test problems.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900298","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900298","","Sociology;Statistics;Vectors;Optimization;Educational institutions;Time complexity;Benchmark testing","evolutionary computation;search problems","adaptive generalized opposition-based differential evolution;continuous search space;control parameters;AGODE algorithm;opposition probability","","2","41","","","","","","IEEE","IEEE Conferences"
"A Case Study on Code Generation of an ERP System from Event-B","N. Cataño; T. Wahls","NA; NA","2015 IEEE International Conference on Software Quality, Reliability and Security","","2015","","","183","188","Most code generation tools for Event-B are designed for generating small, in-memory applications such as embedded controllers. In this work, we investigate whether the EventB2SQL tool (Wang and Wahls, LNCS 8702) can generate satisfactory code for the OpenBravo POS ERP (Enterprise Resource Planning) system by replacing the database core of the system with code generated from an Event-B model. We describe our methodology for generating code with EventB2SQL and enhancements to EventB2SQL that improve the performance of the generated code, and present empirical results and a user study comparing the performance of OpenBravo POS as is and with its core replaced by code generated by EventB2SQL.","","978-1-4673-7989-2978-1-4673-7988","10.1109/QRS.2015.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272930","Event-B;code generation;ERP;database","Databases;Unified modeling language;Java;Timing;Optimization;Benchmark testing;Atmospheric modeling","enterprise resource planning;formal specification;formal verification;program compilers;software tools","code generation tool;enterprise resource planning system;ERP system;Event-B model","","","15","","","","","","IEEE","IEEE Conferences"
"Improving click model by combining mouse movements with click-through data","X. Chen; H. Min","School of Information Science and Technology, Lingnan Normal University, Zhanjiang, Guangdong 524048, China; School of Software Engineering, South China University of Technology, Guangzhou, Guangdong 510006, China","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","","2015","","","183","187","As a typical user activity based model in search engines, click model is currently the mainstream approach to describe and analyze user behaviors when handling a number of search-related applications, such as automated ranking alternations, search quality metrics, and online advertising. However, most of the existing work in this literature only consider click through data but ignore other aspects of the interactions between users and search results (e.g. eye and mouse movements, etc.). To predict the click-through rates (CTRs)<sup>1</sup> more accurately, this paper improves the click model by combining mouse tracking with click through data to obtain more objective measures of user experience and consequently achieve a better understanding of user behaviors. Experimental results that are uniformly sampled from the most popular Chinese search engine - Baidu.com<sup>2</sup> show that the proposed approach outperforms the existing models at predicting CTRs. Online test results covering 25% of the daily Internet traffic in Baidu.com show that the top-part ranking list adjusted based on the predicted CTRs increase the CTR of the first URL and the total CTRs of the URLs in the first page. These experiments prove that the proposed approach improves the quality of search results and meets users' information needs more accurately.","2327-0594;2327-0586","978-1-4799-8353-7978-1-4799-8352-0978-1-4799-8351","10.1109/ICSESS.2015.7339033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339033","mouse tracking;click-through data;click model","Uniform resource locators;Mice;Data models;Search engines;Analytical models;Predictive models;Internet","advertising;information needs;Internet;mouse controllers (computers);search engines;search problems","click model;mouse movements;click-through data;user activity based model;user behavior analysis;search-related applications;automated ranking alternations;search quality metrics;online advertising;click-through rates;CTR;mouse tracking;user experience;user behaviors;Chinese search engines;daily Internet traffic;URL;user information needs","","","13","","","","","","IEEE","IEEE Conferences"
"Maximizing Hardware Prefetch Effectiveness with Machine Learning","S. Rahman; M. Burtscher; Z. Zong; A. Qasem","NA; NA; NA; NA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","383","389","Modern processors are equipped with multiple hardware prefetchers, each of which targets a distinct level in the memory hierarchy and employs a separate prefetching algorithm. However, different programs require different subsets of these prefetchers to maximize their performance. Turning on all available prefetchers rarely yields the best performance and, in some cases, prefetching even hurts performance. This paper studies the effect of hardware prefetching on multithreaded code and presents a machine-learning technique to predict the optimal combination of prefetchers for a given application. This technique is based on program characterization and utilizes hardware performance events in conjunction with a pruning algorithm to obtain a concise and expressive feature set. The resulting feature set is used in three different learning models. All necessary steps are implemented in a framework that reaches, on average, 96% of the best possible prefetcher speedup. The framework is built from open-source tools, making it easy to extend and port to other architectures.","","978-1-4799-8937-9978-1-4799-8936","10.1109/HPCC-CSS-ICESS.2015.175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336192","","Prefetching;Hardware;Algorithms;Optimization;Testing;Training","learning (artificial intelligence);multi-threading;performance evaluation;public domain software;storage management","hardware prefetch effectiveness maximization;machine learning;multiple hardware prefetchers;memory hierarchy;performance maximization;multithreaded code;optimal prefetcher prediction;program characterization;hardware performance events;pruning algorithm;feature set;learning models;prefetcher speedup;open-source tools","","3","14","","","","","","IEEE","IEEE Conferences"
"Tiled regression reduces type I error rates in tests of association of rare single nucleotide variants with non-normally distributed traits, compared with simple linear regression","H. Sung; A. J. M. Sorant; J. A. Sabourin; T. Schwantes-An; C. M. Justice; J. E. Bailey-Wilson; A. F. Wilson","Computational and Statistical Genomics Branch, National Human Genome Research Institute, National Institutes of Health, Baltimore MD, USA; Computational and Statistical Genomics Branch, National Human Genome Research Institute, National Institutes of Health, Baltimore MD, USA; Computational and Statistical Genomics Branch, National Human Genome Research Institute, National Institutes of Health, Baltimore MD, USA; Computational and Statistical Genomics Branch, National Human Genome Research Institute, National Institutes of Health, Baltimore MD, USA; Computational and Statistical Genomics Branch, National Human Genome Research Institute, National Institutes of Health, Baltimore MD, USA; Computational and Statistical Genomics Branch, National Human Genome Research Institute, National Institutes of Health, Baltimore MD, USA; Computational and Statistical Genomics Branch, National Human Genome Research Institute, National Institutes of Health, Baltimore MD, USA","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","1336","1340","The effects of the minor allele frequency of single nucleotide variants and the degree of departure from normality of a quantitative trait on type I error rates were evaluated using Genetic Analysis Workshop 17 mini-exome sequence data. Four simulated traits were generated: standard normal and gamma distributed traits and two transformations of the gamma distributed trait by log10 and rank-based inverse normal functions. Tiled regression was compared with simple linear regression. Average type I error rates were obtained for minor allele frequency classes. The distribution of the type I error rate for tiled regression analysis followed a pattern similar to that of simple linear regression analysis, but with much lower type I error.","","978-1-4673-6799-8978-1-4673-6798","10.1109/BIBM.2015.7359872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359872","type I error rate;tiled regression;minor allele frequency;non-normality","Genomics;Bioinformatics;Software;Chlorine","genomics;molecular biophysics;regression analysis","rare single nucleotide variant association;genetic analysis workshop 17 miniexome sequence data;standard normal trait;gamma distributed trait;inverse normal function;allele frequency class;tiled regression analysis;linear regression analysis","","","5","","","","","","IEEE","IEEE Conferences"
"Polynomial Models Identification Using Real Data Acquisition Applied to Didactic System","A. F. O. A. Dantas; A. D. O. S. Dantas; A. L. Maitelli; E. F. d. Queiroz; H. G. C. V. Freire; O. G. Filho","NA; NA; NA; NA; NA; NA","2013 Symposium on Computing and Automation for Offshore Shipbuilding","","2013","","","1","6","Models of real systems are of fundamental importance for its analysis, making it possible to simulate or predict its behavior. Additionally, advanced techniques for controller design, optimization, monitoring, fault detection and diagnosis components are also based on process models. One of the most used techniques to model a system is by identification. System identification or process identification is the field of mathematical modeling of systems, in which the parameters are obtained from test or experimental data. Given the importance of obtaining a model able to represent the dynamics of real processes, we developed a software that aggregates identification algorithms using Least squares (LS), Least squares Extended (ELS), Generalized Least Squares (GLS) Recursive least squares with Compensator Polarization (BCRLS). This identification package is used in this paper to identify an educational level plant. Its actual data was inserted in the package and thus, results from different identification techniques implemented in the algorithm were compared. All steps necessary to carry out the identification and analysis of the autocorrelation of the output data for the definition of the sampling period, the design of excitation signals and data collection were taken in consideration. The conclusions reached are that the software provides consistency and the implemented algorithms return a model capable of representing the linear part of the system's dynamics.","","978-0-7695-5123","10.1109/NAVCOMP.2013.7","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6686105","identification package;didactic system","Mathematical model;Software;Computational modeling;Process control;Heuristic algorithms;Visualization;Software algorithms","data acquisition;identification;least squares approximations;polynomials;recursive estimation;software packages","polynomial models identification;real data acquisition;didactic system;identification algorithms;least squares extended algorithm;generalized least squares algorithm;recursive least squares with compensator polarization algorithm;educational level plant;software identification package;output data autocorrelation analysis;excitation signals design;data collection;LS algorithm;ELS algorithm;GLS algorithm;BCRLS algorithm","","","6","","","","","","IEEE","IEEE Conferences"
"New roles & responsibilities of hospital biomedical engineering","P. H. Frisch; B. Stone; P. Booth; W. Lui","Biomedical Engineering in the Department of Medical Physics at Memorial Sloan-Kettering Cancer Center, New York, NY. 10021; Clinical Engineering an operating section within Biomedical Engineering, Department of Medical Physics at Memorial Sloan-Kettering Cancer Center, New York, NY. 10021; Biomedical Systems an operating section of Biomedical Engineering, Department of Medical Physics at Memorial Sloan-Kettering Cancer Center, New York, NY. 10021; Medical Physics Department at Memorial Sloan-Kettering Cancer Center, New York, NY. 10021","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2014","","","3488","3491","Over the last decade the changing healthcare environment has required hospitals and specifically Biomedical Engineering to critically evaluate, optimize and adapt their operations. The focus is now on new technologies, changes to the environment of care, support requirements and financial constraints. Memorial Sloan Kettering Cancer Center (MSKCC), an NIH-designated comprehensive cancer center, has been transitioning to an increasing outpatient care environment. This transition is driving an increase in-patient acuity coupled with the need for added urgency of support and response time. New technologies, regulatory requirements and financial constraints have impacted operating budgets and in some cases, resulted in a reduction in staffing. Specific initiatives, such as the Joint Commission's National Patient Safety Goals, requirements for an electronic medical record, meaningful use and ICD10 have caused institutions to reevaluate their operations and processes including requiring Biomedical Engineering to manage new technologies, integrations and changes in the electromagnetic environment, while optimizing operational workflow and resource utilization. This paper addresses the new and expanding responsibilities and approach of Biomedical Engineering organizations, specifically at MSKCC. It is suggested that our experience may be a template for other organizations facing similar problems. Increasing support is necessary for Medical Software - Medical Device Data Systems in the evolving wireless environment, including RTLS and RFID. It will be necessary to evaluate the potential impact on the growing electromagnetic environment, on connectivity resulting in the need for dynamic and interactive testing and the growing demand to establish new and needed operational synergies with Information Technology operations and other operational groups within the institution, such as nursing, facilities management, central supply, and the user departments.","1094-687X;1558-4615","978-1-4244-7929","10.1109/EMBC.2014.6944374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944374","Biomedical Engineering;evolving technology;technology management;equipment management;regulatory;MDDS;electromagnetic spectrum;connectivity","","biomedical communication;cancer;electronic health records;health care;hospitals;information technology;patient care;radiofrequency identification","hospital biomedical engineering;healthcare environment;support requirements;financial constraints;Memorial Sloan Kettering Cancer Center;MSKCC;NIH-designated comprehensive cancer center;outpatient care environment;in-patient acuity;support time;response time;regulatory requirements;staffing reduction;Joint Commission's National Patient Safety Goals;electronic medical record;ICD10;electromagnetic environment;operational workflow;resource utilization;Biomedical Engineering organizations;Medical Software;Medical Device Data Systems;wireless environment;RTLS;RFID;connectivity;dynamic testing;interactive testing;operational synergies;Information Technology operations;operational groups;nursing;facilities management;central supply;user departments","Biomedical Engineering;Biomedical Technology;Delivery of Health Care;Hospital Communication Systems;Hospitals;Humans;Maintenance and Engineering, Hospital;Patient Care;Quality Improvement;Software;Wireless Technology","3","8","","","","","","IEEE","IEEE Conferences"
"Adjustment of GCC compiler frontend for embedded processors","D. Bokan; M. Ðukić; M. Popović; N. Četić","Faculty of Technical Sciences Novi Sad, Trg Dositeja Obradovića 6, Serbia; Faculty of Technical Sciences Novi Sad, Trg Dositeja Obradovića 6, Serbia; Faculty of Technical Sciences Novi Sad, Trg Dositeja Obradovića 6, Serbia; Faculty of Technical Sciences Novi Sad, Trg Dositeja Obradovića 6, Serbia","2014 22nd Telecommunications Forum Telfor (TELFOR)","","2014","","","983","986","This paper describes one solution for adjustment and modification of GCC compiler front-end to be used with compilers for embedded processors. This project was developed as an effort to provide open source front-end for RTCC compiler library. Modified GCC translates code until certain point, and dumps the IR. RTCC parses that IR and proceeds with further compilation. It is shown that adjusting GCC frontend for embedded processors is not as simple as modifying target description.","","978-1-4799-6191-7978-1-4799-6190","10.1109/TELFOR.2014.7034571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7034571","compiler;front-end;GCC;gimple;programming language C;embedded processor;DSP","Program processors;Testing;Optimization;Digital signal processing;Licenses;Electronic mail","C language;embedded systems;program compilers;software libraries","GCC compiler;embedded processor;open source;RTCC compiler library;GNU compiler collection","","","8","","","","","","IEEE","IEEE Conferences"
"Modeling and Simulation in Performance Optimization of Big Data Processing Frameworks","R. Ranjan","Commonwealth Scientific and Industrial Research Organization, Australia","IEEE Cloud Computing","","2014","1","4","14","19","The enormous increase in data has led to the next grand challenge in computing: the big data problem-that is, the practice of collecting complex datasets so large that they're difficult to store, process, and interpret manually or using traditional data management applications. This column discusses the role of modeling and simulation science in the era of big data applications. Modeling and simulation can empower practitioners and academics in conducting ""what-if"" analyses for scheduling policies under variable cloud resource configurations, big data processing framework configurations, and workload.","2325-6095;2372-2568","","10.1109/MCC.2014.84","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057589","cloud;simulation;benchmarks;big data;processing framework","Big data;Computational modeling;Data models;Hardware;Cloud computing;Benchmark testing","Big Data;cloud computing;scheduling;software performance evaluation","performance optimization;Big Data processing frameworks;complex dataset collection;data management applications;what-if analysis;scheduling policies;variable cloud resource configurations","","9","35","","","","","","IEEE","IEEE Journals & Magazines"
"Towards Dynamic Application Distribution Support for Performance Optimization in the Cloud","S. G. Sáez; V. Andrikopoulos; F. Leymann; S. Strauch","NA; NA; NA; NA","2014 IEEE 7th International Conference on Cloud Computing","","2014","","","248","255","The Cloud computing paradigm emerged by establishing new resources provisioning and consumption models. Together with the improvement of resource management techniques, these models have contributed to an increase in the number of application developers that are strong supporters of partially or completely migrating their application to a highly scalable and pay-per-use infrastructure. In this paper we derive a set of functional and non-functional requirements and propose a process-based approach to support the optimal distribution of an application in the Cloud in order to handle fluctuating over time workloads. Using the TPC-H workload as the basis, and by means of empirical workload analysis and characterization, we evaluate the application persistence layer's performance under different deployment scenarios using generated workloads with particular behavior characteristics.","2159-6190;2159-6182","978-1-4799-5063-8978-1-4799-5062","10.1109/CLOUD.2014.42","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973748","Synthetic Workload;Benchmark;Application Distribution;Application Deployment;Relational Database;TPC;Database-as-a-Service (DBaaS)","Topology;Throughput;Weibull distribution;Databases;Benchmark testing;Cloud computing;Computer architecture","cloud computing;resource allocation;software performance evaluation","dynamic application distribution support;performance optimization;cloud computing paradigm;resource provisioning model;consumption model;resource management techniques;pay-per-use infrastructure;nonfunctional requirement;process-based approach;TPC-H workload;empirical workload analysis;application persistence layer;performance evaluation;workload generation;behavior characteristics","","1","24","","","","","","IEEE","IEEE Conferences"
"Optimizing OLAP Cubes Construction by Improving Data Placement on Multi-nodes Clusters","B. Arres; N. Kabachi; O. Boussaid","NA; NA; NA","2015 23rd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing","","2015","","","520","524","The increasing volumes of relational data let us find an alternative to cope with them. The Hadoop framework - which is an open source project based on the MapReduce paradigm - is a popular choice for big data analytics. However, the performance gained from Hadoop's features is currently limited by its default block placement policy, which does not take any data characteristics into account. Indeed, the efficiency of many operations can be improved by a careful data placement, including indexing, grouping, aggregation and joins. In this paper we propose a data warehouse placement policy to improve query gain performances on multi nodes clusters, especially Hadoop clusters. We investigate the performance gain for OLAP cube construction query with and without data organization. And this, by varying the number of nodes and data warehouse size. It has been found that, the proposed data placement policy has lowered global execution time for building OLAP data cubes up to 20 percent compared to default data placement.","1066-6192;2377-5750","978-1-4799-8491","10.1109/PDP.2015.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092769","MapReduce;HDFS;Data warehouses;Block Placement","Data warehouses;Organizations;Context;Warehousing;Benchmark testing;Distributed databases;Indexing","Big Data;data mining;data warehouses;parallel processing;public domain software;query processing;relational databases","OLAP cubes construction optimization;data placement improvement;multinodes clusters;relational data;Hadoop framework;open source project;MapReduce paradigm;big data analytics;default block placement policy;indexing;grouping;aggregation;joins;data warehouse placement policy;query gain performance improvement;OLAP cube construction query;data organization","","1","24","","","","","","IEEE","IEEE Conferences"
"ASCAR: Automating contention management for high-performance storage systems","Y. Li; X. Lu; E. L. Miller; D. D. E. Long","Storage Systems Research Center, University of California, Santa Cruz; Storage Systems Research Center, University of California, Santa Cruz; Storage Systems Research Center, University of California, Santa Cruz; Storage Systems Research Center, University of California, Santa Cruz","2015 31st Symposium on Mass Storage Systems and Technologies (MSST)","","2015","","","1","16","High-performance parallel storage systems, such as those used by supercomputers and data centers, can suffer from performance degradation when a large number of clients are contending for limited resources, like bandwidth. These contentions lower the efficiency of the system and cause unwanted speed variances. We present the Automatic Storage Contention Alleviation and Reduction system (ASCAR), a storage traffic management system for improving the bandwidth utilization and fairness of resource allocation. ASCAR regulates I/O traffic from the clients using a rule based algorithm that controls the congestion window and rate limit. The rule-based client controllers are fast responding to burst I/O because no runtime coordination between clients or with a central coordinator is needed; they are also autonomous so the system has no scale-out bottleneck. Finding optimal rules can be a challenging task that requires expertise and numerous experiments. ASCAR includes a SHAred-nothing Rule Producer (SHARP) that produces rules in an unsupervised manner by systematically exploring the solution space of possible rule designs and evaluating the target workload under the candidate rule sets. Evaluation shows that our ASCAR prototype can improve the throughput of all tested workloads - some by as much as 35%. ASCAR improves the throughput of a NASA NPB BTIO checkpoint workload by 33.5% and reduces its speed variance by 55.4% at the same time. The optimization time and controller overhead are unrelated to the scale of the system; thus, it has the potential to support future large-scale systems that can have millions of clients and thousands of servers. As a pure client-side solution, ASCAR needs no change to either the hardware or server software.","2160-195X;2160-1968","978-1-4673-7619","10.1109/MSST.2015.7208287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208287","","Servers;Throughput;Bandwidth;Optimization;Software;Process control;Control systems","parallel processing;resource allocation;storage management","ASCAR;automating contention management;high-performance parallel storage systems;performance degradation;speed variances;automatic storage contention alleviation and reduction system;storage traffic management system;bandwidth utilization;fairness;resource allocation;I/O traffic;rule based algorithm;congestion window;rate limit;rule-based client controllers;burst I/O;runtime coordination;central coordinator;optimal rules;shared-nothing rule producer;SHARP;rule designs;target workload;rule sets;workloads throughput;NASA NPB BTIO checkpoint workload;optimization time;controller overhead;client-side solution;hardware software;server software","","2","51","","","","","","IEEE","IEEE Conferences"
"VBIW: Optimizing Indirect Branch in Dynamic Binary Translation","X. Zhang; X. Gao; Q. Guo; J. Huang; H. Liu; X. Meng","NA; NA; NA; NA; NA; NA","2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing","","2013","","","1456","1462","A major challenge of Dynamic Binary Translation (DBT) is to efficiently handle the indirect branch. Conventionally, to translate indirect branches (IBs), DBT systems need to conduct address mapping between Source Binary Blocks (SBB) and Translated Binary Blocks (TBB). However, even a dedicated address mapping process still results in non-trivial performance overheads to DBT. This paper first provides exhaustive analysis of the overheads of address mapping, and finds that hash lookup, context switching and consistency maintenance are three main sources of overheads. To address these overheads, we further propose a novel approach called Virtual Branch Instruction Write-back (VBIW). The key idea is to dynamically write a Virtual Branch Instruction (VBI) into the SBB once the mapping is determined. Since the VBI contains the target TBB address of a branch, the costly address mapping can be eliminated for further reference of the same branch. In addition to theoretical analysis of VBIW, we also implement VBIW on a X86 to MIPS DBT system of Godson-3. The experimental results show that VBIW can reduce DBT execution time by 29.5% on average (ranging from 1.8% to 58.5%) for single threaded benchmarks, and by 19.6% on average (4.5% to 62.5%) for multithreaded benchmarks.","","978-0-7695-5088","10.1109/HPCC.and.EUC.2013.206","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832088","dynamic binary translation;indirect branch;virtual branch instruction;consistency maintenance;multithreaded emulation","Benchmark testing;Maintenance engineering;Emulation;Writing;Context;Switches;Radiation detectors","multi-threading;program interpreters;source code (software);storage allocation","VBIW;indirect branch optimization;dynamic binary translation;DBT systems;indirect branch translation;IB translation;source binary blocks;SBB;translated binary blocks;address mapping process;nontrivial performance overheads;hash lookup;context switching;consistency maintenance;virtual branch instruction write-back;dynamic writing;target TBB address;MIPS DBT system;Godson-3;DBT execution time reduction;single-threaded benchmarks;multithreaded benchmarks","","","20","","","","","","IEEE","IEEE Conferences"
"Development of mission planning tool including optimization module","T. Nilnarong; P. Tangpattanakul","Geo-Informatics and Space Technology Development, Agency (Public Organization) 120, The Government Complex (Building B), Chaeng Wattana Road, Laksi, Bangkok 10210, Thailand; Geo-Informatics and Space Technology Development, Agency (Public Organization) 120, The Government Complex (Building B), Chaeng Wattana Road, Laksi, Bangkok 10210, Thailand","The 20th Asia-Pacific Conference on Communication (APCC2014)","","2014","","","213","218","In order to achieve the next level services of earth observation satellite data, the web-based tool for mission planning is developed. The objectives are joint-usage spare resources of earth observation satellite amongst ASEAN, customer-self mission planning and optimization of mission plan. The tools are developed in three modules; web-based interface module for world-wide service, Orbital propagator/Earth projection module for the computation of feasible accessibility and optimization module for efficiency of mission planning. Three segments are carried out in parallel so as to get the tool ready-to-use as soon as possible. Each module will be initially tested in separation and finally as a whole once they are completed with current-used mission planning tool.","2163-0771","978-1-4799-6435","10.1109/APCC.2014.7091635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091635","Mission Plan;Satellite position;Field of Regard;API Map;Accessibility","Satellites;Earth;Imaging;Planning;Optimization;Orbits;Strips","aerospace computing;artificial satellites;Internet;planning;software tools","mission planning tool;timization module;earth observation satellite data;Web-based tool;ASEAN;customer-self mission planning;Web-based interface module;world-wide service;orbital propagator-Earth projection module","","","10","","","","","","IEEE","IEEE Conferences"
"Improved Method to Select the Lagrange Multiplier for Rate-Distortion Based Motion Estimation in Video Coding","J. L. González-de-Suso; A. Jiménez-Moreno; E. Martínez-Enríquez; F. Díaz-de-María","Department of Signal Theory and Communications, Carlos III University, Leganés, Madrid, Spain; Department of Signal Theory and Communications, Carlos III University, Leganés, Madrid, Spain; Department of Signal Theory and Communications, Carlos III University, Leganés, Madrid, Spain; Department of Signal Theory and Communications, Carlos III University, Leganés, Madrid, Spain","IEEE Transactions on Circuits and Systems for Video Technology","","2014","24","3","452","464","The motion estimation (ME) process used in the H.264/AVC reference software is based on minimizing a cost function that involves two terms (distortion and rate) that are properly balanced through a Lagrangian parameter, usually denoted as λmotion. In this paper we propose an algorithm to improve the conventional way of estimating λmotion and, consequently, the ME process. First, we show that the conventional estimation of λmotion turns out to be significantly less accurate when ME-compromising events, which make the ME process to perform poorly, happen. Second, with the aim of improving the coding efficiency in these cases, an efficient algorithm is proposed that allows the encoder to choose between three different values of λmotion for the Inter 16x16 partition size. To be more precise, for this partition size, the proposed algorithm allows the encoder to additionally test λmotion=0 and λmotion arbitrarily large, which corresponds to minimum distortion and minimum rate solutions, respectively. By testing these two extreme values, the algorithm avoids making large ME errors. The experimental results on video segments exhibiting this type of ME-compromising events reveal an average rate reduction of 2.20% for the same coding quality with respect to the JM15.1 reference software of H.264/AVC. The algorithm has been also tested in comparison with a state-of-the-art algorithm called context adaptive Lagrange multiplier. Additionally, two illustrative examples of the subjective performance improvement are provided.","1051-8215;1558-2205","","10.1109/TCSVT.2013.2276857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6576132","H264/Advanced Video Coding (AVC);Lagrange multiplier;motion estimation;rate-distortion optimization;video coding","Encoding;Estimation;Video coding;Optimization;Software;Motion segmentation;Standards","image segmentation;motion estimation;video coding","rate distortion;motion estimation;video coding;ME process;H.264/AVC reference software;Lagrangian parameter;ME errors;video segmentation;coding quality;context adaptive Lagrange multiplier","","4","27","","","","","","IEEE","IEEE Journals & Magazines"
"New graph-based text summarization method","S. alZahir; Q. Fatima; M. Cenek","Image Processing and Graphics Lab,, CS Department, UNBC, PG, British Columbia, V2N 4Z9, Canada; Image Processing and Graphics Lab,, CS Department, UNBC, PG, British Columbia, V2N 4Z9, Canada; Computer Science and Engineering Department, UAA, Anchorage, Alaska, USA","2015 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)","","2015","","","396","401","The exponential growth of text data on the World Wide Web as well as on databases off line created a critical need for efficient text summarizers that significantly reduce its size while maintaining its integrity. In this paper, we present a new multigraph-based text summarizer method. This method is unique in that it produces a multi-edge-irregular-graph that represents words occurrence in the sentences of the target text. This graph is then converted into a symmetric matrix from which we can produce the ranking of sentences and hence obtain the summarized text using a threshold. To test our method performance, we compared our results with those from the most popular publicly available text summarization software using a corpus of 1000 samples from 6 different applications: health, literature, politics, religion, science and sports. The simulation results show that the proposed method produced better or comparable summaries in all cases. The proposed method is fast and can be implement for real time summarization.","2154-5952","978-1-4673-7788-1978-1-4673-7787","10.1109/PACRIM.2015.7334869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334869","Text Summarization;extraction;abstraction;software testing;frequency of occurance;sentence ranking;aelevance","Symmetric matrices;Semantics;Frequency measurement;Electronic mail;Companies;Natural languages;Computers","graph theory;Internet;matrix algebra;text analysis;Web sites","graph-based text summarization method;World Wide Web;text data exponential growth;multigraph-based text summarizer method;symmetric matrix","","","29","","","","","","IEEE","IEEE Conferences"
"GWIS<inf>FI</inf>: A universal GPU interface for exhaustive search of pairwise interactions in case-control GWAS in minutes","Q. Wang; F. Shi; A. Kowalczyk; R. M. Campbell; B. Goudey; D. Rawlinson; A. Harwood; H. Ferra; A. Kowalczyk","NICTA Victorian Research Lab, The University of Melbourne, Parkville, VIC 3010, Australia; NICTA Victorian Research Lab, The University of Melbourne, Parkville, VIC 3010, Australia; NICTA Victorian Research Lab, The University of Melbourne, Parkville, VIC 3010, Australia; NICTA Victorian Research Lab, The University of Melbourne, Parkville, VIC 3010, Australia; NICTA Victorian Research Lab, The University of Melbourne, Parkville, VIC 3010, Australia; NICTA Victorian Research Lab, The University of Melbourne, Parkville, VIC 3010, Australia; Computing and Information Systems, The University of Melbourne, Parkville, VIC 3010, Australia; NICTA Victorian Research Lab, The University of Melbourne, Parkville, VIC 3010, Australia; NICTA Victorian Research Lab, The University of Melbourne, Parkville, VIC 3010, Australia","2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2014","","","403","409","Epistatic interactions between genes are believed to be a critical component in the genetic architecture of complex diseases. Genome Wide Association Studies (GWAS) may be able to detect such genetic interactions indirectly, via the identification of associated SNP markers. Major obstacles to progress in this area are: the unknown nature of epistatic interactions, little understanding of the capabilities of different filtering methods, and the computational difficulties for exhaustive analysis. A common platform enabling various detection methods is needed to avoid practical issues such as software compatibility and portability, incompatible input and output formats and varying demands on computational resources. We developed a highly optimised GPU system capable of exhaustively analysing all SNP-pairs in typical GWAS data (0.5M SNPs, 5K samples) in a few minutes on a standard desktop computer. A number of programming elements provided by a functional interface can be used to construct user-defined statistical tests to efficiently score every SNP pair. As a proof of principle, we have implemented 8 methods from the literature via our interface. We have applied all of them using a single GPU to exhaustively scan the 7 popular WTCCC case-control GWAS datasets. We present timing results for these methods, both in their original software implementations and using our platform. Significant improvements in timing are observed, up to 10000 times for CPU implementations of the popular FastEpistasis in PLINK and up to 2 orders of magnitude for some GPU implementations in the literature. As an initial discovery we show plots for overlaps of list of selected pairs by 8 algorithms for Type 2 Diabetes, WTCCC data.","","978-1-4799-5669","10.1109/BIBM.2014.6999192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999192","GWISFI: bioinformatics.research.nicta.com.au/gwisfi","Graphics processing units;Runtime;Diseases;Standards;Software algorithms;Kernel","bioinformatics;diseases;genetics;genomics;graphical user interfaces;graphics processing units","single GPU;WTCCC case-control GWAS datasets;original software implementations;FastEpistasis;PLINK;orders of magnitude;GPU implementation;Type 2 Diabetes;WTCCC data;user-defined statistical tests;functional interface;programming elements;standard desktop computer;SNP-pairs;highly optimised GPU system;computational resources;incompatible output formats;incompatible input formats;portability;software compatibility;detection methods;exhaustive analysis;computational difficulties;filtering methods;associated SNP marker identification;genetic interactions;Genome Wide Association Studies;complex diseases;genetic architecture;epistatic interactions;pairwise interactions;universal GPU interface;GWISFI","","1","37","","","","","","IEEE","IEEE Conferences"
"Managing technical debt: An industrial case study","Z. Codabux; B. Williams","Dept. of Computer Science and Engineering Mississippi State University Starkville, MS, USA; Dept. of Computer Science and Engineering Mississippi State University Starkville, MS, USA","2013 4th International Workshop on Managing Technical Debt (MTD)","","2013","","","8","15","Technical debt is the consequence of trade-offs made during software development to ensure speedy releases. The research community lacks rigorously evaluated guidelines to help practitioners characterize, manage and prioritize debt. This paper describes a study conducted with an industrial partner during their implementation of Agile development practices for a large software development division within the company. The report contains our initial findings based on ethnographic observations and semi-structured interviews. The goal is to identify the best practices regarding managing technical debt so that the researchers and the practitioners can further evaluate these practices to extend their knowledge of the technical debt metaphor. We determined that the developers considered their own taxonomy of technical debt based on the type of work they were assigned and their personal understanding of the term. Despite management's high-level categories, the developers mostly considered design debt, testing debt and defect debt. In addition to developers having their own taxonomy, assigning dedicated teams for technical debt reduction and allowing other teams about 20% of time per sprint for debt reduction are good initiatives towards lowering technical debt. While technical debt has become a well-regarded concept in the Agile community, further empirical evaluation is needed to assess how to properly apply the concept for various development organizations.","","978-1-4673-6443","10.1109/MTD.2013.6608672","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608672","technical debt Agile methods industrial case study;Scrum;semi-structure interviews","Software;Interviews;Organizations;Encoding;Taxonomy;Maintenance engineering;Training","software prototyping","managing technical debt;industrial case study;software development;industrial partner;agile development practices;software development division;ethnographic observations;development organizations","","18","13","","","","","","IEEE","IEEE Conferences"
"Characterizing Mobile Open APIs in smartphone apps","L. Zhang; C. Stover; A. Lins; C. Buckley; P. Mohapatra","Computer Science Department, University of California, Davis; Computer Science Department, University of California, Davis; Computer Science Department, University of California, Davis; Computer Science Department, University of California, Davis; Computer Science Department, University of California, Davis","2014 IFIP Networking Conference","","2014","","","1","9","Mobile applications used in smartphones are increasingly using Open APIs, and the trend is likely to continue in the foreseeable future. However, the performance of the Open APIs integrated in smartphone apps (Mobile Open APIs) remains hidden from app developers and app users because of the lack of a method to isolate the Open API calls from the whole app execution process. In this paper, we present the very first effort on characterizing Mobile Open APIs and analyzing their performance in terms of four metrics: response latency, network traffic, energy consumption, and CPU usage. We first develop APIExtractor (APIX), a software tool to extract the Mobile Open API calls as fine-grained as in the function level from Android app files (.apk). Then the popularity of the Open API functions were ranked by running APIX on 200 top popular apps downloaded from the Android app store. We then perform in-depth case studies on the the top 17 most popular Mobile Open APIs, by wrapping each of them in a specifically designed app (called APISymphone) and test the apps on both Wi-Fi and cellular network. Furthermore, we conduct a global scale measurements of the Mobile Open APIs by using Amazon Elastic Computing service. Our comprehensive measurement-based results provides very intriguing as well as interesting insights to the performance characteristics of Mobile APIs.","","978-3-901882-58","10.1109/IFIPNetworking.2014.6857130","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6857130","","Mobile communication;Protocols;Java;Androids;Humanoid robots;Servers;Probes","application program interfaces;cellular radio;mobile computing;smart phones;software metrics;software performance evaluation;software tools;wireless LAN","mobile open API characterization;smartphone apps;mobile applications;app execution process;response latency;network traffic;energy consumption;CPU usage;APIExtractor;APIX;software tool;Android app files;Android app store;APISymphone;Wi-Fi network;cellular network;Amazon elastic computing service;open application programming interface","","","20","","","","","","IEEE","IEEE Conferences"
"Integrated, Distributed Traffic Control in Multidomain Networks","W. Su; C. Liu; C. M. Lagoa; H. Che; K. Xu; Y. Cui","Department of Electrical Engineering, Pennsylvania State University, University Park, PA, USA; Department of Computer Science, Tsinghua University, Beijing, China; Department of Electrical Engineering, Pennsylvania State University, University Park, PA, USA; Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, USA; Department of Computer Science, Tsinghua University, Beijing, China; Department of Computer Science, Tsinghua University, Beijing, China","IEEE Transactions on Control Systems Technology","","2015","23","4","1373","1386","In this paper, we put forward an integrated traffic control structure and the associated control laws for multidomain networks. This control structure performs per-edge-to-edge-based multinext-hop or multipath rate adaptation and load balancing among domain edge nodes in a multidomain network. This control structure is underpinned by a large family of distributed control laws, with provable convergence and optimality properties. With any user-defined global design objective, a set of control laws can be selected from this family of control laws that track an operational point where the global design objective is achieved, while providing traffic engineering (TE) and fast failure recovery (FFR) features for class-of-service (CoS)-aware flow aggregates. The structure allows the user to have full control over how the domains should be created and whether to use point-to-multipoint and/or point-to-point multipath. The flexibility and versatility of the control structure makes it an ideal theoretical underpinning for the development of integrated traffic control solutions for large-scale networking systems, in particular, software-defined networks in which the data plane is fully programmable via a well-defined south-bound interface, such as OpenFlow. The simulation testing demonstrates the viability of the solution in providing TE, FFR, and CoS features.","1063-6536;1558-0865;2374-0159","","10.1109/TCST.2014.2366724","National Science Foundation through the Division of Electrical, Communications and Cyber Systems; Division of Computer and Network Systems; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964813","Decentralized control;multidomain traffic engineering (TE);optimization;quality of service;sliding mode control.;Decentralized control;multidomain traffic engineering (TE);optimization;quality of service;sliding mode control","Load management;Aggregates;IP networks;Protocols;Internet;Resource management;Decentralized control","convergence;distributed control;resource allocation;software defined networking;telecommunication control;telecommunication traffic","distributed traffic control;multidomain networks;integrated traffic control structure;per-edge-to-edge-based multinext-hop;multipath rate adaptation;load balancing;domain edge nodes;distributed control laws;provable convergence;optimality properties;user-defined global design objective;traffic engineering;fast failure recovery;FFR features;class-of-service;CoS-aware flow aggregates;point-to-multipoint multipath;control structure flexibility;control structure versatility;large-scale networking systems;software-defined networks;south-bound interface;OpenFlow;TE features","","3","32","","","","","","IEEE","IEEE Journals & Magazines"
"KPI signal analysis of the custom behavior in Kenting Spring Scream festival hot zone","Y. H. Chen; C. L. Chu; W. J. Lyu","Oriental Institute of Technology, Institute of Information and Communication Engineering, New Taipei city, Taiwan; Oriental Institute of Technology, Institute of Information and Communication Engineering, New Taipei city, Taiwan; Oriental Institute of Technology, Institute of Information and Communication Engineering, New Taipei city, Taiwan","2014 Fourth International Conference on Digital Information and Communication Technology and its Applications (DICTAP)","","2014","","","285","290","In this study, the speed test software Nemo Handy installed on an engineering-mode mobile phone was used to measure the key performance indicators (KPI) of circuit-switched (CS) and packet-switched (PS) signal parameters generated from Node B located around the venue of the Spring Scream festival held in Kenting, Taiwan between April 3, 2013 and April 7, 2013. The measured signal parameters were then processed using the postprocessing software of Nemo Outdoor and Nemo Analyze to examine the quality of the communication signals. The items of the KPI parameters that were analyzed included slope, standard deviation (SD), drop call, UL throughput, DL throughput, and web open time (web time). The resulting data were then compared with the internal data generated at the system end of the switching device to understand the behavior of mobile phone users in various environments. Based on the varying behavioral models, the parameters of the node B were adjusted to optimize the performance of the node B. During the mobility test, 1 drop call occurred out of the 55 call attempts, yielding a DCR of 1.818%. According to the Splunk software at the system end, the DCRs during circuit switching and packet switching remained below 2%. The connect success rate of the radio resource control (RRC) and radio access bearer (RAB) was maintained above 98% and 97%, respectively. This high success rate was sustained despite using the mobile phone during peak hours, indicating that the resources available at the system end are sufficient.","","978-1-4799-3724-0978-1-4799-3723","10.1109/DICTAP.2014.6821697","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821697","Circuit Switch;Packet Switch;Mobility;RSCP;Ec/No;DCR;RRC;RAB;UL throughput;DL throughput;web time","Mobile handsets;Throughput;Member and Geographic Activities Board committees;Mobile communication;Histograms;Springs;Software","indicators;mobile radio;packet switching;radio access networks;telecommunication computing;telecommunication control;telecommunication signalling","KPI signal analysis;custom behavior;Kenting spring scream festival hot zone;speed test software Nemo Handy;engineering-mode mobile phone;key performance indicators;circuit-switched signal parameters;packet-switched signal parameters;Node B;Taiwan;postprocessing software;Nemo Outdoor;Nemo Analyze;communication signals;standard deviation;drop call;UL throughput;DL throughput;Web open time;Web time;switching device;Splunk software;circuit switching;packet switching;radio resource control;RRC;radio access bearer;RAB","","","8","","","","","","IEEE","IEEE Conferences"
"Research on a hardware-in-the-loop simulation method for wireless network based on OPNET","J. Zhao; X. Zhu","Science and Technology on Information System, Engineering Laboratory, Nanjing, China; Nanjing Research Institute of Electronics Engineering, Nanjing, China","2015 IEEE Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","","2015","","","821","825","Hardware-in-the-loop (HITL) simulation method can effectively improve the accuracy and reliability of wireless network simulation, it can be used for hardware test, software test and performance optimization etc. This paper analyzes the deficiency of the OPNET's existing external interface, and presents a HITL simulation interface based on agent (HSIBA), which is used for realizing the protocol mapping from external simulation member to OPNET's wireless node. After the implement framework and key technology of HSIBA is given, this paper verifies that HSIBA can meet OPNET's needs in wireless network HITL simulation.","","978-1-4799-1980-2978-1-4799-1979-6978-1-4799-1978","10.1109/IAEAC.2015.7428671","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7428671","Wireless Network simulation;OPNET;HITL;SITL","Decision support systems;Analytical models;Wireless communication;Data models;Servers;Protocols;Physical layer","radio networks;telecommunication network reliability","hardware-in-the-loop simulation method;OPNET;wireless network simulation reliability;performance optimization;software test;hardware test;HITL simulation interface based on agent;HSIBA;protocol mapping","","2","9","","","","","","IEEE","IEEE Conferences"
"A metaheuristic based on simulation for stochastic Job-shop optimization","S. Kemmoé-Tchomté; D. Lamy; N. Tchernev","CRCGM EA 38 49, Université d'Auvergne, Clermont Ferrand, France; LIMOS UMR 6158, Université Blaise Pascal, Aubière, France; LIMOS UMR 6158, Université d'Auvergne, Aubière, France","2015 International Conference on Industrial Engineering and Systems Management (IESM)","","2015","","","108","116","This paper deals with stochastic Job-shop with random processing times where the objective is to find schedules robust enough in order to minimize the total completion time of all the operations. The problem is handled by use of a multi-start metaheuristic and a simulation software. At each iteration of the metaheuristic the best deterministic schedule is tested using the SIMAN simulation language; at this given schedule is then adjoined the average simulated makespan. The metaheuristic is then searching for another different schedule in the deterministic space which could be worse than the previous one. The proposed approach is applied to a small instance for demonstration. A set of instances for job-shop problems is then adapted for stochastic use in order to validate this work. The results, both in term of computational time and of quality, show the relevance of this study.","","978-2-9600-5326-5978-2-9600-5325","10.1109/IESM.2015.7380144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7380144","stochastic Job-shop;metaheuristic;simulation;random processing times","Stochastic processes;Optimization;Schedules;Robustness;Computational modeling;Electric breakdown;Mathematical model","computational complexity;job shop scheduling;minimisation;random processes;search problems;stochastic programming","stochastic job-shop optimization;random processing times;robust schedules;total completion time minimization;multistart metaheuristics;simulation software;deterministic scheduling;SIMAN simulation language;average simulated makespan;schedule search;computational time;quality factor","","1","32","","","","","","IEEE","IEEE Conferences"
"A Simple but Powerful Heuristic Method for Accelerating<formula formulatype=""inline""><tex Notation=""TeX"">$k$</tex></formula>-Means Clustering of Large-Scale Data in Life Science","K. Ichikawa; S. Morishita","Department of Computational Biology, Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Japan; Department of Computational Biology, Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa, Japan","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2014","11","4","681","692","K-means clustering has been widely used to gain insight into biological systems from large-scale life science data. To quantify the similarities among biological data sets, Pearson correlation distance and standardized Euclidean distance are used most frequently; however, optimization methods have been largely unexplored. These two distance measurements are equivalent in the sense that they yield the same k-means clustering result for identical sets of k initial centroids. Thus, an efficient algorithm used for one is applicable to the other. Several optimization methods are available for the Euclidean distance and can be used for processing the standardized Euclidean distance; however, they are not customized for this context. We instead approached the problem by studying the properties of the Pearson correlation distance, and we invented a simple but powerful heuristic method for markedly pruning unnecessary computation while retaining the final solution. Tests using real biological data sets with 50-60K vectors of dimensions 10-2001 (~400 MB in size) demonstrated marked reduction in computation time for k = 10-500 in comparison with other state-of-the-art pruning methods such as Elkan's and Hamerly's algorithms. The BoostKCP software is available at http://mlab.cb.k.u-tokyo.ac.jp/~ichikawa/boostKCP/.","1545-5963","","10.1109/TCBB.2014.2306200","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6739991","Bioinformatics;clustering;mining methods and algorithms;optimization","Correlation;Euclidean distance;Bioinformatics;Clustering algorithms;Correlation coefficient;Bioinformatics;Computational biology","bioinformatics;data mining;optimisation;pattern clustering","powerful heuristic method;accelerating k-means clustering;biological systems;large-scale life science data;Pearson correlation distance;standardized Euclidean distance;optimization methods;k-initial centroids;real biological data sets;Elkan algorithms;Hamerly algorithms;BoostKCP software","Algorithms;Cluster Analysis;Computational Biology;Data Mining;Databases, Factual;Heuristics;Humans;Models, Biological","6","49","","","","","","IEEE","IEEE Journals & Magazines"
"Software tool for the analysis of components characteristic for ECG signal","M. Bernat; Z. Piotrowski","Faculty of Electronics, Military University of Technology, Warsaw, Poland; Faculty of Electronics, Military University of Technology, Warsaw, Poland","2015 22nd International Conference Mixed Design of Integrated Circuits & Systems (MIXDES)","","2015","","","104","109","The authors developed the software tool with user's interface for designing the complete processing steps of ECG signal: from the filtering, through transformation and decision-making logic up to RR intervals analysis. The algorithm configuration function is replaced by the verification mechanism for R waves recognition efficiency. The program enables to download and process (in one or two-channel system) the signals recorded in the data files, e.g. as MAT-File formats from the test bases and signals from the real equipment (VENTUS system). The project assumption was to create the tool for the optimization of ECG signal preparation process, selection and analysis of R waves, which was achieved. Moreover, the article gives the results of the implemented algorithm efficiency as for the recognition of R waves in the ECG biological signal.","","978-8-3635-7807-7978-8-3635-7806","10.1109/MIXDES.2015.7208490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208490","ECG;filtering;R detection","Electrocardiography;Databases;Filtering;Signal processing algorithms;Algorithm design and analysis;Heart rate;Lead","electrocardiography;filtering theory;medical signal processing;Rayleigh waves;software tools","software tool;components characteristic analysis;user interface;filtering;decision-making logic;RR intervals analysis;algorithm configuration function;verification mechanism;R waves recognition efficiency;MAT-file formats;VENTUS system;ECG signal preparation process;R waves selection;R waves analysis;ECG biological signal","","","46","","","","","","IEEE","IEEE Conferences"
"Evaluating architecture and compiler design through static loop analysis","Y. Kashnikov; P. de Oliveira Castro; E. Oseret; W. Jalby","Exascale Computing Research, University of Versailles, Versailles, France; Exascale Computing Research, University of Versailles, Versailles, France; Exascale Computing Research, University of Versailles, Versailles, France; Exascale Computing Research, University of Versailles, Versailles, France","2013 International Conference on High Performance Computing & Simulation (HPCS)","","2013","","","535","544","Using the MAQAO loop static analyzer, we characterize a corpus of binary loops extracted from common benchmark suits such as SPEC, NAS, etc. and several industrial applications. For each loop, MAQAO extracts low-level assembly features such as: integer and floating-point vectorization ratio, number of registers used and spill-fill, number of concurrent memory streams accessed, etc. The distributions of these features on a large representative code corpus can be used to evaluate compilers and architectures and tune them for the most frequently used assembly patterns. In this paper, we present the MAQAO loop analyzer and a characterization of the 4857 binary loops. We evaluate register allocation and vectorization on two compilers and propose a method to tune loop buffer size and stream prefetcher based on static analysis of benchmarks.","","978-1-4799-0838-7978-1-4799-0836-3978-1-4799-0837","10.1109/HPCSim.2013.6641465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6641465","Benchmarking and Assessment;Software Monitoring and Measurement;HPC Monitoring and Instrumentation;Modeling;Simulation and Evaluation Techniques","Registers;Benchmark testing;Vectors;Assembly;Measurement;Computer architecture;Ports (Computers)","feature extraction;optimising compilers;program control structures;program diagnostics;software architecture;storage management","prefetcher;loop buffer size;vectorization evaluation;register allocation evaluation;assembly patterns;large representative code corpus;low-level assembly feature extraction;benchmark suits;binary loops;MAQAO loop static analyzer;static loop analysis;architecture design evaluation;compiler design evaluation","","4","18","","","","","","IEEE","IEEE Conferences"
"Facilitating testing and debugging of Markov Decision Processes with interactive visualization","S. McGregor; H. Buckingham; T. G. Dietterich; R. Houtman; C. Montgomery; R. Metoyer","School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, 97331-5501, USA; Department of Forest Engineering, Oregon State University, Corvallis, 97331-5501, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, 97331-5501, USA; Department of Forest Engineering, Oregon State University, Corvallis, 97331-5501, USA; Department of Forest Engineering, Oregon State University, Corvallis, 97331-5501, USA; School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, 97331-5501, USA","2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)","","2015","","","53","61","Researchers in AI and Operations Research employ the framework of Markov Decision Processes (MDPs) to formalize problems of sequential decision making under uncertainty. A common approach is to implement a simulator of the stochastic dynamics of the MDP and a Monte Carlo optimization algorithm that invokes this simulator to solve the MDP. The resulting software system is often realized by integrating several systems and functions that are collectively subject to failures of specification, implementation, integration, and optimization. We present these failures as queries for a computational steering visual analytic system (MDPVIS). MDPVIS addresses three visualization research gaps. First, the data acquisition gap is addressed through a general simulator-visualization interface. Second, the data analysis gap is addressed through a generalized MDP information visualization. Finally, the cognition gap is addressed by exposing model components to the user. MDPVIS generalizes a visualization for wildfire management. We use that problem to illustrate MDPVIS.","","978-1-4673-7457-6978-1-4673-7456","10.1109/VLHCC.2015.7357198","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357198","","Fitting;Computational modeling;Sensitivity;Uncertainty;Visualization;Analytical models;Couplings","data acquisition;data analysis;data visualisation;decision making;interactive systems;Markov processes;mathematics computing;program debugging;program testing","Markov decision processes debugging;Markov decision processes testing;interactive visualization;sequential decision making;wildfire management;cognition gap;generalized MDP information visualization;data analysis gap;general simulator-visualization interface;data acquisition gap;visualization research gaps;MDPVIS;computational steering visual analytic system","","","34","","","","","","IEEE","IEEE Conferences"
"A Multirate Simulation Method for Large Timescale Systems Applied for Lifetime Simulations","S. Hmam; J. Olivier; S. Bourguet; L. Loron","NA; NA; NA; NA","2015 IEEE Vehicle Power and Propulsion Conference (VPPC)","","2015","","","1","6","This paper introduces an original approach of the modeling and simulation of multiphysics systems that exhibit a wide range of time scales. This approach will be illustrated by the simulation of the energy storage unit (ESU) of an all-electric ferry. The implementation of the models was performed using two different modeling approaches (causal and acausal) highlighting the main difficulties of modeling multiphysics systems. In order to optimize the sizing of the ESU considering its ageing, the system should be simulated for a lifetime of 20 years. For this purpose, a multirate method was developed to speed-up the simulation and the optimization by a factor of 100 or more.","","978-1-4673-7637-2978-1-4673-7636","10.1109/VPPC.2015.7352878","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7352878","","Mathematical model;Object oriented modeling;Aging;Computational modeling;Supercapacitors;Software packages","boats;electric vehicles;energy storage;life testing;optimisation","ESU sizing optimization;acausal modeling;causal modeling;all-electric ferry;ESU simulation;energy storage unit;multiphysics system modeling;multiphysics system simulation;lifetime simulations;large timescale systems;multirate simulation method","","1","15","","","","","","IEEE","IEEE Conferences"
"Performance Implications of SSDs in Virtualized Hadoop Clusters","S. Ahn; S. Park; J. Hong; W. Chang","NA; NA; NA; NA","2014 IEEE International Congress on Big Data","","2014","","","586","593","BigData manipulates a massive volume of data for which the traditional techniques are not effective. Apache Hadoop is currently a most popular software framework supporting BigData analysis. As the scale of Hadoop cluster grows larger, building Hadoop clusters in virtualized environment draws a great attention. However, the performance optimization of Hadoop cluster in virtualized environment is difficult because of the virtualization overhead. In this paper the performance implications of SSDs in virtualized Hadoop clusters is identified and the overhead of virtualization is shown to be minimized with SSDs. The study presented in this paper reveals that the main virtualization overhead is I/O bottleneck due to fragmented and randomized I/O workload aggravated by virtualization. However, SSDs are more tolerable to the workload than HDDs. As a result, the virtualization overhead with SSDs is much less than with HDDs. Also, in the case of SSDs, the virtualized Hadoop cluster sustains good performance regardless of the number of VMs.","2379-7703","978-1-4799-5057-7978-1-4799-5056","10.1109/BigData.Congress.2014.90","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906832","Hadoop;Virtualization;SSD;BigData;Cloud computing","Virtualization;Benchmark testing;Servers;Bandwidth;Virtual machine monitors;Performance evaluation;Degradation","Big Data;cloud computing;data analysis;input-output programs;public domain software;software performance evaluation;storage management;virtualisation","performance implications;SSD;virtualized Apache Hadoop clusters;BigData analysis;software framework;virtualization overhead;I-O bottleneck;I-O workload;cloud computing;solid-state drives","","1","21","","","","","","IEEE","IEEE Conferences"
"Video++, a modern image and video processing C++ framework","M. Garrigues; A. Manzanera","ENSTA-ParisTech, 828 Boulevard des Maréchaux, 91762 Palaiseau CEDEX, France; ENSTA-ParisTech, 828 Boulevard des Maréchaux, 91762 Palaiseau CEDEX, France","Proceedings of the 2014 Conference on Design and Architectures for Signal and Image Processing","","2014","","","1","6","We present in this paper Video++, a new framework targeting image and video applications running on multi-core processors. While offering a high expressive power, we show that it generates code running up to 32 times faster than the naive equivalents. Taking advantage of the new C++11/C++14 features, tools, we propose simple abstractions matching the performance of hand optimized code. This paper gives an overview of the library and demonstrates its efficiency with some benchmarks.","","979-10-92279-06","10.1109/DASIP.2014.7115639","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7115639","","Libraries;Image processing;Kernel;Containers;Streaming media;Standards;Benchmark testing","C++ language;program compilers;software libraries;video signal processing","Video++ framework;image processing;video processing;C++ framework;image applications;video applications;multicore processors;code generation;C++11 features;code optimization;C++14 features","","1","17","","","","","","IEEE","IEEE Conferences"
"Pushing to the top","A. Ivrii; A. Gurfinkel","IBM Research; Software Engineering Institute","2015 Formal Methods in Computer-Aided Design (FMCAD)","","2015","","","65","72","IC3 is undoubtedly one of the most successful and important recent techniques for unbounded model checking. Understanding and improving IC3 has been a subject of a lot of recent research. In this regard, the most fundamental questions are how to choose Counterexamples to Induction (CTIs) and how to generalize them into (blocking) lemmas. Answers to both questions influence performance of the algorithm by directly affecting the quality of the lemmas learned. In this paper, we present a new IC3-based algorithm, called QUIP1, that is designed to more aggressively propagate (or push) learned lemmas to obtain a safe inductive invariant faster. QUIP modifies the recursive blocking procedure of IC3 to prioritize pushing already discovered lemmas over learning of new ones. However, a naive implementation of this strategy floods the algorithm with too many useless lemmas. In QUIP, we solve this by extending IC3 with may-proof-obligations (corresponding to the negations of learned lemmas), and by using an under-approximation of reachable states (i.e., states that witness why a may-proof-obligation is satisfiable) to prune non-inductive lemmas. We have implemented QUIP on top of an industrial-strength implementation of IC3. The experimental evaluation on HWMCC benchmarks shows that the QUIP is a significant improvement (at least 2x in runtime and more properties solved) over IC3. Furthermore, the new reasoning capabilities of QUIP naturally lead to additional optimizations and new techniques that can lead to further improvements in the future.","","978-0-9835-6785-1978-1-5090-4151","10.1109/FMCAD.2015.7542254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542254","","Optimization;Algorithm design and analysis;Model checking;Cognition;Software engineering;Terminology;Benchmark testing","formal verification;reachability analysis;theorem proving","unbounded model checking;counterexample-to-induction;blocking lemmas;IC3-based algorithm;learned lemma pushing;learned lemma propagation;recursive blocking procedure;may-proof-obligations;reachable state under-approximation;may-proof-obligation;noninductive lemmas;HWMCC benchmarks;QUIP reasoning capabilities","","5","18","","","","","","IEEE","IEEE Conferences"
"Image Set Based Face Recognition Using Self-Regularized Non-Negative Coding and Adaptive Distance Metric Learning","A. Mian; Y. Hu; R. Hartley; R. Owens","School of Computer Science and Software Engineering, The University of Western Australia, Crawley, Australia; Paypal Innovation Team, Singapore; Research School of Engineering, Australian National University, Canberra, Australia; School of Computer Science and Software Engineering, The University of Western Australia, Crawley, Australia","IEEE Transactions on Image Processing","","2013","22","12","5252","5262","Simple nearest neighbor classification fails to exploit the additional information in image sets. We propose self-regularized nonnegative coding to define between set distance for robust face recognition. Set distance is measured between the nearest set points (samples) that can be approximated from their orthogonal basis vectors as well as from the set samples under the respective constraints of self-regularization and nonnegativity. Self-regularization constrains the orthogonal basis vectors to be similar to the approximated nearest point. The nonnegativity constraint ensures that each nearest point is approximated from a positive linear combination of the set samples. Both constraints are formulated as a single convex optimization problem and the accelerated proximal gradient method with linear-time Euclidean projection is adapted to efficiently find the optimal nearest points between two image sets. Using the nearest points between a query set and all the gallery sets as well as the active samples used to approximate them, we learn a more discriminative Mahalanobis distance for robust face recognition. The proposed algorithm works independently of the chosen features and has been tested on gray pixel values and local binary patterns. Experiments on three standard data sets show that the proposed method consistently outperforms existing state-of-the-art methods.","1057-7149;1941-0042","","10.1109/TIP.2013.2282996","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605601","Image set classification;face recognition;nonnegative coding;distance metric learning","Measurement;Face;Vectors;Image coding;Face recognition;Manifolds;Optimization","convex programming;face recognition;gradient methods;learning (artificial intelligence)","image set-based face recognition;self-regularized nonnegative coding;adaptive distance metric learning;robust face recognition;set distance;nearest set points;orthogonal basis vectors;nonnegativity constraint;self-regularization constraint;positive linear combination;convex optimization problem;accelerated proximal gradient method;linear-time Euclidean projection;optimal nearest points;query set;gallery sets;discriminative Mahalanobis distance;gray pixel values;local binary patterns","Algorithms;Artificial Intelligence;Biometric Identification;Databases, Factual;Face;Humans;Image Processing, Computer-Assisted;Principal Component Analysis","25","56","","","","","","IEEE","IEEE Journals & Magazines"
"A generic database benchmarking service","M. Kaufmann; P. M. Fischer; D. Kossmann; N. May","Systems Group, ETH Z&#x00FC;rich, Switzerland; Albert-Ludwigs-Universit&#x00E4;t Freiburg, Germany; Systems Group, ETH Z&#x00FC;rich, Switzerland; SAP AG, Walldorf, Germany","2013 IEEE 29th International Conference on Data Engineering (ICDE)","","2013","","","1276","1279","Benchmarks are widely applied for the development and optimization of database systems. Standard benchmarks such as TPC-C and TPC-H provide a way of comparing the performance of different systems. In addition, micro benchmarks can be exploited to test a specific behavior of a system. Yet, despite all the benefits that can be derived from benchmark results, the effort of implementing and executing benchmarks remains prohibitive: Database systems need to be set up, a large number of artifacts such as data generators and queries need to be managed and complex, time-consuming operations have to be orchestrated. In this demo, we introduce a generic benchmarking service that combines a rich meta model, low marginal cost and ease of use, which drastically reduces the time and cost to define, adapt and run a benchmark.","1063-6382;1063-6382","978-1-4673-4910-9978-1-4673-4909-3978-1-4673-4908","10.1109/ICDE.2013.6544923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544923","","Benchmark testing;Databases;Generators;Servers;Data models;Standards;Tuning","benchmark testing;database management systems;software performance evaluation","meta model;data generator;microbenchmark;TPC-H;TPC-C;database system optimization;database system development;generic database benchmarking service","","","13","","","","","","IEEE","IEEE Conferences"
"Automated Discovery and Maintenance of Enterprise Topology Graphs","T. Binz; U. Breitenbücher; O. Kopp; F. Leymann","NA; NA; NA; NA","2013 IEEE 6th International Conference on Service-Oriented Computing and Applications","","2013","","","126","134","Enterprise Topology Graphs (ETGs) represent a snapshot of the complete enterprise IT, including all its applications, processes, services, components, and their dependencies. In the past, ETGs have been applied in analysis, optimization, and adaptation of enterprise IT. But how to discover and maintain a complete, accurate, fresh, and fine-grained Enterprise Topology Graph? Existing approaches either do not provide enough technical details or do not cover the complete scope of Enterprise Topology Graphs. Although existing tools are able to discover valuable information, there is no means for seamless integration. This paper proposes a plug in-based approach and extensible framework for automated discovery and maintenance of Enterprise Topology Graphs. The approach is able to integrate various kinds of tools and techniques into a unified model. We implemented the proposed approach in a prototype and applied it to different scenarios. Due to the vital role of discovery plugins in our approach, we support plug in development with a systematic testing method and discuss the lessons we learned. The results presented in this paper enable new ways of enterprise IT optimization, analysis, and adaptation. Furthermore, they unlock the full potential of past research, which previously required manual modeling of ETGs.","2163-2871","978-1-4799-2702-9978-1-4799-2701","10.1109/SOCA.2013.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717295","Enterprise Topology Graph;Enterprise IT;Discovery;Maintenance;Crawling","Topology;Linux;Servers;Computer architecture;Semantics;Java;Maintenance engineering","business data processing;graph theory;optimisation;software architecture","automated discovery;enterprise topology graph;ETG;plug in-based approach;extensible framework;discovery plugins;systematic testing method;enterprise IT optimization;maintenance","","3","30","","","","","","IEEE","IEEE Conferences"
"Improvements the Seccomp Sandbox Based on PBE Theory","M. Bo; M. Dejun; F. Wei; H. Wei","NA; NA; NA; NA","2013 27th International Conference on Advanced Information Networking and Applications Workshops","","2013","","","323","328","Providing a safe computing condition to unknown user is a crucial task in the existing network computing, and usually we can use the sandbox technology to shield security issues, but the behavior of malicious-occupying the resource has not been well controlled in the sandbox. In this passage, permission rate to access the computational efficiency and accuracy can be available by improving the Linux Kernel Secure Computing Mode(Seccomp) System, furthermore using the system calls judgment technology to prevent its malicious acts from user code can protect the system. During the calculations procedure, specifically, the improved Perfect Bayesian Equilibrium (PBE) Algorithm can be used to determine user behavior in system-call process, utilize this algorithm to construct policy engine, and use the engine decision-making engine to decide existing users' behavior as a result to maximize the profits of both the user code operating and server system capacity. Moreover agent technology that works in achieving the interrupted determination and interrupted access separate the computing and operating systems simultaneously. After all, improving sandbox technology is to achieve the relative optimization between the user service efficiency and security guarantees. Finally, the experiments show that compared with the Sandboxie and Buffer Zone technology, the proposed algorithm optimizes the consumption of the system resources in the original Seccomp Sandbox, and its access determine in rate also speeds up in the certain degree. In particular, it can effectively prevent special system call from malicious code, which can protect the system mainly in large extent. Moreover, the testing speed and the performance of several regular system calls such as file access operation, write operation also are under the progressive improvement.","","978-1-4673-6239-9978-0-7695-4952","10.1109/WAINA.2013.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6550418","Sandbox;Seccomp;Perfect Bayesian Equilibrium (PBE) algorithm;Virtualization Technology","Games;Security;Bayes methods;Kernel;Engines;Algorithm design and analysis","authorisation;Bayes methods;decision making;invasive software;Linux;optimisation","Seccomp sandbox;permission rate;profit maximization;network computing;write operation;file access operation;testing speed improvement;system call performance improvement;malicious code;system resource consumption optimization;user security guarantees;user service efficiency;user code server system capacity;user code operating system capacity;agent technology;policy engine decision-making engine;user behavior determination;perfect Bayesian equilibrium algorithm;system protection;malicious act prevention;system call judgment technology;Linux kernel secure computing mode system;computational accuracy;computational efficiency;unknown user;safe computing condition;PBE theory","","","20","","","","","","IEEE","IEEE Conferences"
"Optimize design on quantum effect photo-detector array board-level packaging","Y. P. Ge; F. M. Guo; L. Ding","Laboratory of Polarized Material & Device, East China Normal University, Shanghai, China; Laboratory of Polarized Material & Device, East China Normal University, Shanghai, China; Laboratory of Polarized Material & Device, East China Normal University, Shanghai, China","Proceedings of the 20th IEEE International Symposium on the Physical and Failure Analysis of Integrated Circuits (IPFA)","","2013","","","556","559","Models of quantum effect photo-detector array towards MCM-package is established to systematically study the fatigue life of solder joint under different geometrical sizes, and stress of different material with temperature changed by softwares. Thermal-mechanical simulations are carried out on assembly model in order to positioning SED dissipated in solder joints and carrier during thermal cycles. Different temperature cycling tests are studied the complex warping effect in BGA. The influence of cycle life by different solder diameter and carrier thickness is studied in detail. At the same time, interactive visualization package model and coupling model is created. The heat conduction problem of pad, new TSV (Through Silicon Via) structure design is discussed. The crosstalk of “a hole with four lines” structure of TSV and transmission lines on the carrier is analyzed in details by ADS software.","1946-1550;1946-1542","978-1-4799-0480-8978-1-4799-1241","10.1109/IPFA.2013.6599222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6599222","MCM;Thermal Stress;Photo-detector;TSV(Through Silicon Via);Coupling;Crosstalk","Heating;Stress;Through-silicon vias;Reliability;Finite element analysis;Crosstalk;Integrated circuit modeling","packaging;photodetectors;three-dimensional integrated circuits","quantum effect;photodetector array;board-level packaging;MCM-package;fatigue life;solder joint;thermal-mechanical simulations;thermal cycles;warping effect;BGA;cycle life;carrier thickness;visualization package model;heat conduction problem;TSV;through silicon via structure;hole with four lines structure;ADS software","","","11","","","","","","IEEE","IEEE Conferences"
"Through glass via thermomechanical analysis: Geometrical parameters effect on thermal stress","A. Benali; M. Bouya; M. Faqir; A. El Amrani; M. Ghogho; A. Benabdellah","Engineering research department Rabat International University Technopolis-Sala al Jadida 11100, Morocco; Engineering research department Rabat International University Technopolis-Sala al Jadida 11100, Morocco; Engineering research department Rabat International University Technopolis-Sala al Jadida 11100, Morocco; Engineering research department Rabat International University Technopolis-Sala al Jadida 11100, Morocco; Engineering research department Rabat International University Technopolis-Sala al Jadida 11100, Morocco; Team Engineering Research, Innovation and Management of Industrial Systems Abdelamalik Essaadi University, FST Tanger, Morocco","2013 8th IEEE Design and Test Symposium","","2013","","","1","5","As the request for high preferment and reliable 3D Integrated Circuit (IC) packages is increasing. Investigations in this technology are accelerating with the aim to reduce both cost and size. Glass interposer is recently used in this domain, it represents an effective alternative to the silicon due to its low cost, ease manufacturing and low electrical parasitic effects and cross talk. Although, the study of the reliability of the Through Glass Via (TGV) and its most critical stress areas remains a major concern. This study is about the thermal stress simulation in the glass interposer used in a three dimensional (3D) miniaturized camera package, in which we are using the Finite Element Analysis (FEA) method within ANSYS software to analyze the thermal stress concentration areas and inspect the geometric parameters effect on the resulting stress in a single TGV 3D model. Results are relevant to the boundary conditions and material properties used through all simulations. This work can be implemented to optimize via geometry and the whole glass interposer used in the 3D packaging technology.","2162-0601;2162-061X","978-1-4799-3525","10.1109/IDT.2013.6727093","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727093","Through Glass Via(TGV);Interposer;3D Packaging;Thermal Stress;Finite Element Analysis (FEA)","Stress;Glass;Thermal stresses;Three-dimensional displays;Copper;Thermomechanical processes","finite element analysis;integrated circuit packaging;integrated circuit reliability;thermal stresses;three-dimensional integrated circuits","3D packaging technology;via geometry;boundary conditions;TGV 3D model;thermal stress concentration areas;ANSYS software;FEA method;finite element analysis method;3D miniaturized camera package;three dimensional miniaturized camera package;TGV reliability;cross talk;electrical parasitic effects;glass interposer;3D IC packages;3D integrated circuit package reliability;thermal stress simulation;geometrical parameter effect;through glass via thermomechanical analysis","","2","8","","","","","","IEEE","IEEE Conferences"
"Path loss modeling for Indian cities — Some observations","K. N. R. S. V. Prasad; H. K. Rath; A. Simha","TCS Networks Lab, Bangalore 560 066, India; TCS Networks Lab, Bangalore 560 066, India; TCS Networks Lab, Bangalore 560 066, India","2014 Twentieth National Conference on Communications (NCC)","","2014","","","1","6","This paper interprets drive test based signal measurements in both urban and semi-urban Indian scenario using high-end smart phones with open source applications and high-end spectrum analyzers integrated with planning and optimization tool provided by Tata Tele Services, a major Indian telecom operator. From the drive test results, we observe that the existing path loss models such as Hata, Okumura, COST-231-Hata and COST-231-Walfisch-Ikegami models are not accurate for Indian terrain conditions and a fresh study is required in this direction.","","978-1-4799-2361","10.1109/NCC.2014.6811314","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6811314","","Cities and towns;Computational modeling;Roads;Buildings;Drives;Analytical models;Propagation losses","optimisation;public domain software;smart phones;spectral analysers;telecommunication network planning","path loss modeling;Indian cities;drive test;signal measurements;semiurban Indian scenario;high-end smart phones;open source applications;high-end spectrum analyzers;planning;optimization;Tata Tele Services;Indian telecom operator;Okumura;COST-231-Hata","","2","16","","","","","","IEEE","IEEE Conferences"
"Accelerating Irregular Algorithms on GPGPUs Using Fine-Grain Hardware Worklists","J. Y. Kim; C. Batten","NA; NA","2014 47th Annual IEEE/ACM International Symposium on Microarchitecture","","2014","","","75","87","Although GPGPUs are traditionally used to accelerate workloads with regular control and memory-access structure, recent work has shown that GPGPUs can also achieve significant speedups on more irregular algorithms. Data-driven implementations of irregular algorithms are algorithmically more efficient than topology-driven implementations, but issues with memory contention and memory-access irregularity can make the former perform worse in certain cases. In this paper, we propose a novel fine-grain hardware work list for GPGPUs that addresses the weaknesses of data-driven implementations. We detail multiple work redistribution schemes of varying complexity that can be employed to improve load balancing. Furthermore, a virtualization mechanism supports seamless work spilling to memory. A convenient shared work list software API is provided to simplify using our proposed mechanisms when implementing irregular algorithms. We evaluate challenging irregular algorithms from the Lonestar GPU benchmark suite on a cycle-level simulator. Our findings show that data-driven implementations running on a GPGPU using the hardware work list outperform highly optimized software-based implementations of these benchmarks running on a baseline GPGPU with speedups ranging from 1.2 - 2.4× and marginal area overhead.","1072-4451;2379-3155","978-1-4799-6998","10.1109/MICRO.2014.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011379","","Kernel;Instruction sets;Benchmark testing;Optimization;Load management;Hardware;Heuristic algorithms","application program interfaces;benchmark testing;graphics processing units;multiprocessing systems;resource allocation;virtualisation","irregular algorithm acceleration;fine-grain hardware worklists;memory contention;memory-access irregularity;data-driven implementations;work redistribution schemes;load balancing improvement;virtualization mechanism;work spilling;shared worklist software API;irregular algorithm implementation;LonestarGPU benchmark suite;cycle-level simulator;hardware worklist;baseline GPGPU;marginal area overhead;general-purpose graphics-processing units","","19","30","","","","","","IEEE","IEEE Conferences"
"Novel electromagnetism-like mechanism method for multiobjective optimization problems","L. Han; S. Jiang; S. Lan","School of Computer Science and Technology, China University of Mining and Technology, Xuzhou 221116, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou 221116, China; School of Computer Science and Technology, China University of Mining and Technology, Xuzhou 221116, China","Journal of Systems Engineering and Electronics","","2015","26","1","182","189","As a new-style stochastic algorithm, the electromagnetism-like mechanism (EM) method gains more and more attention from many researchers in recent years. A novel model based on EM (NMEM) for multiobjective optimization problems is proposed, which regards the charge of all particles as the constraints in the current population and the measure of the uniformity of non-dominated solutions as the objective function. The charge of the particle is evaluated based on the dominated concept, and its magnitude determines the direction of a force between two particles. Numerical studies are carried out on six complex test functions and the experimental results demonstrate that the proposed NMEM algorithm is a very robust method for solving the multiobjective optimization problems.","1004-4132","","10.1109/JSEE.2015.00023","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7064084","electromagnetism-like mechanism (EM) method;multi-objective optimization problem;particle;Pareto optimal solutions","Electromagnetic measurements;Linear programming;Pareto optimization;Genetic algorithms","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"Function Based Benchmarks to Abstract Parallel Hardware and Predict Efficient Code Partitioning","I. Zgeras; J. Brehm; M. Akselrod","NA; NA; NA","26th International Conference on Architecture of Computing Systems 2013","","2013","","","1","13","To increase the performance of a program, developers have to parallelize their code due to trends in modern hardware development. Since the parallelization of source code is paired with additional programming effort, it is desirable to know if a parallelization would result in an advantage in performance before implementing it. This paper examines the use of benchmarks for estimating the performance gain looking at the parallelization of Population Based Algorithms (PBAs) like Genetic Algorithms (GAs) and Particle Swarm Optimization Algorithms (PSOs) to be implemented on multi- and many-cores. These benchmarks are named function based benchmarks due to their dependence on the PBAs' functions. Furthermore, the software-hardware mapping with the most performance gain is suggested.","","978-3-8007-3492","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6468871","","Benchmark testing;Performance gain;Statistics;Performance evaluation;Complexity theory;Market research;Particle swarm optimization","","","","","","","","","","","VDE","VDE Conferences"
"Multi-resolution rapid prototyping of vehicle cooling systems","M. Z. Pindera; Y. Sun; J. Malosse; S. R. Vosen","Computational Sciences, LLC, 8000 Madison Blvd. STE D-102, PMB351, AL 35758, USA; Computational Sciences, LLC, 8000 Madison Blvd. STE D-102, PMB351, AL 35758, USA; Computational Sciences, LLC, 8000 Madison Blvd. STE D-102, PMB351, AL 35758, USA; Computational Sciences, LLC, 8000 Madison Blvd. STE D-102, PMB351, AL 35758, USA","2014 IEEE Aerospace Conference","","2014","","","1","20","This paper describes an Open Architecture, multiresolution, distributed design and simulation tool for rapid analysis and optimization of a general class of complex systems, composed of coupled, interacting sub-systems or components. Such complexity is well exemplified by cooling systems in ground vehicles with hybrid powerplants, which typically consist of multiple sub-systems. Distributed, multi-resolution simulations sidestep these difficulties by: a) partitioning the system into interacting components that can be represented by models of varying levels of fidelity; and b) executing the component models in parallel, with each model exchanging information with the others in real time. The subsystems can be coupled to one another through the interactions of selected elements. Vehicle tests were conducted to evaluate the modeling approach. Prototyping and validation examples are given multi-component systems used for cooling of hybrid powerplants over a wide range of operating conditions. Simulation results were compared to experimental data gathered at component and vehicle levels. Most of the model predictions deviated from the test data by less than 5%. Results indicate that distributed multi-resolution simulations can significantly accelerate the analysis of flow-thermal processes in complex vehicle systems. Moreover, the approach allows coupling of different codes with different functionalities to obtain integrated results not possible with any one individual code.","1095-323X","978-1-4799-1622-1978-1-4799-5582","10.1109/AERO.2014.6836169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6836169","","Atmospheric modeling;Heat transfer;Temperature measurement;Electron tubes;Cooling;Heat engines","aerospace computing;cooling;design engineering;digital simulation;distributed processing;hybrid power systems;optimisation;software prototyping;space vehicle power plants;space vehicles","multiresolution rapid prototyping;vehicle cooling system;open architecture;distributed design;design optimization;coupled subsystem;interacting subsystem;ground vehicle test;component models;information exchange;modeling approach;multicomponent system;distributed multiresolution simulation tool;flow thermal process;complex vehicle systems","","","21","","","","","","IEEE","IEEE Conferences"
"User centered design practices in healthcare: A systematic review","M. Ghazali; N. A. Mat Ariffin; R. Omar","Software Engineering Department Faculty of Computing Universiti Teknologi Malaysia, Skudai; Software Engineering Department Faculty of Computing Universiti Teknologi Malaysia, Skudai; Graphics and Multimedia Department College of Information Technology Universiti Tenaga Nasional","2014 3rd International Conference on User Science and Engineering (i-USEr)","","2014","","","91","96","Effectiveness in modern healthcare and services and the optimization of processes and operational sequences must be designed from the perspective of the end user. This paper aims to identify and analyse the common practices of User Centred Design (UCD) that had been implemented in previous studies in the context of analysis, design and evaluation phases and provide an overview from the research findings as a reference for other researchers. Studies were manually searched via online databases and further analysed based on the predefined research questions. This study reviews 60 papers published in the field of healthcare services on adopted UCD approaches between 1992 and 2014 (May). The results show that although UCD approach has a history of 22 years, it is not yet a normalcy in Asia countries. Interview, prototyping and usability testing are the most common activities adopted in respect to the UCD phases.","","978-1-4799-5813-9978-1-4799-5812","10.1109/IUSER.2014.7002683","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7002683","user-centered design;healthcare;systematic literature review;UCD","Medical services;Interviews;User centered design;Informatics;Testing;Pervasive computing;Prototypes","health care;medical computing;user centred design","user centered design practices;health care services;end user perspective;UCD approach;interview activity;prototyping activity;usability testing activity","","","10","","","","","","IEEE","IEEE Conferences"
"Improving TLB Performance by Increasing Hugepage Ratio","T. Luo; X. Wang; J. Hu; Y. Luo; Z. Wang","NA; NA; NA; NA; NA","2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing","","2015","","","1139","1142","Linux supports transparent huge page since 2.6.38. It can automatically map huge pages. But this implementation fails to adjust to page alignment in memory allocation and thus cannot use huge page in some situations. The design is not efficient. Our work aims to increase huge page allocation, so as to improve the utilization ratio of huge page and overall performance. The experimental results show that the optimization almost reaches the upper bound of huge page utilization. This software approach delivers a notable performance improvement for a few benchmarks with moderate overhead in physical memory consumption.","","978-1-4799-8006","10.1109/CCGrid.2015.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7152606","Linux transparent hugepage;hugepage utilization ratio;memory manager","Kernel;Benchmark testing;Optimization;Memory management;Resource management;Linux;Upper bound","Linux;resource allocation;storage management","TLB performance;hugepage ratio;Linux;memory allocation;page alignment;huge page allocation;upper bound;software approach;physical memory consumption","","1","3","","","","","","IEEE","IEEE Conferences"
"Index-based join operations in Hive","M. Mofidpoor; N. Shiri; T. Radhakrishnan","Computer Science and Software Engineering Concordia University Montreal, Canada; Computer Science and Software Engineering Concordia University Montreal, Canada; Computer Science and Software Engineering Concordia University Montreal, Canada","2013 IEEE International Conference on Big Data","","2013","","","26","33","Indexing techniques are crucial for efficiency and scalability of processing queries over big data. Hive is a batch-oriented big data management engine that is well suited for data OLAP and data analysis applications. For very “selective” queries whose output sizes are a small fraction of the contributing data, the brute-force approach suffers from poor performance due to redundant disk I/O's or initiations of extra map operations. We make a first attempt and propose an index-based join technique to speed up the process and integrate it in Hive by mapping our design to the conceptual optimization flow. To evaluate the performance, we create and evaluate test queries on datasets generated using TPC-H benchmark. Our results indicate significant performance gain over relatively large data and/or highly selective queries having a two-way join and a single join condition.","","978-1-4799-1293","10.1109/BigData.2013.6691768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691768","Indexing Techniques;Map and Reduce functions;Join Operation;Hive;Hadoop","Optimization;Time factors;Indexing;Data structures;Information management;Data handling","data mining;indexing;query processing","index based join operations;Hive;Indexing techniques;query processing;batch oriented big data management engine;data analysis applications;data OLAP applications;selective queries;TPC-H benchmark","","4","11","","","","","","IEEE","IEEE Conferences"
"Hardware for precise in vivo pulse transmit time CNIBP tests","L. Peter; M. Cerny","Department of Cybernetics and Biomedical Engineering, VSB - Technical University of Ostrava, Ostrava, Czech Republic; Department of Cybernetics and Biomedical Engineering, VSB - Technical University of Ostrava, Ostrava, Czech Republic","2013 International Conference on Applied Electronics","","2013","","","1","4","Monitoring of changes in blood pressure is one the most commonly used medical methods. However as standard procedure it can only be done in discontinuous time intervals. It would be beneficial for diagnostic purposes if it was possible to monitor changes of blood pressure in continuous time intervals. Currently continuous measurement of blood pressure is only possible using invasive methods, which makes the measurement impractical for the doctors and uncomfortable for the patients. Physiologically there is connection between electrical and mechanical heart functions and blood pressure value. The article describes the method of measurement and ECG and PPG signal processing so as to obtain information about pulse wave transit related to value of blood pressure. This value is called pulse transit time - PTT. The measurement set up used for simultaneous receiving of all necessary biosignals and measurement software in LabView environment for signal processing of the acquired data are described here. This method can be a part of body sensor networks. Using this method will optimize the network device for capturing biosignals and current expansion monitored signals.","1803-7232","978-80-261-0165-9978-80-261-0166","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6636514","CNIBP;Blood pressure;EKG;PPG;PTT","Blood pressure;Biomedical monitoring;Pressure measurement;Electrocardiography;Sensors;Finite impulse response filters","biomedical equipment;blood pressure measurement;body sensor networks;electrocardiography;medical signal detection;medical signal processing;patient monitoring;photoplethysmography;virtual instrumentation","current expansion monitored signal;network device optimization;body sensor network;LabView environment;measurement software;simultaneous necessary biosignal acquisition;measurement set up;pulse transit time;pulse wave transit;PPG signal processing;ECG signal processing;blood pressure value;mechanical heart function;electrical heart function;physiological;invasive method;continuous blood pressure measurement;diagnostic purpose;discontinuous time interval;medical method;blood pressure change monitoring;precise in vivo pulse transmit time CNIBP test hardware","","","5","","","","","","IEEE","IEEE Conferences"
"Achieving 100,000,000 database inserts per second using Accumulo and D4M","J. Kepner; W. Arcand; D. Bestor; B. Bergeron; C. Byun; V. Gadepally; M. Hubbell; P. Michaleas; J. Mullen; A. Prout; A. Reuther; A. Rosa; C. Yee","MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.; MIT Lincoln Laboratory, Lexington, MA, U.S.A.","2014 IEEE High Performance Extreme Computing Conference (HPEC)","","2014","","","1","6","The Apache Accumulo database is an open source relaxed consistency database that is widely used for government applications. Accumulo is designed to deliver high performance on unstructured data such as graphs of network data. This paper tests the performance of Accumulo using data from the Graph500 benchmark. The Dynamic Distributed Dimensional Data Model (D4M) software is used to implement the benchmark on a 216-node cluster running the MIT SuperCloud software stack. A peak performance of over 100,000,000 database inserts per second was achieved which is 100× larger than the highest previously published value for any other database. The performance scales linearly with the number of ingest clients, number of database servers, and data size. The performance was achieved by adapting several supercomputing techniques to this application: distributed arrays, domain decomposition, adaptive load balancing, and single-program-multiple-data programming.","","978-1-4799-6233-4978-1-4799-6232","10.1109/HPEC.2014.7040945","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7040945","Accumulo;Hadoop;Big Data;Graph500;D4M;MIT SuperCloud","Databases;Servers;Arrays;Benchmark testing;Measurement;Optimization;Parallel processing","cloud computing;distributed databases;public domain software","database inserts;D4M;Apache Accumulo database;open source relaxed consistency database;government applications;Graph500 benchmark;dynamic distributed dimensional data model;216-node cluster running;MIT SuperCloud software stack;supercomputing techniques;distributed arrays;domain decomposition;adaptive load balancing;single program multiple data programming","","9","18","","","","","","IEEE","IEEE Conferences"
"Incentives for rescheduling residential electricity consumption to promote renewable energy usage","C. Akasiadis; K. Panagidi; N. Panagiotou; P. Sernani; A. Morton; I. A. Vetsikas; L. Mavrouli; K. Goutsias","Institute of Informatics and Telecommunications, N.C.S.R. "Demokritos"; Department of Informatics and Telecommunications, National and Kapodistrian, University of Athens; Department of Informatics and Telecommunications, National and Kapodistrian, University of Athens; Department of Information Engineering, Universita Politecnica delle Marche; Oak Ridge, National Laboratory; Institute of Informatics and Telecommunications, N.C.S.R. "Demokritos"; Institute of Informatics and Telecommunications, N.C.S.R. "Demokritos"; Institute of Informatics and Telecommunications, N.C.S.R. "Demokritos"","2015 SAI Intelligent Systems Conference (IntelliSys)","","2015","","","328","337","Managing energy consumption and production is a challenging problem and proactive balancing between the amount of electricity produced and consumed is needed. In this work, we examine mechanisms that give incentives to consumers to efficiently reschedule their demand, thus balancing the overall energy production and consumption. Viewing the smart grid as a MAS, each agent represents a consumer; this agent takes into account its user's preferences and proposes an optimal energy consumption plan via a gamified GUI. To implement this we propose a distributed architecture through which we give the incentives (either economic, or social); we test a number of pricing mechanisms and we develop a very fast agent optimization strategy. We also present experiments both from software simulations on real data and pilot tests with human participants: the simulations allow to evaluate the mechanisms and agents, whilst the gamified tests are useful to assess the usability of the GUI and the usefulness of the agent suggestions. With human subjects, we evaluated which type of incentives is more compelling: economic or social. Results validate that by using our agent optimization approach the performance of the smart grid can be improved, and that specific mechanisms allow better utilization of renewable sources.","","978-1-4673-7606-8978-1-4673-7605","10.1109/IntelliSys.2015.7361163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7361163","Demand-side Management;Serious Games;Multi-agent Systems","Data visualization;Energy management;Games;Green products;Informatics;Telecommunications;Production","digital simulation;energy consumption;graphical user interfaces;multi-agent systems;optimisation;power engineering computing;power generation scheduling;renewable energy sources;smart power grids","agent optimization approach;agent suggestions;software simulations;agent optimization strategy;pricing mechanisms;distributed architecture;gamified GUI;optimal energy consumption plan;MAS;smart grid;energy production;energy consumption;renewable energy usage;residential electricity consumption rescheduling incentives","","","31","","","","","","IEEE","IEEE Conferences"
"Locational Load Shedding Marginal Pricing","A. G. Tikdari; H. Bevrani; M. Rashidi-Nejad; M. Montazeri","Oil &amp; Gas Dept., Kerman Tablo Corp. (KTC) Kerman, Iran; Electrical and Computer Engineering Dept., University of Kurdistan, Sanandaj, Iran; Electrical and Electronic Engineering Dept., Shahid Bahonar University of Kerman, Kerman, Iran; Electrical and Electronic Engineering Dept., Kerman Graduated University of High Technology, Kerman, Iran","2015 23rd Iranian Conference on Electrical Engineering","","2015","","","1522","1526","This paper presents the Locational Load Shedding Marginal Pricing (LSMP), a useful index for market-based real time intelligent load shedding problems. This parameter is well-described by introducing the load shedding program as an optimization problem very similar to, but in inverse vision, to Economic Dispatch (ED) problem. The load shedding is formulated as an optimal non-linear constrained problem. The LSMPs are calculated for some test systems and the simulation results are demonstrated as a function of active power. The LSMP curves that are finally shown demonstrate the practical advantages of the proposed index for real-time intelligent load shedding schemes.","2164-7054","978-1-4799-1972-7978-1-4799-1971","10.1109/IranianCEE.2015.7146461","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7146461","Load Shedding;Electric Market;LSMP;Intelligent Load Shedding","Generators;Optimization;Power systems;Pricing;Indexes;Computer aided software engineering;Companies","load dispatching;load shedding;nonlinear programming;power markets;pricing","locational load shedding marginal pricing;economic dispatch problem;ED problem;LSMP curve;real time intelligent load shedding problem;optimization problem;optimal nonlinear constrained problem;power market","","","12","","","","","","IEEE","IEEE Conferences"
"Research on the configuration method of mobile robot and its realization","Z. Song; Z. Wang; L. Chaoru; G. Caimeng","Fuzhou University Advanced Control Technology Research Center, Fuzhou, 350108, China; Fujian HISTRON Group Research Institute, Fuzhou, 350008, China; Fujian HISTRON Group Research Institute, Fuzhou, 350008, China; Fuzhou University Advanced Control Technology Research Center, Fuzhou, 350108, China","2013 25th Chinese Control and Decision Conference (CCDC)","","2013","","","2877","2883","The development mode of robotÿs control software has a close relationship with its operation management, dynamic reconfiguration, online optimization and other demands. Through taking control strategy realized by graphical configuration method as research object, industry automation universal technology platform (IAP technology) was adopted to build a technological environment used for mobile robot's control strategy configuration, which is based on graphical control configuration, visual computational process and component-based control algorithm. The algorithm principle and application method of several control configuration components are introduced in detail. And then, the enforcement mechanisms of relevant configuration components are verified through the design, development and test of mobile robot's obstacle avoidance strategy. Several experimental results show that IAP platform technology can be applied in robot's control field. Its graphical configuration components can largely reduce the complexity of developing robot's control software and improve the capability of real-time monitoring of robot's motion process.","1948-9439;1948-9447","978-1-4673-5534-6978-1-4673-5533-9978-1-4673-5532","10.1109/CCDC.2013.6561436","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6561436","Mobile robot;Control configuration;Componentization;System architecture;Obstacle avoidance control","Mobile robots;Process control;Control systems;Software;Programming;Automation","collision avoidance;control engineering computing;factory automation;mobile robots;object-oriented programming;process monitoring;real-time systems;software engineering","robot control software;operation management;dynamic reconfiguration;online optimization;industry automation universal technology platform;mobile robot control strategy configuration;graphical control configuration method;visual computational process;component-based control algorithm;enforcement mechanisms;relevant configuration component;mobile robot obstacle avoidance strategy;IAP platform technology;robot control field;real-time robot motion process monitoring","","1","6","","","","","","IEEE","IEEE Conferences"
"Practical use of the energy management system with day-ahead electricity prices","D. Lebedev; A. Rosin","Tallinn University of Technology, Tallinn, Estonia; Tallinn University of Technology, Tallinn, Estonia","2015 IEEE 5th International Conference on Power Engineering, Energy and Electrical Drives (POWERENG)","","2015","","","394","398","Our goal was to define a possible profit resulting from the use of batteries as an electric energy storage (EES) device in apartment buildings or small houses as typical households. Focus was on the theoretical and practical studies of an energy management system (EMS) and financial analysis of investments return. Use of a battery bank charge-discharge-schedule (BCDS) allows an optimal operation of EES. The benefits lie in importing and storing more energy at low price periods while decreasing the imported power from the grid at high price periods and dispatching the stored energy for load demands. The BCDS is optimized by help of the price and load forecast, at the same time guaranteeing load demand supply. Nord Pool Spot day-ahead market provides all required data for the EMS to evaluate the cost of imported energy in each period of time. The optimization algorithm creates the best BCDS to bring the highest profit to the end customer. The calculations were tested in a laboratory on the EMS to confirm the theoretical part.","2155-5532;2155-5516","978-1-4799-9978-1978-1-4673-7203-9978-1-4799-9977","10.1109/PowerEng.2015.7266349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7266349","battery storage;EES;energy management system;power system economics;smart grids;Time of Use energy price","Energy management;Batteries;Optimization;Investment;Software;US Department of Defense","battery storage plants;energy management systems;load forecasting;optimisation;power markets","energy management system;day-ahead electricity prices;electric energy storage device;apartment buildings;financial analysis;investments return;battery bank charge-discharge-schedule;optimal operation;load forecast;load demand supply;Nord Pool Spot day-ahead market;optimization algorithm","","1","11","","","","","","IEEE","IEEE Conferences"
"Software-Based Lightweight Multithreading to Overlap Memory-Access Latencies of Commodity Processors","C. Jiang; Y. Zhang; W. Zheng","NA; NA; NA","2015 44th International Conference on Parallel Processing","","2015","","","619","628","Emerging services applications operate on vast datasets that are kept in DRAM to minimize latency and to improve throughput. A considerable part of them have irregular memory references and then caused the serious locality issue. This paper presents a Software-based LIght weight Multithreading framework, SLIM, to conquer this problem for commodity hardware, which still keeps the simple style of multithreading programming. The principle is fairly straight: as issuing an irregular memory reference, the current fine-granularity thread uses some primitive of asynchronous memory-accesses and then switches itself out for others' execution to overlap long memory-latencies. Meanwhile, SLIM tries to maintain most contents of thread-contexts in the on-chip cache to reduce cache-misses. Therefore, the main challenge lies in how to improve the cache behavior at the expense of more instructions involved for context-switches and smaller cache-space left for applications. Consequently, we have proposed a corresponding performance model to guide the design, which is also verified by tests. Moreover, an optimized synchronization mechanism has been designed. For some classic irregular application, excessive tests have been carried out to explore the effects on performance of system configurations, including the aggressiveness of data-pre-fetch, the distribution of tasks among cores / CPUs, etc. Results show that it can achieve higher performance than the counterpart using traditional threads, under different data scales. Even compared to some tricky codes with manual optimizations, its performance is comparable and it has still reserved the simple programing manner of high-concurrency applications.","0190-3918","978-1-4673-7587","10.1109/ICPP.2015.71","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349617","multithreading;user-level threads;prefetch","Instruction sets;Multithreading;Hardware;Context;Programming","cache storage;DRAM chips;multi-threading","high-concurrency applications;task distribution;data prefetch aggressiveness;optimized synchronization mechanism;cache-misses;on-chip cache;thread-contexts;asynchronous memory-access;fine-granularity thread;memory reference;multithreading programming;commodity hardware;SLIM;software-based lightweight multithreading framework;DRAM;commodity processors;memory-access latencies","","","33","","","","","","IEEE","IEEE Conferences"
"Development of vibration energy harvester fabricated by rapid prototyping technology","O. Rubes; J. Smilek; Z. Hadas","Faculty of Mechanical Engineering, Brno University of Technology, Technick&#x00E1; 2896/2, 616 69, Czech Republic; Faculty of Mechanical Engineering, Brno University of Technology, Technick&#x00E1; 2896/2, 616 69, Czech Republic; Faculty of Mechanical Engineering, Brno University of Technology, Technick&#x00E1; 2896/2, 616 69, Czech Republic","Proceedings of the 16th International Conference on Mechatronics - Mechatronika 2014","","2014","","","178","182","This paper presents a modification of the electromagnetic vibration energy harvester for autonomous applications, which was developed at Brno University of Technology. New design of the device is proposed to improve mass distribution and magnetic circuits' parameters of the vibration energy harvester. Open source FEMM finite element analysis software is used to optimize both stiffness and excitation magnetic circuits, and CAD software is employed to design individual parts and obtain mechanical properties of the harvester. Calculated parameters are then applied in Matlab/Simulink model to evaluate the harvesting output power and to find an optimal coil and electrical load. A rapid prototyping technology is used to create a testing prototype with plastic and metallic parts, which is then subjected to measurements and validation of simulation results.","","978-80-214-4816-2978-80-214-4817","10.1109/MECHATRONIKA.2014.7018255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018255","energy harvesting;rapid prototyping;vibrations;magnetic circuit;electromagnetic transducer","Vibrations;Solid modeling;Magnetic circuits;Coils;Oscillators;Integrated circuit modeling;Energy harvesting","energy harvesting;finite element analysis;power engineering computing;public domain software;rapid prototyping (industrial);vibrations","rapid prototyping technology;electromagnetic vibration energy harvester;Brno University of Technology;mass distribution;magnetic circuit parameters;open source FEMM finite element analysis software;magnetic circuits;CAD software;Matlab-Simulink model","","6","12","","","","","","IEEE","IEEE Conferences"
"Hybrid NSGA-II of Three-Term Backpropagation network for multiclass classification problems","A. O. Ibrahim; S. M. Shamsuddin; N. B. Ahmad; M. N. M. Salleh","Soft Computing Research Group (SCRG), Faculty of Computing, Universiti Teknologi Malaysia (UTM), 81310 Skudai, Johor, Malaysia; Soft Computing Research Group (SCRG), Faculty of Computing, Universiti Teknologi Malaysia (UTM), 81310 Skudai, Johor, Malaysia; Soft Computing Research Group (SCRG), Faculty of Computing, Universiti Teknologi Malaysia (UTM), 81310 Skudai, Johor, Malaysia; Software and Multimedia Centre, Faculty of Computer Science and Infomation Technology, Universiti Tun Hussein Onn Malaysia, Parit Raja 86400 Batu Pahat, Johor, Malaysia","2014 International Conference on Computer and Information Sciences (ICCOINS)","","2014","","","1","6","Hybridization has become one of the current focuses of new research areas of the evolutionary algorithms over the past few years. Hybridization offers better speed of convergence to the evolutionary approach and better accuracy of the final solutions. This paper presents a hybrid non-dominated sorting genetic algorithm-II (NSGA-II) to optimize Three-Term Backpropagation (TBP) network in terms of two objectives which are: accuracy and complexity of the network. Backpropagation algorithm (BP) is often used as a local search algorithm and when combined with NSGA-II, the performance of NSGA II is enhanced due to the improvement of the individuals in the population. The experimental results show that the proposed method is effective in multiclass classification problems. The results of the hybrid approach to the classification problems are compared with multiobjective genetic algorithm based TBP network (MOGATBP) and some methods found in the literature. Moreover, the results indicate that the proposed method is a potentially useful classifier for enhancing classification process ability.","","978-1-4799-4390-6978-1-4799-4391-3978-1-4799-4392","10.1109/ICCOINS.2014.6868364","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6868364","Artificial Neural Network;Hybridization;Genetic algorithm;NSGA-II;Multiobjective optimization","Accuracy;Genetic algorithms;Training;Optimization;Artificial neural networks;Testing;Sociology","backpropagation;genetic algorithms;pattern classification;search problems","hybrid NSGA-II;three-term backpropagation network;multiclass classification problems;evolutionary algorithms;hybridization;evolutionary approach;hybrid nondominated sorting genetic algorithm-II;TBP;local search algorithm;classification problems;multiobjective genetic algorithm based TBP network;MOGATBP;classification process ability","","1","33","","","","","","IEEE","IEEE Conferences"
"Closed-loop analysis of soft decisions for serial links","C. A. Lansdowne; G. F. Steele; J. P. Zucha; A. M. Schlesinger","National Aeronautics and Space Administration, Houston, TX 77058 USA; National Aeronautics and Space Administration, Houston, TX 77058 USA; ITT at the National Aeronautics and Space Administration, Houston, TX 77058 USA; National Aeronautics and Space Administration, Houston, TX 77058 USA","IEEE International Conference on Wireless for Space and Extreme Environments","","2013","","","1","6","We describe the benefit of using closed-loop measurements for a radio receiver paired with a counterpart transmitter. We show that real-time analysis of the soft decision output of a receiver can provide rich and relevant insight far beyond the traditional hard-decision bit error rate (BER) test statistic. We describe a Soft Decision Analyzer (SDA) implementation for closed-loop measurements on single- or dual-(orthogonal) channel serial data communication links. The analyzer has been used to identify, quantify, and prioritize contributors to implementation loss in live-time during the development of software defined radios. This test technique gains importance as modern receivers are providing soft decision symbol synchronization as radio links are challenged to push more data and more protocol overhead through noisier channels, and software-defined radios (SDRs) use error-correction codes that approach Shannon's theoretical limit of performance.","","978-1-4799-2958","10.1109/WiSEE.2013.6737541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737541","block codes;channel coding;codes;satellite communication;test equipment","Receivers;Decoding;Clocks;Synchronization;Correlation;Correlators;Histograms","error correction codes;error statistics;radio links;radio receivers;radio transmitters;software radio","closed-loop analysis;serial links;closed-loop measurements;radio receiver;counterpart transmitter;real-time analysis;bit error rate;BER;soft decision analyzer;SDA;channel serial data communication links;software defined radios;noisier channels;error correction codes;SDR;Shannon theoretical limit","","1","9","","","","","","IEEE","IEEE Conferences"
"Reducing Compiler-Inserted Instrumentation in Unified-Parallel-C Code Generation","M. Alvanosl; J. N. Amaral; E. Tiotto; M. Farreras; X. Martorell","NA; NA; NA; NA; NA","2014 IEEE 26th International Symposium on Computer Architecture and High Performance Computing","","2014","","","270","277","Programs written in Partitioned Global Address Space (PGAS) languages can access any location of the entire address space via standard read/write operations. However, the compiler have to create the communication mechanisms and the runtime system to use synchronization primitives to ensure the correct execution of the programs. However, PGAS programs may have fine-grained shared accesses that lead to performance degradation. One solution is to use the inspector-executor technique to determine which accesses are indeed remote and which accesses may be coalesced in larger remote access operations. A straightforward implementation of the inspector-executor in a PGAS system may result in excessive instrumentation that hinders performance. This paper introduces a shared-data localization transformation based on linear memory descriptors (LMADs) that reduces the amount of instrumentation introduced by the compiler into programs written in the UPC language and describes a prototype implementation of the proposed transformation. A performance evaluation, using up to 2048 cores of a POWER 775 supercomputer, allows for a prediction that applications with regular accesses can achieve up to 180% of the performance of handoptimized versions while applications with irregular accesses yield performance gain from 1.12X up to 6.3X speedup.","1550-6533","978-1-4799-6905","10.1109/SBAC-PAD.2014.34","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6970674","","Runtime;Arrays;Prefetching;Benchmark testing;Electronics packaging;Marine animals;Optimization","C language;instrumentation;program compilers;software performance evaluation;software prototyping;synchronisation","compiler-inserted instrumentation;unified-parallel-C code generation;partitioned global address space languages;PGAS languages;read-write operations;program compiler;communication mechanisms;runtime system;synchronization primitives;PGAS programs;fine-grained shared accesses;inspector-executor technique;PGAS system;shared-data localization transformation;linear memory descriptors;LMAD;UPC language;prototype implementation;performance evaluation;POWER 775 supercomputer","","1","35","","","","","","IEEE","IEEE Conferences"
"Validation of an optimized algorithm to use Kinect in a non-structured environment for Sit-to-Stand analysis","E. Cippitelli; S. Gasparrini; S. Spinsante; E. Gambi; F. Verdini; L. Burattini; F. Di Nardo; S. Fioretti","Dipartimento di Ingegneria dell'Informazione, Universit&#x00E0; Politecnica delle Marche, Ancona I-60131, Italy; Dipartimento di Ingegneria dell'Informazione, Universit&#x00E0; Politecnica delle Marche, Ancona I-60131, Italy; Dipartimento di Ingegneria dell'Informazione, Universit&#x00E0; Politecnica delle Marche, Ancona I-60131, Italy; Dipartimento di Ingegneria dell'Informazione, Universit&#x00E0; Politecnica delle Marche, Ancona I-60131, Italy; Dipartimento di Ingegneria dell'Informazione, Universit&#x00E0; Politecnica delle Marche, Ancona I-60131, Italy; Dipartimento di Ingegneria dell'Informazione, Universit&#x00E0; Politecnica delle Marche, Ancona I-60131, Italy; Dipartimento di Ingegneria dell'Informazione, Universit&#x00E0; Politecnica delle Marche, Ancona I-60131, Italy; Dipartimento di Ingegneria dell'Informazione, Universit&#x00E0; Politecnica delle Marche, Ancona I-60131, Italy","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","5078","5081","The aim of this work is to obtain reliable kinematic measures relative to the execution of the Sit-to-Stand functional evaluation test, by low-cost and widely diffused instrumentation, that even non-experienced users can adopt in non-structured environments, like ambulatory or domestic settings. In particular, the paper refers to a low cost RGB-Depth sensor widely used in the gaming scenario like the Microsoft Kinect sensor. An algorithm is proposed that allows a reliable measure of human motion in a sagittal view. The performance of the proposed algorithm is compared to other two classic commercial algorithms. Results obtained by all the three algorithms have been compared to kinematic results obtained by the use of a stereophotogrammetric system that represents the gold-standard for kinematic measurement of human movement. Average errors of about 4 degrees, both for the trunk/leg angle and for the knee flexion/extension angle, have been obtained by the proposed algorithm and open the way to its possible adoption in non-clinical environments and further applications.","1094-687X;1558-4615","978-1-4244-9271-8978-1-4244-9270","10.1109/EMBC.2015.7319533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319533","","Joints;Algorithm design and analysis;Trajectory;Kinematics;Estimation;Approximation algorithms","biomechanics;image motion analysis;medical image processing;motion measurement;stereo image processing","knee flexion-extension angle;trunk-leg angle;human movement;stereophotogrammetric system;Microsoft Kinect sensor;RGB-depth sensor;sit-to-stand functional evaluation test","Algorithms;Humans;Knee Joint;Monitoring, Physiologic;Movement;Pattern Recognition, Automated;Software Validation;Torso","1","11","","","","","","IEEE","IEEE Conferences"
"Design of helical antenna for SAW passive wireless temperature sensor","X. Wang; Z. Chen; X. Huang; P. Ruan; L. Jiang","College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Automation Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, China","2013 Symposium on Piezoelectricity, Acoustic Waves, and Device Applications","","2013","","","1","4","A sensor antenna which is suitable for using as SAW passive wireless temperature measuring system has been designed based on the basic theory of the helical antenna in this paper. The size of the helical antenna has been identified. Firstly the modeling and optimization of the helical antenna have been carried out by using the HFSS software, thus the final dimension of the helical antenna with 433MHz resonant frequency and 300~500 MHz broadband has been determined. Then a impendence matching network between the antenna and the sensor has been designed by using the ADS software, which makes their input impendence conjugate. The simulation results are in good agreement with the measured ones. The voltage standing wave ratio (VSWR) of the antenna is less than 1.2 over the whole frequency range. Finally the sensor is wireless tested combining with the reader, showing the practicability of the sensor.","","978-1-4799-3288-7978-1-4799-3289","10.1109/SPAWDA.2013.6841104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6841104","Surface acoustic wave (SAW);Passive wireless temperature measuring;Helical antenna","","helical antennas;optimisation;surface acoustic wave devices;temperature sensors","helical antenna;SAW passive wireless temperature sensor;sensor antenna;SAW passive wireless temperature measuring system;HFSS software;impendence matching network;ADS software","","","5","","","","","","IEEE","IEEE Conferences"
"e-Learning Content Authentication Using Bipartite Matching","J. Dewan; M. Chowdhury; L. Batten","NA; NA; NA","2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing","","2013","","","51","55","Content authenticity and correctness is one of the important challenges in eLearning as there can be many solutions for one specific problem in the cyber space. Therefore, we feel the necessity of mapping problem to solutions using graph partition and weighted bipartite matching. This paper presents a novel architecture and methodology for a personal eLearning system called PELS that is developed by us. We also present an efficient algorithm to partition question-answer (QA) space and explore best possible solution to a particular problem. Our approach can be efficiently applied to social eLearning space where there is one-to-many and many-to-many relationship with a level of bonding. The main advantage of our approach is that we use QA ranking by adjusted edge weights provided by subject matter experts (SME) or expert database. Finally, we use statistical methods called confidence interval and hypothesis test on the data to check the reliability and dependability of the quality of results.","","978-0-7695-5005","10.1109/SNPD.2013.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598444","Bipartite Matching;Graph partitioning;Graph Clustering;eLearning;DRM (Digital Right Management);Confidence Interval;Hypothesis Test","Electronic learning;Bipartite graph;Educational institutions;Reliability;Social network services;Authentication","computer aided instruction;graph theory;message authentication;statistical analysis","eLearning content authentication;content correctness;graph partition;weighted bipartite matching;personal eLearning system;PELS;partition question-answer space;social eLearning space;QA ranking;subject matter expert;expert database;statistical method;confidence interval","","","19","","","","","","IEEE","IEEE Conferences"
"EvoAE -- A New Evolutionary Method for Training Autoencoders for Deep Learning Networks","S. Lander; Y. Shang","NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","2","","790","795","Although deep learning has achieved outstanding performances on several difficult machine learning applications, there are multiple issues that make its application on new problems difficult: speed of training, local minima, and manual selection of hyper-parameters. To overcome these problems, this paper proposes a new evolutionary method, EvoAE, to train auto encoders for deep learning networks. By evolving a population of auto encoders, EvoAE learns multiple features in each auto encoder in the form of hidden nodes, evaluates the auto encoders based on their reconstruction quality, and generates new auto encoders using crossover and mutation with chromosomes made up of hidden nodes and associated connections and weights. EvoAE optimizes network weights and structures of auto encoders simultaneously and employs a mini-batch variant, called Evo-batch, to speed up auto encoder search on large datasets. Furthermore, EvoAE supports different training methods in data partitioning and selection, requires little manual intervention, and reduces overall training time drastically over traditional methods on large datasets.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273701","autoencoder;neural networks;deep learning;evolutionary algorithm","Training;Backpropagation;Sociology;Statistics;Machine learning;Optimization;Testing","evolutionary computation;learning (artificial intelligence)","machine learning applications;evolutionary method;EvoAE;deep learning networks;reconstruction quality;crossover;mutation;chromosomes;network weights optimization;minibatch variant;Evo-batch;autoencoder search;large datasets;training methods;data partitioning;data selection","","6","10","","","","","","IEEE","IEEE Conferences"
"A fast algorithm for mining association rules in image","W. Z. Cheng; X. Li Xia","No.38 Research Institute, China Electronics Technology, Group Corporation Anhui Sun Create Electronics CO., LTD, Hefei 230031, China; School of Computer and Information, Hefei University of Technology, Hefei 230009","2014 IEEE 5th International Conference on Software Engineering and Service Science","","2014","","","513","516","Association rules have been used in data mining applications to capture relationships present among attributes in large data sets. It can be adapted to capture frequently occurring local structures in images. The frequency of occurrence of these structures can be used to characterize texture. In order to mine the frequency patterns of texture, each image can be considered as one transaction. If image data mining drills down to pixel level, each pixel or its neighborhood can be taken as transaction too, and data mining was processed in all the transactions. In textural image, the frequent patterns are texture cells in fact. Because of different size of texture cells, multi-levels and multi-resolution data mining can be accomplished. One texture image has many texture cells so that the texture combined association rule can represent the texture feature; and segmentation can be accomplished based on texture combined association rules, image. The experimental results demonstrated that the combined association rules can represent both regular and random texture perfectly. Many experiments testify that 3 degrading ranks and 3×3 mask or 4 degrading ranks and 2×2 mask can mine the combined association rules which can represent the image texture perfectly. Simulation results using images consisting of man made and natural textures showed that combined association rule features performed well compared to other widely used texture features.","2327-0594;2327-0586","978-1-4799-3279-5978-1-4799-3278-8978-1-4799-3277","10.1109/ICSESS.2014.6933618","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933618","data mining;textural Image;combined association rules","Association rules;Educational institutions;Image processing;Itemsets;Algorithm design and analysis;Testing","data mining;image segmentation;image texture","association rules;texture frequency patterns;image data mining;texture cells;multilevel data mining;multiresolution data mining;texture image;image segmentation","","2","10","","","","","","IEEE","IEEE Conferences"
"Highly-reliable integer matrix multiplication via numerical packing","I. Anarado; M. A. Anam; D. Anastasia; F. Verdicchio; Y. Andreopoulos","Electrical Engineering Department, University College London, Roberts Building, Torrington Place, London, WC1E 7JE, UK; Electrical Engineering Department, University College London, Roberts Building, Torrington Place, London, WC1E 7JE, UK; Electrical Engineering Department, University College London, Roberts Building, Torrington Place, London, WC1E 7JE, UK; School of Engineering, University of Aberdeen, King's College, Fraser Noble building, Aberdeen, AB24 3UE, UK; School of Engineering, University of Aberdeen, King's College, Fraser Noble building, Aberdeen, AB24 3UE, UK","2013 IEEE 19th International On-Line Testing Symposium (IOLTS)","","2013","","","19","24","The generic matrix multiply (GEMM) routine comprises the compute and memory-intensive part of many information retrieval, relevance ranking and object recognition systems. Because of the prevalence of GEMM in these applications, ensuring its robustness to transient hardware faults is of paramount importance for highly-efficientlhighly-reliable systems. This is currently accomplished via error control coding (ECC) or via dual modular redundancy (DMR) approaches that produce a separate set of “parity” results to allow for fault detection in GEMM. We introduce a third family of methods for fault detection in integer matrix products based on the concept of numerical packing. The key difference of the new approach against ECC and DMR approaches is the production of redundant results within the numerical representation of the inputs rather than as a separate set of parity results. In this way, high reliability is ensured within integer matrix products while allowing for: (i) in-place storage; (ii) usage of any off-the-shelf 64-bit floating-point GEMM routine; (iii) computational overhead that is independent of the GEMM inner dimension. The only detriment against a conventional (i.e. fault-intolerant) integer matrix multiplication based on 32-bit floating-point GEMM is the sacrifice of approximately 30.6% of the bitwidth of the numerical representation. However, unlike ECC methods that can reliably detect only up to a few faults per GEMM computation (typically two), the proposed method attains more than “12 nines” reliability, i.e. it will only fail to detect 1 fault out of more than 1 trillion arbitrary faults in the GEMM operations. As such, it achieves reliability that approaches that of DMR, at a very small fraction of its cost. Specifically, a single-threaded software realization of our proposal on an Intel i7-3632QM 2.2GHz processor (Ivy Bridge architecture with AVX support) incurs, on average, only 19% increase of execution time against an optimized, fault-intolerant, 32-bit GEMM routine over a range of matrix sizes and it remains more than 80% more efficient than a DMR-based GEMM.","1942-9398;1942-9401","978-1-4799-0664","10.1109/IOLTS.2013.6604045","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604045","integer matrix multiplication;sum-of-products;fault tolerance;soft errors;numerical packing","Error correction codes;Circuit faults;Fault detection;Fault tolerant systems;Redundancy","fault diagnosis;floating point arithmetic;matrix multiplication;microprocessor chips;multiprocessing systems","highly-reliable integer matrix multiplication;numerical packing;generic matrix multiply routine;GEMM;memory-intensive part;compute-intensive part;information retrieval systems;relevance ranking systems;object recognition systems;transient hardware faults;error control coding;dual modular redundancy approaches;fault detection;ECC approaches;DMR approaches;off-the-shelf 64-bit floating-point GEMM routine;Intel i7-3632QM 2.2GHz processor;single-threaded software realization","","3","18","","","","","","IEEE","IEEE Conferences"
"Improving Precision of Java Script Program Analysis with an Extended Domain of Intervals","A. Younang; L. Lu","NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","3","","441","446","Interpretation has been a promising approach for static analysis of Java Script programs. Static analysis is used for security auditing, debugging, optimization and error checking. Java Script is dynamically typed, uses prototype-based inheritance and first class functions. It supports reflective calls, access to object fields and allows object fields to be dynamically added and deleted. These dynamic features make Java Script flexible to use. At the same time, they make Java Script applications more susceptible to programming errors. The challenge that comes with the analysis of such programs is the design of abstract domains that will precisely track properties of interest without affecting performance. This paper presents our work on improving analysis precision of Java Script programs. We used an extended domain of intervals to track ranges of numeric values of variables. This is the first time interval domain has been applied to the analysis of the full Java Script language. We implemented the new abstract domain within a Java Script abstract interpreter. Our experiments show that the new abstract domain enables the abstract interpreter to infer more precise type information for most of the benchmark programs and strikes a good balance between analysis precision and cost. While the analysis of some benchmarks take more time as expected, some other benchmarks actually take less time.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.175","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273400","Java Script;static analysis;abstract interpretation;numeric abstract domain;interval analysis","Benchmark testing;Optimization;Concrete;Security;Browsers;Lattices;Sensitivity","formal verification;Java;program debugging;program diagnostics;program interpreters","Java script program analysis;static analysis;security auditing;debugging;optimization;error checking;prototype-based inheritance;dynamic feature;programming error;abstract domain;Java Script language;Java Script abstract interpreter;analysis precision","","1","36","","","","","","IEEE","IEEE Conferences"
"Latency and policy aware hierarchical partitioning for NFV systems","D. Krishnaswamy; R. Kothari; V. Gabale","IBM Research; IBM Research; IBM Research","2015 IEEE Conference on Network Function Virtualization and Software Defined Network (NFV-SDN)","","2015","","","205","211","This paper explores latency-aware and policy-aware optimized placement of virtual network functions across data centers in NFV systems. A hierarchy of distributed data centers is suggested to support network function software appliances with the flexibility to place functions based on performance requirements in the hierarchy, and different test scenarios for optimal VNF placement are studied. The paper discusses options for distributed function virtualization such as hierarchical partitioning, collapsing, replication, and north/south function splitting, to explore VNF placement options in NFV systems.","","978-1-4673-6884","10.1109/NFV-SDN.2015.7387428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387428","","Home appliances;Cloud computing;Hardware;Distributed databases;Virtualization","computer centres;virtual machines","policy aware hierarchical partitioning;NFV systems;policy-aware optimized placement;latency-aware optimized placement;virtual network functions;distributed data center hierarchy;network function software appliances;performance requirements;distributed function virtualization;hierarchical partitioning;collapsing;south function splitting;north function splitting;VNF placement options","","5","12","","","","","","IEEE","IEEE Conferences"
"Bytecode-to-C ahead-of-time compilation for Android Dalvik Virtual Machine","H. Oh; J. H. Yeo; S. Moon","Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea; Department of Electrical and Computer Engineering, Seoul National University, Seoul, Korea","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","1048","1053","Android employs Java for programming its apps which is executed by its own virtual machine called the Dalvik VM (DVM). One problem of the DVM is its performance. Its just-in-time compiler (JITC) cannot generate high-performance code due to its trace-based compilation with short traces and modest optimizations, compared to JVM's method-based compilation with ample optimziations. This paper proposes a bytecode-to-C ahead-of-time compilation (AOTC) for the DVM to accelerate pre-installed apps. We translated the bytecode of some of the hot methods used by these apps to C code, which is then compiled together with the DVM source code. AOTC-generated code works with the existing Android zygote mechanism, with corrects garbage collection and exception handling. Due to off-line, method-based compilation using existing compiler with full optimizations and Java-specific optimizations, AOTC can generate quality code while obviating runtime compilation overhead. For benchmarks, AOTC can improve the performance by 10% to 500%. When we compare this result with the recently-introduced ART, which also performs ahead-of-time compilation, our AOTC performs better.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092544","","Decision support systems;Automation;Europe;Cryptography;Androids;Humanoid robots;Registers","Java;operating systems (computers);program compilers;software performance evaluation;virtual machines","bytecode-to-C ahead-of-time compilation;android Dalvik virtual machine;Java;Dalvik VM;DVM;just-in-time compiler;JITC;high performance code;JVM method;ahead-of-time compilation;bytecode translation;C code;DVM source code;AOTC generated code;Android zygote mechanism;garbage collection;exception handling;Java specific optimizations","","","13","","","","","","IEEE","IEEE Conferences"
"Enabling Portable Optimizations of Data Placement on GPU","G. Chen; B. Wu; D. Li; X. Shen","North Carolina State University; Colorado School of Mines; University of California, Merced; North Carolina State University","IEEE Micro","","2015","35","4","16","24","Modern GPU memory systems manifest more varieties, increasing complexities, and rapid changes. Different placements of data on memory systems often cause significant differences in program performance. Most current GPU programming systems rely on programmers to indicate the appropriate placements, but finding the appropriate placements is difficult for programmers in practice owing to the complexity and fast changes of memory systems, as well as the input sensitivity of appropriate data placements--that is, the best placements often differ when a program runs on a different input data set. This article introduces a software framework called Porple. It offers a solution that, for the first time, makes it possible to automatically enhance data placement across a GPU. Through Porple, a GPU program's data gets placed appropriately on memory on the fly, customized to the current input dataset. Moreover, when new memory systems arrive, it can easily adapt the placements accordingly. Experiments on three types of GPU systems show that Porple consistently finds optimal or near-optimal placement, yielding up to 2.93 times (1.75 times average on three generations of GPU) speedups compared to programmers' decisions.","0272-1732;1937-4143","","10.1109/MM.2015.53","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7106396","GPU;cache;compiler;data placement;hardware specification language","Graphics processing units;Runtime;Benchmark testing;Computer programs;Memory;Complexity theory","computational complexity;data handling;graphics processing units;storage management","portable data placement optimization;modern GPU memory systems;complexity;program performance;GPU programming system;Porple software framework;near-optimal placement;programmer decision","","4","6","","","","","","IEEE","IEEE Journals & Magazines"
"Adaptive intra prediction filtering (AIPF)","Y. He; Q. Wang; X. You; D. Xu","Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Multimedia Software, Wuhan University, Wuhan, China; Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, China; Department of Electronics and Information Engineering, Huazhong University of Science and Technology, Wuhan, China","Proceedings 2014 IEEE International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)","","2014","","","358","362","Intra prediction is an important coding tool to exploit correlation within one picture in image and video compression. Before the ultimate intra prediction values are generated for current block along oblique angles, a fixed low-pass filtering with 3-tap filter (1, 2, 1) will be applied to the three prediction pixel values to avoid the effect of pulse noise. In this paper, we use adaptive intra prediction filter (AIPF) to replace the fixed filter to minimize the prediction errors. To get the adaptive filter coefficients in an on-line way with an acceptable accuracy and no coding overhead, we combine it with template matching (TM). After the best estimation of current block through template matching, the optimal adaptive filter coefficients are calculated with least-square optimization through considering the best estimation as `current' block. The adaptive filter is used to obtain intra prediction values instead of the 3-tap fixed low-pass filter. Experimental results show that the AIPF can get a stable coding gain on all test sequences, and reduce the bit-rate by up to 1.74% comparing with that using only TM.","","978-1-4799-5353-0978-1-4799-5352","10.1109/SPAC.2014.6982715","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6982715","adaptive;intra;prediction;filtering","","adaptive filters;data compression;image coding;image matching;least mean squares methods;optimisation;video coding","adaptive intraprediction filtering;AIPF;coding tool;image compression;video compression;ultimate intra prediction values;oblique angles;pulse noise;template matching;TM;optimal adaptive filter coefficients;least-square optimization;3-tap fixed low-pass filter;test sequences;stable coding gain","","2","11","","","","","","IEEE","IEEE Conferences"
"Identifying Code Phases Using Piece-Wise Linear Regressions","H. Servat; G. Llort; J. González; J. Giménez; J. Labarta","NA; NA; NA; NA; NA","2014 IEEE 28th International Parallel and Distributed Processing Symposium","","2014","","","941","951","Node-level performance is one of the factors that may limit applications from reaching the supercomputers' peak performance. Studying node-level performance and attributing it to the source code results into valuable insight that can be used to improve the application efficiency, albeit performing such a study may be an intimidating task due to the complexity and size of the applications. We present in this paper a mechanism that takes advantage of combining piece-wise linear regressions, coarse-grain sampling, and minimal instrumentation to detect performance phases in the computation regions even if their granularity is very fine. This mechanism then maps the performance of each phase into the application syntactical structure displaying a correlation between performance and source code. We introduce a methodology on top of this mechanism to describe the node-level performance of parallel applications, even for first-time seen applications. Finally, we demonstrate the methodology describing optimized in-production applications and further improving their performance applying small transformations to the code based on the hints discovered.","1530-2075","978-1-4799-3800-1978-1-4799-3799","10.1109/IPDPS.2014.100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877324","performance analysis;node-level performance;piece-wise linear regression;application tuning;sampling;instrumentation","Radiation detectors;Linear regression;Instruments;Benchmark testing;Biological system modeling;Frequency measurement","parallel processing;regression analysis;software performance evaluation;source code (software)","code phase identification;piecewise linear regressions;node-level performance;supercomputer peak performance;source code;coarse-grain sampling;performance phase detection;application syntactical structure;optimized in-production applications","","","28","","","","","","IEEE","IEEE Conferences"
"Heterogeneous dataflow architectures for FPGA-based sparse LU factorization","Siddhartha; N. Kapre","School of Computer Engineering, Nanyang Technological University, 50 Nanyang Avenue, S639798, Singapore; School of Computer Engineering, Nanyang Technological University, 50 Nanyang Avenue, S639798, Singapore","2014 24th International Conference on Field Programmable Logic and Applications (FPL)","","2014","","","1","4","FPGA-based token dataflow architectures with heterogeneous computation and communication subsystems can accelerate hard-to-parallelize, irregular computations in sparse LU factorization. We combine software pre-processing and architecture customization to fully expose and exploit the underlying heterogeneity in the factorization algorithm. We perform a one-time pre-processing of the sparse matrices in software to generate dataflow graphs that capture raw parallelism in the computation through substitution and reassociation transformations. We customize the dataflow architecture by picking the right mixture of addition and multiplication processing elements to match the observed balance in the dataflow graphs. Additionally, we modify the network-on-chip to route certain critical dependencies on a separate, faster communication channel while relegating less-critical traffic to the existing channels. Using our techniques, we show how to achieve speedups of up to 37% over existing state-of-the-art FPGA-based sparse LU factorization systems that can already run 3-4× faster than CPU-based sparse LU solvers using the same hardware constraints.","1946-147X;1946-1488","978-3-00-044645","10.1109/FPL.2014.6927401","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927401","","Benchmark testing;Computer architecture;Field programmable gate arrays;Sparse matrices;Hardware;Parallel processing;Optimization","data flow graphs;field programmable gate arrays;matrix decomposition;network-on-chip;reconfigurable architectures;sparse matrices","heterogeneous dataflow architectures;FPGA-based sparse LU factorization algorithm;FPGA-based token dataflow architectures;heterogeneous computation;communication subsystems;software pre-processing;architecture customization;sparse matrices;dataflow graphs;raw parallelism;multiplication processing elements;network-on-chip;communication channel;CPU-based sparse LU solvers;hardware constraints","","1","4","","","","","","IEEE","IEEE Conferences"
"Comparing Provisioning and Scheduling Strategies for Workflows on Clouds","M. E. Frincu; S. Genaud; J. Gossa","NA; NA; NA","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum","","2013","","","2101","2110","Cloud computing is emerging as a leading solution for deploying on-demand applications in both the industry and the scientific community. An important problem which needs to be considered is that of scheduling tasks on existing resources. Since clouds are linked to grid systems much of the work done on the latter can be ported with some modifications due to specific aspects that concern clouds, e.g., virtualization, scalability and on-demand provisioning. Two types of applications are usually considered for cloud migration: bag-of-tasks and workflows. This paper deals with the second case and investigates the impact virtual machine provisioning policies have on the scheduling strategy when various workflow types and execution times are used. Five provisioning methods are proposed and tested on well known workflow scheduling algorithms such as CPA, Gain and HEFT. We show that some correlation between the application characteristics and provisioning method exists. This result paves the way for adaptive scheduling in which based on the workflow properties a specific provisioning can be applied in order to optimize execution times or costs.","","978-0-7695-4979-8978-0-7695-4979","10.1109/IPDPSW.2013.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651116","workflow scheduling;virtual machine provisioning;cloud computing","Resource management;Processor scheduling;Job shop scheduling;Clouds;Cloud computing;Schedules","adaptive scheduling;cloud computing;grid computing;optimisation;processor scheduling;task analysis;virtual machines;workflow management software","cost optimization;adaptive scheduling;workflow scheduling algorithm;workflow type;execution time optimisation;virtual machine provisioning policy;bag of task;cloud migration;grid system;task scheduling;on-demand application;cloud computing","","9","24","","","","","","IEEE","IEEE Conferences"
"Optimization of non-uniform grid projection image super-resolution algorithms by reduced granularity and modified addressing","T. Szydzik; G. M. Callico; A. Nunez; F. Tobajas; R. Sarmiento; E. Quevedo","Institute for Applied Microelectronics (IUMA) University of Las Palmas de Gran Canaria (ULPGC) Campus Universitario de Tafira, 35017 Las Palmas (Spain); Institute for Applied Microelectronics (IUMA) University of Las Palmas de Gran Canaria (ULPGC) Campus Universitario de Tafira, 35017 Las Palmas (Spain); Institute for Applied Microelectronics (IUMA) University of Las Palmas de Gran Canaria (ULPGC) Campus Universitario de Tafira, 35017 Las Palmas (Spain); Institute for Applied Microelectronics (IUMA) University of Las Palmas de Gran Canaria (ULPGC) Campus Universitario de Tafira, 35017 Las Palmas (Spain); Institute for Applied Microelectronics (IUMA) University of Las Palmas de Gran Canaria (ULPGC) Campus Universitario de Tafira, 35017 Las Palmas (Spain); Oceanic Platform of the Canary Islands, Ctra. de Tallarte s/n, 35200 Telde (Spain)","Design of Circuits and Integrated Systems","","2014","","","1","6","In this work, the factors of reduction of memory requirements and increase in memory traffic associated with the change from reference frame level (baseline algorithm) to macroblock-level for the Super-Resolution (SR) image restoration non-uniform grid projection algorithm are compared over combinations of algorithm parameter values. Then, based on the results of a study on the share of algorithm steps code in the total requirements, the code with larger room for optimization has been identified and optimized. The introduction of a new addressing scheme that takes advantage of the algorithms static characteristics have led to a state in which for 87 out of 96 tested combinations (QCIF frame format) the factor of memory occupancy reduction is greater than the factor of increase in memory traffic. This result opens a way for efficient hardware implementations.","","978-1-4799-5743","10.1109/DCIS.2014.7035527","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7035527","super-resolution;communications traffic evaluation;image reconstruction;macroblock-level flow","Memory management;Optimization;Image resolution;Algorithm design and analysis;Tunneling magnetoresistance;Mathematical model;Software","image resolution;image restoration","nonuniform grid projection image super-resolution algorithm;modified addressing;reduced granularity;memory requirements;memory traffic;SR image restoration nonuniform grid projection algorithm;addressing scheme;QCIF frame format;memory occupancy reduction factor","","","7","","","","","","IEEE","IEEE Conferences"
"A Hybrid Swarm-Based Approach to University Timetabling","C. W. Fong; H. Asmuni; B. McCollum","Department of Computer Sciences and MathematicsFaculty of Applied Sciences and Computing, Tunku Abdul Rahman University College, Segamat, Malaysia; Software Engineering DepartmentSoftware Engineering Research Group, Universiti Teknologi Malaysia, Johor Bahru, Malaysia; Department of Computer Science, Queen’s University Belfast, Belfast, U.K.","IEEE Transactions on Evolutionary Computation","","2015","19","6","870","884","This paper is concerned with the application of an automated hybrid approach in addressing the university timetabling problem. The approach described is based on the nature-inspired artificial bee colony (ABC) algorithm. An ABC algorithm is a biologically-inspired optimization approach, which has been widely implemented in solving a range of optimization problems in recent years such as job shop scheduling and machine timetabling problems. Although the approach has proven to be robust across a range of problems, it is acknowledged within the literature that there currently exist a number of inefficiencies regarding the exploration and exploitation abilities. These inefficiencies can often lead to a slow convergence speed within the search process. Hence, this paper introduces a variant of the algorithm which utilizes a global best model inspired from particle swarm optimization to enhance the global exploration ability while hybridizing with the great deluge (GD) algorithm in order to improve the local exploitation ability. Using this approach, an effective balance between exploration and exploitation is attained. In addition, a traditional local search approach is incorporated within the GD algorithm with the aim of further enhancing the performance of the overall hybrid method. To evaluate the performance of the proposed approach, two diverse university timetabling datasets are investigated, i.e., Carter's examination timetabling and Socha course timetabling datasets. It should be noted that both problems have differing complexity and different solution landscapes. Experimental results demonstrate that the proposed method is capable of producing high quality solutions across both these benchmark problems, showing a good degree of generality in the approach. Moreover, the proposed method produces best results on some instances as compared with other approaches presented in the literature.","1089-778X;1089-778X;1941-0026","","10.1109/TEVC.2015.2411741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057548","artificial bee colony;great deluge algorithm;university timetabling;metaheuristics;evolutionary algorithm;Artificial bee colony (ABC);evolutionary algorithm;great deluge (GD) algorithm;metaheuristics;university timetabling","Educational institutions;Search problems;Sociology;Statistics;Optimization;Convergence;Benchmark testing","educational institutions;evolutionary computation","hybrid swarm-based approach;university timetabling problem;nature-inspired artificial bee colony;ABC algorithm;biologically-inspired optimization approach;job shop scheduling problem;machine timetabling problem;great deluge algorithm;Carter examination timetabling dataset;Socha course timetabling dataset","","9","68","","","","","","IEEE","IEEE Journals & Magazines"
"Exploring tradeoffs between power and performance for a scientific visualization algorithm","S. Labasan; M. Larsen; H. Childs","University of Oregon; University of Oregon; Lawrence Berkeley Nat'l Lab, University of Oregon","2015 IEEE 5th Symposium on Large Data Analysis and Visualization (LDAV)","","2015","","","73","80","Power is becoming a major design constraint in the world of high-performance computing (HPC). This constraint affects the hardware being considered for future architectures, the ways it will run software, and the design of the software itself. Within this context, we explore tradeoffs between power and performance. Visualization algorithms themselves merit special consideration, since they are more data-intensive in nature than traditional HPC programs like simulation codes. This data-intensive property enables different approaches for optimizing power usage. Our study focuses on the isosurfacing algorithm, and explores changes in power and performance as clock frequency changes, as power usage is highly dependent on clock frequency. We vary many of the factors seen in the HPC context - programming model (MPI vs. OpenMP), implementation (generalized vs. optimized), concurrency, architecture, and data set - and measure how these changes affect power-performance properties. The result is a study that informs the best approaches for optimizing energy usage for a representative visualization algorithm.","","978-1-4673-8517","10.1109/LDAV.2015.7348074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7348074","","Clocks;Computer architecture;Data visualization;Benchmark testing;Runtime;Hardware;Data models","application program interfaces;clocks;data visualisation;message passing;parallel processing;power aware computing;scientific information systems","scientific visualization algorithm;design constraint;high-performance computing;software design;HPC programs;data-intensive property;power usage optimization;isosurfacing algorithm;clock frequency;programming model;MPI;OpenMP;generalized implementation;optimized implementation;concurrency control;hardware architecture;data set;energy usage optimization","","1","23","","","","","","IEEE","IEEE Conferences"
"JolokiaC++: Optimizing Irregular Accesses for GPGPU","V. Patel; S. Aggarwal; A. Karkare","NA; NA; NA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","583","590","We present JolokiaC++ a compiler framework to ease coding of irregular data applications on GPUs. The effectiveness of the compiler and runtime systems of JolokiaC++ is tested using three kernels IRREG, MOLDYN and NBF, executed on NVIDIA GPUs. We developed extensions for the generic parallel constructs that allow portable and efficient programming of codes with irregular accesses on the GPU. We present experimental results from compiling the kernels for execution on Fermi GTX 480, Tesla C1060 and Tesla K20c GPUs.","","978-1-4799-8937-9978-1-4799-8936","10.1109/HPCC-CSS-ICESS.2015.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336221","GPU;GPGPU;Irregular Accesses;Parallel Constructs","Graphics processing units;Arrays;Kernel;Runtime;Programming;Schedules;Algorithm design and analysis","graphics processing units;program compilers","JolokiaC++;irregular access optimization;GPGPU;compiler framework;irregular data applications;runtime systems;IRREG;MOLDYN;NBF;NVIDIA GPU;generic parallel constructs;code programming;Fermi GTX 480;Tesla C1060;Tesla K20c GPU","","","42","","","","","","IEEE","IEEE Conferences"
"Simulation-based optimization using simulated annealing for optimal equipment selection within print production environments","S. Rai; R. K. Ettam","Xerox Corporation, 800 Phillips Road, Webster, NY 14450 USA; Xerox Corporation, 800 Phillips Road, Webster, NY 14450 USA","2013 Winter Simulations Conference (WSC)","","2013","","","1097","1108","Xerox has invented, tested, and implemented a novel class of operations-research-based productivity improvement offerings that has been described in Rai et al. (2009) and was a finalist in the 2008 Franz Edelman competition. The software toolkit that enables the optimization of print shops is data-driven and simulation based. It enables quick modeling of complex print production environments under the cellular production framework. The software toolkit automates several steps of the modeling process by taking declarative inputs from the end-user and then automatically generating complex simulation models that are used to determine improved design and operating points. This paper describes the addition of another layer of automation consisting of simulation-based optimization using simulated-annealing that enables automated search of a large number of design alternatives in the presence of operational constraints to determine a cost-optimal solution. The results of the application of this approach to a real-world problem are also described.","0891-7736;1558-4305","978-1-4799-3950-3978-1-4799-2077","10.1109/WSC.2013.6721499","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6721499","","Production;Simulated annealing;Printing;Stochastic processes;Algorithm design and analysis;Computational modeling","cellular manufacturing;printing industry;production equipment;productivity;simulated annealing","simulation-based optimization;simulated annealing;optimal equipment selection;print production;Xerox;operations research-based productivity;software toolkit;cellular production;automated search;cost optimal solution","","1","17","","","","","","IEEE","IEEE Conferences"
"Lessons learned and challenges of developing the NATO air command and control information services","S. Aker; C. Audin; E. Lindy; L. Marcelli; J. Massart; Y. Okur","NATO NCI Agency: Capability Development C2 Services, North Atlantic Treaty Organization (NATO), The Hague, the Netherlands; NATO NCI Agency: Capability Development C2 Services, North Atlantic Treaty Organization (NATO), The Hague, the Netherlands; NATO NCI Agency: Capability Development C2 Services, North Atlantic Treaty Organization (NATO), The Hague, the Netherlands; NATO NCI Agency: Capability Development C2 Services, North Atlantic Treaty Organization (NATO), The Hague, the Netherlands; NATO NCI Agency: Capability Development C2 Services, North Atlantic Treaty Organization (NATO), The Hague, the Netherlands; NATO NCI Agency: Capability Development C2 Services, North Atlantic Treaty Organization (NATO), The Hague, the Netherlands","2013 IEEE International Systems Conference (SysCon)","","2013","","","791","800","The North Atlantic Treaty Organization (NATO) Communications and Information (NCI) Agency is responsible for procuring and maintaining systems that are aligned with NATO Alliance operational requirements and national agreements, and are interoperable, when appropriate, with national systems. In the current NATO environment, long lead items, such as obtaining nationally agreed to capability packages and financial investments, are now leaving less time to engineer complex solutions in a fluctuating financial and mission environment. In addition, NATO is challenged with fielding systems to operational and system administrative users provided by 28 allied nations. This presents challenges with language, data exchange, security issues, and training for users that may rotate back to their nation every three years. This unique NATO environment has forced Project Managers (PMs) and Technical Leads (TLs) to operate with constraints imposed by contracts built around traditional systems engineering waterfall methods. In contrast, system lifecycle short timelines demand engineering solutions using agile methods supported by iterative, user validation of the system fit for purpose and usability with regard to changing peace-time and war-time missions (International Security Assistance Force (ISAF), Libyan Operation Unified Protector (OUP), etc.). The NCI Agency will be fielding a new Air C2 information service (AirC2IS) in 2013. This system, AirC2IS, was partially installed for initial system validation 21 months after contract award and will be fielded to over 20 NATO sites 35 months after contract award. The system will replace an interim capability and offer a vast array of software functionalities, using a web-based design, including, but not limited to, air track management, shared early warning, air planning, theatre ballistic missile defense planning and monitoring, and collaborative tool integration. The system capabilities are being procured by NCI Agency and developed by an industry partner. The AirC2IS design phase utilized a Human Machine Interface (HMI) driven approach and the development phase an agile methodology with user validation of functionalities before formal testing. The overall systems engineering approach was tailored to reduce risks of system non-acceptance and ensure high usability and software fit for purpose, matching user requirements. This paper will present lessons learned in the procurement, development, and fielding of AirC2IS in the following areas: Project management of agile development in a traditional waterfall contract environment; Agile software development with a HMI driven approach; and Validation of systems optimizing mission flexibility.","","978-1-4673-3108-1978-1-4673-3107-4978-1-4673-3106","10.1109/SysCon.2013.6549974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549974","Air C2;Agile;HMI Driven;User Centric;User Validation;User Centered Design","Contracts;Conferences;Training;Standards;Project management;Usability","ballistics;command and control systems;contracts;groupware;Internet;man-machine systems;military computing;missiles;monitoring;planning;program testing;program verification;project management;software prototyping;user interfaces","NATO Air Command and Control Information Services;North Atlantic Treaty Organization Communications and Information Agency;NATO Alliance operational requirements;national agreements;project managers;technical leads;system lifecycle short timelines;agile methods;peace-time missions;war-time missions;International Security Assistance Force;Libyan Operation Unified Protector;OUP;ISAF;NCI Agency;Air C2 information service;contract award;Web-based design;air track management;shared early warning;air planning;theatre ballistic missile defense planning;theatre ballistic missile defense monitoring;collaborative tool integration;AirC2IS design phase;human machine interface driven approach;HMI driven approach;agile methodology;user validation;formal testing;system nonacceptance risk reduction;procurement;agile development;waterfall contract environment;system validation","","1","6","","","","","","IEEE","IEEE Conferences"
"Experimental study of rate-aware scheduling for 802.11n Wi-Fi network with legacy devices","G. Sun; J. Zhu; R. He; C. Xiao","School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China","2013 IEEE Global High Tech Congress on Electronics","","2013","","","46","49","In this paper, we investigate the scheduling issue in the coexistence scenarios of 802.11n devices and legacy devices. To guarantee compatibility with legacy devices such as 802.11b/g devices in the ISM bands, IEEE 802.11n protocols allow low-rate devices access wireless medium with the equal priority. However, the network system capacity decreases and user-experience of 802.11n clients degrades significantly with the introduction of legacy devices. In this paper, our aim is to improve user-experience and network capacity of 802.11n devices in case of the negative impact brought by the 802.11 legacy devices in low rate transmission mode. The main contribution in this paper is our proposed scheduling algorithm for a mixed Wi-Fi networking scenario provided with the method of identifying low-rate and high-rate packets. We implement this method on Atheros 9340 chipset with its Software Development Kit(SDK). We evaluate the performance with the Chariot testing tool to show the gains on our experimental prototype system.","","978-1-4799-3209","10.1109/GHTCE.2013.6767238","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6767238","Wi-Fi network;802.11n;scheduling","IEEE 802.11n Standard;Throughput;IEEE 802.11g Standard;Optimization;Scheduling algorithms;Downlink","IEEE standards;protocols;scheduling;telecommunication standards;wireless LAN","rate-aware scheduling;802.11n Wi-Fi network;legacy devices;802.11n devices;802.11b/g devices;ISM bands;IEEE 802.11n protocols;low-rate devices access wireless medium;network system capacity;Atheros 9340 chipset;Software Development Kit;Chariot testing","","","5","","","","","","IEEE","IEEE Conferences"
"Exploratory study to identify critical success factors penetration in ERP implementations","S. Nagpal; S. K. Khatri; P. K. Kapur","Amity Institute of Information Technology, Amity University Uttar Pradesh, NOIDA, India; Amity Institute of Information Technology, Amity University Uttar Pradesh, NOIDA, India; Amity International Business School, Amity University Uttar Pradesh, NOIDA, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","6","Over the years, Enterprise Resource Planning (ERP) systems have proved themselves as a practical answer to link all Enterprise-wide operations. Despite robust ERP software packages available in the market with heavy investments in their respective R&amp;D budgets, many ERP implementations fail, thus burning a big hole in corporate budgets. Past and current research delves into Critical Success Factors (CSF). In this study, we explore the penetration of CSF, that is, CSF usage for enhancement of efficiency and effectiveness, of ERP Implementations. In this respect, a research instrument in the form of structured questionnaire has been used to extradite information from the respondents working on ERP Implementations about the current usability of CSF in ERP Implementations. Hypotheses were formulated to determine the extent of CSF penetration, which were then tested by applying the Chi-Square Test for acceptance or rejection of Hypotheses.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014760","Critical Success Factors (CSF);Enterprise Resource Planning (ERP);Key Performance Indicators (KPI)","Sociology;Statistics;Project management;Organizations;Enterprise resource planning;Industries","enterprise resource planning","critical success factors penetration;ERP implementations;enterprise resource planning systems;ERP systems;enterprise-wide operations;ERP software packages;R&D budgets;corporate budgets;CSF usage;efficiency enhancement;effectiveness enhancement;CSF usability;CSF penetration;chi-square test","","3","11","","","","","","IEEE","IEEE Conferences"
"Discriminative Gabor Feature Selection for Hyperspectral Image Classification","L. Shen; Z. Zhu; S. Jia; J. Zhu; Y. Sun","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University Shenzhen, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Shenzhen Key Laboratory of Biomedical Engineering, College of Medicine, Shenzhen University, Shenzhen","IEEE Geoscience and Remote Sensing Letters","","2013","10","1","29","33","Three-dimensional Gabor wavelets have recently been successfully applied for hyperspectral image classification due to their ability to extract joint spatial and spectrum information. However, the dimension of the extracted Gabor feature is incredibly huge. In this letter, we propose a symmetrical-uncertainty-based and Markov-blanket-based approach to select informative and nonredundant Gabor features for hyperspectral image classification. The extracted Gabor features with large dimension are first ranked by their information contained for classification and then added one by one after investigating the redundancy with already selected features. The proposed approach was fully tested on the widely used Indian Pine site data. The results show that the selected features are much more efficient and can achieve similar performance with previous approach using only hundreds of features.","1545-598X;1558-0571","","10.1109/LGRS.2012.2191761","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6194995","Feature selection;Gabor wavelet;hyperspectral imagery classification","Feature extraction;Hyperspectral imaging;Accuracy;Support vector machines;Redundancy","Gabor filters;geophysical image processing;geophysical techniques;image classification","hyperspectral image classification;discriminative Gabor feature selection;three-dimensional Gabor wavelets;symmetrical-uncertainty-based approach;Markov-blanket-based approach;Indian Pine site data","","41","26","","","","","","IEEE","IEEE Journals & Magazines"
"DME/DME navigation using a single low-cost SDR and sequential operation","T. Jalloul; W. Ajib; O. A. Yeste-Ojeda; R. Landry; C. Thibeault","Université du Québec à Montréal, Montreal, Canada; Université du Québec à Montréal, Montreal, Canada; Laboratory of Specialized Embedded System, Navigation and Avionics (LASSENA), École de Technologie Supérieure, Montreal, Canada; Laboratory of Specialized Embedded System, Navigation and Avionics (LASSENA), École de Technologie Supérieure, Montreal, Canada; Department of Electrical Engineering, École de Technologie Supérieure, Montreal, Canada","2014 IEEE/AIAA 33rd Digital Avionics Systems Conference (DASC)","","2014","","","3C2-1","3C2-9","Federal Aviation Administration is initiating an Alternative Position, Navigation, and Timing (APNT) program to insure continuous services in the event of GNSS failure. One of the promising solutions is an Optimized Distance Measuring Equipment (DME) Network based on DME/DME navigation. In comparison to other proposed APNT architectures, such as DME pseudolite network and passive Wide-Area Multilateration, airline operators find DME/DME more attractive for backup, as this solution requires no change to avionics used by commercial aircraft. This paper presents the implementation of the DME/DME system on the low-cost, Software Defined Radio (SDR) Nutaq ZeptoSDR platform. The SDR technology was chosen for its flexibility and reconfiguration. In fact, the success of implementing the DME avionic in a SDR platform will provide opportunities to implement other technologies. With such a versatile platform many problems can be avoided in an aircraft like the excessive wiring. To enable DME/DME in a single radio SDR, a new concept: “Sequential DME/DME” is introduced. This paper describes the principle of operation of the system and its implementation. To evaluate the Sequential DME/DME in a laboratory environment, a test bench platform is developed using avionics certification equipment. In lab tests using this platform are conducted to evaluate the system performances and results are presented. The accuracy of the system is studied by examining distance and position measurement precision.","2155-7195;2155-7209","978-1-4799-5001-0978-1-4799-5002","10.1109/DASC.2014.6979451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6979451","","Transponders;Aerospace electronics;Aircraft;Global Positioning System;Aircraft navigation;Accuracy","aircraft navigation;avionics;distance measurement;satellite navigation;software radio","excessive wiring;position measurement precision;avionics certification equipment;laboratory environment;test bench platform;DME avionics;software defined radio Nutaq ZeptoSDR platform;commercial aircraft;DME pseudolite network;airline operators;passive wide-area multilateration;APNT architectures;optimized distance measuring equipment network;GNSS failure;alternative position-navigation and timing program;Federal Aviation Administration;sequential operation;single low-cost SDR technology;sequential DME-DME navigation","","1","14","","","","","","IEEE","IEEE Conferences"
"Heterogeneity-Aware Workload Placement and Migration in Distributed Sustainable Datacenters","D. Cheng; C. Jiang; X. Zhou","NA; NA; NA","2014 IEEE 28th International Parallel and Distributed Processing Symposium","","2014","","","307","316","While major cloud service operators have taken various initiatives to operate their sustainable data enters with green energy, it is challenging to effectively utilize the green energy since its generation depends on dynamic natural conditions. Fortunately, the geographical distribution of data enters provides an opportunity for optimizing the system performance by distributing cloud workloads. In this paper, we propose a holistic heterogeneity-aware cloud workload placement and migration approach, sCloud, that aims to maximize the system good put in distributed self-sustainable data enters. sCloud adaptively places the transactional workload to distributed data enters, allocates the available resource to heterogeneous workloads in each data enter, and migrates batch jobs across data enters, while taking into account the green power availability and QoS requirements. We formulate the transactional workload placement as a constrained optimization problem that can be solved by nonlinear programming. Then, we propose a batch job migration algorithm to further improve the system good put when the green power supply varies widely at different locations. We have implemented sCloud in a university cloud test bed with real-world weather conditions and workload traces. Experimental results demonstrate sCloud can achieve near-to-optimal system performance while being resilient to dynamic power availability. It outperforms a heterogeneity-oblivious approach by 26% in improving system good put and 29% in reducing QoS violations.","1530-2075","978-1-4799-3800-1978-1-4799-3799","10.1109/IPDPS.2014.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877265","Heterogeneious Workload;Placement and Migration;Distributed Sustainable Datacenters","Green products;Clouds;Power supplies;Optimization;Quality of service;System performance;Availability","cloud computing;computer centres;green computing;nonlinear programming;quality of service;software performance evaluation","cloud service operators;green energy;dynamic natural conditions;geographical distribution;system performance optimization;cloud workload distribution;heterogeneity-aware cloud workload placement;heterogeneity-aware cloud workload migration;sCloud;system goodput maximization;distributed self-sustainable datacenters;green power availability;QoS requirements;transactional workload placement;constrained optimization problem;nonlinear programming;batch job migration algorithm;system goodput improvement;green power supply;university cloud testbed;weather conditions;near-to-optimal system performance;dynamic power availability","","11","32","","","","","","IEEE","IEEE Conferences"
"Dalvik bytecode acceleration using Fetch/Decode Hardware Extension with hybrid Execution","S. Thongkaew; T. Isshiki; D. Li; H. Kunieda","Kunieda-Isshiki Laboratory, Department of Communications and Computer Engineering, Tokyo Institute of Technology; Kunieda-Isshiki Laboratory, Department of Communications and Computer Engineering, Tokyo Institute of Technology; Kunieda-Isshiki Laboratory, Department of Communications and Computer Engineering, Tokyo Institute of Technology; Kunieda-Isshiki Laboratory, Department of Communications and Computer Engineering, Tokyo Institute of Technology","2014 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)","","2014","","","375","378","The significant disadvantage of Android Operating System is Dalvik bytecode interpretation using Dalvik Virtual Machine (VM) [1], [2]. However there are many techniques [3] to improve the performance of VM. In this paper, we propose an alternative methodology which is ""Fetch/Decode Hardware Extension with hybrid Execution"". It is a particular hardware that specially designed to fetch and decode Dalvik bytecode directly. In the hybrid execution stage complex bytecodes will be emulated by optimized Dalvik bytecode handler software of the native processor but simple bytecodes will be executed on hardware of the native processor directly. The outstanding success key of our technique is the Dalvik handler software optimization which utilized the extended hardware to reduce the operation steps in the original handler software. The experimental results show the speed up improvements on the individual simple bytecodes, the individual complex bytecodes, the test Java program of simple bytecodes and complex bytecodes can be achieved up to 22×, 3×, 10.44× and 2.12× respectively.","","978-1-4799-5230","10.1109/APCCAS.2014.7032798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7032798","Dalvik processor;Dalvik hardware extension;Android;Virtual Machine acceleration","Hardware;Software;Java;Androids;Humanoid robots;Computer architecture;Registers","Android (operating system);Java;virtual machines","Dalvik bytecode acceleration;fetch/decode hardware extension;hybrid execution;Android operating system;Dalvik virtual machine;VM;optimized Dalvik bytecode handler software;native processor;individual simple bytecodes;individual complex bytecodes;test Java program","","","10","","","","","","IEEE","IEEE Conferences"
"Development of a High Power Density Motor Made of Amorphous Alloy Cores","T. Fan; Q. Li; X. Wen","Institute of Electrical Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Electrical Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Electrical Engineering, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Industrial Electronics","","2014","61","9","4510","4518","In this paper, the development of a permanent-magnet (PM) motor made of amorphous alloy (AA) core is presented. The motor is rated as 20 kW, and base speed is 2500 r/min. A massive produced interior PM motor used in electric vehicle traction application is chosen as the baseline motor. An optimization software package built based on Matlab platform is used to design the stator made of AA, while the rotor is preliminary designed with the help of analytical formulation aiming at lowest core loss in stator teeth and finally tuned by finite-element analysis. The optimized AA motor is 31% smaller in volume than the baseline motor while keeping the continuous power rating unchanged, which means 45% more in power density. To validate the mathematical model beneath the optimization software package, two prototype motors have been manufactured. The first motor is the AA motor, optimized in this paper, and the second motor has exactly the same lamination shape and structural design of motor 1 but its material is nonoriented silicon steel like the baseline motor, instead of AA. The test result show the efficiency of motor 1 is higher than the baseline motor in full speed range, and motor 2 is badly behaved in terms of efficiency. The observation from the test proves that simply replacing the silicon steel with AA does not work under all circumstances, and the optimization experience gained for the AA cannot be used directly in designing a silicon steel motor.","0278-0046;1557-9948","","10.1109/TIE.2013.2290766","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6671397","Amorphous alloy (AA);finite-element analysis (FEA);permanent-magnet (PM) motor","Silicon;Permanent magnet motors;Traction motors;Steel;Iron;Core loss","amorphous magnetic materials;cores;finite element analysis;optimisation;permanent magnet motors;rotors;stators","permanent-magnet motor;amorphous alloy core;AA core;interior PM motor;electric vehicle traction application;baseline motor;optimization software package;Matlab platform;rotor;core loss;stator teeth;finite-element analysis;continuous power rating;power density;prototype motors;nonoriented silicon steel;silicon steel motor","","30","13","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal spectrum decision with channel quality considered in cognitive radio subnets","L. Mingxue; H. Xiaoxin; X. Fanjiang","Science and Techonology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Science and Techonology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; Science and Techonology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China","2013 IEEE Globecom Workshops (GC Wkshps)","","2013","","","929","934","This paper formulates a new QoS model for a tree-based cognitive radio network (CRN) which is under test for communication in mountainous areas. The model takes spectrum quality into account, which results in a more complex spectrum decision involving 3-dimension information: cognitive users, spectrums and their quality for combination optimization making. An optimal algorithm with correctness and performance proofs is proposed to search the best solution among all possible candidates. Each candidate takes a form of extreme maximal bicliques (EMB) in a viewpoint of cognitive users and spectrums, or a user-spectrum space and the best one adds more rich features to EMB. Simulations with parameters coinciding with real application requirements and experiment data from many days of experiments are conducted, showing a real-time performance of the algorithm.","2166-0077","978-1-4799-2851","10.1109/GLOCOMW.2013.6825109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825109","cognitive radio network;tree cognitive radio network;spectrum decision;maximal biclique;extreme maximal biclique;channel quality","Quality of service;Topology;Conferences;Cognitive radio;Sensors;Throughput;Data models","cognitive radio;quality of service;radio networks;radio spectrum management;trees (mathematics)","user-spectrum space;EMB;extreme maximal bicliques;optimal algorithm;combination optimization making;cognitive users;3-dimension information;spectrum quality;tree-based cognitive radio network;QoS model;cognitive radio subnets;channel quality;optimal spectrum decision","","","10","","","","","","IEEE","IEEE Conferences"
"Exact Confidence Intervals for Channelized Hotelling Observer Performance in Image Quality Studies","A. Wunderlich; F. Noo; B. D. Gallas; M. E. Heilbrun","Center for Devices and Radiological Health, U.S. Food and Drug Administration, Silver Spring, MD, USA; Department of Radiology, University of Utah, Salt Lake City, UT, USA; Center for Devices and Radiological Health, U.S. Food and Drug Administration, Silver Spring, MD, USA; Department of Radiology, University of Utah, Salt Lake City, UT, USA","IEEE Transactions on Medical Imaging","","2015","34","2","453","464","Task-based assessments of image quality constitute a rigorous, principled approach to the evaluation of imaging system performance. To conduct such assessments, it has been recognized that mathematical model observers are very useful, particularly for purposes of imaging system development and optimization. One type of model observer that has been widely applied in the medical imaging community is the channelized Hotelling observer (CHO), which is well-suited to known-location discrimination tasks. In the present work, we address the need for reliable confidence interval estimators of CHO performance. Specifically, we show that the bias associated with point estimates of CHO performance can be overcome by using confidence intervals proposed by Reiser for the Mahalanobis distance. In addition, we find that these intervals are well-defined with theoretically-exact coverage probabilities, which is a new result not proved by Reiser. The confidence intervals are tested with Monte Carlo simulation and demonstrated with two examples comparing X-ray CT reconstruction strategies. Moreover, commonly-used training/testing approaches are discussed and compared to the exact confidence intervals. MATLAB software implementing the estimators described in this work is publicly available at http://code.google.com/p/iqmodelo/.","0278-0062;1558-254X","","10.1109/TMI.2014.2360496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6912001","Image quality assessment;linear discriminant analysis (LDA);Mahalanobis distance;model observers;noncentral F-distribution;noncentrality parameter","Signal to noise ratio;Observers;Vectors;Standards;Monte Carlo methods;Manganese;Mathematical model","computerised tomography;image reconstruction;mathematical analysis;mathematics computing;medical image processing;Monte Carlo methods;optimisation;probability","exact confidence intervals;channelized hotelling observer performance;image quality studies;task-based assessments;imaging system performance;mathematical model observers;imaging system development;optimization;medical imaging community;known-location discrimination tasks;reliable confidence interval estimators;CHO performance;point estimates;Mahalanobis distance;theoretically-exact coverage probabilities;Monte Carlo simulation;X-ray CT reconstruction strategies;commonly-used training-testing approaches;exact confidence intervals;MATLAB software","Algorithms;Computer Simulation;Discriminant Analysis;Humans;Image Processing, Computer-Assisted;Models, Biological;Monte Carlo Method;Phantoms, Imaging;Signal-To-Noise Ratio;Tomography, X-Ray Computed;Torso","11","49","","","","","","IEEE","IEEE Journals & Magazines"
"Development of Mobile Radiation Monitoring System Utilizing Smartphone and Its Field Tests in Fukushima","Y. Ishigaki; Y. Matsumoto; R. Ichimiya; K. Tanaka","Graduate School of Information Systems, University of Electro-Communications, Chofu, Japan; Department of Applied Physics and Physico-Informatics, Keio University, Yokohama, Japan; High Energy Accelerator Research Organization, Tsukuba, Japan; Graduate School of Information Systems, University of Electro-Communications, Chofu, Japan","IEEE Sensors Journal","","2013","13","10","3520","3526","We developed a series of inexpensive but accurate and mobile radiation detectors, which we named Pocket Geiger (POKEGA), to address the desire of ordinary people to own a radiation detector following the March 2011 Daiichi Nuclear Power Plant accidents in Fukushima, Japan. To reduce costs while maintaining accuracy and flexibility, we used a combination of a p-i-n photodiode detector connected to a smartphone via a microphone cable. The detector circuit design is optimized for simplicity and low cost, whereas the smartphone software application is tasked with handling the complex processing required. Furthermore, the device also used the GPS and networking capabilities of the smartphone for logging and data sharing. The <sup>137</sup>Cs measurement range for a POKEGA-equipped smartphone is approximately from 0.05 to 10 mSv/h, which covers most radiation levels measured in Japan. Approximately 12000 POKEGA units were shipped in the six months following its release, and 2000 users have joined a Facebook community where they report measurement results and discuss hardware and software improvements. In parallel, we have addressed practical problems for POKEGA, such as vibration noise, energy consumption, and operating temperature, by conducting field tests in the Fukushima evacuation zone. The POKEGA series has been improved by solving such issues. This article reports on a new style of pragmatic sensor networking methodology, from the aspects of emergency response engineering, open-sourced development, and consumer-generated measurements.","1530-437X;1558-1748;2379-9153","","10.1109/JSEN.2013.2272734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557456","Radiation detector circuits;radiation monitoring;wireless sensor networks;project management","PIN photodiodes;Noise;Mobile communication;Vibrations;Headphones;Detectors","Geiger counters;radiation;radiation monitoring;smart phones","mobile radiation monitoring system;field tests;mobile radiation detectors;Pocket Geiger;POKEGA;March 2011 Daiichi Nuclear Power Plant accidents;p-i-n photodiode detector;microphone cable;detector circuit design;smartphone software application;complex processing;data sharing;radiation levels;Facebook community;Fukushima evacuation zone;pragmatic sensor networking methodology;emergency response engineering;open sourced development;consumer generated measurements","","33","16","","","","","","IEEE","IEEE Journals & Magazines"
"Early Work on the Brain Patch, a Reflective Service for System of Systems Integration","K. L. Bellman; C. A. Landauer","NA; NA","2015 IEEE International Conference on Autonomic Computing","","2015","","","249","254","In previous work we've shown that reflection supports the integration of components and of self-optimization within a complex system. In this paper, we discuss some early work on how some of these capabilities could support a similar integration and conflict resolution among the members of a system of systems (SoS). We start with a brief overview of the Wrappings approach to reflection, the notion of a web of reflection, and the CARS test bed where we are developing our concepts. We then introduce some early work on a new reflective service, the Brain Patch which helps to integrate a system into a System of Systems (SoS) by being both a domain specific expert on the reason for the formation of the SoS and its goals, situation, operating rules, etc. And by also continually observing and building a model of the system assigned to it.","","978-1-4673-6971-8978-1-4673-6970","10.1109/ICAC.2015.73","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7266975","System of Systems integration;reflection;Wrappings;self-optimization component","Reflection;Wrapping;Monitoring;Context;Cognition;Complex systems;Games","data integration;expert systems;self-adjusting systems;software architecture;systems analysis","Brain Patch;system of systems integration;SoS;self-optimization;wrappings approach;CARS test bed;computational architectures for reflective systems;domain specific expert","","2","21","","","","","","IEEE","IEEE Conferences"
"Optimal expansion of linear system using generalized orthogonal basis","A. Mbarek; K. Bouzrara; H. Messaoud","Ecole Nationale d'Ing&#x00E9;nieurs de Monastir Universit&#x00E9; de Monastir Rue Ibn El Jazzar 5019 Monastir Tunisia; Ecole Nationale d'Ing&#x00E9;nieurs de Monastir Universit&#x00E9; de Monastir Rue Ibn El Jazzar 5019 Monastir Tunisia; Ecole Nationale d'Ing&#x00E9;nieurs de Monastir Universit&#x00E9; de Monastir Rue Ibn El Jazzar 5019 Monastir Tunisia","2013 International Conference on Electrical Engineering and Software Applications","","2013","","","1","6","This paper proposes a new method for determining the optimal pole of generalized orthonormal basis (GOB) which leads to an optimal system model developed on such basis. This optimal pole is obtained by solving a set of non linear equations. The proposed procedure is formulated for single-input/single-output (SISO) systems. The proposed algorithm is tested on simulations and good performances in term of approximation and calculus time are obtained.","","978-1-4673-6301-3978-1-4673-6302-0978-1-4673-6300","10.1109/ICEESA.2013.6578448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578448","Identification;General Orthonormal bases filters;SISO system;MISO system;poles;parameters","Mathematical model;Vectors;Estimation;Transfer functions;Biological system modeling;Equations;Optimization","approximation theory;linear systems;nonlinear equations;pole assignment","optimal expansion;linear system;generalized orthogonal basis;optimal pole;generalized orthonormal basis;GOB;optimal system model;nonlinear equations;single-input-single-output systems;SISO systems;approximation;calculus time","","1","5","","","","","","IEEE","IEEE Conferences"
"Scripting language constructs for dynamic parameterizations of PSO-GA hybrids","S. Masrom; S. Z. Z. Abidin; N. Omar","Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Perak, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia; Faculty of Computer and Mathematical Sciences, Universiti Teknologi MARA, Shah Alam, Malaysia","2015 International Symposium on Mathematical Sciences and Computing Research (iSMSC)","","2015","","","18","23","Responding to the difficulties of implementing meta-heuristics hybridization, this paper introduces a set of scripting language constructs for the rapid algorithm design and development focusing on the two well-known metaheuristics, namely Particle Swarm Optimization (PSO) and Genetic Algorithm (GA). Additionally, the PSO-GA hybrids have been embedded with dynamic parameterization. In this paper, the compiler specification and codes for developing the scripting language constructs are described. Then, based on the several algorithms of PSO-GA hybrids that have been developed with the scripting language constructs, the Line of Codes (LOC) are measured in order to test the easiness of the programming language. The results show that across all algorithms, the scripting language is anticipated to enable easy programming, which has been presented by the very less of LOC compared to the JAVA programming language.","","978-1-4799-7896-0978-1-4799-7897","10.1109/ISMSC.2015.7594021","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7594021","Scripting Language Constructs;PSO-GA;Hybridization;Dynamic Parameterizations;Rapid","Heuristic algorithms;Software tools;Algorithm design and analysis;Java;Software algorithms;Programming","genetic algorithms;Java;particle swarm optimisation","scripting language constructs;PSO-GA hybrid;particle swarm optimization;genetic algorithm;metaheuristics hybridization;dynamic parameterization;LOC;line of codes;JAVA programming language","","","24","","","","","","IEEE","IEEE Conferences"
"Variable speed drive-based pressure optimization of a pumping system comprising individual branch flow control elements","J. Tamminen; T. Ahonen; A. Kosonen; J. Ahola; J. Tolvanen","LAPPEENRANTA UNIVERSITY OF TECHNOLOGY, P.O. Box 20 20, FI-53851, Finland; LAPPEENRANTA UNIVERSITY OF TECHNOLOGY, P.O. Box 20 20, FI-53851, Finland; LAPPEENRANTA UNIVERSITY OF TECHNOLOGY, P.O. Box 20 20, FI-53851, Finland; LAPPEENRANTA UNIVERSITY OF TECHNOLOGY, P.O. Box 20 20, FI-53851, Finland; ABB DRIVES, P.O. Box 184, FI-00381, Helsinki, Finland","2014 16th European Conference on Power Electronics and Applications","","2014","","","1","11","Circulation pumping systems hold energy savings potential, which can be seized with intelligent control. In this paper, a novel variable speed drive based method for minimizing the required pump pressure of a pumping system that has individual branch flow control elements is introduced. The method can be used to save energy for example in central heating circulation applications. Moreover, the presented method is purely software-based and can be implemented using a variable speed drive. The operation of the method was verified with simulations and a laboratory test setup and compared with proportional pressure control.","","978-1-4799-3015","10.1109/EPE.2014.6910988","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6910988","Energy efficiency;Industrial application;System integration;Systems engineering;Variable speed drive","Valves;Energy efficiency;Steady-state;Variable speed drives;Pressure control;Heating;Minimization","energy conservation;flow control;intelligent control;optimisation;pressure control;pumping plants;variable speed drives","pressure control;central heating circulation applications;pump pressure;intelligent control;energy savings;circulation pumping systems;branch flow control;pressure optimization;variable speed drive","","5","14","","","","","","IEEE","IEEE Conferences"
"An Evaluation of Safety-Critical Java on a Java Processor","J. R. Rios; M. Schoeberl","NA; NA","2014 IEEE 17th International Symposium on Object/Component/Service-Oriented Real-Time Distributed Computing","","2014","","","276","283","The safety-critical Java (SCJ) specification provides a restricted set of the Java language intended for applications that require certification. In order to test the specification, implementations are emerging and the need to evaluate those implementations in a systematic way is becoming important. In this paper we evaluate our SCJ implementation which is based on the Java Optimized Processor JOP and we measure different performance and timeliness criteria relevant to hard real-time systems. Our implementation targets Level 0 and Level1 of the specification and to test it we use a series of micro benchmarks, an application-based benchmark, and a reduced set of a SCJ technology compatibility kit. We evaluate the accuracy of periods, linear-time memory allocation, aperiodicevent handling, dispatch latency for interrupts, context switch preemption latency, and synchronization.","1555-0885;2375-5261","978-1-4799-4430","10.1109/ISORC.2014.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6899160","Real-time systems;Embedded systems;Java;Safety-critical systems;Safety-critical Java;Java processor","Java;Real-time systems;Resource management;Benchmark testing;Memory management;Time measurement;Instruction sets","formal specification;Java;program processors;real-time systems;safety-critical software;synchronisation","safety-critical Java;Java processor;SCJ specification;Java language;certification;specification test;Java optimized processor;JOP;timeliness criteria;real-time systems;microbenchmarks;application-based benchmark;SCJ technology compatibility kit;linear-time memory allocation;aperiodicevent handling;dispatch latency;context switch preemption latency;synchronization","","1","24","","","","","","IEEE","IEEE Conferences"
"Designing an Optimized Alert System Based on Geospatial Location Data","K. Zeitz; R. Marchany; J. Tront","NA; NA; NA","2014 47th Hawaii International Conference on System Sciences","","2014","","","4159","4168","Optimizing responses to various crises is a critical task for the government, institutions, first responders, and everyone involved in public safety. This paper describes the design methodology of ongoing research for planning, implementing, and analyzing the operation of a software application for an optimized crisis alert system. Design decisions are based on previous research, system performance requirements, and adaptive strategies for situational changes. The development includes the creation of an application for smart phones driven by user geospatial location. In the event of a crisis, user location determines the formation of categories assigned a risk level based on proximity to the crisis location. Notifications are distributed to the immediate category followed by lower risk users further and further outward. Integration and testing will be done with the existing Virginia Tech alert system. This design will yield an optimized, reliable, and situation ally customized crisis notification system.","1530-1605","978-1-4799-2504","10.1109/HICSS.2014.514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6759116","Crisis and Emergency Management;Notification System;Geospatial Location","Geospatial analysis;Communication channels;Smart phones;Buildings;Personnel;System performance;Educational institutions","emergency management;geographic information systems;smart phones","geospatial location data;public safety;design methodology;planning;software application;optimized crisis alert system;design decisions;system performance requirements;adaptive strategies;situational changes;smart phones;user geospatial location;user location;risk level;crisis location proximity;Virginia Tech alert system;crisis notification system","","","23","","","","","","IEEE","IEEE Conferences"
"A Device for Emulating Cuff Recordings of Action Potentials Propagating Along Peripheral Nerves","R. Rieger; M. Schuettler; S. Chuang","Electrical Engineering Department, National Sun Yat-Sen University, Kaohsiung, Taiwan; Laboratory for Biomedical Microtechnology, Department of Microsystems Engineering-IMTEK, University of Freiburg, Germany; Electrical Engineering Department, National Sun Yat-Sen University, Kaohsiung, Taiwan","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2014","22","5","937","945","This paper describes a device that emulates propagation of action potentials along a peripheral nerve, suitable for reproducible testing of bio-potential recording systems using nerve cuff electrodes. The system is a microcontroller-based stand-alone instrument which uses established nerve and electrode models to represent neural activity of real nerves recorded with a nerve cuff interface, taking into consideration electrode impedance, voltages picked up by the electrodes, and action potential propagation characteristics. The system emulates different scenarios including compound action potentials with selectable propagation velocities and naturally occurring nerve traffic from different velocity fiber populations. Measured results from a prototype implementation are reported and compared with in vitro recordings from Xenopus Laevis frog sciatic nerve, demonstrating that the electrophysiological setting is represented to a satisfactory degree, useful for the development, optimization and characterization of future recording systems.","1534-4320;1558-0210","","10.1109/TNSRE.2014.2300933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6714581","Cuff electrode recording;device testing instrument;electroneurogram (ENG) emulator;microcontroller;nerve recording;propagation velocity","Electrodes;Resistance;Microcontrollers;Electric potential;Nerve fibers;Testing;Voltage measurement","bioelectric potentials;biomedical electrodes;microcontrollers;neurophysiology","cuff recording emulation;peripheral nerves;biopotential recording systems;nerve cuff electrodes;microcontroller-based stand-alone instrument;neural activity;nerve cuff interface;electrode impedance;action potential propagation characteristics;velocity fiber;electrophysiology","Action Potentials;Algorithms;Animals;Computer Simulation;Electric Stimulation;Electrodes;Models, Neurological;Peripheral Nerves;Sciatic Nerve;Software;Xenopus laevis","5","32","","","","","","IEEE","IEEE Journals & Magazines"
"Detecting Display Energy Hotspots in Android Apps","M. Wan; Y. Jin; D. Li; W. G. J. Halfond","NA; NA; NA; NA","2015 IEEE 8th International Conference on Software Testing, Verification and Validation (ICST)","","2015","","","1","10","Energy consumption of mobile apps has become an important consideration as the underlying devices are constrained by battery capacity. Display represents a significant portion of an app's energy consumption. However, developers lack techniques to identify the user interfaces in their apps for which energy needs to be improved. In this paper, we present a technique for detecting display energy hotspots - user interfaces of a mobile app whose energy consumption is greater than optimal. Our technique leverages display power modeling and automated display transformation techniques to detect these hotspots and prioritize them for developers. In an evaluation on a set of popular Android apps, our technique was very accurate in both predicting energy consumption and ranking the display energy hotspots. Our approach was also able to detect display energy hotspots in 398 Android market apps, showing its effectiveness and the pervasiveness of the problem. These results indicate that our approach represents a potentially useful technique for helping developers to detect energy related problems and reduce the energy consumption of their mobile apps.","2159-4848","978-1-4799-7125","10.1109/ICST.2015.7102585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7102585","","Color;Energy consumption;User interfaces;Power demand;Mobile communication;Smart phones;Monitoring","Android (operating system);mobile computing;power aware computing;user interfaces","display energy hotspots detection;user interfaces;mobile app;energy consumption;display power modeling;automated display transformation techniques;Android market apps","","14","51","","","","","","IEEE","IEEE Conferences"
"GPRS telemetry system for high-efficiency electric competition vehicles","A. G. Calderón; G. G. Ruiz; A. C. G. Bohórquez","Fundación Solitec, Dept. I+D, C\ Marea Baja Nº 19 29006 Málaga; Siemens AG. Dpt. Supply Chain Management; University of Malaga, Dpt. Electronics, 2.2.41","2013 World Electric Vehicle Symposium and Exhibition (EVS27)","","2013","","","1","7","It has been generated a complete wireless electronic telemetry system for experimental electric competition vehicles. Its purpose is to optimize the processes of monitoring, analysis and evaluation of this type of vehicles designed by multidisciplinary groups of engineers, scientists, professors and students belonging to universities or research centers. Participation in competitions allows testing relevant developments and innovations in this technical field under severe real usage conditions and the system proposed offers a tool for real-time monitoring that is vital to optimize these processes during tests and races.","","978-1-4799-3832","10.1109/EVS.2013.6914788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6914788","Project-based Education;Electric Racing Vehicles;Telemetry system;Wireless;GPRS","Telemetry;Vehicles;Monitoring;Batteries;Educational institutions;Software;Wireless communication","cellular radio;electric vehicles;electrical engineering education;packet radio networks;radiotelemetry;sport;student experiments;testing","GPRS telemetry system;high efficiency electric competition vehicles;wireless electronic telemetry system;multidisciplinary groups;usage conditions","","","11","","","","","","IEEE","IEEE Conferences"
"Dynamic Feature Selection for Machine-Learning Based Concurrency Regulation in STM","D. Rughetti; P. D. Sanzo; B. Ciciani; F. Quaglia","NA; NA; NA; NA","2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing","","2014","","","68","75","In this paper we explore machine-learning approaches for dynamically selecting the well suited amount of concurrent threads in applications relying on Software Transactional Memory (STM). Specifically, we present a solution that dynamically shrinks or enlarges the set of input features to be exploited by the machine-learner. This allows for tuning the concurrency level while also minimizing the overhead for input-features sampling, given that the cardinality of the input-feature set is always tuned to the minimum value that still guarantees reliability of workload characterization. We also present a fully heedged implementation of our proposal within the TinySTM open source framework, and provide the results of an experimental study relying on the STAMP benchmark suite, which show significant reduction of the response time with respect to proposals based on static feature selection.","1066-6192;2377-5750","978-1-4799-2729","10.1109/PDP.2014.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6787254","Software Transactional Memory;Performance Models;Concurrency;Performance Optimization","Concurrent computing;Proposals;Correlation;Instruction sets;Benchmark testing;Artificial neural networks;Reliability","concurrency control;feature selection;learning (artificial intelligence)","dynamic feature selection;machine-learning based concurrency regulation;STM;concurrent threads;software transactional memory;concurrency level;input-features sampling;TinySTM open source framework;STAMP benchmark suite;static feature selection","","2","14","","","","","","IEEE","IEEE Conferences"
"Lithium battery swollen detection based on computer vision","Yinyin Zhan; Jiwei Deng; Taihong Wang","Key Laboratory for Micro-Nano-Optoelectronic Devices of Ministry of Education, Hunan University, Changsha, 410082, China; Key Laboratory for Micro-Nano-Optoelectronic Devices of Ministry of Education, Hunan University, Changsha, 410082, China; Key Laboratory for Micro-Nano-Optoelectronic Devices of Ministry of Education, Hunan University, Changsha, 410082, China","2013 IEEE 4th International Conference on Software Engineering and Service Science","","2013","","","728","731","In the process of lithium battery production, swollen battery detection mostly depends on manual testing which is subjective and inefficient. A new method based on computer vision is proposed to detect and separate the swollen battery. The mainstream shapes of lithium batteries are square and cylinder. This work uses square shape battery as example. The work first use an improved bimodal histogram method to segment target area, and extract battery geometrical characteristics as feature vectors, then build the classification model using C-SVM. Genetic algorithm is adopted to optimize SVM parameters and k-fold validation strategy is used to evaluate it. Experiment shows that, the proposed method can achieve a recognition rate of 98.1481%, which provides an intelligent and efficient detection method for swollen battery automated separation in the production.","2327-0594;2327-0586","978-1-4673-5000-6978-1-4673-4997-0978-1-4673-4998","10.1109/ICSESS.2013.6615409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615409","lithium battery;swollen detection;machine vision;support vector machine","Manuals;Computers;Histograms;Gray-scale","computer vision;electrical engineering computing;genetic algorithms;image classification;image segmentation;secondary cells;support vector machines","swollen battery automated separation;k-fold validation strategy;genetic algorithm;C-SVM;classification model;feature vector extraction;battery geometrical characteristics;target area segmentation;bimodal histogram method;square shape battery;manual testing;lithium battery production process;computer vision;lithium battery swollen detection;Li","","","9","","","","","","IEEE","IEEE Conferences"
"Design and implementation of cigarette product quality management information system based on ERP and PDM","Yong Cen; Honglv Wang; Z. Zhang; H. Wang","Information Center, China Tobacco Zhejiang Industrial Co., Ltd, Hangzhou, 310009, China; Information Center, China Tobacco Zhejiang Industrial Co., Ltd, Hangzhou, 310009, China; Information Center, China Tobacco Zhejiang Industrial Co., Ltd, Hangzhou, 310009, China; Information Center, China Tobacco Zhejiang Industrial Co., Ltd, Hangzhou, 310009, China","2013 IEEE 4th International Conference on Software Engineering and Service Science","","2013","","","170","174","How the tobacco industry enterprises enhance cigarette product quality management by information systems, in order to achieve the purpose of quality assurance and improvement has become a new challenge. Combing and optimization of the quality management business processes, using NET platform design and implement of a cigarette product quality management information systems. This system using data acquisition client program to achieve real-time collection of the QTM test station data, and will automate decision-making judgment for cigarette product quality inspection. Meanwhile, SOA and SAP XI Enterprise bus technology to collaborate enterprise resource planning and product data management system with quality management information system, to build a collaborative quality management and quality of data unified management environment for sharing and transfer of the various information of product quality management, and enhance the efficiency of quality management in tobacco industry enterprise.","2327-0594;2327-0586","978-1-4673-5000-6978-1-4673-4997-0978-1-4673-4998","10.1109/ICSESS.2013.6615281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615281","tobacco industrial enterprises;cigarette products;quality management","XML;Inspection;Standards;Instruments;Web services;IP networks","data acquisition;decision making;enterprise resource planning;inspection;management information systems;product quality;production engineering computing;quality assurance;quality management;service-oriented architecture;tobacco industry","information transfer;information sharing;data quality;collaborative quality management;product data management system;enterprise resource planning;SAP XI Enterprise bus technology;SOA technology;cigarette product quality inspection;decision-making judgment;real-time QTM test station data collection;data acquisition client program;NET platform design;quality management business processes;quality improvement;quality assurance;tobacco industry enterprises;PDM;ERP;cigarette product quality management information system","","1","6","","","","","","IEEE","IEEE Conferences"
"Optimization of the picking sequence of an automated storage and retrieval system (AS/RS)","R. Dornberger; T. Hanne; R. Ryter; M. Stauffer","University of Applied Sciences and Arts Northwestern Switzerland, School of Business, Institute for Information Systems, Peter Merian-Str. 86, CH-4002 Basel, Switzerland; University of Applied Sciences and Arts Northwestern Switzerland, School of Business, Institute for Information Systems, Riggenbach-str. 16, CH-4600 Olten, Switzerland; University of Applied Sciences and Arts Northwestern Switzerland, School of Business, Institute for Information Systems, Riggenbach-str. 16, CH-4600 Olten, Switzerland; University of Applied Sciences and Arts Northwestern Switzerland, School of Business, Institute for Information Systems, Riggenbach-str. 16, CH-4600 Olten, Switzerland","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","2817","2824","In this paper we consider the problem of an optimal picking order sequence in a multi-aisle warehouse that is operated by a single automatic storage and retrieval system (AS/RS). The problem is solved by using a genetic algorithm (GA) similar to the one in the earlier research [3]. The problem and the solution approach are implemented in the OpenOpal software which provides a suitable test bed for simulation and optimization (see http://www.openopal.org/). As a result it becomes evident that the genetic algorithm can be improved by changing the selection method and introducing an elitism mechanism.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900254","","Genetic algorithms;Optimization;Sociology;Statistics;Shape;Educational institutions;Approximation methods","genetic algorithms;information retrieval systems","automated storage and retrieval system;AS-RS;optimal picking order sequence problem;genetic algorithm;GA;OpenOpal software;elitism mechanism","","3","9","","","","","","IEEE","IEEE Conferences"
"Reactive coordination of transmission-generation investment planning","Y. Tohidi; M. R. Hesamzadeh; K. Ostman","Electricity Market Research Group, KTH Royal Institute of Technology, Stockholm, Sweden; Electricity Market Research Group, KTH Royal Institute of Technology, Stockholm, Sweden; Electricity Market Research Group, KTH Royal Institute of Technology, Stockholm, Sweden","2015 12th International Conference on the European Energy Market (EEM)","","2015","","","1","5","This paper discusses the reactive coordination of generation and transmission planning in a competitive electricity market. The transmission planner is assumed as a social cost minimizing entity and the behavior of generators is modeled as the Nash equilibrium of a strategic game. Reactive coordination forms a mixed-integer optimisation problem which can be solve by commercially available softwares. This problem is solved for two cases of with and without transmission charges. Efficient coordination of transmission and generation planning is considered as the benchmark in this study. The developed model is implemented for a 6-node illustrative example and IEEE-RTS96 test system and the results are compared.","2165-4093;2165-4077","978-1-4673-6692","10.1109/EEM.2015.7216600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7216600","Mixed-integer optimisation problem;reactive coordination;system planning","Planning;Investment;Portfolios;Generators;Power transmission lines;Optimization","game theory;integer programming;investment;power generation planning;power system simulation;power transmission planning","IEEE-RTS96 test system;generation planning;mixed-integer optimisation;generator modelling;strategic game;Nash equilibrium;social cost minimizing entity;transmission planner;transmission-generation investment planning;reactive coordination","","","15","","","","","","IEEE","IEEE Conferences"
"Code generation for embedded heterogeneous architectures on android","R. Membarth; O. Reiche; F. Hannig; J. Teich","Department of Computer Science, University of Erlangen-Nuremberg, Germany; Department of Computer Science, University of Erlangen-Nuremberg, Germany; Department of Computer Science, University of Erlangen-Nuremberg, Germany; Department of Computer Science, University of Erlangen-Nuremberg, Germany","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","6","The success of Android is based on its unified Java programming model that allows to write platform-independent programs for a variety of different target platforms. However, this comes at the cost of performance. As a consequence, Google introduced APIs that allow to write native applications and to exploit multiple cores as well as embedded GPUs for compute-intensive parts. This paper proposes code generation techniques in order to target the Renderscript and Filterscript APIs. Renderscript harnesses multi-core CPUs and unified shader GPUs, while the more restricted Filterscript also supports GPUs with earlier shader models. Our techniques focus on image processing applications and allow to target these APIs and OpenCL from a common description. We further supersede memory transfers by sharing the same memory region among different processing elements on HSA platforms. As reference, we use an embedded platform hosting a multi-core ARM CPU and an ARM Mali GPU. We show that our generated source code is faster than native implementations in OpenCV as well as the pre-implemented script intrinsics provided by Google for acceleration on the embedded GPU.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.099","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800300","","Graphics processing units;Kernel;Androids;Humanoid robots;Computer architecture;DSL;Optimization","Android (operating system);application program interfaces;embedded systems;graphics processing units;Java;multiprocessing systems;source code (software)","code generation;embedded heterogeneous architectures;unified Java programming model;platform independent programs;multiple cores;embedded GPU;API;image processing applications;embedded platform;multicore ARM CPU;ARM Mali GPU;generated source code","","1","11","","","","","","IEEE","IEEE Conferences"
"Renewable Energy System Design by Artificial Neural Network Simulation Approach","A. Kumar; M. Zaman; N. Goel; V. Srivastava","NA; NA; NA; NA","2014 IEEE Electrical Power and Energy Conference","","2014","","","142","147","An alternative approach for optimization of renewable energy systems using artificial neural network (ANN) models and simulation is applied for standalone wind energy and photovoltaic cell. Feed forward back propagation (FFBP) and radial basis functions (RBF) are considered. Large training data are obtained by simulations in Homer software package. Several input parameters and two output parameters (number of units and cost of energy) are considered for performance analysis of the ANN network models. Irrespective of the input parameters, FFBF model fails to yield results that match with target values both for WT and PVC systems. The R-square values are observed to be scattered and lie in the range of 35 to 90 percent. On the other hand, RBF model output with the same model training data consistently matches with Homer simulation output for all input conditions. The correlation coefficients as measured by R-square are well above 95 percent in all cases. In another words, very good generalizations of the input parameters are observed for RBF model.","","978-1-4799-6038","10.1109/EPEC.2014.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051690","Wind energy;Photovoltaic cell;Optimization;Neural network models;Simulations","Data models;Load modeling;Artificial neural networks;Optimization;Analytical models;Training;Testing","photovoltaic cells;power system simulation;radial basis function networks;renewable energy sources;wind power","renewable energy system design;artificial neural network simulation;wind energy;photovoltaic cell;feed forward back propagation;radial basis functions;Homer software package;RBF","","1","17","","","","","","IEEE","IEEE Conferences"
"Optimized Parallel Label Propagation Based Community Detection on the Intel(R) Xeon Phi(TM) Architecture","A. B. Khlopotine; A. V. Sathanur; V. Jandhyala","NA; NA; NA","2015 27th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD)","","2015","","","9","16","Complex systems such as social, biological and information networks, characterized by millions to billions of sub-entities and relationships between them are best represented by graphs. A distinguishing feature of such complex systems is the self-organization into dense clusters called communities. Detecting such communities in massive graphs is critical to the understanding of such complex systems. However community detection is non-trivial and is expensive due to a vast number of computations needed to fully realize the underlying relationships. While a number of near-exact and heuristic algorithms is available for community detection, parallelizing such algorithms to fully leverage the advantages of parallel hardware is still a challenging problem. Most presently available approaches attempt to optimize run-time complexities by scaling original serial community detection algorithms. Graph algorithms in general and existing community detection algorithms in particular are known for not being commensurate with linear scalability on parallel systems. Therefore, there is a need for a combination of high-performance software and a hardware platform that would support efficient parallel graph processing with respect to community detection. We present an Intel® Xeon Phi Label Propagation algorithm (PLPA) variant of community detection algorithm based on label propagation (LP). Our algorithm was tuned for the Intel® Xeon Phi platform a novel architecture that provides 50+ physical cores with simultaneous multithreading. We outline Phi architecture advantages and limitations for PLPA and massive graph processing in general. We present test results of running our algorithm on large real-world networks while achieving near linear speedups and improving the quality of the detected communities. We also analyze possibilities of processing massive networks that cannot fully fit in a Phi memory and hence we extend our initial solution to a modified PLPA (PLPA-M).","1550-6533","978-1-4673-8011","10.1109/SBAC-PAD.2015.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7379828","Social Networks;Label Propagation Algorithm;Parallel Computing;Many Integrated Core Architecture","Computer architecture;High performance computing;Social network services;Optimization","graph theory;microprocessor chips;parallel algorithms","optimized parallel label propagation;community detection;Intel(R) Xeon Phi(TM) architecture;complex systems;graph representation;massive graphs;heuristic algorithms;parallel hardware;community detection algorithms;parallel systems;parallel graph processing;Intel® Xeon Phi label propagation algorithm;PLPA","","","43","","","","","","IEEE","IEEE Conferences"
"Can we identify smartphone app by power trace? [Extended abstract for special session]","Mian Dong; Po-Hsiang Lai; Zhu Li","Samsung Telecommunications America, Richardson, TX 75082, USA; Samsung Telecommunications America, Richardson, TX 75082, USA; Samsung Telecommunications America, Richardson, TX 75082, USA","2013 18th Asia and South Pacific Design Automation Conference (ASP-DAC)","","2013","","","373","375","Power trace of a smartphone, as time series data, carries important information of the system behavior and is useful for many applications, such as energy management [1-3], software optimization [4-6] and anomaly detection [7, 8]. However, the power trace measured from the battery terminals include the power consumption by all the hardware components and thus describes the activity of the whole system. Yet modern smartphones are multiprocessing, i.e., multiple applications can be running simultaneously in the same system. Our goal is to answer the following question: “Can we identify smartphone app by power trace?” That is, whether the power trace of a smartphone can be different by running different applications.","2153-6961;2153-6961","978-1-4673-3030-5978-1-4673-3029-9978-1-4673-3028","10.1109/ASPDAC.2013.6509624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509624","","Cameras;Spectral analysis;Games;Instruction sets;Power measurement;Benchmark testing;Telecommunications","smart phones;time series","smartphone app;power trace;time series data;energy management;software optimization;anomaly detection;power consumption","","","8","","","","","","IEEE","IEEE Conferences"
"Search-guided activity signals extraction in application service management control","T. D. Sikora; G. D. Magoulas","Department of Computer Science and Information Systems, Birkbeck, University of London; Department of Computer Science and Information Systems, Birkbeck, University of London, Malet Street, London WC1E 7HX","2014 14th UK Workshop on Computational Intelligence (UKCI)","","2014","","","1","8","The increased interest in autonomous control in Application Service Management-ASM environments has driven the demand for analysis of multivariate datasets in this area. Gathered metrics form time-series that can be considered as signals, which should be decomposed in order to find relations between system utilization and effective activity. This paper introduces a metrics signal deconvolution method that can be used to support human administrators or can be incorporated into feature extraction schemes that feed decision blocks of autonomous controllers. The method considers ASM environments signals decomposition as a search problem that is solved using heuristics and metaheuristic strategies. Quantitative and qualitative relations between activity and system resources signals are searched with use of a model that is based on similarity and variability of the changes, under minimal assumptions about the ASM system architecture and design. Experimental results show that the model can be successfully integrated with optimization techniques and the results produced when tested using data produced through queue modeling meet human perception of the signal unmixing problem.","2162-7657","978-1-4799-5538","10.1109/UKCI.2014.6930162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6930162","Signal Extraction;Multidimensional Deconvolution;Optimization;Feature Selection;Application Service Management;Adaptive Controller;Service Level Agreement;Performance","Optimization;Measurement;Load modeling;Deconvolution;Software;Monitoring;Search problems","feature extraction;multivariable systems;signal processing;time series","search-guided activity signals extraction;application service management control;ASM environments;multivariate datasets;time series;system utilization;metrics signal deconvolution method;human administrators;feature extraction;feed decision blocks;autonomous controllers;signal decomposition;search problem;metaheuristic strategies;system resource signals;ASM system architecture;optimization;queue modeling;signal unmixing problem","","1","27","","","","","","IEEE","IEEE Conferences"
"Harnessing Soft Computations for Low-Budget Fault Tolerance","D. S. Khudia; S. Mahlke","NA; NA","2014 47th Annual IEEE/ACM International Symposium on Microarchitecture","","2014","","","319","330","A growing number of applications from various domains such as multimedia, machine learning and computer vision are inherently fault tolerant. However, for these soft workloads, not all computations are fault tolerant (e.g., A loop trip count). In this paper, we propose a compiler-based approach that takes advantage of soft computations inherent in the aforementioned class of workloads to bring down the cost of software-only transient fault detection. The technique works by identifying a small subset of critical variables that are necessary for correct macro-operation of the program. Traditional duplication and comparison are used to protect these variables. For the remaining variables and temporaries that only affect the micro-operation of the program, strategic expected value checks are inserted into the code. Intuitively, a computation-chain result near the expected value is either correct or close enough to the correct result so that it does not matter for non-critical variables. Overall, the proposed solution has, on average, only 19.5% performance overhead and reduces the number of silent data corruptions from 15% down to 7.3% and user-visible silent data corruptions from 3.4% down to 1.2% in comparison to an unmodified application. This unacceptable silent data corruption rate is even lower than a traditional full duplication scheme that has, on average, 57% overhead.","1072-4451;2379-3155","978-1-4799-6998","10.1109/MICRO.2014.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011398","Soft Errors;Compiler Analysis","Benchmark testing;Fault tolerance;Fault tolerant systems;Circuit faults;Decoding;Optimization;Multimedia communication","program compilers;software fault tolerance","soft computations;low-budget fault tolerance;soft workloads;fault tolerant computations;compiler-based approach;software-only transient fault detection;program macrooperation;performance overhead;user-visible silent data corruptions;full duplication scheme","","21","37","","","","","","IEEE","IEEE Conferences"
"Tunable and generic problem instance generation for multi-objective reinforcement learning","D. Garrett; J. Bieger; K. R. Thórisson","Icelandic Institute for Intelligent Machines Reykjavík University, Iceland; Reykjavík University, Iceland; Icelandic Institute for Intelligent Machines Reykjavík University, Iceland","2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)","","2014","","","1","8","A significant problem facing researchers in reinforcement learning, and particularly in multi-objective learning, is the dearth of good benchmarks. In this paper, we present a method and software tool enabling the creation of random problem instances, including multi-objective learning problems, with specific structural properties. This tool, called Merlin (for Multi-objective Environments for Reinforcement LearnINg), provides the ability to control these features in predictable ways, thus allowing researchers to begin to build a more detailed understanding about what features of a problem interact with a given learning algorithm to improve or degrade the algorithm's performance. We present this method and tool, and briefly discuss the controls provided by the generator, its supported options, and their implications on the generated benchmark instances.","2325-1824;2325-1867","978-1-4799-4552","10.1109/ADPRL.2014.7010646","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7010646","","Learning (artificial intelligence);Correlation;Generators;Covariance matrices;Benchmark testing;Heuristic algorithms;Optimization","learning (artificial intelligence);software tools","multiobjective reinforcement learning;problem facing researcher;software tool;random problem instance;multiobjective learning problem;structural property;Merlin;multiobjective environments for reinforcement learning;learning algorithm","","1","25","","","","","","IEEE","IEEE Conferences"
"A Multiple-ISA Reconfigurable Architecture","F. M. Capella; M. Brandalero; J. F. Junior; A. C. S. Beck; L. Carro","NA; NA; NA; NA; NA","2013 III Brazilian Symposium on Computing Systems Engineering","","2013","","","71","76","In these days, every new added hardware feature must not change the underlying instruction set architecture (ISA), in order to avoid adaptation or recompilation of existing code. Nevertheless, this need for compatibility imposes a great number of restrictions to the designers, because it keeps them tied to a specific ISA and all its legacy hardware issues. Considering that the market is mainly dominated by two different ISAs (and, very likely, more to come): x86, used in the general purpose field, and ARM, used in embedded systems, the need for another level (at the Instruction Set Architecture) of adaptability is evident. Binary Translation (BT) appears as a solution for that, since it is capable of transforming binary code so it can be executed on another target architecture. However, BT adds another layer between code and actual execution, therefore bringing huge performance penalties. To overcome this drawback, we propose a new mechanism based on a dynamic two-level binary translation system. The first level translates ARM or X86 code to an intermediate code, which will be optimized by the second level: a dynamic reconfigurable array. In this way, the designer can take advantage of a BT system and program for two different fields of application, without worrying about the underlying architecture. Even though two case studies are presented, the first BT level is easily expandable to other ISAs.","2324-7894;2324-7886","978-1-4799-3890","10.1109/SBESC.2013.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6825345","Binary Translation;Reconfigurable Architecture;Code Optimization;Transparent Execution","Hardware;Arrays;Software;Optimization;Benchmark testing;Registers","binary codes;embedded systems;instruction sets;reconfigurable architectures","multiple-ISA reconfigurable architecture;hardware feature;instruction set architecture;code recompilation;code adaptation;legacy hardware issues;embedded systems;binary code transformation;performance penalties;dynamic two-level binary translation system;X86 code;ARM code;dynamic reconfigurable array;BT system","","","21","","","","","","IEEE","IEEE Conferences"
"A highly parallelized H.265/HEVC real-time UHD software encoder","T. K. Heng; W. Asano; T. Itoh; A. Tanizawa; J. Yamaguchi; T. Matsuo; T. Kodama","Multimedia Laboratory, Corporate Research & Development Center Toshiba Corporation, Kawasaki, Japan; Multimedia Laboratory, Corporate Research & Development Center Toshiba Corporation, Kawasaki, Japan; Multimedia Laboratory, Corporate Research & Development Center Toshiba Corporation, Kawasaki, Japan; Multimedia Laboratory, Corporate Research & Development Center Toshiba Corporation, Kawasaki, Japan; Multimedia Laboratory, Corporate Research & Development Center Toshiba Corporation, Kawasaki, Japan; Multimedia Laboratory, Corporate Research & Development Center Toshiba Corporation, Kawasaki, Japan; Multimedia Laboratory, Corporate Research & Development Center Toshiba Corporation, Kawasaki, Japan","2014 IEEE International Conference on Image Processing (ICIP)","","2014","","","1213","1217","H.265/HEVC standard, promising up to twice the compression efficiency over H.264/AVC standard is suitable for encoding UHD videos and has garnered much attention since its inception. With increasing amount of devices supporting UHD, real-time H.265/HEVC encoder and decoder are needed to complete the UHD media ecosystem. In this paper, we present a highly parallelized HEVC software encoder suitable for broadcasting or network streaming applications. Real-time Main 10 profile encoding of 4K UHD videos at 60fps is achieved through an efficient parallel encoder platform comprising of CPUs interconnected by high speed network. The encoder core is optimized with data parallelism and GPU assisted motion estimation. Utilizing temporal parallelism of GOP and spatial parallelism of picture through slicing to encode, scalability and flexibility are achieved. Results show that our encoder system is about 15794 times faster than HEVC test model HM 11.0 and 13 times faster than ×265, an open source HEVC encoder.","1522-4880;2381-8549","978-1-4799-5751","10.1109/ICIP.2014.7025242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025242","Video Codec;HEVC;UHD;GPU","Encoding;Parallel processing;Real-time systems;Videos;Motion estimation;Video coding;Graphics processing units","graphics processing units;motion estimation;parallel processing;video coding","H.265-HEVC realtime UHD software encoder;H.265-HEVC standard;high efficiency video coding;compression efficiency;UHD media ecosystem;highly parallelized HEVC software encoder;broadcasting application;network streaming application;data parallelism;GPU assisted motion estimation;graphics processing unit;temporal parallelism;spatial parallelism","","9","9","","","","","","IEEE","IEEE Conferences"
"Runtime and Architecture Support for Efficient Data Exchange in Multi-Accelerator Applications","J. Cabezas; I. Gelado; J. E. Stone; N. Navarro; D. B. Kirk; W. Hwu","Department of Computer Science, Barcelona Supercomputing Center; Research Department, NVIDIA Corporation at Santa Clara, CA; Beckman Institute for Advanced Science and Technology, University of Illinois at Urbana-Champaign, Urbana, IL; Department of Computer Science, Barcelona Supercomputing Center; Research Department, NVIDIA Corporation at Santa Clara, CA; Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL","IEEE Transactions on Parallel and Distributed Systems","","2015","26","5","1405","1418","Heterogeneous parallel computing applications often process large data sets that require multiple GPUs to jointly meet their needs for physical memory capacity and compute throughput. However, the lack of high-level abstractions in previous heterogeneous parallel programming models force programmers to resort to multiple code versions, complex data copy steps and synchronization schemes when exchanging data between multiple GPU devices, which results in high software development cost, poor maintainability, and even poor performance. This paper describes the HPE runtime system, and the associated architecture support, which enables a simple, efficient programming interface for exchanging data between multiple GPUs through either interconnects or cross-node network interfaces. The runtime and architecture support presented in this paper can also be used to support other types of accelerators. We show that the simplified programming interface reduces programming complexity. The research presented in this paper started in 2009. It has been implemented and tested extensively in several generations of HPE runtime systems as well as adopted into the NVIDIA GPU hardware and drivers for CUDA 4.0 and beyond since 2011. The availability of real hardware that support key HPE features gives rise to a rare opportunity for studying the effectiveness of the hardware support by running important benchmarks on real runtime and hardware. Experimental results show that in a exemplar heterogeneous system, peer DMA and double-buffering, pinned buffers, and software techniques can improve the inter-accelerator data communication bandwidth by 2×. They can also improve the execution speed by 1.6× for a 3D finite difference, 2.5× for 1D FFT, and 1.6× for merge sort, all measured on real hardware. The proposed architecture support enables the HPE runtime to transparently deploy these optimizations under simple portable user code, allowing system designers to freely employ devices of different capabilities. We further argue that simple interfaces such as HPE are needed for most applications to benefit from advanced hardware features in practice.","1045-9219;1558-2183;2161-9883","","10.1109/TPDS.2014.2316825","Spanish Ministry of Education; Generalitat de Catalunya; CUDA Centers of Excellence at UPC/BSC; University of Illinois; Department of Energy; NIH; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6803940","Distributed architectures;hardware/software interfaces;heterogeneous (hybrid) systems;data communications","Graphics processing units;Instruction sets;Runtime;Hardware;Memory management;Performance evaluation;Programming","electronic data interchange;graphics processing units;parallel processing;software architecture","runtime support;architecture support;data exchange;multiaccelerator application;heterogeneous parallel computing;programming interface;NVIDIA GPU hardware;CUDA 4.0;software technique","","4","21","","","","","","IEEE","IEEE Journals & Magazines"
"Branch prediction and the performance of interpreters — Don't trust folklore","E. Rohou; B. N. Swamy; A. Seznec","Inria, France; Inria, France; Inria, France","2015 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)","","2015","","","103","114","Interpreters have been used in many contexts. They provide portability and ease of development at the expense of performance. The literature of the past decade covers analysis of why interpreters are slow, and many software techniques to improve them. A large proportion of these works focuses on the dispatch loop, and in particular on the implementation of the switch statement: typically an indirect branch instruction. Folklore attributes a significant penalty to this branch, due to its high misprediction rate. We revisit this assumption, considering state-of-the-art branch predictors and the three most recent Intel processor generations on current interpreters. Using both hardware counters on Has well, the latest Intel processor generation, and simulation of the IT-TAGE, we show that the accuracy of indirect branch prediction is no longer critical for interpreters. We further compare the characteristics of these interpreters and analyze why the indirect branch is less important than before.","","978-1-4799-8161","10.1109/CGO.2015.7054191","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7054191","","History;Switches;Radiation detectors;Benchmark testing;Hardware;Bridges;Cryptography","program compilers;program control structures;program interpreters","branch prediction;interpreter performance;software techniques;dispatch loop;switch statement;misprediction rate;indirect branch instruction;Intel processor generations;hardware counters;Haswell;IT-TAGE simulation","","9","37","","","","","","IEEE","IEEE Conferences"
"Applications of McCad for the automatic generation of MCNP 3D models in fusion neutronics","F. Moro; U. Fischer; L. Lu; P. Pereslavtsev; S. Podda; R. Villari","Association EURATOM-ENEA, C.R. Frascati, Via E. Fermi, 45, I-00044 Frascati, Roma, Italy; Association KIT-Euratom, Karlsruhe Institute of Technology, Hermann-von-Helmholtz-Platz 1, D-76344 Eggenstein-Leopoldshafen, Germany; Association KIT-Euratom, Karlsruhe Institute of Technology, Hermann-von-Helmholtz-Platz 1, D-76344 Eggenstein-Leopoldshafen, Germany; Association KIT-Euratom, Karlsruhe Institute of Technology, Hermann-von-Helmholtz-Platz 1, D-76344 Eggenstein-Leopoldshafen, Germany; Association EURATOM-ENEA, C.R. Frascati, Via E. Fermi, 45, I-00044 Frascati, Roma, Italy; Association EURATOM-ENEA, C.R. Frascati, Via E. Fermi, 45, I-00044 Frascati, Roma, Italy","2013 IEEE 25th Symposium on Fusion Engineering (SOFE)","","2013","","","1","5","The Monte Carlo (MC) code MCNP is the reference tool in fusion neutronics, allowing the description and analysis of full and detailed 3D geometry of a tokamak machine. The geometrical models of the components used are typically available through computer aided design (CAD) files: the main benefits of this system are related to its portability and compatibility with several tools commonly used in engineering analyses. However, at the present stage, the information contained in CAD files cannot be directly provided to MC as inputs, because of the different representation scheme used. This issue leads to the necessity to develop interfaces that can translate them into the correct MC geometrical description. McCad is a software developed by the Karlsruhe Institute of Technology (KIT), dedicated to the fully automated generation of MCNP geometrical models from CAD files (STEP, IGES and BREP formats): it's provided with a graphical user interface (GUI) allowing the visualization of the geometries and tools for data exchange and modelling. The present paper summarizes the results of some benchmark tests performed on JET components and a DEMO reactor aimed at the assessment of the suitability of McCad for fusion neutronic applications. The reliability of the conversion algorithm has been evaluated comparing the results of stochastic MCNP volume calculations carried out using the generated models, and the corresponding volumes provided by the CAD kernel of the interface program. Moreover, the consistency of a converted DEMO MCNP model has been verified through particle transport calculations for the estimation of the neutron wall loading poloidal distribution. Several aspects related to the use of the code have been evaluated such as its portability, performances and the impact of the geometric approximation introduced on the neutronic analyses. Furthermore a useful feedback for the optimization and enhancement of the McCad interface has been provided.","1078-8891;2155-9953","978-1-4799-0171-5978-1-4799-0169","10.1109/SOFE.2013.6635377","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6635377","neutronics;design;analysis;JET;DEMO","Solid modeling;Design automation;Geometry;Solids;Monte Carlo methods;Three-dimensional displays;Computational modeling","CAD;computational geometry;data visualisation;electronic data interchange;fusion reactor theory;graphical user interfaces;Monte Carlo methods;optimisation;physics computing;plasma toroidal confinement;reliability;Tokamak devices","conversion algorithm;stochastic MCNP volume calculations;CAD kernel;interface program;converted DEMO MCNP model;particle transport calculations;neutron wall loading poloidal distribution;geometric approximation;neutronic analyses;optimization;McCad interface;MCNP 3D models;fusion neutronic applications;DEMO reactor;JET components;benchmark tests;data modelling;data exchange;data visualization;graphical user interface;IGES format;STEP format;BREP format;MCNP geometrical models;fully automated generation;Karlsruhe Institute of Technology;McCad software;Monte Carlo geometrical description;representation scheme;engineering analyses;compatibility;portability;computer aided design files;tokamak machine;reference tool;Monte Carlo code MCNP","","","11","","","","","","IEEE","IEEE Conferences"
"Early Childhood Educator Assistant with Brain Computer Interface","P. J. Lee; S. W. Chin","KDU College Penang, Malaysia; KDU College Penang, Malaysia","International Conference on Software Intelligence Technologies and Applications & International Conference on Frontiers of Internet of Things 2014","","2014","","","52","57","Education is the most important investment in the modern and complex industrial country. Currently, there are numerous methods to track and grasp how well a child is able to concentrate during lessons. However till date, there are still very limited studies in implementing adaptive self-learning techniques to suit the needs and preferences of each child for better learning experiences. Early Childhood Educator Assistant with Brain Computer Interface (BCI) is an interactive self-learning system capable of monitoring the learning capabilities of children and providing feedback by using visual optimization such as short video clip playback if the concentration of the child drops. An adaptive self-learning system able to change the learning content difficulty based on scores of children towards the lesson. The Emotiv neuroheadgear will detect EEG signals emitted from the scalp then be further processed to determine the state of mind and engagement levels of the child so that feedback can be sent based on the varying conditions. This system has been tried and tested with a group of children between the ages of 4 to 6 years old and shows an improvement in the engagement levels.","","978-1-84919-970","10.1049/cp.2014.1535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284219","Brain Stimulation;Brain Computer Interface;Education Products;Neurofeedback","","brain-computer interfaces;computer aided instruction;electroencephalography;medical signal detection","early childhood educator assistant;brain computer interface;complex industrial country;self-learning techniques;BCI;interactive self-learning system;visual optimization;short video clip playback;learning content;Emotiv neuroheadgear;EEG signal detection","","","","","","","","","IET","IET Conferences"
"View Synthesis Distortion Estimation for AVC- and HEVC-Compatible 3-D Video Coding","B. T. Oh; K. Oh","School of Electronics, Telecommunication and Computer Engineering, Broadcasting and Telecommunications Media Research Laboratory, Korea Aerospace University, Goyang, Korea; Electronics and Telecommunications Research Institute, Daejeon, Korea","IEEE Transactions on Circuits and Systems for Video Technology","","2014","24","6","1006","1015","This paper presents an efficient view synthesis distortion estimation method for 3-D video. It also introduces the application of this method to Advanced Video Coding (AVC)- and High Efficiency Video Coding (HEVC)-compatible 3-D video coding. Although the proposed view synthesis distortion scheme is generic, its use for actual 3-D video codec systems addresses the many issues caused by different video-coding formats and restrictions. The solutions for these issues are herein proposed. The simulation results show that the proposed scheme can achieve approximately 5.4% and 10.2% coding gains for AVC- and HEVC-compatible 3-D coding, respectively. In addition, the results show the remarkable complexity reduction of the scheme compared to the view synthesis optimization method currently used in 3-D-HEVC. The proposed method has been adopted into the presently developing AVC- and HEVC-compatible test model reference software.","1051-8215;1558-2205","","10.1109/TCSVT.2013.2290577","2013 Korea Aerospace University Faculty Research Grant; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662409","3D video codec;3D-AVC;3D-HEVC;View synthesis distortion;view synthesis distortion estimation;multiview coding;depth map coding;3-D Advanced Video Coding (AVC);3-D High Efficiency Video Coding (HEVC);3-D video (3-DV) codec;depth map coding;multiview coding;view synthesis distortion;view synthesis distortion estimation","Encoding;Rendering (computer graphics);Three-dimensional displays;Video coding;Estimation;Complexity theory;Interpolation","optimisation;video coding","HEVC compatible test model reference software;synthesis optimization method;complexity reduction;3-D video codec systems;3D high efficiency video coding;advanced video coding gain;synthesis distortion estimation method;AVC","","23","29","","","","","","IEEE","IEEE Journals & Magazines"
"A GPU-Accelerated Finite-Difference Time-Domain Scheme for Electromagnetic Wave Interaction With Plasma","P. D. Cannon; F. Honary","Space Plasma Environment and Radio Science Group, Lancaster University, Lancaster, U.K; Space Plasma Environment and Radio Science Group, Lancaster University, Lancaster, U.K","IEEE Transactions on Antennas and Propagation","","2015","63","7","3042","3054","A graphical processing unit (GPU)-accelerated finite-difference time-domain (FDTD) scheme for the simulation of radio-frequency (RF) wave propagation in a dynamic, magnetized plasma is presented. This work builds on well-established FDTD techniques with the inclusion of new time advancement equations for the plasma fluid density and temperature. The resulting FDTD formulation is suitable for the simulation of the time-dependent behavior of an ionospheric plasma due to interaction with an RF wave and the excitation of plasma waves and instabilities. The stability criteria and the dependence of accuracy on the choice of simulation parameters are analyzed and found to depend on the choice of simulation grid parameters. It is demonstrated that accelerating the FDTD code using GPU technology yields significantly higher performance, with a dual-GPU implementation achieving a rate of node update almost two orders of magnitude faster than a serial implementation. Optimization techniques such as memory coalescence are demonstrated to have a significant effect on code performance. The results of numerical tests performed to validate the FDTD scheme are presented, with a good agreement achieved when the simulation results are compared to both the predictions of plasma theory and to the results of the Tech-X VORPAL 4.2.2 software that was used as a benchmark.","0018-926X;1558-2221","","10.1109/TAP.2015.2423710","Engineering and Physical Sciences Research Council; STFC; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7088576","Electromagnetic propagation;magnetized plasma;finite-difference time-domain methods;ionosphere;GPU computing;Electromagnetic propagation;finite-difference time-domain (FDTD) methods;graphical processing unit (GPU) computing;ionosphere;magnetized plasma","Mathematical model;Finite difference methods;Time-domain analysis;Plasma temperature;Propagation;Computational modeling","finite difference time-domain analysis;graphics processing units;optimisation;plasma density;plasma electromagnetic wave propagation;plasma instability;plasma temperature;plasma waves;radiowave propagation","GPU-accelerated finite-difference time-domain scheme;plasma electromagnetic wave interaction;graphical processing unit;FDTD scheme;radio-frequency wave propagation simulation;RF wave propagation;dynamic magnetized plasma;time advancement equations;plasma fluid density;plasma temperature;ionospheric plasma;plasma wave excitation;stability criteria;simulation grid parameters;node update rate;optimization techniques;numerical tests;code performance;Tech-X VORPAL 4.2.2 software;plasma theory predictions;plasma instabilities","","11","31","","","","","","IEEE","IEEE Journals & Magazines"
"Decentralized Economic Dispatch in Smart Grids by Self-Organizing Dynamic Agents","V. Loia; A. Vaccaro","Research Consortium on Intelligent Software Agent Technologies-CORISA, Department of Informatics, University of Salerno, Fisciano, Italy; Department of Engineering, University of Sannio, Benevento, Italy","IEEE Transactions on Systems, Man, and Cybernetics: Systems","","2014","44","4","397","408","In this paper, we propose a decentralized and self-organizing solution framework aimed at addressing economic dispatch (ED) analysis in a distributed scenario. In particular we will demonstrate that, under some hypotheses, the solution of the ED analysis can be obtained by computing proper weighted averages of the variable of interests. To compute these global quantities we propose the deployment of a network of cooperative dynamic agents solving distributed average consensus problems. Thanks to this decentralized/nonhierarchical paradigm, all the basic operations needed to solve the economic dispatch problem could be easily processed by the agents. Simulation results obtained on the 118 and 300 bus IEEE test networks are presented and discussed in order to prove the effectiveness of the proposed framework.","2168-2216;2168-2232","","10.1109/TSMC.2013.2258909","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6527962","Intelligent systems;optimization methods;power generation dispatch;smart grids","","multi-agent systems;power generation dispatch;power generation economics;smart power grids","decentralized economic dispatch;smart grids;self-organizing dynamic agents;cooperative dynamic agents;self-organizing solution framework;distributed average consensus problems;IEEE 118 bus test networks;IEEE 300 bus test networks","","74","33","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluating and selecting ERP systems by a fuzzy weighted average approach","Nguyen Phan Anh Huy","Faculty of Economics, University of Technical Education, Ho Chi Minh City, Viet nam","2013 Third World Congress on Information and Communication Technologies (WICT 2013)","","2013","","","7","12","Enterprise Resource Planning (ERP) is the innovative software solution for many fields in this digital era. Information Technology has changed to the world of business application. An ERP software solution integrate operations, processes and information flows in an enterprise, to manage the resources of an organization such as human, material, money and machine effectively. This study introduces the fuzzy weighted average (FWA) approach to evaluate and select the suitable ERP system in order to help the company to increase many benefits. The combination of qualitative and quantitative criteria is adopted in this method to make right decision when choosing an ERP system. The centroid ranking method is applied to rank all the final fuzzy evaluation values from the provided model. A numerical example is used to test the model. The proposed model can help companies to evaluate ERP systems and select their best systems to support the business.","","978-1-4799-3230","10.1109/WICT.2013.7113100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7113100","Fuzzy;fuzzy weighted average (FWA);Enterprise Resource Planning (ERP)","Silicon;Levee;Artificial intelligence","decision making;enterprise resource planning;fuzzy set theory","ERP system selection;ERP system evaluation;information technology;ERP software solution;information flows;fuzzy weighted average approach;FWA approach;centroid ranking method","","","21","","","","","","IEEE","IEEE Conferences"
"High performance homodyne six port receiver using memory polynomial calibration","A. O. AOlopade; M. Helaoui","iRadio Lab., Department of Electrical and Computer Engineering, Schulich School of Engineering, University of Calgary, AB, T2N 1N4, Canada; iRadio Lab., Department of Electrical and Computer Engineering, Schulich School of Engineering, University of Calgary, AB, T2N 1N4, Canada","2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE)","","2014","","","1","4","This paper proposes an optimized memory polynomial (MP) based calibration technique for a homodyne six-port receiver. The performance and complexity of the proposed calibration technique allows for an easy implementation and high linearity performance of the homodyne receiver. For validation purpose, a six-port receiver was implemented and tested using a 3G (WCDMA) signal in the case of a multipath fading channel. The MP calibration technique was implemented and tested. By sending a training I/Q data into the receiver, the calibration constants were estimated from the diode output voltages using the least square algorithm. Subsequent analysis using a 3D plot of the error vector magnitude (EVM), non-linearity order (N) and memory depth (M) of the MP was done to optimize the complexity in terms of the number of calibration constants to ensure a good receiver performance. A bit error rate profile of the communication system was finally plotted to show the viability of the SPR front-end in a high data rate communication system even in the presence of a multipath fading channel.","0840-7789","978-1-4799-3101-9978-1-4799-3099","10.1109/CCECE.2014.6901127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901127","Six-Port Receiver;Direct Conversion Receiver;Software Defined Radio;Calibration","Multiaccess communication;Spread spectrum communication;Receivers;Calibration;Linearity;Correlators;Detectors","calibration;error statistics;fading channels;least squares approximations;multipath channels;polynomials;radio receivers","SPR front-end;bit error rate profile;memory depth;nonlinearity order;EVM;error vector magnitude;3D plot;least square algorithm;diode output voltages;calibration constants;training I-Q data;multipath fading channel;WCDMA signal;3G signal;homodyne six-port receiver;optimized MP based calibration technique;optimized memory polynomial based calibration technique","","1","12","","","","","","IEEE","IEEE Conferences"
"Simplify: A Framework for Enabling Fast Functional/Behavioral Validation of Multiprocessor Architectures in the Cloud","G. M. Almeida; O. B. Longhi; T. Bruckschloegl; M. Hübner; F. Hessel; J. Becker","NA; NA; NA; NA; NA; NA","2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum","","2013","","","2200","2205","The design of high-performance Multiprocessor Systems-on-Chip (MPSoCs) has proven to be an attractive challenge in embedded systems design automation. However, the complexity of such designs associated with short time-to-market constraints impose serious limitations on the exploration of different configurations and scenarios on the design space exploration. The use of virtual platforms may decrease the time-to-market of these architectures while providing the means to exploit, debug and verify architectures with different features. In this paper, we present the web-based Simplify framework, an interactive approach for MPSoC exploration using an instruction-accurate Open Virtual Platform (OVP). The framework provides an environment to define both software and hardware properties in an intuitive way, and allows designers to validate the functionality as well as the behavior of the modeled architectures at high-abstraction levels. Based on the simulation reports generated from the framework, designers can perform further design modifications and optimizations, and re-validate the whole system in an efficient way, allowing increased design space exploration. For the evaluation of the proposed approach, a set of benchmark applications extracted from MiBench has been used. They run on five different processors (MIPS32, ARM7, OpenRISC (OR1K), PowerPC32 and Micro Blaze) on both mono and multiprocessor architectures and the experiments show considerable simulation speed-ups to obtain application profiling at instruction-level compared to existing approaches based on tracing.","","978-0-7695-4979-8978-0-7695-4979","10.1109/IPDPSW.2013.108","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6651130","virtual platforms;multiprocessor systems-on-chip;application profiling;cloud simulation","Program processors;Computer architecture;Benchmark testing;Object oriented modeling;Embedded systems;Multiprocessing systems","cloud computing;embedded systems;multiprocessing systems;optimisation;system-on-chip;time to market","functional validation;behavioral validation;multiprocessor architectures;cloud;high-performance multiprocessor systems-on-chip;embedded systems design automation;short time-to-market constraints;design space exploration;virtual platforms;Web-based Simplify framework;interactive approach;MPSoC exploration;instruction-accurate open virtual platform;OVP;software properties;hardware properties;high-abstraction levels;design modifications;optimizations;MiBench;MIPS32;ARM7;OpenRISC;OR1K;PowerPC32;Micro Blaze;instruction-level","","2","17","","","","","","IEEE","IEEE Conferences"
"An adaptive real time mechanism for IaaS cloud provider selection based on QoE aspects","M. Souidi; S. Souihi; S. Hoceini; A. Mellouk","Image, Signal and Intelligent Systems Laboratory (LISSI) - Dept. of Networks and Telecoms, IUT C/V, University of Paris Est Creteil (UPEC), FRANCE; Image, Signal and Intelligent Systems Laboratory (LISSI) - Dept. of Networks and Telecoms, IUT C/V, University of Paris Est Creteil (UPEC), FRANCE; Image, Signal and Intelligent Systems Laboratory (LISSI) - Dept. of Networks and Telecoms, IUT C/V, University of Paris Est Creteil (UPEC), FRANCE; Image, Signal and Intelligent Systems Laboratory (LISSI) - Dept. of Networks and Telecoms, IUT C/V, University of Paris Est Creteil (UPEC), FRANCE","2015 IEEE International Conference on Communications (ICC)","","2015","","","6809","6814","Traditionally, companies host their own services, platforms and infrastructures on their own servers. This policy results in high costs in terms of material and human resources. It may also be inadequate to the real needs of the company. In this context, one solution is to use cloud computing to outsource their services. The latter is defined by making available to the customer high-performance servers and high bandwidth. The cloud is also defined by renting software and hardware infrastructure to customers according to their needs. Cloud computing is made possible by the improvement of computer networks infrastructures. Indeed, broadband connections have reduced latency and thus enabled the use of remote resources. The success of cloud computing has led to a significant increase in the providers number offering many and varied cloud services. While the access to these services is made possible through a simple subscription, no technique is currently available to select the cloud provider that best fits their needs. Selecting a provider is an optimization problem that has been studied in several areas. Given the large number of parameters and actors in the cloud, this problem is known as NP-complete one. In this work, we propose a new developed platform which plays the role of a broker between clients and cloud providers. Based on a set of benchmark tasks on provider services, it performs an adaptive cloud provider selection in accordance with the client needs. The experimental results show that the proposed approach gives benefits to subscribers in terms of QoE.","1550-3607;1938-1883","978-1-4673-6432-4978-1-4673-6431","10.1109/ICC.2015.7249411","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7249411","","Cloud computing;Extraterrestrial measurements;Servers;Benchmark testing;Bandwidth;Hardware;Virtual machining","cloud computing;computational complexity;computer networks;optimisation;quality of experience;real-time systems","adaptive real time mechanism;infrastructure as a service;IaaS cloud provider selection;quality of experience;QoE aspects;cloud computing;renting software;hardware infrastructure;computer networks infrastructures;broadband connections;varied cloud services;optimization problem;NP-complete problem;client provider;provider services;adaptive cloud provider selection","","2","15","","","","","","IEEE","IEEE Conferences"
"Image transfer optimization for agile development","A. Karve; A. Kochut","IBM T.J. Watson Research Center, 1101 Kitchawan Road, Route 134, Yorktown Heights, N.Y. 10598; IBM T.J. Watson Research Center, 1101 Kitchawan Road, Route 134, Yorktown Heights, N.Y. 10598","2013 IFIP/IEEE International Symposium on Integrated Network Management (IM 2013)","","2013","","","554","560","Cloud computing is becoming a common delivery model for IT services. Development and testing of applications and services is usually conducted on a development cloud environment often within customer premises and deployed in stages to a production cloud. Agile development process integrates development and deployment of IT systems and requires frequent and low cost synchronization between development and deployment cloud environments. This article proposes and evaluates virtual machine transfer algorithm based on image redundancy that allows to reduce bandwidth and time required to transfer specific images from development to production sites. It also explores how a typical image library, including public and private images, evolves over time and what impact it has on potential gain from the proposed algorithm. An analytical model is also proposed that allows to quantify degree of saving from using the algorithm. Evaluation shows up to 80% reduction in terms of transfer time and network bandwidth usage.","1573-0077","978-3-901882-50-0978-1-4673-5229","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6573032","","Libraries;Virtual machining;Indexes;Redundancy;Clustering algorithms;Bandwidth","cloud computing;software prototyping;virtual machines","image transfer optimization;agile development process;cloud computing;IT services;development cloud environment;production cloud;IT system development;IT system deployment;deployment cloud environments;virtual machine transfer algorithm;image redundancy;image library;public image;private image;analytical model;transfer time;network bandwidth usage","","","25","","","","","","IEEE","IEEE Conferences"
"Experimental evaluation of GPUs radiation sensitivity and algorithm-based fault tolerance efficiency","P. Rech; L. Carro","UFRGS, Universidade Federal do Rio Grande do Sul Porto Alegre, Brazil; UFRGS, Universidade Federal do Rio Grande do Sul Porto Alegre, Brazil","2013 IEEE 19th International On-Line Testing Symposium (IOLTS)","","2013","","","244","247","Experimental results demonstrate that Graphic Processing Units are very prone to be corrupted by neutrons. We have performed several experimental campaigns at ISIS, UK and at LANSCE, Los Alamos, NM, USA accessing the sensitivity of the GPU internal resources as well as the error rate of common parallel algorithms. Experiments highlight output error patterns and radiation responses that can be fruitfully used to design optimized Algorithm-Based Fault Tolerance strategies and provide pragmatic programming guidelines to increase the code reliability with low computational overhead.","1942-9398;1942-9401","978-1-4799-0664","10.1109/IOLTS.2013.6604091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6604091","GPU;neutron sensitivity;multiple errors;software-based hardening","Instruction sets;Graphics processing units;Reliability;Error correction codes;Neutrons;Parallel processing;Sensitivity","computational complexity;error correction codes;fault tolerant computing;graphics processing units;parallel algorithms;performance evaluation;radiation effects","GPU radiation sensitivity;algorithm-based fault tolerance efficiency;experimental evaluation;graphic processing units;error rate;parallel algorithms;output error patterns;radiation responses;design optimized algorithm-based fault tolerance strategies;code reliability;computational overhead;ISIS UK;LANSCE Los Alamos NM USA;GPU internal resource sensitivity","","2","10","","","","","","IEEE","IEEE Conferences"
"Winner-Take-All Memetic Differential Evolution for Genetic Interaction: Parameter Identification","S. Wu; C. Wu","NA; NA","2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing","","2013","","","43","48","The emerging large-scale biological tools (e.g., micro array) challenge biologists to realize the connectivity of genes and/or proteins at the system level (global view). Having advantages in good generalization and showing the direct interaction of genes and/or proteins, the S-system becomes one of the popular models, which is able to capture the dynamic behavior of the biological system. Differential evolution (DE) and its variants have recently applied to solve various optimization problems in engineering fields. However, the exploitative and explorative abilities are insufficient. In this study, we propose a winner-take-all memetic differential evolution scheme to infer the parameters of the S-type gene regulatory networks. This method was tested with a genetic-branch pathway and a twenty-gene network. The learning was implemented in a wide search space ([0, 100] for rate constants and [-100, 100] for kinetic orders) with a bad initial start (All parameters were randomly initialized at the neighborhood of 80). Simulation results show high-accuracy solutions are obtained.","","978-0-7695-5005","10.1109/SNPD.2013.99","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598443","inverse problem;memetic algorithm;evolution algorithm;S-system","Sociology;Statistics;Memetics;Kinetic theory;Genetics;Proteins","genetics;optimisation;parameter estimation","winner-take-all memetic differential evolution;genetic interaction;parameter identification;large-scale biological tools;S-system;biological system;optimization","","1","20","","","","","","IEEE","IEEE Conferences"
"An instrument-based testing platform and fuel control algorithm verification for direct methanol fuel cell","S. Chen; Y. Chung; T. Yang; Y. Chiu; J. Lin; C. Chi","Ship and Ocean Industries R&amp;D Center/Marine Industrial Department, New Taipei City, Taiwan; Taipei Chengshih University of Science and Technology/Dept .of Mechanical Engineering, Taiwan; Taipei Chengshih University of Science and Technology/Dept .of Mechanical Engineering, Taiwan; Taipei Chengshih University of Science and Technology/Dept .of Mechanical Engineering, Taiwan; Taipei Chengshih University of Science and Technology/Dept .of Mechanical Engineering, Taiwan; Taipei Chengshih University of Science and Technology/Dept .of Mechanical Engineering, Taiwan","2013 International Conference on System Science and Engineering (ICSSE)","","2013","","","159","164","System integration is crucial to fuel cells. In addition to considering relative control rules and treatment of the reactants, the design and control of the balance of plant (BOP) are vitally important for a self-sustainable and optimized fuel cell system. In this study, a fuel cell control system is developed by utilizing an instrument-based system integration platform and sensor-less fuel concentration estimation and verification. A control measure is determined by both consumption and concentration change of the fuel. The feasibility of the sensor-less fuel concentration estimation is verified by experimental results. A testing platform, integrating software and hardware, which is used for the direct liquid fuel cell system, has developed in this study. It is an beneficial and flexible tool for developing relative controlling algorithms, evaluating efficiency of cells, or designing BOP of fuel cells.","2325-0925;2325-0909","978-1-4799-0009-1978-1-4799-0007","10.1109/ICSSE.2013.6614651","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6614651","","Fuels;Instruments;Fuel cells;Temperature measurement;MATLAB;Temperature control;Actuators","direct methanol fuel cells","instrument-based testing platform;fuel control algorithm verification;direct methanol fuel cell;relative control rules;reactant treatment;balance-of-plant design;BOP control;self-sustainable fuel cell system;optimized fuel cell system;fuel cell control system;instrument-based system integration platform;sensorless fuel concentration estimation;fuel consumption;fuel concentration change;direct liquid fuel cell system;relative controlling algorithm;cell efficiency","","","9","","","","","","IEEE","IEEE Conferences"
"Design of microwave imaging based microstrip ultra-wideband antenna","D. Shukla; B. R. Dutta; B. K. Kanaujia","Department of Electronics & Communication, Shri Ram Murti Smarak College of Engg. & Technology, Bareilly, India; Department of Electronics & Communication, Shri Ram Murti Smarak College of Engg. & Technology, Bareilly, India; Department of Electronics & Communication, Ambedkar Institute of Advanced Communication Technologies & Research, Delhi, India","2015 Annual IEEE India Conference (INDICON)","","2015","","","1","5","The designed micro-strip ultra-wideband (UWB) antenna is based on a travelling wave propagating along the tapered surface with a phase velocity less than the speed of light. It results in end-fire radiation with a good penetration and resolution characteristics. The designed antenna for microwave imaging (MI) works on a technique which requires the measurement of reflected electromagnetic waves by the object under test. The UWB antenna with overall size of 70×40×1 mm using Rogers RO3003 substrate with a relative permittivity of 3 is designed using ANSYS-HFSS software. The results for the frequency band 3 to 20 GHz are optimized on the parameters return loss, VSWR and gain. The effect of different medium on the parameters of antenna is also tested. The designed UWB antenna has obtain return loss better than - 10dB and VSWR less than the 1.8 within the band (3GHz to 20GHz). The designed UWB microstrip antenna has wide practical application in concealed weapon detection, medical imaging, remote sensing, through the wall imaging etc.","2325-9418","978-1-4673-7399-9978-1-4673-7398","10.1109/INDICON.2015.7443760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7443760","Microwave Imaging(MI);Ultra-WideBand(UWB);Device Under Test(DUT);Electro Magnetic (EM);microstrip line","Microstrip antennas;Bandwidth;Imaging;Microstrip;Microwave antennas;Ultra wideband antennas","microstrip antennas;microwave imaging;ultra wideband antennas","microwave imaging based microstrip ultra-wideband antenna;electromagnetic waves;ANSYS-HFSS software;UWB microstrip antenna;end-fire radiation;frequency 3 GHz to 20 GHz","","1","19","","","","","","IEEE","IEEE Conferences"
"Microarchitectural performance characterization of irregular GPU kernels","M. A. O'Neil; M. Burtscher","Department of Computer Science, Texas State University, San Marcos, TX; Department of Computer Science, Texas State University, San Marcos, TX","2014 IEEE International Symposium on Workload Characterization (IISWC)","","2014","","","130","139","GPUs are increasingly being used to accelerate general-purpose applications, including applications with data-dependent, irregular memory access patterns and control flow. However, relatively little is known about the behavior of irregular GPU codes, and there has been minimal effort to quantify the ways in which they differ from regular GPGPU applications. We examine the behavior of a suite of optimized irregular CUDA applications on a cycle-accurate GPU simulator. We characterize the performance bottlenecks in each program and connect source code with microarchitectural characteristics. We also assess the impact of improvements in cache and DRAM bandwidth and latency and discuss the implications for GPU architecture design. We find that, while irregular graph codes exhibit significantly more underutilized execution cycles due to branch divergence, load imbalance, and synchronization overhead than regular programs, these factors contribute less to performance degradation than we expected. It appears that code optimizations are often able to effectively address these performance hurdles. Insufficient bandwidth and long memory latency are the biggest limiters of performance. Surprisingly, we find that applications with irregular memory access patterns are more sensitive to changes in L2 latency and bandwidth than DRAM latency and bandwidth.","","978-1-4799-6454-3978-1-4799-6452","10.1109/IISWC.2014.6983052","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6983052","","Graphics processing units;Benchmark testing;Random access memory;Hardware;Kernel;Bandwidth;Pipelines","cache storage;DRAM chips;graphics processing units;parallel architectures;source code (software);synchronisation","microarchitectural performance characterization;irregular GPU kernels;general-purpose applications;data-dependent-irregular memory access patterns;control flow;irregular GPU code behavior;regular GPGPU applications;optimized irregular CUDA applications;cycle-accurate GPU simulator;source code;cache improvement;DRAM bandwidth improvement;latency improvement;GPU architecture design;irregular graph codes;underutilized execution cycles;branch divergence;load imbalance;synchronization overhead;performance degradation;code optimizations;memory latency;irregular memory access patterns;L2 latency;L2 bandwidth;DRAM latency","","17","34","","","","","","IEEE","IEEE Conferences"
"Modifying a scientific flight control system for balloon launched UAV missions","M. Schwarzbach; S. Wlach; M. Laiacker","German Aerospace Center DLR, Muenchner Str. 20, 82234 Wessling, Germany; German Aerospace Center DLR, Muenchner Str. 20, 82234 Wessling, Germany; German Aerospace Center DLR, Muenchner Str. 20, 82234 Wessling, Germany","2015 IEEE Aerospace Conference","","2015","","","1","10","In this paper we present our work on enabling Balloon launched high altitude UAV Missions for an autopilot system previously used only at lower levels in visual line of sight conditions. One field of our research in the context of flying robotics is focused on high altitude pseudo satellites (HAPS). To gain operational experience in high altitude flying and for system and payload testing, a balloon launched small UAV (sub 10kg) system was designed including building an optimized airframe. Balloon launching was chosen because it offers fast and clearly regulated access to the desired altitudes. Our autopilot system has proven its capabilities in many years of flight experiments with different platforms (helicopter and fixed wing). The main characteristics are modularity and easy use for scientists. On the hardware level the task was to integrate the existing segmented systems of the research autopilot in a compact form factor, with the possible use in larger platforms in mind. The design was driven by the special thermal requirements resulting from flying in stratospheric conditions. In the autopilot software, several mission specific functions had to be added, which only required moderate effort due to the modular system design. Major changes included adding a flight termination manager. A launch routine was developed allowing a safe transition from free-fall to stable horizontal flight in thin air after being dropped from the balloon. Extensive testing was performed to validate the design. Simulating the mission, including balloon ascend, was used to check the mission software. Thermal and pressure conditions at altitude were replicated in a thermal vacuum chamber with additional sensors applied to identify problems. The simulation and control laws were verified by means of low altitude test flights.","1095-323X","978-1-4799-5380-6978-1-4799-5379-0978-1-4799-5377","10.1109/AERO.2015.7119055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7119055","","Biographies","aerospace control;aerospace robotics;aerospace testing;artificial satellites;balloons","scientific flight control system;balloon launched UAV missions;high altitude UAV missions;autopilot system;flying robotics;high altitude pseudo satellites;stratospheric conditions;autopilot software;low altitude test flights","","1","11","","","","","","IEEE","IEEE Conferences"
"Detect Android Malware Variants Using Component Based Topology Graph","T. Shen; Y. Zhongyang; Z. Xin; B. Mao; H. Huang","NA; NA; NA; NA; NA","2014 IEEE 13th International Conference on Trust, Security and Privacy in Computing and Communications","","2014","","","406","413","Smartphone has experienced explosive growth recently. At present, Android system is the most popular mobile platform and attracts lots of developers as well as malware authors. In order to evade detection, malware authors often apply obfuscation techniques to morph malware. Since traditional malware detectors are based on pure syntax, they may fail to detect obfuscated malware variants. We present a novel signature, topology graph based on Android components, which could model malicious payloads properly and resist against common obfuscation used by hackers. We performe stress test on security tools provided by Virus total with ten kinds of malware families from Android Malware Genome Project. Unfortunately, the result is not optimistic that obfuscated malware samples evade most of security tools. Nevertheless, 86.36% of obfuscated malware samples we tested are caught by our detector with tolerable false positive. The evaluation demonstrates that our approach is able to detect malware variants generated by common obfuscation techniques.","2324-898X;2324-9013","978-1-4799-6513","10.1109/TrustCom.2014.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011276","malware;obfuscation;component;topology graph","Malware;Topology;Androids;Humanoid robots;Payloads;Smart phones;Receivers","graph theory;invasive software;program testing;smart phones","Android malware variants detection;component based topology graph;Android components;malicious payloads;stress test;security tools;Android Malware Genome Project;obfuscated malware","","9","27","","","","","","IEEE","IEEE Conferences"
"Requirements driven falsification with coverage metrics","A. Dokhanchi; A. Zutshi; R. T. Sriniva; S. Sankaranarayanan; G. Fainekos","Arizona State University; University of Colorado, Boulder; Arizona State University; University of Colorado, Boulder; Arizona State University","2015 International Conference on Embedded Software (EMSOFT)","","2015","","","31","40","Specication guided falsication methods for hybrid systems have recently demonstrated their value in detecting design errors in models of safety critical systems. In specication guided falsication, the correctness problem, i.e., does the system satisfy the specication, is converted into an optimization problem where local negative minima indicate design errors. Due to the complexity of the resulting optimization problem, the problem is solved iteratively by performing a number of simulations on the system. Even though it is theoretically guaranteed that falsication methods will eventually find the bugs in the system, in practice, the performance of these methods, i.e., how many tests/simulations are executed before a bug is detected, depends on the specication, on the system and on the optimization method. In this paper, we define and utilize coverage metrics on the state space of hybrid systems in order to improve the performance of the falsication methods.","","978-1-4673-8079","10.1109/EMSOFT.2015.7318257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318257","","Robustness;Yttrium;Measurement;Trajectory;Semantics;Testing;Aerospace electronics","formal specification","requirements driven falsification method;optimization problem;local negative minima;iterative number;coverage metrics","","5","52","","","","","","IEEE","IEEE Conferences"
"An Ant Colony System algorithm for automatically schematizing transport network data sets","M. Ware; N. Richards","Faculty of Advanced Technology, University of South Wales, Pontypridd, UK; Faculty of Advanced Technology, University of South Wales, Pontypridd, UK","2013 IEEE Congress on Evolutionary Computation","","2013","","","1892","1900","The work presented here investigates the usefulness of Ant Colony Optimisation to solving network schematization problems. This is a well-established problem domain and a number of solutions have appeared in the literature previously. In this paper an Ant Colony System (ACS) based algorithm is presented, together with experimental results and performance analysis. The aim is to provide an algorithm that produces better results and is more efficient (in terms of execution times) than previous solutions. Throughout the paper, ACS is tested and evaluated empirically - that is, experiments are performed and observed, these observations are recorded and subsequently analysed. In order to perform the experiments, a software implementation of the algorithm is constructed and then applied to test data sets. No attempt has been made here to perform a theoretical analysis of ACS. The results presented demonstrate that ACS can be used as an effective means of providing solutions to network schematization problems. In particular, ACS is shown to outperform a previous Simulated Annealing based solution.","1089-778X;1941-0026","978-1-4799-0454-9978-1-4799-0453-2978-1-4799-0451-8978-1-4799-0452","10.1109/CEC.2013.6557790","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557790","ant colony optimization;automated cartography;metro maps","Cities and towns;Approximation methods;Algorithm design and analysis;Joining processes;Layout;Simulated annealing","ant colony optimisation;cartography","ant colony system algorithm;network schematization problems;transport network data sets;ACS based algorithm;software implementation;automated cartography","","1","24","","","","","","IEEE","IEEE Conferences"
"Effects of Nondeterminism in Hardware and Software Simulation with Thread Mapping","G. Salvador; S. Nilakantan; B. Taskin; M. Hempstead; A. More","NA; NA; NA; NA; NA","2015 28th International Conference on VLSI Design","","2015","","","129","134","In this paper, we explore the simulation performance trade-off under the lens of Monte Carlo design space exploration for multi-threaded programs and thread mapping. The vehicle used for this exploration will be a recent study, whose novel Google Page Rank-based thread mapping approach is compared to hundreds of random mappings, as well as a Round-Robin-based thread mapping approach proposed in this paper used in similar comparisons. The modern simulator landscape presents a choice between cycle-accurate but slow, and fast but inaccurate program simulation. We find that the use of a fast, inaccurate multi-threaded simulator, such as Sniper 5.3, suffers from large nondeterminism in the reported performance of the program. We perform cycle-accurate simulation which demonstrates that the static thread mapping approach does provide benefits in reaching near-optimal design points. Furthermore, the runtime of static thread mapping is significantly reduced using a cycle-accurate simulator compared to the full Monte Carlo exploration of mapping design points.","1063-9667;2380-6923","978-1-4799-6658","10.1109/VLSID.2015.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7031720","","Instruction sets;Aggregates;Benchmark testing;Hardware;Monte Carlo methods;Space exploration","Monte Carlo methods;multi-threading;search engines","hardware simulation;software simulation;simulation performance trade-off;Monte Carlo design space exploration;multithreaded programs;Google PageRank;program simulation;multithreaded simulator;Sniper 5.3;cycle-accurate simulation;static thread mapping approach;cycle-accurate simulator","","","10","","","","","","IEEE","IEEE Conferences"
"Low-cost integration of hardware components into co-simulation for future power and energy systems","O. Nannen; K. Piech; S. Lehnhoff; S. Rohjans; F. Schlögl; J. Velasquez; F. Andren; T. Strasser","OFFIS - Institute for Information Technology, Oldenburg, Germany; OFFIS - Institute for Information Technology, Oldenburg, Germany; OFFIS - Institute for Information Technology, Oldenburg, Germany; OFFIS - Institute for Information Technology, Oldenburg, Germany; OFFIS - Institute for Information Technology, Oldenburg, Germany; OFFIS - Institute for Information Technology, Oldenburg, Germany; Electric Energy Systems - Energy Department, AIT Austrian Institute of Technology Vienna, Austria; Electric Energy Systems - Energy Department, AIT Austrian Institute of Technology Vienna, Austria","IECON 2015 - 41st Annual Conference of the IEEE Industrial Electronics Society","","2015","","","005304","005309","The complexity of energy systems is increasing continuously. The main driver for this is the number of components and systems that are involved in grid operation, optimization, and protection. Moreover, various interconnections among different components exist so that the behavior of one has impacts on many. This raises the requirement to thoroughly test and validate components and systems to be integrated in the overall infrastructure. In order to address the dependencies it is not sufficient to apply stand-alone approaches but to follow co-simulation. Furthermore, hard- and software have to be combined to large-scale simulation scenarios. Thus, real laboratory components have to be connected and integrated with software-based simulations. In this paper the authors present a low-cost and standard-compliant co-simulation approach that integrates controllers for real hardware devices with software simulators through the mosaik framework.","","978-1-4799-1762-4978-1-4799-1761","10.1109/IECON.2015.7392935","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7392935","","IEC Standards;Density estimation robust algorithm;Smart grids;Logic gates;Software;Automation","power grids;power system simulation;software engineering","low-cost hardware component integration;energy systems;power systems;software-based simulations;standard-compliant co-simulation approach;hardware devices;grid operation","","3","33","","","","","","IEEE","IEEE Conferences"
"A mixed-binary linear formulation for the distribution system expansion planning problem","A. T. Pozos; M. L. de Oliveira; J. F. F. Baquero; M. J. R. Flores","Faculdade de Engenharia de Ilha Solteira, Universidade Estadual Paulista, UNESP, Brazil; Faculdade de Engenharia de Ilha Solteira, Universidade Estadual Paulista, UNESP, Brazil; Faculdade de Engenharia de Ilha Solteira, Universidade Estadual Paulista, UNESP, Brazil; Faculdade de Engenharia de Ilha Solteira, Universidade Estadual Paulista, UNESP, Brazil","2014 IEEE PES Transmission & Distribution Conference and Exposition - Latin America (PES T&D-LA)","","2014","","","1","6","This paper presents a mixed-binary linear programming formulation to solve the problem of radial distribution systems expansion planning. The proposed model considers the alternatives of repowering existing substations, allocation and sizing of new substations, reconductoring of existing feeders, construction and type selection of new feeders and changes in the system topology. The use of a mixed-binary linear model guarantees convergence to optimality using existing optimization software. In the proposed model, the steady-state operation of radial distribution systems is modelled through linear expressions. Two test systems found in the literature were used to show the efficiency of the proposed method.","","978-1-4799-6251-8978-1-4799-6250","10.1109/TDC-LA.2014.6955275","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6955275","Distribution system;expansion planning;mixed-binary linear programming;optimization","Substations;Mathematical model;Conductors;Equations;Planning;Linear programming;Integrated circuit modeling","linear programming;power distribution planning;substations","distribution system expansion planning problem;mixed-binary linear programming formulation;radial distribution system expansion planning;substations;feeder reconductoring;system topology;optimization software;linear expressions;test systems;steady-state operation","","","29","","","","","","IEEE","IEEE Conferences"
"Deployment of real-time state estimator and load flow in BC Hydro DMS - challenges and opportunities","D. Atanackovic; V. Dabic","BC Hydro, British Columbia, Canada; BC Hydro, British Columbia, Canada","2013 IEEE Power & Energy Society General Meeting","","2013","","","1","5","The object of this paper is to share experiences on deployment and tuning of real-time advanced applications for distribution and distribution state estimator at BC Hydro control center. During past 4 years, BC Hydro has been engaged in the project to procure Distribution Management System (DMS) with objective to enable real-time power system monitoring and control of distribution network in an optimal manner. The emphasis was placed on advanced network applications such as Volt-Var Optimization that are expected to improve the performance and reliability of distribution network. However, a key application that provides basic inputs to advanced applications is distribution state estimator that calculates power system state of distribution network in the real-time. Deployment of state estimator is a difficult process that relies on a number of prerequisites that include, establishing of distribution network model, calibrating and mapping real-time telemetry, extensive software testing and application tuning.","1932-5517","978-1-4799-1303","10.1109/PESMG.2013.6672408","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6672408","Distribution Management System (DMS);distribution system;State Estimator (SE);Volt-Var optimization (VVO);Real-time sequence;Closed loop control;application tuning","Real-time systems;Tuning;Weight measurement;Substations;Load modeling;Current measurement;Energy management","hydroelectric power stations;load flow;power distribution reliability;power system management;power system measurement;power system state estimation","real-time state estimator;load flow;BC hydroDMS;distribution state estimator;BC hydrocontrol center;distribution management system;real-time power system monitoring;distribution network control;Volt-Var optimization;distribution network reliability;power system state estimation;real-time telemetry mapping;software testing;application tuning","","6","6","","","","","","IEEE","IEEE Conferences"
"Design of a Broadband Planar Equiangular LPDA","Z. Yu; X. Ran; J. Yu","NA; NA; NA","2014 Fourth International Conference on Communication Systems and Network Technologies","","2014","","","41","44","Based on the principle and structure characteristics of traditional log periodic dipole antenna, combined with the advantages of micro strip antenna and broadband planar spiral antenna, a wideband planar equiangular patch log-periodic antenna with tooth type working at the center frequency 2.4GHz is presented in this paper. Then, CST software is used to construct, simulate, and optimize the antenna model. The antenna is tested by PNA3621 vector network analyzer. Measurement result shows that the antenna has good characteristics with symmetrical radiation pattern, broad bandwidth, little number of side lobes, and voltage standing wave ratio with echo loss are relatively ideal in 1.2GH~3.2GHz frequency range, which agree with the simulation results.","","978-1-4799-3070-8978-1-4799-3069","10.1109/CSNT.2014.18","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6821353","LPDA;planar;equiangular antenna;CST;performance test","Dipole antennas;Broadband antennas;Antenna radiation patterns;Microstrip antennas;Spirals;Arrays","antenna radiation patterns;broadband antennas;dipole antennas;log periodic antennas;microstrip antennas;planar antennas;spiral antennas;UHF antennas","echo loss;standing wave ratio;side lobes;symmetrical radiation pattern;PNA3621 vector network analyzer;antenna model;CST software;center frequency;wideband planar equiangular patch log-periodic antenna;broadband planar spiral antenna;microstrip antenna;log periodic dipole antenna;broadband planar equiangular LPDA;frequency 2.4 GHz","","","14","","","","","","IEEE","IEEE Conferences"
"GridSpice: A Distributed Simulation Platform for the Smart Grid","K. Anderson; J. Du; A. Narayan; A. E. Gamal","Department of Electrical Engineering, Stanford University, Stanford, CA, USA; Department of Electrical Engineering, Stanford University, Stanford, CA, USA; AutoGrid, Redwood Shores, CA, USA; Department of Electrical Engineering, Stanford University, Stanford, CA, USA","IEEE Transactions on Industrial Informatics","","2014","10","4","2354","2363","This paper describes GridSpice, a scalable open-source simulation framework for modeling, designing, and planning of the smart grid. GridSpice seamlessly integrates existing electric power simulation tools to enable modeling of large electric networks that blur the boundaries between generation, transmission, distribution, and markets. This is achieved via a cloud-based architecture that allows for parallelizing large simulation jobs across many virtual machines using a pay-as-you-go model. GridSpice simulations can be managed through a Representational State Transfer (REST) application programming interface (API), or through a Python library, allowing users to run simulations programmatically and interface with disparate data inputs, energy management systems (EMS), distribution management systems (DMS), and postprocessing tools. These capabilities make GridSpice an ideal tool for the development and testing of new grid control and optimization algorithms. GridSpice also provides an easy-to-use browser-based interface to allow novice users to begin without any setup or configuration on their local PC. A first implementation of the GridSpice framework integrates Gridlab-D and MATPOWER as simulation tools, and has been used for projects including optimizing the placement of distributed generation and developing optimal dispatch schedules for flexible loads. The GridSpice framework and Gridlab-D are freely available in open-source under the BSD license.","1551-3203;1941-0050","","10.1109/TII.2014.2332115","Cisco Systems; Energy and Environment Affiliate Program; Stanford Graduate Fellowship SGF; Stanford Electrical Engineering Research Experience for Undergraduates REU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6846273","Electric vehicles;multiagent systems;power system simulation","Load modeling;Multi-agent systems;Virtual machining;Power system simulation;Electric vehicles;Smart grids","application program interfaces;cloud computing;distributed power generation;energy management systems;power generation dispatch;power generation scheduling;power system simulation;public domain software;smart power grids;software architecture;virtual machines","GridSpice simulations;distributed simulation platform;smart grid;open-source simulation framework;electric power simulation tools;electric network modeling;cloud-based architecture;simulation job parallelization;virtual machines;pay-as-you-go model;representational state transfer application programming interface;REST API;Python library;disparate data inputs;energy management systems;EMS;distribution management systems;DMS;postprocessing tools;grid control algorithm;optimization algorithm;easy-to-use browser-based interface;Gridlab-D;MATPOWER;distributed generation placement;optimal dispatch schedule development;BSD license","","31","31","","","","","","IEEE","IEEE Journals & Magazines"
"Utilize Signal Traces from Others? A Crowdsourcing Perspective of Energy Saving in Cellular Data Communication","Z. Ou; J. Dong; S. Dong; J. Wu; A. Ylä-Jääski; P. Hui; R. Wang; A. W. Min","Department of Computer Science and Engineering, Aalto University, Espoo, Finland; Department of Computer Science and Engineering, Aalto University, Espoo, Finland; Department of Computer Science and Engineering, Aalto University, Espoo, Finland; Department of Computer Science and Engineering, Aalto University, Espoo, Finland; Department of Computer Science and Engineering, Aalto University, Espoo, Finland; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Systems and Software Research at Intel Labs, 2111 N.E. 25th Avenue, Hillsboro, OR; Systems and Software Research at Intel Labs, 2111 N.E. 25th Avenue, Hillsboro, OR","IEEE Transactions on Mobile Computing","","2015","14","1","194","207","With the tremendous growth in wireless network deployment and increasing use of mobile devices, e.g., smartphones and tablets, improving energy efficiency in such devices, especially with communication driven workloads, is critical to providing a satisfactory user experience. Studies show that signal strength plays an important role on energy consumption of cellular data communications. While energy consumption can be minimized by accurately predicting signal strengths and reacting to it in real-time, the dynamic nature of wireless environments makes signal strengths highly unpredictable. In this paper, after analyzing in detail the signal strength variation and its impact on energy consumption, we propose to use crowdsourcing approach to optimize mobile devices' energy efficiency by utilizing signal strength traces reported/shared by other users/devices in cellular networks. Via a comprehensive measurement study, we observe that signal strength traces collected from different devices are pseudo-identical, and they even exhibit similar threshold-based behaviors in the relationship between signal strength and device power consumption. Based on our observations, we propose a predictive scheduling algorithm that: (i) selects the right set of signal strength traces based on its location, (ii) applies a filter to smooth out signal strengths and hide abrupt changes, (iii) digitizes the signal strength to “good” and “bad” areas, and (iv) schedules transmissions based on power-throughput characteristics to optimize the transmission energy efficiency. To demonstrate the efficacy of the proposed algorithms, we prototype the crowdsourcing-based predicative scheduling algorithm on Android-based smartphones. Our experiment results from real-life driving tests demonstrate that, by leveraging others' signal traces, mobile devices can save energy up to 35 percent compared to the conventional opportunistic scheduling, i.e., schedule transmissions only based on instantaneous channel conditions.","1536-1233;1558-0660;2161-9875","","10.1109/TMC.2014.2316517","Finnish funding agency for technology and innovation (Tekes) in Massive Scale Machine-to-Machine Service (MAMMotH); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6786483","Energy consumption;network traffic scheduling;power control;prediction models;signal strength","Power demand;Throughput;Multiaccess communication;Spread spectrum communication;High definition video;Smart phones","cellular radio;data communication;energy conservation;power consumption;scheduling;smart phones;telecommunication power management","opportunistic scheduling;real-life driving tests;Android-based smartphones;power-throughput characteristics;predictive scheduling;power consumption;cellular networks;energy efficiency;signal strength variation;wireless environments;signal strengths;energy consumption;user experience;tablets;mobile devices;wireless network deployment;cellular data communication;energy saving;crowdsourcing perspective;signal traces","","13","32","","","","","","IEEE","IEEE Journals & Magazines"
"Synthesis of Split-Rings-Based Artificial Transmission Lines Through a New Two-Step, Fast Converging, and Robust Aggressive Space Mapping (ASM) Algorithm","J. Selga; A. Rodríguez; V. E. Boria; F. Martín","GEMMA/CIMITEC, Departament d'Enginyeria Electrònica, Universitat Autònoma de Barcelona, Bellaterra, Spain; Departamento de Comunicaciones-iTEAM, Universidad Politécnica de Valencia, Valencia, Spain; Departamento de Comunicaciones-iTEAM, Universidad Politécnica de Valencia, Valencia, Spain; GEMMA/CIMITEC, Departament d'Enginyeria Electrònica, Universitat Autònoma de Barcelona, Bellaterra, Spain","IEEE Transactions on Microwave Theory and Techniques","","2013","61","6","2295","2308","This paper is focused on the synthesis of artificial transmission lines based on complementary split ring resonators (CSRRs). The considered structures are microstrip lines with CSRRs etched in the ground plane and microstrip lines loaded with both CSRRs and series capacitive gaps. An aggressive space mapping (ASM) optimization algorithm, able to automatically generate the layout of these artificial lines, has been developed. The tool has been optimized in order to achieve fast convergence and to provide accurate results. The main relevant aspects of the proposed algorithm (based on a novel two-step ASM optimization approach) are: 1) the capability to provide the implementable circuit elements of the equivalent circuit model of the considered artificial lines and 2) the ability to converge in a few (unprecedented) iteration steps, due to a new procedure to generate the initial layouts (which are very close to the final ones). First, the software is tested through the synthesis of several CSRR-based microstrip lines, and then some practical application examples of such artificial lines are reported to illustrate the potential of the proposed synthesis tool.","0018-9480;1557-9670","","10.1109/TMTT.2013.2259254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515161","Artificial transmission lines;complementary split ring resonator (CSRR);metamaterial transmission lines;microstrip;space mapping","Integrated circuit modeling;Convergence;Power transmission lines;Layout;Geometry;Optimization;Computational modeling","microstrip lines;optimisation;resonators;transmission lines","split-rings-based artificial transmission lines;aggressive space mapping;ASM algorithm;complementary split ring resonators;CSRR;microstrip lines;optimization","","8","44","","","","","","IEEE","IEEE Journals & Magazines"
"The design of bus accessing timing to NAND flash array for high bandwidth","C. Li-xin; Y. Yang; L. Yun-yun; J. K. Seon; K. L. Man","Suzhou Institute of Nano-Tech and Nano-Bionics (SINANO), Chinese Academy of Sciences, Suzhou 215123, China; Suzhou Institute of Nano-Tech and Nano-Bionics (SINANO), Chinese Academy of Sciences, Suzhou 215123, China; Network Information Center, Harbin Engineering University, Harbin 150001, China; LS Industrial Systems, Korea; Department of Computer Science and Software Engineering, Xi'an Jiaotong-Liverpool University, Suzhou 215123, China","2013 International SoC Design Conference (ISOCC)","","2013","","","274","277","To improve the reading and writing speed of NAND flash array with multi-channel and multi-way, and to obtain the highest available bandwidth, an approach is presented in this paper. One mechanism of high efficiency bus accessing timing scheme based on interleaving is introduced into the approach. In pursuance of this timing, flash controllers are able to make every plane in the flash array to be active in parallel. Therefore the transmission ability to and from the bus can be greatly improved. Utilizing this approach, the accessing efficiency to the NAND flash array will be pushed to a very high level. According to the testing results, as opposed to normal flash bus timing scheme, the data reading efficiency can be increased by 68.5%, and the data writing efficiency can be increased by 457%, with the flash bus timing scheme presented in this paper being used. The conclusion can be drawn that the flash bus timing scheme presented in this paper is effective, and the reading and writing at very high speed to the NAND flash array can be realized.","","978-1-4799-1142","10.1109/ISOCC.2013.6864026","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6864026","NAND Flash;Multi Channel;Interleaving Access;Bus Timing Optimizing;High Bandwidth","Timing;Flash memories;Arrays;Bandwidth;Solids;Clocks;Data communication","flash memories;logic design;NAND circuits","bus accessing timing design;NAND flash array;reading speed;writing speed;high-efficiency bus accessing timing scheme;flash controllers;transmission ability;accessing efficiency;normal flash bus timing scheme;data reading efficiency;data writing efficiency","","","12","","","","","","IEEE","IEEE Conferences"
"The research on network performance management system based on SDN technology","T. Lin; W. Chao","Key Laboratory of Special Fiber Optics and Optical Access Networks, Shanghai University, Shanghai, China; Key Laboratory of Special Fiber Optics and Optical Access Networks, Shanghai University, Shanghai, China","2015 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC)","","2015","","","1","5","Network management plays an important role in improving the utilization ratio of network unit, positioning and troubleshooting fault, optimizing the network performance, ensuring network security and service quality. SDN (Software Defined Networking) has brought new chances and challenges for the development of network management technology. Thus, this paper has designed a network performance management system based on SDN. SNMP4J class library is used to develop network management system of the underlying application in this system. The data forwarding and controlling is separated through the SDN technology, which realizes functions: the collection and analysis of data, alarm and automatic control. Some test cases indicated that this system has many advantages, such as intelligence, high reliability and security, and so on.","","978-1-4799-8920-1978-1-4799-8918-8978-1-4799-8919","10.1109/ICSPCC.2015.7338924","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7338924","SDN (Software Defined Network);network performance management;SNMP (Simple Network Management Protocol)","Switches;Performance evaluation;Data analysis;Performance analysis;Data acquisition;Ports (Computers);Packet loss","computer network performance evaluation;software defined networking","SDN technology;network performance management system;software defined networking;network security;service quality;SNMP4J class library","","","9","","","","","","IEEE","IEEE Conferences"
"A TEO-Based Algorithm to Detect Events Over OTDR Measurements in FTTH PON Networks","G. Flavio Mendes Lima; E. Afonso Lamounier; S. Barcelos; A. Cardoso; I. Santos Peretta; E. Rigon; W. Sadaiti Muramoto","NA; NA; NA; NA; NA; NA; NA","IEEE Latin America Transactions","","2013","11","3","886","891","The FTTH business needs new network maintenance technologies that can, economically and effectively, cope with the massive FTTH fiber plants that are yet to come. Based on the Teager Energy Operator (TEO), we have developed a method for testing and evaluating FTTH networks from the Central Office, which allows the identification of event failures in the optical branches after the PON splitter.","1548-0992","","10.1109/TLA.2013.6568828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6568828","FTTH;OTDR signal Optimization;PON networks;Teager Energy Operator (TEO)","Passive optical networks;Optical fiber subscriber loops;Optical fibers;Software;Optical fiber testing","optical time-domain reflectometry;passive optical networks;telecommunication network management;telecommunication network reliability","TEO-based algorithm;event detection;OTDR measurement;FTTH PON network;FTTH business;network maintenance technology;FTTH fiber plant;Teager energy operator;FTTH network testing;FTTH network evaluation;event failure identification;optical branch;PON splitter","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Trace-Guided Synthesis of Reactive Behavior Models of Programmable Logic Controllers","R. Schatz; H. Prähofer","NA; NA","2013 39th Euromicro Conference on Software Engineering and Advanced Applications","","2013","","","260","267","Programmable Logic Controller (PLC) programs are programs that control physical devices by continuously reading sensor inputs and writing actuator outputs. A main challenge in designing and comprehending PLC programs is the emergent behavior which arises from the complex interaction between the dynamic behavior of the program and the physical device. In this paper we present an approach for building a formal model characterizing the reactive interaction behavior of a PLC program with the physical device it controls. Based on program recordings, first a model of the transition behavior of the program run is built. Then, using symbolic execution and a formal abstraction process, we generate a specification of the input/output behavior as a state model with transition labelings in terms of conditions on input values. We present the main ideas of the approach, a formal model for representing the reactive behavior, the abstraction process, and two application scenarios.","1089-6503;2376-9505","978-0-7695-5091","10.1109/SEAA.2013.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6619520","program comprehension;reverse engineering;symbolic execution;dynamic analysis;test coverage;programmable logic controllers","Reverse engineering;Symbolic execution;Programmable logic controllers;Optimization;Actuators","control engineering computing;program verification;programmable controllers;sensor fusion;symbol manipulation","trace-guided synthesis;reactive behavior models;programmable logic controller programs;physical device control;actuator output writing;sensor input reading;program dynamic behavior;formal model;PLC program reactive interaction behavior;program recordings;symbolic execution;formal abstraction process;input-output behavior;state model;transition labelings;abstraction process","","2","18","","","","","","IEEE","IEEE Conferences"
"Design of a sensorless controller for PMSM using Krill Herd algorithm","A. Younesi; S. Tohidi","Faculty of Electrical and Computer Engineering, University of Tabriz, Iran; Faculty of Electrical and Computer Engineering, University of Tabriz, Iran","The 6th Power Electronics, Drive Systems & Technologies Conference (PEDSTC2015)","","2015","","","418","423","Due to advantages of the sensorless cascade controllers, i.e. simple structure and no need to use the mechanical sensors, they are widely used in industry. An important problem with such controllers is setting their parameters in order to achieve suitable response. This paper presents a sensorless control scheme for a permanent magnet synchronous motor (PMSM) based on optimizing the parameters of the speed and torque PI-Controllers, using Krill Herd (KH) algorithm. They are optimized to minimize the speed tracking error in steady state. Since the proposed method uses the discrete-time model, it does not depend on initial conditions of integrators. The system is tested under variable operating conditions. Simulation results with MATLAB/Simulink software show a satisfactory performance of the proposed controller against load disturbances as well as robustness against machine parameters' variations.","","978-1-4799-7653-9978-1-4799-7652","10.1109/PEDSTC.2015.7093311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7093311","Krill Herd optimization algorithm;Electrical and mechanical sensorless control;Discrete time model;Permanet magnet synchronous machine (PMSM)","Simulation;Inductance;Switches;Rotors;Inverters;Tracking","angular velocity control;permanent magnet motors;PI control;sensorless machine control;synchronous motors;torque control","permanent magnet synchronous motor;PMSM;Krill Herd algorithm;sensorless cascade controllers;speed controller;torque controller;PI-controller;speed tracking error;discrete-time model;variable operating conditions;MATLAB-Simulink software","","4","15","","","","","","IEEE","IEEE Conferences"
"HEROIC: Homomorphically EncRypted One Instruction Computer","N. G. Tsoutsos; M. Maniatakos","Computer Science and Engineering, New York University Polytechnic School of Engineering; Electrical and Computer Engineering, New York University Abu Dhabi","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","6","As cloud computing becomes mainstream, the need to ensure the privacy of the data entrusted to third parties keeps rising. Cloud providers resort to numerous security controls and encryption to thwart potential attackers. Still, since the actual computation inside cloud microprocessors remains unencrypted, the opportunity of leakage is theoretically possible. Therefore, in order to address the challenge of protecting the computation inside the microprocessor, we introduce a novel general purpose architecture for secure data processing, called HEROIC (Homomorphically EncRypted One Instruction Computer). This new design utilizes a single instruction architecture and provides native processing of encrypted data at the architecture level. The security of the solution is assured by a variant of Paillier's homomorphic encryption scheme, used to encrypt both instructions and data. Experimental results using our hardware-cognizant software simulator, indicate an average execution overhead between 5 and 45 times for the encrypted computation (depending on the security parameter), compared to the unencrypted variant, for a 16-bit single instruction architecture.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800460","Encrypted processor;homomorphic encryption;Paillier;cloud computing;one instruction set computer","Encryption;Computers;Memory management;Optimization","cloud computing;cryptography;data privacy","HEROIC;homomorphically encrypted one instruction computer;cloud computing;data privacy;security controls;cloud microprocessors;general purpose architecture;data processing security;single instruction architecture;Paillier's homomorphic encryption scheme;hardware-cognizant software simulator;encrypted computation","","1","19","","","","","","IEEE","IEEE Conferences"
"Optimizing Concurrent Query Execution on Modern Multisocket Multicore Platform","F. Xi; T. Mishima; H. Yokota","NA; NA; NA","2014 IEEE 33rd International Symposium on Reliable Distributed Systems Workshops","","2014","","","125","130","The upcoming generation of computer hardware brought several new challenges for underling software. The number of cores on a chip grows exponentially, allowing for an ever-increasing number of processes to execute in parallel. Therefore, how to efficiently utilize all of the concurrent processing ability provided by multi-core platform becomes critical for achieving good system performances. We propose a CARIC-DA framework which optimizes the performance of DBMS on multicore platforms. Our middleware can improve the performance of the private cache levels by dispatching concurrent database queries to run on different processor cores according to the data needs of each query. Extensive experiments from microbenchmark to TPC-C benchmark have confirmed the effectiveness of CARIC-DA in improving the cache performance and providing higher throughput for DBMSs on modern AMD multicore platform. However, the multicore platforms are becoming more and more diverse and complex in recent years. Different manufactures are making different efforts to boost their performance by adopting different techniques. Therefore, it is important for software to be well turned according to the specific hardware features in order to fully take advantage of the target multicore platforms. In this paper, we analyzed the efficiency of our CARIC-DA framework on the modern multicore Intel platform. We compared the performance of different deployment strategies where we vary the mapping relationship between database processes and processor cores from Hyper-Threading enable to disable configurations. We evaluated the impact of CARIC-DA for different cache levels and for different query types on the Intel multicore platform.","","978-1-4799-7361","10.1109/SRDSW.2014.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000148","OLTP;multicore platform;middleware","Multicore processing;Middleware;Databases;Hardware;Proposals;Sockets","benchmark testing;concurrency control;middleware;multiprocessing systems;multi-threading;performance evaluation;query processing","concurrent query execution;multisocket multicore platform;computer hardware;concurrent processing;parallel processing;CARIC-DA framework;DBMS performance;middleware;concurrent database query dispatching;TPC-C benchmark;cache performance;AMD multicore platform;hyper-threading;Intel multicore platform","","","10","","","","","","IEEE","IEEE Conferences"
"Using Dual Priority scheduling to improve the resource utilization in the nMPRA microcontrollers","N. C. Gaitan; L. Andries","Faculty of Electrical Engineering and Computer Science, Stefan cel Mare University of Suceava, Suceava, Romania; Faculty of Electrical Engineering and Computer Science, Stefan cel Mare University of Suceava, Suceava, Romania","2014 International Conference on Development and Application Systems (DAS)","","2014","","","73","78","The current practice in most of the safety-critical areas, including automotive, avionics systems and factory automation, encouraging the use of real-time time-trigger schedulers that does not allow interference to take place between safety-critical components and non-critical. Furthermore, in these systems the lack of interference between safety-critical components and non-critical components is achieved by a strict isolation between components with different degrees of severity. This approach can assure, easily, the certification of the safety-critical functionality, but leads to very low resource utilization. For this purpose it will be presented a solution that when the system enters into a state that is different from the normal running state (test service), allowing relaxation and a change in the activation time of tasks (release) violating the fixed priorities scheduling, but avoiding starvation of the system tasks. The proposed solution modifies a static scheduler in a dynamic scheduler depending on the system status using Dual Priority scheduling. The algorithm has been proposed to be implemented on a nMPRA processor, by multiplying hardware resources (PC, pipeline registers and file registers) and other facilities (events, mutexes, interrupts, IPC communication, timer's, the static scheduler and support for dynamic scheduler) provides a switching and response time for events within 1 to 3 machine cycles.","","978-1-4799-5094-2978-1-4799-5092","10.1109/DAAS.2014.6842431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842431","real time system;dual priority scheduling;nMPRA microcontroller","Processor scheduling;Real-time systems;Registers;Dynamic scheduling;Operating systems;Heuristic algorithms","microcontrollers;object-oriented programming;processor scheduling;program diagnostics;real-time systems;safety-critical software","dual priority scheduling;resource utilization;nMPRA microcontroller;safety-critical area;automotive;avionics systems;factory automation;real-time time-trigger scheduler;safety-critical component;noncritical component;safety-critical functionality;normal running state;test service;fixed priorities scheduling;system task;static scheduler;dynamic scheduler;system status;nMPRA processor;multiplying hardware resource;pipeline registers;file registers;IPC communication","","2","12","","","","","","IEEE","IEEE Conferences"
"A web tool for assessing the energy use of buildings in Greece: First results from real life application","I. Papastamatiou; V. Marinakis; H. Doukas; J. Psarras","Decision Support Systems Laboratory, School of Electrical & Computer Engineering, National Technical University of Athens, Greece; Decision Support Systems Laboratory, School of Electrical & Computer Engineering, National Technical University of Athens, Greece; Decision Support Systems Laboratory, School of Electrical & Computer Engineering, National Technical University of Athens, Greece; Decision Support Systems Laboratory, School of Electrical & Computer Engineering, National Technical University of Athens, Greece","2015 6th International Conference on Information, Intelligence, Systems and Applications (IISA)","","2015","","","1","6","Nowadays, cities are key to meet sustainability objectives in the European Union. Almost 75% of European habitants live in cities and the majority of cities authorities have committed to increase energy efficiency by improving buildings, equipment and facilities' performance. Actually, about 35% of the EU's buildings are over 50 years old and by improving the energy efficiency of buildings, the total EU energy consumption can be reduced by 5% to 6% and lower CO<sub>2</sub> emissions by about 5%. In this context, the main aim of this paper is to present an online web tool, for improving the energy performance of existing buildings in Greece based on the actual energy use. The application is able to collect real energy consumption data and provide, with a user friendly way, a toolset which calculates energy savings from a list of improvement scenarios. The online software calculates automatically the economic viability of the chosen actions. The web tool has been tested in different buildings in Greece and it is developed using open source software and programming languages such as PHP and MySQL. The application has been developed by the Decision Support Systems Laboratory (DSS), of the School of Electrical &amp; Computer Engineering, National Technical University of Athens (NTUA). The web tool has been used from the students of NTUA for the academic year 2014-2015, resulting in a number of completed case studies. The main outcomes from the 1st year of the web tool's real life application are summarised in the paper, offering useful results about the energy building capacity in the capital of Greece.","","978-1-4673-9311-9978-1-4673-9310","10.1109/IISA.2015.7387977","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387977","Buildings;Energy efficiency;Economic evaluation;Sustainable development;Energy optimisation scenarios;ICT systems","Buildings;Energy consumption;Energy efficiency;Economics;Insulation;Europe;Air conditioning","building management systems;control engineering computing;energy management systems;Internet;programming languages;public domain software","energy building capacity;programming language;open source software;economic viability;European Union;EU energy consumption;energy efficiency;Greece;building energy use;online Web tool","","","18","","","","","","IEEE","IEEE Conferences"
"Automatic specification granularity tuning for design space exploration","J. Zhang; G. Schirner","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","6","Algorithm Design Environments (ADE), such as Simulink, have been shown to be efficient for development, analysis, and evaluation of algorithms. Recent tools propose to facilitate algorithm / architecture co-design by bridging the gap from ADE to System-Level Design Environments (SLDE) through automatic synthesis from algorithm models to SLDL specifications. With the wide range of block characteristic (from simple logic functions to complex kernels) in the algorithm model, however, it is challenging to select a suitable compositional granularity for SLD Language (SLDL) blocks in the synthesized specification. A high volume of SLDL blocks of little computation will increase the number of mapping possibilities, whereas large blocks with heavy computation on the other hand allow inter-block fusion reducing the computational demands in the overall specification yet sacrificing the mapping flexibility. In this paper, we introduce an automatic specification granularity tuning mechanism to determine the granularity in the synthesized specification model hierarchy guided by the computational demands of algorithm blocks. Our granularity selection significantly simplifies the early design space exploration as only a meaningful block decomposition is exposed in the synthesized specification. It leads to an overall system with less computational demands by leveraging the block fusion capabilities in the ADE. At the same time our granularity decision ensures that sufficient flexibility remains in the system for exploring heterogeneous mapping of the algorithm. Our results on real world examples show that specification models can be synthesized with 80% efficiency through block fusion with 70-90% fewer but coarser grained blocks.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.227","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800428","","Computational modeling;Algorithm design and analysis;Software packages;Tuning;Optimization;Unified modeling language;Merging","electronic design automation;multiprocessing systems;system-on-chip","mapping flexibility;synthesized specification model hierarchy;granularity selection;granularity decision;block decomposition;interblock fusion;SLDL blocks;system level design language;automatic synthesis;SLDE;system-level design environments;architecture co-design;Simulink;ADE;algorithm design environments;design space exploration;automatic specification granularity tuning mechanism","","","16","","","","","","IEEE","IEEE Conferences"
"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)","B. H. Menze; A. Jakab; S. Bauer; J. Kalpathy-Cramer; K. Farahani; J. Kirby; Y. Burren; N. Porz; J. Slotboom; R. Wiest; L. Lanczi; E. Gerstner; M. Weber; T. Arbel; B. B. Avants; N. Ayache; P. Buendia; D. L. Collins; N. Cordier; J. J. Corso; A. Criminisi; T. Das; H. Delingette; Ç. Demiralp; C. R. Durst; M. Dojat; S. Doyle; J. Festa; F. Forbes; E. Geremia; B. Glocker; P. Golland; X. Guo; A. Hamamci; K. M. Iftekharuddin; R. Jena; N. M. John; E. Konukoglu; D. Lashkari; J. A. Mariz; R. Meier; S. Pereira; D. Precup; S. J. Price; T. R. Raviv; S. M. S. Reza; M. Ryan; D. Sarikaya; L. Schwartz; H. Shin; J. Shotton; C. A. Silva; N. Sousa; N. K. Subbanna; G. Szekely; T. J. Taylor; O. M. Thomas; N. J. Tustison; G. Unal; F. Vasseur; M. Wintermark; D. H. Ye; L. Zhao; B. Zhao; D. Zikic; M. Prastawa; M. Reyes; K. Van Leemput","Institute for Advanced Study and Department of Computer Science, Technische Universität München, Munich, Germany; Computer Vision Laboratory, ETH, Zürich, Switzerland; Institute for Surgical Technology and Biomechanics, University of Bern, Switzerland; Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA; Cancer Imaging Program, National Cancer Institute, National Institutes of Health, Bethesda, MD, USA; Cancer Imaging Program, National Cancer Institute, National Institutes of Health, Bethesda, MD, USA; Support Center for Advanced Neuroimaging (SCAN), Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Switzerland; Support Center for Advanced Neuroimaging (SCAN), Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Switzerland; Support Center for Advanced Neuroimaging (SCAN), Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Switzerland; Support Center for Advanced Neuroimaging (SCAN), Institute for Diagnostic and Interventional Neuroradiology, Inselspital, Bern University Hospital, Switzerland; University of Debrecen, Debrecen, Hungary; Department of Neuro-oncology, Massachusetts General Hosptial, Harvard Medical School, Boston, MA, USA; Diagnostic and Interventional Radiology, University Hospital, Heidelberg, Germany; Centre for Intelligent Machines, McGill University, Canada; Penn Image Computing and Science Lab, Department of Radiology, University of Pennsylvania, Philadelphia, PA, USA; Asclepios Project, Inria, Sophia-Antipolis, France; INFOTECH Soft, Inc., Miami, FL, USA; McConnell Brain Imaging Centre, McGill University, Canada; Asclepios Project, Inria, Sophia-Antipolis, France; Computer Science and Engineering, SUNY, Buffalo, NY, USA; Microsoft Research, Cambridge, UK; Cambridge University Hospitals, Cambridge, UK; Asclepios Project, Inria, Sophia-Antipolis, France; Computer Science Department, Stanford University, Stanford, CA, USA; Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA, USA; INRIA Rhône-Alpes, Grenoble, France; INRIA Rhône-Alpes, Grenoble, France; Department of Electronics, University Minho, Portugal; INRIA Rhône-Alpes, Grenoble, France; Asclepios Project, Inria, Sophia-Antipolis, France; BioMedIA Group, Imperial College, London, UK; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Radiology, Columbia University, New York, NY, USA; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; Vision Lab, Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA; Cambridge University Hospitals, Cambridge, UK; INFOTECH Soft, Inc., Miami, FL, USA; Athinoula A. Martinos Center for Biomedical Imaging, Massachusetts General Hospital and Harvard Medical School, Boston, MA, USA; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Life and Health Science Research Institute (ICVS), School of Health Sciences, University of Minho, Braga, Portugal; Institute for Surgical Technology and Biomechanics, University of Bern, Switzerland; Department of Electronics, University Minho, Portugal; School of Computer Science, McGill University, Canada; Cambridge University Hospitals, Cambridge, UK; Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Vision Lab, Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA; INFOTECH Soft, Inc., Miami, FL, USA; Computer Science and Engineering, SUNY, Buffalo, NY, USA; Department of Radiology, Columbia University, New York, NY, USA; Sutton, UK; Microsoft Research, Cambridge, UK; Department of Electronics, University Minho, Portugal; Life and Health Science Research Institute (ICVS), School of Health Sciences, University of Minho, Braga, Portugal; Centre for Intelligent Machines, McGill University, Canada; Computer Vision Laboratory, ETH, Zürich, Switzerland; INFOTECH Soft, Inc., Miami, FL, USA; Cambridge University Hospitals, Cambridge, UK; Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA, USA; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey; INRIA Rhône-Alpes, Grenoble, France; Department of Radiology and Medical Imaging, University of Virginia, Charlottesville, VA, USA; Electrical and Computer Engineering, Purdue University, USA; Computer Science and Engineering, SUNY, Buffalo, NY, USA; Department of Radiology, Columbia University, New York, NY, USA; Microsoft Research, Cambridge, UK; GE Global Research, Niskayuna, NY, USA; Institute for Surgical Technology and Biomechanics, University of Bern, Switzerland; Department of Radiology, Massachusetts General Hospital, Harvard Medical School, Boston, MA, USA","IEEE Transactions on Medical Imaging","","2015","34","10","1993","2024","In this paper we report the set-up and results of the Multimodal Brain Tumor Image Segmentation Benchmark (BRATS) organized in conjunction with the MICCAI 2012 and 2013 conferences. Twenty state-of-the-art tumor segmentation algorithms were applied to a set of 65 multi-contrast MR scans of low- and high-grade glioma patients - manually annotated by up to four raters - and to 65 comparable scans generated using tumor image simulation software. Quantitative evaluations revealed considerable disagreement between the human raters in segmenting various tumor sub-regions (Dice scores in the range 74%-85%), illustrating the difficulty of this task. We found that different algorithms worked best for different sub-regions (reaching performance comparable to human inter-rater variability), but that no single algorithm ranked in the top for all sub-regions simultaneously. Fusing several good algorithms using a hierarchical majority vote yielded segmentations that consistently ranked above all individual algorithms, indicating remaining opportunities for further methodological improvements. The BRATS image data and manual annotations continue to be publicly available through an online evaluation system as an ongoing benchmarking resource.","0278-0062;1558-254X","","10.1109/TMI.2014.2377694","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975210","MRI;Brain;Oncology/tumor;Image segmentation;Benchmark","Image segmentation;Educational institutions;Benchmark testing;Biomedical imaging;Lesions","benchmark testing;biomedical MRI;brain;image segmentation;medical image processing;tumours","BRATS;Multimodal Brain Tumor Image Segmentation Benchmark;MICCAI 2012 conference;MICCAI 2013 conference;tumor segmentation algorithm;multicontrast MR scans;glioma patients;tumor image simulation software;Dice scores;human interrater variability;hierarchical majority vote","Algorithms;Benchmarking;Glioma;Humans;Magnetic Resonance Imaging;Neuroimaging","387","133","","","","","","IEEE","IEEE Journals & Magazines"
"Energy characterization and instruction-level energy model of Intel's Xeon Phi processor","Y. S. Shao; D. Brooks","Harvard University; Harvard University","International Symposium on Low Power Electronics and Design (ISLPED)","","2013","","","389","394","Intel's Xeon Phi is the first commercial many-core/multi-thread ×86-based processor. Xeon Phi belongs to a new breed of high performance computing processors that seek high compute density as well as energy efficiency. However, no highlevel energy model is available for Xeon Phi software developers to quickly evaluate and optimize energy efficiency. This work demonstrates an instruction-level energy model for the Xeon Phi processor to facilitate the development of energy-efficient software. In order to construct this model, we first characterize the energy consumption of the processor, identifying how energy per instruction scales with the number of cores, the number of active threads per core, and instruction types. Based on the energy characterization, we construct an instruction-level energy model and validate the accuracy of the model between 1% and 5% for real world benchmarks. We show that the energy model can be used to identify software inefficiencies for these benchmarks and find that Linpack code can be optimized to increase energy efficiency by as much as 10%.","","978-1-4799-1235-3978-1-4799-1234","10.1109/ISLPED.2013.6629328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6629328","Xeon Phi;Energy Characterization;Instruction-Level Energy Model","Radiation detectors;Vectors;Computational modeling;Prefetching;Bandwidth","benchmark testing;energy consumption;microprocessor chips;multiprocessing systems;multi-threading;power aware computing","energy characterization;instruction-level energy model;Intel Xeon Phi processor;commercial manycore multithread x86-based processor;high performance computing processors;energy efficiency;high-level energy model;Xeon Phi software developers;energy-efficient software;processor energy consumption;Linpack code","","29","8","","","","","","IEEE","IEEE Conferences"
"FPGA based event building and data acqiusition system for the COMPASS experiment","Y. Bai; M. Bodlak; V. Frolov; V. Jary; S. Huber; I. Konorov; D. Levit; J. Novy; R. Salach; D. Steffen; M. Virius; S. Paul","Physikdepartment E18, Technische Universität München; Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University; Joint Institute for Nuclear Research; Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University; Physikdepartment E18, Technische Universität München; Physikdepartment E18, Technische Universität München; Physikdepartment E18, Technische Universität München; Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University; Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University; Physikdepartment E18, Technische Universität München; Faculty of Nuclear Sciences and Physical Engineering, Czech Technical University; Physikdepartment E18, Technische Universität München","2015 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC)","","2015","","","1","2","The purely software based old data acquisition system of the COMPASS experiment at CERN is replaced by the new hybrid FPGA-software system. The two level FPGA subsystem takes over the data handling role. The hardware is built in a compact AMC form factor using a Xilinx Virtex-6 VLX130T FPGA as a data processor. The data handling includes a 15:1 data link multiplexing and complete event building in firmware. All high speed links of the system are commutated by the 144×144 channels cross point switch which is used to provide dynamic load balancing. The designed throughput of the system is 1.6 GB/s while the maximum estimated data rate of the experiment is 1.5 GB/s for the 50 kHz trigger rate. The system makes use of the accelerator spill structure for optimizing the load on the read-out PCs. The distributed software runs on a server farm and integrates control and configuration functionality. The software closely monitors data flow for consistency on all stages of the event building process to perform load balancing and error recovery. The prototype of the system was successfully tested during the commissioning of the experiment and the full system will be used in the physics run 2015.","","978-1-4673-9862-6978-1-4673-9863","10.1109/NSSMIC.2015.7581844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7581844","","Multiplexing;Switches;Field programmable gate arrays;Data acquisition;Buildings;Software;Hardware","field programmable gate arrays;high energy physics instrumentation computing","FPGA based event building;data acqiusition system;COMPASS experiment;CERN;hybrid FPGA-software system;compact AMC form factor;Xilinx Virtex-6 VLX130T FPGA;dynamic load balancing","","","3","","","","","","IEEE","IEEE Conferences"
"Usefulness of infeasible solutions in evolutionary search: An empirical and mathematical study","L. While; P. Hingston","School of Computer Science &amp; Software Engineering, The University of Western Australia, Australia; School of Computer and Security Science, Edith Cowan University, Australia","2013 IEEE Congress on Evolutionary Computation","","2013","","","1363","1370","When evolutionary algorithms are used to solve constrained optimization problems, the question arises how best to deal with infeasible solutions in the search space. A recent theoretical analysis of two simple test problems argued that allowing infeasible solutions to persist in the population can either help or hinder the search process, depending on the structure of the fitness landscape. We report new empirical and mathematical analyses that provide a different interpretation of the previous theoretical predictions: that the important effect is on the probability of finding the global optimum, rather than on the time complexity of the algorithm. We also test a multiobjective approach to constraint-handling, and with an additional test problem we demonstrate the superiority of this multiobjective approach over the previous single-objective approaches.","1089-778X;1941-0026","978-1-4799-0454-9978-1-4799-0453-2978-1-4799-0451-8978-1-4799-0452","10.1109/CEC.2013.6557723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557723","evolutionary algorithms;constraint-handling;multi-objective optimization","Algorithm design and analysis;Sociology;Statistics;Equations;Mathematical model;Prediction algorithms;Evolutionary computation","constraint handling;evolutionary computation;mathematical analysis;search problems","infeasible solutions;evolutionary search;evolutionary algorithms;constrained optimization problems;search space;fitness landscape structure;mathematical analysis;empirical analysis;global optimum finding probability;multiobjective approach;constraint handling","","2","24","","","","","","IEEE","IEEE Conferences"
"Real Time Modeling of Interlocking Control System of Rawalpindi Cantt Train Yard","U. Khan; J. Ahmad; T. Saeed; S. Hayat","NA; NA; NA; NA","2015 13th International Conference on Frontiers of Information Technology (FIT)","","2015","","","347","352","Interlocking system is a safety critical system which governs the safe movement of trains in a train yard. Recent advancement in technology has enabled railway organizations world over to optimize their operations by using software based automated solutions. Since interlocking systems are not only complex but also safety critical, these systems should be modeled and verified against safety requirements to weed out any design bugs which when discovered during testing or deployment phase in the system life-cycle, will result in high cost overruns and can cause catastrophes. Timed automata have effectively been used for the modeling and verification of real-time safety critical systems. In this paper, we model Rawalpindi Cantt (Pakistan) train yard using timed automata and verify its safety properties using UPPAAL model checker. This verified model can effectively be used to implement the design which will be more reliable as compared to the systems which are verified by classical methods of testing and simulation.","","978-1-4673-9666-0978-1-4673-9665","10.1109/FIT.2015.28","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7421026","","Safety;Automata;Rail transportation;Rails;Tracking;Real-time systems;Software","finite automata;program debugging;program verification;railway engineering;railways;safety-critical software","real time modeling;interlocking control system;Rawalpindi Cantt Train Yard;railway organization;software-based automated solution;design bug;system life-cycle;timed automata;real-time safety critical system;UPPAAL model checker","","3","11","","","","","","IEEE","IEEE Conferences"
"Real-time high-quality stereo vision system in FPGA","W. Wang; J. Yan; N. Xu; Y. Wang; F. Hsu","Microsoft Research Asia, Microsoft Building, 2, HaiDian, Beijing, China; Microsoft Research Asia, Microsoft Building, 2, HaiDian, Beijing, China; Microsoft Research Asia, Microsoft Building, 2, HaiDian, Beijing, China; Electronic Engineering Department, TNLIST, Tsinghua University, Beijing, China; Microsoft Research Asia, Microsoft Building, 2, HaiDian, Beijing, China","2013 International Conference on Field-Programmable Technology (FPT)","","2013","","","358","361","Stereo vision is a well-known technique for acquiring depth information. In this paper, we present an FPGA-based real-time high-quality stereo vision system. By using AD-Census cost initialization, cross-based aggregation and semi-global optimization, the system provides high-quality depth results for highdefinition images. This is the first complete real-time hardware system that supports both cost aggregation on cross-based regions and semi-global optimization on FPGA. The system can adjust image resolution, parallelism degree, and support region size to achieve maximum efficiency flexibly during the implementation. We test the accuracy of the system on the Middlebury benchmark and some real-world scenarios with different image resolutions. The results show the accuracy is among the best of FPGA-based stereo vision systems and competitive with current top-performing software implementations. We demonstrate the system using an Altera Stratix-IV FPGA board, processing 1024 × 768 pixel images at 30 frames per second.","","978-1-4799-2198-0978-1-4799-2199","10.1109/FPT.2013.6718387","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6718387","","Stereo vision;Optimization;Field programmable gate arrays;Hardware;Parallel processing;Accuracy;Image resolution","field programmable gate arrays;image resolution;optimisation;stereo image processing","real-time high-quality stereo vision system;AD-census cost initialization;cross-based aggregation;semiglobal optimization;high-definition image;real-time hardware system;image resolution;parallelism degree;Middlebury benchmark;Altera Stratix-IV FPGA board","","14","15","","","","","","IEEE","IEEE Conferences"
"Minimizing stack memory for hard real-time applications on multicore platforms","C. Dong; H. Zeng","McGill University, Montreal, Canada; McGill University, Montreal, Canada","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","6","Multicore platforms are increasingly used in realtime embedded applications. In the development of such applications, an efficient use of RAM memory is as important as the effective scheduling of software tasks. Preemption Threshold Scheduling is a well-known technique for controlling the degree of preemption, possibly improving system schedulability, and allowing savings in stack space. In this paper, we target at the optimal mapping of tasks to cores and the assignment of the scheduling parameters for systems scheduled with preemption thresholds. We formulate the optimization problems using Mixed Integer Linear Programming framework, and propose an efficient heuristic as an alternative. We demonstrate the efficiency and quality of both approaches with extensive experiments using random systems as well as two industrial case studies.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.041","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800242","","Job shop scheduling;Heuristic algorithms;Multicore processing;Runtime;Real-time systems;Simulated annealing;Indexes","integer programming;linear programming;multiprocessing systems;random-access storage;scheduling","stack memory minimization;hard-real-time application;multicore platform;realtime embedded application;RAM memory;software task scheduling;preemption threshold scheduling;preemption degree;system schedulability;optimal mapping;scheduling parameters;optimization problem;mixed integer linear programming framework;random systems;industrial case study","","","22","","","","","","IEEE","IEEE Conferences"
"Accelerating arithmetic kernels with coherent attached FPGA coprocessors","H. Giefers; R. Polig; C. Hagleitner","IBM Research - Zurich; IBM Research - Zurich; IBM Research - Zurich","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","1072","1077","The energy efficiency of computer systems can be increased by migrating computational kernels that are known to under-utilize the CPU to an FPGA based coprocessor. In contrast to traditional I/O-based coprocessors that require explicit data movement, coherently attached accelerators can operate on the same virtual address space than the host CPU. A shared memory organization enables widely accepted programming models and helps to deploy energy efficient accelerators in general purpose computing systems. In this paper we study an FFT accelerator on FPGA attached via the Coherent Accelerator Processor Interface (CAPI) to a POWER8 processor. Our results show that the coherent attached accelerator outperforms device driver based approaches in terms of latency. Hardware acceleration delivers a 5× gain in energy efficiency compared to an optimized parallel software FFT running on a 12-core CPU and improves single thread performance by more than 2×. We conclude that the integration of CAPI into heterogeneous programming frameworks such as OpenCL will facilitate latency critical operations and will further enhance programmability of hybrid systems.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.1123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092548","","Kernel;Field programmable gate arrays;Hardware;Programming;Coprocessors;Computer architecture","coprocessors;fast Fourier transforms;field programmable gate arrays;parallel processing;power aware computing;shared memory systems","arithmetic kernel acceleration;coherent attached FPGA coprocessors;computer system energy efficiency;computational kernels;CPU;FPGA based coprocessor;I/O-based coprocessors;shared memory organization;programming models;FFT accelerator;coherent accelerator processor interface;POWER8 processor;device driver based approach;optimized parallel software FFT;12-core CPU;CAPI;heterogeneous programming frameworks;OpenCL;hybrid system programmability","","2","17","","","","","","IEEE","IEEE Conferences"
"Optimal power flow with limited and discrete controls","W. Murray; T. T. De Rubira; A. Wigington","Stanford University, Stanford, CA; Stanford University, Stanford, CA; Electric Power Research Institute, Palo Alto, CA","2015 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT)","","2015","","","1","5","Operators solve optimal power flow problems to determine how to adjust generator dispatches and control devices in order to maintain a power system running in a secure and efficient manner. However, optimal power flow software typically make use of a number of control adjustments that is impractical for an operator to execute due to time constraints. Moreover, the common rounding techniques for handling the discrete nature of control devices, in particular switched shunts, may result in unnecessarily poor solutions. These deficiencies are addressed by exploring the use of a sparsity-inducing penalty to obtain a more manageable number of control adjustments, and the use of a distributed line-search for exploring the space of discrete variables. The benefits and computational requirements of these techniques have been evaluated on two real North American power networks of approximately 2.5k buses.","","978-1-4799-1785","10.1109/ISGT.2015.7131799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7131799","","Power systems;Optimization;Space exploration;Generators;Software;Switches;Benchmark testing","discrete systems;load flow control;power generation dispatch;search problems","discrete controls;optimal power flow problems;generator dispatches;control devices;power system;optimal power flow software;common rounding techniques;sparsity-inducing penalty;distributed line-search;discrete variables;real North American power networks","","2","21","","","","","","IEEE","IEEE Conferences"
"Fair and delay adaptive scheduler for UC and NGN networks","A. M. Elnaka; Q. H. Mahmoud; Xining Li","School of Computer Science, University of Guelph, ON, Canada N1G 2W1; Dept. of Electrical, Computer and Software Engineering, University of Ontario Institute of Technology, Oshawa, Canada L1H 7K4; School of Computer Science, University of Guelph, ON, Canada N1G 2W1","2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE)","","2014","","","1","6","Fair bandwidth allocation while conforming to stringent end-to-end delay constraints is a major requirement for the successful delivery of next generation QoS demanding traffic. Research has been carried out in the area of processor and scheduler sharing for decades to try to achieve the QoS requirements of network traffic. Fairness and traffic prioritization are two main objectives that many schedulers were originally designed to meet. Another important issue is how schedulers treat the delay sensitive traffic. Although the combination of fairness and prioritization is implemented in several schedulers but, to our knowledge, incorporating adaptive traffic delay treatment in fair and prioritized schedulers has not yet been successfully implemented. In this paper, we introduce a new scheduler that balances between the fairness of bandwidth allocation between flows while implementing prioritization and minimizes the number of end-to-end delay bound breaches. The scheduler combines the virtual clock concept used in well-known fair schedulers together with schedulability testing and evaluation implemented in delay sensitive schedulers. The scheduler is designed to achieve the fairness of bandwidth allocation, such as in fair schedulers, while minimizing the number of possible violation of end-to-end QoS delay of individual flows' packets.","0840-7789","978-1-4799-3101-9978-1-4799-3099","10.1109/CCECE.2014.6900970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900970","UC;NGN;EDF;WFQ;QoS","Delays;Bandwidth;Quality of service;Next generation networking;Scheduling;Schedules;Scheduling algorithms","adaptive scheduling;bandwidth allocation;delays;next generation networks;quality of service;telecommunication traffic","fair adaptive scheduler;delay adaptive scheduler;UC Network;NGN Network;fair bandwidth allocation;end-to-end delay constraint;next generation QoS demanding traffic prioritization;delay sensitive traffic;virtual clock concept;unified communication network;next generation network;delay sensitive scheduler","","2","11","","","","","","IEEE","IEEE Conferences"
"Analyzing communication models for distributed thread-collaborative processors in terms of energy and time","B. Klenk; L. Oden; H. Froning","University of Heidelberg Institute of Computer Engineering Heidelberg, Germany; Fraunhofer Institute for Industrial Mathematics Competence Center High Performance Computing Kaiserslautern, Germany; University of Heidelberg Institute of Computer Engineering Heidelberg, Germany","2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)","","2015","","","318","327","Accelerated computing has become pervasive for increasing the computational power and energy efficiency in terms of GFLOPs/Watt. For application areas with highest demands, for instance high performance computing, data warehousing and high performance analytics, accelerators like GPUs or Intel's MICs are distributed throughout the cluster. Since current analyses and predictions show that data movement will be the main contributor to energy consumption, we are entering an era of communication-centric heterogeneous systems that are operating with hard power constraints. In this work, we analyze data movement optimizations for distributed heterogeneous systems based on CPUs and GPUs. Thread-collaborative processors like GPUs differ significantly in their execution model from generalpurpose processors like CPUs, but available communication models are still designed and optimized for CPUs. Similar to heterogeneity in processing, heterogeneity in communication can have a huge impact on energy and time. To analyze this impact, we use multiple workloads with distinct properties regarding computational intensity and communication characteristics. We show for which workloads tailored communication models are essential, not only reducing execution time but also saving energy. Exposing the impact in terms of energy and time for communication-centric heterogeneous systems is crucial for future optimizations, and this work is a first step in this direction.","","978-1-4799-1957","10.1109/ISPASS.2015.7095817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7095817","","Graphics processing units;Computational modeling;Instruction sets;Data transfer;Benchmark testing;Bandwidth","data warehouses;graphics processing units;parallel processing;power aware computing","communication models;distributed thread-collaborative processors;accelerated computing;computational power;energy efficiency;instance high performance computing;data warehousing;high performance analytics;GPU;Intel MIC;data movement;communication-centric heterogeneous systems;hard power constraints;data movement optimizations;CPU;general-purpose processors;computational intensity;communication characteristics;tailored communication models","","3","37","","","","","","IEEE","IEEE Conferences"
"Interdiction Analysis of Electric Grids Combining Cascading Outage and Medium-Term Impacts","Y. Wang; R. Baldick","Department of Electrical and Computer Engineering, University of Texas, Austin; Department of Electrical and Computer Engineering, University of Texas, Austin","IEEE Transactions on Power Systems","","2014","29","5","2160","2168","This paper presents an improved interdiction model to identify maximal electric grid attacks. The contribution of the model is that it incorporates both short-term (seconds to minutes) and medium-term (minutes to days) impacts of the possible attack. The medium-term impacts are examined by an interdicted dc optimal power flow model. The short-term impacts are addressed by a cascading outage analysis model that uses a set of sequentially applied checkers to perform the simulation of the cascading outage events and assess the short-term impacts of a blackout subsequent to specified terrorist attacks. An integer programming heuristic is applied that can utilize standard optimization software (e.g. CPLEX) to solve master problems generated by the heuristic. The proposed model has been verified using the IEEE 300-Bus Test System and IEEE RTS 96 Test System. Discussions of the results and future research plans are also presented in this paper.","0885-8950;1558-0679","","10.1109/TPWRS.2014.2300695","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6730731","Cascading outages;interdiction;power flow;power system security","Load modeling;Power system stability;Terrorism;Transient analysis;Stability analysis;Relays","heuristic programming;integer programming;load flow;power grids;power system reliability;power system security","electric grid interdiction analysis;medium-term impacts;cascading outage-term impacts;maximal electric grid attack identification;interdicted dc optimal power flow model;cascading outage analysis model;cascading outage event simulation;integer programming heuristic;standard optimization software;IEEE 300-bus test system;IEEE RTS 96 test system","","13","43","","","","","","IEEE","IEEE Journals & Magazines"
"An Adaptive Auto-configuration Tool for Hadoop","C. Li; H. Zhuang; K. Lu; M. Sun; J. Zhou; D. Dai; X. Zhou","NA; NA; NA; NA; NA; NA; NA","2014 19th International Conference on Engineering of Complex Computer Systems","","2014","","","69","72","With the coming concept of 'big data', the ability to handle large datasets has become a critical consideration for the success of industrial organizations such as Google, Amazon, Yahoo! and Facebook. As an important Cloud Computing framework for bulk data processing, Hadoop is widely used in these organizations. However, the performance of MapReduce is seriously limited by its stiff configuration strategy. Even for a single simple job in Hadoop, a large number of tuning parameters have to be set by users. This may easily lead to performance loss due to some misconfigurations. In this paper, we present an adaptive automatic configuration tool (AACT) for Hadoop to achieve performance optimization. To achieve this goal, we propose a mathematical model which will accurately learn the relationship between system performance and configuration parameters, then configure Hadoop system based on this mathematical model. With the help of AACT, Hadoop is able to adapt the hardware and software configurations dynamically and drive the system to an optimal configuration in acceptable time. Experimental results show its efficiency and adaptability, and that it is ten times faster compared with default configuration.","","978-1-4799-5482-7978-1-4799-5481","10.1109/ICECCS.2014.17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923119","Hadoop;Auto-Configuration;Self-Learning","System performance;Hardware;Benchmark testing;Optimization;Cloud computing;Mathematical model;Computers","Big Data;cloud computing;social networking (online)","adaptive auto-configuration to;hadoop;big data;industrial organizations;Google;Amazon;Yahoo!;Facebook;cloud computing framework;bulk data processing;MapReduce;tuning parameters;adaptive automatic configuration tool;AACT;performance optimization;mathematical model;hardware configuration;software configuration;optimal configuration","","3","14","","","","","","IEEE","IEEE Conferences"
"Error recovery in digital microfluidics for personalized medicine","M. Ibrahim; K. Chakrabarty","Department of Electrical and Computer Engineering Duke University, Durham, NC 27708, USA; Department of Electrical and Computer Engineering Duke University, Durham, NC 27708, USA","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","247","252","Due to its emergence as an efficient platform for point-of-care clinical diagnostics, design optimization of digital-microfluidic biochips (DMFBs) has received considerable attention in recent years. In particular, error recoverability is of key interest in medical applications due to the need for system reliability. Errors are likely during droplet manipulation due to defects, chip degradation, and the lack of precision inherent in biochemical experiments. We present an illustrative survey on recently proposed techniques for error recovery. The parameters of the error-recovery design space are shown and evaluated for these schemes. Next, we make use of these evaluations to describe how they can guide error recovery in DMFBs. Finally, an experimental case study is presented to demonstrate how an error-recovery scheme can be applied to real-life biochips.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.1126","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092390","","Electrodes;Charge coupled devices;Software;Arrays;System-on-chip;Reliability;Monitoring","biological techniques;computational fluid dynamics;diseases;lab-on-a-chip;medicine;microfluidics;molecular biophysics;patient treatment","infectious disease treatment improvement;infectious disease management improvement;biochemical experiments;chip degradation;droplet manipulation;system reliability;error recoverability;point-of-care clinical diagnostics;design optimization;personalized medicine;DMFB;digital-microfluidic biochips;error-recovery design space","","2","23","","","","","","IEEE","IEEE Conferences"
"FPGA implementation of PSO-based object tracking system using SSIM","N. N. Morsi; M. B. Abdelhalim; K. A. Shehata","Electronics and Communications Department, College of Engineering and Technology, Arab Academy for Science and Technology and Maritime Transport (AASTMT), Cairo, Egypt; College of Computing and Information Technology (CCIT), Arab Academy for Science and Technology and Maritime Transport (AASTMT), Cairo, Egypt; Electronics and Communications Department, College of Engineering and Technology, Arab Academy for Science and Technology and Maritime Transport (AASTMT), Cairo, Egypt","2013 25th International Conference on Microelectronics (ICM)","","2013","","","1","4","In this paper, we present implementation of object tracking system targeting video sequence based on Particle Swarm Optimization (PSO) using Structural SIMilarity index (SSIM) as PSO's fitness function on field programmable gate array (FPGA). The proposed algorithm's performance has been tested and evaluated over different video sequences, showing its adaption for tracking in real-time. After presenting PSO algorithm and SSIM index, we show how the system's architecture has been simplified for FPGA implementation and achieved better results than its software implementation counterpart.","2159-1660;2159-1679","978-1-4799-3570-3978-1-4799-3569","10.1109/ICM.2013.6735003","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735003","Particle Swarm Optimization (PSO);Structural SIMilarity (SSIM);Object Tracking;Hardware Implementation;Field Programmable Gate Array (FPGA)","Atmospheric measurements;Particle measurements;Missiles;Probability distribution;Clocks;Pattern recognition;Pipelines","field programmable gate arrays;image sequences;particle swarm optimisation;video signal processing","FPGA implementation;PSO-based object tracking system;video sequence;particle swarm optimization;structural similarity index;PSO fitness function;field programmable gate array;SSIM index;system architecture","","","18","","","","","","IEEE","IEEE Conferences"
"Resilient tree-based live streaming in reality","B. Schiller; G. Nguyen; T. Strufe","P2P Networks, TU Darmstadt, Germany; P2P Networks, TU Darmstadt, Germany; P2P Networks, TU Darmstadt, Germany","IEEE P2P 2013 Proceedings","","2013","","","1","2","Our main contribution in this work is a deployable multitree-push system for P2P-based live streaming. It runs on both desktop PCs and Android-based mobile devices. Additionally, it provides controlling, monitoring, and measurement functionalities which help with debugging in the development phase, visualize the topology during a demonstration, and support the deployment of test scenarios in a distributed setting. Besides, the generic architecture of the system also allows for the extension to other classes of streaming systems.","2161-3559;2161-3567","978-1-4799-0515","10.1109/P2P.2013.6688735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688735","","Peer-to-peer computing;Topology;Streaming media;Monitoring;Bandwidth;Optimization;Network topology","computer network security;peer-to-peer computing;program debugging;software architecture;video streaming","resilient tree;deployable multitree-push system;P2P-based live streaming;desktop PCs;Android-based mobile devices;controlling functionality;monitoring functionality;measurement functionality;debugging;development phase;topology visualization;generic architecture;denial-of-service attack;video streaming","","","2","","","","","","IEEE","IEEE Conferences"
"How to Extract Knowledge from Professional E-Mails","R. Francois; M. Nada; A. Hassan","NA; NA; NA","2015 11th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)","","2015","","","687","692","Computer mediated communication is ubiquitous in Software design projects. Email is used for project coordination, but also for design, implementation and test. Especially with currents agile development methods, it is very common to interact through computer meditated communication like email, instant messaging and other collaborative tools in order to express functional needs, notify of issues and take appropriate decisions. In this paper we propose a Knowledge Trace Retrieval (KTR) system. It addresses the problem of retrieving elements of problem solving and design rationale inside business emails from a project. Even if knowledge management tools and practices are well spread in industry, they are rarely used for small projects. Our system aims at helping user retrieve traces of problem solving knowledge in large corpus of email from a past project. The framework and methodology is based on enhanced context (project data, user competencies and profiles), and use machine learning technics and ranking algorithm.","","978-1-4673-9721","10.1109/SITIS.2015.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7400638","Knowledge Management;Traceability;Problem Solving;e-mails;project memory","Electronic mail;Problem-solving;Context;Pragmatics;Speech;XML;Software","computer mediated communication;electronic mail;knowledge acquisition;knowledge management;learning (artificial intelligence);problem solving;software prototyping","professional e-mail;knowledge extract;software design project;agile development method;computer mediated communication;collaborative tool;knowledge trace retrieval system;KTR system;knowledge management tool;problem solving knowledge;machine learning","","","22","","","","","","IEEE","IEEE Conferences"
"Grid connection and power quality optimization of wind power plants","F. B. Uzunlar; Ö. Guler; O. Kalenderli","Istanbul Technical University, Energy Institute, Istanbul, Turkey; Istanbul Technical University, Energy Institute, Istanbul, Turkey; Istanbul Technical University, Department of Electrical Engineering, Istanbul, Turkey","2015 9th International Conference on Electrical and Electronics Engineering (ELECO)","","2015","","","415","419","This paper covers the integration of the wind turbines with the national grids which includes the quality of the power generated at the wind plants, the impact to the power systems and the details of connection systems. The system has been considered in detail together with PSS-E (Power System Simulation for Engineering) software program. The scope of the study consists of two parts: technical and economical. 30 MW Wind Power Plant consisting of 13 pieces of 2.3 MW asynchronous WTG of type 690 V, 50 Hz is investigated due to the grid compliance of WPP according to Turkish Grid Code for Wind Turbines (Annex 18) in technical part. Firstly, the reactive capability of the wind power plant and comparison of it with the requirements of the Grid Code is determined. Secondly, the low voltage right through (LVRT) capability of the wind park is assessed. Finally, the system frequency and voltage test shall be applied. In the economic section, by changing some of technical parameters, cost-benefit analysis of the economic impact will be made in order to get use of maximum wind potential that could be obtained.","","978-6-0501-0737","10.1109/ELECO.2015.7394528","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7394528","","Reactive power;Wind turbines;Voltage control;Wind power generation;Steady-state;Power transformers;Integrated circuit modeling","power grids;power supply quality;wind power plants;wind turbines","LVRT;low voltage right through;Annex 18;Turkish grid code;asynchronous WTG;power system simulation for engineering;PSS-E software program;connection systems;power systems;wind plants;national grids;wind turbines;wind power plants;power quality optimization;grid connection;power 30 MW;power 2.3 MW;voltage 690 V;frequency 50 Hz","","","15","","","","","","IEEE","IEEE Conferences"
"WIMAGINE: Wireless 64-Channel ECoG Recording Implant for Long Term Clinical Applications","C. S. Mestais; G. Charvet; F. Sauter-Starace; M. Foerster; D. Ratel; A. L. Benabid","CEA-LETI-CLINATEC, Grenoble, FRANCE; CEA-LETI-CLINATEC, Grenoble, FRANCE; MINATEC Campus, CEA-LETI-CLINATEC, Grenoble, FRANCE; CEA-LETI-CLINATEC, Grenoble, FRANCE; CEA-LETI-CLINATEC, Grenoble, FRANCE; CEA-LETI-CLINATEC, Grenoble, FRANCE","IEEE Transactions on Neural Systems and Rehabilitation Engineering","","2015","23","1","10","21","A wireless 64-channel ElectroCorticoGram (ECoG) recording implant named WIMAGINE has been designed for various clinical applications. The device is aimed at interfacing a cortical electrode array to an external computer for neural recording and control applications. This active implantable medical device is able to record neural activity on 64 electrodes with selectable gain and sampling frequency, with less than 1 μV RMS input referred noise in the [0.5 Hz - 300 Hz] band. It is powered remotely through an inductive link at 13.56 MHz which provides up to 100 mW. The digitized data is transmitted wirelessly to a custom designed base station connected to a PC. The hermetic housing and the antennae have been designed and optimized to ease the surgery. The design of this implant takes into account all the requirements of a clinical trial, in particular safety, reliability, and compliance with the regulations applicable to class III AIMD. The main features of this WIMAGINE implantable device and its architecture are presented, as well as its functional performances and long-term biocompatibility results.","1534-4320;1558-0210","","10.1109/TNSRE.2014.2333541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6846362","Active implantable medical device (AIMD);brain–computer interface (BCI);ElectroCorticoGrams (ECoG);neural interface;neural prosthesis","Implants;Electrodes;Arrays;Wireless communication;Base stations;Titanium;Throughput","biomedical electrodes;biomedical electronics;brain-computer interfaces;medical control systems;neural nets;phantoms;prosthetics;surgery;wireless channels","Wireless Implantable Multichannel Acquisition System for Generic Interface with Neurons;WIMAGINE long-term biocompatibility;WIMAGINE functional performances;WIMAGINE architecture;WIMAGINE features;class III AIMD;implant design;surgery;hermetic housing;digitized data;inductive link;neural activity;medical device;control applications;neural recording;external computer;cortical electrode array;long term clinical application;ECoG recording implant;wireless 64-channel ElectroCorticoGram","Animals;Brain-Computer Interfaces;Electrodes, Implanted;Electroencephalography;Electronics;Equipment Design;Humans;Macaca fascicularis;Macaca mulatta;Materials Testing;Neural Prostheses;Signal Processing, Computer-Assisted;Software;Wireless Technology","38","41","","","","","","IEEE","IEEE Journals & Magazines"
"Extending the Capabilities of Mobile Devices for Online Social Applications through Cloud Offloading","A. Olteanu; N. Tapus; A. Iosup","NA; NA; NA","2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing","","2013","","","160","163","Handheld devices are becoming an attractive option for users to interact with their social network, through online social applications. We are witnessing a rapid adoption of smarter devices all around us, which brings with it orders of magnitude in heterogeneity. Thus, researchers in the field of distributed systems are faced with new challenges: How to optimize performance for devices that are so diverse in terms of energy consumption, processing power and communication capabilities? My PhD research focuses on this challenge, adopting techniques for offloading operations from mobile to more powerful cloud-based infrastructure, and brings a three-fold contribution. First, we have characterized and modeled workloads of online social applications, and empirically validated them using traces of hundreds of real applications. Second, we are currently investigating offloading mechanisms, including: communication offloading, lossy performance offloading, and loss less performance offloading. We have been testing and evaluating these mechanisms with several mobile applications, measuring performance and energy consumption. Third, we will create an integrated cloud based offloading system that aims to improve the performance of online social applications. We will empirically evaluate this system using both simulations and open-source real-world applications.","","978-0-7695-4996-5978-1-4673-6465","10.1109/CCGrid.2013.52","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6546075","mobile;cloud offloading;workload characterization;workload modeling;scheduling;allocation","Mobile communication;Educational institutions;Energy consumption;Smart phones;Data models;Home automation","cloud computing;mobile computing;power aware computing;public domain software;social networking (online);software performance evaluation","mobile devices;handheld devices;social network;online social applications;distributed systems;performance optimization;energy consumption;processing power;communication capability;cloud-based infrastructure;communication offloading;lossy performance offloading;lossless performance offloading;integrated cloud-based offloading system;performance improvement;open-source real-world applications","","6","23","","","","","","IEEE","IEEE Conferences"
"Efficient characteristic 3 Galois field operations for elliptic curve cryptographic applications","V. S. Iyengar","Oregon Episcopal School, Portland, Oregon, U.S.A.","2013 International Conference on Security and Cryptography (SECRYPT)","","2013","","","1","6","Galois fields of characteristic 3, where the number of field elements is a power of 3, have a distinctive application in building high-security elliptic curve cryptosystems. However, they are not typically used because of their relative inefficiency in computing polynomial operations when compared to conventional prime or binary Galois fields. The purpose of this research was to design and implement characteristic 3 Galois field arithmetic algorithms with greater overall efficiency than those presented in current literature, and to evaluate their applicability to elliptic curve cryptography. The algorithms designed were tested in a C++ program and using a mapping of field element logarithms, were able to simplify the operations of polynomial multiplication, division, cubing, and modular reduction to that of basic integer operations. They thus significantly outperformed the best characteristic 3 algorithms presented in literature and showed a distinct applicability to elliptic curve cryptosystems. In conclusion, this research presents a novel method of optimizing the performance of characteristic 3 Galois fields and has major implications for the field of elliptic curve cryptography.","","978-9-8975-8131","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223211","Public-Key Cryptography;Elliptic Curves;Characteristic 3 Galois Field Theory;Performance Optimization","Galois fields;Polynomials;Algorithm design and analysis;Elliptic curves;Elliptic curve cryptography;Software algorithms;Finite element analysis","","","","","20","","","","","","IEEE","IEEE Conferences"
"Optimization weather parameters influencing rainfall prediction using Adaptive Network-Based Fuzzy Inference Systems (ANFIS) and linier regression","D. Munandar","Research Center for Informatics, Indonesian Institute of Sciences, Bandung, Indonesia","2015 International Conference on Data and Software Engineering (ICoDSE)","","2015","","","154","159","This paper conducted a study to investigate the ability of Adaptive Network-Based Fuzzy Inference System (ANFIS) in doing modeling to determine the weather parameters that influence the output parameters of rainfall (RF) and have good predictive ability. Plotting the data of the prediction is also made to the Linear Regression (LR). The data is tested daily at the weather station in Badau area, Belitung province, Indonesia. A total consisting of 433 pairs of data for 1 year containing seven weather parameters as input and one parameter as output. As for the performance evaluation criteria used indicator of the ability of ANFIS statistic model: Pearson correlation coefficient (r), coefficient of determination (R2) and root mean squared error (RMSE), from several input parameters in the analysis, 1-input RHmax most optimal influencing rainfall (RF) output, (RMSE = 1.8896 mm / day at the training phase and RMSE = 3.2370 mm / day at the checking phase). Plot the data ANFIS against Linear Regression, 1-input parameter RHmax has optimal value of the influence of rainfall (RF) output with optimal statistical indicator (R2 = 0.7065, r = 0.8405, RMSE = 0.8732 mm / day).","","978-1-4673-8430-8978-1-4673-8428-5978-1-4673-8427-8978-1-4673-8429","10.1109/ICODSE.2015.7436989","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436989","Ontology;ANFIS;Linear Regression;Parameter combinations;Plotting data","Meteorology;Predictive models;Radio frequency;Linear regression;Mathematical model;Fuzzy logic;Adaptation models","","","","","17","","","","","","IEEE","IEEE Conferences"
"Embedded real-time MPC implementation using Rapid Prototyping Tools: A thermal process case study","G. Harja; A. Hernandez; R. De Keyser; I. Nascu","Department of Electrical energy, Systems and Automation, Ghent University, Belgium; Department of Electrical energy, Systems and Automation, Ghent University, Belgium; Department of Electrical energy, Systems and Automation, Ghent University, Belgium; Department of Automation, Technical University of Cluj-Napoca, Romania","2013 17th International Conference on System Theory, Control and Computing (ICSTCC)","","2013","","","250","255","Recently, a strong interest has been observed in the development of new control strategies for solving optimization problems on embedded devices. Particularly, Model Predictive Control (MPC) became quite popular nowadays due to its multiple successful implementations in real-life industrial applications. The aim of this contribution is to provide practical information throughout the use of VeriStand and CompactRIO as Rapid Prototyping Tools (RPT) for implementing MPC algorithms from Matlab/Simulink code. The effectiveness of this tool is tested on a nonlinear thermal process with variable time delay, in which three different implementations of MPC to deal with both nonlinear dynamics and time delay were evaluated.","","978-1-4799-2228-4978-1-4799-2227","10.1109/ICSTCC.2013.6688968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6688968","","Delay effects;Mathematical model;Electron tubes;Process control;Prediction algorithms;Real-time systems;MATLAB","control engineering computing;delays;embedded systems;nonlinear dynamical systems;optimisation;predictive control;process control;software prototyping","embedded real-time MPC implementation;control strategies;optimization problems;embedded devices;model predictive control;real-life industrial applications;VeriStand;CompactRIO;rapid prototyping tools;RPT;MPC algorithms;Matlab-Simulink code;nonlinear thermal process;variable time delay;nonlinear dynamics","","2","14","","","","","","IEEE","IEEE Conferences"
"Automatic deployment of industrial embedded model predictive control using qpOASES","D. K. M. Kufoalor; B. J. T. Binder; H. J. Ferreau; L. Imsland; T. A. Johansen; M. Diehl","Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), O.S. Bragstads plass 2D N-7491 Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), O.S. Bragstads plass 2D N-7491 Trondheim, Norway; ABB Corporate Research, Segelhofstrasse 1K, CH-5405 Baden-Dättwil, Switzerland; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), O.S. Bragstads plass 2D N-7491 Trondheim, Norway; Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU), O.S. Bragstads plass 2D N-7491 Trondheim, Norway; Institute of Microsystems Engineering (IMTEK) University of Freiburg, Germany","2015 European Control Conference (ECC)","","2015","","","2601","2608","Different high-speed quadratic programming (QP) solvers are incorporated into an ANSI C code generation framework for embedded Model Predictive Control (MPC). The controllers developed are based on step response (linear) models and design configurations obtained from SEPTIC, Statoil's software tool for MPC applications. In order to achieve high online computational efficiency, offline computations/preparations are made at the code generation stage, and appropriate problem data are used in the QP solvers. We discuss implementation aspects arising when running an embedded MPC controller on an industrial PLC and present results of hardware-in-the-loop simulation tests for two challenging industrial applications. The results indicate that the online active-set strategy as implemented in the software package qpOASES exhibits superior performance compared to both a tailored interior-point method and a primal-dual first-order method for the step response class of models considered in this paper.","","978-3-9524-2693-7978-3-9524-2694","10.1109/ECC.2015.7330930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7330930","","Generators;Optimization;Yttrium;IP networks;Predictive control;Predictive models;Europe","C language;control system analysis computing;control system synthesis;embedded systems;predictive control;program compilers;quadratic programming;software tools;step response","automatic industrial embedded model predictive control deployment;qpOASES;high-speed quadratic programming solvers;ANSI C code generation framework;step response models;design configurations;SEPTIC;Statoil software tool;online computational efficiency;code generation stage;QP solvers;embedded MPC controller;industrial PLC;online active-set strategy;tailored interior-point method;primal-dual first-order method;step response class","","8","23","","","","","","IEEE","IEEE Conferences"
"Development and design of a platform for arbitration and sharing control applications","J. Pérez; D. González; F. Nashashibi; G. Dunand; F. Tango; N. Pallaro; A. Rolfsmeier","INRIA, France; INRIA, France; INRIA, France; Intempora, France; Centro Ricerche FIAT, Italy; Centro Ricerche FIAT, Italy; dSpace, Germany","2014 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS XIV)","","2014","","","322","328","In this paper, a description of the ADAS development platform in DESERVE project framework is presented. This work is framed within the Sub Project 2 (Development platform) of DESERVE project, and it is divided in 6 different and complementary lines of work. Most of the functions described in the tools and development systems, perception layer and the platform system architecture show the modularity and scalability of our proposal. Moreover, based on vehicle modelling, driver behaviour and intention, a first approach for arbitration and control strategies, which can anticipate the priorities on the control in emergency situations, is described. Furthermore, some simulations will allow the virtual testing for the future implementation in demonstrators. The presented work is the core of DESERVE project, and it is developed in parallel with Driver behaviour and HMI activities (SP3). This work presents some of the achievements in SP2, mainly the application platform integration in one of the demonstrators, along with the arbitration and sharing control, based on intelligent techniques (Fuzzy logic). Simulation shows the feasibility of proposal. This approach will be tested, integrated and validated in a real vehicle in the next stages of the project.","","978-1-4799-3770","10.1109/SAMOS.2014.6893228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6893228","ADAS;Embedded Systems;Arbitration;Control;Virtual testing;modelling","Vehicles;Computer architecture;Computational modeling;Field programmable gate arrays;Hardware;Software;Fuzzy logic","driver information systems;fuzzy logic;road traffic control","sharing control application;ADAS development platform;DESERVE project framework;perception layer;platform system architecture;vehicle modelling;driver behaviour;emergency situation;virtual testing;HMI activities;intelligent technique;fuzzy logic;advanced driver assistance systems","","2","12","","","","","","IEEE","IEEE Conferences"
"Practical out-of-band interference reduction for OFDM systems","A. Selim; L. Doyle","The Telecommunications Research Center (CTVR), Trinity College, University of Dublin, Ireland; The Telecommunications Research Center (CTVR), Trinity College, University of Dublin, Ireland","2013 IEEE Global Communications Conference (GLOBECOM)","","2013","","","3510","3515","Orthogonal frequency-division multiplexing (OFDM) systems suffers from large out-of-band (OOB) emissions caused by high sidelobes of the modulated subcarriers. In the literature, performing considerable OOB emissions reduction at all frequencies in the OOB regions can be done at the expense of wasting the available resources drastically. In this paper, we introduce the idea of combining two efficient OOB emissions reduction techniques (the shaping and the advanced cancellation carriers (ACC)). These two techniques are chosen to enable considerable reduction at all frequencies in the OOB regions while at the same time to utilize the available resources. Also, these techniques do not require performing complex optimizations (which is essential for practical implementations). The proposed combination is implemented in a software defined radio and tested for over-the-air transmissions.","1930-529X","978-1-4799-1353","10.1109/GLOCOM.2013.6831617","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6831617","","Throughput;Peak to average power ratio;Signal to noise ratio;Degradation;Optimization","interference suppression;OFDM modulation;radiofrequency interference;software radio","out-of-band interference reduction;OFDM systems;orthogonal frequency division multiplexing;OOB emissions reduction technique;shaping technique;advanced cancellation carrier;software defined radio;over-the-air transmission","","1","20","","","","","","IEEE","IEEE Conferences"
"Enhancement of Incremental Performance Parameter Estimation on ppOpen-AT","R. Murata; J. Irie; A. Fujii; T. Tanaka; T. Katagiri","NA; NA; NA; NA; NA","2015 IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip","","2015","","","203","210","We propose an efficient implementation of discretized spline function (d-Spline) based incremental performance parameter estimation (IPPE) on 2D parameter space. IPPE generates a data fitting function based on d-Splines for a few sampling points, inserting each new sampling point successively until the optimal parameter among chosen parameter values stops changing. This method updates a data fitting function according to a new sampling parameter value for every iteration. D-Splines can be calculated by solving the least-squares problem, in which the number of unknowns is equal to the number of combinations of performance parameter values. In this study, we propose an efficient implementation method based on the iterative solver such as a conjugate gradient method. In a prior work, the least-square problem was solved using QR decomposition. It can efficiently update d-Spline functions according to new sampling parameter values but requires substantial memory usage. Through numerical tests, our proposed method can reduce the memory usage significantly, and its performance is comparable with QR decomposition. Moreover, we implement this method in ppOpen-AT, a framework that adds auto-tuning functionality to user code by inserting a few lines of tuning directives. In addition, to test the effectiveness of our method, it is applied to the algebraic multigrid method that has many performance parameters and is used repeatedly in one simulation, e.g., a fluid simulation. Finally, our method found near-optimal parameters with a little cost for searching parameters. It used only 25 combinations among the 160 parameter combinations for the test. Its performance difference from the optimal parameter was only 5%. Our method turned out to be twice as fast as the brute-force search method.","","978-1-4799-8670-5978-1-4799-8669","10.1109/MCSoC.2015.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328206","Auto-Tuning;Performance Parameter Optimization;Data Fitting Function","Matrix decomposition;Parameter estimation;Estimation;Splines (mathematics);Memory management;Jacobian matrices;Mathematical model","conjugate gradient methods;least squares approximations;sampling methods;software performance evaluation;splines (mathematics)","incremental performance parameter estimation;discretized spline function;d-Spline function;2D parameter space;data fitting function;sampling point;sampling parameter value;least-squares problem;conjugate gradient method;QR decomposition;ppOpen-AT framework;algebraic multigrid method;software automatic performance tuning;brute-force search method","","3","9","","","","","","IEEE","IEEE Conferences"
"A generic infrastructure for OpenCL performance analysis","R. Dietrich; R. Tschüter","Center for Information Services and High Performance Computing (ZIH), Technische Universität Dresden, 01062 Dresden, Germany; Center for Information Services and High Performance Computing (ZIH), Technische Universität Dresden, 01062 Dresden, Germany","2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)","","2015","1","","334","341","OpenCL is an open standard for programming of parallel heterogeneous systems. It is designed for portability, therefore being utilized in the area of embedded system programming as well as high performance computing (HPC). Due to the applicability on different platforms, OpenCL library vendors have a certain freedom in implementing parts of the OpenCL execution model. Multiple versions of the standard increase the diversity of OpenCL implementations. Sophisticated performance analysis tools are required to optimize the performance of an OpenCL program for a specific OpenCL implementation and therewith efficiently utilize available hardware resources. This paper presents a generic tools interface for performance measurement of OpenCL programs. We depict the functionality and implementation of a respective measurement library and its integration in the performance infrastructure Score-P. Tests with a synthetic and several real-world OpenCL applications on AMD, ARM, NVIDIA, and Intel platforms validate the applicability of our framework.","","978-1-4673-8361-5978-1-4673-8359-2978-1-4673-8358-5978-1-4673-8360","10.1109/IDAACS.2015.7340754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340754","performance analysis;performance measurement;OpenCL","Libraries;Kernel;Performance evaluation;Computational modeling;Runtime;Performance analysis;Standards","embedded systems;parallel programming;software libraries;software performance evaluation;software tools","generic infrastructure;OpenCL performance analysis;parallel heterogeneous system programming;embedded system programming;high performance computing;HPC;OpenCL library vendors;OpenCL execution model;performance analysis tools;performance measurement;measurement library;performance infrastructure","","3","15","","","","","","IEEE","IEEE Conferences"
"Performance study of MIMO-OFDM platform in narrow-band sub-1 GHz wireless LANs","S. Aust; R. V. Prasad; I. G. M. M. Niemegeers","NEC Communication Systems, Ltd., 1753 Shimonumabe, Nakahara-ku, Kawasaki, Kanagawa 211-8666, Japan; EEMCS, Delft University of Technology, P.O. Box 5031, 2600 GA, The Netherlands; EEMCS, Delft University of Technology, P.O. Box 5031, 2600 GA, The Netherlands","2013 11th International Symposium and Workshops on Modeling and Optimization in Mobile, Ad Hoc and Wireless Networks (WiOpt)","","2013","","","89","94","Using multiple antennas at the transceivers has become a necessity in high data rate wireless communication systems. Multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) has been adopted as a mandatory modulation scheme in the upcoming IEEE 802.11ah standard. IEEE 802.11ah will specify carrier frequencies for Wireless Local Area Networks (WLANs) operating in the sub-1 GHz band. This is challenging for MIMO-OFDM in sub-1 GHz because of limited bandwidth. Thus building a prototype will provide necessary information to understand this new scenario. We build a prototype in order to test and validate modifications to physical layer (PHY) and media access control (MAC) for narrow-band data transmissions. We present here, in detail, the steps to build a real-time MIMO-OFDM testing platform that is useful for evaluating narrow-band sub-1GHz transmission characteristics. We analyzed potential MIMO-OFDM implementations and conducted extensive measurements on our platform in order to verify the required system enhancements. This paper gives a hands-on account of designing and testing a sub-1 GHz WLAN platform.","","978-3-901882-54-8978-1-61284-824","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6576417","WLAN;IEEE 802.11ah;sub-1 GHz;MIMO-OFDM;narrow-band;platform","OFDM;MIMO;Wireless LAN;Software;Modulation;Hardware;IEEE 802.11n Standard","access protocols;data communication;MIMO communication;OFDM modulation;wireless LAN","narrow-band data transmission;MAC;media access control;PHY;physical layer;wireless local area network;IEEE 802.11ah standard;multiple-input multiple-output orthogonal frequency division multiplexing;high data rate wireless communication system;transceiver;multiple antenna;narrow-band Sub-1 GHz wireless LAN;MIMO-OFDM platform;frequency 1 GHz","","1","15","","","","","","IEEE","IEEE Conferences"
"Droplets, vapours and clouds - A new approach to capacitive transducer manufacture","R. L. O'Leary; G. Brown; G. Harvey","Dept of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, UK; Dept of Electronic and Electrical Engineering, University of Strathclyde, Glasgow, UK; PZFlex, Weidlinger Associates, Glasgow, UK","2013 IEEE International Ultrasonics Symposium (IUS)","","2013","","","1113","1116","Capacitive ultrasonic transducers (cUT) comprise a substrate patterned with a regular array of uniformly dimensioned cavities above which a membrane is positioned. Transmission and reception of ultrasound is effected via controlled electrical or mechanical stimulus of the membrane, respectively. The dimensions of the cavity and the mechanical properties of the membrane determine the vibrational behavior of the resultant transducer. This paper employs a facile process for the micropatterning of polymer substrates for the manufacture of capacitive transducers. A positive mask of the desired cavity microstructure is deposited onto a polystyrene substrate. The substrate is then exposed to a saturated toluene vapour which is absorbed by the polystyrene causing swelling of the surface except in the areas where the droplets are situated resulting in the formation of micro-cavities at the position of each droplet. The PZFlex finite element (FE) software, deployed on custom cloud computing architecture allowing for 1000's of simultaneous parallel simulations, has been employed to explore the potential for the use of the micropatterned substrates in the manufacture of capacitive ultrasonic transducers. This new cloud approach has facilitated reducing months of intensive FE modelling to a few days. The results have been employed to guide the optimisation of the patterning process in order to manufacture devices suited to air-coupled non-destructive testing. The paper describes the results of the finite element modelling, the optimisation of the manufacturing route and the characterisation of the resultant devices.","1051-0117","978-1-4673-5686-2978-1-4673-5684","10.1109/ULTSYM.2013.0285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6725263","capacitive transducer;modelling;finite element;air-coupled ultrasound","Cavity resonators;Substrates;Transducers;Surface treatment;Computational modeling;Aerosols;Ultrasonic transducers","capacitive sensors;cloud computing;finite element analysis;polymers;production engineering computing;swelling;ultrasonic transducers;vibrations","droplets;vapours;capacitive transducer manufacture;capacitive ultrasonic transducer;ultrasound transmission;ultrasound reception;electrical stimulus;mechanical stimulus;vibrational behavior;micropatterning;polymer substrate;cavity microstructure;polystyrene substrate;saturated toluene vapour;swelling;microcavities;PZFlex finite element software;FE software;custom cloud computing architecture;parallel simulation;micropatterned substrate;air-coupled nondestructive testing","","","10","","","","","","IEEE","IEEE Conferences"
"Comparison of hardware accelerators for some ALICE online computing applications","B. Changaival; D. Bouron; S. Chapeland; T. Achalakul","Computer Engineering, KMUTT, Bangkok, Thailand; Computer Engineering, EPITA, Paris, France; ALICE DAQ Team, CERN, Geneva, Switzerland; Computer Engineering, KMUTT, Bangkok, Thailand","2015 4th International Conference on Software Engineering and Computer Systems (ICSECS)","","2015","","","21","26","ALICE (A Large Ion Collider Experiment) is an experiment which studies about interacting matter and the quark-gluon plasma at the European Organization for Nuclear Research (CERN) Large Hadron Collider (LHC). Due to the detector upgrade in 2018, 1TB/s data are expected to flow from the detector and the computing system needs to process these data online. To speed up the online processing, different hardware accelerators must be tested thoroughly to see whether they are suitable for the tasks. In this paper, we propose a benchmark method for the ALICE O<sup>2</sup> project on various accelerators. There are three targeted computing platform for benchmark, namely, Graphics Processing Unit (GPU), Many-Integrated Core (MIC), and Accelerated Processing Unit (APU). For completeness, CUDA, OpenMP and OpenCL were used to implement the selected algorithms, which correspond to some of the real processing tasks to be implemented for ALICE. The initial results together with the discussion on algorithm optimizations and hardware limitations are also presented.","","978-1-4673-6722-6978-1-4673-6721","10.1109/ICSECS.2015.7333117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7333117","Accelerator;Accelerated Processing Unit;Circular Hough Transform;CUDA;Graphics Processing Unit;Many-Integrated Core;OpenMP;OpenCL","Graphics processing units;Microwave integrated circuits;Central Processing Unit;Clustering algorithms;Detectors;Benchmark testing;Acceleration","graphics processing units;high energy physics instrumentation computing;nuclear matter;quark-gluon plasma","hardware accelerators;ALICE online computing application;A Large Ion Collider Experiment;quark-gluon plasma;interacting matter;CERN;Large Hadron Collider;LHC;data online processing;ALICE O<sup>2</sup> project;graphics processing unit;GPU;many-integrated core;MIC;accelerated processing unit;APU;CUDA;OpenMP;OpenCL;algorithm optimization","","","18","","","","","","IEEE","IEEE Conferences"
"Towards creating a GPGPU-accelerated framework for pattern matching","T. Fekete; G. Mezei","Evosoft Hungary Ltd./Budapest, Hungary; Budapest University of Technology and Economics/Budapest, Hungary","2015 IEEE 13th International Symposium on Intelligent Systems and Informatics (SISY)","","2015","","","67","72","Model-driven engineering (MDE) is a popular software development methodology in the software industry. Finding a predefined pattern in a domain-specific model can be requested in MDE. This technique can help in optimizing or refactoring the models or to translate from one language to another one. The goal of the current researching is to create a framework for MDE which can find patterns defined by the users. Performance is a key issue. Using heterogeneous computation system (e.g.: CPU+GPU) is a promising way to increase the performance of the calculation. Therefore, we created a solution based on the OpenCL framework which is one of the most popular heterogeneous platforms. In this paper, the new pattern matching framework and the main steps of its creation are presented. The applied conception consists of two main steps. Firstly, a simpler case study is solved and experiences are collected from the occurring challenges. Secondly, the achieved solution was extended for general pattern matching. In both steps, the core algorithms are implemented according to the test-driven development methodology. To elaborate these steps, a new technique is provided which can be useful in creating any GPU-based model transformation and thus MDE approaches are improved in general.","1949-047X;1949-0488","978-1-4673-9388-1978-1-4673-9387","10.1109/SISY.2015.7325353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7325353","","Graphics processing units;Pattern matching;Kernel;Algorithm design and analysis;Performance evaluation;Hardware;Buffer overflows","graphics processing units;pattern matching;software engineering","GPGPU-accelerated framework;pattern matching;model-driven engineering;MDE;software development methodology;domain-specific model;heterogeneous computation system;OpenCL framework;test-driven development methodology;GPU-based model transformation","","","14","","","","","","IEEE","IEEE Conferences"
"Decoder-side Motion Estimation and Wiener filter for HEVC","Y. Chiu; L. Xu; W. Zhang; H. Jiang","Intel Corporation, 12200 Mission College Blvd, Santa Clara, CA 95054, USA; Raycom Park A, No. 2 KeXueYuan South Rd., Beijing 100190, China; Intel Corporation, 12200 Mission College Blvd, Santa Clara, CA 95054, USA; Raycom Park A, No. 2 KeXueYuan South Rd., Beijing 100190, China","2013 Visual Communications and Image Processing (VCIP)","","2013","","","1","6","This paper presents the coding techniques of Decoder-side Motion Vector Derivation (DMVD) and Adaptive Loop Filter (ALF) to apply on the state-of-the-art High Efficiency Video Coding (HEVC) standard. Both DMVD and ALF can provide the improvement of coding efficiency on HEVC test Model (HM) reference software, which are reported in multiple proposals during the development period of HEVC. In DMVD algorithm, Motion Vectors (MVs) are derived at video decoder side by utilizing the spatial and temporal correlation among decoded pictures without the signaling of motion vectors from video encoder side. The transmission of MVs from video encoder is skipped and thus better coding efficiency of inter prediction can be achieved. In ALF algorithm, a group of adaptive Wiener filters are applied to the samples of reconstructed pictures to improve the output video visual quality. The filter coefficients can be trained by video encoder and will be conveyed to video decoder signaled by the slice header to apply for the entire pixels in the slice. The utilization of DMVD and ALF can be controlled by the process of coding mode decision at video encoder side based on Rate-Distortion Optimization (RDO) metric. Experiments have demonstrated that the DMVD achieves 0.6%-1.1% BD rate reduction when compared to HM10.0 on the Random Access (RA) test condition. For ALF, average BD rate savings of 1.5%-2.6% are observed compared to HM10.0 under the HM common test condition. Experiment results also demonstrate that the coding gain from the integrated pipe of DMVD and ALF can be additive.","","978-1-4799-0290-3978-1-4799-0288","10.1109/VCIP.2013.6706446","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6706446","DMVD;motion estimation and motion compensation;ALF;in-loop filtering;HEVC;H.265","Encoding;Wiener filters;Adaptive filters;Decoding;Finite impulse response filters;Information filters","adaptive filters;decoding;motion estimation;optimisation;video coding;Wiener filters","random access;RDO metric;rate-distortion optimization;video visual quality;video encoder;temporal correlation;spatial correlation;video decoder;HEVC test Model;high efficiency video coding;ALF;adaptive loop filter;DMVD;decoder-side motion vector derivation;coding technique;adaptive Wiener filter;decoder-side motion estimation","","3","26","","","","","","IEEE","IEEE Conferences"
"An Efficient Minimum-Time Trajectory Generation Strategy for Two-Track Car Vehicles","A. Rucco; G. Notarstefano; J. Hauser","Department of Engineering, Università del Salento, Lecce, Italy; Department of Engineering, Università del Salento, Lecce, Italy; Department of Electrical, Computer, and Energy Engineering, University of Colorado, Boulder, CO, USA","IEEE Transactions on Control Systems Technology","","2015","23","4","1505","1519","In this paper, we propose a novel approach to compute minimum-time trajectories for a two-track car model, including tires and (quasi-static) longitudinal and lateral load transfer. Given the car model and a planar track, including lane boundaries, our goal is to find a trajectory of the car minimizing the traveling time subject to steering and tire limits. Moreover, we enforce normal force constraints to avoid wheel liftoff. Based on a projection operator nonlinear optimal control technique, we propose a minimum-time trajectory generation strategy to compute the fastest car trajectory. Numerical computations are presented on two testing scenarios, a 90° turn and a real testing track. The computations allow us to both demonstrate the efficiency and accuracy of the proposed approach and highlight important features of the minimum-time trajectories. Finally, we integrate our strategy into a commercial vehicle dynamics software, thus computing minimum-time trajectories for a complex multibody vehicle model. The matching between the predicted trajectory and the one of the commercial toolbox further highlights the effectiveness of the proposed methodology.","1063-6536;1558-0865;2374-0159","","10.1109/TCST.2014.2377777","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7047758","Minimum-time;nonlinear optimal control;race car;trajectory optimization;two-track;vehicle dynamics.;Minimum-time;nonlinear optimal control;race car;trajectory optimization;two-track;vehicle dynamics","Trajectory;Tires;Vehicles;Computational modeling;Force;Vehicle dynamics;Load modeling","automobiles;nonlinear control systems;optimal control;tyres;vehicle dynamics;wheels","minimum-time trajectory generation strategy;two-track car vehicles;two-track car model;tires;complex multibody vehicle model;vehicle dynamics software;numerical computations;projection operator nonlinear optimal control;wheel liftoff;normal force constraints;tire limit;steering limit;lane boundaries;planar track;lateral load transfer;longitudinal load transfer","","13","27","","","","","","IEEE","IEEE Journals & Magazines"
"FPGA-Based Test-Bench for Resonant Inverter Load Characterization","Ó. Jiménez; Ó. Lucía; L. A. Barragán; D. Navarro; J. I. Artigas; I. Urriza","Electronic Engineering and Communications, University of Zaragoza, Zaragoza, Spain; Department of Electronic Engineering and Communications, University of Zaragoza, Zaragoza, Spain; Electronic Engineering and Communications, University of Zaragoza, Zaragoza, Spain; Electronic Engineering and Communications, University of Zaragoza, Zaragoza, Spain; Electronic Engineering and Communications, University of Zaragoza, Zaragoza, Spain; Electronic Engineering and Communications, University of Zaragoza, Zaragoza, Spain","IEEE Transactions on Industrial Informatics","","2013","9","3","1645","1654","Resonant converters often require accurate load characterization in order to ensure appropriate and safe control. Besides, for systems with a highly variable load, as the induction heating systems, a real-time load estimation is mandatory. This paper presents the development of an FPGA-based test-bench aimed to extract the electrical equivalent of the induction heating loads. The proposed test-bench comprises a resonant power converter, sigma-delta ADCs, and an embedded system implemented in an FPGA. The characterization algorithm is based on the discrete-time Fourier series computed directly from the ΔΣ ADC bit-streams, and the FPGA implementation has been partitioned into hardware and software platforms to optimize the performance and resources utilization. Analytical and simulation results are verified through experimental measurements with the proposed test-bench. As a result, the proposed platform can be used as a load identification tool either for stand-alone or PC-hosted operation.","1551-3203;1941-0050","","10.1109/TII.2012.2226184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6338295","Embedded systems;FPGA;resonant power conversion;induction heating","Electromagnetic heating;Hardware;Embedded systems;Modulation;Field programmable gate arrays;Power harmonic filters","embedded systems;field programmable gate arrays;Fourier series;induction heating;resonant invertors;resonant power convertors;sigma-delta modulation","PC-hosted operation;load identification tool;resource utilization;ΔΣ ADC bit-stream;discrete-time Fourier series;embedded system;sigma-delta ADC;resonant power converter;induction heating load system;electrical equivalent extraction;real-time load estimation;resonant inverter load characterization;test-bench;FPGA","","30","47","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal reliability allocation with minimum cost for Web service composition","L. Changzhi; F. Xiaodong; T. Qiang; W. Wei; X. Yongymg","Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China; Faculty of Information Engineering and Automation, Kunming University of Science and Technology, Kunming 650500, China","2013 25th Chinese Control and Decision Conference (CCDC)","","2013","","","58","63","By allocating the reliability constraint of the Web service composition to each of component services in the design phase, a service composition with high reliability and low cost could be provided. For this purpose, we first analysis structure patterns of service composition and corresponding failure modes of these patterns are given. Then, a method is proposed to map these failure modes to fault tree. With fault tree analysis method, we obtain the failure function of the service composition. Based on the relationship between the failure rate and the cost of the component services, we design a nonlinear programming model to allocate reliability constraint to component services reasonably. The proposed optimization model can satisfy the reliability constraint of the service composition with minimum cost. We test the effectiveness, practicality and efficiency of the proposed method by extensive experiments.","1948-9439;1948-9447","978-1-4673-5534-6978-1-4673-5533-9978-1-4673-5532","10.1109/CCDC.2013.6560894","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6560894","Web service composition;reliability allocation;failure mode;FTA(fault tree analysis);nonlinear programming","Quality of service;Web services;Reliability engineering;Resource management;Fault trees;Programming","fault trees;nonlinear programming;software reliability;Web services","optimal reliability allocation;Web service composition;structure patterns;failure modes;fault tree analysis method;failure function;nonlinear programming model;optimization model;reliability constraint","","","11","","","","","","IEEE","IEEE Conferences"
"Ant Colony Algorithm Based on Dynamic Adaptive Pheromone Updating and Its Simulation","G. Liu; J. Xiong","NA; NA","2013 Sixth International Symposium on Computational Intelligence and Design","","2013","1","","220","223","In order to effectively overcome the defects of local and global pheromone updating for the basic Ant Colony Algorithm, this paper has proposed a new improved Ant Colony Algorithm based on the dynamic adaptive weight in the updating strategy. The proposed algorithm can update pheromone dynamically and adaptively according to the change of taboo lists and the quality of iteration-best solutions. By the experiments of several typical Traveling Salesman Problems (TSP), the proposed algorithm is clearly better than several other typically Ant Colony Algorithms in the convergence speed and the solution quality. The test results can reflect its effectiveness and feasibility.","","978-0-7695-5079","10.1109/ISCID.2013.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6804975","Ant Colony Algorithm;taboo list;weight;pheromone updating;TSP","Heuristic algorithms;Cities and towns;Educational institutions;Algorithm design and analysis;Polymers;Software algorithms;Convergence","ant colony optimisation;convergence","ant colony algorithm;dynamic adaptive pheromone updating;local pheromone updating;global pheromone updating;dynamic adaptive weight;iteration-best solutions;traveling salesman problems;TSP","","","11","","","","","","IEEE","IEEE Conferences"
"Characterizing Data Analytics Workloads on Intel Xeon Phi","B. Xie; X. Liu; J. Zhan; Z. Jia; Y. Zhu; L. Wang; L. Zhang","NA; NA; NA; NA; NA; NA; NA","2015 IEEE International Symposium on Workload Characterization","","2015","","","114","115","With the growing computation demands of data analytics, heterogeneous architectures become popular for their support of high parallelism. Intel Xeon Phi, a many-core coprocessor originally designed for high performance computing applications, is promising for data analytics workloads. However, to the best of knowledge, there is no prior work systematically characterizing the performance of data analytics workloads on Xeon Phi. It is difficult to design a benchmark suite to represent the behavior of data analytics workloads on Xeon Phi. The main challenge resides in fully exploiting Xeon Phi's features, such as long SIMD instruction, simultaneous multithreading, and complex memory hierarchy. To address this issue, we develop Big Data Bench-Phi, which consists of seven representative data analytics workloads. All of these benchmarks are optimized for Xeon Phi and able to characterize Xeon Phi's support for data analytics workloads. Compared with a 24-core Xeon E5-2620 machine, Big Data Bench-Phi achieves reasonable speedups for most of its benchmarks, ranging from 1.5 to 23.4X. Our experiments show that workloads working on high-dimensional matrices can significantly benefit from instruction- and thread-level parallelism on Xeon Phi.","","978-1-5090-0088-3978-1-5090-0087","10.1109/IISWC.2015.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314155","Xeon Phi;Data Analytics;Characterization","Data analysis;Benchmark testing;Computer architecture;Google;Principal component analysis;Scalability;Instruction sets","Big Data;data analysis;matrix algebra;multiprocessing programs;multi-threading;parallel processing;software architecture","data analytics workloads;Intel Xeon Phi;heterogeneous architectures;high parallelism;many-core coprocessor;high performance computing;SIMD instruction;multithreading;complex memory hierarchy;24-core Xeon E5-2620 machine;Big Data Bench-Phi;high-dimensional matrices","","1","8","","","","","","IEEE","IEEE Conferences"
"ScaAnalyzer: a tool to identify memory scalability bottlenecks in parallel programs","X. Liu; B. Wu","NA; NA","SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis","","2015","","","1","12","It is difficult to scale parallel programs in a system that employs a large number of cores. To identify scalability bottlenecks, existing tools principally pinpoint poor thread synchronization strategies or unnecessary data communication. Memory subsystem is one of the key contributors to poor parallel scaling in multicore machines. State-of-the-art tools, however, either lack sophisticated capabilities or are completely ignorant in pinpointing scalability bottlenecks arising from the memory subsystem. To address this issue, we develop a tool - ScaAnalyzer - to pinpoint scaling losses due to poor memory access behaviors of parallel programs. ScaAnalyzer collects, attributes, and analyzes memory-related metrics during program execution while incurring very low overhead. ScaAnalyzer provides high-level, detailed guidance to programmers for scalability optimization. We demonstrate the utility of ScaAnalyzer with case studies of three parallel programs. For each benchmark, ScaAnalyzer identifies scalability bottlenecks caused by poor memory access behaviors and provides optimization guidance that yields significant improvement in scalability.","2167-4337","978-1-4503-3723-6978-1-5090-0273","10.1145/2807591.2807648","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7832821","","Scalability;Instruction sets;Optimization;Benchmark testing;Hardware;Monitoring;Bandwidth","data communication;file organisation;parallel programming;software tools;synchronisation","ScaAnalyzer tool;memory scalability bottleneck identification;parallel programs;thread synchronization;data communication;memory subsystem;parallel scaling;multicore machines;memory access behaviors","","8","44","","","","","","IEEE","IEEE Conferences"
"Using best practices to improve portfolio management PPM accelerate: A benchmarking study","M. M. Menke","Value Creation Associates, USA","2013 Proceedings of PICMET '13: Technology Management in the IT-Driven Services (PICMET)","","2013","","","352","374","Portfolio management has a long history and good track record for improving strategy execution, optimizing business value and balancing risk &amp; return. Methods for strategic alignment, opportunity evaluation and portfolio optimization are well established. There is also much portfolio management software available. But even though methods and tools abound, many organizations are still not very effective with portfolio management. Maximum effectiveness also requires a well-designed process supported by good management practices. Much of this boils down to good behavior.This presentation discusses how to identify best practices, diagnosis the strengths and weaknesses of actual portfolio management organizations, and then develop practical recommendations for improving the process. It is illustrated by the results of an international project portfolio management (PPM) benchmarking study involving about fifty organizations from a wide range of R&amp;D-intensive and capital-intensive industries. Participants include Bayer, Boeing, Cisco, Dow, ExxonMobil, Genentech, J&amp;J, Lockheed-Martin, P&amp;G, Pfizer, Philips, Takeda and Unilever. The presentation covers: . Identifying PPM best practices . Assessing their importance and usage . Determining best-in-class performance . Using best practices to diagnose an organization's PPM . Developing recommendations for improving PPM . Improvements made by benchmarking participants This study offers a powerful yet inexpensive way to diagnose portfolio management organizations and develop recommendations for improvement.","2159-5100","978-1-890843-27","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6641602","","Portfolios;Best practices;Benchmark testing;Organizations;Technology management;Industries","benchmark testing;investment;organisational aspects;research and development;risk analysis;strategic planning","portfolio management practices;PPM strategic alignment;opportunity evaluation;portfolio optimization;international project portfolio management;benchmarking;R and D-intensive industries;capital-intensive industries;Bayer;Boeing;Cisco;Dow;ExxonMobil;Genentech;J&J;Lockheed-Martin;P&G;Pfizer;Philips;Takeda;Unilever;PPM best practices;portfolio management organizations;risks","","","10","","","","","","IEEE","IEEE Conferences"
"Memory-constrained static rate-optimal scheduling of synchronous dataflow graphs via retiming","X. Zhu; M. Geilen; T. Basten; S. Stuijk","State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, China, Beijing 100190; Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, the Netherlands; Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, the Netherlands; Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, the Netherlands","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","6","Synchronous dataflow graphs (SDFGs) are widely used to model digital signal processing (DSP) and streaming media applications. In this paper, we use retiming to optimize SDFGs to achieve a high throughput with low storage requirement. Using a memory constraint as an additional enabling condition, we define a memory constrained self-timed execution of an SDFG. Exploring the state-space generated by the execution, we can check whether a retiming exists that leads to a rate-optimal schedule under the memory constraint. Combining this with a binary search strategy, we present a heuristic method to find a proper retiming and a static scheduling which schedules the retimed SDFG with optimal rate (i.e., maximal throughput) and with as little storage space as possible. Our experiments are carried out on hundreds of synthetic SDFGs and several models of real applications. Differential synthetic graph results and real application results show that, in 79% of the tested models, our method leads to a retimed SDFG whose rate-optimal schedule requires less storage space than the proven minimal storage requirement of the original graph, and in 20% of the cases, the returned storage requirements equal the minimal ones. The average improvement is about 7.3%. The results also show that our method is computationally efficient.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.338","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800539","","Schedules;Memory management;Computational modeling;Vectors;Digital signal processing;Delays;Throughput","data flow graphs;directed graphs;heuristic programming;scheduling;search problems","memory-constrained static rate-optimal scheduling;synchronous dataflow graphs via retiming;SDFGs;digital signal processing;DSP;media streaming;memory constrained self-timed execution;binary search strategy;heuristic method;static scheduling;storage space;differential synthetic graph","","","17","","","","","","IEEE","IEEE Conferences"
"Efficient design and implementation of LTE UE link-layer protocol stack","M. Qian; Y. Zhou; W. Wei; Y. Huang; Y. Wang; J. Shi","Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, 100190; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, 100190; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, 100190; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, 100190; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, 100190; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China, 100190","2013 IEEE Wireless Communications and Networking Conference (WCNC)","","2013","","","895","900","3GPP Long Term Evolution (LTE) and its enhancement has become the main candidate of 4G standards. Due to the requirements on high data rate and low latency for broadband wireless communication systems, LTE air-interface protocol stack must be designed and implemented with high data processing efficiency. Following a general description of the LTE user equipment (TIE), this paper analyzes the main challenges in the link-layer protocol stack design and implementation, including the data processing efficiency, synchronization, flexibility and portability. A reference design of LTE TIE link-layer protocol stack software is then presented with several design methodologies. A memory access optimization and light-weighted thread model are proposed to improve the data processing efficiency, while a hardware triggered synchronization mechanism is exploited to maintain synchronous between the link-layer protocol stack and the physical layer. Finally, the platform flexibility and portability is achieved by employing interface abstraction and adapters. Extensive system level testing shows that the proposed LTE TIE protocol stack software achieves high data rate and low data processing latency in a hardware resource restricted platform, which can be further extended to commercial LTE TIE products.","1525-3511;1525-3511;1558-2612","978-1-4673-5939-9978-1-4673-5938-2978-1-4673-5937","10.1109/WCNC.2013.6554682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6554682","","Protocols;IP networks;Message systems;Data processing;Downlink;Uplink;Physical layer","3G mobile communication;4G mobile communication;data communication;Long Term Evolution;optimisation;protocols;radio equipment;synchronisation;telecommunication standards","LTE UE link layer protocol stack;3GPP;long term evolution;4G standard;broadband wireless communication system;air interface protocol stack;data processing efficiency;user equipment;memory access optimization;light weighted thread model;hardware triggered synchronization mechanism;physical layer;flexibility;portability;interface abstraction;extensive system level testing","","5","13","","","","","","IEEE","IEEE Conferences"
"Performance evaluation of Operating Systems in the private cloud with XenServer hypervisor using SIGAR Framework","P. V. V. Reddy; L. Rajamani","Osmania University, Hyderabad, India; Osmania University, Hyderabad, India","2014 9th International Conference on Computer Science & Education","","2014","","","183","188","Virtualization Technology plays prominent role in the success of cloud computing because the delivery of services is simplified by providing a platform for optimizing complex IT resources in a scalable manner, which makes cloud computing more cost effective. Hypervisors give near native performance by negating virtualization overhead. XenServer is one of the hypervisors which give near native performance using para virtualization technique. It is encouraging to study how different Operating Systems (OSes) perform in a virtualized environment in the Private Cloud with XenServer hypervisor. Guest OSes do come in Paravirtualized and Hardware Virtualized flavours and in free and commercial versions. It is innovative idea to compare them from performance perspective to give an idea about their performance variations in different tests and also to analyse the results with respect to Para versus Hardware virtualization techniques. This paper conducts different performance tests with low, medium, high workloads and results are gathered using SIGAR API (System Information Gatherer and Reporter) and explains the behaviour of each operating system and gives the reasoning of results with the help of three different virtualization techniques namely Full Virtualization, Paravirtualization and Hardware Assisted Virtualization. In the experiment, CloudStack (open source cloud computing software) is used to create a private cloud (proprietary computing architecture deployed behind a firewall with full control over infrastructure), in which management server is installed on Ubuntu 12.04 - 64 bit operating system. XenServer hypervisor is installed on bare metal as a host in the cluster. The guest operating systems are installed on XenServer hypervisor and their respective performances have been evaluated in detail by using SIGAR Framework.","","978-1-4799-2951-1978-1-4799-2949","10.1109/ICCSE.2014.6926451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6926451","Virtualization;Hypervisor;PrivateCloud;CloudStack;SIGAR","Computers;Throughput;Computer architecture","application program interfaces;cloud computing;data privacy;operating systems (computers);public domain software;virtualisation","performance evaluation;private cloud;XenServer hypervisor;SIGAR API framework;virtualization technology;cloud computing;IT resource optimization;information technology;para virtualization technique;virtualized environment;hardware virtualization;system information gatherer and reporter;application program interface;full virtualization;CloudStack;open source cloud computing software;proprietary computing architecture;Ubuntu operating system","","5","20","","","","","","IEEE","IEEE Conferences"
"Parametric Study of a Divertor Cooling System for a Liquid-Metal Plasma-Facing Component","A. Khodak; M. A. Jaworski","Princeton Plasma Physics Laboratory, Princeton, NJ, USA; Princeton Plasma Physics Laboratory, Princeton, NJ, USA","IEEE Transactions on Plasma Science","","2014","42","8","2161","2165","Novel divertor cooling system concept is currently under development at Princeton Plasma Physics Laboratory. This concept utilizes supercritical carbon dioxide as a coolant for the liquid lithium filled porous divertor front plate. Coolant is flowing in closed loop in the T-tube-type channel. Application of CO<sub>2</sub> eliminates safety concerns associated with water cooling of liquid lithium systems, and promises higher overall efficiency compared with systems using He as a coolant. Numerical analysis of divertor system initial configuration was performed using ANSYS software. Initially conjugated heat transfer problem was solved involving computational fluid dynamics (CFD) simulation of the coolant flow, and heat transfer in the coolant and solid regions of the cooling system. Redlich-Kwong real gas model was used for equation of state of supercritical CO<sub>2</sub> together with temperature- and pressure-dependent transport properties. Porous region filled with liquid lithium was modeled as a solid body with liquid lithium properties. Evaporation of liquid lithium from the front face was included via special temperature-dependent boundary condition. Results of CFD and heat transfer analysis were used as external conditions for structural analysis of the system components. Simulations were performed within ANSYS Workbench framework using ANSYS CFX for conjugated heat transfer and CFD analysis, and ANSYS Mechanical for structural analysis. Initial results were obtained using simplified 2-D model of the cooling system. The 2-D model allowed direct comparison with previous cooling concepts, which use He as a coolant. Optimization of the channel geometry in 2-D allowed increase in efficiency of the cooling system by reducing the total pressure drop in the coolant flow. Optimized geometrical parameters were used to create a 3-D model of the cooling system which eventually can be implemented and tested experimentally. The 3-D numerical simulation will be used to validate design variants of the divertor cooling system.","0093-3813;1939-9375","","10.1109/TPS.2014.2330292","U.S. Department of Energy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6847163","Computational fluid dynamics (CFD);cooling system;divertor;lithium;numerical simulations.;Computational fluid dynamics (CFD);cooling system;divertor;lithium;numerical simulations","Coolants;Heating;Lithium;Plasma temperature;Liquids;Numerical models","carbon compounds;computational fluid dynamics;coolants;cooling;fusion reactor divertors;plasma simulation;plasma-wall interactions","parametric study;divertor cooling system;liquid-metal plasma-facing component;Princeton Plasma Physics Laboratory;supercritical carbon dioxide;liquid lithium filled porous divertor front plate;closed loop;T-tube-type channel;numerical analysis;divertor system initial configuration;ANSYS software;heat transfer problem;computational fluid dynamics;CFD simulation;coolant flow;Redlich-Kwong real gas model;equation of state;temperature-dependent transport properties;pressure-dependent transport properties;porous region;special temperature-dependent boundary condition;heat transfer analysis;structural analysis;ANSYS CFX;conjugated heat transfer;helium coolant;channel geometry optimization;total pressure drop;optimized geometrical parameters;3D numerical simulation;CO<sub>2</sub>","","","14","","","","","","IEEE","IEEE Journals & Magazines"
"An open source domain decomposition solver for time-harmonic electromagnetic wave problems","C. Geuzaine; B. Thierry; N. Marsic; D. Colignon; A. Vion; S. Tournier; Y. Boubendir; M. E. Bouajaji; X. Antoine","Dept. of Electrical Engineering and Computer Science, Université de Liège, Institut Montefiore, B-4000 Liège, Belgium; Dept. of Electrical Engineering and Computer Science, Université de Liège, Institut Montefiore, B-4000 Liège, Belgium; Dept. of Electrical Engineering and Computer Science, Université de Liège, Institut Montefiore, B-4000 Liège, Belgium; Dept. of Electrical Engineering and Computer Science, Université de Liège, Institut Montefiore, B-4000 Liège, Belgium; Dept. of Electrical Engineering and Computer Science, Université de Liège, Institut Montefiore, B-4000 Liège, Belgium; Dept. of Electrical Engineering and Computer Science, Université de Liège, Institut Montefiore, B-4000 Liège, Belgium; Department of Mathematical Sciences and Center for Applied Mathematics and Statistics, NJIT, Univ. Heights. 323 Dr. M. L. King Jr. Blvd, Newark, NJ 07102, USA; Institut Elie Cartan de Lorraine (IECL), Université de Lorraine, CNRS, INRIA Alice Team, F-54506 Vandoeuvre-lès-Nancy Cedex, France; Institut Elie Cartan de Lorraine (IECL), Université de Lorraine, CNRS, INRIA Alice Team, F-54506 Vandoeuvre-lès-Nancy Cedex, France","2014 IEEE Conference on Antenna Measurements & Applications (CAMA)","","2014","","","1","4","We present a flexible finite element solver for testing optimized Schwarz domain decomposition techniques for the time-harmonic Maxwell equations. After a review of non-overlapping Schwarz domain decomposition methods and associated transmission conditions, we discuss the implementation, based on the open source software GetDP and Gmsh. The solver, along with ready-to-use examples, is available online for further testing.","","978-1-4799-3678","10.1109/CAMA.2014.7003342","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7003342","","Maxwell equations;Boundary conditions;Convergence;Finite element analysis;Electromagnetic scattering;Jacobian matrices","computational electromagnetics;electromagnetic wave propagation;finite element analysis;Maxwell equations;public domain software","open source domain decomposition solver;time-harmonic electromagnetic wave problems;flexible finite element solver;optimized Schwarz domain decomposition techniques;time-harmonic Maxwell equations;open source software GetDP;Gmsh","","3","30","","","","","","IEEE","IEEE Conferences"
"An Experiment Platform for Common Mountain Orchard Transport","X. Bao; S. Li; L. Meng; M. Zhong; Y. Zhang","NA; NA; NA; NA; NA","2015 International Conference on Computer Science and Mechanical Automation (CSMA)","","2015","","","267","271","In order to provide orchard transport with an objective assessment of running performance, and provide reasonable improvement of relevant technical parameters with basic data, a common mountain orchard transport test system on experiment platform was constructed based on PLC. Torque and rotational speed of the driving shaft, guide wheel speed, wear and tear of the driving shaft wheels groove, vehicle vibration acceleration and other varieties of physical quantity can be measured on the system. The system hardware include load adjustable orchard transport vehicle, slope adjustable track, some test element such as eddy current sensor, data logger etc. The system software obtain test data through muti-channel data logger added in the PC, and most function such as data collection, data processing, final graphic display and data storage will be realized on it. It was revealed by running experiment that the test system can be used for multiple parameters of overall performance of the rail transport and offer test conditions or theoretical basis to optimization of transport running performance.","","978-1-4673-9166-5978-1-4673-9165","10.1109/CSMA.2015.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371664","orchard transport;experiment platform;wear;torque","Wheels;Wires;Torque;Vehicles;Real-time systems;Shafts;Current measurement","agricultural machinery;data loggers;drives;eddy currents;sensors;shafts;vibrations;wear;wheels","mountain orchard transport;running performance;orchard transport test system;PLC;shaft;guide wheel speed;wear;tear;driving shaft wheels groove;vehicle vibration acceleration;system hardware;system software;mutichannel data logger;data processing;graphic display;data collection;data storage;rail transport;transport running performance","","","12","","","","","","IEEE","IEEE Conferences"
"Cooling of high power active phased array antenna using axially grooved heat pipe for a space application","M. Parlak; R. J. McGlen","Aselsan Inc., REHIS-Engineering Division, 06172, Ankara-Turkey; Thermacore Europe Ltd., 12 Wansbeck Business Park, Ashington, NE63 8QW, Northumberland, England","2015 7th International Conference on Recent Advances in Space Technologies (RAST)","","2015","","","743","748","An engineering model of a phased array antenna, cooled using axially grooved heat pipes (AGHP) is presented. Solid state power amplifier's (SSPA) located inside the satellite are wave transmitted to the reflector through waveguides. This integration technique is preferred to minimise electrical losses. However, usually when designing waveguides, minimising antenna losses takes precedence, which leads to an un-optimised design for thermal performance. Because of this situation, a thermal solution has become a challenging problem on this project. With a total heat load of 578 W, it is essential that the waste heat is transferred to the radiator of the satellite which is designed to operate in GEO orbit. This work presents a thermal solution being developed for a satellite phased array antenna that incorporates 4 AGHP's to carry the heat from the SSPA to the radiator. Thermal design work for the antenna and heat pipes completed for the specific boundary conditions for the application, using Icepack® computational fluid dynamics (CFD) thermal software is presented. It is crucial in this pre-development phase to identify problems which can arise from electronic performance loss or thermal issues in advance, both by theoretical design and ground testing.","","978-1-4799-7697-3978-1-4673-7760","10.1109/RAST.2015.7208439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208439","Electronic Cooling;Axially Grooved Heat Pipe;GEO Satellite;Phased Array Antenna","Space heating;Heat transfer;Arrays;Space vehicles;Computational fluid dynamics","active antenna arrays;antenna phased arrays;computational fluid dynamics;cooling;heat pipes;losses;power amplifiers;reflector antennas;satellite antennas;waste heat;waveguides","high power active phased array antenna cooling;axially grooved heat pipe;space application;AGHP;solid state power amplifier;SSPA;waveguides;reflector;electrical losses minimization;antenna losses minimization;thermal performance;un-optimised design;GEO orbit;waste heat;satellite radiator;satellite phased array antenna;thermal design;specific boundary conditions;Icepack computational fluid dynamics thermal software;CFD thermal software;electronic performance loss;ground testing;power 578 W","","2","7","","","","","","IEEE","IEEE Conferences"
"Fast CU partition decision using machine learning for screen content compression","F. Duanmu; Z. Ma; Y. Wang","New York University, Brooklyn, NY 11201, USA; Nanjing University, 22 Hankou Road, Nanjing 210093, P.R. China; New York University, Brooklyn, NY 11201, USA","2015 IEEE International Conference on Image Processing (ICIP)","","2015","","","4972","4976","Screen Content Coding (SCC) extension is currently being developed by Joint Collaborative Team on Video Coding (JCT-VC), as the final extension for the latest High-Efficiency Video Coding (HEVC) standard. It employs some new coding tools and algorithms (including palette coding mode, intra block copy mode, adaptive color transform, adaptive motion compensation precision, etc.), and outperforms HEVC by over 40% bitrate reduction on typical screen contents. However, enormous computational complexity is introduced on encoder primarily due to heavy optimization processing, especially rate distortion optimization (RDO) for Coding Unit (CU) partition decision and mode selection. This paper proposes a novel machine learning based approach for fast CU partition decision using features that describe CU statistics and sub-CU homogeneity. The proposed scheme is implemented as a ""preprocessing"" module on top of the Screen Content Coding reference software (SCM-3.0). Compared with SCM-3.0, experimental results show that our scheme can achieve 36.8% complexity reduction on average with only 3.0% BD-rate increase over 11 JCT-VC testing sequences when encoded using ""All Intra"" (AI) configuration.","","978-1-4799-8339-1978-1-4799-8338","10.1109/ICIP.2015.7351753","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351753","Screen Content Coding;HEVC;Machine Learning;Neural Network;Partition Decision","Image color analysis;Histograms;Training;Channel coding;Color;Neural networks","computational complexity;data compression;learning (artificial intelligence);optimisation;rate distortion theory;statistics;video coding","subCU homogeneity;preprocessing module;screen content coding reference software;SCM-3.0;all intra configuration;CU statistics;coding unit partition decision;RDO;rate distortion optimization;computational complexity;HEVC standard;high-efficiency video coding standard;JCT-VC;Joint Collaborative Team on Video Coding;SCC extension;screen content coding;screen content compression;machine learning;CU partition decision","","11","12","","","","","","IEEE","IEEE Conferences"
"An HEVC-Compliant Perceptual Video Coding Scheme Based on JND Models for Variable Block-Sized Transform Kernels","J. Kim; S. Bae; M. Kim","Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, Korea","IEEE Transactions on Circuits and Systems for Video Technology","","2015","25","11","1786","1800","In this paper, a High Efficiency Video Coding (HEVC)-compliant perceptual video coding (PVC) scheme is introduced based on just-noticeable difference (JND) models in both transform and pixel domains. We adopt an existing pixel-domain JND model for the transform skip mode of HEVC and propose a transform-domain JND model for the transform nonskip modes of HEVC. The proposed transform-domain JND model is designed by considering the spatial JND characteristics such as contrast sensitivity, luminance adaptation, and contrast masking effects as well as by considering the summation effects of variable block-sized transforms in HEVC. A temporal JND model is additionally incorporated into the proposed transform-domain JND model to further reduce perceptual redundancy. To incorporate the transform- and pixel-domain JND models into the encoding process in an HEVC-compliant manner, the transform coefficients and residues are suppressed in harmonization with the transform/quantization process and the quantization-only process of HEVC, respectively. To make the JND-based suppression effective, a distortion compensation factor is also proposed to reflect the perceptual distortion in the rate-distortion optimization-based encoding process. Based on subjective quality assessments of the encoded bit streams of test sequences, the proposed HEVC-compliant PVC scheme yields remarkable bitrate reductions of a maximum 49.10% and an average 16.10% with negligible subjective quality loss, compared with an HEVC reference software HEVC test model (HM 11.0). In addition, the proposed HEVC-compliant PVC scheme increases the encoding complexity of HM 11.0 only by an average of 11.25%.","1051-8215;1558-2205","","10.1109/TCSVT.2015.2389491","IT Research and Development Program through the Ministry of Science, ICT and Future Planning/Institute for Information and Communications Technology Promotion; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005446","High Efficiency Video Coding;Just Noticeable Difference;Perceptual Video Coding;High Efficiency Video Coding (HEVC);just-noticeable difference (JND);perceptual video coding (PVC)","Transforms;Encoding;Quantization (signal);Video coding;Image edge detection;Decoding;Materials","encoding;optimisation;quantisation (signal);video coding;visual perception","HEVC-compliant perceptual video coding scheme;variable block-sized transform kernels;high efficiency video coding;just-noticeable difference models;pixel-domain JND model;transform skip mode;transform-domain JND model;transform nonskip modes;contrast sensitivity;luminance adaptation;and contrast masking effects;summation effects;temporal JND model;perceptual redundancy;transform/quantization process;quantization-only process;distortion compensation factor;perceptual distortion;rate-distortion optimization-based encoding process;encoded bit streams;test sequences;bitrate reductions","","23","38","","","","","","IEEE","IEEE Journals & Magazines"
"Robust Real-Time Load Profile Encoding and Classification Framework for Efficient Power Systems Operation","E. D. Varga; S. F. Beretka; C. Noce; G. Sapienza","Faculty of Technical Sciences, University of Novi Sad, Novi Sad, Serbia; Schneider Electric DMS NS, Novi Sad, Serbia; Enel, Italy; Enel, Italy","IEEE Transactions on Power Systems","","2015","30","4","1897","1904","Neatly represented and properly classified load profiles are fundamental to many control optimization techniques of modern power systems, especially in a distribution area. This paper presents a novel load profile management software framework for boosting the efficiency of power systems operation. The proposed framework encodes and classifies load profiles in real-time. Imperfections as well as time-shifts in the input (measured power consumption levels) are tolerated by the suggested system, thus always providing accurate, fast and reliable output. The framework's fully component based structure allows easy customizations of the encoding as well as the classification engines. The default encoding engine is based on an artificial neural network, a variant known as a deep learning auto-encoder comprised from stacked sparse auto-encoders. The default classifier engine is based on an implementation of a locality sensitive hashing algorithm. The developed methodology was tested on the real case of a set of anonymous customers supplied by a power distribution company. The paper also contains an elaboration about the experiences gained during the design, implementation and testing phase of this system as well as a detailed engineering use case of the framework's applicability.","0885-8950;1558-0679","","10.1109/TPWRS.2014.2354552","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6898891","Classification algorithms;load modeling;multidimensional systems;multi-layer neural network;multilevel systems;real-time systems;unsupervised learning","Encoding;Real-time systems;Biological neural networks;Training;Feature extraction;Vectors;Engines","cryptography;encoding;file organisation;learning (artificial intelligence);neural nets;optimisation;power distribution reliability;power engineering computing;power system management","robust real-time load profile encoding;power system operation;control optimization technique;load profile management software framework;classification engine framework;artificial neural network;deep learning autoencoder;stacked sparse autoencoder;locality sensitive hashing algorithm;power distribution company;power consumption","","13","41","","","","","","IEEE","IEEE Journals & Magazines"
"Optimal cost analysis of wind-solar hybrid system powered AC and DC irrigation pumps using HOMER","B. F. Ronad; S. H. Jangamshetti","Department of Electronics and Electrical, Jain University, Bangalore, India; Department of Electrical and Electronics, Basaveshwar Engineering College (A), Bagalkot, India","2015 International Conference on Renewable Energy Research and Applications (ICRERA)","","2015","","","1038","1042","This paper presents the optimal cost analysis of wind-solar hybrid systems connected to DC and AC irrigation pumps. HOMER software is used for optimizing the systems components based on load profiles and wind-solar energy potentials at the Bagalkot, India. AC load profile is considered constant for seven hours/day and DC load profile is obtained by conducting performance tests on 1 HP DC pump installed in Energy Park, Basaveshwar Engineering College (Autonomous), Bagalkot, India. Developed HOMER model include SPV panels and two DC wind turbine generators for generation. Optimal configurations of wind-solar capacities are listed by considering Total Net Present Cost as objective function. Cost and electricity generation are compared and analyzed. It is observed that, annual electricity generated by both systems is nearly equal. However, cost comparison of AC and DC loads indicate that, for renewable energy powered small scale irrigation systems, AC pumps leads to uneconomic results and excess generation. On other hand, DC pumps prove to be economic and reliable.","","978-1-4799-9982-8978-1-4799-9981","10.1109/ICRERA.2015.7418568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418568","HOMER;Total Net Present Cost (TNPC);AC and DC Irrigation pump;Solar Photovoltaic;Wind Turbine Generators","Irrigation;Wind speed;Wind turbines;Load modeling;Hybrid power systems;Optimization;Batteries","hybrid power systems;irrigation;power generation economics;pumps;solar power stations;wind power plants;wind turbines","wind-solar hybrid system;AC irrigation pumps;DC irrigation pumps;HOMER software;load profiles;Bagalkot;India;Energy Park;Basaveshwar Engineering College;SPV panels;DC wind turbine generators;total net present cost;electricity generation","","1","8","","","","","","IEEE","IEEE Conferences"
"Performance evaluation of different SRAM topologies using 180, 90 and 45 nm technology","S. Shaik; P. Jonnala","School of Electronics, Andhra Pradesh, India Vignan University; School of Electronics, Andhra Pradesh, India Vignan University","2013 International Conference on Renewable Energy and Sustainable Energy (ICRESE)","","2013","","","15","20","The design of various standard SRAM topologies with different technologies has been designed and tested for delay and power dissipation with respect to the different supply voltages. For this consideration, different topologies viz. 6T, 7T, 8T, 9T and 10T SRAM cells have taken. And these cells are designed using generic process development kit (gpdk) 45, 90 and 180 nm technologies. And all these are tested in cadence tool. The detailed analysis about these cells functionality and their characteristic behavior with the applied parameter of supply voltage is presented. The results of the delay, power dissipation with respect to the Vdd are plotted using MATLAB software. Also their layouts were designed and tabulated their areas.","","978-1-4799-2075","10.1109/ICRESE.2013.6927819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927819","low power;optimization;SRAM design;subthreshold SRAM","Transistors;SRAM cells;Layout;Power dissipation;Delays;Topology","performance evaluation;SRAM chips;transistors","SRAM topologies;performance evaluation;SRAM cells;generic process development kit;cadence tool;supply voltage parameter;power dissipation;MATLAB software","","","7","","","","","","IEEE","IEEE Conferences"
"Quality of Service (QoS)-Guaranteed Network Resource Allocation via Software Defined Networking (SDN)","A. V. Akella; K. Xiong","NA; NA","2014 IEEE 12th International Conference on Dependable, Autonomic and Secure Computing","","2014","","","7","13","Quality of Service (QoS) -- based bandwidth allocation plays a key role in real-time computing systems and applications such as voice IP, teleconferencing, and gaming. Likewise, customer services often need to be distinguished according to their service priorities and requirements. In this paper, we consider bandwidth allocation in the networks of a cloud carrier in which cloud users' requests are processed and transferred by a cloud provider subject to QoS requirements. We present a QoS-guaranteed approach for bandwidth allocation that satisfies QoS requirements for all priority cloud users by using Open vSwitch, based on software defined networking (SDN). We implement and test the proposed approach on the Global Environment for Networking Innovations (GENI). Experimental results show the effectiveness of the proposed approach.","","978-1-4799-5079-9978-1-4799-5078","10.1109/DASC.2014.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945296","","Quality of service;Bandwidth;Routing;Switches;Multimedia communication;Cloud computing;Streaming media","bandwidth allocation;cloud computing;computer network performance evaluation;quality of service;real-time systems","quality of service guaranteed network resource allocation;software defined networking;SDN;quality of service based bandwidth allocation;real-time computing systems;voice IP;teleconferencing;gaming;customer services;cloud user requests;cloud carrier networks;QoS-guaranteed approach;QoS requirements;Open vSwitch;global environment for networking innovations;GENI","","25","21","","","","","","IEEE","IEEE Conferences"
"Shielding heterogeneous MPSoCs from untrustworthy 3PIPs through security-driven task scheduling","C. Liu; J. Rajendran; C. Yang; R. Karri","Department of Electrical and Computer Engineering, University of Delaware, 140 Evans Hall, Newark, 19716, USA; Department of Electrical and Computer Engineering, Polytech Institute of New York University, Brooklyn, 11201, USA; Department of Electrical and Computer Engineering, University of Delaware, 140 Evans Hall, Newark, 19716, USA; Department of Electrical and Computer Engineering, Polytech Institute of New York University, Brooklyn, 11201, USA","2013 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFTS)","","2013","","","101","106","Outsourcing of the various aspects of IC design and fabrication flow strongly questions the classic assumption that “hardware is trustworthy”. Multiprocessor System-on-Chip (MPSoC) platforms face some of the most demanding security concerns, as they process, store, and communicate sensitive information using third-party intellectual property (3PIP) cores that may be untrustworthy. The complexity of an MPSoC makes it expensive and time consuming to fully analyze and test it during the design stage. Consequently, the trustworthiness of the 3PIP components cannot be ensured. To protect MPSoCs against malicious modifications, we propose to incorporate trojan toleration into MPSoC platforms by revising the task scheduling step of the MPSoC design process. We impose a set of security-driven diversity constraints into the scheduling process, enabling the system to detect the presence of malicious modifications or to mute their effects during application execution. Furthermore, we pose the security-constrained MPSoC task scheduling as a multi-dimensional optimization problem, and propose a set of heuristics to ensure that the introduced security constraints can be fulfilled with minimum performance and hardware overhead.","1550-5774;2377-7966","978-1-4799-1585-9978-1-4799-1583","10.1109/DFT.2013.6653590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6653590","Security;Hardware Trojan;Heterogeneous MPSoCs;Task Scheduling;Multi-dimension Optimization","Fault tolerance;Fault tolerant systems;Complexity theory;Trojan horses;Size measurement;Discrete Fourier transforms","integrated circuit design;invasive software;multiprocessing systems;processor scheduling;system-on-chip","shielding heterogeneous MPSoC;untrustworthy 3PIP;security-driven task scheduling;outsourcing;IC design;IC fabrication flow;multiprocessor system-on-chip platforms;MPSoC platforms;third-party intellectual property core;3PIP cores;design stage;3PIP components;malicious modifications;trojan toleration;MPSoC design process;security-driven diversity constraints;scheduling process;security-constrained MPSoC task scheduling;multidimensional optimization problem;security constraints","","8","31","","","","","","IEEE","IEEE Conferences"
"A tightly-coupled hardware controller to improve scalability and programmability of shared-memory heterogeneous clusters","P. Burgio; R. Danilo; A. Marongiu; P. Coussy; L. Benini","DEI - Università degli Studi di Bologna - Italy; LabSTICC - Université de Bretagne-Sud, Lorient - France; DEI - Università degli Studi di Bologna - Italy; LabSTICC - Université de Bretagne-Sud, Lorient - France; DEI - Università degli Studi di Bologna - Italy","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","4","Modern designs for embedded many-core systems increasingly include application-specific units to accelerate key computational kernels with orders-of-magnitude higher execution speed and energy efficiency compared to software counterparts. A promising architectural template is based on heterogeneous clusters, where simple RISC cores and specialized HW units (HWPU) communicate in a tightly-coupled manner via L1 shared memory. Efficiently integrating processors and a high number of HW Processing Units (HWPUs) in such an system poses two main challenges, namely, architectural scalability and programmability. In this paper we describe an optimized Data Pump (DP) which connects several accelerators to a restricted set of communication ports, and acts as a virtualization layer for programming, exposing FIFO queues to offload “HW tasks” to them through a set of lightweight APIs. In this work, we aim at optimizing both these mechanisms, for respectively reducing modules area and making programming sequence easier and lighter.","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800239","","Ports (Computers);Programming;Optimization;System-on-chip;Hardware;Computer architecture;Integrated circuit interconnections","application program interfaces;embedded systems;pattern clustering;shared memory systems","tightly-coupled hardware controller;scalability improvement;programmability improvement;shared-memory heterogeneous clusters;embedded many-core systems;application-specific units;key computational kernels;orders-of-magnitude;execution speed;energy efficiency;architectural template;RISC cores;specialized HW units;HW processing units;HWPU;data pump;DP;virtualization layer;FIFO queues;lightweight API","","","18","","","","","","IEEE","IEEE Conferences"
"Using Network Knowledge to Improve Workload Performance in Virtualized Data Centers","D. Erickson; B. Heller; N. McKeown; M. Rosenblum","NA; NA; NA; NA","2014 IEEE International Conference on Cloud Engineering","","2014","","","185","194","The scale and expense of modern data centers motivates running them as efficiently as possible. This paper explores how virtualized data center performance can be improved when network traffic and topology data informs VM placement. Our practical heuristics, tested on network-heavy, scale-out workloads in an 80 server cluster, improve overall performance by up to 70% compared to random placement in a multi-tenant configuration.","","978-1-4799-3766","10.1109/IC2E.2014.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903473","virtualization;data center;migration;network;software defined networking;sdn;optimization;performance;beacon;virtue;algorithms;openflow","Optimization;Knowledge engineering;Switches;Network topology;Bandwidth;Servers;Heuristic algorithms","computer centres;computer network performance evaluation;network servers;telecommunication network topology;telecommunication traffic;virtual machines;virtualisation","network knowledge;workload performance improvement;virtualized data center performance;network traffic;network topology data;VM placement;network-heavy scale-out workloads;server cluster;virtual machines","","4","46","","","","","","IEEE","IEEE Conferences"
"A theoretical e-government portals' benchmarking framework","A. Fath-Allah; L. Cheikhi; R. E. Al-Qutaish; A. Idri","Dept. of Computer Science, ENSIAS - University Mohammed V, Rabat, Morocco; Dept. of Computer Science, ENSIAS - University Mohammed V, Rabat, Morocco; Dept. of Software Engineering & IT, École de Technologie Supérieure University of Québec, Montréal, Canada; Dept. of Computer Science, ENSIAS, University Mohammed V, Rabat, Morocco","2015 10th International Conference on Intelligent Systems: Theories and Applications (SITA)","","2015","","","1","6","E-government benchmarking is the process of ranking e-government according to some agreed best practices. It can be used not only to benchmark but also to assess achievements and identify missing best practices for stakeholders. The purpose of this paper is to propose guidelines to build a new benchmarking framework for e-government portals. This framework is based on measurement of best practices using a best practice model. For this purpose, we have first identified and presented five examples of the benchmarking frameworks available in the literature. Based on the conducted comparison, the findings show that although the benchmarking frameworks are serving their intended purposes, they still suffer from some limitations. The paper also explains how the new framework overcomes these limitations.","","978-1-5090-0220-7978-1-5090-0219","10.1109/SITA.2015.7358379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7358379","e-government;portal;best practice;benchmarking framework;benchmark","Portals;Benchmark testing;Best practices;Electronic government;Europe","government data processing;portals","e-government portal benchmarking framework;e-government ranking process;best practice model","","","35","","","","","","IEEE","IEEE Conferences"
"Improving polyhedral code generation for high-level synthesis","W. Zuo; P. Li; D. Chen; L. Pouchet; Shunan Zhong; J. Cong","Beijing Institute of Technology, China; Peking University, China; University of Illinois at Urbana-Champaign, USA; University of California, Los Angeles, USA; Beijing Institute of Technology, China; University of California, Los Angeles, USA","2013 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)","","2013","","","1","10","High-level synthesis (HLS) tools are now capable of generating high-quality RTL codes for a number of programs. Nevertheless, for best performance aggressive program transformations are still required to exploit data reuse and enable communication/computation overlap. The polyhedral compilation framework has shown great promise in this area with the development of HLS-specific polyhedral transformation techniques and tools. However, all these techniques rely on polyhedral code generation to translate a schedule for the program's operations into an actual C code that is input to the HLS tool. In this work we study the changes to the state-of-the-art polyhedral code generator CLooG which are required to tailor it for HLS purposes. In particular, we develop various techniques to significantly improve resource utilization on the FPGA. We also develop a complete technique geared towards effective code generation of rectangularly tiled code, leading to further improvements in resource utilization. We demonstrate our techniques on a collection of affine benchmarks, reducing by 2x on average (up to 10x) the area used after high-level synthesis.","","978-1-4799-1417","10.1109/CODES-ISSS.2013.6659002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6659002","Polyhedral Compilation;High-Level Synthesis;Loop tiling","Field programmable gate arrays;Optimization;Tiles;Benchmark testing;Table lookup;Digital signal processing;Measurement","field programmable gate arrays;high level synthesis;program compilers;resource allocation","polyhedral code generation;high-level synthesis;HLS tools;high-quality RTL code generation;program transformations;data reuse;communication-computation overlap;polyhedral compilation framework;CLooG polyhedral code generator;resource utilization;FPGA;field programmable gate array;rectangularly tiled code","","15","28","","","","","","IEEE","IEEE Conferences"
"Dynamical Re-striping Data on Storage Servers in Parallel File Systems","J. Liao; X. Liu; Y. Chen","NA; NA; NA","2013 IEEE 37th Annual Computer Software and Applications Conference","","2013","","","65","73","This paper proposes a mechanism for tracing I/O operations and their corresponding physical access on the storage servers in a parallel file system, then it guides data re-striping on the storage servers for handling complex physical access cases. To put this framework to work, we first figured out the way to map logical I/O operations on the client side to physical I/O access on the storage server side. We then developed a toolkit to generate I/O access patterns and their corresponding disk access. Finally, the information about both logical I/O access patterns and physical I/O access patterns can benefit to dynamical data re-striping and data pre-fetching on the storage servers for improving I/O performance. Experimental results show that the proposed data re-striping method by analyzing I/O access patterns and their associated disk access patterns can boost I/O data throughput significantly for applications with complicated access patterns. Especially, it work fairy well for the applications that deal with multi-dimension data sets across the fields of science, technology, education and business.","0730-3157","978-0-7695-4986","10.1109/COMPSAC.2013.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6649800","","Servers;Optimization;Benchmark testing;Educational institutions;Databases;Computers;Layout","input-output programs;parallel processing;storage management","multidimension data sets;input-output performance;physical input-output access patterns;logical input-output access patterns;disk access;input-output access patterns;logical input-output operations;input-ouput operations tracing;parallel file systems;storage servers;dynamical re-striping data","","1","27","","","","","","IEEE","IEEE Conferences"
"Constraint-driven frequency scaling in a Coarse Grain Reconfigurable Array","W. Hussain; H. Hoffmann; T. Ahonen; J. Nurmi","Department of Electronics and Communications Engineering, Tampere University of Technology, FI-33101, Finland; Department of Computer Science, University of Chicago, IL-60637, USA; Department of Electronics and Communications Engineering, Tampere University of Technology, FI-33101, Finland; Department of Electronics and Communications Engineering, Tampere University of Technology, FI-33101, Finland","2014 International Symposium on System-on-Chip (SoC)","","2014","","","1","6","This paper introduces a self-optimizing processor/coprocessor model supported by a feedback control system to achieve power efficiency. The software on the processor receives high-level performance constraints (i.e., real-time limits) as goal from the user and in return controls the clock speed of the coprocessor and memories, ensuring the performance constraints are met while minimizing power dissipation. The system is prototyped on a Stratix-V Field Programmable Gate Array device. The self-optimization feature requires less than 0.5% of the overall logic resources and provides a 33% reduction in average dynamic power dissipation when the control system activates for a proof-of-concept test case derived from Fast Fourier Transform processing at the IEEE-802.11n demodulator.","","978-1-4799-6890","10.1109/ISSOC.2014.6972451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6972451","","Avatars;Clocks;Phase locked loops;Reduced instruction set computing;Power dissipation;Feedback control","circuit optimisation;field programmable gate arrays;logic design","constraint driven frequency scaling;coarse grain reconfigurable array;self-optimizing processor model;self-optimizing coprocessor model;feedback control system;power efficiency;clock speed control;power dissipation;Stratix-V field programmable gate array;fast Fourier transform processing;IEEE-802.11n demodulator","","2","15","","","","","","IEEE","IEEE Conferences"
"Quantifying security risk by measuring network risk conditions","C. Suh-Lee; J. Jo","Department of Computer Science, University of Nevada, Las Vegas, USA; Department of Computer Science, University of Nevada, Las Vegas, USA","2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS)","","2015","","","9","14","Software vulnerabilities are the weaknesses in the software that inadvertently allow dangerous operations. If the vulnerability is in a network service, it poses serious security threats because a cyber-attacker can exploit it to gain unauthorized access to the system. Hence, rapid discovery and remediation of network vulnerabilities is critical issues in network security. In today's dynamic IT environment, it is common practice that an organization prioritizes the mitigation of discovered vulnerabilities according to their risk levels. Currently available technologies, however, associate each vulnerability to the static risk level which does not take the unique characteristics of the target network into account. This often leads to inaccurate risk prioritization and less-than-optimal resource allocation. In this research, we introduce a novel way of quantifying the risk of network vulnerability by augmenting the static risk level with conditions specific to the target network. The method calculates the risk value of each vulnerability by measuring the proximity to the untrusted network and risk of the neighboring hosts. The resulting risk value, RCR is a composite index of the individual risk, network location and neighborhood risk conditions. Thus, it can be effectively used for prioritization, comparison and trending. We tested the methodology through the network intrusion simulation. The results shows average 88.9% the correlation between RCR and number of successful attacks on each vulnerability.","","978-1-4799-8679","10.1109/ICIS.2015.7166562","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166562","risk management;network security;vulnerability management;useable security;quantitative risk analysis","Security;Internet;Servers;Workstations;Organizations;Standards organizations;Reliability","computer network security;resource allocation;risk management","security risk quantification;network risk condition measurement;software vulnerability;network service;security threats;cyber-attacker;network vulnerability;network security;dynamic IT environment;risk prioritization;less-than-optimal resource allocation;RCR;network location;network intrusion simulation","","5","22","","","","","","IEEE","IEEE Conferences"
"Design and Test of a Thermal Measurement System Prototype for SPIDER Experiment","M. Dalla Palma; N. Pomaro; C. Taliercio; R. Pasqualotto","Consorzio RFX - Associazione Euratom-ENEA Sulla Fusione, Padova, Italy; Consorzio RFX - Associazione Euratom-ENEA Sulla Fusione, Padova, Italy; Consorzio RFX - Associazione Euratom-ENEA Sulla Fusione, Padova, Italy; Consorzio RFX - Associazione Euratom-ENEA Sulla Fusione, Padova, Italy","IEEE Transactions on Plasma Science","","2014","42","7","1971","1976","SPIDER, the full-size prototype of the ITER heating neutral beams (HNBs) radio frequency (RF) ion source, has been designed and is under construction at Consorzio RFX in Padua, Italy. Several thermocouples will be installed in both SPIDER and HNB source to characterize and monitor the possibly severe thermal load on mechanical components. The presence of about 1 MW RF power ionizing the gas and frequent electrical breakdowns on the 100 kV beam accelerator represent a very harsh environment for thermal measurements, both in terms of possible sensors damage and of disturbances induced on the signals. A complete measurement chain prototype, composed of two sensors, conditioning electronics and data acquisition system, was realized to test and validate technologies and design solutions. The prototype was installed and tested inside the BATMAN RF ion source at IPP-Garching, where measurement conditions are very similar to those expected in SPIDER, and in particular high RF power and high voltage breakdowns are present. A careful installation was carried out, with extensive optimization of grounding and shielding configuration, to achieve sufficient ElectroMagnetic Interference immunity. Analog and digital filtering were also implemented on signals to eliminate the residual noise. Results produced during a dedicated experimental campaign on BATMAN have been very successful: 0.5 °C measurement global accuracy can be obtained during RF plasma pulses and no damage occurred on sensors or electronic equipments. The paper describes the prototype design detailing the technical solutions implemented, in particular regarding sensors design and installation, signal conditioning, and noise reduction. Hardware and software characteristics of the data acquisition system are presented and discussed. Experimental results obtained are also described and their significance for future work is analyzed.","0093-3813;1939-9375","","10.1109/TPS.2014.2328236","F4E; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844165","EMI immunity;grounding;noise reduction;radio frequency (RF) ion source;RF shielding;thermocouples.;EMI immunity;grounding;noise reduction;radio frequency (RF) ion source;RF shielding;thermocouples","Probes;Sensors;Plasma measurements;Radio frequency;Assembly;Noise;Temperature measurement","data acquisition;fusion reactor instrumentation;high-frequency discharges;ion sources;plasma beam injection heating;plasma sources;plasma toroidal confinement;thermocouples;thermometers;Tokamak devices","thermal measurement system;SPIDER experiment;ITER heating neutral beams;radiofrequency ion source;Consorzio RFX;thermocouples;HNB source;thermal load;mechanical components;RF power;electrical breakdowns;beam accelerator;conditioning electronics;data acquisition system;BATMAN RF ion source;high voltage breakdowns;grounding configuration;shielding configuration;electromagnetic interference immunity;analog filtering;digital filtering;residual noise;RF plasma pulses;electronic equipments;sensor design;sensor installation;signal conditioning;noise reduction;hardware characteristics;software characteristics;voltage 100 kV","","2","12","","","","","","IEEE","IEEE Journals & Magazines"
"Automotive Ethernet: In-vehicle networking and smart mobility","P. Hank; S. Müller; O. Vermesan; J. Van Den Keybus","NXP Semiconductors, Hamburg, Germany; NXP Semiconductors, Hamburg, Germany; SINTEF, Oslo, Norway; Triphase NV, Leuven, BELGIUM","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","1735","1739","This paper discusses novel communication network topologies and components and describes an evolutionary path of bringing Ethernet into automotive applications with focus on electric mobility. For next generation in-vehicle networking, the automotive industry identified Ethernet as a promising candidate besides CAN and FlexRay. Ethernet is an IEEE standard and is broadly used in consumer and industry domains. It will bring a number of changes for the design and management of in-vehicle networks and provides significant re-use of components, software, and tools. Ethernet is intended to connect inside the vehicle high-speed communication requiring sub-systems like Advanced Driver Assistant Systems (ADAS), navigation and positioning, multimedia, and connectivity systems. For hybrid (HEVs) or electric vehicles (EVs), Ethernet will be a powerful part of the communication architecture layer that enables the link between the vehicle electronics and the Internet where the vehicle is a part of a typical Internet of Things (IoT) application. Using Ethernet for vehicle connectivity will effectively manage the huge amount of data to be transferred between the outside world and the vehicle through vehicle-to-x (V2V and V2I or V2I+I) communication systems and cloud-based services for advanced energy management solutions. Ethernet is an enabling technology for introducing advanced features into the automotive domain and needs further optimizations in terms of scalability, cost, power, and electrical robustness in order to be adopted and widely used by the industry.","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.349","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513795","Ethernet;automotive;electric vehicle;smart grid;EV communication architecture;domain based commuication;in-vehicle networking;vehicle network topology","Vehicles;Automotive engineering;Computer architecture;Standards;Bandwidth;Protocols;Internet","","","","8","7","","","","","","IEEE","IEEE Conferences"
"Translating of MATLAB/SIMULINLK model to synchronous dataflow graph for parallelism analysis and programming embedded multicore systems","K. Guesmi; S. Hasnaoui","SYSCOM Research laboratory University of Tunis Al-Manar, National School of Engineering of Tunis - ENIT, Tunis, Tunisia; SYSCOM Research laboratory University of Tunis Al-Manar, National School of Engineering of Tunis - ENIT, Tunis, Tunisia","2014 9th International Design and Test Symposium (IDT)","","2014","","","156","161","Software implementation of compute-intensive applications in digital signal processing requires large computing power and has real-time performance requirements. Employing multicore architecture is usually the only means for solving the grand challenge of computational problems. Developing multicore-based systems requires a high degree of concurrency for optimizing performances of systems. For this purpose, this paper addresses the redesigning of MATLAB\Simulink models for efficient concurrent implementation using multiple processors. Our approach consists of translating a Simiüink model into discrete synchronous dataflow graph in order to treat them as concurrent system by exploiting task-level parallelism without alter the input-output behavior of the system.","2162-0601;2162-061X","978-1-4799-8200","10.1109/IDT.2014.7038605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7038605","multicore embedded systems;Dataflow graph;MATLAB/Simulink","Mathematical model;Computational modeling;Delays;MATLAB;Analytical models;Program processors","data flow graphs;embedded systems;multiprocessing systems;parallel processing","MATLAB model;SIMULINLK model;synchronous dataflow graph;parallelism analysis;system programming;embedded multicore system;task-level parallelism","","2","11","","","","","","IEEE","IEEE Conferences"
"Introduction to MEMS; their applications as sensors for chemical &amp; bio sensing","N. S. Kale","Nanosniff Technologies Pvt Ltd, M-01 SINE CSRE Bldg, IIT Bombay, Powai, Mumbai - 400076","2015 19th International Symposium on VLSI Design and Test","","2015","","","1","2","The microfabrication technology has had a chequered history of over 50 years in the field of microelectronics. Aggressive miniaturization of microelectronic devices has resulted in faster logic circuits; &amp; and it has also reduced their power requirements. MOSFET device dimensions have already entered the sub-100 nanometer regime. The same successful principles of microfabrication were applied to make miniaturized 3-dimensional mechanical structures. This helped in the advent of micro electro-mechanical systems or MEMS. Initially, i.e. in early nineties, the MEMS field was dominated by mechanical applications. However, now MEMS refers to all miniaturized systems including silicon based mechanical drivers, chemical and biological sensors and actuators, and miniature devices made from plastics or ceramics. The half-day tutorial would begin with a synoptic overview of the area of Nanotechnology &amp; then shift towards MEMS. It will highlight some of the challenges and outline the scope of the tutorial. Next we discuss about why is it that they are becoming so popular; for eg the advantages they offer: orders of magnitude smaller size, better performance than other solutions, possibilities for batch fabrication, cost-effective integration with electronics, virtually zero dc power consumption, potentially large reduction in power consumption, etc. Next, we discuss some of the popular MEMS applications &amp; some of the emerging MEMS applications: The application domains cover micro sensors and actuators for physical quantities of which MEMS for automobile &amp; consumer electronics forms a large segment; and microfabricated systems for chemical assay (microTAS) and for biochemical and biomedical assay (bioMEMS and DNA chips). This tutorial would give an introduction to these exciting developments and the technology and design approaches for the realization of these integrated systems. We will also introduce the importance of material selection by understanding the impact of material properties at the micron scale. We will discuss polymeric materials such as SU-8 and also compare them with traditional materials such as Silicon. We will also discuss about the possibility of integrating MEMS with VLSI electronics. Unit processes for bulk and surface micromachining of silicon and integration of processes for fabricating silicon microsensors will be presented. Simulators provide an excellent way to design, optimize and understand micromechanical systems. Particularly so because such systems are not of isolated, stand-alone type; instead, they are based on the interplay of several domains. For example, in a microcantilever based biosensing system the different domains are: materials, mechanical, biological, electrical and chemical. Recently developed software packages such as Coventorware, Intellisuite etc. have the ability to simulate a system in different domains. We will discuss basic philosophy of using MEMS simulation tools for simple devices. While MEMS represents a diverse family of designs; devices with simple cantilever configurations &amp; microheater configurations are especially attractive as transducers for chemical and biological sensors. We will review several important aspects of these transducers - namely: (i) operation principles; (ii) fabrication; and (iii) applications. We will discuss about our work in the area of healthcare applications to make a MEMS based instrument that can help physicians in diagnosing a heart attack event. We will also present our work to develop a MEMS based explosives detector. Finally, we discuss about the instruments (Omnicant &amp; Sensimer), that we have developed to help researchers experiment with microcantilevers &amp; microheaters.","","978-1-4799-1743-3978-1-4799-1742","10.1109/ISVDAT.2015.7208158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7208158","","Micromechanical devices;Silicon;Sensors;Chemicals;Tutorials;Chemical sensors;Nanobioscience","biological techniques;chemical sensors;microsensors","MEMS;microelectromechanical systems;chemical sensors;biological sensors;microsensors;microelectronics","","1","","","","","","","IEEE","IEEE Conferences"
"A Parameter Estimation Method for Biological Systems modelled by ODE/DDE Models Using Spline Approximation and Differential Evolution Algorithm","C. Zhan; W. Situ; L. F. Yeung; P. W. Tsang; G. Yang","Department of Electronics Communication and Software Engineering, Nanfang College of Sun Yat-Sen University, China; Department of Electronic Engineering, City University of Hong Kong, Hong Kong, China; Department of Electronic Engineering, City University of Hong Kong, Hong Kong, China; Department of Electronic Engineering, City University of Hong Kong, Hong Kong, China; Department of Automation, Shanghai Jiao Tong University","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2014","11","6","1066","1076","The inverse problem of identifying unknown parameters of known structure dynamical biological systems, which are modelled by ordinary differential equations or delay differential equations, from experimental data is treated in this paper. A two stage approach is adopted: first, combine spline theory and Nonlinear Programming (NLP), the parameter estimation problem is formulated as an optimization problem with only algebraic constraints; then, a new differential evolution (DE) algorithm is proposed to find a feasible solution. The approach is designed to handle problem of realistic size with noisy observation data. Three cases are studied to evaluate the performance of the proposed algorithm: two are based on benchmark models with priori-determined structure and parameters; the other one is a particular biological system with unknown model structure. In the last case, only a set of observation data available and in this case a nominal model is adopted for the identification. All the test systems were successfully identified by using a reasonable amount of experimental data within an acceptable computation time. Experimental evaluation reveals that the proposed method is capable of fast estimation on the unknown parameters with good precision.","1545-5963;1557-9964;2374-0043","","10.1109/TCBB.2014.2322360","CityU; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6811223","Systems biology;parameter estimation;inverse problem;differential evolution (DE);spline;optimization","Biological system modeling;Mathematical model;Parameter estimation;Computational biology;Splines (mathematics);Biological systems;Data models","cellular biophysics;differential equations;evolutionary computation;inverse problems;nonlinear programming;parameter estimation;splines (mathematics)","parameter estimation method;ODE-DDE models;spline approximation;differential evolution algorithm;inverse problem;dynamical biological systems;ordinary differential equations;delay differential equations;nonlinear programming;NLP;optimization problem;algebraic constraint;benchmark models","Algorithms;Animals;Mammals;Models, Biological;Reproducibility of Results;Signal Transduction;Systems Biology;Yeasts","10","39","","","","","","IEEE","IEEE Journals & Magazines"
"Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling","X. Gu; A. Easwaran; K. Phan; I. Shin","NA; NA; NA; NA","2015 27th Euromicro Conference on Real-Time Systems","","2015","","","13","24","Mixed-criticality real-time scheduling has been developed to improve resource utilization while guaranteeing safe execution of critical applications. These studies use optimistic resource reservation for all the applications to improve utilization, but prioritize critical applications when the reservations become insufficient at runtime. Many of them however share an impractical assumption that all the critical applications will simultaneously demand additional resources. As a consequence, they under-utilize resources by penalizing all the low-criticality applications. In this paper we overcome this shortcoming using a novel mechanism that comprises a parameter to model the expected number of critical applications simultaneously demanding more resources, and an execution strategy based on the parameter to improve resource utilization. Since most mixed criticality systems in practice are component-based, we design our mechanism such that the component boundaries provide the isolation necessary to support the execution of low-criticality applications, and at the same time protect the critical ones. We also develop schedulability tests for the proposed mechanism under both a flat as well as a hierarchical scheduling framework. Finally, through simulations, we compare the performance of the proposed approach with existing studies in terms of schedulability and the capability to support low-criticality applications.","2377-5998;1068-3070","978-1-4673-7570","10.1109/ECRTS.2015.9","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176022","Mixed-Criticality;Scheduling;Resource","Real-time systems;Resource management;Runtime;Electronic mail;Processor scheduling;Computational modeling;Market research","object-oriented programming;real-time systems;resource allocation;safety-critical software;scheduling","resource efficient isolation mechanism;mixed-criticality real-time scheduling;resource utilization;optimistic resource reservation;component-based software","","13","21","","","","","","IEEE","IEEE Conferences"
"""CERE"": A CachE Recommendation Engine: Efficient Evolutionary Cache Hierarchy Design Space Exploration","G. Yessin; A. H. A. Badawy; V. Narayana; D. Mayhew; T. E. Ghazawi","NA; NA; NA; NA; NA","2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)","","2014","","","566","573","Design Space Exploration is a critical step in chip design. Unfortunately, it takes significant amounts of time and resources to explore a fraction of the design space. Herein, we present a heuristic, evolutionary approach (Genetic Algorithm) to exploration that significantly cuts down on the time and resources, obtaining a near optimal design. We demonstrate the real-world utility of our tool-chain ""CERE"" by rapidly and efficiently designing the cache hierarchy which maximizes the performance of a web-browser navigating to a set of famous websites running on a single ARM core. We rapidly traverse 134,136 possible configurations. At about two days per simulation on the Gem5 full system simulator, the entire space would have taken 268,272 CPU-Days or ≃734 CPU-Years (≃3.7 Years on a cluster of a hundred dual core machines) to brute force search. ""CERE"" provided results in ≃4.5 days and used ≃17.5 CPU-Days on our cluster. We ran the configurations that ""CERE"" chose through Gem5 to verify that ""CERE"" made the right choices and we were able to observe a 17.1% speedup going from the ""best"" hierarchy relative to the ""Worst"" hierarchy.","","978-1-4799-6123-8978-1-4799-6122","10.1109/HPCC.2014.97","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056800","Design Space Exploration;Machine Intelligence;Genetic Algorithms;Dinero;Cache Simulation;Gem5","Space exploration;Genetic algorithms;Sociology;Statistics;Computer architecture;Benchmark testing;Optimization","cache storage;genetic algorithms;logic design;microprocessor chips","CERE;cache recommendation engine;evolutionary cache hierarchy design space exploration;chip design;genetic algorithm;Web-browser;Web sites;ARM core;Gem5","","","47","","","","","","IEEE","IEEE Conferences"
"Framework for multipoint sensing simulation for energy efficient HVAC operation in buildings","H. Zhang; A. Ukil","School of EEE, Nanyang Technological University, Singapore; School of EEE, Nanyang Technological University, Singapore","IECON 2015 - 41st Annual Conference of the IEEE Industrial Electronics Society","","2015","","","000398","000403","Building energy efficiency has increasing focus worldwide. The heating, ventilation, air-conditioning (HVAC) devices account for highest energy consumption in buildings in Singapore. Multipoint sensing information in the various scenario is critical to optimize the HVAC operation, and increase the energy efficiency of the air-conditioning. A software called FloVENT® is used for simulation of room models in different scenario. It calculates the temperature, airflow speed, pressure and other parameters in the mathematical way, providing the solutions for the models. The first experiment is done to find the effect of the multipoint sensing system due to the moving heat source. A testing heat source, represented by 11 visitors, is located in 5 different positions for the sensitivity test of the monitors. Another experiment simulates the dynamic situation of visitors coming in and leaving the room. The experiment results are analyzed for identifying the most sensitive sensor and the most optimal sensing point. The parameters of temperature change from the most optimal monitor can be used in the control system for improving the energy efficiency of the motors.","","978-1-4799-1762-4978-1-4799-1761","10.1109/IECON.2015.7392132","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7392132","Building automation;HVAC;energy efficiency;Flovent;sensor;smart building;user behavior;user occupancy","Temperature sensors;Monitoring;Temperature measurement;Heating;Testing;Buildings;Atmospheric modeling","building management systems;energy conservation;HVAC;power engineering computing","multipoint sensing simulation;energy efficient HVAC operation;building energy efficiency;HVAC devices;heating, ventilation, air-conditioning devices;Singapore;multipoint sensing information;FloVENT software","","5","13","","","","","","IEEE","IEEE Conferences"
"Simulation Approach to Calculate the Separation Error of Target and Background from Metal Pylon Deformation","D. An; W. Chen","NA; NA","2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC)","","2015","","","719","722","Metal pylon is an important equipment in the radar cross measurement. It can support the test target in the quiet zone to reduce the pylon-ground interaction. A method using translation target to separate measured signals of target and background has been developed. It demands that the variation of background signals is small enough during the measurement. As the pylon deformation caused by target load yields the variation of scattered signals from pylon, it should be restricted to reduce the variation of total background signals. However the weight of pylon also should be restricted for transportation convenience. So it demands a coordination optimization in the pylon design. A simulation model created in the mechanical and electromagnetic simulation software is raised to calculate the separation error caused by pylon deformation, and an example is given. It can provide the quantitative relationship between the separation error and various parameters of metal pylon, and the structure parameters of pylon can be unified to a single variable for simplifying design.","","978-1-4673-7723-2978-1-4673-7722","10.1109/IMCCC.2015.157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405936","background;metal pylon;radar cross section measurement;separation;simulation","Poles and towers;Scattering;Metals;Deformable models;Finite element analysis;Radar cross-sections;Data models","deformation;design engineering;finite element analysis;poles and towers;radar cross-sections;radar signal processing;source separation","background separation error;target separation error;metal pylon deformation;radar cross measurement;pylon-ground interaction reduction;target signal separation;background signal separation;background signal variation;scattered signal variation;transportation convenience;coordination optimization;pylon design;electromagnetic simulation software;mechanical simulation software","","","9","","","","","","IEEE","IEEE Conferences"
"Design and implementation of accurate reactive power compensator for renewable grid connected transmission system","A. S. Stalin; Shiva Balan M.; Revanth R.; Velbharathi A.; Durai Raj D.","Electrical and Electronics Engineering, Sri Manakula Vinayagar Engineering College, Pondicherry, India; Electrical and Electronics Engineering, Sri Manakula Vinayagar Engineering College, Pondicherry, India; Electrical and Electronics Engineering, Sri Manakula Vinayagar Engineering College, Pondicherry, India; Electrical and Electronics Engineering, Sri Manakula Vinayagar Engineering College, Pondicherry, India; Electrical and Electronics Engineering, Sri Manakula Vinayagar Engineering College, Pondicherry, India","2015 International Conference on Industrial Instrumentation and Control (ICIC)","","2015","","","1392","1397","Operational Performance in transmission, optimization of the hybrid, reliability of the system and efficiency of the transmission are key characteristics of the hybrid grid systems. In our paper, an integrated model of hybrid grid-connected Photo-Voltaic and Wind Turbine system is developed and an accurate reactive power compensator for the same is also developed. Management of Reactive Power is defined as the improvement of the power factor value so that we can cease the generation of reactive power. The concept of Reactive Power compensation is related to issues faced in power quality but in general, reactive power compensation can be done by considering in two parameter i.e. load compensation and voltage support. In load compensation, the aim is to increase the power factor of the system thus real power in the system can be increased. In Voltage support, the aim is to reduce the harmonic at the output due to fluctuating supply. Our proposed system comprises photovoltaic array, wind turbine, induction generator, controller, converters and capacitor bank. The reactive power compensator model is designed by using Ansys 14 software package and the same is implemented by using MATLAB software package. Perturb and Observe algorithm is used for maximum power point tracker (MPPT). Thus the behavior of our proposed model is tested under various operational parameters. Our proposed model and control technique implemented offers a smart tool for hybrid grid transmission system which good optimization in its performance.","","978-1-4799-7165-7978-1-4799-7164","10.1109/IIC.2015.7150966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150966","Photo-Voltaic System;Wind Turbine System;Maximum Power Point Tracking;MATLAB;AnsysHybrid Grid Transmission;Capacitor Bank;Unity Power factor correction","Reactive power;Capacitors;Software packages;Temperature;Wind turbines;Hybrid power systems;Mathematical model","asynchronous generators;hybrid power systems;maximum power point trackers;photovoltaic power systems;power supply quality;transmission networks;wind turbines","reactive power compensator;renewable grid connected transmission system;hybrid grid-connected photovoltaic-wind turbine system;reactive power management;power quality;photovoltaic array;induction generator;capacitor bank;Ansys 14 software package;MATLAB software package;perturb and observe algorithm;maximum power point tracker;MPPT;hybrid grid transmission system","","3","15","","","","","","IEEE","IEEE Conferences"
"An Automated Aero-Engine Thrust Detecting Method Based on Sound Recognition","T. Teng; Z. Zhihua","NA; NA","2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)","","2015","","","565","569","In order to confirm the aero-engine thrust before a carrier aircraft is launched, an automated aero-engine thrust detecting method based on sound recognition has been presented. Using the flight simulation software, it is possible to obtain an aero-engine's sound signal which can be divided into sections and put into the frequency domain by using FFT (Fast Fourier Transform). A 3-layer BP (Back Propagation) neural network is introduced to classify the spectrum of sound signal sections. In training the network, the L-BFGS algorithm is used to optimize the network parameters to make the accuracy of the network up to 99% in terms of test data. The result proves the algorithm to be effective. In practical application, the decision logic for 3 continuous sections is used to make the misjudgment rate for the launching thrust less than one millionth. The automated aero-engine thrust detecting method based on sound recognition can be adopted for a real-time detection of the aero-engine thrust, thereby replacing the current artificial confirmation methods of engine thrust to increase the efficiency and reliability of carrier aircraft launch.","","978-1-4673-7211-4978-1-4673-7212","10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7518292","automated aero-engine thrust detection;BP neural network;machine learning;decision logic","Neural networks;Aircraft;Engines;Training;Aircraft propulsion;Software;Aerospace simulation","acoustic signal detection;aerospace computing;aerospace engines;backpropagation;fast Fourier transforms;frequency-domain analysis;neural nets;signal classification","sound signal section spectrum classification;3-layer back propagation neural network;3-layer BP neural network;fast Fourier transform;FFT;frequency domain;flight simulation software;sound recognition;automated aero-engine thrust detecting method","","","14","","","","","","IEEE","IEEE Conferences"
"Optimal partition with block-level parallelization in C-to-RTL synthesis for streaming applications","S. Li; Y. Liu; X. S. Hu; Xinyu He; Yining Zhang; P. Zhang; H. Yang","Tsinghua National Laboratory for Information Science and Technology, Dept. of Electronic Engineering, Tsinghua University, Beijing, 100084, China; Tsinghua National Laboratory for Information Science and Technology, Dept. of Electronic Engineering, Tsinghua University, Beijing, 100084, China; Dept. of Computer Science and Engineering, University of Notre Dame, IN 46556, U.S.; Tsinghua National Laboratory for Information Science and Technology, Dept. of Electronic Engineering, Tsinghua University, Beijing, 100084, China; Tsinghua National Laboratory for Information Science and Technology, Dept. of Electronic Engineering, Tsinghua University, Beijing, 100084, China; Y Explorations Inc. San Jose, CA 95134, U.S.; Tsinghua National Laboratory for Information Science and Technology, Dept. of Electronic Engineering, Tsinghua University, Beijing, 100084, China","2013 18th Asia and South Pacific Design Automation Conference (ASP-DAC)","","2013","","","225","230","Developing FPGA solutions for streaming applications written in C (or its variants) can benefit greatly from automatic C-to-RTL (C2RTL) synthesis. Yet, the complexity and stringent throughput/cost constraints of such applications are rather challenging for existing C2RTL synthesis tools. This paper considers automatic partition and block-level parallelization to address these challenges. An MILP-based approach is introduced for finding an optimal partition of a given program into blocks while allowing block-level parallelization. In order to handle extremely large problem instances, a heuristic algorithm is also discussed. Experimental results based on seven well known multimedia applications demonstrate the effectiveness of both solutions.","2153-6961;2153-6961","978-1-4673-3030-5978-1-4673-3029-9978-1-4673-3028","10.1109/ASPDAC.2013.6509600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509600","","Throughput;Partitioning algorithms;Heuristic algorithms;Benchmark testing;Hardware;Silicon compounds;Hardware design languages","C language;circuit optimisation;field programmable gate arrays;hardware-software codesign;integer programming;linear programming;logic partitioning","optimal partition;block level parallelization;C-to-RTL synthesis;streaming application;FPGA solutions;automatic partition;MILP;mixed integer-linear programming","","3","27","","","","","","IEEE","IEEE Conferences"
"Using Grammatical Evolution Techniques to Model the Dynamic Power Consumption of Enterprise Servers","J. C. S. Hilburg; M. Zapater; J. L. R. Martín; J. M. Moya; J. L. Rodrigo","NA; NA; NA; NA; NA","2015 Ninth International Conference on Complex, Intelligent, and Software Intensive Systems","","2015","","","110","117","The increasing demand for computational resources has led to a significant growth of data center facilities. A major concern has appeared regarding energy efficiency and consumption in servers and data centers. The use of flexible and scalable server power models is a must in order to enable proactive energy optimization strategies. This paper proposes the use of Evolutionary Computation to obtain a model for server dynamic power consumption. To accomplish this, we collect a significant number of server performance counters for a wide range of sequential and parallel applications, and obtain a model via Genetic Programming techniques. Our methodology enables the unsupervised generation of models for arbitrary server architectures, in a way that is robust to the type of application being executed in the server. With our generated models, we are able to predict the overall server power consumption for arbitrary workloads, outperforming previous approaches in the state-of-the-art.","","978-1-4799-8870-9978-1-4799-8869","10.1109/CISIS.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7185174","Data Center;power modeling;Grammatical Evolution;application profiling","Servers;Radiation detectors;Power demand;Mathematical model;Hardware;Benchmark testing;Computational modeling","computer centres;energy conservation;genetic algorithms;network servers;power aware computing;power consumption","grammatical evolution technique;enterprise server dynamic power consumption model;computational resource;data center facilities;energy efficiency;energy consumption;scalable server power model;flexible server power model;proactive energy optimization strategy;evolutionary computation;sequential applications;parallel applications;genetic programming technique;arbitrary server architecture","","2","26","","","","","","IEEE","IEEE Conferences"
"Using STK Toolkit for Evaluating a GA Base Algorithm for Ground Station Scheduling","F. Xhafa; X. Herrero; A. Barolli; M. Takizawa","NA; NA; NA; NA","2013 Seventh International Conference on Complex, Intelligent, and Software Intensive Systems","","2013","","","265","273","The satellite scheduling and its version of ground station scheduling are increasingly attracting the attention of researchers from aerospace and optimization domain. While in the recent past satellite mission arise from large aero-spacial agencies, nowadays even smaller companies are interested in satellite missions for basic tasks such as telemetry, imaging, remote sensing, etc. The ground station scheduling problem consists in computing an optimal planning of communications between satellites or spacecraft (SC) and operations teams of Ground Station (GS). The problem is highly complex and multi-objective and in its general formulation has been shown NP-hard. Therefore, its resolution is tackled by heuristic and meta-heuristic methods. Although heuristic and meta-heuristic methods are well understood, their evaluation for specific problems, like ground station scheduling, remain a challenge. The design and development of benchmarks of instances is thus needful to evaluate such methods and also to provide the community with means to reproduce the experimental study for the same benchmark under the same or different parameter setting. In this paper, we present an XML-based benchmark of instances for the ground station scheduling generated with the STK simulation toolkit. Then we show the experimental evaluation of a Basic Genetic Algorithm using the benchmark.","","978-0-7695-4992-7978-0-7695-4992","10.1109/CISIS.2013.50","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6603899","Ground station scheduling;Satellite scheduling;Genetic Algorithms;Simulation;Benchmark;XML","Space vehicles;Satellites;Scheduling;Benchmark testing;Genetic algorithms;Telemetry;Planning","genetic algorithms;satellite communication;scheduling;telecommunication computing;XML","GA base algorithm;ground station scheduling;satellite scheduling;aerospace domain;optimization domain;aero-spacial agencies;satellite missions;optimal planning;spacecraft;operations teams;meta-heuristic methods;XML-based benchmark;STK simulation toolkit;basic genetic algorithm","","1","14","","","","","","IEEE","IEEE Conferences"
"Data layout transformation for structure vectorization on SIMD architectures","P. Li; Q. Zhang; R. Zhao; H. Yu","State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, China","2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","","2015","","","1","7","Structure references are commonly-used at the core of applications in a multitude of domains such as image processing, signal processing, especially the scientific and engineering applications. SIMD instruction sets, as SSE, AVX, AltiVec and 3DNow, provide a promising and widely available avenue for enhancing performance on modern processors. However existing memory accessing shackles limit the achieved performance for structure reference on modern SIMD architectures. In this paper, we propose a novel data layout transformation technology that addresses the accessing obstacles, along with a static analysis technique for detecting the legal loops in where this transformation is suitable. And this approach is implemented in the Optimizing Compiler Open64. The experimental results show that the proposed method can translate application with structure access into vectorizable codes, thereby advancing the execution efficiency adequately.","","978-1-4799-8676","10.1109/SNPD.2015.7176233","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176233","structure reference;data layout transformation;vectorization;SIMD","Decision support systems;Arrays;Layout;Switches;Indexes;Transforms;Benchmark testing","parallel processing;program compilers;program diagnostics","structure vectorization;SIMD architectures;data layout transformation technology;static analysis technique;optimizing compiler Open64","","","21","","","","","","IEEE","IEEE Conferences"
"Modeling cache coherence misses on multicores","X. Pan; B. Jonsson","Department of Information Technology, Uppsala University, Sweden; Department of Information Technology, Uppsala University, Sweden","2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)","","2014","","","96","105","While maintaining the coherency of private caches, invalidation-based cache coherence protocols introduce cache coherence misses. We address the problem of predicting the number of cache coherence misses in the private cache of a parallel application when running on a multicore system with an invalidation-based cache coherence protocol. We propose three new performance models (uniform, phased and symmetric) for estimating the number of coherence misses from information about inter-core data sharing patterns and the individual core's data reuse patterns. The inputs to the uniform and phased models are the write frequency and reuse distance distribution of shared data from different cores. This input can be obtained either from profiling the target application on a single core or by analyzing the data access pattern statically, and does not need a detailed simulation of the pattern of interleaving accesses to shared data. The output of the models is an estimated number of coherence misses of the target application. The output can be combined with the number of other kinds of misses to estimate the total number of misses in each core's private cache. This output can also be used to guide program optimization to improve cache performance. We evaluate our models with a set of benchmarks from the PARSEC benchmark suite on real hardware.","","978-1-4799-3606-9978-1-4799-3604","10.1109/ISPASS.2014.6844465","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844465","","Coherence;Instruction sets;Multicore processing;Predictive models;Data models;Benchmark testing;Protocols","cache storage;data handling;multiprocessing systems;protocols","private cache coherency;invalidation-based cache coherence protocols;cache coherence misses;multicore system;uniform performance models;phased performance models;symmetric performance models;intercore data sharing patterns;core data reuse patterns;data access pattern;program optimization;cache performance improvement;PARSEC benchmark suite","","","24","","","","","","IEEE","IEEE Conferences"
"Improved gravitational search algorithm based on free search differential evolution","Y. Liu; L. Ma","School of Management, University of Shanghai of Science and Technology, Shanghai 200093, China; Department of Foundation Courses Teaching, Yancheng Institute of Technology, Yancheng 224051, China; School of Management, University of Shanghai of Science and Technology, Shanghai 200093, China","Journal of Systems Engineering and Electronics","","2013","24","4","690","698","This paper presents an improved gravitational search algorithm (IGSA) as a hybridization of a relatively recent evolutionary algorithm called gravitational search algorithm (GSA), with the free search differential evolution (FSDE). This combination incorporates FSDE into the optimization process of GSA with an attempt to avoid the premature convergence in GSA. This strategy makes full use of the exploration ability of GSA and the exploitation ability of FSDE. IGSA is tested on a suite of benchmark functions. The experimental results demonstrate the good performance of IGSA.","1004-4132","","10.1109/JSEE.2013.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6587342","gravitational search algorithm (GSA);free search differential evolution (FSDE);global optimization","Optimization;Gravity;Benchmark testing;Convergence;Search problems;Systems engineering and theory;Algorithm design and analysis","","","","3","","","","","","","BIAI","BIAI Journals & Magazines"
"A feedback control design of buck converter: An artificial immune system based approach","T. K. Nizami; K. Sundareshwaran","Department of Electronics and Electrical Engineering, Indian Institute of Technology, Guwahati Assam, India; Department of Electrical and Electronics Engineering, National Institute of Technology Tiruchirappalli, Tamil Nadu, India","2015 39th National Systems Conference (NSC)","","2015","","","1","6","This article presents design of a Proportional-Integral-Derivative (PID) control for a buck converter by incorporating the principles derived from the processes of Artificial Immune System present in vertebrates in the control design algorithm. The buck converter represents a class of variable structure systems and its controller design by conventional means yields a near-satisfactory performance, however not the best transient and steady state dynamics at wider range of operating points. Therefore a feedback control design problem of buck converter is rearranged as an optimization goal and the concern parameters of the PID controller are found through an intelligent Artificial Immune System (AIS) based technique. Computations have been done by using Matlab software. The output voltage response of buck converter is tested for 1.) reference voltage change, 2.) load resistance change and 3.) input voltage change. To verify the findings obtained in simulations, a prototype of buck converter is build and controlled in the laboratory with the proposed methodology. The results found are then evaluated against the performance of conventional controller design and genetic algorithm (GA) based approach, followed by tabulation of performance measures, which clearly indicates that the AIS method of PID controller design provides better static and dynamic response in the output voltage besides achieving a faster convergence of parameters, thereby confirming the validity of new approach.","","978-1-4673-6829","10.1109/NATSYS.2015.7489101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489101","buck converter;genetic algorithm;optimization techniques;Artificial Immune System","Immune system;Voltage control;Optimization;Genetic algorithms;Sociology;Statistics;Algorithm design and analysis","artificial immune systems;control system synthesis;feedback;genetic algorithms;power convertors;three-term control;voltage control","feedback control design;buck converter;intelligent artificial immune system;proportional-integral-derivative control;PID control;vertebrates;variable structure system;optimization goal;output voltage response;reference voltage change;load resistance change;input voltage change;genetic algorithm;static response;dynamic response","","1","18","","","","","","IEEE","IEEE Conferences"
"Planning algorithm for optimal Combined Heat &amp; Power generation plant connection in urban distribution network (UDN)","S. Boljevic","Department of Electrical and Electronic Engineering, Cork Institute of Technology, Cork, Ireland","11th International Conference on the European Energy Market (EEM14)","","2014","","","1","7","Combined Heat &amp; Power (CHP) generation is the most efficient way of energy supply in urban area available today. It delivers significant benefits to its host facilities and urban distributed network (UDN) to which is connected. Economic viability of CHP generation for many sites requires integration with the UDN for backup and supplementary power needs and in some case the export of excess power to the UDN. CHP system integration into existing UDN entail installation costs. How these integration costs are distributed will have considerable impact on development and implementation of CHP generation in urban areas. The objective of this paper is to use analytical and statistical methods to develop an algorithm that provide means of determining the optimum capacity of a CHP generating plant that can be accommodated within the UDN, which correspond to Least Cost Technically Acceptable (LCTA) principle, and the UDN long term network planning policy. In order to determine optimal size of CHP generating plant that could be connected at any particular busbar on the UDN without causing a significant adverse impact on performance of the UDN, an algorithm is created that incorporate an analytical and multiple regression analysis model. It is tested using data obtained from ERAC power analysing software incorporating load flow, fault current level and power losses analysis. Additional data needed for effective algorithm creation was obtained via surveys of local UDN operators and planners. These analyses are performed on a 34 busbar network resembling part of the real UDN of Cork city for validation purposes and accuracy of the algorithm proposed.","2165-4093;2165-4077","978-1-4799-6095","10.1109/EEM.2014.6861261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6861261","CHP Plant;UDN;Cost;Connection;LCTA principle;Optimisation","Cogeneration;Economics;Electricity;Urban areas;Thermal energy;Reliability;Load flow","busbars;cogeneration;fault currents;load flow;power distribution planning;power generation economics;regression analysis","optimal combined heat & power generation plant connection;energy supply;urban distributed network;statistical method;analytical method;least cost technically acceptable principle;LCTA;UDN long term network planning policy;CHP generating plant;multiple regression analysis model;ERAC power analysing software;load flow;fault current level;power loss analysis;busbar network;Cork city","","1","22","","","","","","IEEE","IEEE Conferences"
"A study on the route selection problem for ship evacuation","Y. Qiao; D. Han; J. Shen; G. Wang","College of Shipbuilding Engineering, Harbin Engineering University, 150001, Heilongjiang, China; College of Shipbuilding Engineering, Harbin Engineering University, 150001, Heilongjiang, China; College of Mathematics, Harbin Engineering University, 150001, Heilongjiang, China; College of Shipbuilding Engineering, Harbin Engineering University, 150001, Heilongjiang, China","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","","2014","","","1958","1962","Evacuation in a large-scale passenger ship is a complex process. During the ship design stage, in order to ensure that evacuees can evacuate to safe areas efficiently, it's very important to select an optimum evacuation route and give evacuation training to each evacuee. For this purpose, we proposed a method to select an optimum evacuation route. From the perspective of simulation, each evacuee will select route considering the length of the passage, actual congestion and individual complex behavior attributes. Finally, we tested the feasibility of the proposed method in MATLAB software environment.","1062-922X","978-1-4799-3840","10.1109/SMC.2014.6974208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974208","human factors;evacuation;route optimization;simulation","Marine vehicles;Assembly;Educational institutions;Personnel;Psychology;Layout;Training","emergency management;ships;vehicle routing","optimum route selection problem;ship evacuation;large-scale passenger ship;evacuation training;complex behavior attributes;Matlab software environment","","","12","","","","","","IEEE","IEEE Conferences"
"A Practical Transfer Learning Algorithm for Face Verification","X. Cao; D. Wipf; F. Wen; G. Duan; J. Sun","NA; NA; NA; NA; NA","2013 IEEE International Conference on Computer Vision","","2013","","","3208","3215","Face verification involves determining whether a pair of facial images belongs to the same or different subjects. This problem can prove to be quite challenging in many important applications where labeled training data is scarce, e.g., family album photo organization software. Herein we propose a principled transfer learning approach for merging plentiful source-domain data with limited samples from some target domain of interest to create a classifier that ideally performs nearly as well as if rich target-domain data were present. Based upon a surprisingly simple generative Bayesian model, our approach combines a KL-divergence based regularizer/prior with a robust likelihood function leading to a scalable implementation via the EM algorithm. As justification for our design choices, we later use principles from convex analysis to recast our algorithm as an equivalent structured rank minimization problem leading to a number of interesting insights related to solution structure and feature-transform invariance. These insights help to both explain the effectiveness of our algorithm as well as elucidate a wide variety of related Bayesian approaches. Experimental testing with challenging datasets validate the utility of the proposed algorithm.","1550-5499;2380-7504","978-1-4799-2840","10.1109/ICCV.2013.398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6751510","transfer learning;face verification","Bayes methods;Face;Joints;Algorithm design and analysis;Computational modeling;Testing;Vectors","Bayes methods;convex programming;expectation-maximisation algorithm;face recognition;image classification;learning (artificial intelligence);minimisation","face verification;facial images;labeled training data;family album photo organization software;principled transfer learning approach;source-domain data;target-domain data;generative Bayesian model;KL-divergence-based regularizer-prior;robust likelihood function;EM algorithm;convex analysis;equivalent structured rank minimization problem;feature-transform invariance;Kullback-Leibler divergence-prior","","57","28","","","","","","IEEE","IEEE Conferences"
"Array Shadow State Compression for Precise Dynamic Race Detection (T)","J. R. Wilcox; P. Finch; C. Flanagan; S. N. Freund","NA; NA; NA; NA","2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)","","2015","","","155","165","Precise dynamic race detectors incur significant time and space overheads, particularly for array-intensive programs, due to the need to store and manipulate analysis (or shadow) state for every element of every array. This paper presents SlimState, a precise dynamic race detector that uses an adaptive, online algorithm to optimize array shadow state representations. SlimState is based on the insight that common array access patterns lead to analogous patterns in array shadow state, enabling optimized, space efficient representations of array shadow state with no loss in precision. We have implemented SlimState for Java. Experiments on a variety of benchmarks show that array shadow compression reduces the space and time overhead of race detection by 27% and 9%, respectively. It is particularly effective for array-intensive programs, reducing space and time overheads by 35% and 17%, respectively, on these programs.","","978-1-5090-0025-8978-1-5090-0024","10.1109/ASE.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372005","concurrency;data race detection;dynamic analysis","Arrays;Clocks;Instruction sets;Detectors;Synchronization;Heuristic algorithms;Java","Java;program testing;system monitoring","array shadow state compression;precise dynamic race detection;SLIMSTATE;adaptive online algorithm;array shadow state representations;array access patterns;analogous patterns;space efficient representations;Java;space overhead;time overhead;array-intensive programs","","5","46","","","","","","IEEE","IEEE Conferences"
"Voltage estimation using ICA on distribution system","M. Mohd Hussain; Z. H. Zakaria; S. Serwan","Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Advanced Power Solution, Shah Alam, Selangor, Malaysia","2013 IEEE 7th International Power Engineering and Optimization Conference (PEOCO)","","2013","","","267","272","This paper presents a method to estimate voltage profiles on source distribution system using Independent Component Analysis (ICA) algorithm. The profiles are estimated for 24- hours with time interval of 1 minute. The objective of this research is to reduce loss by controlling source of voltage per-unit. When voltage source is controlled, the losses occur on the system is reduced drastically. Power System Simulation for Engineering (PSS™E) Software and Python Programming was used in the simulation of IEEE 69 test bus system, while ICA is programmed using MATLAB R2012b version. The losses of simulation results with different tap changer and voltage set are compared and discussed in this paper.","","978-1-4673-5074-7978-1-4673-5072-3978-1-4673-5073","10.1109/PEOCO.2013.6564555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564555","Electrical Distribution System;Voltage Profile;Independent Component Analysis;Power System Simulation Network","Voltage control;Propagation losses;Independent component analysis;Reactive power;Principal component analysis;Power engineering","independent component analysis;power distribution;power system simulation;voltage measurement","voltage estimation;ICA;independent component analysis;source distribution system;voltage per-unit;power system simulation for engineering;Python programming;IEEE 69 test bus system;MATLAB R2012b;time 24 hour;time 1 min","","1","9","","","","","","IEEE","IEEE Conferences"
"A scalable physical model for Nano-Electro-Mechanical relays","H. Alrudainy; A. Mokhov; A. Yakovlev","School of Electrical and Electronic Engineering, Newcastle University, Newcastle upon Tyne, NE1 7RU, England, UK; School of Electrical and Electronic Engineering, Newcastle University, Newcastle upon Tyne, NE1 7RU, England, UK; School of Electrical and Electronic Engineering, Newcastle University, Newcastle upon Tyne, NE1 7RU, England, UK","2014 24th International Workshop on Power and Timing Modeling, Optimization and Simulation (PATMOS)","","2014","","","1","7","Nano-Electro-Mechanical (NEM) relay is a promising device overcoming the energy-efficiency limitations of CMOS transistors operating at or near the sub-threshold voltage. Many exploratory research projects are currently under way investigating the mechanical, electrical and logical characteristics of NEM relays. One particular issue that this paper addresses is the need for a scalable and accurate physical model of the NEM switch that can be plugged into the standard EDA software. The existing models are accurate and detailed but they suffer from the convergence problem. This problem requires finding ad-hoc workarounds and significantly impacts the designer's productivity. In this paper we propose a new simplified Verilog-A mode. To test scalability of the proposed model we cross-checked it against our analysis of a range of benchmark circuits. Results show that, compared to standard model, the proposed model is sufficiently accurate with an average of 6% error and can handle larger designs without divergence. In particular the largest circuit we could handle with the standard model in our experiments contained only 22 NEM relays, while the proposed approach could handle circuits comprised of 192 NEM relays.","","978-1-4799-5412","10.1109/PATMOS.2014.6951889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6951889","","Logic gates;Relays;Integrated circuit modeling;Delays;Electrodes;Semiconductor device modeling;Mathematical model","CMOS integrated circuits;hardware description languages;nanoelectromechanical devices;semiconductor device models;semiconductor relays","scalable physical model;nano-electro-mechanical relays;NEM relay;CMOS transistors;subthreshold voltage;NEM switch;EDA software;Verilog-A mode","","7","23","","","","","","IEEE","IEEE Conferences"
"Digitized Walsh algorithm for reactive power estimation","G. Aliyu; S. B. A. Khalid; M. W. Mustafa; J. Usman; H. Shareef","Department of Electrical Power System, Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310, Johor Bahru, Malaysia; Department of Electrical Power System, Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310, Johor Bahru, Malaysia; Department of Electrical Power System, Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310, Johor Bahru, Malaysia; Department of Electrical Power System, Faculty of Electrical Engineering, Universiti Teknologi Malaysia, 81310, Johor Bahru, Malaysia; Faculty of Electrical Engineering and Build Environment, Universiti Kabangsaan Malaysia, 43600 Bangi, Selangor, Malaysia","2013 IEEE 7th International Power Engineering and Optimization Conference (PEOCO)","","2013","","","144","148","This paper presents a digitized Walsh function algorithm as an alternative approach for power components estimation with emphasis on reactive power measurement in linear and nonlinear three-phase load system. The developed algorithm was modeled on the Matlab Simulink software; linear and nonlinear loads were modeled based on practical voltage and current waveforms and tested with the proposed improved Walsh algorithm. IEEE standard 1459-2000 which is based on fast Fourier transform FFT approach was used as benchmark for the linear load system while a laboratory experiment validates the proposed improved algorithm for nonlinear load measurement. The results showed that the algorithm can effectively measure three-phase reactive power components under different load conditions. Walsh function unified approach has high inherent accuracy level as a result of coefficient features and energy behaviour representation.","","978-1-4673-5074-7978-1-4673-5072-3978-1-4673-5073","10.1109/PEOCO.2013.6564532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6564532","Walsh function;reactive power;linear and nonlinear load;Matlab Simulink;fast Fourier transform","Reactive power;Power measurement;Load modeling;Mathematical model;Harmonic analysis;Distortion measurement;Power system harmonics","fast Fourier transforms;IEEE standards;power system measurement;reactive power;Walsh functions","reactive power estimation;digitized Walsh function algorithm;power components estimation;reactive power measurement;three-phase load system;Matlab Simulink software;nonlinear loads;voltage waveform;current waveform;IEEE standard 1459-2000;fast Fourier transform;linear load system;nonlinear load measurement;three-phase reactive power components;load conditions;energy behaviour representation","","","22","","","","","","IEEE","IEEE Conferences"
"State of Charge balancing control of a multi-functional battery energy storage system based on a 11-level cascaded multilevel PWM converter","S. Wang; R. Teodorescu; L. Mathe; E. Schaltz; P. Dan Burlacu","Dep. of Electrical Engineering and New Material, China Electric Power Research Institute, Beijing, China; Dep. of Energy Technology, Aalborg University, Denmark; Dep. of Energy Technology, Aalborg University, Denmark; Dep. of Energy Technology, Aalborg University, Denmark; Dep. of Energy Technology, Aalborg University, Denmark","2015 Intl Aegean Conference on Electrical Machines & Power Electronics (ACEMP), 2015 Intl Conference on Optimization of Electrical & Electronic Equipment (OPTIM) & 2015 Intl Symposium on Advanced Electromechanical Motion Systems (ELECTROMOTION)","","2015","","","336","342","This paper focuses on modeling and SOC (State of Charge) balancing control of lithium-ion battery energy storage system based on cascaded multilevel converter for both grid integration and electric vehicle propulsion applications. The equivalent electrical circuit model of lithium-ion battery module is established based on the relationship between SOC (State of Charge) and OCV (Open Circuit Voltage) which is obtained from the battery charge and discharge test curves. A hierarchical control structure is proposed to realize different operating modes. The decoupled current control scheme is adopted to control active power and reactive power independently, and the zero-sequence voltage injection and a sorting and select algorithm are employed for SOC balancing control. The simulation results have been carried out with PLECS Simulation Software and are presented to validate the SOC control schemes.","","978-1-4673-7239-8978-1-4673-7547","10.1109/OPTIM.2015.7427002","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7427002","battery energy storage system;state of charge;battery modeling;cascaded multilevel converter;zero sequence voltage injection","Batteries;Integrated circuit modeling;Resistance;Mathematical model;Control systems;Numerical models","battery storage plants;lithium compounds;PWM power convertors;reactive power;secondary cells","state of charge balancing control;multifunctional battery energy storage system;multilevel PWM converter;SOC;lithium-ion battery energy storage system;cascaded multilevel converter;electric vehicle propulsion;equivalent electrical circuit model;lithium-ion battery module;open circuit voltage;reactive power;zero-sequence voltage injection;PLECS simulation software","","2","18","","","","","","IEEE","IEEE Conferences"
"An interconnection reconfiguration method for concentrator photovoltaic array","F. L. Siaw; K. K. Chong","Faculty of Engineering and Science, Universiti Tunku Abdul Rahman, Off Jalan Genting Kelang, Setapak, Kuala Lumpur, 53300, Malaysia; Faculty of Engineering and Science, Universiti Tunku Abdul Rahman, Off Jalan Genting Kelang, Setapak, Kuala Lumpur, 53300, Malaysia","2013 IEEE 39th Photovoltaic Specialists Conference (PVSC)","","2013","","","0486","0488","This paper reports on the application of heuristic algorithms for one-dimensional bin-packing problems in solving current mismatch of concentrator photovoltaic (CPV) arrays that is caused by non-uniform solar flux distribution. As a case study, actual flux profile for a non-imaging planar concentrator (NIPC) is measured based on CPV cells' location in a dense-array. Then, forty-four solar cells are reconfigured according to heuristic algorithm results. From this study, current mismatch is minimized from 1.03A to 0.07A resulting an increase in output power from 83.99W to 112.91 W. This reconfiguration method is tested by comparisons with Simulink current-voltage (I-V) curve results.","0160-8371","978-1-4799-3299-3978-1-4799-3298","10.1109/PVSC.2013.6744195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6744195","arrays;current-voltage characteristics;heuristic algorithms;photovoltaic systems;optimal matching;optimization methods","Arrays;Photovoltaic cells;Optical sensors;Photovoltaic systems;Heuristic algorithms;Software packages","photovoltaic cells;solar cells;solar energy concentrators","interconnection reconfiguration method;concentrator photovoltaic array;CPV;1D bin-packing problems;nonuniform solar flux distribution;nonimaging planar concentrator;NIPC;solar cells;heuristic algorithm;Simulink current-voltage curve;current 0.07 A to 1.03 A;power 83.99 W to 112.91 W","","","6","","","","","","IEEE","IEEE Conferences"
"The study of different transmission lines in high speed optical module","H. He; B. Li; Y. Sun","National Center for Advanced Packaging (NCAP), Wuxi, Jiangsu, China, 214135; National Center for Advanced Packaging (NCAP), Wuxi, Jiangsu, China, 214135; National Center for Advanced Packaging (NCAP), Wuxi, Jiangsu, China, 214135","2014 15th International Conference on Electronic Packaging Technology","","2014","","","1052","1055","In this paper, two kinds of transmission line, microstrip line and coplanar waveguide, with different lengths and widths are designed to study the transmission loss of different geometries. The material between the signal line and reference plane is a kind of organic substrate material which has low permittivity. Firstly, each transmission line is designed with 50 Ohm impedance to ensure little reflected signal when 50 Ohm lump port was used. Then, GSG pads for probe test at the ends of transmission line and transition sections between GSG pads and transmission lines is designed and optimized by electromagnetic simulation software to minimize port reflection. Finally, the optimized structures are simulated and the S parameter is obtained. Additionally, the unit length loss is calculated. The results show that coplanar waveguide (with -0.5dB/cm loss at 25GHz)shows less transmission loss than microstrip (with -1dB/cm at 15GHz). By testing the samples, the property of organic substrate material and the process of organic substrate fabrication can be evaluated. Meanwhile, the influence of process tolerance to different transmission lines can be demonstrated and back to guide the design of highspeed transmission lines on this substrate.","","978-1-4799-4707","10.1109/ICEPT.2014.6922826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6922826","transmission line;microstrip line;coplanar waveguide;permittivity;S parameter","Coplanar waveguides;Microstrip;Dielectric losses;Conductors;Propagation losses;Optical waveguides","coplanar waveguides;microstrip lines;optical interconnections;permittivity;reflectivity;S-parameters;substrates","transmission line;high speed optical module;microstrip line;coplanar waveguide;transmission loss;organic substrate material;permittivity;GSG pad;probe test;electromagnetic simulation software;port reflection;S parameter;unit length loss;organic substrate fabrication;ground-signal-ground probe;optical interconnection","","","7","","","","","","IEEE","IEEE Conferences"
"An Adaptive Routing Scheme for Heterogeneous Data-Flows Using Openflow","P. Goel; L. Srinivasan; V. Varma","NA; NA; NA","2015 IEEE International Conference on Cloud Computing in Emerging Markets (CCEM)","","2015","","","52","58","In this paper, we propose an open flow controller that takes into account different application's needs such as latency, bandwidth, reliability, network jitter, etc and optimizes the flow on a combination of such factors. It is a dynamic system that constantly keeps track of these parameters and adapts to changes happening in the network and pushes updated rules on the fly to the switches. Such a system can efficiently route traffic in data-center by routing packets from different applications in separate routes as per requirement, thereby utilizing the resources more judiciously. We built a prototype controller using POX and studied its routing characteristics extensively. We used network-bandwidth, latency and reliability as the parameters to test the system and simulated the network using mininet and found that the system was able to balance these three parameters for different loads according to the requirements.","","978-1-4673-8566","10.1109/CCEM.2015.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7436931","openflow;mininet;adaptive routing;heterogenous data flows","Bandwidth;Control systems;Routing;Ports (Computers);Reliability;Software;Streaming media","computer centres;data flow computing;software defined networking","heterogeneous data-flows;adaptive routing scheme;Openflow;open flow controller;data-center;POX;mininet;software defined networking","","","9","","","","","","IEEE","IEEE Conferences"
"In situ understanding of performance bottlenecks through visually augmented code","F. Beck; O. Moseler; S. Diehl; G. D. Rey","VISUS, University of Stuttgart, Germany; University of Trier, Germany; University of Trier, Germany; FernUniversität in Hagen, Germany","2013 21st International Conference on Program Comprehension (ICPC)","","2013","","","63","72","Finding and fixing performance bottlenecks requires sound knowledge of the program that is to be optimized. In this paper, we propose an approach for presenting performance-related information to software engineers by visually augmenting source code shown in an editor. Small diagrams at each method declaration and method call visualize the propagation of runtime consumption through the program as well as the interplay of threads in parallelized programs. Advantages of in situ visualization like this over traditional representations, where code and profiling information are shown in different places, promise to be the prevention of a split-attention effect caused by multiple views; information is presented where required, which supports understanding and navigation. We implemented the approach as an IDE plug-in and tested it in a user study with four developers improving the performance of their own programs. The user study provides insights into the process of understanding performance bottlenecks with our approach.","1092-8138","978-1-4673-3092","10.1109/ICPC.2013.6613834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613834","","Runtime;Visualization;Data visualization;Instruction sets;Measurement;Context","data visualisation;multi-threading;software performance evaluation","in-situ performance bottlenecks;visually augmented source code;program optimization;performance-related information;software editor;method declaration;method call;runtime consumption propagation visualization;parallelized programs;multithreading;in-situ visualization;IDE plug-in;program performance improvement","","16","34","","","","","","IEEE","IEEE Conferences"
"Improving the performance of assembly line: Review with case study","A. B. Rane; D. S. S. Sudhakar; V. k. Sunnapwar; S. Rane","Department of Mechanical Engineering, Fr. C.R.I.T., Vashi, Navi Mumbai, India; Department of Production Engineering, Fr. C.R.C.E., Bandra (w), Mumbai, India; Research and Academics, LTCE, Navi Mumbai, India; Department of Mechanical Engineering, SPCE, Andheri(w), Mumbai, India","2015 International Conference on Nascent Technologies in the Engineering Field (ICNTE)","","2015","","","1","14","Context: Continuous Improvement in manufacturing to attain manufacturing excellence is today's requirement. It gives cutting edge over the competitors. This has developed lot of interest for the researchers during the last few years. Vehicle Assembly Line being the most complex Assembly line has been active area of research over years and has attracted many researchers. Objective: This paper systematically identifies and unfolds the important parameters by systematic review. Goal of this Research is to develop a novel classification of literature on Assembly Line and to improve the performance of vehicle assembly line using Lean implementation and Simulation approach. Methodology: Total of 116 referred journal articles in the domain of Vehicle assembly are reviewed here. Data is sourced from 40 publications, majority of which includes Elsevier, IEEE, and Springer. Further a mathematical model is developed considering the effect of number of resources, Break time, Downtime of machines and Absenteeism. A real world case study is done for a period of one year in a reputed vehicle assembly manufacturing plant. Results: Analysis of research on Assembly Line Balancing and optimization within many different industrial scenarios has been done her. This paper contributes to existing domain by classifying and comparing the means for input data, constraints and methodology used. It has been classified into five heads. A real world case study is done in reputed Company. Cycle time was reduced from 110 seconds to 100 seconds. Conclusion: It is difficult to perform the experiments like Design of Experiments (DOE) in complex manufacturing systems. Simulation allows us to test different concepts, various options without having to build prototypes. A case study done in Vehicle assembly shows the importance of Simulations to improve its performance measures. Limitations of the Research: The research does not completely optimize the production parameters. It improves the production system performance significantly.","","978-1-4799-7263-0978-1-4799-7261","10.1109/ICNTE.2015.7029913","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029913","Simulation;Optimization;Lean;Mathematical modeling;Search Algorithm","Assembly;Mathematical model;Vehicles;Computational modeling;Analytical models;Software","assembling;automobile industry;design of experiments;manufacturing systems;production management","assembly line performance;continuous improvement;manufacturing excellence;vehicle assembly line;literature classification;lean implementation;simulation approach;Elsevier;IEEE;Springer;machine downtime;absenteeism;reputed vehicle assembly manufacturing plant;assembly line balancing;optimization;industrial scenarios;design of experiments;DOE;complex manufacturing systems;production system performance","","1","116","","","","","","IEEE","IEEE Conferences"
"On a High-Performance and Balanced Method of Hardware Implementation for AES","X. Zhang; H. Li; S. Yang; S. Han","NA; NA; NA; NA","2013 IEEE Seventh International Conference on Software Security and Reliability Companion","","2013","","","16","20","Hardware implementation provides a higher level of security and cryptography speed at some lower resource cost, compared to software implementation of AES. In this paper, we present a balanced hardware design and implementation for AES, considering several existing implementations. FPGA implementation offers higher speed solution and can be easily adapted to protocol changes, although the AES can be implemented with software or pure hardware. So, this implementation is equipped with regard to FPGA. Optimized and Synthesizable Verilog HDL is developed as the design entry to Quartus II 10.0 software. After obtaining gate-level netlists, timing simulations are performed using ModelSim SE 6.1f. Both 128 bits data block encryption and decryption processes are tested. The major part of an AES design is the realization of substitute boxes (S-boxes). S-boxes in our design are compared between two main existing implementations. With Quartus II device family of Stratix, throughput of up to 2.33 Gb/s is received.","","978-1-4799-2925-2978-1-4799-2924","10.1109/SERE-C.2013.13","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616320","AES;FPGA;Rijndael;Balanced Hardware Implementation","Encryption;Field programmable gate arrays;Hardware;Ciphers;Table lookup;Throughput;Application specific integrated circuits","cryptographic protocols;field programmable gate arrays;hardware description languages;logic design","high-performance method;balanced method;hardware implementation;AES design;advanced encryption standard;balanced hardware design;FPGA implementation;protocol changes;Verilog HDL;Quartus II 10.0 software;gate-level netlists;timing simulations;ModelSim SE 6.1f;data block encryption process;data block decryption process;substitute boxes;S-boxes;Stratix;bit rate 2.33 Gbit/s","","1","15","","","","","","IEEE","IEEE Conferences"
"OpenTuner: An extensible framework for program autotuning","J. Ansel; S. Kamil; K. Veeramachaneni; J. Ragan-Kelley; J. Bosboom; U. O'Reilly; S. Amarasinghe","Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA; Massachusetts Institute of Technology, Cambridge, MA","2014 23rd International Conference on Parallel Architecture and Compilation Techniques (PACT)","","2014","","","303","315","Program autotuning has been shown to achieve better or more portable performance in a number of domains. However, autotuners themselves are rarely portable between projects, for a number of reasons: using a domain-informed search space representation is critical to achieving good results; search spaces can be intractably large and require advanced machine learning techniques; and the landscape of search spaces can vary greatly between different problems, sometimes requiring domain specific search techniques to explore efficiently. This paper introduces OpenTuner, a new open source framework for building domain-specific multi-objective program autotuners. OpenTuner supports fully-customizable configuration representations, an extensible technique representation to allow for domain-specific techniques, and an easy to use interface for communicating with the program to be autotuned. A key capability inside OpenTuner is the use of ensembles of disparate search techniques simultaneously; techniques that perform well will dynamically be allocated a larger proportion of tests. We demonstrate the efficacy and generality of OpenTuner by building autotuners for 7 distinct projects and 16 total benchmarks, showing speedups over prior techniques of these projects of up to 2.8χ with little programmer effort.","","978-1-4503-2809-8978-1-5090-6607","10.1145/2628071.2628092","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7855909","","Search problems;Manipulators;Optimization;Buildings;Benchmark testing;Quality of service;Libraries","configuration management;learning (artificial intelligence);public domain software","domain-specific multi-objective program autotuners;open source framework;machine learning;domain-informed search space representation;OpenTuner","","35","35","","","","","","IEEE","IEEE Conferences"
"The development of a data-driven application benchmarking approach to performance modelling","A. Osprey; G. D. Riley; M. Manjunathaiah; B. N. Lawrence","University of Reading, UK; University of Manchester, UK; University of Reading, UK; University of Reading, UK","2014 International Conference on High Performance Computing & Simulation (HPCS)","","2014","","","715","723","Performance modelling is a useful tool in the lifeycle of high performance scientific software, such as weather and climate models, especially as a means of ensuring efficient use of available computing resources. In particular, sufficiently accurate performance prediction could reduce the effort and experimental computer time required when porting and optimising a climate model to a new machine. Yet as architectures become more complex, performance prediction is becoming more difficult. Traditional methods of performance prediction, based on source code analysis and supported by machine benchmarks, are proving inadequate to the task. In this paper, the reasons for this are explored by applying some traditional techniques to predict the computation time of a simple shallow water model which is illustrative of the computation (and communication) involved in climate models. These models are compared with real execution data gathered on AMD Opteron-based systems, including several phases of the U.K. academic community HPC resource, HECToR. Some success is had in relating source code to achieved performance for the K10 series of Opterons, but the method is found to be inadequate for the next-generation Interlagos processor. The experience leads to the investigation of a data-driven application benchmarking approach to performance modelling. Results for an early version of the approach are presented using the shallow model as an example. In addition, the data-driven approach is compared with a novel analytical model based on fitting logarithmic curves to benchmarked application data. The limitations of this analytical method provide further motivation for the development of the data-driven approach and results of this work have been published elsewhere.","","978-1-4799-5313-4978-1-4799-5312-7978-1-4799-5311","10.1109/HPCSim.2014.6903760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903760","Performance modelling;benchmarking;multicore;shallow water model","Computational modeling;Analytical models;Benchmark testing;Bandwidth;Meteorology;Computer architecture;Mathematical model","curve fitting;geophysics computing;parallel processing;source code (software)","data-driven application benchmarking approach development;performance modelling;high performance scientific software lifeycle;climate model;performance prediction;source code analysis;machine benchmarks;shallow water model;AMD Opteron-based systems;UK academic community HPC resource;HECToR;next-generation Interlagos processor;logarithmic curve fitting","","","21","","","","","","IEEE","IEEE Conferences"
"Development of web-based savings Kurban management application with YII framework case study: CV Almanna","D. R. Hartadi; Nurhayati","Department of Informatics Engineering, Syarif Hidayatullah State Islamic University Jakarta; Department of Informatics Engineering, Syarif Hidayatullah State Islamic University Jakarta","2014 International Conference on Cyber and IT Service Management (CITSM)","","2014","","","98","103","Management application of Tabungan Kurban is an application that integrates business process of data management clients are involved in a business process, specifically in the process of saving sacrifice or kurban. The objective of developing the application is to replace manual activities in managing kurban savings, including deposits and records of savings. The inefficiency of having a manual booking of kurban savings increases spendings and consumes time, therefore an effective solution to solve this issue is needed. The author proposes a solution to these problems by creating a Tabungan Kurban Management application, so that the issues existing as a result of using the manual systems in business processes can be optimized, specifically in the CV Almanna Corporation. This application will contain functions such as bookkeeping clients include credit, debit and balance of the savings account, then the customer data management, and financial statements. The software development methodology used is Rapid Application Development (RAD), built using the programming language PHP with the Yii framework and UML as the modeling language and MySQL as thedatabase management system.","","978-1-4799-7975-2978-1-4799-7973","10.1109/CITSM.2014.7042184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7042184","applications;customer;database;framework;management;RAD;savings YII","Unified modeling language;Business;Databases;Testing;Manuals;Generators","financial data processing;Internet;relational databases;software engineering;SQL;Unified Modeling Language","Web-based savings Kurban management application;YII framework case study;CV Almanna Corporation;business process;data management clients;deposits;savings records;manual booking;credit;debit;savings account balance;bookkeeping clients;customer data management;financial statements;software development methodology;rapid application development;PHP programming language;UML;modeling language;MySQL;database management system","","","13","","","","","","IEEE","IEEE Conferences"
"Newborns' cry analysis classification using signal processing and data mining","F. Feier; I. Enătescu; C. Ilie; I. Silea","University Politehnica Timisoara, Department of Automation and Applied Informatics; “Victor Babes” University of Medicine and Pharmacy Timisoara, BEGA University Clinic; “Victor Babes” University of Medicine and Pharmacy Timisoara, BEGA University Clinic; University Politehnica Timisoara, Department of Automation and Applied Informatics","2014 International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)","","2014","","","880","885","Newborns' cry is one of the very few indicators on the newborns' state of health. A couple of studies have been performed in the last 20 years with the goal of extracting valuable information from the newborns' cry in order to find out valuable information that would normally be obtained from excessive invasive tests or not available at all with current state of the art medical techniques. Among the focuses of the most recent studies, pathologies like asphyxia, hypoxia, hypothyroidism or hearing disorders are investigated to determine correlations between these and features from the cry signal. In this study, a considerable amount of recorded cry signals from newborns' (300 subjects) is studied in order to classify, by the use of data mining techniques. For this classification four categories of newborns' were considered: newborns' with no detected health issues or problems at birth, newborns' that suffered umbilical cord strangulation at birth, premature born babies (before 38 weeks of pregnancy) and newborns' with different pathologies not included in one of the above categories. By the use of data mining techniques (classification trees, decision rules and lazy algorithms) a classification with very good accuracy has been made for the first three of the above mentioned newborn categories.","1842-0133","978-1-4799-5183","10.1109/OPTIM.2014.6850990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850990","","Pediatrics;Classification algorithms;Data mining;Software;Training;Pathology;Accuracy","data mining;medical signal processing;signal classification","newborn cry analysis classification;signal processing;data mining;medical techniques;asphyxia;hypoxia;hypothyroidism;hearing disorders;umbilical cord strangulation;classification trees;decision rules;lazy algorithms","","","22","","","","","","IEEE","IEEE Conferences"
"A study on anti-islanding detection algorithms for grid-tied photovoltaic systems","I. V. Banu; M. Istrate; D. Machidon; R. Pantelimon","&#x201C;Gheorghe Asachi&#x201D; Technical University of Iasi, Romania; &#x201C;Gheorghe Asachi&#x201D; Technical University of Iasi, Romania; &#x201C;Gheorghe Asachi&#x201D; Technical University of Iasi, Romania; &#x201C;Gheorghe Asachi&#x201D; Technical University of Iasi, Romania","2014 International Conference on Optimization of Electrical and Electronic Equipment (OPTIM)","","2014","","","655","660","This study analyzes various anti-islanding (AI) protection relays when the islanding condition of Grid-Tied PV (photovoltaic) System appears at the Point of Common Coupling (PCC) between the PV Solar Power System and the power grid. The main purpose of the study is to determine the performance of several AI prevention schemes in detecting the presence of an island, by monitoring the detection time of the islanding condition through different methods. The devices used to implement the methods include over-current and under-current (OI/UI) relays, over-voltage and under-voltage (OV/UV) relays, over-frequency and under-frequency (OF/UF) relays, rate of change of frequency (ROCOF) and Vector Shift relays. The protection was tested in case of complete disconnection of the PV system from the electric power grid and also in case of various grid faults.","1842-0133","978-1-4799-5183","10.1109/OPTIM.2014.6850940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850940","","Relays;Artificial intelligence;Islanding;Software packages;Power conversion;Voltage control","photovoltaic power systems;relay protection","antiislanding detection algorithms;grid tied photovoltaic systems;protection relays;point of common coupling;solar power system;power grid;overcurrent relays;undercurrent relays;overvoltage relays;undervoltage relays;overfrequency relays;underfrequency relays;rate of change of frequency;vector shift relays","","7","15","","","","","","IEEE","IEEE Conferences"
"Computationally efficient HEVC/H.265 motion estimation algorithm for low power applications","F. Yasir; S. Hasan","Computer & Information Systems Engineering Department, NED University of Engineering & Technology, University Road, Karachi, Pakistan; Computer & Information Systems Engineering Department, NED University of Engineering & Technology, University Road, Karachi, Pakistan","2015 International Conference on Information and Communication Technologies (ICICT)","","2015","","","1","6","High Efficiency Video Coding (HEVC/H.265) is an emerging standard for video compression that provides almost double compression efficiency at the cost of major computational complexity increase as compared to current industry-standard Advanced Video Coding (AVC/H.264). In HEVC more flexible coding options and partitioning types are provided for prediction units (PUs) and consequently motion estimation (ME) is more involved than previous video-coding standards. The existing fast ME algorithms, including the test zone search provided in the HEVC reference software, cannot generate high-quality video with reasonable computational complexity. In this paper, four algorithms are proposed and designed for optimizing the motion estimation process for the H.265/HEVC whilst maintaining the same quality and the compression rate as the standard. The conducted experiments show significant speed improvements, thus making a novel contribution to the implementation of realtime H.265 standard encoders in computationally constrained environments such as low-power mobile devices and general purpose computers.","","978-1-4673-8907-5978-1-4673-8906","10.1109/ICICT.2015.7469487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469487","low power design;HEVC;video coding;architecture;algorithm;energy efficiency;analysis;test zone search;motion estimation;TZ search","Motion estimation;Algorithm design and analysis;Diamond;Encoding;Standards","computational complexity;data compression;motion estimation;video coding","computers;low-power mobile devices;H.265 standard encoders;high-quality video;HEVC reference software;test zone search;video-coding standards;ME algorithms;motion estimation process;prediction units;flexible coding options;AVC-H.264;advanced video coding;computational complexity;video compression;HEVC-H.265 motion estimation algorithm;high efficiency video coding","","","31","","","","","","IEEE","IEEE Conferences"
"An algorithm for assessment of the impact of renewable energy sources on the power flow of the electric power system","N. Nikolaev; K. Gerasimov; Y. Rangelov; K. Gerasimov","Department Electric Power Engineering, Technical University of Varna, Bulgaria; Department Electric Power Engineering, Technical University of Varna, Bulgaria; Department Electric Power Engineering, Technical University of Varna, Bulgaria; Department Electric Power Engineering, Technical University of Varna, Bulgaria","2013 12th International Conference on Environment and Electrical Engineering","","2013","","","169","174","The problem for the assessment of renewable energy sources impact on the power flows and transients of the electric power system becomes relevant, due to their continuously growing share in the energy mix. In the literature there are many algorithms for power flow analysis which consider the uncertain nature of the renewables. These algorithms in some cases involve solving optimizations problems and require statistical initial data. This paper proposes an easy to implement probabilistic power flow algorithm which requires minimal input data and it is based on the Monte Carlo method. The algorithm is fast and capable to assess the impact of renewable energy sources on the loading of the power lines and transformers, as well as the voltage levels in the network. It is implemented and tested in a software tool for power system analysis developed by the authors. The presented results prove the practical application of the algorithm for solving this type of problems.","","978-1-4673-3059-6978-1-4673-3060-2978-1-4673-3058","10.1109/EEEIC.2013.6549611","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549611","probabilistic power flow;overloading;wind farm;Monte Carlo method;renewable energy sources","Generators;Power generation;Load flow;Loading;Wind farms;Renewable energy sources","load flow;Monte Carlo methods;optimisation;power cables;power system analysis computing;power transformers;probability;renewable energy sources","renewable energy sources;electric power system analysis;energy mix;optimizations problems;statistical initial data;probabilistic power flow algorithm;input data;Monte Carlo method;power lines;power transformers;software tool","","","12","","","","","","IEEE","IEEE Conferences"
"Phase detection for conductivity based on electromagnetic measurement system","Y. Fu; F. Dong; C. Tan","Tianjin Key Laboratory of Process Measurement and Control, School of Electrical Engineering and Automation, Tianjin University, China; Tianjin Key Laboratory of Process Measurement and Control, School of Electrical Engineering and Automation, Tianjin University, China; Tianjin Key Laboratory of Process Measurement and Control, School of Electrical Engineering and Automation, Tianjin University, China","2013 IEEE International Conference on Imaging Systems and Techniques (IST)","","2013","","","184","189","In electromagnetic tomography, phase of measurements from receivers contain the significant information of conductivity for target to test. The quality of reconstruction image mainly depends on the precision of phase detection. To discuss the effects in this aspect, simulate the relative conditions by the finite element simulation software, COMSOL Multi-physics. Conduct simulations for the target in object field with different conductivity and simulate the conditions when the target place at different positions in object field. To verify the validity of the results from simulations, design a prototype EMT system. This system utilizes NI PXI-5105 board to collect the high frequency signal directly, which could guarantee the precision and simplify the complexity of hardware. Based on the prototype EMT system, use the salt solution with different conductivities as target and conduct a series of relevant experiments. Results from simulations and experiments show the same variation tendency for each condition. It makes foundation for the system developing and optimization in future.","1558-2809","978-1-4673-5791-3978-1-4673-5790","10.1109/IST.2013.6729688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6729688","electromeagnetic tomography;phase detection;conductivity;object field distribution","Conductivity;Coils;Receivers;Phase detection;Impedance;Magnetic flux;Imaging","biomagnetism;biomedical measurement;electric impedance imaging;image reconstruction;medical image processing;optimisation;tomography","electromagnetic measurement system;electromagnetic tomography;phase measurements;receivers;target conductivity;image reconstruction quality;phase detection precision;finite element simulation software;COMSOL Multi-physics;object field;target place;prototype EMT system;NI PXI-5105 board;high frequency signal;hardware complexity;optimization","","","11","","","","","","IEEE","IEEE Conferences"
"Comparative Evaluation of Registration Algorithms in Different Brain Databases With Varying Difficulty: Results and Insights","Y. Ou; H. Akbari; M. Bilello; X. Da; C. Davatzikos","Center for Biomedical Image Computing and Analytics (CBICA), Department of Radiology, University of Pennsylvania, Philadelphia; Center for Biomedical Image Computing and Analytics (CBICA), Department of Radiology, University of Pennsylvania, Philadelphia; Center for Biomedical Image Computing and Analytics (CBICA), Department of Radiology, University of Pennsylvania, Philadelphia; Center for Biomedical Image Computing and Analytics (CBICA), Department of Radiology, University of Pennsylvania, Philadelphia; Center for Biomedical Image Computing and Analytics (CBICA), Department of Radiology, University of Pennsylvania, Philadelphia","IEEE Transactions on Medical Imaging","","2014","33","10","2039","2065","Evaluating various algorithms for the inter-subject registration of brain magnetic resonance images (MRI) is a necessary topic receiving growing attention. Existing studies evaluated image registration algorithms in specific tasks or using specific databases (e.g., only for skull-stripped images, only for single-site images, etc.). Consequently, the choice of registration algorithms seems task- and usage/parameter-dependent. Nevertheless, recent large-scale, often multi-institutional imaging-related studies create the need and raise the question whether some registration algorithms can 1) generally apply to various tasks/databases posing various challenges; 2) perform consistently well, and while doing so, 3) require minimal or ideally no parameter tuning. In seeking answers to this question, we evaluated 12 general-purpose registration algorithms, for their generality, accuracy and robustness. We fixed their parameters at values suggested by algorithm developers as reported in the literature. We tested them in 7 databases/tasks, which present one or more of 4 commonly-encountered challenges: 1) inter-subject anatomical variability in skull-stripped images; 2) intensity homogeneity, noise and large structural differences in raw images; 3) imaging protocol and field-of-view (FOV) differences in multi-site data; and 4) missing correspondences in pathology-bearing images. Totally 7,562 registrations were performed. Registration accuracies were measured by (multi-)expert-annotated landmarks or regions of interest (ROIs). To ensure reproducibility, we used public software tools, public databases (whenever possible), and we fully disclose the parameter settings. We show evaluation results, and discuss the performances in light of algorithms' similarity metrics, transformation models and optimization strategies. We also discuss future directions for the algorithm development and evaluations.","0278-0062;1558-254X","","10.1109/TMI.2014.2330355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6834815","Brain magnetic resonance imaging (MRI);deformable image registration;evaluation;registration accuracy","Databases;Accuracy;Magnetic resonance imaging;Protocols;Algorithm design and analysis;Tuning","biomedical MRI;brain;image registration;medical image processing;optimisation","comparative evaluation;brain databases;intersubject registration;brain magnetic resonance images;MRI;image registration algorithms;skull-stripped images;single-site images;task-dependent;usage/parameter-dependent;multiinstitutional imaging-related studies;general-purpose registration algorithms;intersubject anatomical variability;intensity homogeneity;noise;large structural differences;raw images;imaging protocol;field-of-view differences;FOV;multisite data;pathology-bearing images;registration accuracies;expert-annotated landmarks;regions of interest;ROI;public software tools;public databases;parameter settings;algorithm similarity metrics;transformation models;optimization strategies;algorithm development;algorithm evaluations","Adult;Algorithms;Alzheimer Disease;Brain;Brain Neoplasms;Databases, Factual;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Imaging;Neuroimaging","38","129","","","","","","IEEE","IEEE Journals & Magazines"
"Evaluation of NoSQL prototypes for the CMS conditions database","R. Sipos","CMS Collaboration","2015 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC)","","2015","","","1","7","With the restart of the LHC in 2015, the growth of the CMS conditions dataset will continue, therefore the need of a consistent and highly available access to the conditions makes a great cause to revisit different aspects of the current data storage solutions. We present a study of alternative data storage backends for the CMS Conditions Databases, evaluating some of the most popular NoSQL ones to support a key-value representation of the CMS conditions. In addition to the baseline performance comparison between a document store, a column-oriented, and a plain key-value store, the access layer for these databases in the CMS software was developed, in order to provide transparent support for these alternative data stores in the CMS context. The necessary changes in the software infrastructure, and in the modeling approaches are also discussed in this paper. We also discuss the validation phase, which plays a key role in the optimization of the different solutions with fine-tuning critical performance factors. In this paper we present a performance comparison between the NoSQL prototypes and the current Oracle solution, using the real dataset, accessed by the CMS software framework.","","978-1-4673-9862-6978-1-4673-9863","10.1109/NSSMIC.2015.7581881","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7581881","","Testing;Payloads;Software;Distributed databases;Routing;Prototypes","relational databases;storage management","NoSQL prototype evaluation;CMS condition database;data storage backends;key-value representation;document store;plain key-value store;column-oriented store;fine-tuning critical performance factors;Oracle solution;CMS software framework","","","11","","","","","","IEEE","IEEE Conferences"
"Establishing an alternative communication network in disaster situations using smart devices","Y. Çeri; A. Gülbağ","Bilgisayar Mühendisliği Bölümü, Sakarya Üniversitesi, Sakarya, Türkiye; Bilgisayar Mühendisliği Bölümü, Sakarya Üniversitesi, Sakarya, Türkiye","2014 22nd Signal Processing and Communications Applications Conference (SIU)","","2014","","","1331","1334","This study aims to set up an alternative communication network which enables rescue crew to communicate with each other and disaster victims in case of a cellular network breakdown or overload in a natural disaster such as earthquake or storm. The communication network is created via establishing mesh network by using Wi-fi interface of smart devices. Commonly used smart phones with Android operating systems are preferred as smart devices. A “push to talk” appliction is developed on Android. Open source software MANET Manager framework is used for creating mesh network. A series of tests on Android devices and computers are applied by creating mesh network with a software using OLSR (Optimized Link State Routing) routing protocol. As a result of the tests, communication among devices is succeeded.","2165-0608","978-1-4799-4874","10.1109/SIU.2014.6830483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6830483","Android;MANET;OLSR;MANET Manager","Mobile ad hoc networks;Smart phones;Kernel;Androids;Humanoid robots;IEEE 802.11 Standards","Android (operating system);earthquakes;emergency management;mobile ad hoc networks;routing protocols;smart phones;wireless LAN;wireless mesh networks","alternative communication network;disaster situations;smart devices;cellular network breakdown;mesh network;Wi-fi interface;smart phones;Android operating systems;push-to-talk appliction;open source software;MANET Manager framework;optimized link state routing;OLSR protocol","","","8","","","","","","IEEE","IEEE Conferences"
"Can Malware Be Exterminated by Better Understanding Its Roots?","M. C. Y. Cho; C. Hsu; S. Shieh; C. Wang","National Chaio Tung University, Taiwan; National Chaio Tung University, Taiwan; National Chaio Tung University, Taiwan; National Chaio Tung University, Taiwan","IT Professional","","2014","16","6","47","53","Can malware be exterminated? Is there such a utopia? Or is there at least a Holy Grail in malware detection that almost gets us there? These questions inspire debate. Pessimists believe that complete malware detection is an unsolvable and nonboundable problem; optimists acknowledge that the problem is difficult but argue for eventual solvability. The authors' own investigation into malware reveals pitfalls in current research. If these pitfalls can be avoided, they argue that there might be a path to some form of malware freedom, even if malware recognition is an undecidable problem.","1520-9202;1941-045X","","10.1109/MITP.2014.86","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964976","malware;computer security;cloud computing;security;cloud","Malware;Computer security;Database systems;Software development;Benchmark testing;Information security","invasive software","malware extermination;malware detection;malware recognition","","","16","","","","","","IEEE","IEEE Journals & Magazines"
"Design and preliminary test results of a novel microsurgical telemanipulator system","R. Cau; F. B. F. Schoenmakers; M. Steinbuch; T. J. M. van Mulken; R. R. W. J. van der Hulst","Eindhoven University of Technology (TU/e), P.O. Box 513, 5600 MB Eindhoven, the Netherlands; Eindhoven University of Technology (TU/e), P.O. Box 513, 5600 MB Eindhoven, the Netherlands; Eindhoven University of Technology (TU/e), P.O. Box 513, 5600 MB Eindhoven, the Netherlands; Maastricht University Medical Center (MUMC), P.O. Box 5800, 6202 AZ Maastricht, the Netherlands; Maastricht University Medical Center (MUMC), P.O. Box 5800, 6202 AZ Maastricht, the Netherlands","5th IEEE RAS/EMBS International Conference on Biomedical Robotics and Biomechatronics","","2014","","","352","356","In this paper a novel telemanipulator system is proposed, able to assist during reconstructive surgery procedures involving microsurgical techniques. The proposed solution is based on maintaining work methods and infrastructure in the operating room (OR). An extensive analysis of these conventional methodologies, combined with a review of currently available alternative solutions, has led to the design of a new 7DOF master-slave system. The modular design concept is focused on precision, safety, ease-of-use, and cost-efficiency. A proof-of-concept has been tested, whereas preliminary results indicate a bidirectional precision at the slave end effector of 70 μm. Through optimization of the control software, a bidirectional precision down to 30-40 μm can be achieved.","2155-1774;2155-1782","978-1-4799-3128-6978-1-4799-3126","10.1109/BIOROB.2014.6913801","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913801","","Microsurgery;Manipulators;Gears;Friction;Instruments","end effectors;medical robotics;multi-robot systems;surgery;telerobotics","microsurgical telemanipulator system;reconstructive surgery procedures;microsurgical techniques;operating room;7DOF master-slave system;modular design concept;cost-efficiency;ease-of-use;safety;bidirectional precision;slave end effector;control software;size 30 mum to 40 mum;size 70 mum","","1","20","","","","","","IEEE","IEEE Conferences"
"Intelligent system for multivariables reconfiguration of distribution networks","A. P. Mello; D. P. Bernardon; L. L. Pfitscher; M. Sperandio; B. B. Toller; M. Ramos","Universidade Federal de Santa Maria, Av. Roraima, 1000 Santa Maria/RS, 97105-900, Brasil; Universidade Federal de Santa Maria, Av. Roraima, 1000 Santa Maria/RS, 97105-900, Brasil; Universidade Federal de Santa Maria - UFSM, Santa Maria - Brasil; Universidade Federal do Pampa, Av. Tiarajú, 810 Alegrete/RS, 97546-550, Brasil; Universidade Federal do Pampa, Av. Tiarajú, 810 Alegrete/RS, 97546-550, Brasil; AES Distribuidora Gaúcha de Energia AS","2013 IEEE PES Conference on Innovative Smart Grid Technologies (ISGT Latin America)","","2013","","","1","6","This paper proposes a new distribution network reconfiguration approach in normal operating conditions. The multivariables are considered such as the energy losses minimization of the primary network and two reliability indices. The selection strategy of network configuration is based on a heuristic method. The method considers only the automated equipment such as switches and remote controlled reclosers in the new configuration analysis. The AHP (Analytic Hierarchic Process) method is used to define the weights for the optimization criteria and to determine the best switching sequence of the network reconfiguration. The reconfiguration is performed by monitoring the real time network. Changes in demand feeders are considered and analyzed by load rates. Also, the best setting for each operation level is defined. The results of the implemented techniques indicate a satisfactory methodology. Tests were performed using a real power utility system.","","978-1-4673-5274-1978-1-4673-5272-7978-1-4673-5273","10.1109/ISGT-LA.2013.6554380","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6554380","AHP;Automatic Reconfiguration;Distribution Network;Multicriteria Decision Making;Smart Grid","Electronic mail;Noise measurement;Smart grids;Monitoring;Analytic hierarchy process;Software;Intelligent systems","analytic hierarchy process;distribution networks;optimisation","intelligent system;multivariables reconfiguration;distribution network reconfiguration approach;normal operating conditions;energy losses minimization;primary network;reliability indices;selection strategy;heuristic method;automated equipment;remote controlled reclosers;optimization criteria;switching sequence;AHP method;analytic hierarchic process method;real time network;demand feeders;load rates;operation level;real power utility system","","3","11","","","","","","IEEE","IEEE Conferences"
"Small inductive safe invariants","A. Ivrii; A. Gurfinkel; A. Belov","IBM Research; Software Engineering Institute; Synopsys","2014 Formal Methods in Computer-Aided Design (FMCAD)","","2014","","","115","122","Computing minimal (or even just small) certificates is a central problem in automated reasoning and, in particular, in automated formal verification. For example, Minimal Unsatisfiable Subsets (MUSes) have a wide range of applications in verification ranging from abstraction and generalization to vacuity detection and more. In this paper, we study the problem of computing minimal certificates for safety properties. In this setting, a certificate is a set of clauses In&#x03C5; such that each clause contains initial states, and their conjunction is safe (no bad states) and inductive. A certificate is minimal, if no subset of In&#x03C5; is safe and inductive. We propose a two-tiered approach for computing a Minimal Safe Inductive Subset (MSIS) of Inv. The first tier is two efficient approximation algorithms that under-and over-approximate MSIS, respectively. The second tier is an optimized reduction from MSIS to a sequence of computations of Maximal Inductive Subsets (MIS). We evaluate our approach on the HWMCC benchmarks and certificates produced by our variant of IC3. We show that our approach is several orders of magnitude more effective than the naive reduction of MSIS to MIS.","","978-0-9835-6784","10.1109/FMCAD.2014.6987603","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6987603","","Approximation algorithms;Model checking;Safety;Approximation methods;Benchmark testing;Software engineering;Cognition","computability;formal verification;inference mechanisms;set theory","inductive safe invariants;automated reasoning;automated formal verification;minimal unsatisfiable subsets;MUSes;abstraction;vacuity detection;safety property;minimal safe inductive subset;MSIS;approximation algorithms;maximal inductive subsets;MIS;HWMCC benchmark;naive reduction","","4","17","","","","","","IEEE","IEEE Conferences"
"Active Measurement of Memory Resource Consumption","M. Casas; G. Bronevetsky","NA; NA","2014 IEEE 28th International Parallel and Distributed Processing Symposium","","2014","","","995","1004","Hierarchical memory is a cornerstone of modern hardware design because it provides high memory performance and capacity at a low cost. However, the use of multiple levels of memory and complex cache management policies makes it very difficult to optimize the performance of applications running on hierarchical memories. As the number of compute cores per chip continues to rise faster than the total amount of available memory, applications will become increasingly starved for memory storage capacity and bandwidth, making the problem of performance optimization even more critical. We propose a new methodology for measuring and modeling the performance of hierarchical memories in terms of the application's utilization of the key memory resources: capacity of a given memory level and bandwidth between two levels. This is done by actively interfering with the application's use of these resources. The application's sensitivity to reduced resource availability is measured by observing the effect of interference on application performance. The resulting resource-oriented model of performance both greatly simplifies application performance analysis and makes it possible to predict an application's performance when running with various resource constraints. This is useful to predict performance for future memory-constrained architectures.","1530-2075","978-1-4799-3800-1978-1-4799-3799","10.1109/IPDPS.2014.105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6877329","","Memory management;Interference;Benchmark testing;Bandwidth;Hardware;Cache storage;Instruction sets","cache storage;software performance evaluation;storage management","memory resource consumption;active measurement;hierarchical memory;hardware design;high memory performance;complex cache management policies;multiple memory levels;memory storage capacity;memory storage bandwidth;performance optimization problem;memory resources;application sensitivity;reduced resource availability;interference effect;resource-oriented model;application performance analysis;application performance prediction;resource constraints;memory-constrained architectures","","8","24","","","","","","IEEE","IEEE Conferences"
"Collaborative Analytics with Genetic Programming for Workflow Recommendation","C. S. Chong; T. Zhang; K. K. Lee; G. G. Hung; Terence; B. Lee","NA; NA; NA; NA; NA; NA","2013 IEEE International Conference on Systems, Man, and Cybernetics","","2013","","","657","662","Formulation of appropriate data analytics workflows requires intricate knowledge and rich experiences of data analytics experts. This problem is further compounded by continuous advancement and improvement in analytical algorithms. In this paper, a generic non-domain specific solution for the creation of appropriate workflows targeted at supervised learning problems is proposed. Our adaptive workflow recommendation engine based on collaborative analytics matches analytics needs with relevant workflows in repository. It is capable of picking workflows with better performance as compared to randomly selected workflows. The recommendation engine is now augmented by a workflow optimizer that applies genetic programming to further improve the recommended workflows through iterative evolution, leading to better alternative workflows. This unique Collaborative Analytics Recommender System is tested on seven UCI benchmark datasets. It is shown that the final workflows produced by the system could closely approximate, in terms of accuracy, the best workflows that analytics experts could possibly design.","1062-922X","978-1-4799-0652","10.1109/SMC.2013.117","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6721870","Workflow recommendation;collaborative analytics;genetic programming","Accuracy;Training;Benchmark testing;Breast cancer;Collaboration;Engines;Classification algorithms","data analysis;genetic algorithms;groupware;learning (artificial intelligence);recommender systems;workflow management software","collaborative analytics recommender system;genetic programming;adaptive workflow recommendation engine;data analytics workflow formulation;data analytics expert;analytical algorithm;generic nondomain specific solution;supervised learning problems;workflow optimizer;UCI benchmark datasets","","2","20","","","","","","IEEE","IEEE Conferences"
"Effective resource and workload management in data centers","L. Lu; E. Smirni","VMware Inc., Palo Alto, CA, USA; College of William and Mary, Williamsburg, VA, USA","2014 IEEE Network Operations and Management Symposium (NOMS)","","2014","","","1","7","The increasing demand for storage, computation, and business continuity has driven the growth of large data centers. Managing data centers efficiently is a difficult task because of the wide variety of datacenter applications, their ever-changing intensities, and the fact that application performance targets may differ widely. Server virtualization has been a game-changing technology for IT, providing the possibility to support multiple virtual machines (VMs) simultaneously. This dissertation focused on how virtualization technologies can be utilized to develop new tools for maintaining high resource utilization, for achieving high application performance, and for reducing the cost of data center management. This dissertation first focused on application workload management to improve web service performance especially under bursty conditions. Secondly, it concentrated on a resource measurement problem which serves as the basis of many autonomic computing solutions such as system optimization, adaptation, and troubleshooting. Thirdly, it presented AppRM, a resource allocation system that autonomically adapts to dynamic workload changes in a shared virtualized infrastructure to achieve application service level objectives (SLOs). Last, this dissertation presented PREMATCH, a tool that best co-locate different virtual machines (VMs) such that the performance of co-located VMs is maximized. All works have been implemented and tested over real enterprise applications and were accepted for publication at leading autonomic management conferences.","1542-1201;2374-9709","978-1-4799-0913","10.1109/NOMS.2014.6838287","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6838287","","Lead;Virtualization;Monitoring;Benchmark testing;Engines","business continuity;business data processing;computer centres;cost reduction;resource allocation;software fault tolerance;virtual machines;virtualisation;Web services","data center management;effective resource management;business continuity;server virtualization;game-changing technology;IT;multiple virtual machines;VMs;high resource utilization;cost reduction;application workload management;Web service performance;resource measurement problem;autonomic computing solutions;system optimization;AppRM;resource allocation system;shared virtualized infrastructure;application service level objectives;SLOs;PREMATCH tool;real enterprise applications;autonomic management conferences","","2","33","","","","","","IEEE","IEEE Conferences"
"Minecraft computer game performance analysis and network traffic emulation by a custom bot","T. Alstad; J. R. Duncan; S. Detlor; B. French; H. Caswell; Z. Ouimet; Y. Khmelevsky; G. Hains; R. Bartlett; A. Needham","COSC Dept., OC, Kelowna, BC V1Y 4X8, Canada; COSC Dept., OC, Kelowna, BC V1Y 4X8, Canada; COSC Dept., OC, Kelowna, BC V1Y 4X8, Canada; COSC Dept., OC, Kelowna, BC V1Y 4X8, Canada; COSC Dept., OC, Kelowna, BC V1Y 4X8, Canada; COSC Dept., OC, Kelowna, BC V1Y 4X8, Canada; COSC Dept., OC, Kelowna, BC V1Y 4X8, Canada; Laboratoire d'Algorithmique, Complexité et Logique (LACL) Univ. Paris-Est Créteil, Paris, France; WTFast, Kelowna, BC; WTFast, Kelowna, BC","2015 Science and Information Conference (SAI)","","2015","","","227","236","To simulate player traffic within the game we developed an automated bot for a popular online game Minecraft. The first emulation goal was for the bot to realistically replicate network traffic that a normal player would use while playing the game. The second emulation goal was to investigate the maximum possible workload on a virtual multicores and multi-CPUs CentOS server by running different number of active Minecraft games on many cores of the multi-CPU servers simultaneously. We created a scriptable bot capable of performing many common game actions, while generating comparable traffic to that of a player. This facilitates network and game server world optimization. It is allowed us to create a new testing and emulation environment to investigation network and server performance in our virtual Gaming Private Network (GPN) infrastructure.","","978-1-4799-8547-0978-1-4799-8546","10.1109/SAI.2015.7237149","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237149","","Games;Servers;Testing;Performance analysis;Emulation;Computer architecture;Unified modeling language","computer games;computer networks;software agents;virtual private networks","Minecraft computer game performance analysis;network traffic emulation;custom bot;player traffic;automated bot;online game;virtual multicore;multiCPU CentOS server;active Minecraft games;multiCPU servers;scriptable bot;game actions;game server world optimization;emulation environment;network performance;virtual gaming private network infrastructure;virtual GPN infrastructure","","2","25","","","","","","IEEE","IEEE Conferences"
"Improved Ant Colony Algorithm for Finding the Maximum Clique in Social Network","S. Zhang; Y. Dong; J. Yin; J. Guo","NA; NA; NA; NA","2015 IEEE 2nd International Conference on Cyber Security and Cloud Computing","","2015","","","433","438","The Maximum Clique is the most compact cohesive subgroup in Social Network. Finding the maximum clique in the Social Network has become an important aspect of social network analysis, such as privacy protection, citation and co-citation analysis, cohesive subgroup analysis et al. With the development of big data, the mass of nodes in the graph and complexity of analysis set a higher requirement for solving the maximum clique problem (MCP). Therefore, we propose an improved ant colony algorithm. Particularly, the strategy of the ant to select the nodes is improved so that the search space can be expanded and the variety of the solution is increased, with this approach local optimal solution can be avoided. Local improvement of the clique is also adopted to improve the accuracy and convergence speed of the proposed algorithm. The proposed algorithm has been tested on the DIMACS benchmark dataset and several typical social networks. Experimental results show the effectiveness and feasibility of the proposed algorithm.","","978-1-4673-9300-3978-1-4673-9299","10.1109/CSCloud.2015.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7371518","the maximum clique;social network;ant colony algorithm;privacy preserve;local improvement","Social network services;Convergence;Wheels;Algorithm design and analysis;Heuristic algorithms;Benchmark testing;Software algorithms","ant colony optimisation;information analysis;social networking (online)","improved ant colony algorithm;maximum clique problem;MCP;social network analysis;Big Data;DIMACS benchmark dataset","","","10","","","","","","IEEE","IEEE Conferences"
"Vulnerability assessments: a case study of Jordanian universities","M. Akour; I. Alsmadi","Computer Information Systems Department, Yarmouk University, Jordan; Department of Computer Science, Boise State University, USA","2015 International Conference on Open Source Software Computing (OSSCOM)","","2015","","","1","7","Websites of universities are considered the most important gateways to those Universities. They are heavily used by faculty members, employees, past, current and future students. They have a significant impact on University popularity and ranking. From a security perspective, those websites can be targets for a large number of possible security attacks such as: Flooding, denial of service (DoS), web defacement, etc. Attacks can be also from outsiders as well as insiders. In this paper, we conducted a vulnerability assessment on Websites of universities in Jordan. To ensure that our tests are constructive, we only employed passive penetration testing methods. Results showed that a significant number of those evaluated universities have critical or sever level vulnerabilities. Such vulnerabilities can be relatively easily be exploited by security attacks or attackers.","","978-1-4673-7465-1978-1-4673-7464","10.1109/OSSCOM.2015.7372688","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372688","vulnerability;universities;risk","Operating systems;Organizations;Buffer overflows;Open source software;Servers;Computer crime","educational institutions;security of data","vulnerability assessments;Jordanian universities;security attacks;Web sites","","","5","","","","","","IEEE","IEEE Conferences"
"A novel algorithm for voltage stability augmentation through optimal placement and sizing of SVC","P. Balachennaiah; P. H. Reddy; U. N. K. Raju","Annamacharya Institute of Technology and sciences, Rajampet, Kadapa (DT), A.P (ST), India; Annamacharya Institute of Technology and sciences, Rajampet, Kadapa (DT), A.P (ST), India; Annamacharya Institute of Technology and sciences, Rajampet, Kadapa (DT), A.P (ST), India","2015 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems (SPICES)","","2015","","","1","5","In the power system an unexpected increment of load demand has leads to the system to experience stressed condition. This phenomenon has led to voltage profile detract below the permissible limit. These Voltage profile in a stressed power system could be enhanced by having commensurate reactive power compensation. And this reactive power requirement is furnish through fast acting and self-commutated FACTS devices. This work presents an application of Artificial Bee Colony (ABC) in optimizing the rating of Static VAR compensator (SVC) for voltage stability augmentation in power system. The reactive power provided by SVC is randomly distributed and it is the control variable in ABC algorithm and produces near optimal solutions. The proposed approach has been evaluated with three different objective functions namely, loss minimization, voltage profile and stability enhancement. The IEEE 14-Bus test system is used as test systems to demonstrate the applicability and efficiency of the proposed method. All the above analysis is carried out in MATLAB software environment. Results for the proposed method are presented and interpreted.","","978-1-4799-1823-2978-1-4799-1821","10.1109/SPICES.2015.7091477","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7091477","SVC;L-Index;ABC algorithm;Voltage stability","Static VAr compensators;Power system stability;Reactive power;Stability criteria;Loading;Thermal stability","flexible AC transmission systems;losses;power system stability;reactive power;static VAr compensators;voltage control","flexible AC transmission systems;Matlab software environment;IEEE 14- bus test system;loss minimization;objective functions;ABC algorithm;control variable;static VAR compensator;artificial bee colony;fast acting self-commutated FACTS devices;reactive power compensation;voltage profile;load demand;power system stress condition;SVC;optimal sizing;optimal placement;voltage stability augmentation enhancement","","2","12","","","","","","IEEE","IEEE Conferences"
"Optimal reactive power dispatch by furnishing UPFC using multi-objective hybrid GAPSO approach for transmission loss minimisation and voltage stability","S. S. Shrawane; M. Diagavane; N. Bawane","Department of Electical Engg., GHRCE, Nagpur, India; VIT, Nagpur, India; SBJIT, Nagpur,India","2015 International Conference on Nascent Technologies in the Engineering Field (ICNTE)","","2015","","","1","6","The goal of optimal reactive power dispatch is to make the transmission loss minimal in addition to control of voltage profile such that the voltage deviations at the load buses for various loading conditions. By furnishing Flexible AC Transmission System devices in the transmission system, the power control can be finely achieved. The paper describes Optimal Reactive Power Dispatch for the power loss minimisation using optimal location of Unified Power Flow Controller for loss minimisation. The power flow is first solved by Newton Raphson method without implementing or furnishing Unified Power Flow Controller and then by furnishing them randomly in the IEEE -30 bus system. Unified Power Flow Controller allows control of real and reactive power both in addition to voltage magnitude control at various buses. In this paper, optimal reactive power dispatch is applied using the hybrid GA -PSO. Simulations are performed on IEEE-30 bus test system using MATLAB software package to ensure efficacy of the proposed algorithm. This paper aims at to find the optimum usage of Unified Power Flow Controller which means the finding of the optimal location where their influence would be more useful.","","978-1-4799-7263-0978-1-4799-7261","10.1109/ICNTE.2015.7029923","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029923","Unified Power flow controller UPFC;Genetic algorithm GA;Particle swarm optimisation PSO;Flexible AC transmission devices FACTs;Optimal Reactive Power Dispatch ORPD","Propagation losses;Reactive power;Power transmission lines;Load flow;Optimization;Power system stability;Stability analysis","flexible AC transmission systems;load dispatching;load flow control;Newton-Raphson method;optimal control;reactive power control;voltage control","optimal reactive power dispatch;UPFC;multiobjective hybrid GAPSO approach;transmission loss minimisation;voltage stability;flexible AC transmission system devices;power control;unified power flow controller;Newton Raphson method;IEEE -30 bus system;MATLAB software package","","2","13","","","","","","IEEE","IEEE Conferences"
"Utilizing confidence bounds in Failure Mode Effects Analysis (FMEA) Hazard Risk Assessment","M. Banghart; K. Fuller","Mississippi State University, Starkville, USA; University of North Florida, Jacksonville, USA","2014 IEEE Aerospace Conference","","2014","","","1","6","The objective of this contribution is to provide a review and suggest possible extensions of the Failure Mode Effects Analysis (FMEA), Hazard Risk Assessment (HRA) [2] and to demonstrate the importance of these tools to general probabilistic design for reliability (PDfR) [8]. FMEA was first introduced in the 1960s by the U.S. National Aeronautics and Space Administration (NASA) and is currently used extensively across many industries. FMEA is useful in understanding the failure modes of various products, qualifying the effects of failure and aiding in the development of mitigation strategies. It is a useful tool in improving quality, reliability, and the maintainability of designs, and is a critical component in risk management strategies and evaluations. This is, actually, the approach of the prognostics and health monitoring/management (PHM) engineering. Failure mode effects and criticality analysis (FMECA) [1] is an extension of (FMEA). While FMEA is a bottom-up, inductive analytical method which may be performed at either the functional or piece-part level, FMECA extends FMEA by including a criticality analysis that is aimed, like PDfR is, at charting the probability of failure modes against the severity of their consequences. The result highlights failure modes with relatively high probability and severity of consequences, allowing remedial effort to be directed where it will produce the greatest value. FMECA tends to be preferred over FMEA in space and North Atlantic Treaty Organization (NATO) military applications, while various forms of FMEA predominate in other industries. Being extensions of the FMEAs, FMECAs add severity and probability ranking aspects to the problems of interest. This is accomplished through an appropriate HRA - an engineering process of where the risk of an event is quantified by examining the chain of the preceding events, starting with, e.g., the failure mode, then stepping through to the end effects. The approach allows quantification of risk through the use of probabilistic risk analysis (PRA) and is addressed and discussed in detail. Failure oriented accelerated testing (FOAT) [9] could and should be viewed as an important constituent part of the effort. It is shown that care must be taken to establish the appropriate probabilities, to identify the statistical independence of the random variables of importance, as well as to assess the trustworthiness of the available or obtained data. It is indicated that an important drawback of the FMEA is the lack of pure operational (field) failure data. These data are frequently utilized from the computerized maintenance management system (CMMS) software, which does not always provide a true snapshot of the Mean Time Between Failures (MTBF) or other critical characteristics of the product. This results in the situation that personal judgment plays a large part in the development of the FMEA. Several papers have been published recently on development of Fuzzy FMEA methodologies (see, e.g., [7]). This application of fuzzy logic to Hazard Risk Analysis will allow additional uncertainty and inaccuracy to be modeled throughout FMECA development, leading to a more robust decision making with consideration of various uncertainties.","1095-323X","978-1-4799-1622-1978-1-4799-5582","10.1109/AERO.2014.6836222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6836222","","Lead;NASA;Reliability;Frequency modulation;Monitoring;Safety","aircraft maintenance;condition monitoring;decision making;failure analysis;fuzzy logic;mechanical engineering computing;probability;random processes;reliability;risk analysis;statistical analysis;uncertainty handling","confidence bounds;failure mode effects analysis;hazard risk assessment;probabilistic design for reliability;PDfR;mitigation strategy;prognostics and health monitoring;PHM;failure mode effects and criticality analysis;inductive analytical method;North Atlantic Treaty Organization;NATO military applications;probability ranking aspects;HRA;end effects;probabilistic risk analysis;PRA;failure oriented accelerated testing;FOAT;statistical analysis;random variables;trustworthiness assess;computerized maintenance management system;CMMS software;mean time between failure;MTBF;fuzzy FMEA methodology;fuzzy logic;hazard risk analysis;FMECA development;decision making;uncertainty handling","","1","10","","","","","","IEEE","IEEE Conferences"
"Network-Constrained Day-Ahead Auction for Consumer Payment Minimization","R. Fernández-Blanco; J. M. Arroyo; N. Alguacil","Departamento de Ingenierı´a Eléctrica, Electrónica, Automática y Comunicaciones, E.T.S.I. Industriales, Universidad de Castilla-La Mancha, Ciudad Real, Spain; Departamento de Ingenierı´a Eléctrica, Electrónica, Automática y Comunicaciones, E.T.S.I. Industriales, Universidad de Castilla-La Mancha, Ciudad Real, Spain; Departamento de Ingenierı´a Eléctrica, Electrónica, Automática y Comunicaciones, E.T.S.I. Industriales, Universidad de Castilla-La Mancha, Ciudad Real, Spain","IEEE Transactions on Power Systems","","2014","29","2","526","536","This paper presents an alternative day-ahead auction based on consumer payment minimization for pool-based electricity markets. This auction is an instance of price-based market clearing wherein market-clearing prices are explicitly modeled as decision variables of the optimization. The auction design includes network constraints, inter-temporal constraints associated with generation scheduling, and marginal pricing. Hence, consumer payment is expressed in terms of locational marginal prices. The proposed solution approach is based on bilevel programming. In the upper-level optimization, generation is scheduled with the goal of minimizing the total consumer payment while taking into account that locational marginal prices are determined by a multiperiod optimal power flow in the lower level. In this bilevel programming setting, locational marginal prices are the Lagrange multipliers or dual variables associated with the nodal power balance equations of the lower-level problem. The resulting mixed-integer linear bilevel program is transformed into an equivalent single-level mixed-integer linear program suitable for efficient off-the-shelf software. This transformation relies on the application of results from duality theory of linear programming and integer algebra. The proposed methodology has been successfully applied to several test systems including the IEEE 118-bus system. Numerical results have been compared with those obtained from declared social welfare maximization.","0885-8950;1558-0679","","10.1109/TPWRS.2013.2284283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654344","Bilevel programming;consumer payment minimization;locational marginal pricing;transmission network","Minimization;Pricing;Optimization;Indexes;Mathematical model;Electricity supply industry;Linear programming","consumer electronics;power markets;pricing","network-constrained day-ahead auction;consumer payment minimization;pool-based electricity markets;price-based market clearing;market-clearing prices;optimization;network constraints;inter-temporal constraints;generation scheduling;Lagrange multipliers;off-the-shelf software;social welfare maximization","","12","48","","","","","","IEEE","IEEE Journals & Magazines"
"A FML-based fuzzy tuning for a memetic ontology alignment system","G. Acampora; U. Kaymak; V. Loia; A. Vitiello","School of Industrial Engineering, Information Systems, Eindhoven University of Technology, P.O. Box 513, 5600 MB, The Netherlands; School of Industrial Engineering, Information Systems, Eindhoven University of Technology, P.O. Box 513, 5600 MB, The Netherlands; Department of Computer Science, University of Salerno, Fisciano, 84084, Italy; Department of Computer Science, University of Salerno, Fisciano, 84084, Italy","2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)","","2013","","","1","8","Ontology alignment systems are software tools aimed at producing a set of correspondences, called alignment, between two heterogeneous ontologies in order to bring them in a mutual agreement. Performing this task is an essential step to allow the exchange of information between people, organizations and web applications using ontologies for representing their view of the world. Currently, in spite of several ontology alignment systems have been developed, there is no a robust solution that seems capable of producing alignments with the same high quality on different alignment task instances. Mainly, this weakness of ontology alignment systems is due to the dependence of their behavior on a set of specific instance parameters. This work proposes to improve performance of a well-known memetic algorithm based ontology alignment system by adaptively regulating its specific instance parameters through a FML-based fuzzy tuning. The validity of our proposal is shown by aligning ontologies belonging to two well-known OAEI datasets and by performing a Wilcoxon's signed rank test which highlights that our proposal statistically outperforms its not fuzzy adaptive counterpart.","1098-7584","978-1-4799-0022-0978-1-4799-0020","10.1109/FUZZ-IEEE.2013.6622490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622490","Ontology Alignment;Memetic Algorithms;Fuzzy Markup Language","Ontologies;Biological cells;Memetics;Fuzzy logic;Tuning;Proposals;Weight measurement","fuzzy set theory;ontologies (artificial intelligence);XML","FML-based fuzzy tuning;memetic ontology alignment system;software tools;heterogeneous ontology;robust solution;alignment task instances;ontology alignment systems;specific instance parameters;memetic algorithm based ontology alignment system;aligning ontology;OAEI datasets;Wilcoxon signed rank test;fuzzy adaptive","","","23","","","","","","IEEE","IEEE Conferences"
"An OpenGL ES 2.0 3D graphics SoC with versatile HW/SW development support","H. Dow; C. Huang; C. Lai; K. Tsao; S. Tseng; K. Wu; T. Wu; H. Yang; D. Z. Jain; Y. Chang; S. W. Haga; S. Hsiao; I. Huang; S. Kuang; C. Lee","Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.; Department of Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, R.O.C.","VLSI Design, Automation and Test(VLSI-DAT)","","2015","","","1","4","A multi-threaded programmable shader pipeline 3D graphics SoC with support for OpenGL ES 2.0 has been developed and fabricated. The sample chip is ARMv4T compatible with the 3D processing capability of 14.9 Mvertices/s, 3.6 Mpixels/s and up to 4K resolution. The die size is 3.85×3.85 mm<sup>2</sup>, with 2.96M gates on a TSMC 90nm CMOS 1P9M. This new SoC includes software to support OpenGL ES API libraries, GLSL compilation and simulation. The SoC also comes with various development tools, including GPU simulators for hardware validation, profile assisted compiler optimization and compiler verification. For developers, we also present a QEMU-based simulation platform and SoC Performance Monitoring Tool Suite (PMTS) to assist developers in optimizing the system and detecting performance bottlenecks.","","978-1-4799-6275","10.1109/VLSI-DAT.2015.7114496","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7114496","3D Computer Graphics;Multi-Core;GPU;System-on-Chip;OpenGL ES;Shading Language Compiler;GPU Simulator","Graphics processing units;System-on-chip;Hardware;Three-dimensional displays;Monitoring;Protocols","CMOS integrated circuits;computer graphics;microcontrollers;multi-threading;system-on-chip","OpenGL ES 2.0 3D graphics SoC;versatile HW-SW development support;multithreaded programmable shader pipeline 3D graphics SoC;ARMv4T;TSMC CMOS 1P9M;OpenGL ES API library;GLSL compilation;GPU simulators;hardware validation;profile assisted compiler optimization;compiler verification;QEMU-based simulation platform;SoC performance monitoring tool suite;PMTS;picture size 3.6 Mpixel;size 90 nm","","1","10","","","","","","IEEE","IEEE Conferences"
"The study on sensibility of self-elevating unit legs strength to different bay heights under storm environment condition","Y. Zhu; B. Jiang; H. Qin; C. Sun; Y. Fan","College of Shipbuilding Engineering Harbin Engineering University Harbin, China; College of Shipbuilding Engineering Harbin Engineering University Harbin, China; College of Shipbuilding Engineering Harbin Engineering University Harbin, China; Ocean Engineering Institute Branch, Bo-Hai Petroleum Equipment Manufacturing Co., Ltd. Panjing, China; Ocean Engineering Institute Branch, Bo-Hai Petroleum Equipment Manufacturing Co., Ltd. Panjing, China","2015 International Conference on Fluid Power and Mechatronics (FPM)","","2015","","","138","143","Legs are the main load-carrying structures of Self-elevating Unit. To optimize leg structure can improve unit environmental bearing capacity. Aiming at studying on the truss legs optimum design, the paper analyzes the sensibility of leg strength to different bay height under storm condition. It obtains wind loads from wind Tunnel Test. Based on STOKES 5 orders wave theory and Morison equation, the wave and current loads are obtained by WAJAC, calculation software of SESAM developed by DNV. It obtains the inertial load considering the DAF factor through the natural vibration period and It obtains the inertial moment considering the P-A effect through one order offset value, the results are all calculated by eigenvalue analysis. This paper emphasizes on checking structure strength by the above calculation results and gives the most optimal value of bay height for legs. It also makes up the sensitivity analysis technique of bay heights, and it can provide reference for the self design for Key parts of self-elevating unit.","","978-1-4799-8770-2978-1-4799-8769","10.1109/FPM.2015.7337100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7337100","Self-elevating Unit;environment loads;leg strength;bay height;UC value","Legged locomotion;Storms;Finite element analysis;Software;Computer aided software engineering;Mathematical model;Force","design engineering;eigenvalues and eigenfunctions;offshore installations;vibrations;wind tunnels","self-elevating unit;storm environment condition;load-carrying structure;unit environmental bearing capacity;truss legs optimum design;wind load;wind tunnel testing;STOKES 5 orders wave theory;Morison equation;wave load;current load;WAJAC;SESAM;inertial load;natural vibration period;eigenvalue analysis;sensitivity analysis","","","18","","","","","","IEEE","IEEE Conferences"
"A modular AC optimal power flow implementation for distribution grid planning","A. Hauswirth; T. Summers; J. Warrington; J. Lygeros; A. Kettner; A. Brenzikofer","Automatic Control Laboratory, Swiss Federal Institute of Technology (ETH), Zurich, Switzerland; Automatic Control Laboratory, Swiss Federal Institute of Technology (ETH), Zurich, Switzerland; Automatic Control Laboratory, Swiss Federal Institute of Technology (ETH), Zurich, Switzerland; Automatic Control Laboratory, Swiss Federal Institute of Technology (ETH), Zurich, Switzerland; Supercomputing Systems AG, Zurich, Switzerland; Supercomputing Systems AG, Zurich, Switzerland","2015 IEEE Eindhoven PowerTech","","2015","","","1","6","We present a computational tool for solving semidefinite relaxations of multi-period AC optimal power flow (OPF) problems. Chordal conversion techniques are used to exploit problem sparsity. Three features set it apart from similar implementations: First, a new, concise real-valued model exploits the problem structure and avoids introducing redundant constraints. Second, a dynamic choice of constraint type improves computation time for grids with extensive radial subgraphs. Third, a modular software design enables the easy integration of additional models for photovoltaic inverters, optimal storage placement, etc. Benchmark results indicate that our computational improvements significantly enhance performance compared to a standard implementation. This holds in particular for large-scale networks and power grids with large radial subgraphs. Finally, a case study showcases the potential of our modular OPF software design.","","978-1-4799-7693-5978-1-4799-7692","10.1109/PTC.2015.7232675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7232675","Convex Optimization;Optimal Power Flow;Smart Grids","Benchmark testing;Smart grids;Government;Lead","load flow;mathematical programming;power distribution planning","modular AC optimal power flow;distribution grid planning;semidefinite relaxations;chordal conversion techniques;extensive radial subgraphs;modular software design;photovoltaic inverters;optimal storage placement;power grids","","1","11","","","","","","IEEE","IEEE Conferences"
"Kvazaar HEVC encoder for efficient intra coding","M. Viitanen; A. Koivula; A. Lemmetti; J. Vanne; T. D. Hämäläinen","Department of Pervasive Computing, Tampere University of Technology, Finland; Department of Pervasive Computing, Tampere University of Technology, Finland; Department of Pervasive Computing, Tampere University of Technology, Finland; Department of Pervasive Computing, Tampere University of Technology, Finland; Department of Pervasive Computing, Tampere University of Technology, Finland","2015 IEEE International Symposium on Circuits and Systems (ISCAS)","","2015","","","1662","1665","This paper presents an open-source Kvazaar encoder for HEVC intra coding. This academic software encoder has been developed from the scratch using C as an implementation language by prioritizing modularity, portability, and readability of the source code. Kvazaar implements almost the same intra coding functionality as HEVC reference encoder (HM) but its rewritten source code makes it significantly faster. In all-intra (AI) coding, a single-threaded C implementation of Kvazaar is 2.3 times faster than HM at a cost of 1.7% bit rate increase. The respective values with a high speed preset of Kvazaar are 10.6 and 8.8%. Compared to a single-threaded C++ implementation of x265, Kvazaar improves rate-distortion performance and increases encoding speed in both high-quality and high-speed test cases. Kvazaar has a particular edge in the high-speed test case where it almost halves the BD-rate loss and more than doubles the performance.","0271-4302;2158-1525","978-1-4799-8391","10.1109/ISCAS.2015.7168970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168970","HEVC;Kvazaar HEVC encoder;intra coding;open-source implementation;rate-distortion-complexity","Encoding;Video coding;Open source software;Transforms;Standards;IP networks;Artificial intelligence","computational complexity;rate distortion theory;source code (software);video coding","Kvazaar HEVC encoder;open-source Kvazaar encoder;HEVC intra coding;single-threaded C implementation;rate-distortion-complexity;high efficiency video coding","","18","18","","","","","","IEEE","IEEE Conferences"
"Optimal Conductor Size Selection and Reconductoring in Radial Distribution Systems Using a Mixed-Integer LP Approach","J. F. Franco; M. J. Rider; M. Lavorato; R. Romero","Departamento de Engenharia Elétrica, Faculdade de Engenharia de Ilha Solteira, UNESP—Universidade Estadual Paulista, Ilha Solteira, Brazil; Departamento de Engenharia Elétrica, Faculdade de Engenharia de Ilha Solteira, UNESP—Universidade Estadual Paulista, Ilha Solteira, Brazil; Departamento de Engenharia Elétrica, Faculdade de Engenharia de Ilha Solteira, UNESP—Universidade Estadual Paulista, Ilha Solteira, Brazil; Departamento de Engenharia Elétrica, Faculdade de Engenharia de Ilha Solteira, UNESP—Universidade Estadual Paulista, Ilha Solteira, Brazil","IEEE Transactions on Power Systems","","2013","28","1","10","20","This paper presents a mixed-integer linear programming model to solve the conductor size selection and reconductoring problem in radial distribution systems. In the proposed model, the steady-state operation of the radial distribution system is modeled through linear expressions. The use of a mixed-integer linear model guarantees convergence to optimality using existing optimization software. The proposed model and a heuristic are used to obtain the Pareto front of the conductor size selection and reconductoring problem considering two different objective functions. The results of one test system and two real distribution systems are presented in order to show the accuracy as well as the efficiency of the proposed solution technique.","0885-8950;1558-0679","","10.1109/TPWRS.2012.2201263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6221971","Distribution system optimization;mixed-integer linear programming;optimal conductor size selection","Conductors;Mathematical model;Cascading style sheets;Load modeling;Steady-state;Equations;Reactive power","conductors (electric);distribution networks;heuristic programming;linear programming","optimal conductor size selection;radial distribution system;mixed-integer LP approach;mixed-integer linear programming model;steady-state operation;optimization software;heuristic model;reconductoring problem","","26","26","","","","","","IEEE","IEEE Journals & Magazines"
"Detection of soft errors through checksums in redundant execution systems","L. Bustamante; H. Al-Asaad","Department of Electrical and Computer Engineering, University of California, Davis, U.S.A.; Department of Electrical and Computer Engineering, University of California, Davis, U.S.A.","2015 IEEE AUTOTESTCON","","2015","","","134","137","As technology trends keep pushing transistor dimensions to smaller geometries and higher densities, the gains in reliability over the past decades have started to reverse trends due to the gradual increase in soft error rates. Consumer applications that cannot afford hardware redundancy solutions to reduce the effects of soft errors, can find a cost effective alternative using time redundancy. The memory requirements needed for a redundant execution system can be minimized without compromising soft error detection by using hardware execution signatures. The proposed technique calculates checksum signatures that summarize the past and present state of execution of a microprocessor. In this system, the soft error detection logic generates partial checksum signatures during the first execution of a microprocessor and compares them to the signatures generated during the second execution. Soft errors are detected when there is a signature mismatch between the two program executions. To minimize time overhead, the microprocessor was optimized to automatically run redundant executions without the need of duplicating software. The system based on redundant execution using state execution signatures achieved high soft error detection while simultaneously maintaining reduced memory requirements.","","978-1-4799-8190-8978-1-4799-8189","10.1109/AUTEST.2015.7356479","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7356479","On-line testing;soft errors;digital signatures;checksums;double execution;error detection;built-in self-test","Microprocessors;Hardware;Redundancy;Memory management;Clocks;Transistors","error detection;integrated circuit reliability;microprocessor chips;radiation hardening (electronics);redundancy","memory requirement reduction;partial checksum signatures;soft error detection logic;hardware execution signature;time redundancy;redundant execution systems","","","9","","","","","","IEEE","IEEE Conferences"
"Detecting stealthy false data injection using machine learning in smart grid","M. Esmalifalak; Nam Tuan Nguyen; R. Zheng; Z. Han","ECE Department, University of Houston, TX 77004, USA; ECE Department, University of Houston, TX 77004, USA; Department of Computing and Software, McMaster University, Ontario, Canada; ECE Department, University of Houston, TX 77004, USA","2013 IEEE Global Communications Conference (GLOBECOM)","","2013","","","808","813","Aging power industries together with increase in the demand from industrial and residential customers are the main incentive for policy makers to define a road map to the next generation power system called smart grid. In smart grid, the overall monitoring costs will be decreased but at the same time, the risk of cyber attacks might be increased. Recently a new type of attacks (called the stealth attack) has been introduced, which cannot be detected by the bad data detection using state estimation. In this paper, we show how normal operations of power networks can be statistically distinguished from the case under stealthy attacks. We devise two machine learning based techniques for stealthy attack detection. The first method utilizes supervised learning over labeled data and trains a support vector machine. The second method requires no training data and detects the deviation in measurement In both methods, principle component analysis is used to reduce the dimensionality of the data to be processed, and thus leads to lower computation complexities. The results of the proposed detection methods on the IEEE standard test systems demonstrate effectiveness of both schemes.","1930-529X","978-1-4799-1353","10.1109/GLOCOM.2013.6831172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6831172","","Measurement uncertainty;Optimization;Zirconium;Training","distribution networks;learning (artificial intelligence);power engineering computing;principal component analysis;security of data;smart power grids;support vector machines;transmission networks","smart grid;machine learning;stealthy false data injection detection;power industries;industrial customers;residential customers;policy makers;next generation power system;overall monitoring costs;cyber attacks;stealth attack;bad data detection;state estimation;power networks;supervised learning;support vector machine;principle component analysis;IEEE standard test systems","","15","20","","","","","","IEEE","IEEE Conferences"
"A Novel Cloud based and Cyclic Approach for Supervised Learning","D. Bujji Babu; R. Siva Rama Prasad","Dept. of Computer Science and Engineering, Acharya Nagarjuna University, Guntur, A.P, India; Dept. of I. B Studies, Acharya Nagarjuna University, Guntur, A.P., India","2014 International Conference on Reliability Optimization and Information Technology (ICROIT)","","2014","","","374","379","The Clouds provide more services to the users. In the past era the software users have to purchase the software with licence. But now a days clouds provide these softwares on pay and use basis to use it. This pay and use service brought more comfort to the users. Through this paper, we propose `A Novel Cloud based and Cyclic Approach for Supervised Learning', in the area of Data Mining. Because the data warehouses are utilizing the virtualized Iaas cloud service. i.e., the data warehouses are stored in the clouds and utilizing services of cloud. We concentrated more on classification problem in the area of data mining, because the global business scenario is entirely changed. According to these changes the need of classification become essential in all areas. Particularly to the data, which is stored in the virtual data warehouses. In the cloud hosted data warehouses the current test data set will become as training data set after some period of time. In our proposed approach we introduced the post-mortem technique on the classification model to know the facts, how good the model is induced to classify the data set from the training data set. To provoke a high-quality and an efficient classification model, the model must go through the post-mortem operation to know the reality of the classification. The test data set must go through the pre-processing operation to make the data set pure and clean. This process must be done in routine.","","978-1-4799-2995-5978-1-4799-3958-9978-1-4799-2996","10.1109/ICROIT.2014.6798357","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798357","Data warehouse;Cloud;Data mining;Supervised Learning;Classification;Pre-processing;Post-mortem","Computer architecture;Educational institutions;Manganese;Computational modeling","cloud computing;data mining;data warehouses;learning (artificial intelligence);pattern classification","cyclic approach;cloud based approach;supervised learning;data mining;virtualized Iaas cloud service;pattern classification problem;global business scenario;virtual data warehouses;cloud hosted data warehouses;post-mortem technique;training data set","","","29","","","","","","IEEE","IEEE Conferences"
"Optimized Use of Parallel Programming Interfaces in Multithreaded Embedded Architectures","A. F. Lorenzon; A. L. Sartor; M. C. Cera; A. C. S. Beck","NA; NA; NA; NA","2015 IEEE Computer Society Annual Symposium on VLSI","","2015","","","410","415","Thread-level parallelism (TLP) exploitation for embedded systems has been a challenge for software developers: while it is necessary to take advantage of the availability of multiple cores, it is also mandatory to consume less energy. To speed up the development process and make it as transparent as possible, software designers use parallel programming interfaces (PPIs). However, as will be shown in this paper, each one implements different ways to exchange data, influencing performance, energy consumption and energy-delay product (EDP), which varies across different embedded processors. By evaluating four PPIs and three multicore processors, we demonstrate that it is possible to save up to 62% in energy consumption and achieve up to 88% of EDP improvements by just switching the PPI, and that the efficiency (i.e., The best possible use of the available resources) decreases as the number of threads increases in almost all cases, but at distinct rates.","2159-3469;2159-3477","978-1-4799-8719-1978-1-4799-8718","10.1109/ISVLSI.2015.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7309602","Parallel programming interfaces;Performance and energy efficiency evaluation;Multithreaded embedded architectures","Energy consumption;Message systems;Memory management;Synchronization;Instruction sets;Benchmark testing","embedded systems;multiprocessing systems;multi-threading;parallel architectures;power aware computing","parallel programming interface;multithreaded embedded architecture;thread-level parallelism exploitation;TLP exploitation;embedded system;PPI;energy-delay product;EDP;embedded processor;multicore processor","","2","23","","","","","","IEEE","IEEE Conferences"
"Optimization of the perturb and observe maximum power point tracker for a distributed photovoltaic system","A. F. Murtaza; H. A. Sher; M. Chiaberge; D. Boero; M. De Giuseppe; K. E. Addoweesh","Faculty of Engineering, University of Central Punjab, Pakistan; Department of Electrical Engineering, King Saud University, Saudi Arabia; Department of Electronics and Telecommunication Engineering, Politecnico di Torino, Italy; Department of Mechanical and Aerospace Engineering, Politechnico Politecnico di Torino, Italy; Department of Electronics and Telecommunication Engineering, Politecnico di Torino, Italy; Department of Electrical Engineering, King Saud University, Saudi Arabia","INMIC","","2013","","","77","82","With everyday passing, power generation from photovoltaic (PV) systems are getting increased around the globe. One of the drawbacks of PV systems is that they exhibit non-linear I-V curves with respect to atmospheric conditions. Therefore, maximum power point tracker (MPPT) technique is installed in every PV system. One of the most popular technique is perturb &amp; observe (P&amp;O) but this technique has some shortcomings. In this paper, MPPT technique is proposed which will optimize the P&amp;O with respect to distributed PV (DPV) plant. Such that the deficiencies of P&amp;O are removed. The proposed technique is designed for DPV and is working in two modes. In mode-1, decision making of the proposed technique revolves around the mutual co-ordination between two modules/arrays. During this mode, one array will behave as leader (L) while other array as follower (F). In mode-2, proposed technique utilizes the individual P&amp;O for both arrays. Model is the superior performance mode in which efficiency of the proposed technique is better than P&amp;O. While, in mode-2 both techniques have same efficiency. Finally, the proposed technique is modeled in Matlab/Simulink. The proposed technique is tested under various weather conditions and superior performance of the proposed technique is proved through simulation results.","","978-1-4799-3043","10.1109/INMIC.2013.6731328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6731328","Solar Photovoltaic;Modeling & Simulation;Distributed MPPT;Perturb and Observe MPPT","Meteorology;Heuristic algorithms;Temperature sensors;Maximum power point trackers;Awards activities;Algorithm design and analysis;Software packages","decision making;distributed power generation;maximum power point trackers;photovoltaic power systems;solar cell arrays","perturb and observe maximum power point tracker;distributed photovoltaic system;power generation;PV systems;nonlinear I-V curves;atmospheric conditions;MPPT technique;P&O;distributed PV plant;decision making;modules-arrays;Matlab/Simulink","","1","11","","","","","","IEEE","IEEE Conferences"
"Computational savings based on Three-Dimensional automotive geometries' simplifications in electromagnetics simulations","A. N. de São José; A. C. P. M. Colin; J. F. Mologni; G. M. Dip; Ú. do Carmo Resende; S. T. M. Gonçalves","Electromagnetic Compatibility Section, FIAT Automóveis S/A, Betim, Brazil; Electromagnetic Compatibility Section, FIAT Automóveis S/A, Betim, Brazil; Electronic Design Automation Section, ESSS - Engineering Simulation and Scientific Software, São Paulo, Brazil; Electronic Design Automation Section, ESSS - Engineering Simulation and Scientific Software, São Paulo, Brazil; Electrical Engineering Department, Centro Federal de Educação, Tecnológica de Minas Gerais, Belo Horizonte, Brazil; Electrical Engineering Department, Centro Federal de Educação, Tecnológica de Minas Gerais, Belo Horizonte, Brazil","2013 SBMO/IEEE MTT-S International Microwave & Optoelectronics Conference (IMOC)","","2013","","","1","5","This paper explores how much detailed an automotive electric system geometry can be to optimize simulation computational effort. For this purpose, simulations related to two kinds of analysis - crosstalk and external radiated disturbance - have been run through a Finite Element Method (FEM) based software and a Circuit Theory based one. Crosstalk simulated signals in time domain have been statistically compared with an inside vehicle measurement. Different complexity models have been tested in this correlation process. This procedure has been used to verify automotive geometric models' accuracy for a certain frequency range. The main goal is to verify the computational savings by using some techniques and suppressing some negligible details of car's geometry.","","978-1-4799-1397","10.1109/IMOC.2013.6646423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6646423","Electromagnetic Compatibility;crosstalk;automotive harnesses;ISO 11451-2","Integrated circuit modeling;Crosstalk;Solid modeling;Atmospheric modeling;Computational modeling;Automotive engineering;Antennas","automotive electronics;circuit simulation;computational electromagnetics;crosstalk;finite element analysis","computational savings;three-dimensional automotive geometries simplifications;electromagnetics simulations;crosstalk;external radiated disturbance;finite element method;circuit theory;complexity models","","2","6","","","","","","IEEE","IEEE Conferences"
"WLAN oriented optimization of process bus in IEC 61850-based substation communication network","N. H. Ali; B. B. M. Ali; O. Basir; M. L. Othman; F. B. Hashim","Department of Computer and Communications Systems Engineering, Faculty of Engineering, University Putra Malaysia, 43400 UPM Serdang, Selangor, Malaysia; Department of Computer and Communications Systems Engineering, Faculty of Engineering, University Putra Malaysia, 43400 UPM Serdang, Selangor, Malaysia; Department of Computer and Communications Systems Engineering, Faculty of Engineering, University Putra Malaysia, 43400 UPM Serdang, Selangor, Malaysia; Department of Computer and Communications Systems Engineering, Faculty of Engineering, University Putra Malaysia, 43400 UPM Serdang, Selangor, Malaysia; Department of Computer and Communications Systems Engineering, Faculty of Engineering, University Putra Malaysia, 43400 UPM Serdang, Selangor, Malaysia","2015 3rd International Renewable and Sustainable Energy Conference (IRSEC)","","2015","","","1","6","The communication standard IEC 61850 which is defined for information exchange in smart grids, has been increasingly used for Substation Automation Systems (SAS). Current mainstream communication technologies such as switched Ethernet and TCP/IP have been adopted for the communications networking. However, with the growing amount of IEDs in one collision domain, a risk of violation of transmission time requirement increases. This raises a concern for optimization of IEC 61850 based substation communication network. The industrial wireless LAN technologies are gaining interest power utility sector, especially for less critical smart distribution network applications. This paper proposes methods of software simulation for performance evaluation of communications network in power substations. This method enables to test the performance of process bus only using Sampled Value (SV) and obtains a maximum number of WLAN IEDs in one collision domain subject to various conditions.","2380-7393","978-1-4673-7894-9978-1-4673-7893","10.1109/IRSEC.2015.7454979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7454979","IIEC-61850;smart grids;simulation;process bus;sampled value;substation communication","IEC Standards;Wireless LAN;Substations;Delays;Smart grids;Protocols","distribution networks;smart power grids;substation automation;wireless LAN","WLAN oriented optimization;process bus;IEC 61850-based substation communication network;smart grids;substation automation systems;SAS;switched Ethernet;TCP/IP;industrial wireless LAN technology;critical smart distribution network;power utility sector;power substations;communication network performance evaluation","","1","23","","","","","","IEEE","IEEE Conferences"
"Feedwater heater system fault diagnosis during dynamic transient process based on two-stage neural networks","L. Ma; X. Wang; X. Cao","School of Control and Computer Engineering, North China Electric Power University, Baoding 071003 China; School of Control and Computer Engineering, North China Electric Power University, Baoding 071003 China; School of Control and Computer Engineering, North China Electric Power University, Baoding 071003 China","Proceedings of the 32nd Chinese Control Conference","","2013","","","6148","6153","At present, researches on power plant fault diagnosis are mostly for steady-state work conditions and can not well adapt to the load-changing dynamic process, which greatly limits the practical application of a fault diagnosis system. Thus, a transient fault diagnosis approach based on two-stage neural networks was put forward for power plant thermal system fault diagnosis. An Elman recurrent neural network with time-delay inputs was applied to predict the expected normal values of the fault feature variables, and a BP neural network was used to identify the fault types. To improve the diagnostic effect for faults of varying severity under transient conditions, fault symptom zoom optimization technique was also used. Taking the high-pressure feedwater heater system of a 600MW supercritical power unit as the object investigated, the predictive model was built, trained and validated with large amount of historical operating data. The BP network fault diagnosis model was trained with the fault fuzzy knowledge library including typical fault samples. The real-time fault diagnosis program was then developed with MATLAB software. By communicating with the power plant simulator, intensive fault diagnosis tests were carried out. It was shown the suggested method can achieve good diagnosis results for the power plant thermal system under load-varying transient process.","1934-1768","978-9-8815-6383","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6640515","Thermal System;Dynamic Transient Conditions;Fault Diagnosis;Neural Network;Feature Variables Prediction;Feedwater Heaters","Fault diagnosis;Transient analysis;Neural networks;Power generation;Electronic mail;Process control;Heating","backpropagation;fault diagnosis;heating;power engineering computing;power plants;power system faults;recurrent neural nets","load-varying transient process;intensive fault diagnosis tests;power plant simulator;Matlab software;fault fuzzy knowledge library;high-pressure feedwater heater system;backpropagation;BP neural network;fault feature variables;time-delay inputs;Elman recurrent neural network;power plant thermal system fault diagnosis;transient fault diagnosis approach;load-changing dynamic process;steady-state work conditions;two-stage neural networks;dynamic transient process;power 600 MW","","","6","","","","","","IEEE","IEEE Conferences"
"A study on MEMS acoustic vibration sensor technology","Yonghe Qin; Yingjie Qiao; Wenjiang Zou; Xingyue Xu","Institute of materials science and chemical engineering, Harbin engineering university, China; Institute of materials science and chemical engineering, Harbin engineering university, China; The 49th research institute of China electronics technology group corporation, Harbin, China; The 49th research institute of China electronics technology group corporation, Harbin, China","2013 International Conference on Optoelectronics and Microelectronics (ICOM)","","2013","","","56","61","By using ANASYS finite element analysis software to analyze the first three order resonance frequencies and strain values of the porous and porous free structure of the acoustic sensing chips, the optimization of the structure indicates: the resonance frequency of the porous structure varies lower relatively, but the stress improves relatively higher, we utilize beam membrane structure, the featured size of the beam thickness is 50 um. We utilize MEMS technology to fabricate acoustic vibration sensor chips on single crystalline silicon, and use second order vibration mode test unit to measure vibration signal, on condition of 5v power supply, the sensitivity of chip is about 26.8mV/g, resonance frequency is 5.19kHz, frequency response range is over 20~2000Hz, the frequency at the point of 3dB is 2.93kHz.","","978-1-4799-1216-2978-1-4799-1214","10.1109/ICoOM.2013.6626490","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626490","MEMS;resonance frequency;acoustic vibration;sensor;single crystalline silicon","Vibrations;Vibration measurement;Acoustics;Sensors;Sensitivity;Resonant frequency;Semiconductor device measurement","acoustic transducers;finite element analysis;microsensors;vibration measurement","MEMS acoustic vibration sensor technology;ANASYS finite element analysis software;three order resonance frequency;porous free structure;resonance frequency;beam membrane structure;acoustic vibration sensor chips;single crystalline silicon;second order vibration mode test unit;vibration signal measurment;size 50 mum;voltage 5 V;frequency 5.19 kHz;frequency 2.93 kHz","","1","4","","","","","","IEEE","IEEE Conferences"
"The PigMix Benchmark on Pig, MapReduce, and HPCC Systems","K. Ouaknine; M. Carey; S. Kirkpatrick","NA; NA; NA","2015 IEEE International Congress on Big Data","","2015","","","643","648","Soon after Google published MapReduce, their paradigm for processing large amounts of data, the open-source world followed with the Hadoop ecosystem. Later on, Lexis Nexis, the company behind the world's largest database of legal documents, open-sourced its Big Data processing platform, called the High-Performance Computing Cluster (HPCC). This paper makes three contributions. First, we describe our additions and improvements to the Pig Mix benchmark, the set of queries originally written for Apache Pig, and the porting of Pig Mix to HPCC. Second, we compare the performance of queries written in Pig, Java MapReduce, and ECL. Last, we draw conclusions and issue recommendations for future system benchmarks and large-scale data-processing platforms.","2379-7703","978-1-4673-7278-7978-1-4673-7277","10.1109/BigDataCongress.2015.99","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207283","PigMix;HPCC Systems;MapReduce;Benchmark;Big Data;Performance","Benchmark testing;Java;Big data;Servers;Programming;Optimization;XML","Big Data;Java;parallel processing;pattern clustering;public domain software","large-scale data-processing platform;ECL;Java MapReduce;Apache Pig;high-performance computing cluster;Big Data processing platform;legal document;Lexis Nexis;Hadoop ecosystem;open-source world;Google;HPCC system;PigMix benchmark","","3","23","","","","","","IEEE","IEEE Conferences"
"Scalable certification framework for behavioral synthesis front-end","Z. Yang; K. Hao; K. Cong; L. Lei; S. Ray; F. Xie","Dept. of Computer Science, Portland State University, OR 97207, USA; Dept. of Computer Science, Portland State University, OR 97207, USA; Dept. of Computer Science, Portland State University, OR 97207, USA; Dept. of Computer Science, Portland State University, OR 97207, USA; Strategic CAD Labs, Intel Corporation, Hillsboro, OR 97124, USA; Dept. of Computer Science, Portland State University, OR 97207, USA","2014 51st ACM/EDAC/IEEE Design Automation Conference (DAC)","","2014","","","1","6","Behavioral synthesis entails application of a sequence of transformations to compile a high-level description of a hardware design (e.g., in C/C++/SystemC) into a register-transfer level (RTL) implementation. In this paper, we present a scalable equivalence checking framework to validate the correctness of compiler transformations employed by behavioral synthesis front-end. Our approach makes use of dual-rail symbolic simulation of the input and output of a transformation, together with identification and inductive verification of their loop structures. We have evaluated our framework on transformations applied by an open source behavioral synthesis tool to designs from the CHStone benchmark. Our tool can automatically validate more than 75 percent of the total of 1008 compiler transformations applied, taking an average time of 1.5 seconds per transformation.","0738-100X","978-1-4799-3017","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881476","","Optimization;Explosions;Benchmark testing;Complexity theory;Image edge detection;Hardware;Manuals","program compilers;program control structures;program verification;public domain software","scalable certification framework;behavioral synthesis front-end;transformation sequence;high-level hardware design description;C language;register-transfer level implementation;RTL implementation;scalable equivalence checking framework;automatic compiler transformation correctness validation;dual-rail symbolic simulation;transformation input;transformation output;loop-structure identification;inductive loop structure verification;open source behavioral synthesis tool;CHStone benchmark;C++ language;SystemC language","","2","24","","","","","","IEEE","IEEE Conferences"
"The performance impact analysis of loop unrolling","G. Velkoski; M. Gusev; S. Ristov","Innovation LLC, Vostanichka 118, Skopje, Macedonia; Univ. Ss Cyril and Methodius, FINKI Rugjer Boshkovic 16, Skopje, Macedonia; Univ. Ss Cyril and Methodius, FINKI, Rugjer Boshkovic 16, Skopje, Macedonia","2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","","2014","","","307","312","Loop unrolling is a well known technique, which usually results with speedup of a program that contains loops. The effect is obtained by reducing the operations that require counter increases and branch jumps at the end of the loops. This paper analyzes the impact of loop unrolling on various processor types and memory patterns. The experiments show a high correlation between the cache and the problem size. The loop unrolling results with a higher speedup for the execution of a smaller size problem, while it does not have impact for a problem whose size is greater than the capacity of the last level cache size, due to the huge number of cache misses. Another important result is that the loop unrolling achieves greater speedup on Intel, rather than AMD CPU. In this paper we analyze and discuss the various behaviors of loop unrolling.","","978-953-233-077-9978-953-233-081","10.1109/MIPRO.2014.6859582","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859582","AMD;Intel;Performance","Hardware;Optimization methods;Testing;Correlation;Computer architecture;Cache memory","program control structures;software performance evaluation","performance impact analysis;loop unrolling;processor types;memory patterns;cache misses;Intel","","1","31","","","","","","IEEE","IEEE Conferences"
"Cost/Performance Evaluation for Cloud Applications Using Simulation","M. Rak; A. Cuomo; U. Villano","NA; NA; NA","2013 Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises","","2013","","","152","157","The pay-per-use business model is one of the key factors for the success of the cloud computing paradigm: resources are acquired only when needed and charged on the basis of their actual usage. The execution of applications in the cloud implies costs that depend on the usage of the leased resources and on the resource pricing model adopted by the providers. This paper presents a technique to evaluate the trade-off between costs and performance of cloud applications through the use of benchmarks and simulation. Given a mOSAIC cloud application, it is possible to predict performance indexes and resource consumption under generic workloads. This makes it possible to choose the deployment on the resources of the provider that guarantees the desired performance levels and minimizes the costs for executing the application.","1524-4547","978-1-4799-0405","10.1109/WETICE.2013.36","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570602","Cloud Computing;Discrete-event Simulation;Performance Prediction","Computational modeling;Benchmark testing;Pricing;Predictive models;Time factors;Optimization;Load modeling","cloud computing;costing;resource allocation;software performance evaluation","cost-performance evaluation;pay-per-use business model;cloud computing paradigm;resource pricing model;mOSAIC cloud application;performance indexes;resource consumption;generic workloads","","4","14","","","","","","IEEE","IEEE Conferences"
"A novel hybrid approach to restore historical degraded documents","M. I. Quraishi; M. De; K. G. Dhal; S. Mondal; G. Das","Dept. of Information Technology, Kalyani Govt, Engineering College, Kalyani, Nadia, India; Dept. of Engg and Tech Studies, University of Kalyani, Kalyani, Nadia, India; Dept. of Comp. Sc. and Engg. Kalyani Govt. Engg., college Kalyani, Nadia, India; Dept. of Information Technology, Kalyani Govt Engineering College, Kalyani, Nadia, India; Dept. of Information Technology, Kalyani Govt Engineering College, Kalyani, Nadia, India","2013 International Conference on Intelligent Systems and Signal Processing (ISSP)","","2013","","","185","189","Old degraded historical documents carry various important information regarding our culture, economics etc. proper restoration of these documents is very necessary. After digitization of these documents there remain noises and other low resolution components. These affect the overall visual appearance of the documents. In this paper a novel approach is proposed to enhance ancient historical documents. To enhance these digital format documents a two way approach is considered. At first Particle Swarm Optimization (PSO) and bilateral filter is applied. At second level Non-Linear Enhancement with bilateral filter is applied. Both the approaches are then tested visually and quantitively to show the effectiveness of the approach.","","978-1-4799-0317-7978-1-4799-0316","10.1109/ISSP.2013.6526899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6526899","image enhancement;low resolution;particle swarm optimization;bilateral filter;noise;historical document;nonlinear enhancement;PSNR;NAE","Optical filters;Maximum likelihood detection;Nonlinear filters;Handwriting recognition;Optical character recognition software;Signal processing algorithms","document image processing;history;optical character recognition","historical degraded document restoration;culture;economics;document digitization;ancient historical documents;digital format documents;particle swarm optimization;PSO;bilateral filter;nonlinear enhancement","","4","30","","","","","","IEEE","IEEE Conferences"
"3D visual analytics for quality control in engineering","M. Klemenčić; K. Skala","University of Zagreb, Faculty of Graphic Arts, (PhD study) Getaldićeva 2, 10000, Croatia; Ruđer Bošković Institute, Bijenićka cesta 54, 10000 Zagreb, Croatia","2013 36th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","","2013","","","258","262","This article will focus on 3D visual analytics for quality control in engineering. The world today is a busy place with a rapidly increasing amount of data to be dealt with every day. Problems are encountered when most of the data are stored without filtering and refinement for later use. Industry has raised the demands for the high technological performance of final products, such as short production time, low manufacturing costs and overall product quality. Digitizing in the real-world has various application domains, and is of vital importance when it comes to methods involving industrial quality assurance. The process of the rapid development of products depends on new technologies, such as 3D scanning, 3D printing and prototyping. Detailed digitizing can provide more product information, making it easier to locate the causes of inaccuracies and optimize production. These possibilities help us check and improve tools and gadgets, control the form of prototypes and test series in production optimization, quality assurance of serial production etc. Quality and rapid digitizing using these systems makes copying easy and thus accelerates serial production, which is why these procedures are used by numerous companies. This paper explores and analyzes existing technology and devices used in industrial quality assurance, and provides a brief review of current options and application opportunities.","","978-953-233-073-1978-953-233-076","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6596263","","Three-dimensional displays;Visual analytics;Production;Software;Data visualization;Materials","data analysis;data visualisation;production engineering computing;quality assurance;quality control","3D visual analytics;quality control;engineering;high technological performance;production time;manufacturing costs;product quality;industrial quality assurance;3D scanning;3D printing;prototyping;production optimization;serial production;application opportunities","","","3","","","","","","IEEE","IEEE Conferences"
"Design & performance of a hydraulic micro-turbine with counter-rotating runners","D. Biner; V. Hasmatuchi; F. Avellan; C. Münch-Alligné","University of Applied Sciences HES-SO Valais//Wallis, Route du Rawyl 47, 1950 Sion, Switzerland; University of Applied Sciences HES-SO Valais//Wallis, Route du Rawyl 47, 1950 Sion, Switzerland; École Polytechnique Fédérale de Lausanne, Laboratory for Hydraulic Machines, avenue de cour 33bis, 1007, Switzerland; University of Applied Sciences HES-SO Valais//Wallis, Route du Rawyl 47, 1950 Sion, Switzerland","2015 5th International Youth Conference on Energy (IYCE)","","2015","","","1","10","The largely unexploited potential of small scale energy hydropower remains crucial the development of new technologies to harvest the hydraulic energy on existing facilities. In this framework, several projects have been set up by the HES-SO Valais//Wallis and the EPFL Laboratory for Hydraulic Machines. One of the developed technologies is a new hydraulic micro- turbine, for recovering the energy lost in release valves of water supply networks. One stage of the micro-turbine consists of two axial counter-rotating runners. This paper deals with the hydraulic design process of the runners for a given site, including numerical flow simulations, fabrication and performance measurements of the micro-turbine. An overview of theoretical basics, simulation settings and assumptions, simulation results and test results is given. In the last part, the design optimization process is discussed.","","978-1-4673-7172-8978-1-4673-7171","10.1109/IYCE.2015.7180737","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180737","Hydraulic design;5 axis machining;numerical flow simulation;performance measurements;optimization process","Blades;Numerical models;Discharges (electric);Software;Velocity control;Turbines;Mathematical model","flow simulation;hydraulic turbines;hydroelectric power","small scale energy hydropower;hydraulic energy;HES-SO Valais//Wallis;EPFL Laboratory;hydraulic machines;hydraulic microturbine;release valves;water supply networks;axial counter-rotating runners;hydraulic design process;numerical flow simulations;design optimization process","","2","7","","","","","","IEEE","IEEE Conferences"
"Cross-plane chroma enhancement for SHVC Inter-Layer Prediction","J. Dong; Y. Ye; Y. He","InterDigital Communications, Inc., San Diego, USA; InterDigital Communications, Inc., San Diego, USA; InterDigital Communications, Inc., San Diego, USA","2013 Picture Coding Symposium (PCS)","","2013","","","309","312","This paper proposes a cross-plane chroma enhancement (CPCE) scheme to enhance the chroma planes of the inter layer reference (ILR) pictures for the Scalable extensions of HEVC (SHVC), the on-going scalable video coding project in JCT-VC. The CPCE scheme restores the blurred edges and textures in the chroma planes using the corresponding information from the luma plane. Experimental results under the SHVC common test conditions show that the average BD-rate reductions for the Cb and Cr chroma planes are as much as -7.5% and -8.5%, respectively, when compared with SHM-1.0.","","978-1-4799-0294-1978-1-4799-0292","10.1109/PCS.2013.6737745","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6737745","","IEC;Optimized production technology;Silicon;Artificial intelligence;Software","image restoration;image texture;video coding","cross-plane chroma enhancement;CPCE;scalable high efficiency video coding;SHVC interlayer prediction;interlayer reference;ILR;JCT-VC;blurred edge restoration;luma plane;BD-rate reductions","","4","8","","","","","","IEEE","IEEE Conferences"
"Intelligent crawler for web forums based on improved regular expressions","M. Pavković; J. Protić","University of Belgrade, School of Electrical Engineering, Bulevar kralja Aleksandra 73, 11120, Serbia; University of Belgrade, School of Electrical Engineering, Bulevar kralja Aleksandra 73, 11120, Serbia","2013 21st Telecommunications Forum Telfor (TELFOR)","","2013","","","817","820","In this paper, we present the development and characteristics of a specialized Web-scale forum crawler. The main idea is to crawl relevant forum content from the Web with minimal server resource consumption, and to organize crawled content into logical units, in order to make it easier for further processing and analysis. Forum posts contain relevant information that are of interest to forum crawler. Although forums have different designs, and are built on different technologies, they always have identical logic navigation that connects homepage and particular posts through forum lists and threads by specific URLs. Considering this common implicit navigation, we have optimized Web crawling problem to be URL-type recognition problem. URL-type database and regular expressions are used in order to achieve URL-type recognition. These regular expressions are expanded with special custom characters and commands that gave this forum crawler advantage over other Web based crawlers. The results shown in this paper are obtained by crawling a set of Web forums with different technology, location and design. Each test compared the results obtained by standard Web based crawler and our specialized forum crawler. Our test results show that by crawling only specific data and URL paths on the forum, we have managed to reduce the time of crawling and to achieve lower server resources consumption.","","978-1-4799-1420-3978-1-4799-1419","10.1109/TELFOR.2013.6716355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6716355","crawler;regular expressions;forum;URL type;web search","Crawlers;Knowledge based systems;Message systems;Internet;Software packages;Databases;Educational institutions","information retrieval;knowledge based systems;search engines;Web sites","intelligent crawler;Web-scale forum crawler development;Web-scale forum crawler characteristics;minimal server resource consumption;information processing;information analysis;forum posts;logical navigation;homepage;forum lists;implicit navigation;optimized Web crawling problem;URL-type recognition problem;URL-type database;regular expressions;custom characters;URL paths","","","14","","","","","","IEEE","IEEE Conferences"
"Comprehensive performance analysis and comparison of vehicles routing algorithms in smart cities","S. Wang; S. Djahel; J. McManis; C. McKenna; L. Murphy","Lero, RINCE, School of Electronic Engineering, Dublin City University, Ireland; Lero, School of Computer Science and Informatics, University College Dublin, Ireland; Lero, RINCE, School of Electronic Engineering, Dublin City University, Ireland; IBM Software Group, Industry Solutions Development, Ireland; IBM Software Group, Industry Solutions Development, Ireland","Global Information Infrastructure Symposium - GIIS 2013","","2013","","","1","8","Due to the severe impact of road traffic congestion on both economy and environment, several vehicles routing algorithms have been proposed to optimize travelers itinerary based on real-time traffic feeds or historical data. However, their evaluation methodologies are not as compelling as their key design idea because none of them had been tested under both real transportation map and real traffic data. In this paper, we conduct a deep performance analysis and comparison of four typical vehicles routing algorithms under various scalability levels (i.e. trip length and traffic load) based on realistic transportation simulation. The ultimate goal of this work is to suggest the most suitable routing algorithm to use in different transportation scenarios, so that it can provide a valuable reference for both traffic managers and researchers when they deploy or optimize a large scale centralized Traffic Management System (TMS). The obtained simulation results reveal that dynamic A* is the best routing algorithm if the TMS has sufficient memory or storage capacities, otherwise static A* is also a great alternative.","2150-3281;2150-329X","978-1-4799-2969","10.1109/GIIS.2013.6684365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6684365","ITS;Smart Transportation;Vehicles Routing Algorithms;Comparative Study;Shortest Path;Performance Evaluation;Smart Cities","Heuristic algorithms;Vehicles;Roads;Routing;Vehicle dynamics;Measurement;Algorithm design and analysis","road traffic;vehicle routing","comprehensive performance analysis;vehicle routing algorithms;smart cities;road traffic congestion;real-time traffic feeds;historical data;transportation map;scalability levels;realistic transportation simulation;large scale centralized traffic management system;TMS;dynamic A* algorithm;storage capacity","","7","21","","","","","","IEEE","IEEE Conferences"
"Introduction to Multi-criteria Decision Support Minitrack","H. R. Weistroffer","NA","2013 46th Hawaii International Conference on System Sciences","","2013","","","1164","1164","Almost all decisions people make are based on multiple factors or criteria. Decision makers generally pursue multiple, and often conflicting, objectives. A feasible solution that is optimum with respect to all such objectives or decision criteria almost never exists, and a satisfactory compromise solution is generally sought. Multi-criteria decision-making as a field of research deals with problem theory and solution approaches directly involving multiple decision criteria. Information technology and systems may help in dealing with such multi-criteria decision problems. This mini-track focuses on solution approaches, technology, and systems that support decision-making under consideration of multiple decision criteria. This is the third time that this minitrack is included in the HICSS program, and five contributed papers have been accepted. The papers deal with a wide range of decision problems, from software component selection, to recommendations for location-based services. Furthermore, the relevance of trust in multi-criteria decision support, support for research proposal grouping, and skyline operation for multi-criteria decision support are investigated and discussed. Becker, Kraxner, Plangg, and Rauber, in their paper on Improving Decision Support for Software Component Selection through Systematic Cross-referencing and Analysis of Multiple Decision Criteria discuss challenges and opportunities in using particular characteristics of scale in decision scenarios for software component selection. Building on an existing decision support framework, they formalize quality criteria so that they can be cross-referenced and analyzed across scenarios. The paper by Xu, Xu, and Ma, titled An Ontology based Frequent Item set Method to Support Research Proposal Grouping for Research Project Selection, introduces a novel approach to support grouping of research proposals to aid in research project selection. In this approach, first an ontology is constructed to standardize research keywords, and then a frequent item set with various degrees of support is extracted from the research proposals, based on the ontology. In their paper on Success of Multi Criteria Decision Support Systems: The Relevance of Trust, Maida, Maier, Obwegeser, and Stix present a consolidated view on different dimensions of trust and discuss the specific characteristics and dynamics of trust in multi-criteria decision support, based on a multidimensional model. They test the validity of their model with an empirical study, asking participants to complete a survey after using a specially developed decision aide. Emrich, Chapko, and Werth, in their paper on Adaptive, Multi-criteria Recommendations for Location-based Services, analyze influence factors of mobile users for choice of interest. They derive an adaptable ranking function capable of adjusting preference weights on the influence factors, so as to learn from user behavior and evolve the knowledge base. And finally, the paper by Chai, Liu, Yiu, Wang, and Li on A Novel Dynamic Skyline Operation for Multicriteria Decision Support investigates preference relations in skyline operations, a multi-criteria ranking procedure which generally relies on a predetermined preference system. The authors introduce the concept of preference intensity and propose a new decision model, the Tolerant Skyline operation, or T-skyline, which allows for dynamic decision preferences.","1530-1605;1530-1605","978-1-4673-5933-7978-0-7695-4892","10.1109/HICSS.2013.317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479974","","Proposals;Ontologies;Software;Mobile radio mobility management;Itemsets;Educational institutions;Decision support systems","","","","","","","","","","","IEEE","IEEE Conferences"
"Evaluating the Energy Efficiency of Data Management Systems","R. Niemann; T. Ivanov","NA; NA","2015 IEEE/ACM 4th International Workshop on Green and Sustainable Software","","2015","","","22","28","Nowadays developers and end users of data management systems are challenged with the reduction of the ""energy consumption footprint"" of existing implementations and configurations. In other words, the energy efficiency has to be optimized, either by increasing the performance or by consuming less resources. In fact, there is a big number of factors that influence the performance and energy efficiency of a particular data management system. For example, the replacement of hardware components or the surrounding operating system can have a significant impact. Both developers and end users put much effort into finding performance ""bottlenecks"", better hardware resource utilization and configurations. Besides, when it comes to a scale-out scenario, end users often face the situation to find a hardware configuration that offers both a reasonable performance and energy consumption, i.e. A resource planning. This paper proposes a new approach to evaluate the performance of a data management system and the impact on the energy efficiency with the goal to optimize it. The approach introduces a Queued Petri Nets model, whose simulation runs are intended to drastically reduce the investments, both in time and hardware, compared to traditional ways, for example regression and compatibility tests. The model's prediction in terms of performance and energy efficiency were evaluated and compared to the actual experimental results. On average the predicted and experimental results (response time and energy efficiency) differ by 24 percent.","","978-1-4673-7049","10.1109/GREENS.2015.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168334","Data management system;energy efficiency;performance;Queued Petri Nets","Data models;Benchmark testing;Hardware;Operating systems;Energy consumption;Petri nets;Servers","information management;operating systems (computers);Petri nets","energy efficiency;data management systems;energy consumption footprint;hardware components;operating system;hardware resource utilization;hardware resource configurations;queued Petri nets model","","3","12","","","","","","IEEE","IEEE Conferences"
"Comparing standard translation methods for predicting photovoltaic energy production","B. C. Duck; C. J. Fell; B. Marion; K. Emery","CSIRO Energy Technology, PO Box 330, Newcastle NSW 2300, Australia; CSIRO Energy Technology, PO Box 330, Newcastle NSW 2300, Australia; NREL, 15013 Denver West Parkway, Golden, CO 80401, USA; NREL, 15013 Denver West Parkway, Golden, CO 80401, USA","2013 IEEE 39th Photovoltaic Specialists Conference (PVSC)","","2013","","","0763","0768","Translation equations underpin all predictive models for the energy output of photovoltaics in the outdoor environment. These equations translate the performance of a PV device to an arbitrary temperature and irradiance, based on measurements taken under reference conditions. Little work has been done to compare and contrast the three translation methods recommended under IEC 60891. This is partly due to a lack of comprehensive test data, and partly due to the flexibility in the way these methods can be applied. Based on comprehensive outdoor test data, we have used software scripts to evaluate the performance of these three standard translation methods, where the choice of reference conditions has been optimized to produce the best result in different ranges of irradiance and temperature. This allows a fair comparison of their performance that is independent of the test site location. We map the performance of the three methods over a wide range of irradiance and temperature.","0160-8371","978-1-4799-3299-3978-1-4799-3298","10.1109/PVSC.2013.6744261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6744261","accuracy;energy measurement;photovoltaic effects;prediction algorithms;standards","Temperature measurement;Temperature distribution;Accuracy;Mathematical model;Energy measurement;Standards;Equations","solar cells","standard translation methods;photovoltaic energy production prediction;translation equations;outdoor environment;PV device;arbitrary temperature;arbitrary irradiance;IEC 60891;comprehensive test data;comprehensive outdoor test data;software scripts;predictive models","","4","9","","","","","","IEEE","IEEE Conferences"
"Reverse Engineering Digital Circuits Using Structural and Functional Analyses","P. Subramanyan; N. Tsiskaridze; W. Li; A. Gascón; W. Y. Tan; A. Tiwari; N. Shankar; S. A. Seshia; S. Malik","Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA; Computer Science Laboratory, SRI International, Menlo Park, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA; Computer Science Laboratory, SRI International, Menlo Park, CA, USA; Computer Science Laboratory, SRI International, Menlo Park, CA, USA; Department of Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, USA; Department of Electrical Engineering, Princeton University, Princeton, NJ, USA","IEEE Transactions on Emerging Topics in Computing","","2014","2","1","63","80","Integrated circuits (ICs) are now designed and fabricated in a globalized multivendor environment making them vulnerable to malicious design changes, the insertion of hardware Trojans/malware, and intellectual property (IP) theft. Algorithmic reverse engineering of digital circuits can mitigate these concerns by enabling analysts to detect malicious hardware, verify the integrity of ICs, and detect IP violations. In this paper, we present a set of algorithms for the reverse engineering of digital circuits starting from an unstructured netlist and resulting in a high-level netlist with components such as register files, counters, adders, and subtractors. Our techniques require no manual intervention and experiments show that they determine the functionality of >45% and up to 93% of the gates in each of the test circuits that we examine. We also demonstrate that our algorithms are scalable to real designs by experimenting with a very large, highly-optimized system-on-chip (SOC) design with over 375000 combinational elements. Our inference algorithms cover 68% of the gates in this SOC. We also demonstrate that our algorithms are effective in aiding a human analyst to detect hardware Trojans in an unstructured netlist.","2168-6750;2376-4562","","10.1109/TETC.2013.2294918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6683016","Digital circuits;computer security;design automation;formal verification","Algorithm design and analysis;Logic gates;Reverse engineering;Trojan horses;Inference algorithms;Hardware;Globalization;Integrated circuits","industrial property;integrated circuit design;invasive software;reverse engineering;system-on-chip","algorithmic reverse engineering digital circuits;functional analysis;structural analysis;integrated circuits;ICs;globalized multivendor environment;hardware trojans-malware;intellectual property;IP theft;IP violation detection;high-level netlist;unstructured netlist;subtractors;adders;counters;register files;test circuits;very large highly-optimized system-on-chip design;combinational elements;SoC design","","27","31","","","","","","IEEE","IEEE Journals & Magazines"
"Asynchronous auxiliary signal design for failure detection","S. L. Campbell; J. R. Scott","Department of Mathematics, North Carolina State University, Raleigh, 27695-8205, USA; Department of Mathematics, North Carolina State University, Raleigh, 27695-8205, USA","2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","","2014","","","2727","2732","Fault detection and identification (FDI) are important tasks in most modern industrial systems and processes. A variety of approaches both active and passive have been investigated. A test signal or input is used in an active approach. Usually in the active approach the same time window is used for the running of the test and the application of the test signal. However, that is not always the most desirable or practical thing to do. The effect of different signal and observation windows is investigated here for one active approach.","1062-922X","978-1-4799-3840","10.1109/SMC.2014.6974340","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6974340","","Noise;Uncertainty;Noise measurement;Software;Optimization;Fault detection;Computational modeling","fault diagnosis;reliability theory","system reliability;test signal;time window;FDI;fault detection and identification;failure detection;asynchronous auxiliary signal design","","","24","","","","","","IEEE","IEEE Conferences"
"Handwritten Chinese/Japanese Text Recognition Using Semi-Markov Conditional Random Fields","X. Zhou; D. Wang; F. Tian; C. Liu; M. Nakagawa","Institute of Software of Chinese Academy of Sciences, Beijing; Institute of Automation of Chinese Academy of Sciences, Beijing; Institute of Software of Chinese Academy of Sciences, Beijing; Institute of Automation of Chinese Academy of Sciences, Beijing; Tokyo University of Agriculture and Technology, Tokyo","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2013","35","10","2413","2426","This paper proposes a method for handwritten Chinese/Japanese text (character string) recognition based on semi-Markov conditional random fields (semi-CRFs). The high-order semi-CRF model is defined on a lattice containing all possible segmentation-recognition hypotheses of a string to elegantly fuse the scores of candidate character recognition and the compatibilities of geometric and linguistic contexts by representing them in the feature functions. Based on given models of character recognition and compatibilities, the fusion parameters are optimized by minimizing the negative log-likelihood loss with a margin term on a training string sample set. A forward-backward lattice pruning algorithm is proposed to reduce the computation in training when trigram language models are used, and beam search techniques are investigated to accelerate the decoding speed. We evaluate the performance of the proposed method on unconstrained online handwritten text lines of three databases. On the test sets of databases CASIA-OLHWDB (Chinese) and TUAT Kondate (Japanese), the character level correct rates are 95.20 and 95.44 percent, and the accurate rates are 94.54 and 94.55 percent, respectively. On the test set (online handwritten texts) of ICDAR 2011 Chinese handwriting recognition competition, the proposed method outperforms the best system in competition.","0162-8828;2160-9292;1939-3539","","10.1109/TPAMI.2013.49","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6472237","Character string recognition;semi-Markov conditional random field;lattice pruning;beam search","Lattices;Character recognition;Training;Handwriting recognition;Context;Text recognition;Context modeling","computational geometry;handwritten character recognition;image segmentation;Markov processes;natural language processing;search problems","handwritten Chinese-Japanese text recognition;semiMarkov conditional random fields;high-order semi-CRF model;segmentation-recognition hypotheses;linguistic contexts;geometric contexts;feature functions;character recognition;negative log-likelihood loss;training string sample set;forward-backward lattice pruning algorithm;trigram language models;beam search techniques;decoding speed;CASIA-OLHWDB;TUAT Kondate","Algorithms;Artificial Intelligence;Asian Continental Ancestry Group;Computer Simulation;Handwriting;Humans;Image Enhancement;Image Interpretation, Computer-Assisted;Language;Markov Chains;Models, Statistical;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Subtraction Technique","22","64","","","","","","IEEE","IEEE Journals & Magazines"
"Machine-Learning-Based Feature Selection Techniques for Large-Scale Network Intrusion Detection","O. Y. Al-Jarrah; A. Siddiqui; M. Elsalamouny; P. D. Yoo; S. Muhaidat; K. Kim","NA; NA; NA; NA; NA; NA","2014 IEEE 34th International Conference on Distributed Computing Systems Workshops (ICDCSW)","","2014","","","177","181","Nowadays, we see more and more cyber-attacks on major Internet sites and enterprise networks. Intrusion Detection System (IDS) is a critical component of such infrastructure defense mechanism. IDS monitors and analyzes networks' activities for potential intrusions and security attacks. Machine-learning (ML) models have been well accepted for signature-based IDSs due to their learn ability and flexibility. However, the performance of existing IDSs does not seem to be satisfactory due to the rapid evolution of sophisticated cyber threats in recent decades. Moreover, the volumes of data to be analyzed are beyond the ability of commonly used computer software and hardware tools. They are not only large in scale but fast in/out in terms of velocity. In big data IDS, the one must find an efficient way to reduce the size of data dimensions and volumes. In this paper, we propose novel feature selection methods, namely, RF-FSR (Random Forest-Forward Selection Ranking) and RF-BER (Random Forest-Backward Elimination Ranking). The features selected by the proposed methods were tested and compared with three of the most well-known feature sets in the IDS literature. The experimental results showed that the selected features by the proposed methods effectively improved their detection rate and false-positive rate, achieving 99.8% and 0.001% on well-known KDD-99 dataset, respectively.","1545-0678;2332-5666","978-1-4799-4181-0978-1-4799-4182","10.1109/ICDCSW.2014.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888858","intrusion detection system;feature selection;machine learning;random forest","Intrusion detection;Feature extraction;Computational modeling;Data models;Radio frequency;Big data;Training","Big Data;computer network security;digital signatures;feature selection;Internet;learning (artificial intelligence);random processes","machine-learning-based feature selection techniques;large-scale network intrusion detection system;cyber-attacks;security attacks;Internet sites;enterprise networks;machine-learning models;signature-based IDSs;big data IDS;RF-FSR;random forest-forward selection ranking;RF-BER;random forest-backward elimination ranking;KDD-99 dataset","","8","24","","","","","","IEEE","IEEE Conferences"
"Voltage control of distribution networks using fuzzy approach and capacitors offline planning","R. Aboli; M. Ramezani; H. Falaghi","Department of Electrical Engineering, Faculty of Engineering, Birjand University, IRAN; Department of Electrical Engineering, Faculty of Engineering, Birjand University, IRAN; Department of Electrical Engineering, Faculty of Engineering, Birjand University, IRAN","2015 20th Conference on Electrical Power Distribution Networks Conference (EPDC)","","2015","","","150","155","This paper proposes a new approach to real time control of the bus voltages in the distribution systems. This approach consists of two control parts; includes offline and online control. In the offline part, switchable capacitors are scheduled based on day-ahead load forecasting. This step is solved using an efficient coding PSO algorithm. The under load tap changer (ULTC) is not a control variable in the offline scheduling and is only operated to improve voltage based on fuzzy approach. Once the switchable capacitors are scheduled, they are fixed on their hourly position in the real time operation. Then the ULTC is controlled based on the fuzzy system in the real time operation of the network. Easy implementation of the offline scheduling due to elimination of the ULTC as a control variable, removal of switching operation constraint and removal approximately of the voltage constraint is the main advantage of the proposed method. In addition, self-healing and possibility control of the bus voltages in different conditions such as unpredictable load changes and contingencies are other benefits. The 69 bus IEEE test system has been used to analyze and validation of the proposed approach.","","978-1-4673-6612-0978-1-4673-6611","10.1109/EPDC.2015.7330488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7330488","distribution system;capacitor;fuzzy system;offline control;PSO algorithm;real time control;ULTC","Voltage measurement;Voltage control;Software;Switches;Sociology;Statistics;Capacitors","busbars;capacitor switching;fuzzy control;IEEE standards;load forecasting;particle swarm optimisation;power distribution control;power distribution planning;voltage control","distribution network voltage control;fuzzy approach;capacitor offline planning;online control;offline control;switchable capacitor scheduling;day-ahead load forecasting;coding PSO algorithm;under load tap changer;ULTC;switching operation constraint removal;bus IEEE test system","","","17","","","","","","IEEE","IEEE Conferences"
"Contiki80211: An IEEE 802.11 Radio Link Layer for the Contiki OS","I. Glaropoulos; V. Vukadinovic; S. Mangold","NA; NA; NA","2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)","","2014","","","621","624","We believe that the existing 802.11 MAC layer can be optimized (especially for energy-efficiency) to make Wi-Fi suitable for a wide range of IoT applications. However, there is a lack of low-cost embedded platforms to be used for experimentation with 802.11 MAC. The majority of low-power Wi-Fi modules for embedded systems has closed source firmware and protocol stack implementations, which prevents implementation and testing of new protocol features. Here we describe Contiki80211, an open source 802.11 radio link layer implementation for Contiki OS, optimized for resource constrained embedded platforms, whose purpose is to enable experimentation with 802.11 MAC layer management mechanisms on embedded devices.","","978-1-4799-6123-8978-1-4799-6122","10.1109/HPCC.2014.116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056808","IEEE 802.11;IoT;Contiki OS;prototyping","IEEE 802.11 Standards;Universal Serial Bus;Resource management;Hardware;Delays;Media Access Protocol","embedded systems;Internet of Things;radio links;wireless LAN","Contiki80211;IEEE 802.11 radio link layer;Wi-Fi;IoT applications;embedded systems;closed source firmware;protocol stack;resource constrained embedded platforms;802.11 MAC layer management mechanisms","","1","8","","","","","","IEEE","IEEE Conferences"
"Energy-Aware Performance Evaluation of Android Custom Kernels","L. Corral; A. B. Georgiev; A. Janes; S. Kofler","NA; NA; NA; NA","2015 IEEE/ACM 4th International Workshop on Green and Sustainable Software","","2015","","","1","7","Smartphones play a key role in several aspects of our daily life. Their range of application is constantly growing, making them versatile and necessary. However, mobile devices face an important problem: they hold an important autonomy requirement, which is constantly challenged by the short life of batteries. Researchers and practitioners have proposed different strategies to preserve battery life in mobile devices, both from hardware and software point of views. One of the software-based approaches is to apply optimizations at the level of the kernel of the operating system. This strategy is attractive, as it may improve the battery consumption of all applications running on top of the software kernel. The scope of this paper is to compare current Android kernel-based modifications evaluating their impact on battery consumption. To do it, we performed performance tests on each kernel, monitoring the battery consumption in background. Additionally, we run a general performance test to see the impact of the applied kernel modifications to the overall performance of the optimized device. Our results show that kernel level enhancements do improve the battery life and the device's performance. According to our tests, the analyzed custom kernels can reduce the battery consumption up to 33% for isolated tasks, improving the general performance of the device by up to 16%.","","978-1-4673-7049","10.1109/GREENS.2015.8","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7167751","Android;Energy;Kernel;Mobile;Performance","Kernel;Batteries;Smart phones;Performance evaluation;Androids;Humanoid robots","Android (operating system);energy consumption;power aware computing","energy-aware performance evaluation;Android custom kernels;smartphones;Android kernel-based modifications;battery consumption monitoring","","5","24","","","","","","IEEE","IEEE Conferences"
"Warped register file: A power efficient register file for GPGPUs","M. Abdel-Majeed; M. Annavaram","Electrical Engineering Department, University of Southern California, Los Angeles, 90089, USA; Electrical Engineering Department, University of Southern California, Los Angeles, 90089, USA","2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)","","2013","","","412","423","General purpose graphics processing units (GPGPUs) have the ability to execute hundreds of concurrent threads. To support massive parallelism GPGPUs provide a very large register file, even larger than a cache, to hold the state of each thread. As technology scales, the leakage power consumption of the SRAM cells is getting worse making the register file static power consumption a major concern. As the supply voltage scaling slows, dynamic power consumption of a register file is not reducing. These concerns are particularly acute in GPGPUs due to their large register file size. This paper presents two techniques to reduce the GPGPU register file power consumption. By exploiting the unique software execution model of GPGPUs, we propose a tri-modal register access control unit to reduce the leakage power. This unit first turns off any unallocated register, and places all allocated registers into drowsy state immediately after each access. The average inter-access distance to a register is 789 cycles in GPGPUs. Hence, aggressively moving a register into drowsy state immediately after each access results in 90% reduction in leakage power with negligible performance impact. To reduce dynamic power this paper proposes an active mask aware activity gating unit that avoids charging bit lines and wordlines of registers associated with all inactive threads within a warp. Due to insufficient parallelism and branch divergence warps have many inactive threads. Hence, registers associated with inactive threads can be identified precisely using the active mask. By combining the two techniques we show that the power consumption of the register file can be reduced by 69% on average.","1530-0897;1530-0897","978-1-4673-5587-2978-1-4673-5585-8978-1-4673-5586","10.1109/HPCA.2013.6522337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522337","","Registers;Benchmark testing;Instruction sets;Power demand;SRAM cells;Microarchitecture;Parallel processing","authorisation;cache storage;concurrency control;graphics processing units;multi-threading;optimising compilers;performance evaluation;power aware computing;power consumption","warped register file;power efficient register file;general purpose graphics processing units;concurrent threads;parallelism GPGPUs;technology scales;leakage power consumption;SRAM cells;register file static power consumption;supply voltage scaling;dynamic power consumption;register file size;GPGPU register file power consumption;software execution model;tri-modal register access control unit;register allocation;average interaccess distance;leakage power reduction;active mask aware activity gating unit;charging bit lines;power consumption","","51","29","","","","","","IEEE","IEEE Conferences"
"A wide band black-box model of power transformers in ATP/MODELS","A. I. Chrysochos; A. I. Nousdilis; T. A. Papadopoulos; G. K. Papagiannis","Power Systems Laboratory, School of Electrical &amp; Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Power Systems Laboratory, School of Electrical &amp; Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Power Systems Laboratory, School of Electrical &amp; Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece; Power Systems Laboratory, School of Electrical &amp; Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece","2014 49th International Universities Power Engineering Conference (UPEC)","","2014","","","1","6","The high-frequency power transformer modeling is of significant importance for most power systems applications. This paper presents a simple black-box modeling methodology for power transformers using transfer functions, defined by the recorded voltage ratios at the transformer open-circuited terminals. The model parameters are estimated using an optimization method, minimizing the error between the measured and the calculated data. The proposed model is implemented in the ATP/EMTP software, using the ATP/MODELS language in order to simulate the transmission of power-line communication signals through the transformer.","","978-1-4799-6557-1978-1-4799-6556","10.1109/UPEC.2014.6934674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6934674","ATP/MODELS language;black-box modeling;laboratory tests;power-line communication;power transformers","Transfer functions;Power transformers;Resonant frequency;Frequency measurement;Mathematical model;Computational modeling;Power cables","carrier transmission on power lines;minimisation;parameter estimation;power system simulation;power transformers;transfer functions","wide band black-box model;ATP-MODELS language;high-frequency power transformer modeling;transfer functions;recorded voltage ratios;transformer open-circuited terminals;model parameter estimation;optimization method;error minimization;ATP-EMTP software;power-line communication signals;transmission simulation","","3","26","","","","","","IEEE","IEEE Conferences"
"Active pedal exerciser for leg rehabilitation","F. Garcia; J. P. Ferreira; P. Ferreira; S. Cruz; M. Crisóstomo; A. P. Coimbra","Dept. of Physics, Faculty of Science and Technology of the Univ. of Coimbra, Portugal; Institute of Systems and Robotics, Dept. of Electrical and Computer Engineering, Univ. of Coimbra; Institute of Systems and Robotics, Dept. of Electrical and Computer Engineering, Univ. of Coimbra; Dept. of Electrical Engineering, Superior Institute of Engineering of Coimbra, Coimbra, Portugal; Institute of Systems and Robotics, Dept. of Electrical and Computer Engineering, Univ. of Coimbra; Institute of Systems and Robotics, Dept. of Electrical and Computer Engineering, Univ. of Coimbra","2015 IEEE 4th Portuguese Meeting on Bioengineering (ENBENG)","","2015","","","1","1","Summary form only given. Given the importance of the ability to drive the lower limbs to perform most daily activities for all people and knowing that there is a constant need to develop new ways to help people who do not fully make use of this ability, either by external or physical causes, a tool for motion rehabilitation is being developed. In the market there are already some products for leg rehabilitation. Pedal exercisers and static bicycles are the most common. Mostly, they are very rudimentary because its practise can't be controlled by the health staff through software. A number of them have virtual environments for simulation of outdoor exercise. Some products that aren't yet on the market have new and important features, like reacting to the patient's performance to optimize the rehabilitation process, through an adjustable resistant motor [1]. Other can stimulate patient's legs movement because of their motor, and alternate between cycling forward or backward [2]. There is also a prototype of one device that trains lower limbs bilaterally [3]. The rehabilitation device presented here is based on a kind of motor assisted bicycle, which will gradually be triggered, depending on the pressure exerted on the force sensors existing on the pedals, allowing to compensate the leg with mobility problems, helping it to perform the expected cycling movement. There is also a sensor to monitor the patient's heart rate. To obtain the pressure sensor and heart rate values a data acquisition system is used. It is connected to a computer and to the motor controller. With it, it is possible to perform motor control to ensure the efficiency of the treatment and the patient safety. There is also the capability of exercising each leg with different parameters, which represents a very useful advantage for stroke patients, and can also compensate for a missing or impaired limb by mimicking the performance of the healthy leg. A computer interface allows the physiotherapist in charge to make a responsible management and an efficient monitoring of the equipment. It is expected to test a prototype in a public hospital soon.","","978-1-4799-8269","10.1109/ENBENG.2015.7088802","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7088802","Rehabilitation;Bioengineering;Pedal Exerciser","Biomedical engineering;Legged locomotion;Computers;Bicycles;Performance evaluation;Prototypes;Monitoring","biomechanics;biomedical equipment;biomedical measurement;brain;cardiology;data acquisition;force sensors;medical control systems;medical disorders;neurophysiology;patient monitoring;patient rehabilitation;pressure sensors;prototypes;safety;user interfaces","active pedal exerciser;lower limb motion rehabilitation tool;static bicycle;virtual environment;outdoor exercise simulation;patient performance;rehabilitation optimization;adjustable resistant motor;patient leg movement stimulation;forward cycling;backward cycling;bilateral lower limb training;rehabilitation device;motor assisted bicycle triggering;pressure dependence;pedal force sensor;leg mobility compensation;leg mobility problem;patient heart rate sensor;patient heart rate monitoring;pressure sensor;data acquisition system;motor controller;treatment efficiency;patient safety;leg exercise parameter variation;stroke patient;missing limb compensation;impaired limb compensation;healthy leg performance mimicking;physiotherapist computer interface;leg rehabilitation equipment management;leg rehabilitation equipment monitoring;prototype testing;public hospital","","","3","","","","","","IEEE","IEEE Conferences"
"RMDN: New Approach to Maximize Influence Spread","Q. Hu; Y. Zhang; X. Xu; C. Li; C. Xing","NA; NA; NA; NA; NA","2015 IEEE 39th Annual Computer Software and Applications Conference","","2015","2","","702","711","Influence maximization modeling and analyzing is an important problem in Online Social Networks (OSNs). Influential nodes provide information on hot topics and forward interesting information, which can yield great influence on other nodes in OSNs. For word-of-mouth viral marketing, it is critical to find influential nodes within budgets. However, finding the optimal solution to maximize the influence spread in a given OSN has been proven to be NP-Hard. The existing algorithms suffer the following three defects: (1) need acquire the topological structure of the network, which is impractical for the continuously changing networks in real life, (2) are easy to get trapped in Rich Club phenomena, (3) can not balance very well between influence spread and running time. To solve this challenging problem, based on the randomly heuristic algorithm and the scale-free property of Complex Networks, we propose RMDN (Random Maximal Degree Neighbor) and its improved version RMDN++. We prove the feasibility and scalability of RMDNs in theory. Five real datasets are used to test the effectiveness and efficiency of RMDNs under two different influence diffusion models. The result shows that our methods have a comparable performance in terms of influence spread as state-of-the-art algorithms, but decrease the time by 1-2 orders of magnitude.","0730-3157","978-1-4673-6564-2978-1-4673-6563","10.1109/COMPSAC.2015.110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7273686","social network;information diffusion;influence maximization","Greedy algorithms;Computational modeling;Integrated circuit modeling;Heuristic algorithms;Algorithm design and analysis;Social network services","information dissemination;marketing;network theory (graphs);optimisation;social networking (online)","influence spread maximization;influence maximization modeling;online social networks;OSN;word-of-mouth viral marketing;topological network structure;rich club phenomena;random maximal degree neighbor;RMDN++;influence diffusion models","","","28","","","","","","IEEE","IEEE Conferences"
"Broadband pulsed high power amplifier design using load-pull technique","S. K. Garg; S. Aich; J. Dhar","MSTD/MSTG/MRSA, Space Applications Centre, ISRO, DOS, Ahmedabad -380015, India; MSTD/MSTG/MRSA, Space Applications Centre, ISRO, DOS, Ahmedabad -380015, India; MSTD/MSTG/MRSA, Space Applications Centre, ISRO, DOS, Ahmedabad -380015, India","2015 IEEE Applied Electromagnetics Conference (AEMC)","","2015","","","1","2","This paper illustrates the nonlinear design and realization of L-Band broadband pulsed high power amplifier to deliver peak output power of 45 W at centre frequency of 1.25 GHz with 20% bandwidth. To account for the non-linear behavior of devices under large signal conditions, the extracted load-pull data is used to design both stages of power amplifier. Power amplifier has been realized on a single 25-mil RT-duroid substrate (RT6010.2LM) with 1mm Cu-cladding at back. Paper covers load-pull parameter extraction, nonlinear broadband amplifier design using extracted load-pull data, verification, optimization of design using CAD software and finally, validation of the design procedure through test data of five amplifiers.","","978-1-4673-9536-6978-1-4673-9537","10.1109/AEMC.2015.7509248","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7509248","","Power amplifiers;Power generation;Reflection coefficient;Broadband amplifiers;Bandwidth;Data mining","microwave power amplifiers;technology CAD (electronics);wideband amplifiers","CAD software;design optimization;verification;nonlinear broadband amplifier design;load-pull parameter extraction;RT-duroid substrate;load-pull technique;broadband pulsed high power amplifier design;power 45 W;frequency 1.25 GHz","","1","4","","","","","","IEEE","IEEE Conferences"
"A Novel Packaging Structure for High Power LED Based on Chip on Heat-Sink Method","K. Pan; P. Huang; Y. Guo; T. Lu; B. Zhou","NA; NA; NA; NA; NA","2013 International Conference on Information Technology and Applications","","2013","","","436","440","Recently, LED lighting receives extensive attention and is put in practice in many respects with the great advantages of low power consumption, high luminous efficiency and long service life. However, it is confronted with lots of problems such as light failure and short service life caused by high temperature of LED chip. So the thermal management is one of the most important aspects of successful LED systems design. In this paper, firstly, a novel packaging structure with excellent capacity of heat dissipation for high power LED which is based on Chip on Heat-sink (COH) method was proposed and investigated. Secondly, some holes were dug in the Printed Circuit Board (PCB) according to the design of heat-sink structure to realize electrical connection between chips and PCB. Then, the heat distribution for three kinds of package structure was simulated by using the software ANSYS, respectively. The simulation result shows that heat dissipation potential of the packaging structure based on COH method is best of all. After that, the main dimension parameters of LED package structure were optimized with the application of Design of Experiment (DOE) and statistical analysis. Finally, samples of the packaging structure based on COH were made successfully. The parameters of thermal properties of the samples were tested by The Thermal Transient Tester (T3ster) in order to verify the accuracy of t simulation results.","","978-1-4799-2877-4978-1-4799-2876","10.1109/ITA.2013.106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6710022","System-In-Package;chip on board;thermal resistance","Light emitting diodes;Packaging;Thermal resistance;Electronic packaging thermal management;Resistance heating;Temperature distribution","chip-on-board packaging;design of experiments;electric connectors;finite element analysis;heat sinks;light emitting diodes;lighting;printed circuit design;printed circuit interconnections;system-on-package","T3ster;thermal transient tester;DOE;design of experiment;LED package structure dimension parameter optimization;COH method;heat dissipation potential;software ANSYS;heat distribution;PCB;electrical connection;heat sink structure;printed circuit board;high power LED lighting;LED systems design;thermal management;LED chip;service life;luminous efficiency;novel packaging structure","","","10","","","","","","IEEE","IEEE Conferences"
"Broadband rectangular high power divider/combiner","M. Ahmadzadeh; P. Rasekh; R. Safian; G. Askari; H. Mirmohammad-sadeghi","Isfahan University of Technology, Iran; Isfahan University of Technology, Iran; Isfahan University of Technology, Iran; Isfahan University of Technology, Iran; Isfahan University of Technology, Iran","IET Microwaves, Antennas & Propagation","","2015","9","1","58","63","In this study, a reciprocal broadband four-way spatial divider/combiner with coaxial probes entering a rectangular cavity is simulated, experimentally fabricated and tested. A combination of circuit model and full electromagnetic wave methods is used to simplify the design procedure by increasing the role of the circuit model and, in contrast, reducing the amount of full wave optimisation. The presented structure is compact and easy to fabricate. Keeping its return loss greater than 10 dB, the constructed combiner operates with a 100% bandwidth from 5 to 15 GHz. A through scenario is also analysed whereas both measured and simulated results indicate a negligible loss. The measurements are in good agreement with the simulations done by high frequency structural simulator software.","1751-8725;1751-8733","","10.1049/iet-map.2014.0089","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996084","","","power combiners;power dividers","HFSS software;wave optimisation;design procedure;electromagnetic wave methods;circuit model;rectangular cavity;coaxial probes;reciprocal broadband four-way spatial divider-combiner;broadband rectangular high power divider-combiner;bandwidth 5 GHz to 15 GHz","","5","19","","","","","","IET","IET Journals & Magazines"
"A hybrid fuzzy approach for landing of a quad-rotor MAV based on a novel vision localization method","S. N. B. Hashemi; A. Pasdar","Faculty of Aerospace, Malek Ashtar University, Tehran, Iran; School of Mechatronics, Faculty of Electrical engineering, Qazvin IAU University, Qazvin, Iran","2015 3rd RSI International Conference on Robotics and Mechatronics (ICROM)","","2015","","","767","771","The aim of this paper is to present a practical method to land a Quadrotor MAV (Micro Arial Vehicle) on a landing pad or charger dock station. Nowadays Quadrotor MAVs are very popular but due to nonlinear dynamics and ground effects problems landing of a Quadrotor MAV is a control challenge specially in outdoor environments. For this purpose a high level fuzzy controler and a low level PID stablizer along with fast and optimized vision localization method is proposed. to implement this method we built a low cost experimental robot platform hardware which were designed for outdoor environments and is also presented as well. The proposed algorithm also tested by SITL(Software In The Loop) method and the results of simulation prove the proposed algorithm's performance.","","978-1-4673-7234-3978-1-4673-7233","10.1109/ICRoM.2015.7367879","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367879","Quad-rotor;MAV;Fuzzy Control;Vision based localization;Autonomous landing","Cameras;Robots;Vehicles;Hardware;Algorithm design and analysis;Navigation;MATLAB","aircraft control;autonomous aerial vehicles;fuzzy control;helicopters;microrobots;nonlinear control systems;robot dynamics;robot vision;stability;three-term control","hybrid fuzzy approach;quadrotor MAV landing pad;microariel vehicle;charger dock station;nonlinear dynamics;ground effects;outdoor environments;high-level fuzzy controller;low-level PID stabilizer;optimized vision localization method;low-cost experimental robot platform hardware;SITL method;software-in-the-loop method","","","11","","","","","","IEEE","IEEE Conferences"
"Research of protection simulation on microwave weapons","G. Yuan-yuan","Postgraduate Brigade, Engineering University of CAPF, Xi'an 710086, China","2013 3rd International Conference on Consumer Electronics, Communications and Networks","","2013","","","513","516","This paper sets to research pointing at the threat of microwave weapons when the armed police officers and soldiers encounter in dealing with contingencies and maintaining the stability, and also discusses the shielding effectiveness of electromagnetic shielding fabric. With mental shield as the research object, use the genetic algorithm optimization principle and the HOBBIES software to simulate. Shielding effectiveness test is carried out among 3GHz~90GHz bandwidth in electromagnetic shielding fabric with single and double layer structure and then compared with the outcome of simulation.","","978-1-4799-2860-6978-1-4799-2859-0978-1-4799-2858","10.1109/CECNet.2013.6703382","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6703382","microwave weapons;protection;simulation;shielding effectiveness;genetic algorithm","Fabrics;Weapons;Wires;Microwave theory and techniques;Apertures;Electromagnetic shielding","electromagnetic shielding;fabrics;genetic algorithms;microwave devices;protection;weapons","protection simulation research;microwave weapons;armed police officers;armed soldiers;electromagnetic shielding fabric;genetic algorithm optimization principle;HOBBIES software;double layer structure;single layer structure;frequency 3 GHz to 90 GHz","","","5","","","","","","IEEE","IEEE Conferences"
"Intricacies in digital CMOS implementation of a reconfigurable fuzzy logic traffic light controller","A. Khalaji; S. Seyedtabaii","Electrical Engineering, Shahed University, Tehran, Iran; Electrical Engineering, Shahed University, Tehran, Iran","The 5th Conference on Information and Knowledge Technology","","2013","","","187","191","In this paper, the procedure of CMOS design of a reconfigurable fuzzy traffic light control that may work in a severe condition is discussed. The chip receives the membership degree parameters and traffic density indexes. The membership degree is allowed to have variable numbers of reconfigurable trapezoid patterns. Import/export of data is carried out through a serial link. The chip is designed using Very High Speed Integrated Circuit (VHSIC) Hardware Description Language (VHDL) implementing an optimized fuzzy traffic controller developed in [1]. Based on the traffic density, the algorithm decides whether to terminate the current green phase or to extend it for some more time for better intersection traffic handling. Design is tested versus full software implementation of the algorithm. The result exhibits successful hardware implementation. Then, the layout of the chip based on minimum die area is also derived.","","978-1-4673-6490-4978-1-4673-6489","10.1109/IKT.2013.6620062","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6620062","Fuzzy logic;Traffic light control;Fuzzy CMOS implementation;VHDL","Vehicles;CMOS integrated circuits;Hardware;Fuzzy logic;Indexes;Layout;Algorithm design and analysis","CMOS digital integrated circuits;fuzzy control;integrated circuit design;road traffic control","data import;green phase;minimum die area;hardware implementation;software implementation;traffic handling;optimized fuzzy traffic controller;VHDL;VHSIC;very high speed integrated circuit hardware description language;chip design;serial link;data export;reconfigurable trapezoid patterns;traffic density index;membership degree parameters;CMOS design;reconfigurable fuzzy logic traffic light controller;digital CMOS implementation","","2","7","","","","","","IEEE","IEEE Conferences"
"A Performance Analysis of SIMD Algorithms for Monte Carlo Simulations of Nuclear Reactor Cores","D. Ozog; A. D. Malony; A. R. Siegel","NA; NA; NA","2015 IEEE International Parallel and Distributed Processing Symposium","","2015","","","733","742","A primary characteristic of history-based Monte Carlo neutron transport simulation is the application of MIMD-style parallelism: the path of each neutron particle is largely independent of all other particles, so threads of execution perform independent instructions with respect to other threads. This conflicts with the growing trend of HPC vendors exploiting SIMD hardware, which accomplishes better parallelism and more FLOPS per watt. Event-based neutron transport suits vectorization better than history-based transport, but it is difficult to implement and complicates data management and transfer. However, the Intel Xeon Phi architecture supports the familiar ×86 instruction set and memory model, mitigating difficulties in vector zing neutron transport codes. This paper compares the event-based and history-based approaches for exploiting SIMD in Monte Carlo neutron transport simulations. For both algorithms, we analyze performance using the three different execution models provided by the Xeon Phi (offload, native, and symmetric) within the full-featured OpenVMS framework. A representative micro-benchmark of the performance bottleneck computation shows about 10x performance improvement using the event-based method. In an optimized history-based simulation of a full-physics nuclear reactor core in OpenVMS, the MIC shows a calculation rate 1.6x higher than a modern 16-core CPU, 2.5x higher when balancing load between the CPU and 1 MIC, and 4x higher when balancing load between the CPU and 2 Macs. As far as we are aware, our calculation rate per node on a high fidelity benchmark (17, 098 particles/second) is higher than any other Monte Carlo neutron transport application. Furthermore, we attain 95% distributed efficiency when using MPI and up to 512 concurrent MIC devices.","1530-2075","978-1-4799-8649","10.1109/IPDPS.2015.105","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7161560","Monte Carlo;neutron transport;reactor simulation;performance;SIMD;Intel Xeon Phi coprocessor;MIC","Microwave integrated circuits;Banking;Neutrons;Computational modeling;Benchmark testing;Monte Carlo methods;Inductors","concurrency control;fission reactors;fusion reactors;instruction sets;Monte Carlo methods;neutron transport theory;nuclear engineering computing;parallel processing;software performance evaluation","performance analysis;SIMD algorithms;Monte Carlo simulations;nuclear reactor cores;history-based Monte Carlo neutron transport simulation;MIMD-style parallelism;neutron particle;HPC vendors;event-based neutron transport suits vectorization;data management;Intel Xeon Phi architecture;×86 instruction set;memory model;vector zing neutron transport codes;event-based approach;history-based approach;Monte Carlo neutron transport simulations;OpenVMS framework;representative microbenchmark;performance bottleneck computation;event-based method;optimized history-based simulation;full-physics nuclear reactor core;CPU;Macs;MPI;concurrent MIC devices","","1","19","","","","","","IEEE","IEEE Conferences"
"Evaluating Architecture-Dependent Linux Performance","L. Mogosanu; M. Carabas; C. Condurache; L. Gheorghe; N. Tapus","NA; NA; NA; NA; NA","2015 20th International Conference on Control Systems and Computer Science","","2015","","","499","505","Modern operating system kernels, such as Linux, address the trade-off between portability and performance by exposing a generic interface to user space programs, while maintaining architecture-dependent functionality as a set of separate components inside the kernel space. In particular, performance can only be achieved by ensuring that the architecture-dependent code takes advantage of the facilities offered by the underlying hardware. In turn, architecture-dependent optimization requires tools to observe the system's behaviour and evaluate its performance. In this paper we analyze the state-of-the art performance measurement frameworks for the Linux kernel. We describe our experience with said frameworks in a scenario comprising a Linux kernel running Para virtualized on top of an L4 micro kernel, and assess their suitability for our scenario by using them to uncover a set of particular performance issues. In this respect we demonstrate their effectiveness through a detailed evaluation and propose an approach to generalize their usage for evaluating architecture-dependent performance.","2379-0474;2379-0482","978-1-4799-1780-8978-1-4799-1779","10.1109/CSCS.2015.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7168474","Performance Evaluation;Operating Systems;Linux;Virtualization","Linux;Kernel;Hardware;Performance evaluation;Benchmark testing;Radiation detectors","computer architecture;Linux;operating system kernels;software performance evaluation;virtualisation","architecture-dependent Linux performance evaluation;operating system kernel;portability;user space program;architecture-dependent functionality;kernel space;architecture-dependent code;architecture-dependent optimization;system behaviour;performance measurement;Linux kernel;Para;virtualization;L4 microkernel;performance issues","","","26","","","","","","IEEE","IEEE Conferences"
"Load Balancing of Java Applications by Forecasting Garbage Collections","A. O. Portillo-Dominguez; M. Wang; D. Magoni; P. Perry; J. Murphy","NA; NA; NA; NA; NA","2014 IEEE 13th International Symposium on Parallel and Distributed Computing","","2014","","","127","134","Modern computer applications, especially at enterprise-level, are commonly deployed with a big number of clustered instances to achieve a higher system performance, in which case single machine based solutions are less cost-effective. However, how to effectively manage these clustered applications has become a new challenge. A common approach is to deploy a front-end load balancer to optimise the workload distribution between each clustered application. Since then, many research efforts have been carried out to study effective load balancing algorithms which can control the workload based on various resource usages such as CPU and memory. The aim of this paper is to propose a new load balancing approach to improve the overall distributed system performance by avoiding potential performance impacts caused by Major Java Garbage Collection. The experimental results have shown that the proposed load balancing algorithm can achieve a significant higher throughput and lower response time compared to the round-robin approach. In addition, the proposed solution only has a small overhead introduced to the distributed system, where unused resources are available to enable other load balancing algorithms together to achieve a better system performance.","2379-5352","978-1-4799-5919-8978-1-4799-5918","10.1109/ISPDC.2014.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900210","","Java;Resource management;Load management;Measurement;Prediction algorithms;Benchmark testing;Iron","distributed processing;Java;resource allocation;software performance evaluation;storage management","load balancing approach;Java applications;computer applications;front-end load balancer;workload distribution optimisation;CPU;overall distributed system performance improvement;major Java garbage collection;response time;enterprise applications","","3","18","","","","","","IEEE","IEEE Conferences"
"Adaptive GC-Aware Load Balancing Strategy for High-Assurance Java Distributed Systems","A. O. Portillo-Dominguez; M. Wang; J. Murphy; D. Magoni","NA; NA; NA; NA","2015 IEEE 16th International Symposium on High Assurance Systems Engineering","","2015","","","68","75","High-Assurance applications usually require achieving fast response time and high throughput on a constant basis. To fulfil these stringent quality of service requirements, these applications are commonly deployed in clustered instances. However, how to effectively manage these clusters has become a new challenge. A common approach is to deploy a front-end load balancer to optimise the workload distribution among the clustered applications. Thus, researchers have been studying how to improve the effectiveness of a load balancer. Our previous work presented a novel load balancing strategy which improves the performance of a distributed Java system by avoiding the performance impacts of Major Garbage Collection, which is a common cause of performance degradation in Java applications. However, as that strategy used a static configuration, it could only improve the performance of a system if the strategy was configured with domain expert knowledge. This paper extends our previous work by presenting an adaptive GC-aware load balancing strategy which self-configures according to the GC characteristics of the application. Our results have shown that this adaptive strategy can achieve higher throughput and lower response time, compared to the round-robin load balancing, while also avoiding the burden of manual tuning.","1530-2059","978-1-4799-8111-3978-1-4799-8110","10.1109/HASE.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7027416","High-Assurance Systems;Performance and Reliability;Java Garbage Collection","Load management;Java;Memory management;Benchmark testing;Prediction algorithms;Resource management;History","distributed processing;Java;quality of service;resource allocation;software quality;storage management","adaptive GC-aware load balancing strategy;high-assurance Java distributed systems;quality of service requirements;front-end load balancer;workload distribution optimisation;performance improvement;distributed Java system;performance impact avoidance;major garbage collection;performance degradation;Java applications;static configuration;domain expert knowledge","","6","26","","","","","","IEEE","IEEE Conferences"
"Bungee jumps: Accelerating indirect branches through HW/SW co-design","D. S. McFarlin; C. Zilles","Carnegie Mellon University; University of Illinois at Urbana-Champaign","2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)","","2015","","","370","382","Indirect branches have historically been a challenge for microarchitectures and code generators alike. The recent steady increase in indirect branch predictability has translated into continual performance improvements especially for Out-of-Order processors which benefit more readily from improvements in branch prediction. In contrast, in-order processors which rely on code generators for performance are still challenged by indirect branches; they are a frequent source of issue stalls and the large number of indirect branch targets and unbiased nature of indirect branches complicate the use of traditional branch handling techniques like assert conversion and predication. To address these limitations, we propose an ISA enhancement with associated code transformation and hardware support that collectively enable the current trend of improved indirect branch predictability to be directly leveraged by code-generators for in-orders. By separating the prediction point of an indirect branch from its resolution point, we enable code generators to emit schedules which more readily match those found by the Out-of-Order. Our technique is particularly beneficial to those processors which leverage dynamic binary translation and optimization such as Transmeta's Efficeon and more recently Nvidia's Project Denver. On a set of indirect branch intensive benchmarks from SPEC 2006, 2000 and 95, we achieve a Geomean speedup on a 4-wide of 11%. We further demonstrate speedups of 23% and 14% speedup on PHP and Python benchmarks.","2379-3155","978-1-4503-4034-2978-1-5090-6601","10.1145/2830772.2830781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7856612","","Generators;Out of order;Benchmark testing;Schedules;Prefetching;Hardware","hardware-software codesign;program compilers","optimization;dynamic binary translation;code generators;improved indirect branch predictability;Out-of-Order processors;HW-SW co-design;bungee jumps","","3","40","","","","","","IEEE","IEEE Conferences"
"Evaluating performance of cloud computing environments","A. Nagy; B. Kovari","Budapest University of Technology and Economics, Department of Automation and Applied Informatics, Budapest, Hungary; Budapest University of Technology and Economics, Department of Automation and Applied Informatics, Budapest, Hungary","2013 IEEE 14th International Symposium on Computational Intelligence and Informatics (CINTI)","","2013","","","267","272","The emerging technology of cloud computing offers off-premise, reliable solutions for common IT related needs of enterprises, but as different providers often envision these solutions in a different way, architects face a complex problem when trying to decide which provider to choose and compare or estimate the costs of different options. This paper presents a generalized, provider-neutral service and cost model to describe the capabilities of different environments and the prices associated with them. This model can serve as the basis for mathematical optimization algorithms.","","978-1-4799-0197-5978-1-4799-0194","10.1109/CINTI.2013.6705204","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6705204","","Benchmark testing;Cloud computing;Computational modeling;Mathematical model;Analytical models;Pricing;Data transfer","cloud computing;pricing;software performance evaluation;virtual machines","cloud computing environments;IT related enterprise needs;generalized provider-neutral service;cost model;mathematical optimization algorithms;performance evaluation;virtual machines;VM","","","15","","","","","","IEEE","IEEE Conferences"
"Toward variability management to tailor high dimensional index implementations","V. Köppen; M. Schäler; R. Schröter","Faculty of Computer Science, Otto von Guericke University, Magdeburg, Germany; Faculty of Computer Science, Otto von Guericke University, Magdeburg, Germany; Faculty of Computer Science, Otto von Guericke University, Magdeburg, Germany","2014 IEEE Eighth International Conference on Research Challenges in Information Science (RCIS)","","2014","","","1","6","The increasing amount of complex data requires a solution to store and query these data efficiently. One possibility to speed-up various query types is the application of high dimensional index structures. In prior work, we introduced QuEval as platform to evaluate these indexes for user-defined use cases. Our design allows to easily extend QuEval with new index structure implementations. However, based on our experiences, we encountered severe challenges by tailoring index structure implementations to specific use cases. In particular, we face challenges to manage several similar implementation variants of the same index. In this paper, we consequently show benefits and drawbacks that emphasize the necessity to tailor index structure implementations with the help of a short evaluation study. Finally, we outline approaches for adequate variability management to address the aforementioned drawbacks.","2151-1349;2151-1357","978-1-4799-2393","10.1109/RCIS.2014.6861069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6861069","High-Dimensionality;Software Engineering;Index Structures;Databases","Indexes;Time factors;Optimization;Benchmark testing;Memory management;Sociology;Statistics","data warehouses;query processing;storage management","data warehouses;user-defined use cases;QuEval;data querying;data storage;complex data;high dimensional index structure implementations;variability management","","1","19","","","","","","IEEE","IEEE Conferences"
"Density K-means: A new algorithm for centers initialization for K-means","X. Lan; Q. Li; Y. Zheng","College of Computer, National University of Defense, Changsha China, 410073; School of Economics, Minzu University of China, Beijing China, 100083; College of Computer, National University of Defense, Changsha China, 410073","2015 6th IEEE International Conference on Software Engineering and Service Science (ICSESS)","","2015","","","958","961","K-means is one of the most significant clustering algorithms in data mining. It performs well in many cases, especially in the massive data sets. However, the result of clustering by K-means largely depends upon the initial centers, which makes K-means difficult to reach global optimum. In this paper, we developed a novel algorithm based on finding density peaks to optimize the initial centers for K-means. In the experiment, together with our algorithm, nine different clustering algorithms were extensively compared on four well-known test data sets. According to our experimental results, the performance of our algorithm is significantly better than other eight algorithms, which indicates that it is a valuable method to select initial center for K-means.","2327-0594;2327-0586","978-1-4799-8353-7978-1-4799-8352-0978-1-4799-8351","10.1109/ICSESS.2015.7339213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339213","K-means;Initial cluster centers;Density peaks","Clustering algorithms;Couplings;Iris;Computational complexity;Computers;Data mining;Refining","data mining;pattern clustering","center initialization;data mining;K-means clustering algorithm;initial cluster center;density peak","","1","17","","","","","","IEEE","IEEE Conferences"
"High-resolution online power monitoring for modern microprocessors","F. Oboril; J. Ewert; M. B. Tahoori","Chair of Dependable Nano Computing (CDNC), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Chair of Dependable Nano Computing (CDNC), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Chair of Dependable Nano Computing (CDNC), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany","2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2015","","","265","268","The power consumption of computing systems is nowadays a major design constraint that affects performance and reliability. To co-optimize these aspects, fine-grained adaptation techniques at runtime are of growing importance. However, to use these tools efficiently, fine-grained information about the power consumption of various on-chip components at runtime is required. Therefore, here we propose a novel software-implemented high-resolution (spatial and temporal) power monitoring approach that relies on micro-models to estimate the power consumption of all microarchitectural components inside a processor core. Combined with a self-calibration technique that uses an available on-chip power sensor, our power estimation approach can achieve an accuracy of more than 99 % and provides deep insights about the power dissipation inside a processor core during workload execution.","1530-1591;1558-1101","978-3-9815-3705-5978-3-9815-3704","10.7873/DATE.2015.0208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7092393","","Power demand;Estimation;Microarchitecture;Monitoring;Runtime;Spatial resolution;System-on-chip","calibration;microprocessor chips;power aware computing","high-resolution online power monitoring;modern microprocessors;fine-grained adaptation technique;software-implemented high-resolution power monitoring approach;processor core;self-calibration technique;on-chip power sensor;power dissipation","","1","19","","","","","","IEEE","IEEE Conferences"
"Teaching ΔΣ modulators with PyDSM and scientific Python","S. Callegari; F. Bizzarri","ARCES/DEI, University of Bologna, Italy; DEIB, Politecnico di Milano, Italy","2015 IEEE International Symposium on Circuits and Systems (ISCAS)","","2015","","","1802","1805","This paper reports on the introduction in circuits and systems education of a software toolbox named PyDSM, dedicated to the experimentation on ΔΣ modulators with particular emphasis on their noise shaping features, digital implementations and simulation. The toolbox has been tried on field for a half day tutorial at the ICECS 2014 conference, and is being gently proposed to students of the Master Degree in Electronics and Telecommunications Engineering at the University of Bologna. In addition to the value that the toolbox has in itself, it is also the premise to test, on a restricted topic, the potential of scientific Python in circuits and systems education.","0271-4302;2158-1525","978-1-4799-8391","10.1109/ISCAS.2015.7169005","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169005","","Modulation;Optimization;Software;Education;Programming profession;Psychoacoustic models","computer aided instruction;delta-sigma modulation;electronic engineering education","delta-sigma modulators;PyDSM;scientific Python;software toolbox;noise shaping features;Master Degree in Electronics and Telecommunications Engineering;University of Bologna;systems education","","","23","","","","","","IEEE","IEEE Conferences"
"Study in performance analyses of hospital emergency services: Case Habib Bourguiba hospital","S. Hajer; B. Mounir","University of Sfax, LOGIQ Laboratory, Sfax, Tunisia; University of Sfax, OASIS Laboratory, ENIT, Tunis, Tunisia","2013 International Conference on Advanced Logistics and Transport","","2013","","","500","505","This work deals with the problem of the minimization of the waiting time for patients at the emergency services of Habib Bourguiba Hospital in Sfax. The performance evaluation of a real system consists of a modeling stage from system to model and a stage of analysis of the performance of the mode. Then, we moved on the analysis though the simulation technique. Simulation is useful for modeling the complex flow of patients and for testing the scenarios resulting from changing certain parametres. In this research, we have chosen the simulation of the flow of patients by using the software Arena.","","978-1-4799-0313-9978-1-4799-0314-6978-1-4799-0312","10.1109/ICAdLT.2013.6568509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6568509","Performance;Logistics;hospital structure of emergency (SU);modeling;simulation","Hospitals;Analytical models;Analysis of variance;Logistics;Optimization;Software","emergency services;hospitals","performance analyses;hospital emergency services;Habib Bourguiba hospital;waiting time minimization;mode performance;simulation technique;complex patients flow;software Arena","","","14","","","","","","IEEE","IEEE Conferences"
"Accurate 3-D capacitance extractions for advanced nanometer CMOS nodes","K. Chang; S. Lee; K. Lee; P. Yuh; H. Yu; W. Huang; V. C. Y. Chang","Research and Development, Taiwan Semiconductor Manufacturing Company; Research and Development, Taiwan Semiconductor Manufacturing Company; Research and Development, Taiwan Semiconductor Manufacturing Company; Research and Development, Taiwan Semiconductor Manufacturing Company; Research and Development, Taiwan Semiconductor Manufacturing Company; Research and Development, Taiwan Semiconductor Manufacturing Company; Research and Development, Taiwan Semiconductor Manufacturing Company","VLSI Design, Automation and Test(VLSI-DAT)","","2015","","","1","4","During the R&amp;D of advanced nanometer CMOS technologies such as 20nm and beyond, we implemented in-house 3-D capacitance extraction software to provide R&amp;D engineers with an accurate modeling tool to optimize the complex 3-D nanometer dimensions and materials that may be used for competitive CMOS devices in terms of power consumption, performance, and area. Our extractor solves 3-D Laplace's equation and extracts capacitances and resistances targeting accurate on-chip parasitic modeling. In essence, the numerical method we adopted features flexible grids for arbitrary shapes in nanometer CMOS devices. Robust and rigorous algorithms are described that allow the R&amp;D engineers to monitor the convergence and specify the corresponding accuracy level based on the resource and allowed turnaround time.","","978-1-4799-6275","10.1109/VLSI-DAT.2015.7114565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7114565","","Finite element analysis;Capacitance;Mathematical model;Semiconductor device modeling;CMOS integrated circuits;Conductors;Numerical models","CMOS integrated circuits;integrated circuit modelling;Laplace equations;nanoelectronics","advanced nanometer CMOS nodes;in-house 3D capacitance extraction software;power consumption;3D Laplace equation;on-chip parasitic modeling;numerical method;size 20 nm","","1","15","","","","","","IEEE","IEEE Conferences"
"Performance analysis of current parallel programming models for many-core systems","Y. Cao; Baodong Wu; Yongcai Tao; Lei Shi","School of Software Technology, Zhengzhou University, China; School of Information Engineering, Zhengzhou University, China; School of Information Engineering, Zhengzhou University, China; School of Information Engineering, Zhengzhou University, China","2013 8th International Conference on Computer Science & Education","","2013","","","132","135","Recent developments in microprocessor design show a clear trend towards multi-core and many-core architectures. Nowadays, processors consisting of dozens of general-purpose cores are already available in the market, and with the rapid development of semiconductor technology and increasing computing demand, it will be very common to have a processor consisting of hundreds or even thousands of cores in the near future. This kind of processors is commonly referred as many-core processors. In the many-core processor era, however, exploiting all the advantages offered by these processors will not be trivial. In this paper, we focus on studying and analyzing several typical parallel programming models for many-core systems, and summarize the common performance-optimized problems faced by current parallel programming models.","","978-1-4673-4463-0978-1-4673-4464-7978-1-4673-4462","10.1109/ICCSE.2013.6553897","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553897","Many-core processor;Parallel programming model;Performance analysis","Computational modeling;Benchmark testing;Computers;Hardware;Jacobian matrices;Heating","multiprocessing systems;parallel programming;performance evaluation","microprocessor design;general-purpose core;semiconductor technology;many-core processor;parallel programming model;many-core systems;performance-optimized problem;many-core architecture;multicore architecture;performance analysis","","","12","","","","","","IEEE","IEEE Conferences"
"Automatic Runtime Customization for Variability Awareness on Multicore Platforms","G. Ayad; R. Nittala; R. Lemaire","NA; NA; NA","2015 IEEE 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip","","2015","","","143","150","Driven by increasingly aggressive CMOS technology scaling, sub-wavelength lithography is incurring more evident variability in the technology parameters of the semiconductors fabrication process. That variability results in otherwise identical designs displaying very different performances, power consumption levels and lifespans once fabricated. Hence, process variability may lead to execution uncertainties, impacting the expected quality of service and energy efficiency of the running software. As such uncertainties are intolerable in certain application domains such as automotive and avionic infotainment systems, it has become a persistent necessity to customize runtime engines to introduce measures for variability awareness in task allocation decisions. The purpose of compensating process variability is to avoid performance degradation and energy inefficiency. And customization is meant to take place automatically through exporting the variability-impacted platform characteristics - such as per-core manufactured clock frequency - for the runtime library to perform variability-aware workload sharing on the target cores of the hardware platform. Hence, we can eventually achieve noticeable optimization results, not only on the system performance and energy consumption levels, but also in increasing productivity in systems development, testing, integration, and marketing. This paper presents a holistic approach starting from a system model of the target multicore platform, to building and integrating the runtime library, and finally highlighting the optimization results achieved through the proposed runtime customization paradigm.","","978-1-4799-8670-5978-1-4799-8669","10.1109/MCSoC.2015.19","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328198","","Hardware;Multicore processing;Unified modeling language;Runtime;Resource management;Runtime library;Software","CMOS integrated circuits;energy consumption;lithography;microprocessor chips;multiprocessing systems;power aware computing;software libraries","automatic runtime customization;variability awareness;multicore platforms;aggressive CMOS technology scaling;subwavelength lithography;semiconductors fabrication process;power consumption levels;energy efficiency;automotive systems;avionic infotainment systems;runtime engines;task allocation decisions;performance degradation;energy inefficiency;per-core manufactured clock frequency;runtime library;variability-aware workload sharing;energy consumption levels;multicore platform;runtime library;runtime customization paradigm","","2","18","","","","","","IEEE","IEEE Conferences"
"Modeling and performance of contact-free discharge systems for space inertial sensors","T. Ziegler; P. Bergner; G. Hechenblaikner; N. Brandt; W. Fichter","Airbus Defence and Space, Science Programs Department (AED41), 88039 Friedrichshafen, Germany; Airbus Defence and Space, Science Programs Department (AED41), 88039 Friedrichshafen, Germany; Airbus Defence and Space, Science Programs Department (AED41), 88039 Friedrichshafen, Germany; Airbus Defence and Space, Science Programs Department (AED41), 88039 Friedrichshafen, Germany; Institute of Flight Mechanics and Control, University of Stuttgart, Pfaffenwaldring 7a, 70569 Stuttgart, Germany","IEEE Transactions on Aerospace and Electronic Systems","","2014","50","2","1493","1510","This article presents a detailed overview and assessment of contact-free ultraviolet light discharge systems (UVDSs) needed to control the variable electric charge level of free-flying test masses, which are part of high-precision inertial sensors in space. A comprehensive numerical analysis approach on the basis of experimental data is detailed. This includes ultraviolet light ray tracing, the computation of time variant electric fields inside the complex inertial sensor geometry, and the simulation of individual photoelectron trajectories. Subsequent data analysis allows determination of key parameters to set up an analytical discharge model. Such a model is an essential system engineering tool needed for requirement breakdown and subsystem specification, performance budgeting, onboard charge control software development, and instrument modeling within spacecraft end-to-end performance simulators. Different types of UVDS design concepts are presented and assessed regarding their robustness and performance. Critical hardware aspects like electron emission from air-contaminated surfaces, interfaces with other subsystems, and spacecraft operations are considered. The focus is on the modeling and performance evaluation of the existing UVDS onboard the LISA Pathfinder, a European Space Agency technology demonstrator spacecraft, which is in implementation phase. The results have motivated the design of a more robust discharge system concept for cubical test mass inertial sensors for future space missions. The analysis tools developed have been used for design optimization and performance assessment of the proposed design. A significant improvement of relevant robustness and performance figures has been achieved.","0018-9251;1557-9603;2371-9877","","10.1109/TAES.2014.120661","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850170","","Sensors;Discharges (electric);Surface discharges;Noise;Acceleration;Surface contamination;Space vehicles","data analysis;electric charge;electric fields;electron emission;inertial systems;ray tracing;sensors;space vehicle electronics","cubical test mass inertial sensors;european space agency technology demonstrator spacecraft;LISA pathfinder;air-contaminated surfaces;electron emission;spacecraft end-to-end performance simulator;charge control software development;data analysis;photoelectron trajectories;complex inertial sensor geometry;time variant electric fields;ultraviolet light ray tracing;high-precision inertial sensors;free-flying test masses;variable electric charge;UVDS;contact-free ultraviolet light discharge systems;space inertial sensors;contact-free discharge systems","","6","47","","","","","","IEEE","IEEE Journals & Magazines"
"An open and configurable embedded system for EMG pattern recognition implementation for artificial arms","J. Liu; F. Zhang; H. H. Huang","UNC/NCSU Joint Department of Biomedical Engineering, NC State University, Raleigh, NC 27606 USA; UNC/NCSU Joint Department of Biomedical Engineering, NC State University, Raleigh, NC 27606 USA; UNC/NCSU Joint Department of Biomedical Engineering, NC State University, Raleigh, NC 27606 USA","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2014","","","4095","4098","Pattern recognition (PR) based on electromyographic (EMG) signals has been developed for multifunctional artificial arms for decades. However, assessment of EMG PR control for daily prosthesis use is still limited. One of the major barriers is the lack of a portable and configurable embedded system to implement the EMG PR control. This paper aimed to design an open and configurable embedded system for EMG PR implementation so that researchers can easily modify and optimize the control algorithms upon our designed platform and test the EMG PR control outside of the lab environments. The open platform was built on an open source embedded Linux Operating System running a high-performance Gumstix board. Both the hardware and software system framework were openly designed. The system was highly flexible in terms of number of inputs/outputs and calibration interfaces used. Such flexibility enabled easy integration of our embedded system with different types of commercialized or prototypic artificial arms. Thus far, our system was portable for take-home use. Additionally, compared with previously reported embedded systems for EMG PR implementation, our system demonstrated improved processing efficiency and high system precision. Our long-term goals are (1) to develop a wearable and practical EMG PR-based control for multifunctional artificial arms, and (2) to quantify the benefits of EMG PR-based control over conventional myoelectric prosthesis control in a home setting.","1094-687X;1558-4615","978-1-4244-7929","10.1109/EMBC.2014.6944524","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944524","EMG pattern recognition;open and configurable design;embedded system;artificial arms","Electromyography;Embedded systems;Prosthetics;Real-time systems;Training;Pattern recognition;Control systems","artificial limbs;electromyography;embedded systems;medical signal processing;pattern recognition","open system;configurable embedded system;EMG pattern recognition implementation;electromyographic signals;multifunctional artificial arms;EMG PR control assessment;daily prosthesis use;portable system;EMG PR implementation;control algorithms;designed platform;lab environments;open platform;open source;Linux Operating System;high-performance Gumstix board;hardware system framework;software system framework;calibration interfaces;flexibility;embedded system integration;prototypic artificial arms;system precision;wearable EMG PR-based control;practical EMG PR-based control;conventional myoelectric prosthesis control;home setting","Algorithms;Artificial Limbs;Electromyography;Humans;Movement;Pattern Recognition, Automated;Software","","15","","","","","","IEEE","IEEE Conferences"
"Development of a new piezoelectric sensor to measure loads","S. Engelsberger; A. Harasim; T. Schmidt","Hochschule Landshut, Landshut, Germany; Hochschule Landshut, Landshut, Germany; CeramTec, Lauf an der Pegnitz, Germany","2015 European Microelectronics Packaging Conference (EMPC)","","2015","","","1","5","The aim of the project, which includes the development of a new sensor, is to create a completely encapsulated sensor system, which measures the load on a patient's legs and feet and transfers the data wirelessly to a watch. Therefore it was necessary to develop and test a novel type of sensor. The focus here was on reliability - it will be used on a medical device-and minimal energy consumption. Together with our project partners we decided to use a piezoelectric sensor. The analysis measures the change of capacity caused by the applied load. By gradually changing the parameters thickness, diameter, material and polarization, the sensor was optimized. In addition the hysteresis behavior and temperature dependence was examined. The interdependence between time, heavy loads and capacity was found and had to be compensated. Therefore different ideas for compensating hardware and software were developed, tested and analyzed. Furthermore a suitable case had to be developed and studied, which could prestress the sensor without taking away too much of the load applied to the sensor.","","978-0-9568-0862","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390695","","Temperature measurement;Prototypes;Temperature sensors;Force;Europe;Microelectronics;Packaging","biomedical measurement;biomedical transducers;piezoelectric transducers;reliability;test equipment","piezoelectric sensor;load measurement;encapsulated sensor system;reliability;medical device;minimal energy consumption;hysteresis behavior;temperature dependence","","","","","","","","","IEEE","IEEE Conferences"
"Early partial evaluation in a JIT-compiled, retargetable instruction set simulator generated from a high-level architecture description","H. Wagstaff; M. Gould; B. Franke; N. Topham","Institute for Computing Systems Architecture, School of Informatics, University of Edinburgh, UK; Institute for Computing Systems Architecture, School of Informatics, University of Edinburgh, UK; Institute for Computing Systems Architecture, School of Informatics, University of Edinburgh, UK; Institute for Computing Systems Architecture, School of Informatics, University of Edinburgh, UK","2013 50th ACM/EDAC/IEEE Design Automation Conference (DAC)","","2013","","","1","6","Modern processor design tools integrate in their workflows generators for instruction set simulators (Iss) from architecture descriptions. Whilst these generated simulators are useful for design evaluation and software development, they suffer from poor performance. We present an ultra-fast JIT-compiled Iss generated from an ARCHC description. We also introduce a novel partial evaluation optimisation, which further improves JIT compilation time and code quality. This results in a simulation rate of 510MiPs for an ARM target across 45 EEMBC and SPEC benchmarks. On average, our Iss is 1.7 times faster than SIMIT-ARM, one of the fastest Iss generated from an architecture description.","0738-100X;0738-100X","978-1-4503-2071","10.1145/2463209.2488760","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6560614","","Benchmark testing;Optimization;Generators;Heuristic algorithms;Computer architecture;Standards;Decoding","digital simulation;instruction sets;just-in-time;program compilers","partial evaluation;JIT-compiled instruction set simulator;retargetable instruction set simulator;high level architecture description;modern processor design tools;workflow generator;software development;JIT compilation time;code quality;SPEC benchmark;SIMIT-ARM;High Level Architecture description","","7","14","","","","","","IEEE","IEEE Conferences"
"A high precision EEG acquisition system based on the CompactPCI platform","J. Chen; X. Li; X. Mi; S. Pan","School of Life Science, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China; School of Life Science, Beijing Institute of Technology, Beijing, China","2014 7th International Conference on Biomedical Engineering and Informatics","","2014","","","511","516","Electroencephalography (EEG) is the recording of electrical activities generated by nerve cells in the cerebral cortex. As the only non-invasive method for measuring brain activities from human scalp, it has been widely used in the area of medical diagnosis, neuroscience research, commercial application, etc. Most of the existing EEG acquisition systems, which are usually comprised of analog switches, operational amplifiers, analog-digital converters and specific instruments, have the weakness of high expenses, limited accuracy and difficult to update. Aiming at the need of cognitive science research and brain computer interface (BCI) design, a high precision EEG acquisition system based on the compact peripheral component interconnect (CompactPCI) platform was designed in this paper. A commercial EEG hat with adjustable electrodes was used to collect interested EEG signals. And ADS1299, a high resolution analogy front end IC proposed by Texas Instruments, was adopted in the system to acquire and convert EEG signals. To facilitate hardware expansion, system integration and data processing, the device was designed in the form of a 3U CompactPCI card and connected to an embedded computer. A graphical user interface (GUI) software was provided to control the data acquiring, storing and processing. In addition, the least mean squares (LMS) adaptive filter was realized to optimize the signals. The compact and highly integrated analogy circuit in this design significantly avoided the contamination of signals by environmental noises and reduced device-to-device connection complexity. Meanwhile, the system structure based on the CompactPCI platform made it easier to adjust signal channels and evaluate novel algorithms. The accuracy tests and alpha wave detection experiments showed that our EEG acquisition system had high accuracy and was applicable to various EEG-related experiments. The research in this paper may have reference value for the development of portable EEG acquisition system and BCI device.","1948-2914;1948-2922","978-1-4799-5838-2978-1-4799-5837","10.1109/BMEI.2014.7002828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7002828","electroencephalography;acquisition system;multi-lead;ADS1299;CompactPCI","Electroencephalography;Software;Field programmable gate arrays;Computers;Electrodes;Least squares approximations;Adaptive filters","adaptive filters;analogue-digital conversion;biomedical electrodes;brain-computer interfaces;cognition;data acquisition;electroencephalography;embedded systems;graphical user interfaces;least mean squares methods;medical signal detection;neural nets;neurophysiology;operational amplifiers;signal denoising","high precision EEG acquisition system;CompactPCI platform;electroencephalography;electrical activities;nerve cells;cerebral cortex;noninvasive method;brain activities;human scalp;medical diagnosis;neuroscience research;commercial application;EEG acquisition systems;analog switches;operational amplifiers;analog-digital converters;specific instruments;cognitive science research;brain computer interface design;compact peripheral component interconnect platform;commercial EEG hat;adjustable electrodes;ADS1299;high resolution analogy front end IC;Texas Instruments;EEG signal acquistion;EEG signal convertion;hardware expansion;system integration;data processing;3U CompactPCI card;embedded computer;graphical user interface software;GUI;data acquiring;data storing;least mean squares adaptive filter;LMS;highly integrated analogy circuit;signal contamination;environmental noises;reduced device-to-device connection complexity;system structure;signal channels;accuracy tests;alpha wave detection experiments;EEG-related experiments;portable EEG acquisition system;BCI device","","4","13","","","","","","IEEE","IEEE Conferences"
"Compiler-driven dynamic reliability management for on-chip systems under variabilities","S. Rehman; F. Kriebel; M. Shafique; J. Henkel","Chair for Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Germany; Chair for Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Germany; Chair for Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Germany; Chair for Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Germany","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2014","","","1","4","This paper presents a novel Dynamic Reliability Management System (DyReMS) for on-chip systems that performs resilience-driven resource allocation and mapping. It accounts for both the tasks' resilience properties and heterogeneous error recovery features of different cores. DyReMS also chooses a reliable task version (out of multiple reliability-aware transformed options) depending upon the reliability level of the allocated core. In case of error detection, rollbacks are performed. Our system provides 70%-87% improved task reliability compared to a timing reliability-optimizing core assignment, i.e. minimizing the probability of deadline misses (with EDF scheduling).","1530-1591;1558-1101","978-3-9815370-2","10.7873/DATE.2014.119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800320","Reliability;Dependability;Soft Errors;Aging;Process Variations;Compiler;Run-Time Management","Software reliability;Resilience;Reliability engineering;Aging;Timing;Computer architecture","integrated circuit reliability;microprocessor chips;program compilers;resource allocation","compiler-driven dynamic reliability management;DyReMS;on-chip systems;variabilities;resilience-driven resource allocation;mapping;tasks resilience properties;heterogeneous error recovery features;allocated core;error detection;rollbacks","","","25","","","","","","IEEE","IEEE Conferences"
"Competence transfer through enterprise mobile application development","S. Vojvodić; M. Zović; V. Režić; H. Maračić; M. Kusek","Ericsson Nikola Tesla, Zagreb, Croatia; Ericsson Nikola Tesla, Zagreb, Croatia; Ericsson Nikola Tesla, Zagreb, Croatia; University of Zagreb / Faculty of Electrical Engineering and Computing, Zagreb, Croatia; University of Zagreb / Faculty of Electrical Engineering and Computing, Zagreb, Croatia","2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","","2014","","","448","452","Large world corporations need corresponding information technology (IT) support as well as constant improvement of software tools, which should enable further business development and more efficient work to operational organizations. The main interest of IT support organizations is currently more and more connected with mobile IT equipment of the employees. Specific business mobile applications improve the efficiency and result in new operating possibilities. The cooperation between Ericsson Nikola Tesla (ENT) and Faculty of Electrical Engineering and Computing (FER) of the Zagreb University, has enabled a fast development of competencies in ENT, necessary for mobile application development and quality realization of innovative solution for platforms iOS and Android. In this cooperation at the project realization the methodology of application development was defined, the corresponding competencies were developed, the system architecture was designed together with the communication of mobile application and back-end IT systems. In the project the iterative development approach, tools for software code versioning and project control have been used thus enabling continuous insight in project progressing. At any moment it was possible to determine priorities of functional development and solution elements, as well as to realize the necessary additional transfer of knowledge. The project has resulted in an enterprise mobile application for iOS and Android platforms, which had been implemented and tested in several countries. The complete solution enables data management by means of the portal, thus decreasing the frequent changes of users' mobile application and significantly accelerating the process of new functionalities introduction.","","978-953-233-077-9978-953-233-081","10.1109/MIPRO.2014.6859609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6859609","","Smart phones;Mobile communication;Servers;Knowledge transfer;Organizations;Computer crashes","Android (operating system);business data processing;iOS (operating system);mobile computing;organisational aspects;software engineering","competence transfer;enterprise mobile application development;information technology support;business development;operational organizations;IT support organizations;mobile IT equipment;business mobile applications;Ericsson Nikola Tesla;ENT;faculty of electrical engineering and computing;FER;Zagreb university;iOS;Android;back-end IT systems;iterative development approach;software code versioning;project control;project progressing;functional development","","2","22","","","","","","IEEE","IEEE Conferences"
"Coordination of robot paths for cycle time minimization","D. Spensieri; R. Bohlin; J. S. Carlson","Geometry and Motion Planning Group at the Fraunhofer-Chalmers Research Centre for Industrial Mathematics, Göteborg, Sweden; Geometry and Motion Planning Group at the Fraunhofer-Chalmers Research Centre for Industrial Mathematics, Göteborg, Sweden; Geometry and Motion Planning Group at the Fraunhofer-Chalmers Research Centre for Industrial Mathematics, Göteborg, Sweden","2013 IEEE International Conference on Automation Science and Engineering (CASE)","","2013","","","522","527","In this work we study the problem of coordinating robot paths sharing a common environment in order to minimize cycle time, by avoiding their mutual collisions. This problem is particularly relevant in the automotive industry where several robots perform welding operations to assemble and join a car body. The main contributions of this article are: to model the path coordination problem in a graph based way similar to job shop scheduling problem; to solve the path coordination problem by a branch and bound optimization algorithm exploiting the cylindrical structure of the problem. A computational study is presented where the correctness and performance of the new algorithm are evaluated by comparing it with a Mixed Integer Linear Programming formulation, solved by a general purpose package: good results are presented, with computing time differences of even three orders of magnitude. Finally, the algorithm has been interfaced with a state-of-the-art simulation software: within this framework an industrial test case from the automotive industry is solved. A straightforward way to modify pre-computed robot programs, implementing the optimized schedule is also described in pseudo-code. The efficiency of the solver and the robustness of the generated robot programs make the method very appealing in practice.","2161-8070;2161-8089","978-1-4799-1515","10.1109/CoASE.2013.6654032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654032","","Robot kinematics;Collision avoidance;Service robots;Computational modeling;Industries;Welding","automobile industry;collision avoidance;control engineering computing;digital simulation;graph theory;integer programming;linear programming;production engineering computing;robotic welding;tree searching","robot path coordination;cycle time minimization;mutual collision avoidance;automotive industry;welding operations;car body;graph based way;job shop scheduling problem;branch and bound optimization algorithm;cylindrical problem structure;mixed integer linear programming formulation;general purpose package;simulation software;industrial test case;precomputed robot programs;pseudo-code","","7","36","","","","","","IEEE","IEEE Conferences"
"Understanding of gas engines responses to system load changes","Z. Sadikhov","Shell U.K. Limited Aberdeen, AB12 3FY, United Kingdom","2015 Petroleum and Chemical Industry Conference Europe (PCIC Europe)","","2015","","","1","7","Use of reciprocating gas engines for small, 1-3MW, and islanded power generation systems becomes more economical in comparison to gas turbine or diesel engine driven systems. This is due to increased efficiency of the gas engines, availability of the fuel gas and reduced emissions. Design of power generation systems includes transient studies carried out to understand the systems response to disturbances caused by sudden load demand changes e.g. motor starts, partial loss of generation, loss of interlinks. Usually, those studies done by software simulations, using wide range of available governor models for gas/steam turbines and diesel engines. This paper examines lessons learnt from various power generation projects when gas engines were used as main power sources. It analyses attempts to model gas engines governors and their validation using engines responses' results from factory tests. It is also reviews the project when software simulation was not deemed feasible and comprehensive onshore testing of power generation and power management system was undertaken to fine tune various operating scenarios before offshore installation. The paper is intended to aid design engineers to select and optimise power generation systems.","","978-3-9524-0594-9978-1-4673-7426","10.1109/PCICEurope.2015.7790030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7790030","gas engine;governor;governor modelling;testing","Turbines;Load modeling;Fuels;Diesel engines;Generators","distributed power generation;gas turbine power stations;offshore installations","system load changes;reciprocating gas engines;islanded power generation systems;gas engine efficiency;fuel gas availability;emission reduction;power system transient;system response;sudden load demand changes;power generation projects;power sources;governor models;power management system;offshore installation;power 1 MW to 3 MW","","","2","","","","","","IEEE","IEEE Conferences"
"The upset-fault-observer: A concept for self-healing adaptive fault tolerance","B. Navas; J. Öberg; I. Sander","Dept. of Electronic Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Dept. of Electronic Systems, KTH Royal Institute of Technology, Stockholm, Sweden; Dept. of Electronic Systems, KTH Royal Institute of Technology, Stockholm, Sweden","2014 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)","","2014","","","89","96","Advancing integration reaching atomic-scales makes components highly defective and unstable during lifetime. This demands paradigm shifts in electronic systems design. FPGAs are particularly sensitive to cosmic and other kinds of radiations that produce single-event-upsets (SEU) in configuration and internal memories. Typical fault-tolerance (FT) techniques combine triple-modular-redundancy (TMR) schemes with run-time-reconfiguration (RTR). However, even the most successful approaches disregard the low suitability of fine-grain redundancy in nano-scale design, poor scalability and programmability of application specific architectures, small performance-consumption ratio of board-level designs, or scarce optimization capability of rigid redundancy structures. In that context, we introduce an innovative solution that exploits the flexibility, reusability, and scalability of a modular RTR SoC approach and reuse existing RTR IP-cores in order to assemble different TMR schemes during run-time. Thus, the system can adaptively trigger the adequate self-healing strategy according to execution environment metrics and user-defined goals. Specifically the paper presents: (a) the upset-fault-observer (UFO), an innovative run-time self-test and recovery strategy that delivers FT on request over several function cores but saves the redundancy scalability cost by running periodic reconfigurable TMR scan-cycles, (b) run-time reconfigurable TMR schemes and self-repair mechanisms, and (c) an adaptive software organization model to manage the proposed FT strategies.","","978-1-4799-5356","10.1109/AHS.2014.6880163","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6880163","partial and run-time-reconfiguration;fault-tolerance;self-healing;self-configuration;system-on-chip;hardware systems;reconfigurable IP-cores;adaptive embedded systems;reconfigurable computing","Tunneling magnetoresistance;Hardware;Redundancy;Fault tolerant systems;System-on-chip;Software","fault tolerant computing;redundancy","upset fault observer;self-healing adaptive fault tolerance;integration reaching atomic scales;electronic systems design;FPGA;single event upsets;SEU;triple modular redundancy;run time reconfiguration;RTR;fine grain redundancy;nanoscale design;performance consumption ratio;board level designs;scarce optimization capability;rigid redundancy structures;reusability;self-healing strategy;UFO;recovery strategy;redundancy scalability cost;periodic reconfigurable TMR scan cycles;run-time reconfigurable TMR schemes;adaptive software organization model;FT strategies","","4","28","","","","","","IEEE","IEEE Conferences"
"Different Implementations of AES Cryptographic Algorithm","G. Guo; Q. Qian; R. Zhang","NA; NA; NA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","1848","1853","Currently, AES is regarded as the most popular symmetric cryptographic algorithm. It is very significant to develop high performance AES to further broaden its widespread applications. And in this paper, it is mainly about the different optimized designs and implementations of AES algorithm. Firstly, it tests the fast implementation of AES algorithm and the performance has been improved by about 50 times when compared to the standard AES algorithm, Secondly, using the Intel AES-NI extended instruction sets, and the performance has been improved by about 50 times compared with the fast implementation of AES algorithm, Finally, using CUDA and GPU to execute the AES in parallel, and it can improve the performance by about 18 times compared with the fast implementation of AES algorithm.","","978-1-4799-8937-9978-1-4799-8936","10.1109/HPCC-CSS-ICESS.2015.215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336441","High Performance AES;AES-NI;CUDA based AES","Graphics processing units;Encryption;Optimization;Algorithm design and analysis;Kernel;Instruction sets","cryptography;parallel processing","symmetric cryptographic algorithm;high performance AES cryptographic algorithm;Intel AES-NI extended instruction set;advanced encryption standard algorithm","","5","14","","","","","","IEEE","IEEE Conferences"
"Optimization of a self-converging algorithm at assembly level to improve SEU fault-tolerance","G. Marques-Costa; W. Mansour; F. Pancher; R. Velazco; A. Bui; D. Sohier","Laboratoire TIMA, Inst. National Polytechnique de Grenoble, France; Laboratoire TIMA, Inst. National Polytechnique de Grenoble, France; Laboratoire TIMA, Inst. National Polytechnique de Grenoble, France; Laboratoire TIMA, Inst. National Polytechnique de Grenoble, France; Laboratoire PRISM, Univ. de Versailles-St-Quentin-en-Yvelines, France; Laboratoire PRISM, Univ. de Versailles-St-Quentin-en-Yvelines, France","2013 IEEE 4th Latin American Symposium on Circuits and Systems (LASCAS)","","2013","","","1","4","The robustness with respect to SEUs (Single-Event Upset) of a self-converging algorithm is improved by fault-tolerance techniques implemented at software level. SEU-sensitivity evaluation was done by fault injection campaigns performed using a devoted test platform. Experimental results show that implementing fault-tolerance by modifying the assembly code leads to significant improvements of the fault tolerance.","","978-1-4673-4900-0978-1-4673-4897-3978-1-4673-4899","10.1109/LASCAS.2013.6519033","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6519033","Single-event uptset (SEU);CEU (Code Emulated Upset);Self-stabilization;Self-converging algorithm;Fault injection;Fault tolerance;Triple Modular Redundancy;Assembly code","Circuit faults;Registers;Assembly;Fault tolerance;Fault tolerant systems;Program processors;Field programmable gate arrays","fault tolerance;radiation hardening (electronics)","self-converging algorithm;assembly level;SEU fault-tolerance improvement;single-event upset;fault-tolerance technique;software level;SEU-sensitivity evaluation;fault injection campaigns;assembly code modification","","","17","","","","","","IEEE","IEEE Conferences"
"Cost-Effective Virtual Machine Image Replication Management for Cloud Data Centers","D. Shen; F. Dong; J. Zhang; J. Luo","NA; NA; NA; NA","2014 IEEE Intl Conf on High Performance Computing and Communications, 2014 IEEE 6th Intl Symp on Cyberspace Safety and Security, 2014 IEEE 11th Intl Conf on Embedded Software and Syst (HPCC,CSS,ICESS)","","2014","","","229","236","Cloud computing offers infrastructure as a service to deliver large amount of computation and storage resources, in which fast provisioning of virtual machine(VM) instances has significant impacts on the overall system performance and elasticity. In this paper, we analyze the characteristics of image provisioning by studying the traces collected from the real-world cloud data centre. From the analysis results, we observe that the overloaded and dynamic requests for some popular images result in degradation and fluctuation of performance and availability of the system. Addressing this issue, we propose a stochastic model based on queueing theory, which captures the main factors in image provisioning to optimize the number and placement of image replication, so as to manage the VM images in a cost-effective manner. We implement our theoretical model based on open-source cloud platform and carry out trace driven evaluation to validate its effectiveness. The evaluation results show that our system is cost-effective and can achieve high and stable performance in VM provisioning while remaining high availability under different test scenarios.","","978-1-4799-6123-8978-1-4799-6122","10.1109/HPCC.2014.41","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7056744","","Availability;Queueing analysis;Optimization;Image storage;Time factors;Data models;Educational institutions","cloud computing;computer centres;queueing theory;stochastic processes;virtual machines","virtual machine image replication management;VM;cloud data center;cloud computing;image provisioning;stochastic model;queueing theory","","","22","","","","","","IEEE","IEEE Conferences"
"High-level synthesis for run-time hardware Trojan detection and recovery","X. Cui; K. Ma; L. Shi; K. Wu","College of computer Science, Chongqing University, China 400044; Department of ECE, University of Illinois, Chicago, 60607, USA; College of computer Science, Chongqing University, China 400044; College of computer Science, Chongqing University, China 400044","2014 51st ACM/EDAC/IEEE Design Automation Conference (DAC)","","2014","","","1","6","Current Integrated Circuit (IC) development process raises security concerns about hardware Trojan which are maliciously inserted to alter functional behavior or leak sensitive information. Most of the hardware Trojan detection techniques rely on a golden (trusted) IC against which to compare a suspected one. Hence they cannot be applied to designs using third party Intellectual Property (IP) cores where golden IP is unavailable. Moreover, due to the stealthy nature of hardware Trojan, there is no technique that can guarantee Trojan-free after manufacturing test. As a result, Trojan detection and recovery at run time acting as the last line of defense is necessary especially for mission-critical applications. In this paper, we propose design rules to assist run-time Trojan detection and fast recovery by exploring diversity of untrusted third party IP cores. With these design rules, we show the optimization approach to minimize the cost of implementation in terms of the number of different IP cores used by the implementation.","0738-100X","978-1-4799-3017","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6881484","Hardware Trojan;detection and recovery;design for security;run time;IP;high-level synthesis","Trojan horses;IP networks;Payloads;Hardware;Integrated circuits;Educational institutions;Schedules","hardware-software codesign;integrated circuit design;invasive software","high level synthesis;run time hardware Trojan detection;integrated circuit development process;intellectual property cores;golden IP;design rules","","","13","","","","","","IEEE","IEEE Conferences"
"ccnSim: An highly scalable CCN simulator","R. Chiocchetti; D. Rossi; G. Rossini","Telecom ParisTech, Paris, France; Telecom ParisTech, Paris, France; Telecom ParisTech, Paris, France","2013 IEEE International Conference on Communications (ICC)","","2013","","","2309","2314","Research interest about Information Centric Networking (ICN) has grown at a very fast pace over the last few years, especially after the 2009 seminal paper of Van Jacobson et al. describing a Content Centric Network (CCN) architecture. While significant research effort has been produced in terms of architectures, algorithms, and models, the scientific community currently lacks common tools and scenarios to allow a fair cross-comparison among the different proposals. The situation is particularly complex as the commonly used general-purpose simulators cannot cope with the expected system scale: thus, many proposals are currently evaluated over small and unrealistic scale, especially in terms of dominant factors like catalog and cache sizes. As such, there is need of a scalable tool under which different algorithms can be tested and compared. Over the last years, we have developed and optimized ccnSim, an highly scalable chunk-level simulator especially suitable for the analysis of caching performance of CCN network. In this paper, we briefly describe the tool, and present an extensive benchmark of its performance. To give an idea of ccnSim scalability, a common off-the-shelf PC equipped with 8GB of RAM memory is able to simulate 2-hours of a 50-nodes CCN network, where each nodes is equipped with 10GB caches, serving a 1PB catalog in about 20 min CPU time.","1550-3607;1938-1883","978-1-4673-3122","10.1109/ICC.2013.6654874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654874","","Catalogs;Random access memory;Synchronization;YouTube;Memory management;Libraries","cache storage;information networks;public domain software;software tools","chunk level simulator;information centric networking;highly scalable CCN simulator;ccnSim","","25","22","","","","","","IEEE","IEEE Conferences"
"dispel4py: An Agile Framework for Data-Intensive eScience","R. Filgueira; A. Krause; M. Atkinson; I. Klampanos; A. Spinuso; S. Sanchez-Exposito","NA; NA; NA; NA; NA; NA","2015 IEEE 11th International Conference on e-Science","","2015","","","454","464","We present dispel4py a versatile data-intensive kit presented as a standard Python library. It empowers scientists to experiment and test ideas using their familiar rapid-prototyping environment. It delivers mappings to diverse computing infrastructures, including cloud technologies, HPC architectures and specialised data-intensive machines, to move seamlessly into production with large-scale data loads. The mappings are fully automated, so that the encoded data analyses and data handling are completely unchanged. The underpinning model is lightweight composition of fine-grained operations on data, coupled together by data streams that use the lowest cost technology available. These fine-grained workflows are locally interpreted during development and mapped to multiple nodes and systems such as MPI and Storm for production. We explain why such an approach is becoming more essential in order that data-driven research can innovate rapidly and exploit the growing wealth of data while adapting to current technical trends. We show how provenance management is provided to improve understanding and reproducibility, and how a registry supports consistency and sharing. Three application domains are reported and measurements on multiple infrastructures show the optimisations achieved. Finally we present the next steps to achieve scalability and performance.","","978-1-4673-9325","10.1109/eScience.2015.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7304329","eSciences workflows;data intensive application;run time analysis;distributed systems;python frameworks","Storms;Production;Standards;Distributed databases;Libraries;Data models;Collaboration","cloud computing;data analysis;natural sciences computing;parallel processing;software libraries;software prototyping","dispel4py;agile framework;data-intensive e-science;standard Python library;rapid-prototyping environment;cloud technologies;HPC architectures;encoded data analyses;data handling;provenance management","","2","30","","","","","","IEEE","IEEE Conferences"
"Microcredit risk assessment using crowdsourcing and social networks","T. Hasanov; M. Ozeki; N. Oka","Interactive Intelligence Laboratory Kyoto Institute of Technology Kyoto, Japan; Interactive Intelligence Laboratory Kyoto Institute of Technology Kyoto, Japan; Interactive Intelligence Laboratory Kyoto Institute of Technology Kyoto, Japan","15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)","","2014","","","1","5","The task of automated risk assessment is attracting significant attention in the light of the recent microloan popularity growth. The industry requires a real time method for the timely processing of the extensive number of applicants for short-term small loans. Owing to the vast number of applications, manual verification is not a viable option. In cooperation with a microloan company in Azerbaijan, we have researched automated risk assessment using crowdsourcing. The principal concept behind this approach is the fact that a significant amount of information relating to a particular applicant can be retrieved from the social networks. The suggested approach can be divided into three parts: First, applicant information is collected on social networks such as LinkedIn and Facebook. This can only occur with the applicant's permission. Then, this data is processed using a program that extracts the relevant information segments. Finally, these information segments are evaluated using crowdsourcing. We attempted to evaluate the information segments using social networks. To that end, we automatically posted requests on the social networks regarding certain information segments and evaluated the community response by counting “likes” and “shares”. For example, we posted the status, “Do you think that a person who has worked at ABC Company is more likely to repay a loan? Please “like” this post if you agree.” From the results, we were able to estimate public opinion. Once evaluated, each information segment was then given a weight factor that was optimized using available loan-repay test data provided to us by a company. We then tested the proposed system on a set of 400 applicants. Using a second crowdsourcing approach, we were able to confirm that the resulting solution provided a 92.5% correct assessment, with 6.45% false positives and 11.11% false negatives, with an assessment duration of 24 hours.","","978-1-4799-5604","10.1109/SNPD.2014.6888682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6888682","Crowdsourcing Social Networks;Risk Assessment;Microloans","Crowdsourcing;LinkedIn;Facebook;Risk management;Data mining;Companies","finance;risk management;social networking (online)","automated microcredit risk assessment;social networks;microloan popularity growth;short-term small loans;loan-repay test data;crowdsourcing approach","","","16","","","","","","IEEE","IEEE Conferences"
"High Temperature Superconducting YBCO Insert for 25 T Full Superconducting Magnet","Q. Wang; J. Liu; S. Song; G. Zhu; Y. Li; X. Hu; L. Yan","NA; NA; NA; NA; NA; NA; NA","IEEE Transactions on Applied Superconductivity","","2015","25","3","1","5","A 25 T superconducting magnet with a 10 T YBCO high temperature superconducting (HTS) insert and a 15 T low temperature superconducting (LTS) (Nb<sub>3</sub>Sn and NbTi) outsert is fabricated. To design the superconducting magnet system, an optimal design software which combined the advantages of global optimization and local optimization has been proposed to study the number and type of coils to affect the final optimal results, and some important characteristics, for example, coil hoop stress and operating margin and quench characteristics are studied. The high temperature superconductor YBCO inserts with the inner diameter of 41 mm, the height of 115 mm, and outer diameter of 124 mm have been designed and fabricated with the operating temperature at 4.2 K. The outserts of NbTi and Nb<sub>3</sub>Sn with the inner diameter, outer diameter and height of 180.22, 443, and 400.5 mm, respectively, have been fabricated and tested. It reaches to the operating magnetic field at 15 T. The combined HTS and LTS coils can generate a central field of 25 T at 4.2 K. In this paper, the 25 T superconducting magnet with LTS and HTS coils are reported.","1051-8223;1558-2515;2378-7074","","10.1109/TASC.2014.2365630","National Natural Science Foundation of China; National Science and Technology Pillar Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960030","YBCO HTS insert;25 T full superconducting magnet;Operating temperature of 4.2 K;Operating temperature of 4.2 K;YBCO HTS insert;25 T full superconducting magnet","Coils;Superconducting magnets;Yttrium barium copper oxide;Magnetic fields;Niobium-tin;Wires","electrical engineering computing;high-temperature superconductors;niobium compounds;superconducting coils;superconducting magnets;yttrium compounds","high temperature superconductor;full superconducting magnet system;low temperature superconductor;LTS outsert;HTS insert;global optimization;local optimization;optimal design software;coil hoop stress;operating margin;quench characteristics;LTS coils;magnetic flux density 25 T;magnetic flux density 10 T;size 41 mm;size 124 mm;temperature 4.2 K;YBCO;Nb<sub>3</sub>Sn;NbTi","","27","26","","","","","","IEEE","IEEE Journals & Magazines"
"Memoise: A tool for memoized symbolic execution","G. Yang; S. Khurshid; C. S. Păsăreanu","Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712, USA; Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX 78712, USA; Carnegie Mellon Silicon Valley, NASA Ames, M/S 269-2 Moffett Field, CA 94035, USA","2013 35th International Conference on Software Engineering (ICSE)","","2013","","","1343","1346","This tool paper presents a tool for performing memoized symbolic execution (Memoise), an approach we developed in previous work for more efficient application of symbolic execution. The key idea in Memoise is to allow re-use of symbolic execution results across different runs of symbolic execution without having to re-compute previously computed results as done in earlier approaches. Specifically, Memoise builds a trie-based data structure to record path exploration information during a run of symbolic execution, optimizes the trie for the next run, and re-uses the resulting trie during the next run. Our tool optimizes symbolic execution in three standard scenarios where it is commonly applied: iterative deepening, regression analysis, and heuristic search. Our tool Memoise builds on the Symbolic PathFinder framework to provide more efficient symbolic execution of Java programs and is available online for download. The tool demonstration video is available at http://www.youtube.com/watch?v=ppfYOB0Z2vY.","0270-5257;1558-1225","978-1-4673-3076-3978-1-4673-3073","10.1109/ICSE.2013.6606713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6606713","","Regression analysis;Java;Testing;Data structures;Iterative methods;NASA;Standards","data structures;iterative methods;Java;program verification;regression analysis","Memoise;memoized symbolic execution;trie-based data structure;path exploration information;iterative deepening;regression analysis;heuristic search;symbolic PathFinder framework;Java programs;tool demonstration video","","5","13","","","","","","IEEE","IEEE Conferences"
"Challenges in the design and implementation of wireless sensor networks: A holistic approach-development and planning tools, middleware, power efficiency, interoperability","G. Papadopoulos","Department of Electrical and Computer Engineering, University of Patras, Greece","2015 4th Mediterranean Conference on Embedded Computing (MECO)","","2015","","","1","3","Wireless Sensor Networks (WSNs) constitute a networking area with promising impact in the environment, health, security, industrial applications and more. Each of these presents different requirements, regarding system performance and QoS, and involves a variety of mechanisms such as routing and MAC protocols, algorithms, scheduling policies, security, OS, all of which are residing over the HW, the sensors, actuators and the Radio Tx/Rx. Furthermore, they encompass special characteristics, such as constrained energy, CPU and memory resources, multi-hop communication, leading to a few steps higher the required special knowledge. Although the status of WSNs is nearing the stage of maturity and wide-spread use, the issue of their sustainability hinges upon the implementation of some features of paramount importance: Low power consumption to achieve long operational life-time for battery-powered unattended WSN nodes, joint optimization of connectivity and energy efficiency leading to best-effort utilization of constrained radios and minimum energy cost, self-calibration and self-healing to recover from failures and errors to which WSNs are prone, efficient data aggregation lessening the traffic load in constrained WSNs, programmable and reconfigurable stations allowing for long life-cycle development, system security enabling protection of data and system operation, short development time making more efficient the time-to-market process and simple installation and maintenance procedures for wider acceptance. Despite the considerable research and important advances in WSNs, large scale application of the technology is still hindered by technical, complexity and cost impediments. Ongoing R&amp;D is addressing these shortcomings by focusing on energy harvesting, middleware, network intelligence, standardization, network reliability, adaptability and scalability. However, for efficient WSN development, deployment, testing, and maintenance, a holistic unified approach is necessary which will address the above WSN challenges by developing an integrated platform for smart environments with built-in user friendliness, practicality and efficiency. This platform will enable the user to evaluate his design by identifying critical features and application requirements, to verify by adopting design indicators and to ensure ease of development and long life cycle by incorporating flexibility, expandability and reusability. These design requirements can be accomplished to a significant extent via an integration tool that provides a multiple level framework of functionality composition and adaptation for a complex WSN environment consisting of heterogeneous platform technologies, establishing a software infrastructure which couples the different views and engineering disciplines involved in the development of such a complex system, by means of the accurate definition of all necessary rules and the design of the `glue-logic' which will guarantee the correctness of composition of the various building blocks. Furthermore, to attain an enhanced efficiency, the design/development tool must facilitate consistency control as well as evaluate the selections made by the user and, based on specific criteria, provide feedback on errors concerning consistency and compatibility as well as warnings on potentially less optimal user selections. Finally, the WSN planning tool will provide answers to fundamental issues such as the number of nodes needed to meet overall system objectives, the deployment of these nodes to optimize network performance and the adjustment of network topology and sensor node placement in case of changes in data sources and network malfunctioning.","2377-5475","978-1-4799-1976-5978-1-4799-8999-7978-1-4799-8998","10.1109/MECO.2015.7181857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7181857","","","computer network reliability;computer network security;data protection;energy conservation;energy harvesting;middleware;open systems;optimisation;quality of service;sensor placement;telecommunication network planning;telecommunication network topology;telecommunication power management;telecommunication traffic;time to market;wireless sensor networks","wireless sensor network planning tools;middleware;power efficiency;interoperability;WSN reliability;QoS;power consumption;energy efficiency;constrained radio best-effort utilization;failure recovery;data aggregation;traffic load;data security enabling protection;time-to-market process;energy harvesting;network intelligence;holistic unified approach;heterogeneous platform technology;design-development tool;network topology adjustment;sensor node placement","","","","","","","","","IEEE","IEEE Conferences"
"Machine Learning-Based Coding Unit Depth Decisions for Flexible Complexity Allocation in High Efficiency Video Coding","Y. Zhang; S. Kwong; X. Wang; H. Yuan; Z. Pan; L. Xu","Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; Department of Computer Science, The City University of Hong Kong, Hong Kong; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; School of Information Science and Engineering, Shandong University, Jinan, China; Jiangsu Engineering Center of Network Monitoring, School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; Key Laboratory of Solar Activity, National Astronomical Observatories, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Image Processing","","2015","24","7","2225","2238","In this paper, we propose a machine learning-based fast coding unit (CU) depth decision method for High Efficiency Video Coding (HEVC), which optimizes the complexity allocation at CU level with given rate-distortion (RD) cost constraints. First, we analyze quad-tree CU depth decision process in HEVC and model it as a three-level of hierarchical binary decision problem. Second, a flexible CU depth decision structure is presented, which allows the performances of each CU depth decision be smoothly transferred between the coding complexity and RD performance. Then, a three-output joint classifier consists of multiple binary classifiers with different parameters is designed to control the risk of false prediction. Finally, a sophisticated RD-complexity model is derived to determine the optimal parameters for the joint classifier, which is capable of minimizing the complexity in each CU depth at given RD degradation constraints. Comparative experiments over various sequences show that the proposed CU depth decision algorithm can reduce the computational complexity from 28.82% to 70.93%, and 51.45% on average when compared with the original HEVC test model. The Bjøntegaard delta peak signal-to-noise ratio and Bjøntegaard delta bit rate are -0.061 dB and 1.98% on average, which is negligible. The overall performance of the proposed algorithm outperforms those of the state-of-the-art schemes.","1057-7149;1941-0042","","10.1109/TIP.2015.2417498","National Natural Science Foundation of China; Shenzhen Overseas High-Caliber Personnel Innovation and Entrepreneurship Project; Shenzhen Emerging Industries of the Strategic Basic Research Project; 100-Talents Program of Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070704","High Efficiency Video Coding;Coding Unit;Machine Learning, Support Vector Machine;High efficiency video coding;coding unit;machine learning;support vector machine","Complexity theory;Video coding;Support vector machines;Joints;Image coding;Classification algorithms;Prediction algorithms","binary decision diagrams;communication complexity;decision theory;decision trees;image classification;learning (artificial intelligence);quadtrees;rate distortion theory;video coding","machine learning;coding unit depth decision method;flexible complexity allocation;high efficiency video coding;HEVC;rate-distortion cost constraint;RD cost constraint;quadtree CU depth decision process;hierarchical binary decision problem;three-output joint classifier;multiple binary classifier;RD-complexity model;computational complexity;Bjøntegaard delta peak signal-to-noise ratio;Bjøntegaard delta bit rate","","52","26","","","","","","IEEE","IEEE Journals & Magazines"
"Simultaneous Recognition and Modeling for Learning 3-D Object Models From Everyday Scenes","M. Liang; H. Min; R. Luo; J. Zhu","School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China","IEEE Transactions on Cybernetics","","2015","45","10","2237","2248","Object recognition and modeling have classically been studied separately, but practically, they are two closely correlated aspects. In this paper, by exploring the interrelations, we propose a framework to address these two problems at the same time, which we call simultaneous recognition and modeling. Differing from traditional recognition process which consists of off-line object model learning and on-line recognition procedures, our method is solely online. Starting with an empty object database, we incrementally build up object models while at the same time using these models to identify newly observed object views. In the proposed framework, objects are modeled as view graphs and a probabilistic observation model is presented. Both the appearance and the spatial structure of the object are examined, and a formulation based on maximum likelihood estimation is developed. Joint object recognition and modeling are achieved by solving the optimization problem. To evaluate the framework, we have developed a method for simultaneously learning multiple 3-D object models directly from the cluttered indoor environment and tested it using several everyday scenes. Experimental results demonstrate that the framework can cope with the recognition and modeling problem together nicely.","2168-2267;2168-2275","","10.1109/TCYB.2014.2368127","National Natural Science Foundation of China; Natural Science Foundation of Guangdong Province of China; Fundamental Research Funds for the Central Universities; Foundation for Distinguished Young Talents in Higher Education of Guangdong; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963454","Maximum likelihood estimation;object modeling;object recognition;on-line;probabilistic model;Maximum likelihood estimation;object modeling;object recognition;on-line;probabilistic model","Solid modeling;Object recognition;Probabilistic logic;Robots;Feature extraction;Maximum likelihood estimation","graph theory;image recognition;learning (artificial intelligence);maximum likelihood estimation;object recognition;probability;visual databases","simultaneous recognition and modelling;SRAM;3D object model learning;object recognition;object modelling;object database;view graph;probabilistic observation model;maximum likelihood estimation;everyday scene","","1","34","","","","","","IEEE","IEEE Journals & Magazines"
"Assessment of packet latency on the 4G LTE S1-U interface: Impact on end-user throughput","A. Rusan; R. Vasiu","Politehnica University of Timisoara, Department of Communications, Romania; Politehnica University of Timisoara, Department of Communications, Romania","2015 23rd International Conference on Software, Telecommunications and Computer Networks (SoftCOM)","","2015","","","305","309","4<sup>th</sup> Generation Long Term Evolution (4G LTE) networks are about to become the standard in mobile communications. These networks' backhaul transport for the LTE user and signaling traffic needs to operate in various environments and over a wide variety of networks built on a mix of technologies and equipments. All network elements of such a network are interconnected over other wired and wireless network technologies, each with performance characteristics of their own - and performance issues which will often be translated into issues seen on the overall LTE network performance. It is important to understand this behavior and the interdependencies in order to understand how to optimize and eventually predict performance of 4G LTE networks. Since 4G LTE itself is a full IP-based network, this means understanding and assessing the effect of IP impairments on LTE and its transport links. In this paper, we analyze the impact of backhaul packet delay (latency) on the LTE S1-U interface, which provides user plane transport between the Core Network and the Evolved NodeBs. We emulate different packet delays on the S1-U interface, measuring the impact on end-user throughput and discussing the results. We use a custom, self-built IP Impairment Tool: its flexible, cost-effective and mobile concept allows quick and easy integration and testing even on live commercial-grade LTE networks.","","978-9-5329-0056","10.1109/SOFTCOM.2015.7314072","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7314072","LTE;packet delay;latency;S1-U interface;IP impairment;network performance","Delays;IP networks;Long Term Evolution;Throughput;Testing;Emulation;Degradation","delays;Long Term Evolution;radio equipment;telecommunication traffic","packet latency assessment;4G LTE S1-U interface;end-user throughput;4<sup>th</sup> Generation Long Term Evolution;mobile communications;backhaul transport;LTE user;signaling traffic;wireless network technologies;4G LTE networks;IP impairments;transport links;backhaul packet delay;LTE S1-U interface;IP Impairment Tool","","2","22","","","","","","IEEE","IEEE Conferences"
"Improving Power Grid Monitoring Data Quality: An Efficient Machine Learning Framework for Missing Data Prediction","W. Shi; Y. Zhu; J. Zhang; X. Tao; G. Sheng; Y. Lian; G. Wang; Y. Chen","NA; NA; NA; NA; NA; NA; NA; NA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","417","422","Big data techniques has been applied to power grid for the evaluation and prediction of grid conditions. However, the raw data quality rarely can meet the requirement of precise data analytics since raw data set usually contains samples with missing data to which the common data mining models are sensitive. Though classic interpolation or neural network methods can been used to fill the gaps of missing data, their predicted data often fail to fit the rules of power grid conditions. This paper presents a machine learning framework (OR_MLF) to improve the prediction accuracy for datasets with missing data points, which mainly combines preprocessing, optimizing support vector machine (OSVM) and refining SVM (RSVM). On top of the OSVM engine, the scheme introduces dedicated data training strategies. First, the original data originating from data generation facilities is preprocessed through standardization. Traditional SVM is then trained to obtain a preliminary prediction model. Next, the optimized SVM predictors are achieved with new training data set, which is extracted based on the preliminary prediction model. Finally, the missing data prediction result depending on OSVM is selectively inputted into the traditional SVM and the refined SVM is lastly accomplished. We test the OR_MLF framework on missing data prediction of power transformers in power grid system. The experimental results show that the predictors based on the proposed framework achieve lower mean square error than traditional ones. Therefore, the framework OR_MLF would be a good candidate to predict the missing data in power grid system.","","978-1-4799-8937-9978-1-4799-8936","10.1109/HPCC-CSS-ICESS.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336197","missing data prediction;machine learning;support vector machine (SVM);power transformer","Support vector machines;Data models;Predictive models;Training;Data mining;Feature extraction;Power grids","Big Data;computerised monitoring;data mining;interpolation;learning (artificial intelligence);mean square error methods;neural nets;power engineering computing;power grids;power system measurement;power transformers;standardisation;support vector machines","power grid monitoring data quality;machine learning framework;missing data prediction;Big data technique;data mining model;neural network method;OR-MLF;optimizing support vector machine;OSVM;refining SVM;RSVM;power transformer;mean square error method","","7","22","","","","","","IEEE","IEEE Conferences"
"Assessing the Suitability of In-Memory Databases in an Enterprise Context","R. Meyer; V. Banova; A. Danciu; D. Prutscher; H. Krcmar","NA; NA; NA; NA; NA","2015 International Conference on Enterprise Systems (ES)","","2015","","","78","89","It is still not fully clear if the increased query execution speed offered by in-memory databases unfolds its potential benefits over traditional disk-based databases in an enterprise context. This paper aims at comparing the performance of in-memory versus disk-based databases in such context to assess the added value of in-memory database technology for businesses. To achieve this goal, the authors conducted a literature review to find performance comparisons and benchmarks of the two database types in business contexts. Based on the results of this review an own performance comparison methodology assembling workloads mostly found in enterprise environments is compiled. The devised methodology is then applied by comparing two implementations of each database technology. The results show, that not all businesses profit equally from deploying in-memory databases but rather only those, which are dealing with large amounts of data and many concurrent users. Additionally, the tests revealed that disk-based and in-memory databases outperform each other at different query types. Limitations were that our research took place in a virtualized setup, that the SQL queries generating the workload were not subjected to any performance optimizations and that no database tuning or data staging took place. The research's genuine value lies in the compilation of a performance comparison for the two database technologies in an enterprise context, which can be used to further examine the suitability of in-memory databases for enterprise data storage requirements.","","978-1-4673-8005-8978-1-4673-8004","10.1109/ES.2015.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406850","In-memory database;Disk-based database;Performance evaluation;Enterprise system","Benchmark testing;Context;Business;Database systems;Hardware;Software","business data processing;disc storage;query processing;SQL;storage management","in-memory database;enterprise context;query execution speed;disk-based database;performance comparison methodology;enterprise environment;database technology;businesses profit;virtualized setup;SQL query;performance optimization;database tuning;data staging;enterprise data storage requirement","","","85","","","","","","IEEE","IEEE Conferences"
"A Multi-order Markov Chain Based Scheme for Anomaly Detection","W. Sha; Y. Zhu; T. Huang; M. Qiu; Y. Zhu; Q. Zhang","NA; NA; NA; NA; NA; NA","2013 IEEE 37th Annual Computer Software and Applications Conference Workshops","","2013","","","83","88","This paper presents a feasible multi-order Markov chain based scheme for anomaly detection in server systems. In our approach, both the high-order Markov chain and multivariate time series are taken into account, along with the detailed design of training and testing algorithms. To evaluate its effectiveness, the Defense Advanced Research Projects Agency (DARPA) Intrusion Detection Evaluation Data Set is used as stimuli to our model, by which system calls and the corresponding return values form a two-dimensional input set. The calculation result shows that this approach is able to produce several effective indicators of anomalies. In addition to the absolute values given by an individual single-order model, we also notice a novelty unprecedented before, i.e., the changes in ranking positions of outputs from different-order ones also correlate closely with abnormal behaviours. Moreover, the analysis and application proves our approach's efficiency in consuming reasonable cost of time and storage.","","978-1-4799-2159","10.1109/COMPSACW.2013.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6605770","Markov chain;Kth-order Markov chain;multivariate time series;anomaly detection","Markov processes;Training;Time series analysis;Data models;Servers;Intrusion detection;Hidden Markov models","Markov processes;security of data;time series","multiorder Markov chain;anomaly detection;server systems;high-order Markov chain;multivariate time series;training algorithms;testing algorithms;Defense Advanced Research Projects Agency;DARPA intrusion detection evaluation data set;system calls;return values;two-dimensional input set;anomalies indicator;absolute values;single-order model","","2","15","","","","","","IEEE","IEEE Conferences"
"Comparative geometric and radiometric evaluation of mobile phone, compact and DSLR cameras","N. A. M. Azhar; A. Ahmad; A. M. Samad; I. Ma'arof; K. A. Hashim","Department of Geoinformation, Faculty of Geoinformation &amp; Real Estate, Universiti Teknologi Malaysia, 81310 Johor Bahru, Johor, Malaysia; Department of Geoinformation, Faculty of Geoinformation &amp; Real Estate, Universiti Teknologi Malaysia, 81310 Johor Bahru, Johor, Malaysia; Pixelgrammetry &amp; Al-Idrisi Research Group (Pi-ALiRG), Faculty of Architecture, Planning &amp; Surveying, Universiti Teknologi MARA, 40450 Shah Alam, Selangor, Malaysia; Pixelgrammetry &amp; Al-Idrisi Research Group (Pi-ALiRG), Faculty of Architecture, Planning &amp; Surveying, Universiti Teknologi MARA, 40450 Shah Alam, Selangor, Malaysia; Pixelgrammetry &amp; Al-Idrisi Research Group (Pi-ALiRG), Faculty of Architecture, Planning &amp; Surveying, Universiti Teknologi MARA, 40450 Shah Alam, Selangor, Malaysia","2013 IEEE 9th International Colloquium on Signal Processing and its Applications","","2013","","","353","358","Progress development in digital imaging system has impacted in many fields including close range photogrammetry (CRP). With the improvement in resolution of digital imaging, the image qualities of digital imaging especially non-metric cameras is increasingly optimized. The recent increase in number of pixels of non-metric digital camera and hand phone camera has encouraged the needs to utilize it for CRP applications such as 3D measurement and creations of orthoimages. The objective of this study is to evaluate the performance of non-metric cameras used in CRP. In this study, three non-metric cameras which are Digital Single-Lens Reflex (DSLR), compact and mobile phone are calibrated and tested based on geometric and radiometric characteristics. Geometric performance test is carried out by using self-calibration technique where a calibration test field in the form of three-dimensional (3D) was used. The 3D distance measurement is made to verify the accuracy for these cameras. The interior parameters of the cameras were evaluated and analyzed using descriptive analysis and subsequently followed by performing distance measurement. The radiometric capabilities of all cameras were evaluated by carrying out modulation transfer function (MTF) analysis and image noise analysis. The results clearly demonstrate that the potential and the capability of non-metric cameras to measure up to sub-millimeter precision. From this study, it can be concluded that the performance of geometric is generally increases with the radiometric performance.","","978-1-4673-5609-1978-1-4673-5608-4978-1-4673-5607","10.1109/CSPA.2013.6530071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530071","Non-metric Camera;Geometric;Calibration Test field;Radiometric;Modulation Transfer Function (MTF)","Calibration;Radiometry;Software;Accuracy;Mobile handsets;Digital cameras","calibration;cameras;distance measurement;image denoising;image resolution;mobile handsets;optical transfer function;performance evaluation;photogrammetry;radiometry","geometric evaluation;radiometric evaluation;mobile phone;compact camera;DSLR camera;progress development;close range photogrammetry;CRP;digital imaging resolution system;image quality;nonmetric digital camera;hand phone camera;orthoimaging creation;performance evaluation;digital single-lens reflex;self-calibration technique;3D distance measurement;descriptive analysis;modulation transfer function analysis;MTF;image noise analysis","","","10","","","","","","IEEE","IEEE Conferences"
"Exploring Permission-Induced Risk in Android Applications for Malicious Application Detection","W. Wang; X. Wang; D. Feng; J. Liu; Z. Han; X. Zhang","School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; National University of Defense Technology, Changsha, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Division of Computer, Electrical and Mathematical Sciences and Engineering, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia","IEEE Transactions on Information Forensics and Security","","2014","9","11","1869","1882","Android has been a major target of malicious applications (malapps). How to detect and keep the malapps out of the app markets is an ongoing challenge. One of the central design points of Android security mechanism is permission control that restricts the access of apps to core facilities of devices. However, it imparts a significant responsibility to the app developers with regard to accurately specifying the requested permissions and to the users with regard to fully understanding the risk of granting certain combinations of permissions. Android permissions requested by an app depict the app's behavioral patterns. In order to help understanding Android permissions, in this paper, we explore the permission-induced risk in Android apps on three levels in a systematic manner. First, we thoroughly analyze the risk of an individual permission and the risk of a group of collaborative permissions. We employ three feature ranking methods, namely, mutual information, correlation coefficient, and T-test to rank Android individual permissions with respect to their risk. We then use sequential forward selection as well as principal component analysis to identify risky permission subsets. Second, we evaluate the usefulness of risky permissions for malapp detection with support vector machine, decision trees, as well as random forest. Third, we in depth analyze the detection results and discuss the feasibility as well as the limitations of malapp detection based on permission requests. We evaluate our methods on a very large official app set consisting of 310 926 benign apps and 4868 real-world malapps and on a third-party app sets. The empirical results show that our malapp detectors built on risky permissions give satisfied performance (a detection rate as 94.62% with a false positive rate as 0.6%), catch the malapps' essential patterns on violating permission access regulations, and are universally applicable to unknown malapps (detection rate as 74.03%).","1556-6013;1556-6021","","10.1109/TIFS.2014.2353996","Ph.D. Programs Foundation, Ministry of Education of China; Fundamental Research Funds through the Central Universities of China; Scientific Research Foundation through the Returned Overseas Chinese Scholars, Ministry of Education of China; Program for New Century Excellent Talents in University; Program for Changjiang Scholars and Innovative Research Team in University; 111 Project; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891250","Android system;Android security;permission usage analysis;malware detection;intrusion detection","Androids;Humanoid robots;Principal component analysis;Smart phones;Security;Correlation;Support vector machines","Android (operating system);invasive software;principal component analysis;smart phones","permission-induced risk;malicious applications;Android security mechanism;permission control;collaborative permissions;mutual information;correlation coefficient;T-test;sequential forward selection;principal component analysis;malapp detection;support vector machine;decision trees;random forest;third-party app sets","","61","41","","","","","","IEEE","IEEE Journals & Magazines"
"BUNGEE: An Elasticity Benchmark for Self-Adaptive IaaS Cloud Environments","N. R. Herbst; S. Kounev; A. Weber; H. Groenda","NA; NA; NA; NA","2015 IEEE/ACM 10th International Symposium on Software Engineering for Adaptive and Self-Managing Systems","","2015","","","46","56","Today's infrastructure clouds provide resource elasticity (i.e. Auto-scaling) mechanisms enabling self-adaptive resource provisioning to reflect variations in the load intensity over time. These mechanisms impact on the application performance, however, their effect in specific situations is hard to quantify and compare. To evaluate the quality of elasticity mechanisms provided by different platforms and configurations, respective metrics and benchmarks are required. Existing metrics for elasticity only consider the time required to provision and deprovision resources or the costs impact of adaptations. Existing benchmarks lack the capability to handle open workloads with realistic load intensity profiles and do not explicitly distinguish between the performance exhibited by the provisioned underlying resources, on the one hand, and the quality of the elasticity mechanisms themselves, on the other hand. In this paper, we propose reliable metrics for quantifying the timing aspects and accuracy of elasticity. Based on these metrics, we propose a novel approach for benchmarking the elasticity of Infrastructure-as-a-Service (IaaS) cloud platforms independent of the performance exhibited by the provisioned underlying resources. We show that the proposed metrics provide consistent ranking of elastic platforms on an ordinal scale. Finally, we present an extensive case study of real-world complexity demonstrating that the proposed approach is applicable in realistic scenarios and can cope with different levels of resource efficiency.","2157-2305;2157-2321","978-0-7695-5567-6978-1-4673-9475","10.1109/SEAMS.2015.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7194656","IaaS;cloud;metrics;benchmarking;elasticity;workload;measurement;provisioning;adaptation","Elasticity;Benchmark testing;Accuracy;Jitter;Load modeling;Timing","cloud computing","BUNGEE;self-adaptive IaaS cloud environments;resource elasticity benchmark;self-adaptive resource provisioning;realistic load intensity profiles;infrastructure-as-a-service cloud platforms;ordinal scale","","14","30","","","","","","IEEE","IEEE Conferences"
"Evaluation of various classifiers performance on biomedical datasets","M. Bursa; L. Lhotska","Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical in Prague, Prague, Czech Republic; Czech Institute of Informatics, Robotics and Cybernetics, Czech Technical in Prague, Prague, Czech Republic","2015 E-Health and Bioengineering Conference (EHB)","","2015","","","1","4","Often, an evaluation of a classifier is performed without deeper analysis. In this paper we decided to perform more rigorous evaluation. We present an evaluation of various classifier methods over biomedical data with orientation towards nature inspired methods. We have performed an experimental assessment of various traditional and nature inspired methods (41 distinct classifiers) over the total of 32 different biomedical datasets. We used 10-fold crossvalidation and for each experiment retrieved multiple objective parameters. The mean and best/worst-so-far values of the measures (accuracy, sensitivity, specificity, ...) have been statistically evaluated using the nonparametric Friedman test and post-hoc analyses. The ant-inspired ACO_DTree algorithm performed significantly better (alpha=0.05) in 29 experimental cases for the mean f-measure parameter and in 14 experimental cases for the best-so-far f-measure parameter. The top results have been obtained for certain subsets of the UCI database and for the dataset combining cardiotocography records and myocardial infarction records.","","978-1-4673-7545-0978-1-4673-7544-3978-1-4673-7543","10.1109/EHB.2015.7391459","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7391459","biomedical data;classification;data mining","Decision trees;Databases;Sociology;Statistics;Optimization;Data mining;Software","cardiology;data mining;medical computing;statistical analysis","biomedical datasets;classifier methods;ten-fold crossvalidation;multiple objective parameter retrieval;statistical evaluation;nonparametric Friedman test;post-hoc analyses;mean f-measure parameter;cardiotocography records;myocardial infarction records","","","9","","","","","","IEEE","IEEE Conferences"
"Scalable scheduling of energy control systems","T. X. Nghiem; R. Mangharam","Automatic Control Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Dept. of Electrical & Systems Engineering, University of Pennsylvania, Philadelphia, Pennsylvania, USA","2015 International Conference on Embedded Software (EMSOFT)","","2015","","","137","146","Peak power consumption is a universal problem across energy control systems in electrical grids, buildings, and industrial automation where the uncoordinated operation of multiple controllers result in temporally correlated electricity demand surges (or peaks). While there exist several diferent approaches to balance power consumption by load shifting and load shedding, they operate on coarse grained time scales and do not help in de-correlating energy sinks. The Energy System Scheduling Problem is particularly hard due to its binary control variables. Its complexity grows exponentially with the scale of the system, making it impossible to handle systems with more than a few variables. We developed a scalable approach for fine-grained scheduling of energy control systems that novelly combines techniques from control theory and computer science. The original system with binary control variables are approximated by an averaged system whose inputs are the utilization values of the binary inputs within a given period. The error between the two systems can be bounded, which allows us to derive a safety constraint for the averaged system so that the original system's safety is guaranteed. To further reduce the complexity of the scheduling problem, we abstract the averaged system by a simple single-state single-input dynamical system whose control input is the upper-bound of the total demand of the system. This model abstraction is achieved by extending the concept of simulation relations between transition systems to allow for input constraints between the systems. We developed conditions to test for simulation relations as well as algorithms to compute such a model abstraction. As a consequence, we only need to solve a small linear program to compute an optimal bound of the total demand. The total demand is then broken down, by solving a linear program much smaller than the original program, to individual utilization values of the subsystems, whose actual schedule is then obtained by a low-level scheduling algorithm. Numerical simulations in Matlab show the e ectiveness and scalability of our approach.","","978-1-4673-8079","10.1109/EMSOFT.2015.7318269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7318269","embedded control systems;peak power management;cyberphysical systems","Computational modeling;Optimization;Safety;Complexity theory;Schedules;Heating","linear programming;power aware computing;scheduling","energy control systems;peak power consumption;energy system scheduling problem;binary control variables;fine-grained scheduling;simple single-state single-input dynamical system;model abstraction;small linear program;low-level scheduling algorithm","","","14","","","","","","IEEE","IEEE Conferences"
"Fault location in combined overhead line and underground cable distribution networks using fault transient based mother wavelets","B. Feizifar; M. R. Haghifam; S. Soleymani; A. Jamilazari","Department of Electrical Engineering, Istanbul Technical University, Turkey; Department of Electrical Engineering, Tarbiat Modares University, Tehran, Iran; Department of Electrical Engineering, Science and Research Branch, Islamic Azad University, Tehran, Iran; Department of Electrical Engineering, Central Tehran Branch, Islamic Azad University, Tehran, Iran","2013 12th International Conference on Environment and Electrical Engineering","","2013","","","41","45","This paper presents an optimized fault location approach in combined overhead line and underground cable distribution networks. Continuous wavelet transform (CWT) is employed for analyzing fault originated travelling waves. The transient voltage waveform is recorded at a measuring point and then analyzed using both standard and fault transient inferred mother wavelets. This approach rely on the relationship between typical frequencies of CWT signal energies and certain paths in the network passed by travelling waves produced by faults. In order to identify characteristic frequencies directly related to the previously mentioned paths, the continuous frequency spectrum of fault transients must be determined. Fault location is then detected using this frequency domain data. The frequency domain data along with the theoretically obtained characteristic frequencies specify the fault position. In order to verify this procedure, the IEEE 34-bus test distribution network is modeled by EMTP-RV software and the relevant transient signal analyses are executed in MATLAB programming environment.","","978-1-4673-3059-6978-1-4673-3060-2978-1-4673-3058","10.1109/EEEIC.2013.6549586","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6549586","Continuous wavelet transform;distribution network;EMTP-RV simulations;fault location;mother wavelet","Transient analysis;Fault location;Continuous wavelet transforms;Power cables;Solids","distribution networks;power overhead lines;power system transients;underground cables;wavelet transforms","distribution networks;underground cable;fault transient;mother wavelets;optimized fault location;combined overhead line;continuous wavelet transform;CWT;travelling waves;transient voltage waveform;continuous frequency spectrum;IEEE 34-bus test;EMTP-RV software;transient signal analysis;MATLAB programming environment","","","18","","","","","","IEEE","IEEE Conferences"
"Optimal Design, Fabrication, and Control of an<formula formulatype=""inline""><tex Notation=""TeX"">$XY$</tex></formula>Micropositioning Stage Driven by Electromagnetic Actuators","S. Xiao; Y. Li","Department of Electromechanical Engineering, Faculty of Science and Technology, University of Macau, Taipa, Macau; Department of Electromechanical Engineering, Faculty of Science and Technology, University of Macau, Taipa, Macau","IEEE Transactions on Industrial Electronics","","2013","60","10","4613","4626","This paper presents the optimal design, fabrication, and control of a novel compliant flexure-based totally decoupled XY micropositioning stage driven by electromagnetic actuators. The stage is constructed with a simple structure by employing double four-bar parallelogram flexures and four noncontact types of electromagnetic actuators to realize the kinematic decoupling and force decoupling, respectively. The kinematics and dynamics modeling of the stage are conducted by resorting to compliance and stiffness analysis based on matrix method, and the parameters are obtained by multiobjective genetic algorithm (GA) optimization method. The analytical models for electromagnetic forces are also established, and both mechanical structure and electromagnetic models are validated by finite-element analysis via ANSYS software. It is found that the system is with hysteresis and nonlinear characteristics when a preliminary open-loop test is conducted; thereafter, a simple PID controller is applied. Therefore, an inverse Preisach model-based feedforward sliding-mode controller is exploited to control the micromanipulator system. Experiments show that the moving range can achieve 1 mm × 1 mm and the resolution can reach ±0.4 μm. Moreover, the designed micromanipulator can bear a heavy load because of its optimal mechanical structure.","0278-0046;1557-9948","","10.1109/TIE.2012.2209613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6244873","Electromagnetic actuators;hysteresis;micro-/nanopositioning;sliding-mode control","Actuators;Force;Fasteners;Mobile communication;Electromagnetics;Coils;Materials","bars;bending;compliant mechanisms;elastic constants;electromagnetic actuators;finite element analysis;genetic algorithms;hysteresis;matrix algebra;micromanipulators;micropositioning;three-term control;variable structure systems","XY micropositioning stage;fabrication;micromanipulator system;inverse Preisach model-based feedforward sliding-mode controller;PID controller;open-loop test;hysteresis characteristics;nonlinear characteristics;ANSYS software;finite-element analysis;mechanical structure;electromagnetic force;GA optimization method;multiobjective genetic algorithm;matrix method;stiffness analysis;force decoupling;kinematic decoupling;double four-bar parallelogram flexure;electromagnetic actuators","","67","24","","","","","","IEEE","IEEE Journals & Magazines"
"Dependability aspects of model-based systems design for mechatronic systems","K. Janschek; A. Morozov","Institute of Automation, Faculty of Electrical and Computer Engineering, Technische Universität Dresden (TU Dresden) Germany; Institute of Automation, Faculty of Electrical and Computer Engineering, Technische Universität Dresden (TU Dresden) Germany","2015 IEEE International Conference on Mechatronics (ICM)","","2015","","","15","22","This paper discusses modern model-based design aspects for ensuring high dependability of mechatronic systems, i.e. ensuring most reliable and safe operation under presence of non-avoidable threats. An introductory assessment clarifies relevant terms of reference such as “systems” (in particular mechatronic systems), “models”, “design” and “dependability” with special focus on the effect of threats (faults, errors, failures). The further considerations give answers to the questions “What `dependability' models (methods) have to be used?” and “How to work with these `dependability' models (methods)?” in the context of building dependable systems that are robust against threats. Results of current research at the TU Dresden Automation Engineering Lab demonstrate the successful applicability of model-based system threat analysis to control systems for robotic vehicles introducing new concepts such as dual graph error propagation model, error propagation for hybrid block diagram and finite state machine models, error propagation in multi-rate time discrete models, optimized software-implemented fault tolerance and model-based selective regression testing.","","978-1-4799-3633","10.1109/ICMECH.2015.7083940","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7083940","dependability;mechatronic systems;model-based design;error propagation;Markov chain;probabilistic model;automated model transform","Unified modeling language;Analytical models;Computational modeling;Mechatronics;System analysis and design;Context","design engineering;mechatronics;mobile robots","dependability aspects;model-based system design;mechatronic systems;operation reliability;operation safety;faults;errors;failures;dependability model;dependable systems;TU Dresden Automation Engineering Lab;model-based system threat analysis;control systems;robotic vehicles;dual graph error propagation model;hybrid block diagram;finite state machine models;multirate time discrete models;optimized software-implemented fault tolerance;model-based selective regression testing","","2","35","","","","","","IEEE","IEEE Conferences"
"Reducing the Communication of Message-Passing Systems Synthesized from Synchronous Programs","D. Baudisch; Y. Bai; K. Schneider","NA; NA; NA","2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing","","2014","","","444","451","This paper presents a method to translate a given synchronous system to a multithreaded system where process nodes communicate via channels with each other. It is well-known that the reduction of communication has been identified to be a crucial key for efficient utilization of multiprocessor systems. For this reason, we first use synchronous elastic design methods to generate a distributed/multithreaded system from a synchronous system, and then, reduce communication overhead between the obtained process nodes. Our benchmarks show that we can save up to 67.5% of communication costs using our method and can achieve an average speed-up of up to 1.09.","1066-6192;2377-5750","978-1-4799-2729","10.1109/PDP.2014.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6787313","","Receivers;Synchronization;Optimization;Benchmark testing;Software;Writing;Educational institutions","message passing;multiprocessing systems;multi-threading","message-passing systems;synchronous programs;multithreaded system;multiprocessor systems;synchronous elastic design methods;communication overhead","","1","31","","","","","","IEEE","IEEE Conferences"
"Modeling virtual machines misprediction overhead","D. César; R. Auler; R. Dalibera; S. Rigo; E. Borin; G. Araújo","Institute of Computing, University of Campinas, São Paulo - Brazil; Institute of Computing, University of Campinas, São Paulo - Brazil; Institute of Computing, University of Campinas, São Paulo - Brazil; Institute of Computing, University of Campinas, São Paulo - Brazil; Institute of Computing, University of Campinas, São Paulo - Brazil; Institute of Computing, University of Campinas, São Paulo - Brazil","2013 IEEE International Symposium on Workload Characterization (IISWC)","","2013","","","153","162","Virtual machines are versatile systems that can support innovative solutions to many problems. These systems usually rely on emulation techniques, such as interpretation and dynamic binary translation, to execute guest application code. Usually, in order to select the best emulation technique for each code segment, the system must predict whether the code is worth compiling (frequently executed) or not, known as hotness prediction. In this paper we show that the threshold-based hot code predictor, frequently mispredicts the code hotness and as a result the VM emulation performance become dominated by miscompilations. To do so, we developed a mathematical model to simulate the behavior of such predictor and using it we quantify and characterize the impact of mispredictions in several benchmarks. We also show how the threshold choice can affect the predictor, what are the major overhead components and how using SPEC to analyze a VM performance can lead to misleading results.","","978-1-4799-0555-3978-1-4799-0553","10.1109/IISWC.2013.6704681","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6704681","","Emulation;Optimization;Benchmark testing;Mathematical model;Virtual machining;Equations;Software","mathematical analysis;virtual machines","virtual machine misprediction overhead modeling;emulation techniques;dynamic binary translation;guest application code;code segment;hotness prediction;threshold-based hot code predictor;VM emulation performance;mathematical model;SPEC","","","29","","","","","","IEEE","IEEE Conferences"
"Parallel Dynamic Step Size Sphere-Gap Transferring Algorithm for Solving Conditional Nonlinear Optimal Perturbation","S. Yuan; J. Yan; B. Mu; H. Li","NA; NA; NA; NA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","559","565","Intelligent algorithms have been extensively applied in scientific computing. Recently, some researchers apply intelligent algorithms to solve conditional nonlinear optimal perturbation (CNOP) which is proposed to study the predictability of numerical weather and climate prediction. The difficulty of solving CNOP using the intelligent algorithm is the high dimensionality of complex numerical models. Therefore, previous researches either are just tested in ideal models or have low time efficiency in complex numerical models which limited the application of CNOP. In this paper, we proposed a parallel dynamic step size sphere-gap transferring algorithm (DSGT) to solve CNOP in complex numerical models. A dynamic step size factor is also designed to speed up convergence of sphere-gap transferring algorithm. Through the singular value decomposition, the original problem is reduced into a low-dimensional space to hunt the coordinate of the optimal CNOP with the DSGT algorithm. Moreover, in order to accelerate the computation speed, we parallelize the DSGT method with MPI technology. To demonstrate the validity, the proposed method has been studied in the Zebiak-Cane model to solve the CNOP. Experimental results prove that the proposed method can efficiently and stably obtain a satisfactory CNOP, and the parallel version can reach the speedup of 7.18 times with 10 cores.","","978-1-4799-8937-9978-1-4799-8936","10.1109/HPCC-CSS-ICESS.2015.261","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336217","sphere-gap transferring algorithm;CNOP;parallel;Zebiak-Cane model","Heuristic algorithms;Algorithm design and analysis;Computational modeling;Atmospheric modeling;Prediction algorithms;Numerical models;Optimization","climate mitigation;convergence of numerical methods;geophysics computing;message passing;parallel algorithms;singular value decomposition;weather forecasting","intelligent algorithms;conditional nonlinear optimal perturbation;numerical weather predictability;climate prediction;high-dimensionality complex numerical model;parallel dynamic step size sphere-gap transferring algorithm;complex models;convergence;singular value decomposition;low-dimensional space;optimal CNOP;DSGT algorithm;computation speed;MPI technology;Zebiak-Cane model","","5","26","","","","","","IEEE","IEEE Conferences"
"On Characterization of Performance and Energy Efficiency in Heterogeneous HPC Cloud Data Centers","A. Qouneh; N. Goswami; R. Zhou; T. Li","NA; NA; NA; NA","2014 IEEE 22nd International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems","","2014","","","315","320","The relocation of high performance computing systems (HPC) to the cloud poses new challenges for data center architects and IT managers. These challenges are due to heterogeneity injected into data centers by cutting-edge virtualization technologies and hardware accelerators used to support emerging cloud applications and services. Although hardware accelerators like General Purpose Graphics Processing Units (GPGPUs) and virtualization technologies have been well studied and evaluated individually, a detailed analysis of their combined architectures and collective behavior from the data center point of view is lacking. Using real platforms and high performance computing workloads, we study the power performance tradeoffs due to various granularities of heterogeneity across hardware and software layers and expose hidden opportunities for optimizing overall data center efficiency. Our approach is to evaluate server power and performance from a data center point of view as opposed to evaluating hardware accelerators and virtualization technologies themselves. Our results show that performance on cloud is affected by virtualization overhead and fraction of serial code. Moreover, GPU workloads achieve 25% and 30% savings in power and energy consumption when executed on low power platforms, and only 50% of our GPU workloads are more energy efficient than their corresponding CPU implementations. The results also show that it is much more power efficient to collocate GPU virtual machines with non-GPU virtual machines.","1526-7539;2375-0227","978-1-4799-5610","10.1109/MASCOTS.2014.46","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7033668","heterogeneous;high performance computing;data center;efficiency;cloud","Graphics processing units;Benchmark testing;Virtualization;Servers;Energy consumption;Hardware;Computer architecture","cloud computing;computer centres;graphics processing units;parallel processing;power aware computing;virtual machines;virtualisation","performance characterization;energy efficiency;heterogeneous HPC cloud data centers;high-performance computing system relocation;virtualization technologies;hardware accelerators;cloud applications;cloud services;general purpose graphics processing units;GPGPU;real platforms;high-performance computing workloads;heterogeneity granularities;hardware layers;software layers;overall data center efficiency optimization;server power evaluation;performance evaluation;virtualization overhead;serial code fraction;GPU workloads;power savings;energy consumption;low-power platforms;CPU implementations;GPU virtual machine collocation;nonGPU virtual machines","","2","22","","","","","","IEEE","IEEE Conferences"
"Deadlock-Free Routing Algorithms for 6D Mesh/iBT Interconnection Networks","R. Feng; P. Zhang; Y. Deng","NA; NA; NA","2013 14th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing","","2013","","","275","282","As an application of interlaced bypass torus (iBT) interconnection networks, a 6D mesh/iBT network has been formed by replacing the 3D torus network in a 6D mesh/torus interconnect (Tofu) with a 3D iBT network, helping further reduce latencies. However, the routing algorithms good for the torus-based systems such as Blue Gene series may deadlock for the new network. This work proposes three deadlock-free routing algorithms for iBT networks and an optimizing method. An iBT network with these routings is simulated and compared with a 3D torus and a 4D torus network. Results for all-to-all communications, when applying the optimal routing algorithm, show a link utilization of 96% of the theoretical peak. An iBT prototyped system is also built and preliminarily tested.","","978-0-7695-5005","10.1109/SNPD.2013.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6598477","routing algorithm;deadlock;interconnection network;simulation;performance evaluation","Routing;Receivers;Three-dimensional displays;System recovery;Multiprocessor interconnection;Heuristic algorithms;Adaptation models","multiprocessing systems;multiprocessor interconnection networks;parallel machines;performance evaluation","deadlock-free routing algorithms;6D mesh-iBT interconnection networks;interlaced bypass torus interconnection networks;3D torus network;latency reduction;torus-based systems;Blue Gene series;optimizing method;performance evaluation;Exascale supercomputers","","1","20","","","","","","IEEE","IEEE Conferences"
"Performance Comparison of OLSR Protocol by Experiments and Simulations for Different TC Packet Intervals","M. Hiyama; S. Sakamoto; E. Kulla; M. Ikeda; S. Caballe; L. Barolli","NA; NA; NA; NA; NA; NA","2014 Eighth International Conference on Complex, Intelligent and Software Intensive Systems","","2014","","","38","43","Mobile Ad-hoc Networks (MANETs) have an increased interest in applications for covering rural areas due to the possibility of usage of low-cost and high-performance mobile terminals, without having to depend on the network infrastructure. MANET terminals are mobile and routes change dynamically, so routing algorithms are an important issue for operation of MANETs. Optimized Link State Routing (OLSR) is a widely-used proactive routing protocol for MANET. In this paper, we investigate the behavior of OLSR Protocol for different values of TC packet interval time by experiments and simulations. We conduct experiments in a MANET test bed and simulations in QualNet environment. The performance is investigated for different number of hops, different TC interval values and two scenarios. From results we found that the low value of TC packet interval increases the throughput performance. While in case of experiments, the throughput is decreased because the route changes very often.","","978-1-4799-4325-8978-1-4799-4326","10.1109/CISIS.2014.6","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6915494","MANET;OLSR;TC Interval;Testbed;Qual-Net;Throughput;Indoor;Outdoor;Static;Moving","Ad hoc networks;Mobile computing;Throughput;Routing protocols;Mobile communication;Routing","mobile ad hoc networks;routing protocols","OLSR protocol;TC packet intervals;mobile ad-hoc networks;MANET terminals;routing algorithms;optimized link state routing protocol;proactive routing protocol","","","22","","","","","","IEEE","IEEE Conferences"
"Increasing the efficiency of minimal key enumeration methods by means of parallelism","F. Benito; P. Cordero; M. Enciso; Á. Mora","Department of Languages and Computer Science, University of Málaga, Spain; Department of Applied Mathematics, University of Málaga, Spain; Department of Languages and Computer Science, University of Málaga, Spain; Department of Applied Mathematics, University of Málaga, Spain","2014 9th International Conference on Software Engineering and Applications (ICSOFT-EA)","","2014","","","512","517","Finding all minimal keys in a table is a hard problem but also provides a lot of benefits in database design and optimization. Some of the methods proposed in the literature are based on logic and, more specifically on tableaux paradigm. The size of the problems such methods deal with is strongly limited, which implies that they cannot be applied to big database schemas. We have carried out an experimental analysis to compare the results obtained by these methods in order to estimate their limits. Although tableaux paradigm may be viewed as a search space guiding the key finding task, none of the previous algorithms have incorporated parallelism. In this work, we have developed two different versions of the algorithms, a sequential and a parallel one, stating clearly how parallelism could naturally be integrated and the benefits we get over efficiency. This work has also guided future work guidelines to improve future designs of these methods.","","978-9-8975-8124-3978-1-4799-7688","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7293909","Functional Dependency;Minimal Key;Parallelism;Logic;Tableaux-method","Parallel processing;Benchmark testing;Databases;Algorithm design and analysis;Design methodology;Data mining;Buildings","","","","","16","","","","","","IEEE","IEEE Conferences"
"Revisiting symbiotic job scheduling","S. Eyerman; P. Michaud; W. Rogiest","Ghent University, Belgium; INRIA Rennes, France; Ghent University, Belgium","2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)","","2015","","","124","134","Symbiotic job scheduling exploits the fact that in a system with shared resources, the performance of jobs is impacted by the behavior of other co-running jobs. By coscheduling combinations of jobs that have low interference, the performance of a system can be increased. In this paper, we investigate the impact of using symbiotic job scheduling for increasing throughput. We find that even for a theoretically optimal scheduler, this impact is very low, despite the substantial sensitivity of per job performance to which other jobs are coscheduled: for example, our experiments on a 4-thread SMT processor show that, on average, the job IPC varies by 37% depending on coscheduled jobs, the per-coschedule throughput varies by 69%, and yet the average throughput gain brought by optimal symbiotic scheduling is only 3%. This small margin of improvement can be explained by the observation that all the jobs need to be eventually executed, restricting the job combinations a symbiotic job scheduler can select to optimize throughput. We explain why previous work reported a substantial gain from symbiotic job scheduling, and we find that (only) reporting turnaround time can lead to misleading conclusions. Furthermore, we show how the impact of scheduling can be evaluated in microarchitectural studies, without having to implement a scheduler.","","978-1-4799-1957","10.1109/ISPASS.2015.7095791","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7095791","","Throughput;Symbiosis;Optimal scheduling;Multicore processing;Processor scheduling;Measurement;Benchmark testing","processor scheduling","symbiotic job scheduling;shared resources;jobs performance;co-running jobs;jobs coscheduling;system performance;4-thread SMT processor;job IPC;coschedule throughput;optimal symbiotic scheduling;job combinations","","2","40","","","","","","IEEE","IEEE Conferences"
"Evaluating the Performance and Energy Consumption of Distributed Data Management Systems","R. Niemann","NA","2015 IEEE 10th International Conference on Global Software Engineering Workshops","","2015","","","27","34","In the last couple of years it became more difficult for end users of data management system to optimize existing systems for performance and energy consumption. The reasons are the big number of data management systems to chose from, extensive use cases and a variety of hardware configurations. The number of factors that influence the performance and energy efficiency rises even more when it comes to distributed data management systems. For instance, in addition to vertical scale-out effects the horizontal scale-out ones have to be considered. This paper introduces an enhanced version of a Queued Petri Nets model whose simulation runs allow to predict and to study the performance and energy consumption of a distributed data management system. In contrast to traditional ways, the simulation runs can reduce both investments in time and hardware. The model's prediction in terms of performance and energy consumption were evaluated and compared with the actual experimental results. The predicted and experimental results for the response times differ by nearly 20 percent on average.","2329-6305;2329-6313","978-1-4799-9874","10.1109/ICGSEW.2015.14","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7227530","Distributed data management system;Energy consumption;Performance;Queued Petri Nets","Data models;Distributed databases;Servers;Energy consumption;Operating systems;Computational modeling;Benchmark testing","distributed processing;energy consumption;Petri nets;power aware computing","energy consumption;distributed data management systems;energy efficiency;vertical scale-out effects;horizontal scale-out effects;queued Petri nets model","","1","14","","","","","","IEEE","IEEE Conferences"
"Support vector machine based approach for quranic words detection in online textual content","T. Sabbah; A. Selamat","Faculty of Computing, Universiti Teknologi Malaysia (UTM), 81310 Skudai, Johor, Malaysia; Faculty of Computing, Universiti Teknologi Malaysia (UTM), 81310 Skudai, Johor, Malaysia","2014 8th. Malaysian Software Engineering Conference (MySEC)","","2014","","","325","330","Quran is the holy book for Muslims around the world. Since it was revealed to the Prophet Muhammad (PBUH) before about 14 hundreds years, Quran is preserved in all imaginable ways from distortion. The rapid and huge growth of digital media and internet usage, cause a wide spread of the Quranic knowledge as well as Quranic Verses, scripts, Translations, and many other Quranic sciences in its digital formats. Some of the online sources, websites, services and social network users are introducing a less authentic Quranic content, services and applications. The ordinary user of such online resources could not detect and authenticate the provided Quranic verses. In this paper, we propose a machine learning approach to detect Quranic words in a text extracted from online sources. The proposed approach of detection utilizes Support Vector Machine to generate a learning model of Quranic words by training the learner on the Quranic words dataset. The generated classification model is used later to classify the words from online content. Experiments based on different features categories such as the Diacritics, and Statistical features are performed and a prototype is developed, Results show that the accuracy and other evaluation measurements achieved by the proposed approach are higher than the previous measurement in the domain. The Future works will focus on incorporating more machine learning and optimization techniques for achieving higher evaluation measurements.","","978-1-4799-5439","10.1109/MySec.2014.6986038","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6986038","Quranic words;Arabic words;detection;classification;learning model;Support Vector Machine","Support vector machines;Feature extraction;Training;Accuracy;Testing;Prototypes;Authentication","learning (artificial intelligence);pattern classification;support vector machines;text analysis","support vector machine;Quranic words detection;online textual content;Quranic sciences;Quranic knowledge;Web sites;social network;Quranic content;machine learning approach;learning model;classification model;diacritics feature;statistical feature","","3","19","","","","","","IEEE","IEEE Conferences"
"Comparison of stamp classification using SVM and random ferns","P. Petej; S. Gotovac","University of Split, Faculty of Electrical Engineering, Mechanical Engineering and Naval Architecture, Croatia; University of Split, Faculty of Electrical Engineering, Mechanical Engineering and Naval Architecture, Croatia","2013 IEEE Symposium on Computers and Communications (ISCC)","","2013","","","000850","000854","In distributed software systems and processes that use large amounts of documents there is an essential need for data mining and document classification algorithms. These algorithms are aimed at optimizing the process, making it less error prone. In this paper we deal with the problem of document classification using two machine learning algorithms. Both algorithms use stamp images in documents to classify the document itself. The idea is to classify the document stamp and then, using known information about the stamp owner, search the rest of the document for relevant data. Our results are based on actual documents used in the process of debt collection and our training and test datasets are randomly picked from an existing database with over three million documents. The mentioned machine learning classification algorithms are implemented and compared in terms of classification accurateness, robustness and speed.","1530-1346","978-1-4799-3755","10.1109/ISCC.2013.6755055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755055","documents;classification;stamps;SVM;random;ferns;distributed;software;system","Supervised learning;Robustness;Graphics processing units;Testing;Training;Support vector machines;Image recognition","document image processing;image classification;learning (artificial intelligence)","machine learning classification algorithms;debt collection;document stamp classification;stamp images;random ferns;SVM","","","17","","","","","","IEEE","IEEE Conferences"
"A novel autonomous management distributed system for cloud computing environments","R. Dinita; G. Wilson; A. Winckles; M. Cirstea; T. Rowsell","Computing and Technology, Anglia Ruskin University, Cambridge, United Kingdom; Computing and Technology, Anglia Ruskin University, Cambridge, United Kingdom; Computing and Technology, Anglia Ruskin University, Cambridge, United Kingdom; Computing and Technology, Anglia Ruskin University, Cambridge, United Kingdom; Computing and Technology, Anglia Ruskin University, Cambridge, United Kingdom","IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society","","2013","","","5620","5625","This paper describes a novel modular design of an autonomous management distributed system (AMDS) for cloud computing environments and it presents its implementation with the Scala programming language. The AMDS was designed from the ground up with distributed deployment, modularity and security in mind, using a full object oriented approach. A key feature of this system is the ability to gather and store information from various networking and monitoring devices from within the same computing cluster. Another key feature is the ability to intelligently control VMWare vSphere local instances based on analysis of collected data and predefined parameters. vSphere in turn, once it receives commands from the AMDS, proceeds to issue instructions to multiple locally monitored ESXi severs in order to maximize energy efficiency, reduce the carbon footprint and minimize running costs. The predefined parameters are based on results from a previous paper written by the authors. The AMDS has been deployed on the authors' test bed and is currently running successfully. Test results show highly potential industrial applications in datacenter energy management and lowering of operating costs.","1553-572X","978-1-4799-0224","10.1109/IECON.2013.6700055","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6700055","cloud;distributed;energy;optimisation;software","Databases;Power demand;Servers;Java;Hardware;Cloud computing;Monitoring","cloud computing;functional languages;green computing;object-oriented languages;power aware computing;security of data;virtual machines","autonomous management distributed system;cloud computing;Scala programming language;AMDS;modularity;security;object oriented approach;VMWare vSphere local instances;intelligent control;ESXi servers;energy efficiency;carbon footprint reduction;running costs minimization;datacenter energy management;operating costs minimization;modular design;virtualized machines;type 1 hypervisor","","2","13","","","","","","IEEE","IEEE Conferences"
"Activity-driven exploration of chemical space with morphing","M. Šícho; D. Svozil; D. Hoksza","Laboratory of Informatics and Chemistry, University of Chemistry and Technology, Prague, Czech Republic; Laboratory of Informatics and Chemistry, University of Chemistry and Technology, Prague, Czech Republic; Faculty of Mathematics and Physics, Charles University in Prague, Czech Republic","2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)","","2015","","","1024","1031","Virtual screening (VS) methods, which became a common complement to the in vitro approaches in drug discovery projects, are naturally restricted by the compound libraries at hand while ignoring the wealth of compounds hidden in general chemical space. To close this gap, various methods for the exploration of chemical space have been proposed. One such approach is Molpher, a software framework that uses the technique of molecular morphing. Molecular morphing generates a series of compounds called morphs that represent a gradual structural transition between two given compounds. Because the exploration is driven solely by structural information, it disregards structurally diverse but possibly active compounds. Thus, we introduce the improvement of the algorithm where the exploration is driven by ligand biological activity rather than by its structure. On its input, the method takes a set of known active and inactive compounds. In the preparatory phase, feature selection is applied to choose descriptors that likely discriminate between active and inactive compounds. These features are then used to define a reference point towards which the exploration is directed. In the exploration phase, morphs are generated from all active compounds and Pareto-ranking scheme is applied to accept morphs for the next generation of molecular morphing. This iterative process results in structurally diverse molecules that share characteristic features of actives that separate them from inactives. The method was tested on four datasets from the PubChem BioAssay database. The results indicate that an activity-based exploration technique is able to generate structurally diverse compounds close to the selected point in the activity space. Thus, this technique is suitable for the generation of virtual libraries that can be further optimized and subsequently screened.","","978-1-4673-6799-8978-1-4673-6798","10.1109/BIBM.2015.7359824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359824","","","biochemistry;chemistry computing;drugs;feature selection;iterative methods;molecular biophysics;Pareto analysis","activity-driven exploration;virtual screening method;drug discovery;chemical space exploration;Molpher;software framework;molecular morphing;virtual libraries;structurally diverse compound generation;activity-based exploration technique;PubChem BioAssay database;iterative process;Pareto-ranking scheme;inactive compounds;feature selection;ligand biological activity;structural information;gradual structural transition","","","26","","","","","","IEEE","IEEE Conferences"
"Time-Dimension Communication Characterization of Representative Scientific Applications on Tianhe-2","W. Zhou; J. Chen; Z. Wang; X. Xu; L. Xu; Y. Tang","NA; NA; NA; NA; NA; NA","2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems","","2015","","","423","429","Exascale computing is one of the major challenges of this decade, and several studies have shown that the communication is becoming one of the bottlenecks for scaling parallel applications. The characteristic analysis of communication is an important means to improve the performance of scientific applications. In this paper, we focus on the statistical regularity in time-dimension communication characteristics of representative scientific applications and find that the distribution of interval of communication events has a power-law decay, which is widely found in scientific interests and human activities. For a quantitative study on characteristics of power-law distribution, we count two groups of typical measures: bursty vs. memory and periodicity vs. dispersion. Our analysis shows that the communication events reflect a ""strong-bursty and weak-memory"" characteristic and we also capture the periodicity and dispersion in interval distribution. All of the quantitative results are verified with eight representative scientific applications on Tianhe-2 supercomputer with a fat-tree-like interconnection network. Finally, our study provides an insight on the relationship between communication optimization and time-dimension communication characteristics.","","978-1-4799-8937-9978-1-4799-8936","10.1109/HPCC-CSS-ICESS.2015.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7336198","Power-law distributions;Supercomputing;Time-dimension Communication Characterization;Tianhe-2","Dispersion;Benchmark testing;Histograms;Supercomputers;Libraries;High performance computing","parallel machines;power aware computing;statistical analysis","statistical regularity;time-dimension communication characteristics;power-law decay;power-law distribution;Tianhe-2 supercomputer;interconnection network","","","37","","","","","","IEEE","IEEE Conferences"
"Verification & validation of simulation code for Linear Fresnel systems","S. Benyakhlef; A. Al Mers; A. Bouatem; N. Boutammachte; O. Merroun","Department of Energy, Ecole Nationale Supérieure d'Arts et Métiers, Meknes, Morocco; Department of Energy, Ecole Nationale Supérieure d'Arts et Métiers, Meknes, Morocco; Department of Energy, Ecole Nationale Supérieure d'Arts et Métiers, Meknes, Morocco; Department of Energy, Ecole Nationale Supérieure d'Arts et Métiers, Meknes, Morocco; Laboratory of Science Engineering, Ecole Nationale Supérieure d'Arts et Métiers, Casablanca, Morocco","2014 International Renewable and Sustainable Energy Conference (IRSEC)","","2014","","","214","218","This work belongs to a research project supported by the IRESEN (Institute of Research on Solar and New Energy-Morocco). Its main goal is to develop an innovative solar thermal field using the Fresnel technology. In order to analyze, optimize and design Linear Fresnel solar concentrating systems, a global simulation code has been developed. The developed code is based on the Monte Carlo /Ray Tracing method and would allow us the visualization and the analysis of optical interactions and the simulation of radiative transfer between the sun and the receiver. It was necessary to verify and validate our simulation code before starting the optimization step. Thus, several verification-validation tests have been established. These tests are based on the comparison of our simulation results for different geometrical configurations with those obtained by SOLTRACE software. The main parameters of comparison would be based on the optical-geometrical interaction, the flux density distribution and the statistical parameters related to the Monte Carlo algorithm.","2380-7385;2380-7393","978-1-4799-7336-1978-1-4799-7335","10.1109/IRSEC.2014.7059856","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7059856","Solar energy;CSP systems;Modelisation;SOLTRACE code;Monte Carlo Method;Fresnel Technology;Ray Tracing Method","Integrated optics;Optical reflection;Geometrical optics;Software;Optical receivers;Visualization","Monte Carlo methods;radiative transfer;ray tracing;solar absorber-convertors","linear Fresnel systems;innovative solar thermal field;Fresnel technology;solar concentrating systems;global simulation code;Monte Carlo method;ray tracing method;optical interactions;radiative transfer;verification-validation tests;optical-geometrical interaction;flux density distribution","","2","7","","","","","","IEEE","IEEE Conferences"
"Analysis of parallel performance of MPI based parallel FDTD on supercomputer","Shugang Jiang; Zhaofeng Lv; Yu Zhang; Bing Wei; Xunwang Zhao","School of Electronic Engineering, Xidian University, Xi'an, Shaanxi 710071, China; School of Electronic Engineering, Xidian University, Xi'an, Shaanxi 710071, China; NA; School of Science, Xidian University, Xi'an, Shaanxi 710071, China; School of Electronic Engineering, Xidian University, Xi'an, Shaanxi 710071, China","IET International Radar Conference 2013","","2013","","","1","4","With the development of multi-core processors, computer clusters and the application of parallel MPI technique, the study of electromagnetic fields numerical arithmetic can improve the capacity of solving large problems. In this paper, based on the platform from National Supercomputing Center in Tianjin (NSCC-TJ: Ranked first in the world in 2010.11), some tests about the parallel efficiency of the parallel FDTD code are made to provide a general rule for a best performance of FDTD on multi-core computers. The results show that following this rule, we can achieve the highest computational efficiency on a more advanced platform.","","978-1-84919-603","10.1049/cp.2013.0486","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6624650","parallel efficiency;parallel FDTD;multi-core computers","","application program interfaces;computational electromagnetics;electromagnetic fields;finite difference time-domain analysis;mainframes;message passing;multiprocessing systems;parallel programming;software performance evaluation","parallel performance analysis;supercomputer;multicore processors;computer clusters;parallel MPI technique;electromagnetic fields;numerical arithmetic;NSCC-TJ;National Supercomputing Center in Tianjin;parallel FDTD code efficiency;multicore computers","","1","","","","","","","IET","IET Conferences"
"Performance-optimized indexes for inequality searches on encrypted data in practice","J. Lehnhardt; T. Rho; A. Spalka; A. B. Cremers","Department of Security and System Architecture, CGMAG, Koblenz, Germany; Department of Security and System Architecture, CGMAG, Koblenz, Germany; Department of Security and System Architecture, CGMAG, Koblenz, Germany; Department of Computer Science III, University of Bonn, Bonn, Germany","2015 International Conference on Information Systems Security and Privacy (ICISSP)","","2015","","","221","229","For information systems in which the server must operate on encrypted data (which may be necessary because the service provider cannot be trusted) solutions need to be found that enable fast searches on that data. In this paper we present an approach for encrypted database indexes that enable fast inequality, i.e., range searches, such that also prefix searches on lexicographically ordered but encrypted data are possible. Unlike common techniques that address this issue as well, like hardware-based solutions or order-preserving encryption schemes, our indexes do not require specialized, expensive hardware and use only well-accredited software components; they also do not reveal any information about the encrypted data besides their order. Moreover, when implementing the indexing approach in a commercial software product, multiple application-centric optimization opportunities of the index's performance did emerge, which are also presented in this paper. They include basic performance-increasing measures, pipelined index scans and updates and caching strategies. We further present performance test results proving that our indexing approach shows good performance on substantial amounts of data.","","978-989-758-135-9978-1-4673-8405","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7509960","Databases;Indexes;Cryptography;Cloud-based Information Systems","Indexes;Encryption;Servers;Binary trees","","","","","22","","","","","","IEEE","IEEE Conferences"
"Trace-based automated logical debugging for high-level synthesis generated circuits","P. Fezzardi; M. Castellana; F. Ferrandi","Politecnico di Milano, Milano, Italy; Politecnico di Milano, Milano, Italy; Politecnico di Milano, Milano, Italy","2015 33rd IEEE International Conference on Computer Design (ICCD)","","2015","","","251","258","In this paper we present an approach for debugging hardware designs generated by High-Level Synthesis (HLS), relieving users from the burden of identifying the signals to trace and from the error-prone task of manually checking the traces. The necessary steps are performed after HLS, independently of it and without affecting the synthesized design. For this reason our methodology should be easily adaptable to any HLS tools. The proposed approach makes full use of HLS compile time informations. The executions of the simulated design and the original C program can be compared, checking if there are discrepancies between values of C variables and signals in the design. The detection is completely automated, that is, it does not need any input but the program itself and the user does not have to know anything about the overall compilation process. The design can be validated on a given set of test cases and the discrepancies are detected by the tool. Relationships between the original high-level source code and the generated HDL are kept by the compiler and shown to the user. The granularity of such discrepancy analysis is per-operation and it includes the temporary variables inserted by the compiler. As a consequence the design can be debugged as is, with no restrictions on optimizations available during HLS. We show how this methodology can be used to identify different kind of bugs: 1) introduced by the HLS tool used for the synthesis; 2) introduced using buggy libraries of hardware components for HLS; 3) undefined behavior bugs in the original high-level source code.","","978-1-4673-7166-7978-1-4673-7165","10.1109/ICCD.2015.7357111","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7357111","","Debugging;Optimization;Computer bugs;Observability;Controllability;Hardware;Layout","C language;electronic design automation;high level synthesis;source code (software)","trace-based automated logical debugging;high-level synthesis generated circuits;hardware design debugging;signal identification;HLS tools;compile time informations;C program;high-level source code;HDL;compiler","","4","18","","","","","","IEEE","IEEE Conferences"
"I-sieve: An inline high performance deduplication system used in cloud storage","Jibin Wang; Zhigang Zhao; Zhaogang Xu; Hu Zhang; Liang Li; Ying Guo","Shandong Computer Science Center (National Supercomputer Center in Jinan), Shandong Provincial Key Laboratory of Computer Networks, 250101, China; Shandong Computer Science Center (National Supercomputer Center in Jinan), Shandong Provincial Key Laboratory of Computer Networks, 250101, China; Shandong Computer Science Center (National Supercomputer Center in Jinan), Shandong Provincial Key Laboratory of Computer Networks, 250101, China; Shandong Computer Science Center (National Supercomputer Center in Jinan), Shandong Provincial Key Laboratory of Computer Networks, 250101, China; Shandong Computer Science Center (National Supercomputer Center in Jinan), Shandong Provincial Key Laboratory of Computer Networks, 250101, China; Shandong Computer Science Center (National Supercomputer Center in Jinan), Shandong Provincial Key Laboratory of Computer Networks, 250101, China","Tsinghua Science and Technology","","2015","20","1","17","27","Data deduplication is an emerging and widely employed method for current storage systems. As this technology is gradually applied in inline scenarios such as with virtual machines and cloud storage systems, this study proposes a novel deduplication architecture called I-sieve. The goal of I-sieve is to realize a high performance data sieve system based on iSCSI in the cloud storage system. We also design the corresponding index and mapping tables and present a multi-level cache using a solid state drive to reduce RAM consumption and to optimize lookup performance. A prototype of I-sieve is implemented based on the open source iSCSI target, and many experiments have been conducted driven by virtual machine images and testing tools. The evaluation results show excellent deduplication and foreground performance. More importantly, I-sieve can co-exist with the existing deduplication systems as long as they support the iSCSI protocol.","1007-0214","","10.1109/TST.2015.7040510","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7040510","I-sieve;cloud storage;data deduplication","Random access memory;Optimization;Indexing;Cloud computing;Engines;Performance evaluation","cache storage;cloud computing;parallel processing;public domain software;random-access storage;table lookup;virtual machines","inline high performance deduplication system;data deduplication;virtual machines;cloud storage systems;I-sieve deduplication architecture;high performance data sieve system;mapping tables;multilevel cache;solid state drive;RAM consumption;lookup performance;open source iSCSI target;virtual machine images;iSCSI protocol","","3","","","","","","","TUP","TUP Journals & Magazines"
"A Parallel and Pipelined Architecture for Accelerating Fingerprint Computation in High Throughput Data Storages","D. Li; Q. Yang; Q. Wang; C. Guyot; A. Narasimha; D. Vucinic; Z. Bandic","NA; NA; NA; NA; NA; NA; NA","2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines","","2015","","","203","206","Rabin fingerprints are short tags for large objects that can be used in a wide range of applications, such as data deduplication, web querying, packet routing, and caching. We present a pipelined hardware architecture for computing Rabin fingerprints on data being transferred on a high throughput bus. The design conducts real-time fingerprinting with short latencies, and can be tuned for optimized clock rate with ""split fresh"" technique. A pipelined sampling logic selects fingerprints based on the Minwise theory and adds only a few clock cycles of latency before returning the final results. The design can be replicated to work in parallel for higher throughput data traffic. This architecture is implemented on a Xilinx Virtex-6 FPGA, and is tested on a storage prototyping platform. The implementation shows that the design can achieve clock rates above 300 MHz with an order of magnitude improvement in latency over prior software implementations, while consuming little hardware resource. The scheme is extensible to other types of fingerprints and CRC computations, and is readily applicable to primary storages and caches in hybrid storage systems.","","978-1-4799-9969","10.1109/FCCM.2015.43","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160072","Rabin fingerprint;data deduplication;caching;parallel and pipeline architecture;storage;PCIe;NVMe","Fingerprint recognition;Clocks;Pipelines;Hardware;Computer architecture;Throughput;Pipeline processing","cache storage;field programmable gate arrays;logic design;logic testing;parallel architectures","parallel architecture;fingerprint computation acceleration;high throughput data storages;rabin fingerprints;pipelined hardware architecture;split fresh technique;pipelined sampling logic;Minwise theory;clock cycles;high throughput data traffic;Xilinx Virtex-6 FPGA;storage prototyping platform;CRC computations;hybrid storage systems;high throughput bus;optimized clock rate","","1","10","","","","","","IEEE","IEEE Conferences"
"Real time evaluation of shortest remaining processing time based schedulers for traffic congestion control using wireless sensor networks","F. Ahmad; I. Khan; S. A. Mahmud; G. M. Khan; F. Z. Yousaf","University of Engineering and Technology, Peshawar, Pakistan; University of Engineering and Technology, Peshawar, Pakistan; University of Engineering and Technology, Peshawar, Pakistan; University of Engineering and Technology, Peshawar, Pakistan; NEC Laboratories Europe, Heidelberg, Germany","2013 International Conference on Connected Vehicles and Expo (ICCVE)","","2013","","","381","387","Pre-timed traffic signals are inefficient in optimizing the traffic flow throughout the day, resulting in greater waiting times at the intersections particularly in congested urban areas during peak hours. Traffic actuated signals use real time traffic data obtained from sensors at the intersections to service queues intelligently. We developed a test bed for the real time evaluation of adaptive traffic light control algorithms using the microscopic traffic simulation open source software, SUMO (Simulation of Urban Mobility), and the AVR 32-bit microcontroller. An interface was developed between SUMO and the AVR microcontroller in which we used the simulation data generated by SUMO as an input to the microcontroller which executed the scheduling algorithms and sent commands back to SUMO for changing the states of the traffic signals accordingly. We implemented four scheduling algorithms in SUMO through the AVR microcontroller, the effect of the algorithms on the traffic network was studied using SUMO and execution times of the scheduling algorithms were measured using the AVR microcontroller. Through this interface, scheduling algorithms can be evaluated more effectively and accurately as compared to the case in which the algorithms are fed with data using pseudo random number generators.","2378-1289;2378-1297","978-1-4799-2491","10.1109/ICCVE.2013.6799824","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799824","Adaptive Traffic Light Control;Intelligent Transportation Systems;Scheduling;SUMO;Traffic Actuated Signal Control","Microcontrollers;Vehicles;Scheduling algorithms;Real-time systems;Data models;Ports (Computers);Adaptation models","microcontrollers;public domain software;random number generation;scheduling;traffic control;traffic engineering computing;wireless sensor networks","real time evaluation;shortest remaining processing time based schedulers;traffic congestion control;wireless sensor networks;pretimed traffic signals;traffic flow;real time traffic data;microscopic traffic simulation open source software;SUMO;simulation of urban mobility;AVR microcontroller;pseudo random number generators;adaptive traffic light control algorithms","","1","17","","","","","","IEEE","IEEE Conferences"
"Design and development of a prototype application for intrusion detection using data mining","K. Jaswal; P. Kumar; S. Rawat","Amity University, UP, India; Amity University, UP, India; Amity University, UP, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","In the world of communication, security is a big concern. Most of our crucial data is stored in a computer system and in most cases we exchange it over a network. But it's not just our data transmitting over the network but different types of attacks too. These attacks can harm our stored data. Monitoring computer system and its logs (administration logs, security logs, system logs, network logs) and protecting our crucial data is necessary. For these necessities we use intrusion detection system. An intrusion detection system is an application that provides protection from malicious activities or policy violations and generates various rules to defend computer security. Intrusion detection system can be designed and developed on any platform but for its better functionality we are using data mining technique. In past years, many techniques have been introduced to improvise the detection rate. Earlier, in the initial stages of its designing, hardware had to be installed to detect and monitor the system. But, with the help of data mining it has become easier to work with software and algorithm development. In the recent trends, many new algorithms have been introduced to increase its efficiency. They are categorized under machine learning algorithms: supervised, unsupervised and hybrid. Though hybrid has not yet been categorized finely but various authors have used it by merging different machine learning algorithms. Machine learning algorithms provide a process of detecting intrusion and generating rules for its detection and prevention. Rule generation is defined by association rule mining and apriori algorithm. An intrusion detection system is not a new application but developing a prototype which will work for the saved logs (administration logs, security logs and system logs) and monitor network logs on host system as well as on client system, so that the Intrusion detection system can alert the user on regular basis, is. In this paper we are using a hybrid machine learning algorithm following with rule generation algorithm to detect the intrusion in the network logs by training KDD dataset. Training and Testing KDD data set provides the way of analyzing the actual behavior and predicted behavior of the network logs. This paper shows the methodology and results in Netbeans IDE 8.1(Java platform using Weka library). Through the obtained results, it can be interpreted that optimal performance is obtained against most attacks after detection.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359266","IDs;apriori algorithm;association rule;supervised approach;unsupervised approach;hybrid approach","Computers;Security","data mining;data protection;learning (artificial intelligence);security of data","data mining;computer system monitoring;data protection;intrusion detection system;malicious activity;computer security;data mining technique;rule generation;association rule mining;apriori algorithm;hybrid machine learning algorithm;KDD dataset;Netbeans IDE 8.1","","1","16","","","","","","IEEE","IEEE Conferences"
"Rapid heterogeneous prototyping from Simulink","S. Feng; C. Driscoll; J. Fevold; H. Jiang; G. Schirner","Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, 02115; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, 02115; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, 02115; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, 02115; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, 02115","Sixteenth International Symposium on Quality Electronic Design","","2015","","","141","146","Designing embedded high-performance systems is challenging due to complex algorithms, real-time operations and conflicting goals (e.g. power v.s. performance). Heterogeneous platforms that combine processors and custom hardware accelerators are a promising approach. However, manually designing HW/SW systems is prohibitively expensive due to the immense manual effort. This paper introduces SimSH: Simulink Sw/Hw CoDesign Framework, which provides an automatic path from an algorithm captured in Simulink to a heterogeneous implementation. Given an allocation and a mapping decision, the SimSH automatically synthesizes the Simulink model onto the heterogeneous target with reconstruction of the synchronization and communication between processing elements. In the process, the SimSH detects an underutilized bus and optimizes communication by packing/unpacking. Synthesizing a heterogeneous implementation from Simulink allows the developer to focus on the algorithm design with rapid validation and test on a heterogeneous platform. We demonstrate synthesis benefits using a Sobel Edge Detection algorithm and target a heterogeneous architecture of Blackfin processor and Spar-tan3E FPGA. The synthesized solution is 2.68x faster (and energy efficient) over pure SW execution.","1948-3287;1948-3295","978-1-4799-7581-5978-1-4799-7580","10.1109/ISQED.2015.7085414","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7085414","","Software packages;Computational modeling;Hardware design languages;Field programmable gate arrays;Digital signal processing;Resource management;Algorithm design and analysis","integrated circuit design;multiprocessing systems;software prototyping;system-on-chip","heterogeneous target;custom hardware accelerators;Simulink;rapid heterogeneous prototyping","","1","15","","","","","","IEEE","IEEE Conferences"
"Adaptive Transversal digital Filter for reference current detection in shunt active power filter","A. A. S. Mohamed; A. Berzoy; O. A. Mohammed","Energy Systems Research Laboratory, Department of Electrical and Computer Engineering, Florida International University, Miami, USA; Energy Systems Research Laboratory, Department of Electrical and Computer Engineering, Florida International University, Miami, USA; Energy Systems Research Laboratory, Department of Electrical and Computer Engineering, Florida International University, Miami, USA","2015 IEEE Power & Energy Society General Meeting","","2015","","","1","5","In this paper, an effective closed loop reference current extraction topology for single phase active power filter applications, is proposed. The main idea of the proposed topology is based on the Adaptive Transversal Filter (ATF) principle. The adaptive filter is designed and optimized using genetic algorithm (GA). The proposed methodology is simulated and tested in MatLab software. The results indicate the ability of the method to extract harmonics, inter-harmonics and noise in case of stationary and non-stationary situations. Moreover, it shows a robust detection performance under transient of supply frequency.","1932-5517","978-1-4673-8040","10.1109/PESGM.2015.7286555","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7286555","Adaptive Reference Current Detection;Single-Phase Shunt Active Power Filter;Harmonics;Inter-harmonics;Adaptive Transversal Filter;Adaptive Filter Design;Optimization;Genetic Algorithm","Power harmonic filters;Harmonic analysis;Active filters;Adaptive filters;Transversal filters;Voltage control","active filters;adaptive filters;closed loop systems;digital filters;genetic algorithms;power filters;transversal filters","adaptive transversal digital filter;reference current detection;shunt active power filter;closed loop reference current extraction topology;single phase active power filter;genetic algorithm","","","13","","","","","","IEEE","IEEE Conferences"
"A comparison of alerting strategies for hemorrhage identification during prehospital emergency transport","J. Liu; A. T. Reisner; S. Edla; J. Reifman","Department of Defense Biotechnology High Performance Computing Software Applications Institute (BHSAI), Telemedicine and Advanced Technology Research Center (TATRC), U.S. Army Medical Research and Materiel Command (USAMRMC), Fort Detrick, MD 21702 USA; BHSAI, TATRC, USAMRMC, Fort Detrick, MD 21702 USA; BHSAI, TATRC, USAMRMC, Fort Detrick, MD 21702 USA; BHSAI, TATRC, USAMRMC, Fort Detrick, MD 21702 USA","2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","","2014","","","2670","2673","Early and accurate identification of physiological abnormalities is one feature of intelligent decision support. The ideal analytic strategy for identifying pathological states would be highly sensitive and highly specific, with minimal latency. In the field of manufacturing, there are well-established analytic strategies for statistical process control, whereby aberrancies in a manufacturing process are detected by monitoring and analyzing the process output. These include simple thresholding, the sequential probability ratio test (SPRT), risk-adjusted SPRT, and the cumulative sum method. In this report, we applied these strategies to continuously monitored prehospital vital-sign data from trauma patients during their helicopter transport to level I trauma centers, seeking to determine whether one strategy would be superior. We found that different configurations of each alerting strategy yielded widely different performances in terms of sensitivity, specificity, and average time to alert. Yet, comparing the different investigational analytic strategies, we observed substantial overlap among their different configurations, without any one analytic strategy yielding distinctly superior performance. In conclusion, performance did not depend as much on the specific analytic strategy as much as the configuration of each strategy. This implies that any analytic strategy must be carefully configured to yield the optimal performance (i.e., the optimal balance between sensitivity, specificity, and latency) for a specific use case. Conversely, this also implies that an alerting strategy optimized for one use case (e.g., long prehospital transport times) may not necessarily yield performance data that are optimized for another clinical application (e.g., short prehospital transport times, intensive care units, etc.).","1094-687X;1558-4615","978-1-4244-7929","10.1109/EMBC.2014.6944172","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6944172","","Biomedical monitoring;Sensitivity;Accuracy;Hemorrhaging;Process control;Monitoring;Medical diagnostic imaging","decision support systems;emergency management;injuries;medical computing;medical disorders;statistical process control","intensive care units;short prehospital transport times;trauma centers;helicopter transport;trauma patients;continuously monitored prehospital vital-sign data;cumulative sum method;risk-adjusted SPRT;sequential probability ratio test;process output;manufacturing process;statistical process control;minimal latency;pathological states;intelligent decision support;physiological abnormality;prehospital emergency transport;hemorrhage identification;alerting strategies","Adult;Emergency Medical Services;Female;Hemorrhage;Hospitals;Humans;Male;Middle Aged;Monitoring, Physiologic;Sensitivity and Specificity;Time Factors;Transportation of Patients","","11","","","","","","IEEE","IEEE Conferences"
"Network-constrained day-ahead auction for consumer payment minimization","R. Fernández-Blanco; J. Arroyo; N. Alguacil","Ing. Eléctrica, Electrónica, Automática y Comunicaciones, Univ. de Castilla - La Mancha; Ing. Eléctrica, Electrónica, Automática y Comunicaciones, Univ. de Castilla - La Mancha; Ing. Eléctrica, Electrónica, Automática y Comunicaciones, Univ. de Castilla - La Mancha","2014 IEEE PES General Meeting | Conference & Exposition","","2014","","","1","1","Summary form only given. This paper presents an alternative day-ahead auction based on consumer payment minimization for pool-based electricity markets. This auction is an instance of price-based market clearing wherein market-clearing prices are explicitly modeled as decision variables of the optimization. The auction design includes network constraints, inter-temporal constraints associated with generation scheduling, and marginal pricing. Hence, consumer payment is expressed in terms of locational marginal prices. The proposed solution approach is based on bilevel programming. In the upper-level optimization, generation is scheduled with the goal of minimizing the total consumer payment while taking into account that locational marginal prices are determined by a multiperiod optimal power flow in the lower level. In this bilevel programming setting, locational marginal prices are the Lagrange multipliers or dual variables associated with the nodal power balance equations of the lower-level problem. The resulting mixed-integer linear bilevel program is transformed into an equivalent single-level mixed-integer linear program suitable for efficient off-the-shelf software. This transformation relies on the application of results from duality theory of linear programming and integer algebra. The proposed methodology has been successfully applied to several test systems including the IEEE 118-bus system. Numerical results have been compared with those obtained from declared social welfare maximization.","1932-5517","978-1-4799-6415","10.1109/PESGM.2014.6938821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6938821","","Minimization;Optimization;Programming;Electricity supply industry;Pricing;Load flow;Equations","integer programming;linear programming;load flow;minimisation;power generation scheduling;power markets;pricing","social welfare maximization;IEEE 118-bus system;integer algebra;linear programming;duality theory;mixed integer linear bilevel program;multiperiod optimal power flow;locational marginal prices;power generation scheduling;intertemporal constraints;auction design;price based market clearing;pool based electricity markets;consumer payment minimization;network-constrained day-ahead auction","","","","","","","","","IEEE","IEEE Conferences"
"Performance Evaluation and Estimation Model Using Regression Method for Hadoop WordCount","J. A. Issa","Department of Electrical and Computer Engineering, Notre Dame University at Louaize, Zouk Mosbeh, Lebanon","IEEE Access","","2015","3","","2784","2793","Given the rapid growth in cloud computing, it is important to analyze the performance of different Hadoop MapReduce applications and to understand the performance bottleneck in a cloud cluster that contributes to higher or lower performance. It is also important to analyze the underlying hardware in cloud cluster servers to enable the optimization of software and hardware to achieve the maximum performance possible. Hadoop is based on MapReduce, which is one of the most popular programming models for big data analysis in a parallel computing environment. In this paper, we present a detailed performance analysis, characterization, and evaluation of Hadoop MapReduce WordCount application. We also propose an estimation model based on Amdahl's law regression method to estimate performance and total processing time versus different input sizes for a given processor architecture. The estimation regression model is verified to estimate performance and run time with an error margin of <;5%.","2169-3536","","10.1109/ACCESS.2015.2509598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7360871","performance analysis;cloud computing;doop WordCount;Performance analysis;cloud computing;Hadoop WordCount","Cloud computing;Estimation;Computational modeling;Analytical models;Computer architecture;Hardware;Benchmark testing","Big Data;cloud computing;data analysis;parallel processing;regression analysis;software performance evaluation","processor architecture;Amdahl law regression method;Hadoop MapReduce WordCount application;parallel computing environment;Big Data analysis;cloud cluster servers;cloud computing;performance estimation model;performance evaluation","","3","22","","","","","","IEEE","IEEE Journals & Magazines"
"Reconstruction of function block logic using metaheuristic algorithm: Initial explorations","D. Chivilikhin; A. Shalyto; S. Patil; V. Vyatkin","Computer Technologies Laboratory, ITMO University, Saint Petersburg, Russia; Computer Technologies Laboratory, ITMO University, Saint Petersburg, Russia; Department of Computer Science, Computer and Space Engineering, Lulea Tekniska Universitet, Sweden; Computer Technologies Laboratory, ITMO University, Saint Petersburg, Russia","2015 IEEE 13th International Conference on Industrial Informatics (INDIN)","","2015","","","1239","1242","This paper presents an approach for automatic reconstruction of automation logic from execution scenarios using a metaheuristic algorithm. The IEC 61499 basic function blocks is chosen as implementation language and reconstruction of Execution Control Charts for basic function blocks is addressed. The synthesis method is based on a metaheuristic algorithm most closely related to ant colony optimization and evolutionary computation. Execution scenarios can be recorded from testing legacy software solutions. At this stage results are only limited to generation of basic function blocks having only Boolean input/output variables.","1935-4576;2378-363X","978-1-4799-6649-3978-1-4799-6648","10.1109/INDIN.2015.7281912","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7281912","","IEC Standards;Input variables;Inference algorithms;Labeling;Algorithm design and analysis;Manipulators;Computers","ant colony optimisation;Boolean algebra;control charts;evolutionary computation;programmable controllers","function block logic;metaheuristic algorithm;automation logic;IEC 61499 basic function;execution control chart;ant colony optimization;evolutionary computation;Boolean input-output variable","","4","8","","","","","","IEEE","IEEE Conferences"
"Rate distortion modeling and adaptive rate control scheme for high efficiency video coding (HEVC)","L. Sun; O. C. Au; C. Zhao; F. H. Huang","Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, China; Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, China; Lenovo Corporate Research Hong Kong Branch, China; ABN Impact, China","2014 IEEE International Symposium on Circuits and Systems (ISCAS)","","2014","","","1933","1936","This paper explores a novel rate control scheme for HEVC, based on Pre-analysis Sum of Absolute Transformed Differences (pSATD). The needs for improving the rate control method of HEVC have been observed since the latest method proposed in JCT-VC H213 is still using the classical quadratic model which is implemented in H.264/AVC. The same rate control scheme on different coding structures will definitely lead to a poor performance. The proposed method aims at selecting accurate quantization parameters (QP) for inter frames according to the target bit rate. After encoding video sequences exhaustively, a strong linear relationship between the pSATD and bpp (bit per pixel) is revealed. Thus, a rate control scheme for HEVC with parameters updating adaptively is proposed accordingly. Apart from that, a new rate and distortion model are established using pSATD and Quantization Step Size (QStep). By minimizing the cost function built based on the proposed rate-distortion (R-D) model, the optimization problem can be solved properly. According to the optimal solution, a bit allocation scheme with consideration of pSATD and buffer status is formed. The experimental results illustrate that the proposed method outperforms JCT-VC H213 implemented in HEVC reference software for all test sequences, up to 19% coding gain can be achieved.","0271-4302;2158-1525","978-1-4799-3432-4978-1-4799-3431","10.1109/ISCAS.2014.6865539","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6865539","pSATD;Rate Model;Distortion Model;adaptive rate control;HEVC","Encoding;Bit rate;Adaptation models;Video coding;Quantization (signal);Curve fitting;Computational modeling","data compression;optimisation;quantisation (signal);rate distortion theory;video codecs;video coding","rate distortion modeling;adaptive rate control scheme;high efficiency video coding;HEVC;pre-analysis sum of absolute transformed differences;pSATD;JCT-VC H213;classical quadratic model;H.264/AVC;quantization parameters;QP;video sequences;quantization step size;QStep;R-D model;optimization problem;bit allocation scheme;buffer status","","6","12","","","","","","IEEE","IEEE Conferences"
"Active torque control of electric power steering system using composite nonlinear feedback control","N. H. Ling; Y. M. Sam","Faculty of Electrical Engineering, University Teknologi Malaysia, 81310, UTM Skudai, Johor Bahru, Malaysia; Faculty of Electrical Engineering, University Teknologi Malaysia, 81310, UTM Skudai, Johor Bahru, Malaysia","2015 IEEE Student Conference on Research and Development (SCOReD)","","2015","","","150","155","Electric power steering (EPS) system plays an important role in assisting the driver to achieve better handling, steering feel and response in securing vehicle stability. In EPS control system, controlling the torque of the EPS motor is crucial where tracking of the robust command of motor angle to generate desired assist torque is very challenging. In this paper, a reference model is adopted to generate an ideal motor angle that can guarantee the desired performance and composite nonlinear feedback (CNF) control is applied to achieve a faster transient response in tracking the robust reference motor angle. The H2 state optimization is used to obtain the linear control parameters whereas Hooke-Jeeves auto-tuning algorithm is used to fine-tune the nonlinear control parameters to achieve the best transient performance. The proposed control strategy is validated with CarSim Software by interfacing Simulink model with CarSim full nonlinear vehicle model under different test procedures and compared to other controllers based on ITAE performance criteria.","","978-1-4673-9572-4978-1-4673-9571","10.1109/SCORED.2015.7449313","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7449313","Electric power steering (EPS);composite nonlinear feedback (CNF);CarSim;Hooke-Jeeves auto-tuning","Mathematical model;Vehicles;Torque;Control systems;Tires;Roads;Transient analysis","feedback;H<sup>2</sup> control;linear systems;nonlinear control systems;optimisation;road vehicles;robust control;steering systems;torque control;transient response","active torque control;electric power steering system;EPS system;composite nonlinear feedback control;CNF control;robust command;motor angle;transient response;H2 state optimization;linear control parameter;Hooke-Jeeves autotuning algorithm;road vehicle stability","","","13","","","","","","IEEE","IEEE Conferences"
"Open JDK Meets Xeon Phi: A Comprehensive Study of Java HPC on Intel Many-Core Architecture","Y. Yu; T. Lei; H. Chen; B. Zang","NA; NA; NA; NA","2015 44th International Conference on Parallel Processing Workshops","","2015","","","156","165","The increasing demand for performance has stimulated the wide adoption of many-core accelerators like Intel Xeon Phi Coprocessor, which is based on Intel's Many Integrated Core architecture. While many HPC applications running in native mode have been tuned to run efficiently on Xeon Phi, it is still unclear how a managed runtime like JVM performs on such an architecture. In this paper, we present the first measurement study of a set of Java HPC applications on Xeon Phi under JVM. One key obstacle to the study is that there is currently little support of Java for Xeon Phi. This paper presents the result based on the first porting of Open JDK platform to Xeon Phi, in which Hot Spot VM acts as the kernel execution engine. The main difficulty includes the incompatibility between Xeon Phi ISA and the assembly library of Hotspot VM. By evaluating the multithreaded Java Grande benchmark suite, we quantitatively study the performance and scalability issues of JVM on Xeon Phi and draw several conclusions from the study. To fully utilize the vector computing capability, we also present a semi-automatic vectorization in Hot Spot VM. Together with this optimization and tuning, our optimized JVM achieves up to 3.4X speedup with 60 physical cores compared to that on Xeon CPU processor. Our study indicates that it is viable and potentially performance-beneficial to run applications written for managed runtime on Xeon Phi.","1530-2016","978-1-4673-7589","10.1109/ICPPW.2015.25","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349907","Many-core;Java;Xeon Phi;HPC","Java;Coprocessors;Computer architecture;Benchmark testing;Libraries;Instruction sets;Scalability","coprocessors;Java;multiprocessing programs;parallel processing;software architecture","Open JDK;Java HPC;Intel many-core architecture;many-core accelerators;Intel Xeon Phi coprocessor;Hot Spot VM;Xeon Phi ISA;semiautomatic vectorization","","","33","","","","","","IEEE","IEEE Conferences"
"PipeCheck: Specifying and Verifying Microarchitectural Enforcement of Memory Consistency Models","D. Lustig; M. Pellauer; M. Martonosi","NA; NA; NA","2014 47th Annual IEEE/ACM International Symposium on Microarchitecture","","2014","","","635","646","We present PipeCheck, a methodology and automated tool for verifying that a particular micro architecture correctly implements the consistency model required by its architectural specification. PipeCheck adapts the notion of a ""happens before"" graph from architecture-level analysis techniques to the micro architecture space. Each node in the ""micro architecturally happens before"" (μhb) graph represents not only a memory instruction, but also a particular location (e.g., Pipeline stage) within the micro architecture. Architectural specifications such as ""preserved program order"" are then treated as propositions to be verified, rather than simply as assumptions. PipeCheck allows an architect to easily and rigorously test whether a micro architecture is stronger than, equal in strength to, or weaker than its architecturally-specified consistency model. We also specify and analyze the behavior of common micro architectural optimizations such as speculative load reordering which technically violate formal architecture-level definitions. We evaluate PipeCheck using a library of established litmus tests on a set of open-source pipelines. Using PipeCheck, we were able to validate the largest pipeline, the Open SPARC T2, in just minutes. We also identified a bug in the O3 pipeline of the gem5 simulator.","1072-4451;2379-3155","978-1-4799-6998","10.1109/MICRO.2014.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011423","","Microarchitecture;Pipelines;Load modeling;Program processors;Radio frequency;Buffer storage;Mathematical model","formal specification;multiprocessing systems;pipeline processing;program verification;software architecture","PipeCheck;microarchitectural enforcement verification;microarchitectural enforcement specification;memory consistency models;architectural specification;architecture-level analysis techniques;microarchitecture space;microarchitecturally-happens-before graph;μhb graph;memory instruction;memory location;pipeline stage;preserved-program order;architecturally-specified consistency model;microarchitectural optimizations;speculative load reordering;formal architecture-level definitions;open-source pipelines;OpenSPARC T2;O3 pipeline;gem5 simulator","","14","48","","","","","","IEEE","IEEE Conferences"
"Designing a fuzzy logic controller for the Reynolds number in a blowdown supersonic wind tunnel","A. N. Shahrbabaki; M. Bazazzadeh; M. D. Manshadi; A. Shahriari","Department of Mechanical &amp; Aerospace Engineering, Malek-Ashtar University of Technology, Shahinshahr, Isfahan, Iran, 83145/115; Department of Mechanical &amp; Aerospace Engineering, Malek-Ashtar University of Technology, Shahinshahr, Isfahan, Iran, 83145/115; Department of Mechanical &amp; Aerospace Engineering, Malek-Ashtar University of Technology, Shahinshahr, Isfahan, Iran, 83145/115; Department of Mechanical &amp; Aerospace Engineering, Malek-Ashtar University of Technology, Shahinshahr, Isfahan, Iran, 83145/115","2014 IEEE Aerospace Conference","","2014","","","1","12","Blowdown supersonic wind tunnel (BSWT) is a ground based facility to simulate flight conditions of space vehicles in the supersonic flow regime. BSWTs are generally operated with a constant stagnation pressure in the settling chamber, and constant Reynolds number in the test section, with control usually provided by one or more control valves. In this paper, first, the nonlinear mathematical model of the special SWT consisting of a set of ordinary differential and algebraic equation was developed in Matlab/Simulink software environment. At the second step, an Artificial Neural Network (ANN) is used for finding the optimum membership functions of the Fuzzy Logic Controller (FLC) system. This method can help for reasonable system recognition. In this step, by designing and training a feed-forward multilayer perceptron neural network according to the available database which is generated from mathematical model; a number of different reasonable functions for valve opening angle (VOA) in various test conditions are determined. These functions are used to define the desired VOA fuzzy membership functions (MFs). Next, a Proportional-Derivative FLC (PD-FLC) system is developed in the Simulink toolbox to control a relationship between stagnation pressure and temperature in the plenum chamber, which presents the Reynolds number in the test section. A synthetic algorithm combined from FLC and ANN is used to design a controller for a blowdown SWT with the aim of achieving the accurate and acceptable desired results. Performance of the blowdown SWT using optimized fuzzy controller by ANN is found to be satisfactory, as confirmed by the results.","1095-323X","978-1-4799-1622-1978-1-4799-5582","10.1109/AERO.2014.6836242","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6836242","","Valves;Mathematical model;Storage tanks;Equations;Software packages;Fuzzy logic;Electric shock","aerodynamics;aerospace computing;aerospace simulation;algebra;flow control;fuzzy control;mechanical engineering computing;multilayer perceptrons;nonlinear differential equations;PD control;supersonic flow;wind tunnels","blowdown supersonic wind tunnel;BSWT;ground based facility;space vehicles;supersonic flow regime;constant stagnation pressure;constant Reynolds number;nonlinear mathematical model;ordinary differential equation;algebraic equation;Matlab/Simulink software environment;artificial neural network;ANN;optimum membership functions;fuzzy logic controller system;feed-forward multilayer perceptron neural network;valve opening angle;VOA fuzzy membership functions;proportional-derivative FLC system;PD-FLC system;plenum chamber","","4","24","","","","","","IEEE","IEEE Conferences"
"Assessment of Adaptive Traffic Signal Control Using Hardware in the Loop Simulation","H. Abdelgawad; K. Rezaee; S. El-Tantawy; B. Abdulhai; T. Abdulazim","NA; NA; NA; NA; NA","2015 IEEE 18th International Conference on Intelligent Transportation Systems","","2015","","","1189","1195","Adaptive Traffic Signal Control (ATSC) can potentially mitigate traffic congestion. Research and development in the area of ATSC have produced a number of new systems with promising potential, however the performance of these systems under real-life conditions has been always a concern for practitioners as well as researchers, particularly if and how the new systems would be implementable in the field on controllers with specific capabilities and limitations. Therefore, testing and refining new ATSC systems on actual hardware and under representative traffic conditions prior to field implementation is essential to bridge this gap. In this paper, a hardware in-the-loop simulation (HILS) framework is developed to evaluate MARLIN, as an example of a new self-learning ATSC system. HILS is used for evaluating hardware components running the ATSC software in a simulation environment in which an actual traffic signal controller and an embedded computer are physically connected to a microscopic traffic simulator. Our focus is on the development, implementation of the HILS framework and the evaluation of MARLIN, on an intersection that suffers significant traffic fluctuation and delays - at the City of Burlington, Ontario, Canada. The performance of MARLIN-ATSC is demonstrated with HILS, which consists of a PEEK ATC-1000 traffic controller, an embedded computer running the ATSC system, and Paramics microscopic simulation model. HILS results indicated that MARLIN-ATSC has the potential to reduce the intersection average delay by up to 20% on average compared to the optimized and coordinated actuated signal timing plans.","2153-0017;2153-0009","978-1-4673-6596-3978-1-4673-6595","10.1109/ITSC.2015.196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313288","Hardware in the loop simulation;adaptive traffic signal control;microsimulation;software and hardware","Hardware;Computers;Standards;Computational modeling;Detectors;Adaptation models;Timing","embedded systems;learning (artificial intelligence);road pricing (tolls);road traffic control","microscopic traffic simulator;City of Burlington;Ontario;Canada;PEEK ATC-1000 traffic controller;embedded computer running;Paramics embedded computer;embedded computer;ATSC software;self-learning ATSC system;MARLIN;HILS;hardware in-the-loop simulation;representative traffic conditions;loop simulation;adaptive traffic signal control assessment","","","32","","","","","","IEEE","IEEE Conferences"
"Research and tests on hydrodynamic performance of a seabed seated horizontal axis tidal turbine","S. Wang; Y. Zhang; P. Yuan; J. Tan","Engineering College, Ocean University of China, 238 Songling Road, Qingdao City, Shandong Province, P.R. China, 266100; Engineering College, Ocean University of China, 238 Songling Road, Qingdao City, Shandong Province, P.R. China, 266100; Engineering College, Ocean University of China, 238 Songling Road, Qingdao City, Shandong Province, P.R. China, 266100; Engineering College, Ocean University of China, 238 Songling Road, Qingdao City, Shandong Province, P.R. China, 266100","OCEANS 2015 - Genova","","2015","","","1","7","Study of this article was conducted on horizontal axis tidal turbines in a demonstration project of a 500kW ocean energy independent power supply system. Considering the depth of the sea site where the turbines would be deployed, seabed seated supporting structure was adopted in construction of two turbine units. As a typical tidal current energy resource zone in North part of China, the flow velocity of the sea area is relatively low for exploitation. In order to extract energy from tidal current with high efficiency and easy to start running, special structure of the turbines and optimized control technology should be adopted. In this article, theoretical analysis was conducted on the rotor and blades' foils of the turbine and numerical simulation on hydrodynamic performances of various turbine blades and hydrofoils was made by using software FLUENT. Then scale model tests of the turbines were performed in a flume to validate the analysis and simulation results. By adopting blade pitch angle adjusting technology with maximum power point tracking control, the turbines can easily start running in low speed current, revers blade direction and work with highest efficiency.","","978-1-4799-8736-8978-1-4673-7164","10.1109/OCEANS-Genova.2015.7271451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271451","tidal current energy;horizontal axis tidal turbine;hydrodynamic performance;power coefficient","Blades;Torque;Mathematical model;Hydraulic turbines;Hydrodynamics;Force","hydrodynamics;maximum power point trackers;tidal power stations;turbines","hydrodynamic performance;seabed seated horizontal axis tidal turbine;ocean energy independent power supply system;tidal current energy resource zone;flow velocity;turbine blades;hydrofoils;blade pitch angle;maximum power point tracking control;power 500 kW","","","8","","","","","","IEEE","IEEE Conferences"
"XML data interface and PMU testing applications for synchrophasor estimation with a reconfigurable controller","A. Katsimichas; P. Brogan; D. Laverty","EPIC, Queens University Belfast; EPIC, Queens University Belfast; EPIC, Queens University Belfast","2014 49th International Universities Power Engineering Conference (UPEC)","","2014","","","1","5","Phasor Measurement Unit (PMU) technology has rapidly improved over the last two decades. This development allowed a number of conventional protocols, both open and proprietary, to be written. Optimised communication and implementation techniques are sought by academia. This motivation has resulted in university projects, such as The OpenPMU project. The aim is to create a completely open source system that will be cost efficient, reliable, modular and platform independent. The discussed work is focused on the detailed description of a data interface XML based communication tool. The outlined schema is used to exchange data between the acquisition system and the phase estimation algorithm of a PMU. Its many advantages establish it as a flexible schema for transmitting sampled data irrespective of hardware or software implementation. The OpenPMU system is based on hardware implementations that use freely available DAQ devices and aim to develop a low cost alternative to commercial PMU's. The hardware described in this paper is based on a National Instruments field programmable gate array (FPGA) reconfigurable controller (cRIO) that due to its parallel nature can perform different processing operations at the same time. This results in higher speed and accuracy and should provide the benchmark for testing various existing and future OpenPMU implementations.","","978-1-4799-6557-1978-1-4799-6556","10.1109/UPEC.2014.6934779","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6934779","OpenPMU;phasor measurement unit;field programmable gate array;National Instruments;phasor estimation","XML;Phasor measurement units;Field programmable gate arrays;Estimation;Payloads;Global Positioning System;Hardware","data acquisition;field programmable gate arrays;phasor measurement;XML","XML data interface;phasor measurement unit;synchrophasor estimation;OpenPMU project;open source system;communication tool;acquisition system;phase estimation algorithm;OpenPMU system;national instruments;field programmable gate array;FPGA;reconfigurable controller","","","18","","","","","","IEEE","IEEE Conferences"
"SociaLite: Datalog extensions for efficient social network analysis","J. Seo; S. Guo; M. S. Lam","Computer Systems Laboratory, Stanford University, Stanford, CA, USA 94305; Computer Systems Laboratory, Stanford University, Stanford, CA, USA 94305; Computer Systems Laboratory, Stanford University, Stanford, CA, USA 94305","2013 IEEE 29th International Conference on Data Engineering (ICDE)","","2013","","","278","289","With the rise of social networks, large-scale graph analysis becomes increasingly important. Because SQL lacks the expressiveness and performance needed for graph algorithms, lower-level, general-purpose languages are often used instead. For greater ease of use and efficiency, we propose SociaLite, a high-level graph query language based on Datalog. As a logic programming language, Datalog allows many graph algorithms to be expressed succinctly. However, its performance has not been competitive when compared to low-level languages. With SociaLite, users can provide high-level hints on the data layout and evaluation order; they can also define recursive aggregate functions which, as long as they are meet operations, can be evaluated incrementally and efficiently. We evaluated SociaLite by running eight graph algorithms (shortest paths, PageRank, hubs and authorities, mutual neighbors, connected components, triangles, clustering coefficients, and betweenness centrality) on two real-life social graphs, Live-Journal and Last.fm. The optimizations proposed in this paper speed up almost all the algorithms by 3 to 22 times. SociaLite even outperforms typical Java implementations by an average of 50% for the graph algorithms tested. When compared to highly optimized Java implementations, SociaLite programs are an order of magnitude more succinct and easier to write. Its performance is competitive, giving up only 16% for the largest benchmark. Most importantly, being a query language, SociaLite enables many more users who are not proficient in software engineering to make social network queries easily and efficiently.","1063-6382;1063-6382","978-1-4673-4910-9978-1-4673-4909-3978-1-4673-4908","10.1109/ICDE.2013.6544832","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6544832","","Aggregates;Social network services;Algorithm design and analysis;Optimization;Java;Semantics;Arrays","DATALOG;graph theory;Java;pattern clustering;recursive functions;social networking (online)","Datalog extension;social network analysis;large-scale graph analysis;graph algorithm;lower-level general-purpose language;SociaLite;high-level graph query language;logic programming language;data layout;evaluation order;recursive aggregate function;shortest path;PageRank;hubs;clustering coefficient;betweenness centrality;real-life social graph;Java;software engineering","","2","36","","","","","","IEEE","IEEE Conferences"
"TMS320C66x multicore DSP efficiency in radar imaging applications","S. Vityazev; A. Kharin; V. Savostyanov; V. Vityazev","Faculty of Radio Engineering and Telecommunications, Ryazan State Radio Engineering University, RSREU, Ryazan, Russia; Faculty of Radio Engineering and Telecommunications, Ryazan State Radio Engineering University, RSREU, Ryazan, Russia; Faculty of Radio Engineering and Telecommunications, Ryazan State Radio Engineering University, RSREU, Ryazan, Russia; Faculty of Radio Engineering and Telecommunications, Ryazan State Radio Engineering University, RSREU, Ryazan, Russia","2015 4th Mediterranean Conference on Embedded Computing (MECO)","","2015","","","115","118","Radar imaging is a very attractive application of digital signal processing widely used in modern space-, air- and ground-borne systems. Radar imaging algorithms are featured with very high computational requirements. New trends in processor platforms with a high level of computational power are very important for radar imaging implementation. TMS320C66x multicore DSP from Texas Instruments is one of a promising computational platform for radar imaging systems. It demonstrates good performance in embedded computing with low power consumptions. A test task implementation on TMS320C6678 multicore DSP is considered in this paper. The test task is described and implemented stage by stage on one core of TMS320C6678 device with an analysis of processing time limitations. 8-core implementation is considered then. It is shown that test task is done in 4.9 ms with 1 core and 0.99 ms with 8 cores implementation. The paper demonstrates an efficiency of TMS320C66x DSP platform in radar imaging applications and discusses the questions of an efficient multicore programming.","2377-5475","978-1-4799-1976-5978-1-4799-8999-7978-1-4799-8998","10.1109/MECO.2015.7181880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7181880","Radar Imaging;TMS320C6678;multicore;optimization;DSP","Multicore processing;Digital signal processing;Radar imaging;Signal processing algorithms;Arrays;Algorithm design and analysis;Software","multiprocessing systems;radar imaging;signal processing","TMS320C66x multicore DSP efficiency;radar imaging applications;digital signal processing;Texas Instruments;radar imaging systems;embedded computing;low power consumptions;8-core implementation;multicore programming","","","11","","","","","","IEEE","IEEE Conferences"
"Rapid prototyping of digital controllers for microgrid inverters","S. Buso; T. Caldognetto","DEI - Dept. of Information Engineering, University of Padova, via G. Gradenigo 6/b, 35131 - ITALY; DEI - Dept. of Information Engineering, University of Padova, via G. Gradenigo 6/b, 35131 - ITALY","IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society","","2013","","","3711","3716","A rapid prototyping methodology for digital controllers is presented in this paper. Its main application is in the development, debugging and test of microgrid inverter controllers. To fulfill the application requirements, these systems are characterized by complex multilayer architectures, extending from PWM and current control loops up to global optimization and high level communication functions. The complexity and the wide variability of the different layer implementations make digital control mandatory. However, developing so complex digital controllers on conventional hardware platforms, like DSPs or even FPGAs, is not the most practical choice. The paper shows how multi-platform control devices, where software configurable DSP functions and programmable logic circuits are efficiently combined, represent the optimal solution for this field of application. Furthermore, the paper proposes hardware in the loop real time simulation as an effective means of developing and debugging complex hardware and software co-designed controllers. A case study is presented and used to illustrate the different design and test phases, from initial concept and numerical simulation to final experimental verification.","1553-572X","978-1-4799-0224","10.1109/IECON.2013.6699726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6699726","Microgrids;inverters;digital control;real time systems","Inverters;Hardware;Field programmable gate arrays;Density estimation robust algorithm;Computational modeling;Pulse width modulation;Digital signal processing","control engineering computing;digital control;distributed power generation;electric current control;invertors;power engineering computing;program debugging;programmable controllers","hardware software codesigned controllers;complex hardware debugging;programmable logic circuits;software configurable DSP functions;multiplatform control devices;microgrid inverters;digital controllers;rapid prototyping","","1","9","","","","","","IEEE","IEEE Conferences"
"Diversified Remote Code Execution Using Dynamic Obfuscation of Conditional Branches","M. Hataba; R. Elkhouly; A. El-Mahdy","NA; NA; NA","2015 IEEE 35th International Conference on Distributed Computing Systems Workshops","","2015","","","120","127","Information leakage via timing side-channel attacksis one of the main threats that target code executing on remoteplatforms such as the cloud computing environment. Theseattacks can be further leveraged to reverse-engineer or eventamper with the running code. In this paper, we propose asecurity obfuscation technique, which helps making the generatedcode more resistant to these attacks, by means of increasinglogical complexity to hinder the formulation of a solid hypothesisabout code behavior. More importantly, this software solutionis portable, generic and does not require special setup orhardware or software modifications. In particular, we considermangling the control-flow inside a program via converting arandom set of conditional branches into linear code, using ifconversiontransformation. Moreover, our method exploits thedynamic compilation technology to continually and randomlyalter the branches. All of this mangling should diversify codeexecution, hence it becomes difficult for an attacker to infertiming correlations through statistical analysis. We extend theLLVM JIT compiler to provide for an initial investigation of thisapproach. This makes our system applicable to a wide varietyof programming languages and hardware platforms. We havestudied the system using a simple test program and selectedbenchmarks from the standard SPEC CPU 2006 suite withdifferent input loads and experimental setups. Initial results showsignificant changes in program's control-flow and hence datadependences, resulting in noticeable different execution timeseven for the same input data, thereby complicating such attacks.More notably, the performance penalty is within reasonablemargins.","1545-0678;2332-5666","978-1-4673-7303","10.1109/ICDCSW.2015.37","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165094","Obfuscation;Side-Channels;JIT Compilation;If-","Runtime;Security;Hardware;Program processors;Optimization;Cloud computing;Benchmark testing","cloud computing;program compilers;security of data","diversified remote code execution;dynamic obfuscation;conditional branches;information leakage;timing side-channel attacks;cloud computing environment;security obfuscation technique;dynamic compilation technology;statistical analysis;LLVM JIT compiler;standard SPEC CPU 2006 suite","","","44","","","","","","IEEE","IEEE Conferences"
"The design and implementation of acoustic echo cancellation subsystem for multi-party videoconferencing systems","Xu Deng; Congxiao Bao; Xing Li","CERNET Research Center, Tsinghua University, Beijing, China; CERNET Research Center, Tsinghua University, Beijing, China; CERNET Research Center, Tsinghua University, Beijing, China","2013 The International Conference on Technological Advances in Electrical, Electronics and Computer Engineering (TAEECE)","","2013","","","152","156","The voice quality of videoconferencing system is seriously affected by acoustic echoes. AEC (acoustic echo cancellation) itself is delicate and complex. It becomes more difficult to cancel acoustic echoes with pure software in an Internet multi-party videoconferencing scenario because of the varying operating system time-delay, complex echo path and network delay jitters. We propose a novel effective and adaptive software AEC subsystem for multi-party videoconferencing systems. By analyzing the characteristics of echo path delay we originally propose a system time-delay estimator and a reference signal delay adjuster to reduce AEC computation amount and to improve its real-time performance. The AEC subsystem is fully implemented, embedded into a new multi-party videoconferencing system (DVTS Plus) and then tested in a real conference scenario. Results show that our proposed subsystem could provide good conference experience with echo free voice in high quality for all users in a multi-party videoconference with small additional amount of computation. The average value of ERLE could reach 50 db during double-talk and it provides a perfect real time performance. Thanks to the computation amount decrease, our proposed AEC subsystem could work perfectly in larger conference room compared to other existing commercial AEC solutions.","","978-1-4673-5613-8978-1-4673-5612-1978-1-4673-5611","10.1109/TAEECE.2013.6557213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6557213","acoustic echo cancellation;MDF;multi-party network video conference;computing optimizing;system timedelay estimation;real-time performance","Multimedia communication;Streaming media;Loudspeakers;Microphones","echo suppression;Internet;jitter;operating systems (computers);teleconferencing;video communication","acoustic echo cancellation subsystem;AEC;Internet multiparty videoconferencing systems;voice quality;varying operating system time-delay;complex echo path;network delay jitters;adaptive software AEC subsystem;echo path delay;system time-delay estimator;reference signal delay adjuster;AEC computation amount reduction;real-time performance improvement;DVTS Plus;noise figure 50 dB","","","12","","","","","","IEEE","IEEE Conferences"
"Prediction Model Using Coverability Tree from a Modeling in Petri Nets Applied in AGVs Dispatching","V. F. Caridá; O. Morandin; P. R. Cerioni; C. C. M. Tuma","NA; NA; NA; NA","2014 Brazilian Conference on Intelligent Systems","","2014","","","140","145","In the last years, the industries are applying automation techniques with the aim to increase their efficiency to remain competitive. Due to this fact, there is an intensive search for techniques and methods applied to manufacturing systems for improvements, whether in quality, service deadlines and / or increased production. Several studies and practical applications have indicated that the Automated Guided Vehicles (AGVs) are efficient for transport task in industries and warehouses. The management of these AGVs is the key to a transportation system that ensures the improvements envisioned by industries. One of the main problems encountered in the management of AGVs is the decision to dispatch. Some authors suggest that a weak point of dispatching rules, even the multi-attributes, is to consider only the values of variables in current time for decision-making. This paper proposes a prediction model in which one achieves an improvement in the optimization objective by reading from specific states of the factory in the near future. The proposal is based on the use of cover ability tree from modelling in Petri nets with ability to provide important data for AGVs dispatch system. From these data, the dispatch system can take more assertive decisions to optimize the performance of Flexible Manufacturing Systems (FMS). The tests are performed using the software CPNtools, to model Petri nets, and Simio software to build the virtual environment, thereby having two different levels of abstraction. The validation is done through the analysis of scenarios that can happen in a production system, and with the use of the proposal of prediction is verified that in these scenarios is possible to extract important information.","","978-1-4799-5618","10.1109/BRACIS.2014.35","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6984821","Prediction;Coverability Tree;AGV Dispacthing;Petri Nets","Dispatching;Predictive models;Production facilities;Petri nets;Proposals;Production systems","automatic guided vehicles;control engineering computing;flexible manufacturing systems;Petri nets;production engineering computing","virtual environment;Simio software;CPNtools;FMS;flexible manufacturing systems;optimization objective;decision-making;transportation system;automated guided vehicles;service deadlines;automation techniques;AGV dispatching;Petri nets;coverability tree;prediction model","","","32","","","","","","IEEE","IEEE Conferences"
"Soft computing tool for restoring failures within wireless sensor networks","S. J. Habib; P. N. Marimuthu; K. Sudha","Computer Engineering Department, Kuwait University, P. O. Box 5969 Safat 13060 Kuwait; Computer Engineering Department, Kuwait University, P. O. Box 5969 Safat 13060 Kuwait; Computer Engineering Department, Kuwait University, P. O. Box 5969 Safat 13060 Kuwait","2014 International Wireless Communications and Mobile Computing Conference (IWCMC)","","2014","","","86","91","The proposed soft computing tool is developed as a suite of software programs capable of envisaging the evolution within wireless sensor networks (WSN) by accomplishing a series of processes. These processes are: modelling a real problem scenario in computing form, formulating the problem as an optimization problem, evaluating the outcomes and finally, presenting the outcomes in a statistical form. We have examined many possible failing scenarios of WSN operations through the proposed tool, which was capable to restore WSN in the presence of failures to continue normal operations. Simulated Annealing (SA) is utilized as a search method, which is embedded within the soft computing tool to explore the restoration space comprising of possible alternative topologies generated for each failing scenario, taking lifespan into consideration. The tested scenarios and their restorations can be hard-coded into a real WSN, thus preparing WSN to evolve through failures. The simulation results of a typical WSN illustrates the capability of the proposed computing tool to restore 79% of the lost days when no restoration scheme was applied.","2376-6492;2376-6506","978-1-4799-0959-9978-1-4799-7324","10.1109/IWCMC.2014.6906337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6906337","failures;energy drain out;optimization;redesign;restoration;soft computing tool;Simulated Annealing","Sensors;Wireless sensor networks;Base stations;Topology;Network topology;Computational modeling;Mathematical model","neural nets;simulated annealing;uncertainty handling;wireless sensor networks","soft computing tool;wireless sensor networks;WSN;software programs;simulated annealing;restoration space","","","16","","","","","","IEEE","IEEE Conferences"
"Effect of fiber orientation in glass filled plastic enclosure on the performance of wire bonds in automotive electronic product under thermal cycling environment","S. N. Bhadri; S. G; J. Zhong","Delphi Electronics &amp; Safety, Technical Center India, Whitefield, Bangalore-560066, India; Delphi Electronics &amp; Safety, Technical Center India, Whitefield, Bangalore-560066, India; Delphi Electronics &amp; Safety, Singapore Design Engineering Centre, Singapore 569621","2015 16th International Conference on Electronic Packaging Technology (ICEPT)","","2015","","","177","182","Typical automotive electronic product consists of housing, cover and a laminated printed circuit board (PCB) with several electrical components mounted on it. Hard mount electrical connector on PCB with integrated pins is sometimes replaced by wire bonds and insert molded terminals in plastic enclosure. Wire bonds provide the much needed flexibility for the movement between connector and PCB wire bond pads, mainly under thermal cycling environment which is critical for the reliability of wire bonds. An extensive literature survey indicated that most of the work related to wire bonds so far focused on wire bond formation, materials, manufacturability, loop optimization, etc. There is a scarcity of work related to the effect of fiber orientation in glass-filled plastic as per the injection molding process on the reliability of solder joints of electronic devices and wire bonds on the circuit board. In this paper, a unique work has been published on the effect of glass fiber orientation in plastic housing and cover on the reliability of wire bonds used in an automotive electronic product under thermal cycling load. Fiber orientations in the glass-filled plastic housing and cover during injection molding are mapped on the structural mesh using DIGIMAT software. Then, a coupled thermal cycling finite element analysis (FEA) has been performed between DIGIMAT-Ansys on the structural mesh of the product assembly. Number of survival thermal cycles of the wire bond from the finite element analysis is compared with the actual product test data. Results show excellent correlation on the displacement behavioral trend at different wire bond pad locations and the number of wire bond thermal cycles. The major highlight of this paper is the importance of capturing the right physics of glass-filled plastic in thermal cycling analysis as per the fiber orientation from the mold flow during injection molding process. Absence of capturing the right physics of glass-filled plastic particularly during thermal cycling load, would lead to incorrect results, conclusions and thereby early failures during product life cycle, including warranty returns.","","978-1-4673-7999-1978-1-4673-7998","10.1109/ICEPT.2015.7236570","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7236570","Glass-filled plastic;Wire bond;Digimat;Thermo-mechanical material model;Glass fiber mapping;Injection Molding;Mold flow;Glass fiber orientation;Thermal cycling;Fatigue cycle;Electronic product","Wires;Software reliability;Plastics;Strain;Software;Correlation","automotive electronics;electric connectors;finite element analysis;glass fibre reinforced plastics;injection moulding;integrated circuit reliability;lead bonding;printed circuits;solders","automotive electronic product;thermal cycling environment;glass fiber orientation;glass filled plastic enclosure;wire bond reliability;laminated printed circuit board;hard mount electrical connector;insert molded terminals;solder joints;plastic housing;thermal cycling load;coupled thermal cycling;finite element analysis;DIGIMAT-Ansys;mold flow;injection molding process","","","8","","","","","","IEEE","IEEE Conferences"
"Compositional analysis of switched Ethernet topologies","R. Schneider; L. Zhang; D. Goswami; A. Masrur; S. Chakraborty","Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany; Institute for Real-Time Computer Systems, TU Munich, Germany","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","1099","1104","In this paper we study distributed automotive control applications whose tasks are mapped onto different ECUs communicating via a switched Ethernet network. As traditional automotive communication buses like CAN, FlexRay, LIN and MOST are gradually reaching their performance limits because of the increasing complexity of automotive architectures and applications, Ethernet-based in-vehicle communication systems have attracted a lot of attention in recent times. However, currently there is very little work on systematic timing analysis for Ethernet which is important for its deployment in safety-critical scenarios like in an automotive architecture. In this work, we propose a compositional timing analysis technique that takes various features of switched Ethernet into account like network topology, frame priorities, communication delay, memory requirement on switches, performance, etc. Such an analysis technique is particularly suitable during early design phases of automotive architectures and control software deployment. We demonstrate its use in analyzing mixed-criticality traffic patterns consisting of messages from performance-oriented control loops and timing-sensitive real-time tasks. We further evaluate the tightness of the obtained analytical bounds with an OMNeT++ based network simulation environment, which involves long simulation time and does not provide formal guarantees.","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.231","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513677","","Delays;Topology;Network topology;Ports (Computers);Automotive engineering;Analytical models;Fabrics","","","","4","14","","","","","","IEEE","IEEE Conferences"
"Cloud workflow scheduling with deadline and time slots constraints","X. Li; L. Qian; J. Yang","School of Computer Science and Engineering, Southeast University, Nanjing, China, 211189; School of Computer Science and Engineering, Southeast University, Nanjing, China, 211189; School of Computer Science and Engineering, Southeast University, Nanjing, China, 211189","2015 IEEE 19th International Conference on Computer Supported Cooperative Work in Design (CSCWD)","","2015","","","606","613","In cloud computing, services are unlimited and available at any time from the perspective of users or tenants. However, the remaining service capability could not satisfy tenants' requirements at any time from the perspective of service providers because the services are shared by multiple tasks. In this paper, we consider the workflow scheduling with both deadline and time slots constraints in cloud computing. An iterated heuristic framework is investigated, which mainly consists of three phases: the initial solution generation, improvement, reconstruction. Two constructive initial solution strategies, two improving methods and an reconstruction algorithm are proposed for the three phases, respectively. These strategies combine four heuristics, which are evaluated on a lot of test-bed instances.","","978-1-4799-2002-0978-1-4799-2001","10.1109/CSCWD.2015.7231027","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7231027","","Optimized production technology","cloud computing;scheduling;workflow management software","cloud workflow scheduling;deadline constraints;time slots constraints;cloud computing;service capability;tenants requirements;service providers;iterated heuristic framework;initial solution generation phase;improvement phase;reconstruction phase;constructive initial solution strategies;improving methods;reconstruction algorithm","","","16","","","","","","IEEE","IEEE Conferences"
"Improving Random Read Performance of Glibc","M. Wang; Y. Zhou; F. Xiao; Q. Luo","NA; NA; NA; NA","2014 13th International Symposium on Distributed Computing and Applications to Business, Engineering and Science","","2014","","","78","82","The Cloud data services, specifically, key/value stores and NoSQL database that require a large number of index lookups that fetch small amount of data. Random I/O becomes the critical performance factor. However, compared with sequential read, the efficiency of random read is very low. Our experiment will explain this. File I/O operation is closely associated with the implementation of I/O mechanism both in kernel space and user space. In this paper, we aims at analyzing the standard I/O mechanism, improving the standard I/O mechanism in user space for random read and implement into the glibc. Our experiment test result proves that our improved I/O mechanism will greatly improve the performance of random read.","","978-1-4799-4169-8978-1-4799-4170","10.1109/DCABES.2014.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6999061","glibc;standard I/O;random read;I/O buffer;kernel file pointer","Kernel;Standards;Libraries;Bandwidth;Optimization;Switches;Linux","cloud computing;database indexing;input-output programs;software libraries;SQL","random read performance improvement;glibc;cloud data services;key/value stores;NoSQL database;index lookups;data fetching;random I/O;critical performance factor;file I/O operation;kernel space;user space;standard I/O mechanism analysis","","","11","","","","","","IEEE","IEEE Conferences"
"Optimal placement of sequentially ordered virtual security appliances in the cloud","A. Shameli-Sendi; Y. Jarraya; M. Fekih-Ahmed; M. Pourzandi; C. Talhi; M. Cheriet","Ecole de Technologie Superieure (ETS), Montreal, Canada; Ericsson Research Security, Montreal, Canada; Ecole de Technologie Superieure (ETS), Montreal, Canada; Ericsson Research Security, Montreal, Canada; Ecole de Technologie Superieure (ETS), Montreal, Canada; Ecole de Technologie Superieure (ETS), Montreal, Canada","2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)","","2015","","","818","821","Traditional enterprise network security is based on the deployment of security appliances placed on some specific locations filtering, monitoring the traffic going through them. In this perspective, security appliances are chained in specific order to perform different security functions on the traffic. In the cloud, the same approach is often adopted using virtual security appliances to protect traffic for different virtual applications with the challenge of dealing with the flexible and elastic nature of the cloud. In this paper, we investigate the problem of placing virtual security appliances within the data center in order to minimize network latency and computing costs for security functions while maintaining the required sequential order of traversing virtual security appliances. We propose a new algorithm computing the best place to deploy these virtual security appliances in the data center. We further integrated our placement algorithm in an open source cloud framework, i.e. Openstack, in our test laboratory. The preliminary results show that we are placing the virtual security appliances in the required sequential order while improving the efficiency compared to the current default placement algorithm in Openstack.","1573-0077","978-1-4799-8241","10.1109/INM.2015.7140384","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7140384","","Security;Home appliances;Middleboxes;Optimization;Computers;Communication networks;Algorithm design and analysis","cloud computing;computer centres;public domain software;security of data","optimal placement;sequentially ordered virtual security appliance;enterprise network security;locations filtering;traffic monitoring;security function;data center;network latency;computing cost;algorithm computing;placement algorithm;open source cloud framework;Openstack","","4","10","","","","","","IEEE","IEEE Conferences"
"The implications from benchmarking three big data systems","J. Quan; Y. Shi; M. Zhao; W. Yang","School of Software Engineering, University of Science and Technology of China, Hefei, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computing and Information Sciences, Florida International University, Florida, USA; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China","2013 IEEE International Conference on Big Data","","2013","","","31","38","Along with today's data explosion and application diversification, a variety of hardware platforms for data centers are emerging and are attracting interests from both industry and academia. The existing hardware platforms represent a wide range of implementation approaches, and different hardware have different strengths. In this paper, we conduct comprehensive evaluations on three representative data center systems based on BigDataBench, which is a benchmark suite for benchmarking and ranking systems running big data applications. Then we explore the relative performance of the three implementation approaches with different big data applications, and provide strong guidance for the data center system construction. Through our experiments, we has inferred that a data center system based on specific hardware has different performance in the context of different applications and data volumes. When we construct a system, we can take into account not only the performance or energy consumption of the pure hardwares, but also the application-level characteristics. Data scale, application type and complexity should be considered comprehensively when researchers or architects plan to choose fundamental components for their data center system.","","978-1-4799-1293","10.1109/BigData.2013.6691706","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6691706","","Hardware;Support vector machines;Information management;Data handling;Data storage systems;Benchmark testing;Energy consumption","Big Data;computer centres","data explosion;application diversification;BigDataBench benchmark suite;big data applications;data center system;data volumes;application-level characteristics;data scale;application type;complexity","","2","15","","","","","","IEEE","IEEE Conferences"
"Securing Network Processors with High-Performance Hardware Monitors","T. Wolf; H. K. Chandrikakutty; K. Hu; D. Unnikrishnan; R. Tessier","Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA; Juniper Networks, Westford, MA; Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA; Altera Corporation, San Jose, CA; Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA","IEEE Transactions on Dependable and Secure Computing","","2015","12","6","652","664","As the Internet becomes integrated into nearly all aspects of everyday life, its reliability grows in importance. This vital communication resource, which has become an inviting target for attackers, must be protected with the same vigor as the end-systems it interconnects. Recent trends in network router architecture towards programmability and flexibility have increased the susceptibility of communication hardware to software attacks which modify intended data processing and forwarding functions. Contemporary routers typically feature network processors, whose protocol processing functions are determined via software. Prior work has shown that these general-purpose software-based processing systems can be attacked with data packets sent through the Internet. As a defense mechanism, the correct functionality of a network processor can be verified by a hardware monitor that observes processor operation and compares it to expected behavior. In the event of an attack, the monitor can interrupt the network processor, suppress malicious behavior, and reset the processor to a usable state for processing of subsequent traffic. In this work, we present several significant advances in hardware monitoring for network processors. A low-overhead monitor architecture that evaluates correct network processor operation in real-time on an instruction-by-instruction basis is described and tested. The monitor is shown to effectively prevent stack smashing attacks on processors that use a Harvard architecture, a widely used network processor configuration. Through experimentation, we show that our approach to hardware monitoring does not affect data plane packet throughput. In the event of an attack, malicious packets are dropped while packets of regular network traffic proceed through the network unaffected. A full evaluation of monitor architectural parameters is provided to create an optimized monitor design.","1545-5971;1941-0018;2160-9209","","10.1109/TDSC.2014.2373378","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964801","computer network;security;hardware monitor;control flow;deterministic finite automaton;Harvard architecture;Computer network;security;hardware monitor;control flow;deterministic finite automaton;Harvard architecture","Internet;Program processors;Routing protocols;Computer security;Automata;Computer architecture","computer network security;Internet;telecommunication traffic","network processor security;hardware monitor;Internet;network router architecture;software attack;general-purpose software-based processing system;stack smashing attack;network traffic","","1","20","","","","","","IEEE","IEEE Journals & Magazines"
"Communication and collaboration of heterogeneous unmanned systems using the joint architecture for Unmanned Systems (JAUS) standards","M. L. Incze; S. R. Sideleau; C. Gagner; C. A. Pippin","Naval Undersea Warfare Center Division, Newport, Newport, RI 02842, USA; Naval Undersea Warfare Center Division, Newport, Newport, RI 02842, USA; Naval Undersea Warfare Center Division, Newport, Newport, RI 02842, USA; Georgia Tech Research Institute, Atlanta, GA 30332 USA","OCEANS 2015 - Genova","","2015","","","1","6","The Naval Undersea Warfare Center Division Newport (NUWCDIVNPT) and Georgia Tech Research Institute (GTRI) completed a successful at-sea exercise with autonomous UAS and UUV systems demonstrating cross-domain unmanned system communication and collaboration. The exercise was held at the NUWC Narragansett Bay Shallow Water Test Facility (NBSWTF) range, and it represented for the first time the use of standard protocols and formats that effectively support cross-domain unmanned system operations. Four man-portable Iver2 UUVs operating in coordinated missions autonomously collected environmental data, which was compressed in-stride, re-formatted, and exfiltrated via UAS relay for display and tactical decision making. Two UAS with autonomous flight take-off and mission execution were sequenced to serve as ISR platforms and to support communications as RF relays for the UUVs performing Intelligence Preparation of the Environment missions. Two Command and Control nodes ashore provided unmanned system tasking and re-tasking, and they served to host and display both geo-positional data and status for UAS and UUV vehicles during the operational scenarios run during the exercise. The SAE Joint Architecture for Unmanned Systems (JAUS) standards were used for all message traffic between shore-based C2 nodes, UAS, and UUVs active in the NBSWTF exercise area. Exercise goals focused on CNO priorities expressed in the Undersea Domain Operating Concept of AUG 2013 which emphasized protocols essential to effective command and control of networked unmanned systems with decentralization and flexibility of command structures. Development for this project highlighted both the strengths and shortfalls of JAUS and captured the requirements for moving forward in effective cross-domain communications that support distributed, agile C2 nodes to meet evolving CONOPS for growing unmanned system presence and mission roles. The scenario employed operating parameters for UAS and UUV that have been established in real-world operations and ongoing unmanned system programs. The tactical information from unmanned systems was displayed in real-time on shore-based C2 displays: the tactical FalconView display and the developmental TOPSIDE command and control station. This work represents a critical step in communications for networking of heterogeneous unmanned systems and establishes a solid platform for alignment of development and ongoing programs. The evaluation of JAUS suitability for near-term operational applications provides significant value as Concepts of Operation that rely on netted heterogeneous systems are being targeted. The focus on affordable commercial unmanned systems for this experimentation establishes the value of highly capable, portable systems to provide economical development and test opportunities with low-cost and low-risk alternatives to many planned and fielded systems. The JAUS architecture was introduced to the NUWC and GTRI unmanned systems though an instantiation of the Mission Oriented Operating Suite (MOOS) autonomy framework on secondary CPUs integrated into the Iver2 UUVs and the GTRI UAS. Since the GTRI UASs already had ROS installed, a MOOS-ROS bridge was employed to support use of the developed JAUS messaging capability. Established JAUS services were employed where the required functions could be met. New JAUS services were developed to meet functionality required for the operational scenarios in this exercise but not yet supported in the existing releases of SAE JAUS. Independent C++ header libraries that could be compiled at run time for specific autonomy frameworks, such as MOOS, were employed to support a software-agnostic approach. Immediate targets for broadening the influence of this work to coalition partners include the NATO Recognized Environmental Picture (REP) 2015 and The Technical Cooperation Program (TTCP) 2015 exercises. This project and demonstration was funded under a NUWC Strategic Initiative and GTRI program support.","","978-1-4799-8736-8978-1-4673-7164","10.1109/OCEANS-Genova.2015.7271613","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271613","autonomous vehicles;command and control systems;communication protocols;environmental characterization;data transmission","Standards;Command and control systems;Protocols;Vehicles;Sea surface;Collaboration;Systems operation","autonomous aerial vehicles;autonomous underwater vehicles;command and control systems;military communication","heterogeneous unmanned system;joint architecture for unmanned system;JAUS standard;naval undersea warfare center division Newport;NUWCDIVNPT;Georgia tech research institute;GTRI program support;autonomous UAS;Narragansett bay shallow water test facility;NBSWTF;cross-domain unmanned system communication;man-portable Iver2 UUV;tactical decision making;autonomous flight take-off;ISR platform;RF relay;geopositional data;UUV vehicle;CNO priority;undersea domain operating concept;CONOPS;tactical information;C2 display;tactical FalconView display;developmental TOPSIDE command and control station;netted heterogeneous system;mission oriented operating suite;MOOS-ROS bridge;SAE JAUS;C++ header library;software-agnostic approach;NATO recognized environmental picture;NATO REP;the technical cooperation program;TTCP","","","","","","","","","IEEE","IEEE Conferences"
"RSUs placement using overlap based greedy method for urban and rural roads","R. Rizk; R. Daher; A. Makkawi","Networks Department, Faculty of Information and Engineering Technology, German University in Cairo, Egypt; Networks Department, Faculty of Information and Engineering Technology, German University in Cairo, Egypt; Networks Department, Faculty of Information and Engineering Technology, German University in Cairo, Egypt","2014 7th International Workshop on Communication Technologies for Vehicles (Nets4Cars-Fall)","","2014","","","12","18","The deployment of efficient roadside networks is a necessity for ITS deployment. The main challenge for the roadside deployment is to find a satisfying or best distribution of RSUs on the roads network according to the given conditions in order to meet the requested requirements of the roads operator. Additionally, various factors affect this process such as traffic, infrastructure and topological characteristics of the roads. This paper introduces an Overlap based Greedy Method (OGM) as a basis for RSUs placement and can be applied on urban and rural roads. This method in its current development mainly considers the RSU coverage radius and the overlap rate into the RSUs distribution process. Moreover, a wide range of influencing factors is considered through the prioritization of Sites of Interest (SoIs). The OGM method is developed within the recently started PRONET project and is integrated into its software platform. The tests conducted on selected roads environment in the city of Rostock (Germany) ensure: (1) decrease in number of chosen SoIs for placement compared to the original scanned number of SoIs, and (2) full (continuous) coverage cannot be guaranteed for a roads network using only the available junctions.","","978-1-4799-5270","10.1109/Nets4CarsFall.2014.7000905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7000905","ITS deployment;Roadside Access Network (RAN);RSUs placement/distribution;overlap rate;junction priority;greedyalgorithm;urban and rural roads","Roads;Junctions;Vehicles;Software;Algorithm design and analysis;Greedy algorithms;Planning","greedy algorithms;intelligent transportation systems;mobile radio;road traffic","planning platform for roadside network;Germany;Rostock;software platform;PRONET project;SoI;sites of interest;roadside unit distribution process;overlap rate;RSU coverage radius;OGM method;road topological characteristic;road infrastructure;road traffic;ITS deployment;rural road;urban road;overlap based greedy method;RSU placement","","4","13","","","","","","IEEE","IEEE Conferences"
"Hardware-efficient stereo estimation using a residual-based approach","A. A. Sharma; K. Neelathalli; D. Marculescu; E. Nurvitadhi","Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Intel Science and Technology Center on Embedded Computing, USA","2013 IEEE International Conference on Acoustics, Speech and Signal Processing","","2013","","","2693","2696","Many promising embedded computer vision applications, such as stereo estimation, rely on inference computation on Markov Random Fields (MRFs). Sequential Tree-Reweighted Message passing (TRW-S) is a superior MRF solving method, which provides better convergence and energy than others (e.g., belief propagation). Since software TRW-S solvers are slow, custom TRW-S hardware has been proposed to improve execution efficiency. This paper proposes hardware mechanisms to further optimize TRW-S hardware efficiency, by tracking differences in input message values (residues) and skipping computation when values no longer change (residue is zero). Evaluations of our hardware mechanisms using Middlebury benchmark show 1.6x to 6x potential reduction in computation (depending on design parameters) while increasing energy by only 0.4% to 4.8%.","1520-6149;2379-190X","978-1-4799-0356","10.1109/ICASSP.2013.6638145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6638145","Hardware optimization;stereo estimation;Markov Random Fields","Convergence;Hardware;Tiles;Stereo vision;Benchmark testing;Estimation;Belief propagation","computer vision;embedded systems;Markov processes;message passing;stereo image processing","stereo estimation;residual-based approach;embedded computer vision;inference computation;Markov random fields;MRF solving method;sequential tree-reweighted message passing;TRW-S solvers;TRW-S hardware efficiency;execution efficiency;Middlebury benchmark","","1","12","","","","","","IEEE","IEEE Conferences"
"Seamless Insertion of Pulmonary Nodules in Chest CT Images","A. Pezeshk; B. Sahiner; R. Zeng; A. Wunderlich; W. Chen; N. Petrick","Division of Imaging, Diagnostics, and Software Reliability, Office of Science and Engineering Laboratories, Center for Devices and Radiological Health, U.S. Food and Drug Administration, Silver Spring, MD, USA; U.S. Food and Drug Administration; U.S. Food and Drug Administration; U.S. Food and Drug Administration; U.S. Food and Drug Administration; U.S. Food and Drug Administration","IEEE Transactions on Biomedical Engineering","","2015","62","12","2812","2827","The availability of large medical image datasets is critical in many applications, such as training and testing of computer-aided diagnosis systems, evaluation of segmentation algorithms, and conducting perceptual studies. However, collection of data and establishment of ground truth for medical images are both costly and difficult. To address this problem, we are developing an image blending tool that allows users to modify or supplement existing datasets by seamlessly inserting a lesion extracted from a source image into a target image. In this study, we focus on the application of this tool to pulmonary nodules in chest CT exams. We minimize the impact of user skill on the perceived quality of the composite image by limiting user involvement to two simple steps: the user first draws a casual boundary around a nodule in the source, and, then, selects the center of desired insertion area in the target. We demonstrate the performance of our system on clinical samples, and report the results of a reader study evaluating the realism of inserted nodules compared to clinical nodules. We further evaluate our image blending techniques using phantoms simulated under different noise levels and reconstruction filters. Specifically, we compute the area under the ROC curve of the Hotelling observer (HO) and noise power spectrum of regions of interest enclosing native and inserted nodules, and compare the detectability, noise texture, and noise magnitude of inserted and native nodules. Our results indicate the viability of our approach for insertion of pulmonary nodules in clinical CT images.","0018-9294;1558-2531","","10.1109/TBME.2015.2445054","Critical Path; Office of Women's Health; U.S. Food and Drug Administration; Research Participation Program at the Center for Devices and Radiological Health; Oak Ridge Institute for Science and Education; U.S. Department of Energy; U.S. Food and Drug Administration; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7123186","Poisson editing;pulmonary nodules;data augmentation;image composition;Data augmentation;image composition;Poisson editing;pulmonary nodules","Lesions;Noise;Computed tomography;Shape;Biomedical imaging;Optimized production technology;Training","computerised tomography;feature extraction;image reconstruction;image segmentation;image texture;lung;medical image processing;noise;object detection;phantoms;sensitivity analysis","medical image datasets;computer-aided diagnosis systems;chest CT image segmentation;lesion extraction;target detection;image blending techniques;phantom simulation;reconstruction filters;area under the ROC curve;Hotelling observer;noise power spectrum;noise texture;noise magnitude;pulmonary nodule insertion","Databases, Factual;Humans;Lung Neoplasms;Phantoms, Imaging;ROC Curve;Radiographic Image Interpretation, Computer-Assisted;Tomography, X-Ray Computed","8","53","","","","","","IEEE","IEEE Journals & Magazines"
"Auto tuning speeds commissioning of the generator excitation system","R. C. Schaefer; K. Kim","Basler Electric Company, 12570 State Route 143, Highland, IL 62249, USA; Basler Electric Company, 12570 State Route 143, Highland, IL 62249, USA","Conference Record of 2014 Annual Pulp and Paper Industry Technical Conference","","2014","","","137","143","This paper discusses PID tuning capabilities of a digital excitation system and a new feature called automatic tuning which can accelerate the process involve of commissioning the generator with the new excitation system. Based on given excitation system parameters, several PID tuning approaches are reported. Since in general, these parameters are not available during commissioning, specifically the machine time constants, this lack of information causes a considerable time delay and cost of fuel usage for commissioning the automatic voltage regulator (AVR). In the automatic tuning method, the excitation system parameters are identified and the PID gains are calculated using well-developed algorithms. With self-tuned PID gains, commissioning is accomplished very quickly with excellent performance results. Also discussed are the new software tools that can ease the burden of testing and aid in achieving compliance with the latest North American Electric Reliability Corporation (NERC) Mods 026 and PRC 19 standards.","0190-2172","978-1-4799-2066-2978-1-4799-5204-5978-1-4799-2067","10.1109/PPIC.2014.6871158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871158","Digital voltage regulator;auto-tuning of the PID controller;commissioning a generator excitation system;recursive least square;particle swarm optimization","Generators;Voltage control;Tuning;Regulators;Testing;Voltage measurement","commissioning;digital control;electric generators;three-term control;tuning;voltage regulators","autotuning speed commissioning;generator excitation system;PID tuning capability;digital excitation system;automatic tuning method;machine time constant;time delay;fuel usage cost;automatic voltage regulator;AVR;self-tuned PID gain;North American electric reliability corporation;NERC;Mods 026 standard;PRC 19 standard","","","10","","","","","","IEEE","IEEE Conferences"
"Epidemic models using Resource Prediction mechanism for optimal provision of multimedia services","Y. Kryftis; C. X. Mavromoustakis; G. Mastorakis; J. M. Batalla; P. Chatzimisios","Department of Computer Science, University of Nicosia, Cyprus; Department of Computer Science, University of Nicosia, Cyprus; Department of Informatics Engineering, Technological Educational Institute of Crete, Heraklion, Greece; National Institute of Telecommunications, Szachowa Str. 1, 04-894 Warsaw, Poland; CSSN Research Lab, Alexander TEI of Thessaloniki, Greece","2015 IEEE 20th International Workshop on Computer Aided Modelling and Design of Communication Links and Networks (CAMAD)","","2015","","","91","96","This paper proposes a network architecture that goes a step beyond the current state-of-the-art, by elaborating on a novel resource reservation and provision scheme, through a Media Distribution Middleware (MDM), that is based on a Resource Prediction Engine (RPE) leading to both high resource utilization and quality guarantees. The architecture defines management planes and software components that provide the mechanisms for collecting monitoring data, predicting possible future values of the network metrics and resources usage, and applying management decisions to keep the provision optimal. The proposed research approach is based on novel time series and epidemic spread models, and the outcome is used for the optimal distribution of streaming data, among Content Delivery Networks, cloud-based providers and Home Media Gateways. The proposed epidemic diseases model adopts the characteristics of the multimedia content delivery over the network architecture. In this context, the paper aims to present the advantages of using such models, by presenting and analyzing an epidemic spread scheme for Video on Demand (VoD) delivery, to predict future epidemic spread behavior. The validity of the proposed system is verified through several sets of extended experimental simulation tests, carried out under controlled simulation conditions.","","978-1-4673-8186-4978-1-4673-8185","10.1109/CAMAD.2015.7390487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7390487","Resource Prediction Engine;Content Delivery Networks;Multimedia Services Systems;Quality of Experience;Epidemic Models","Predictive models;Media;Computational modeling;Multimedia communication;Engines;Streaming media","cloud computing;media streaming;middleware;optimisation;resource allocation;video on demand","resource prediction mechanism;multimedia service optimal provision;resource reservation;media distribution middleware;resource prediction engine;management plane;software components;streaming data optimal distribution;content delivery network;cloud based provider;home media gateways;epidemic diseases model;video on demand","","3","17","","","","","","IEEE","IEEE Conferences"
"Design and development of permanent magnet based focusing lens for J-band klystron","K. Singh; J. Itteera; P. Ukarde; S. Malhotra; Y. K. Taly; A. Bandyopadhay; R. Meena; V. Rawat; L. M. Joshi","Control Instrumentation Division, Bhabha Atomic research Centre, Mumbai, India; Control Instrumentation Division, Bhabha Atomic research Centre, Mumbai, India; Control Instrumentation Division, Bhabha Atomic research Centre, Mumbai, India; Control Instrumentation Division, Bhabha Atomic research Centre, Mumbai, India; Control Instrumentation Division, Bhabha Atomic research Centre, Mumbai, India; Microwave Tubes Division, Central Electronics Engineering Research Institute, Pilani, India; Microwave Tubes Division, Central Electronics Engineering Research Institute, Pilani, India; Microwave Tubes Division, Central Electronics Engineering Research Institute, Pilani, India; Microwave Tubes Division, Central Electronics Engineering Research Institute, Pilani, India","2014 International Symposium on Discharges and Electrical Insulation in Vacuum (ISDEIV)","","2014","","","677","680","Applying permanent magnet technology to beam focusing in klystrons can reduce their power consumption and increase their reliability of operation. Electromagnetic design of the beam focusing elements, for high frequency travelling wave tubes, is very critical. The magnitude and profile of the magnetic field need to match the optics requirement from beam dynamics studies. The rise of the field from cathode gun region to the uniform field region (RF section) is important as the desired transition from zero to peak axial field must occur over a short axial distance. Confined flow regime is an optimum choice to minimize beam scalloping but demands an axial magnetic field greater than 2-3 times the Brillouin flow field. This necessitates optimization in the magnet design achieve high magnetic field within given spatial constraints. Electromagnetic design and simulations were done using 3D Finite element method (FEM) analysis software. A Permanent magnet based focusing lens for a miniature J-Band klystron has been designed and developed at Control Instrumentation Division, BARC. This paper presents the design, simulation studies, beam transmission and RF tests results for J Band klystron with Permanent magnet focusing lens.","1093-2941","978-1-4799-6752-0978-1-4799-6750","10.1109/DEIV.2014.6961773","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6961773","Klystrons;Scalloping;Confined flow;Brillouin field;Beam Oscillations;Solenoid focusing","Focusing;Magnetic noise;Magnetic shielding;Magnetic flux;Klystrons;Radio frequency;Magnetic fields","finite element analysis;klystrons;magnetic lenses;permanent magnets","permanent magnet technology;beam focusing;klystrons;electromagnetic design;high frequency travelling wave tubes;beam dynamics studies;cathode gun region;uniform field region;peak axial field;short axial distance;confined flow regime;axial magnetic field;magnet design;3D finite element method analysis software;3D FEM analysis software;permanent magnet based focusing lens;miniature J-Band klystron","","","5","","","","","","IEEE","IEEE Conferences"
"ULLA-X: A programmatic middleware for generic cognitive radio network control","A. Patra; A. Achtzehn; P. Mähönen","Institute for Networked Systems, RWTH Aachen University, Kackertstrasse 9, D-52072 Aachen, Germany; Institute for Networked Systems, RWTH Aachen University, Kackertstrasse 9, D-52072 Aachen, Germany; Institute for Networked Systems, RWTH Aachen University, Kackertstrasse 9, D-52072 Aachen, Germany","2015 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN)","","2015","","","265","266","Classical cognitive radio concepts such as the cognitive radio manager or the cognitive engine aim to cleverly optimize radio configuration and networking setups to maximize performance throughout the network. However, these concepts regularly requires experimental verification and testing in complex, real-time and signal processing focused SDR frameworks, e.g. directly in FPGAs or in a software like GNURadio. This hinders rapid prototyping and deepens the gap between implementation-focused lower-layer and cross-layer/control plane oriented research. In this demo, we present a novel, fully-networked middleware for centrally controlling distributed radio link setups of SDRs and reconfigurable legacy devices. As an intermediate layer, ULLA-X allows technology-agnostic device parameter monitoring and reconfiguration based on an easy-to-learn programming language. Through its functionality, ULLA-X mitigates the complexities arising from using different radio platforms, and makes various configuration APIs readily accessible to standard research tools. In this demonstration, we showcase the capabilities of ULLA-X, and present by means of examples its adaptation capabilities for custom radio implementations and its programming concept for different SDR platforms.","","978-1-4799-7452","10.1109/DySPAN.2015.7343910","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7343910","","Cognitive radio;Connectors;Nickel;Computer languages;Middleware;Optimization","cognitive radio;field programmable gate arrays;middleware;programming languages;radio networks;telecommunication computing;telecommunication control","programming language;technology-agnostic device parameter monitoring;distributed radio link;GNURadio;FPGA;SDR;signal processing;cognitive engine;cognitive radio manager;generic cognitive radio network control;programmatic middleware;ULLA-X","","1","8","","","","","","IEEE","IEEE Conferences"
"An alternate PowerFactory Matlab coupling approach","A. Latif; M. Shahzad; P. Palensky; W. Gawlik","Energy Department, AIT Austrian Institute of Technology, Vienna, Austria; Energy Department, AIT Austrian Institute of Technology, Vienna, Austria; Energy Department, AIT Austrian Institute of Technology, Vienna, Austria; Institute of Energy Systems and Electrical Drives, Vienna University of Technology, Austria","2015 International Symposium on Smart Electric Distribution Systems and Technologies (EDST)","","2015","","","486","491","PowerFactory is a powerful power system analysis tool used for simulating the electrical grid. `Smart Grid' envisions a modernized electrical grid that uses communication to gather and act on information. The ever increasing communication and controls in power systems increases the complexity of the system. Co-simulation becomes essential to couple system simulators from different domains. This paper gives an overview of possible PowerFactory / Matlab coupling approaches. It further discusses the advantages and limitations of each of these. Additionally, an alternate coupling approach is suggested, its pros and cons are discussed. Two test cases have been implemented that highlight different advantages of the proposed method.","","978-1-4799-7736-9978-1-4799-7735","10.1109/SEDST.2015.7315257","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7315257","PowerFactory;Matlab;Co-Simulation;Software coupling","MATLAB;Couplings;Optimization;Load flow;Linear programming;Reactive power;Voltage control","power system control;power system simulation;smart power grids","PowerFactory Matlab coupling;power system analysis tool;electrical grid;smart grid","","8","10","","","","","","IEEE","IEEE Conferences"
"Sustainable E<sup>2</sup>mobility services for elderly people — Platform system architecture","D. Khadraoui; H. Ayed; D. Nicolas","CRP Henri TUDOR, Luxembourg, 29, Avenue John F. Kennedy, L-1855 Luxembourg; CRP Henri TUDOR, Luxembourg, 29, Avenue John F. Kennedy, L-1855 Luxembourg; CRP Henri TUDOR, Luxembourg, 29, Avenue John F. Kennedy, L-1855 Luxembourg","2014 Science and Information Conference","","2014","","","943","948","This paper is about the development of an online web platform that allows the elderly to plan their trips according to their individual constraints and preferences, as well as a complementary mobile application to provide assistance while on the move. Already tested by seniors on three group trips in different locations around Europe, the platform will soon be made available for wider use. This work has been developed under the AAL/JP project called STIMULATE (http://stimulate-aal.eu) funded by the European Union as well as by the FNR (Fonds National de Luxembourg).","","978-0-9893193-1-7978-0-9893-1933-1978-0-9893-1932","10.1109/SAI.2014.6918300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6918300","Senior's mobility;Travel support;Resource recommender;Health related user profiling;WEB Platform;Software Architecture;On the Move;Algorith","Senior citizens;Planning;Optimization;Service-oriented architecture;Context;Business;Mobile communication","assisted living;geriatrics;Internet;mobile computing","STIMULATE;AAL/JP project;Europe;group trip planning;complementary mobile application;individual preferences;individual constraints;online Web platform;elderly people-platform system architecture;sustainable E2mobility services","","1","5","","","","","","IEEE","IEEE Conferences"
"Scalability and Robustness of Time-Series Databases for Cloud-Native Monitoring of Industrial Processes","T. Goldschmidt; A. Jansen; H. Koziolek; J. Doppelhamer; H. P. Breivold","NA; NA; NA; NA; NA","2014 IEEE 7th International Conference on Cloud Computing","","2014","","","602","609","Today's industrial control systems store large amounts of monitored sensor data in order to optimize industrial processes. In the last decades, architects have designed such systems mainly under the assumption that they operate in closed, plant-side IT infrastructures without horizontal scalability. Cloud technologies could be used in this context to save local IT costs and enable higher scalability, but their maturity for industrial applications with high requirements for responsiveness and robustness is not yet well understood. We propose a conceptual architecture as a basis to designing cloud-native monitoring systems. As a first step we benchmarked three open source time-series databases (OpenTSDB, KairosDB and Databus) on cloud infrastructures with up to 36 nodes with workloads from realistic industrial applications. We found that at least KairosDB fulfills our initial hypotheses concerning scalability and reliability.","2159-6190;2159-6182","978-1-4799-5063-8978-1-4799-5062","10.1109/CLOUD.2014.86","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6973792","linear scalability;support for industrial workloads;workload independence;resiliency and read/write independence","Phasor measurement units;Databases;Time series analysis;Scalability;Monitoring;Benchmark testing;Clouds","cloud computing;database management systems;process monitoring;production engineering computing;public domain software;time series","time-series database scalability;time-series database robustness;cloud-native industrial processes monitoring;industrial control systems;monitored sensor data;industrial processes;plant-side IT infrastructures;cloud technologies;local IT costs;industrial applications;open source time-series databases;OpenTSDB;KairosDB;Databus","","16","19","","","","","","IEEE","IEEE Conferences"
"SmartBricks: A Visual Environment to Design and Explore Novel Custom Domain-Specific Architectures","A. Sistla; X. Luo; M. Malladi; M. Reisner; R. Ganduri; G. Mehta","NA; NA; NA; NA; NA; NA","2014 IEEE International Parallel & Distributed Processing Symposium Workshops","","2014","","","161","169","Custom domain-specific architectures are very promising for creating designs that are highly optimized to the needs of a particular application domain. However, it is extremely difficult to find optimal tradeoffs in designing a new architecture, or even to fully understand the design space. Therefore, there is a great need to develop an optimum design framework that allows designers to explore the design space efficiently and identify efficient architectures quickly for an application domain. In this paper, we describe SmartBricks, a highly visual design environment that we have developed for designing and exploring custom domain-specific architectures quickly and efficiently. This game-like design environment will be accessible to a broad community so that even non-engineers and non-scientists can contribute to building and exploring out-of-the-box architectural designs.","","978-1-4799-4116-2978-1-4799-4117","10.1109/IPDPSW.2014.22","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6969383","","Games;Benchmark testing;Visualization;Architecture;Space exploration;Libraries;Routing","software architecture","SmartBricks environment;visual environment;custom domain-specific architecture;application domain;architecture design;design space;game-like design environment","","","50","","","","","","IEEE","IEEE Conferences"
"Improving Virtual Machine live migration via application-level workload analysis","A. Baruchi; E. T. Midorikawa; M. A. S. Netto","University of Sao Paulo, LAHPC, Brazil; University of Sao Paulo, LAHPC, Brazil; IBM Research, Sao Paulo, Brazil","10th International Conference on Network and Service Management (CNSM) and Workshop","","2014","","","163","168","Virtual Machine (VM) live migration is key for implementing resource management policies to optimize metrics such as server utilization, energy consumption, and quality-of-service. A fundamental challenge for VM live migration is its impact on both user and resource provider sides, including service downtime and high network utilization. Several VM live migration studies have been published in the literature. However, they mostly consider only system level metrics such as CPU, memory, and network usage to trigger VM migrations. This paper introduces ALMA, an Application-aware Live Migration Architecture that explores application level information, in addition to the traditional system level metrics, to determine the best time to perform a migration. Based on experiments with three real applications, by considering application characteristics to trigger the VM live migration, we observed a substantial reduction in data transferred over the network of up to 42% and the total live migration time decrease of up to 63%.","2165-9605;2165-963X","978-3-901882-67","10.1109/CNSM.2014.7014153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014153","Live Migration;Cloud Computing;Performance Prediction;Virtualization","Measurement;Benchmark testing;Accuracy;Virtual machining;Memory management;Servers","cloud computing;energy consumption;quality of service;software architecture;virtual machines","virtual machine live migration;application-level workload analysis;resource management policies;server utilization;energy consumption;quality-of-service;application-aware live migration architecture;cloud computing","","2","23","","","","","","IEEE","IEEE Conferences"
"New Centrality Measure in Social Networks Based on Independent Cascade (IC) Model","I. Gaye; G. Mendy; S. Ouya; D. Seck","NA; NA; NA; NA","2015 3rd International Conference on Future Internet of Things and Cloud","","2015","","","675","680","In this paper, we consider the influence maximization problem in social networks. There are various works to maximize the influence spread. The aim is to find a k - nodes subset to maximize the influence spread in a network. We propose a new algorithm (BRST-algorithm) to determine a particular spanning tree. We also propose a new centrality measure. This heuristic is based on the diffusion probability and on the contribution of the 'th neighbors to maximize the influence spread. Our heuristic uses the Independent Cascade Model (ICM). The two proposed algorithms are effective and their complexity is O(nm). The simulation of our model is done with R software and igraph package. To demonstrate the performance of our heuristic, we implement one benchmark algorithm, the diffusion degree, and we compare it with ours.","","978-1-4673-8103-1978-1-4673-8102","10.1109/FiCloud.2015.122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7300886","Influence maximization;Social network;centrality measure;spanning tree","Social network services;Mathematical model;Dolphins;Organizations;Integrated circuit modeling;Benchmark testing","computational complexity;optimisation;probability;set theory;social networking (online);trees (mathematics)","social networks;independent cascade model;IC model;influence maximization problem;k-nodes subset;BRST-algorithm;spanning tree;diffusion probability;ICM;O(nm) complexity;igraph package;benchmark algorithm;diffusion degree","","2","21","","","","","","IEEE","IEEE Conferences"
"Towards super-resolution computational imaging with GeoSTAR configured mm-band sensor array radar: An aggregated adaptive beamforming compressed sensing approach","Y. V. Shkvarko; V. E. Espadas","CINVESTAV, Unidad Guadalajara, Avenida Del Bosque # 1145, Colonia El Bajío, 45019, Zapopan, Jalisco, México; CINVESTAV, Unidad Guadalajara, Avenida Del Bosque # 1145, Colonia El Bajío, 45019, Zapopan, Jalisco, México","2013 International Kharkov Symposium on Physics and Engineering of Microwaves, Millimeter and Submillimeter Waves","","2013","","","596","598","We address a new adaptive beamforming (ABF) approach for attaining virtual super-resolution performances of radar imaging with differently configured mm-band compressed sensing (CS) array radars. Our new aggregated ABF-CS approach is an adaptive beamforming-oriented generalization of the conventional matched spatial filtering (MSF) method for radar image formation based on the advanced descriptive experiment design regularization (DEDR) framework for radar imagery enhancement. First, we optimize the sensor array configuration employing the celebrated GeoSTAR geometry to attain the desired shape of the MSF system point spread function (PSF). At the second (reconstructive) stage, the low resolution MSF image is next enhanced via performing the aggregated ABF-CS post-processing aimed at attaining the overall super-high resolution remote sensing (RS) performances. The effectiveness of the new aggregated ABF-CS radar imaging method is corroborated via extended simulations of different DEDR-related imaging techniques using the specialized elaborated software that we refer to as “Virtual Remote Sensing Laboratory” (VRSL). The unified ABF-CS computational imaging method has been adapted to the DEDR-optimized GeoSTAR sensor array configuration and exemplified in the reported simulations of super-high resolution localization of the multiple closely spaces targets performed with the elaborated VRSL software. The latter are indicative of the superior operational efficiency of the imaging radar system that employs the new ABF-CS method adapted to the DEDR-optimized GeoSTAR configuration over other tested competing techniques.","","978-1-4799-1068-7978-1-4799-1066","10.1109/MSMW.2013.6622153","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6622153","","Radar imaging;Arrays;Imaging;Image reconstruction;Spatial resolution","array signal processing;compressed sensing;optical transfer function;radar imaging;remote sensing;spatial filters","super-resolution computational imaging;mm-band sensor array radar;adaptive beamforming;compressed sensing;radar imaging;matched spatial filtering;radar image formation;descriptive experiment design regularization;DEDR;GeoSTAR geometry;MSF system;point spread function;specialized elaborated software;virtual remote sensing laboratory;VRSL;ABF-CS method","","","6","","","","","","IEEE","IEEE Conferences"
"A new approach of multi-robot cooperative pursuit","C. Wang; T. Zhang; K. Wang; S. Lv; H. Ma","School of Automation, Beijing Institution of Technology, 100081, China; School of Automation, Beijing Institution of Technology, 100081, China; School of Software, Beijing Institution of Technology, 100081, China; School of Automation, Beijing Institution of Technology, 100081, China; School of Automation, Beijing Institution of Technology, 100081, China","Proceedings of the 32nd Chinese Control Conference","","2013","","","7252","7256","This paper addresses a new method to handle with problems on hunting activities of a multi-robot system in dynamic environment. Some necessary conditions, on which a group of robots with different velocities can develop a successful surround-and-capture process on a target with satisfactory performances, are also discussed by illustrations. Based on the quick-surrounding direction and quick-capture direction obtained, the paper provides an optimized pursuit strategy, which yields an optimal path in certain sense, whose effectiveness withstands the test of algorithm simulation.","1934-1768","978-9-8815-6383","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6640713","Multi-robot Pursuit;Optimal Path;Occupy;Overlapping Angle","Robot kinematics;Multi-robot systems;Mathematical model;Conferences;Resource management;Educational institutions","multi-robot systems;robot dynamics","multi-robot cooperative pursuit approach;multi-robot system;multi-robot dynamic environment;necessary conditions;surround-and-capture process;quick-surrounding direction;quick-capture direction;optimized pursuit strategy","","","18","","","","","","IEEE","IEEE Conferences"
"Design and Simulation for Path Tracking Control of a Commercial Vehicle Using MPC","O. Garcia; J. V. Ferreira; A. M. Neto","NA; NA; NA","2014 Joint Conference on Robotics: SBR-LARS Robotics Symposium and Robocontrol","","2014","","","61","66","The design of the robotic vehicle VILMA at UNICAMP is developed in-vehicle platform Fiat Punto. In addition to a set of sensors, actuators, mechanism and components (hardware and/or software), new technologies should be developed in support of Automation, Control, Perception, Localization and Navigation. This work presents the design and simulation of path tracking control using model predictive control (MPC) which attempts to exploit the characteristics of the structured environment where the future path is previously known. The model for design the controller is based in a single tracking model of the vehicle and in a model of the steering which the state variables are observed by the Extended Kalman Filter (EKF). Finally, it is explained how the path is smoothed generating an arc between the points and making an optimization process by the gradient algorithm.","","978-1-4799-6711-7978-1-4799-6710","10.1109/SBR.LARS.Robocontrol.2014.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7024257","Autonomous Vehicle;Path Tracking;Model Predictive Control;VDA test","Vehicles;Mathematical model;Sensors;Vehicle dynamics;Vectors;Predictive control;Equations","control system synthesis;gradient methods;Kalman filters;mobile robots;nonlinear filters;path planning;predictive control;road vehicles","path tracking control design;path tracking control simulation;commercial vehicle;MPC;VILMA;robotic vehicle;UNICAMP;in-vehicle platform;Fiat Punto;sensors;actuators;model predictive control;extended Kalman filter;EKF;optimization process;gradient algorithm;autonomous vehicle","","2","19","","","","","","IEEE","IEEE Conferences"
"Novel flight management system for improved safety and sustainability in the CNS+A context","S. Ramasamy; R. Sabatini; A. Gardi","School of Aerospace, Mechanical and Manufacturing, RMIT University, Melbourne, VIC 3000, Australia; School of Aerospace, Mechanical and Manufacturing, RMIT University, Melbourne, VIC 3000, Australia; School of Aerospace, Mechanical and Manufacturing, RMIT University, Melbourne, VIC 3000, Australia","2015 Integrated Communication, Navigation and Surveillance Conference (ICNS)","","2015","","","G3-1","G3-11","Avionic system developers are faced with the challenge of researching and introducing innovative technologies that satisfy the requirements arising from the rapid expansion of global air transport while addressing the growing concerns for environmental sustainability of the aviation sector. As a consequence, novel systems are being developed in the Communication, Navigation and Surveillance/Air Traffic Management (CNS/ATM) and Avionics (CNS+A) context. The introduction of dedicated software modules in Next Generation Flight Management Systems (NG-FMS), which are the primary providers of automated navigation and guidance services in manned aircraft and Remotely-Piloted Aircraft Systems (RPAS), has the potential to enable the significant advances brought in by time based operations. In this paper, key elements of the NG-FMS architecture are presented that allow the incorporation of 4-Dimensional Trajectory (4DT) planning and optimisation with inclusion of CNS integrity monitoring and augmentation functions in the overall design. The NG-FMS is designed to be fully interoperable with a future ground based 4DT Planning, Negotiation and Validation (4-PNV) system, enabling automated Trajectory/Intent-Based Operations (TBO/IBO). The mathematical models for 4DT planning are presented and the CNS integrity performance criteria are identified for various mission- and safety-critical tasks. Evaluation of the proposed concepts and methodologies is performed through dedicated simulation test case. The results demonstrate the functional capability of the NG-FMS to generate cost-effective trajectory profiles satisfying operational as well as environmental constraints.","2155-4943;2155-4951","978-1-4799-8952-2978-1-4673-7549","10.1109/ICNSURV.2015.7121225","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7121225","","Trajectory;Aircraft navigation;Aircraft;Planning;Atmospheric modeling;Optimization;Surveillance","air safety;air traffic;aircraft communication;aircraft navigation;autonomous aerial vehicles;avionics;path planning;surveillance;sustainable development","flight management system;CNS+A context;avionic system;global air transport expansion;environmental sustainability;air traffic management;communication-navigation and surveillance-air traffic management and avionics context;CNS-ATM;next generation flight management systems;dedicated software modules;automated navigation;guidance services;manned aircraft;remotely-piloted aircraft systems;RPAS;NG-FMS architecture;4-dimensional trajectory planning;CNS integrity monitoring;augmentation functions;ground based 4DT planning-negotiation and validation system;4-PNV system;automated trajectory-intent-based operations;mathematical models;safety-critical tasks;mission-critical tasks;CNS integrity performance criteria;cost-effective trajectory profiles;environmental constraints","","11","24","","","","","","IEEE","IEEE Conferences"
"Real-time continuous active sonar processing","G. Canepa; A. Munafò; M. Micheli; L. Morlando; S. Murphy","NATO STO Centre for Maritime Research and Experimentation, 19134 La Spezia, Italia; NATO STO Centre for Maritime Research and Experimentation, 19134 La Spezia, Italia; NATO STO Centre for Maritime Research and Experimentation, 19134 La Spezia, Italia; NATO STO Centre for Maritime Research and Experimentation, 19134 La Spezia, Italia; Defence Research & Development Canada, Atlantic Research Centre, Halifax, NS, Canada, B3J 1G2","OCEANS 2015 - Genova","","2015","","","1","6","This work describes the development of continuous active sonar (CAS) processing at CMRE. The software uses subband processing to achieve a faster update rate than is possible with pulsed active sonar (PAS). The software development was based on CMRE's PAS processing software, CAINPro, which has been thoroughly tested during previous sea trials and in postprocessing data analysis. Computational efficiency was carefully considered and many optimizations were made so that the software can run in real-time using the constrained computing resources on board CMRE's Ocean Explorer autonomous underwater vehicles (AUV). The software was successfully tested during the REP14 Atlantic sea trial in July 2014, and was able to demonstrate real-time detection of an echo repeater on all nine sub-bands that were processed. The CAS algorithm runs in real-time on the processing board installed on the AUV.","","978-1-4799-8736-8978-1-4673-7164","10.1109/OCEANS-Genova.2015.7271658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7271658","","Software;Sonar;Arrays;Real-time systems;Signal processing algorithms;Frequency-domain analysis;Repeaters","autonomous underwater vehicles;echo;oceanographic techniques;sonar","real-time continuous active sonar processing;pulsed active sonar;computational efficiency;Ocean Explorer autonomous underwater vehicles;REP14 Atlantic sea trial;real-time detection;echo repeater","","6","6","","","","","","IEEE","IEEE Conferences"
"Improved Inter-Layer Prediction for the Scalable Extensions of HEVC","T. Laude; X. Xiu; J. Dong; Y. He; Y. Ye; J. Ostermann","NA; NA; NA; NA; NA; NA","2014 Data Compression Conference","","2014","","","412","412","Summary form only given. Upon the completion of the single-layer H.265/HEVC, scalable extensions of the H.265/HEVC standard, called Scalable High Efficiency Video Coding (SHVC), are currently under development. Compared to the simulcast solution that simply compresses each layer separately, SHVC offers higher coding efficiency by means of inter-layer prediction which is implemented by inserting inter-layer reference (ILR) pictures generated from reconstructed base layer (BL) pictures into the enhancement layer (EL) decoded picture buffer (DPB) for motion-compensated prediction of the collocated pictures in the EL. If the EL has a higher resolution than that of the BL, the reconstructed BL pictures need to be up-sampled to form the ILR pictures. Given that the ILR picture is generated based on the reconstructed BL picture, its suitability for an efficient inter-layer prediction may be limited due to the following reasons. Firstly, quantization is usually applied when coding the BL pictures. Quantization causes the BL reconstructed texture to contain undesired coding artifacts, such as blocking artifacts, ringing artifacts, and color artifacts. Secondly, in case of spatial scalability, a down-sampling process is used to create the BL pictures. To reduce aliasing, the high frequency information in the video signal is typically removed by the down-sampling process. As a result, the texture information in the ILR picture lacks certain high frequency information. In contrast to the ILR picture, the EL temporal reference pictures contain plentiful high frequency information, which could be extracted to enhance the quality of the ILR picture. To further improve the efficiency of inter-layer prediction, a low pass filter may be applied to the ILR picture to alleviate the quantization noise introduced by the BL coding process. In this paper, an ILR enhancement method is proposed to improve the quality of the ILR picture by combining the high frequency information extracted from the EL temporal reference pictures together with the low frequency information extracted from the ILR picture. Experimental results show that the proposed method can significantly increase the ILR efficiency for EL coding, under the Common Test Condition of SHVC, which defines a number of temporal prediction structures called Random Access (RA), Low-delay B (LD-B) and Low-delay P (LD-P), on average the proposed method provides {Y, U, V} BD-rate (BL+EL) gains of {2.0%, 7.1%, 8.2%}, {2.2%, 6.7%, 7.6%} and {4.0%, 7.4%, 8.4%} for RA, LD-B, and LD-P, respectively, in comparison to the performance of the SHVC reference software SHM-2.0.","1068-0314;2375-0359","978-1-4799-3882","10.1109/DCC.2014.45","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6824464","inter-layer prediction;scalable video coding;filter optimization;SHVC;HEVC","Encoding;Quantization (signal);Data mining;Video coding;Data compression;Standards;Image color analysis","image enhancement;image reconstruction;image texture;low-pass filters;video coding","improved interlayer prediction;scalable extensions;H.265/HEVC standard;scalable high efficiency video coding;SHVC;interlayer reference;ILR;base layer pictures;BL pictures;decoded picture buffer;enhancement layer;EL;DPB;motion compensated prediction;texture reconstruction;coding artifacts;blocking artifacts;ringing artifacts;color artifacts;video signal;down sampling process;low pass filter","","","","","","","","","IEEE","IEEE Conferences"
"Automatic Extraction of pipeline parallelism for embedded heterogeneous multi-core platforms","D. Cordes; M. Engel; O. Neugebauer; P. Marwedel","TU Dortmund University Dortmund, Germany; TU Dortmund University Dortmund, Germany; TU Dortmund University Dortmund, Germany; TU Dortmund University Dortmund, Germany","2013 International Conference on Compilers, Architecture and Synthesis for Embedded Systems (CASES)","","2013","","","1","10","Automatic parallelization of sequential applications is the key for efficient use and optimization of current and future embedded multi-core systems. However, existing approaches often fail to achieve efficient balancing of tasks running on heterogeneous cores of an MPSoC. A reason for this is often insufficient knowledge of the underlying architecture's performance. In this paper, we present a novel parallelization approach for embedded MPSoCs that combines pipeline parallelization for loops with knowledge about different execution times for tasks on cores with different performance properties. Using Integer Linear Programming, an optimal solution with respect to the model used is derived implementing tasks with a well-balanced execution behavior. We evaluate our pipeline parallelization approach for heterogeneous MPSoCs using a set of standard embedded benchmarks and compare it with two existing state-of-the-art approaches. For all benchmarks, our parallelization approach obtains significantly higher speedups than either approach on heterogeneous MPSoCs.","","978-1-4799-1400","10.1109/CASES.2013.6662508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662508","Automatic Parallelization;Heterogeneity;MPSoC;Embedded Software;Integer Linear Programming;Pipeline","Pipelines;Parallel processing;Program processors;Computer architecture;Benchmark testing;Integer linear programming","embedded systems;integer programming;linear programming;multiprocessing systems;parallel processing;pipeline processing;resource allocation;system-on-chip","pipeline parallelism automatic extraction;embedded heterogeneous multicore platforms;automatic parallelization;sequential applications;embedded multicore systems;task balancing;heterogeneous cores;integer linear programming;optimal solution;pipeline parallelization approach;heterogeneous MPSoC;standard embedded benchmarks","","8","24","","","","","","IEEE","IEEE Conferences"
"Design and performance of a power train for mild-hybrid motorcycle prototype","M. Morandin; M. Ferrari; S. Bolognani","Department of Industrial Engineering, University of Padova, via Gradenigo 6/A, 35131 - Italy; Department of Industrial Engineering, University of Padova, via Gradenigo 6/A, 35131 - Italy; Department of Industrial Engineering, University of Padova, via Gradenigo 6/A, 35131 - Italy","2013 International Electric Machines & Drives Conference","","2013","","","1","8","Low weight and volume are the principal requirements for electric components in hybrid motorcycles, and the energy storage system (ESS) is the most important part that contributes to this issues. This paper deals with the choice of the best ESS and with the optimization of the hybrid power train in two-wheeled vehicles. A methodology of investigation is given in order to recognize all the data necessary to find the best technology and size for the ESS. The techniques adopted for the design of the electrical machine (EM) are also reported. Furthermore, simulations with finite elements (FE) and mathematical software, in order to predict the EM and the drive performances, are presented. Efficiency maps from FE simulations are also given. Different kind of ESSs are compared with the goal to choose the best solution balancing cost, volume and weight for a reference motorcycle. The whole power train has been practically realized and the new hybrid vehicle has been tested first in appositely test bench and also in a small private racetrack. The measured experimental results are presented.","","978-1-4673-4974-1978-1-4673-4975-8978-1-4673-4972-7978-1-4673-4973","10.1109/IEMDC.2013.6556121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6556121","Hybrid electric vehicle;SPM machine;electric drive;mild-hybrid motorcycle","Torque;Batteries;Motorcycles;Ice;Prototypes","electric drives;electric machines;energy storage;finite element analysis;hybrid electric vehicles;motorcycles;power transmission (mechanical)","power train design;power train performance;mild-hybrid motorcycle prototype;electric component;energy storage system;ESS;hybrid power train;two-wheeled vehicle;electrical machine;finite element software;mathematical software;EM;FE simulation","","3","18","","","","","","IEEE","IEEE Conferences"
"A Frequency-Reconfigurable Antenna Architecture Using Dielectric Fluids","M. Konca; P. A. Warr","Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K.; Department of Electrical and Electronic Engineering, University of Bristol, Bristol, U.K.","IEEE Transactions on Antennas and Propagation","","2015","63","12","5280","5286","A new frequency-reconfigurable antenna architecture is presented, in which a dielectric fluid is pumped into a cavity behind the antenna to change its resonant frequency. The continuous tuning provided by the changing fluid volume allows the resonant frequency to be adjusted to any value within the tunable range. This tuning method does not affect the power handling capability of the antenna and does not consume power while the resonant frequency is kept constant. This method of tuning also stands out in its class by offering a wide tuning range, high efficiency, and very good electrical isolation between the antenna and the control circuitry. The antenna was designed and optimized using Ansys HFSS software and several prototypes were built and tested. Measured results of the input response, radiation pattern, and efficiency are presented. Castor oil (ε<sub>r</sub>= 2.7) and ethyl acetate (ε<sub>r</sub>= 6) were used in physical tests as the tuning fluids to verify the simulated results. Good agreement between simulated and measured results was observed which is also in line with the behavior suggested by theory and earlier investigations.","0018-926X;1558-2221","","10.1109/TAP.2015.2490243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7297818","Frequency Reconfigurable Antenna;Tunable Antenna;Liquid Dielectric;Fluidic Antenna;Fluidic antenna;frequency reconfigurable antenna;liquid dielectric;tunable antenna","Fluids;Antenna measurements;Antenna radiation patterns;Tuning;Resonant frequency;Dielectrics","antenna radiation patterns;tuning;vegetable oils","frequency-reconfigurable antenna architecture;dielectric fluids;resonant frequency;continuous tuning;power handling capability;electrical isolation;control circuitry;Ansys HFSS software;radiation pattern;castor oil;ethyl acetate;physical tests;tuning fluids","","10","37","","","","","","IEEE","IEEE Journals & Magazines"
"Adaptive Inflationary Differential Evolution","E. Minisci; M. Vasile","Department of Mechanical and Aerospace Engineering, University of Strathclyde, Glasgow, Scotland, UK; Department of Mechanical and Aerospace Engineering, University of Strathclyde, Glasgow, Scotland, UK","2014 IEEE Congress on Evolutionary Computation (CEC)","","2014","","","1792","1799","In this paper, an adaptive version of Inflationary Differential Evolution is presented and tested on a set of real case problems taken from the CEC2011 competition on real-world applications. Inflationary Differential Evolution extends standard Differential Evolution with both local and global restart procedures. The proposed adaptive algorithm utilizes a probabilistic kernel based approach to automatically adapt the values of both the crossover and step parameters. In addition the paper presents a sensitivity analysis on the values of the parameters controlling the local restart mechanism and their impact on the solution of one of the hardest problems in the CEC2011 test set.","1089-778X;1941-0026","978-1-4799-1488-3978-1-4799-6626","10.1109/CEC.2014.6900587","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900587","","Sociology;Statistics;Kernel;Algorithm design and analysis;Optimization;Standards;Computer aided software engineering","evolutionary computation;probability;sensitivity analysis","adaptive inflationary differential evolution;CEC2011 competition;local restart procedure;global restart procedure;probabilistic kernel based approach;crossover parameter;step parameter;sensitivity analysis","","2","17","","","","","","IEEE","IEEE Conferences"
"MIL: A language to build program analysis tools through static binary instrumentation","A. S. Charif-Rubial; D. Barthou; C. Valensi; S. Shende; A. Malony; W. Jalby","Exascale Computing Research Laboratory, FR; Laboratoire LaBRI, University of Bordeaux, Bordeaux, FR; Exascale Computing Research Laboratory, FR; Department of Computer and Information Science, University of Oregon, Eugene, USA; Department of Computer and Information Science, University of Oregon, Eugene, USA; Exascale Computing Research Laboratory, FR","20th Annual International Conference on High Performance Computing","","2013","","","206","215","As software complexity increases, the analysis of code behavior during its execution is becoming more important. Instrumentation techniques, through the insertion of code directly into binaries, are essential for program analyses used in debugging, runtime profiling, and performance evaluation. In the context of high-performance parallel applications, building an instrumentation framework is quite challenging. One of the difficulties is due to the necessity to capture both coarse-grain behavior, such as the execution time of different functions, as well as finer-grain actions, in order to pinpoint performance issues. In this paper, we propose a language, MIL, for the development of program analysis tools based on static binary instrumentation. The key feature of MIL is to ease the integration of static, global program analysis with instrumentation. We will show how this enables both a precise targeting of the code regions to analyze and a better understanding of the optimized program behavior.","1094-7256","978-1-4799-0730","10.1109/HiPC.2013.6799106","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6799106","","Probes;Optimization;Binary codes;Assembly;Benchmark testing;Runtime","instrumentation;parallel processing;program diagnostics;programming languages","MIL language;program analysis tools;static binary instrumentation;code behavior analysis;instrumentation techniques;debugging;runtime profiling;performance evaluation;high-performance parallel applications;coarse-grain behavior;finer-grain actions;static global program analysis;program behavior","","3","26","","","","","","IEEE","IEEE Conferences"
"A low complexity and high performance interpolation filter for MPEG IVC","H. Lv; R. Wang; Y. Cai; H. Jia; X. Xie; W. Gao","School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, Shenzhen, China; School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, Shenzhen, China; School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, Shenzhen, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China","2014 19th International Conference on Digital Signal Processing","","2014","","","141","145","Fractional-pel motion compensation is widely adopted in the modern video coding standards such as H.264/AVC, AVS, and HEVC etc. The interpolation filter is a critical factor that influences the coding efficiency. In this paper, a generation algorithm of interpolation filter coefficients is utilized. Based on the coefficients generation algorithm, three different tap filters, namely 6 tap, 8 tap, 10tap, are tested. A combination of 6 tap and 8 tap interpolation filters is proposed and proved to be optimal scheme considering both performance and computational complexity. And it is beneficial to make software optimization more effective, especially when SIMD-like (Single Instruction Multiple Data) operation is used. Experiments show that the average BD-rate gains on luma Y, chroma U and V are 8.01%, 5.08% and 4.98% for CS1 (Constraint set 1), and 9.21%, 7.53% and 7.63% for CS2 (Constraint set 2) in MPEG IVC reference software ITM5.0, compared with the traditional IVC interpolation method. The coding efficiency gains are significant for some video sequences and can reach up to 28.7%. With the merits of high performance and low complexity, our proposed method is formally adopted by MPEG IVC.","2165-3577;1546-1874","978-1-4799-4612","10.1109/ICDSP.2014.6900817","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6900817","MPEG;IVC;fractional-pel;interpolation;motion estimation","Interpolation;Transform coding;Complexity theory;Video coding;Finite impulse response filters;Digital signal processing;Encoding","computational complexity;filtering theory;interpolation;motion compensation;parallel processing;video coding","MPEG IVC;Fractional pel motion compensation;video coding;H.264/AVC;AVS;HEVC;interpolation filter coefficients;coding efficiency;generation algorithm;coefficients generation algorithm;optimal scheme;computational complexity;software optimization;single instruction multiple data;SIMD","","","10","","","","","","IEEE","IEEE Conferences"
"An Effective Synchronization Cost Reduction Approach","O. Kermia; R. Kechad","NA; NA","2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing","","2013","","","1974","1981","In the area of distributed embedded systems, the literature includes a considerable number of works on synchronization tools or protocols but little ones on the non less important topic of synchronization cost. Synchronization operations impose temporal order on a software system by forcing some tasks to wait until other tasks complete their computations. However, synchronization costs can negatively affect embedded system performances. This paper proposes an effective approach for minimizing the synchronization cost by eliminating redundant synchronizations and describes tests performed under realistic conditions which have shown significant improvements.","","978-0-7695-5088","10.1109/HPCC.and.EUC.2013.285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6832168","Distributed Embedded Systems;Operating Systems;Synchronization Cost Optimization","Synchronization;Program processors;Real-time systems;Processor scheduling;Operating systems;Scheduling","cost reduction;directed graphs;distributed processing;embedded systems;operating systems (computers);redundancy;synchronisation","distributed embedded system performances;synchronization tools;synchronization protocols;software system;synchronization cost minimization;redundant synchronization elimination;synchronization cost reduction approach;operating systems;directed acyclic graph;DAG","","","13","","","","","","IEEE","IEEE Conferences"
"Planning algorithm for optimal CHP generation plant connection in urban distribution network (UDN) according LCTA principle","S. Boljevic","Department of Electrical and Electronic Engineering, Cork Institute of Technology, Cork, Ireland","2014 49th International Universities Power Engineering Conference (UPEC)","","2014","","","1","8","Environmental awareness and sustainable development based on long-term diversification of energy sources are key points on the agenda of energy policy-makers. As part of electrical power systems UDNs are evolving from the present traditional electricity supply network towards more decentralized system with smaller, more efficient generation unit normally connected directly to the network at consumer sites. Combined Heat &amp; Power (CHP) generation is the most efficient way of energy supply in urban area available today. It delivers significant benefits to its host facilities and urban distributed network (UDN) to which is connected. Economic viability of CHP generation for many sites requires integration with the UDN for backup and supplementary power needs and in some case the export of excess power to the UDN. CHP system integration into existing UDN entail installation costs. How these integration costs are distributed will have considerable impact on development and implementation of CHP generation in urban areas. The objective of this paper is to use analytical and statistical methods to develop an algorithm that provide means of determining the optimum capacity of a CHP generating plant that can be accommodated within the UDN, which correspond to Least Cost Technically Acceptable (LCTA) principle, and the UDN long term network planning policy. In order to determine optimal size of CHP generating plant that could be connected at any particular busbar on the UDN without causing a significant adverse impact on performance of the UDN and with minimum connection cost an algorithm is created that incorporates an analytical and multiple regression analysis model. It is tested using data obtained from ERAC power analysing software incorporating load flow, fault current level and power losses analysis. Additional data needed for effective algorithm creation was obtained via surveys of local UDN operators and planners. These analyses are performed on a 34 busbar network resembling part of the real UDN of Cork city for validation purposes and accuracy of the algorithm proposed.","","978-1-4799-6557-1978-1-4799-6556","10.1109/UPEC.2014.6934631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6934631","CHP Plant;UDN;Cost;Connection;LCTA principle;Optimization;Multiple Regression Analysis","Cogeneration;Economics;Electricity;Thermal energy;Urban areas;Load flow;Reliability","busbars;cogeneration;electrical installation;fault currents;load flow;power distribution planning;power generation economics;power generation planning;regression analysis;sustainable development","planning algorithm;optimal CHP generation plant connection;urban distribution network;LCTA principle;environmental awareness;sustainable development;long-term energy source diversification;energy policy-makers;electrical power systems;electricity supply network;decentralized system;generation unit;combined heat and power generation;energy supply;economic viability;backup power;supplementary power;installation costs;statistical methods;analytical methods;optimum capacity determination;least cost technically acceptable principle;UDN long term network planning policy;optimal size determination;multiple regression analysis model;ERAC power analysing software;load flow;fault current level;power losses analysis;local UDN operators;local UDN planners;busbar network;Cork city","","","24","","","","","","IEEE","IEEE Conferences"
"Quench Propagation in YBCO Pancake: Experimental and Computational Results","T. Lécrevisse; X. Chaud; F. Debray; M. Devaux; P. Fazilleau; F. P. Juster; Y. Miyoshi; J. -. Rey; P. Tixador; B. Vincent","CEA-DSM-IRFU-SACM, Gif-sur-Yvette , France; Laboratoire National des Champs Magnétiques Intenses, LNCMI, CNRS/UJF/INSA/UPS, Grenoble, France; Laboratoire National des Champs Magnétiques Intenses, LNCMI, CNRS/UJF/INSA/UPS, Grenoble, France; CEA-DSM-IRFU-SACM, Gif-sur-Yvette, France; CEA-DSM-IRFU-SACM, Gif-sur-Yvette , France; CEA-DSM-IRFU-SACM, Gif-sur-Yvette, France; Laboratoire National des Champs Magnétiques Intenses, LNCMI, CNRS/UJF/INSA/UPS, Grenoble, France; CEA-DSM-IRFU-SACM, Gif-sur-Yvette, France; G2Elab/Institut Néel, CNRS/Grenoble-INP/UJF, Grenoble, France; G2Elab/Institut Néel, CNRS/Grenoble-INP/UJF, Grenoble, France","IEEE Transactions on Applied Superconductivity","","2013","23","3","4601805","4601805","High-temperature superconductors are promising materials for future applications such as high field magnets thanks to their ability to carry high current densities. Nevertheless, their protection still remains a key issue mainly due to the slow velocity of quench propagation. To understand the quench behavior of a YBCO coil, a simulation code has been developed using the CASTEM-CEA finite element software. Simulations can be performed considering constant current and magnetic field. Results of those simulations will be displayed and a way for improving the protection by adding a stabilizer will be discussed. Two well instrumented YBCO coils were fabricated in order to obtain experimental data on quench propagation in pancake configuration. Their design and some measurements are reported in this paper along with another experiment on a double pancake made by and tested at CNRS Grenoble. Finally, we compare the numerical and experimental results and discuss the accuracy of our simulations.","1051-8223;1558-2515;2378-7074","","10.1109/TASC.2013.2246753","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6461069","High-temperature superconductors (HTS) protection optimization;HTS quench propagation;HTS quench simulation;YBCO coil","Coils;Insulation;Copper;Thermal stability;Yttrium barium copper oxide;Stability analysis","barium compounds;current density;finite element analysis;high-temperature superconductors;superconducting coils;yttrium compounds","quench propagation;YBCO pancake;high-temperature superconductors;high held magnets;current density;YBCO coil;simulation code;CASTEM-CEA finite element software;YBCO","","10","12","","","","","","IEEE","IEEE Journals & Magazines"
"Automatic generation of Human Machine Interface screens from component-based reconfigurable virtual manufacturing cell","B. Ahmad; X. Kong; R. Harrison; J. Watermann; A. W. Colombo","WMG, University of Warwick, Coventry, United Kingdom; WMG, University of Warwick, Coventry, United Kingdom; WMG, University of Warwick, Coventry, United Kingdom; University of Emden, Germany; University of Emden, Germany","IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society","","2013","","","7428","7433","Increasing complexity and decreasing time-to-market require changes in the traditional way of building automation systems. The paper describes a novel approach to automatically generate the Human Machine Interface (HMI) screens for component-based manufacturing cells based on their corresponding virtual models. Manufacturing cells are first prototyped and commissioned within a virtual engineering environment to validate and optimise the control behaviour. A framework for reusing the embedded control information in the virtual models to automatically generate the HMI screens is proposed. Finally, for proof of concept, the proposed solution is implemented and tested on a test rig.","1553-572X","978-1-4799-0224","10.1109/IECON.2013.6700369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6700369","virtual engineering;component-based automation;human machine interface;programmable logic controllers;control systems","Actuators;Manuals;Automation;Software;Data models;Manufacturing","cellular manufacturing;human computer interaction;object-oriented programming;time to market;virtual manufacturing","automatic generation;human machine interface screens;component-based reconfigurable virtual manufacturing cell;time-to-market;automation systems;HMI screens;component-based manufacturing cells;virtual models;virtual engineering environment;control behaviour;embedded control information;test rig","","3","21","","","","","","IEEE","IEEE Conferences"
"Modelling of bottom-lit down-draft (BLDD) clean-burning coal stove","M. Ibraimo; H. J. Annegarn; C. Pemberton-Pigott","Eduardo Mondlane University, Maputo, Mozambique; University of Johannesburg, APK Campus, South Africa; University of Johannesburg, APK Campus, South Africa","Twenty-Second Domestic Use of Energy","","2014","","","1","6","The use of coal as a household fuel to meet requirements of cooking and heating in low income communities on the South African Highveld results high levels of indoor and ambient air pollution. The combustion of coal is regarded as the largest source of air pollution in Gauteng Province, in terms of resultant human exposure. As part of measures to combat this problem, an innovative clean-burning multi-use (cooking and heating) coal stove has been developed. Incorporating a bottom-lit down-draft (BLDD) design. However, optimising the fundamental design by construction and testing has been a time and resource consuming exercise. In this paper we report on the use of PHOENICS computational fluid dynamics (CFD) software to develop a model for simulating an optimal geometry for the BLDD coal stove, including heat and gas transfer components. We used mass, momentum and energy balances built within PHOENICS software, through FLAIR program (a special-purpose sub-module program of PHOENICS, designed to provide an air-flow and thermal-simulation facility for heating, ventilation and air conditioning systems), to evaluate the basic design in terms of heat distribution from fuel bed to exhaust pipe. As a simplifying assumption for computational efficiency, a model using rectangular conduits was built and validated by comparing the simulations with experimental performance of the stove. This article reports the initial findings of the CFD modelling, involving air flows and temperature profile, assuming a constant heat source located in the combustion chamber. Validation of the model will lead to an accelerated design cycle for reaching a design solution for optimised thermal (cooking) performance and reduced pollution emissions.","","978-0-9922041-5-0978-0-9922041-4","10.1109/DUE.2014.6827754","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6827754","coal stove;improved stoves;stove modeling;CFD modelling;bottom-lit down-draft;BLDD","Geometry;Coal;Computational modeling;Atmospheric modeling;Mathematical model;Heat transfer","air pollution;coal;combustion;computational fluid dynamics;heat transfer;heating","bottom-lit down-draft clean-burning coal stove;BLDD coal stove;household fuel;cooking;heating;South Africa;Gauteng Province;air pollution;PHOENICS computational fluid dynamics software;FLAIR program;heat transfer;gas transfer;air flow;thermal simulation;heat distribution;fuel bed;exhaust pipe;computational efficiency;rectangular conduits;temperature profile;combustion chamber","","","7","","","","","","IEEE","IEEE Conferences"
"Systolic processing element based on the Viterbi algorithm for DNA sequence alignment","Nur Farah Ain Saliman; N. D. A. Sabri; S. A. M. Al Junid; N. M. Tahir; Z. A. Majid","Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia; Faculty of Electrical Engineering, Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia","2013 IEEE Conference on Systems, Process & Control (ICSPC)","","2013","","","268","274","This paper presents the potential of the one-dimensional systolic processing element of the Viterbi algorithm in optimizing the DNA sequence alignment system processing engine. The objective of this paper was to optimize the sensitive DNA sequence alignment algorithm toward improving the performance and design complexity. In addition, theoretical study, design, and simulation were conducted using the Altera Quartus II version 9.1 software. The proposed architecture has been tested and is capable of accelerating more than 32 bits of input. As a conclusion, the proposed systolic design has been proven and is able to optimize the performance and design complexity of the most sensitive DNA sequence alignment algorithm on hardware-based accelerator platform.","","978-1-4799-2209-3978-1-4799-2208","10.1109/SPC.2013.6735145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6735145","Viterbi algorithm;DNA;sequence alignment","DNA;Silicon","bioinformatics;DNA;parallel algorithms;systolic arrays","Viterbi algorithm;one-dimensional systolic processing element;DNA sequence alignment system processing engine;design complexity;Altera Quartus II version 9.1 software;systolic design;hardware-based accelerator platform;sensitive DNA sequence alignment algorithm","","1","16","","","","","","IEEE","IEEE Conferences"
"Performance evaluation of HDD and SSD on 10GigE, IPoIB &amp; RDMA-IB with Hadoop Cluster Performance Benchmarking System","P. Saxena; P. Kumar","Computer Science and Engineering, Amity University Uttar Pradesh, Noida, India; Computer Science and Engineering, Amity University Uttar Pradesh, Noida, India","2014 5th International Conference - Confluence The Next Generation Information Technology Summit (Confluence)","","2014","","","30","35","Hadoop and Map reduce today are facing huge amounts of data and are moving towards ubiquitous for big data storage and processing. Benchmarking tools available are not capable of analyzing performance of the Hadoop cluster system and are made to either run in a single node system. HiBench is an essential part of Hadoop. It is a comprehensive benchmark suit that consists of a complete deposit of Hadoop applications having micro bench marks &amp; real time applications for benchmarking the performance of Hadoop on the available type of storage device (i.e. HDD and SSD) and machine configuration. This is helpful to optimize the performance and improve the support towards the limitations of Hadoop system. In this paper we will present the Performance evaluation of HDD and SSD with 10GigE, IPoIB &amp; RDMA-IB on Hadoop Cluster Performance Benchmarking System. In addition, we will also demonstrate that the traditional servers and old Cloud systems can be upgraded by software and hardware up gradations to perform at par with the modern technologies to handle these loads, without spending ruthlessly on up gradations or complete changes in the system with the use of Modern storage devices and interconnect networking systems.","","978-1-4799-4236-7978-1-4799-4237","10.1109/CONFLUENCE.2014.6949047","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6949047","Hadoop;HDFS;SSD;HDD;HiBench;Benchmarking;10GigE;IPoIB;RDMA-IB;Sort;Word Count;TeraSort","Performance evaluation;Benchmark testing;Software;Protocols;Big data;Servers;Throughput","cloud computing;disc drives;hard discs","HDD;SSD;IPoIB;RDMA-IB;Hadoop cluster performance benchmarking system;MapReduce;HiBench;storage device;cloud system","","1","18","","","","","","IEEE","IEEE Conferences"
"A Deep Learning Network Approach to<italic>ab initio</italic>Protein Secondary Structure Prediction","M. Spencer; J. Eickholt; J. Cheng","Informatics Institute, University of Missouri, Columbia, MO, USA; Department of Computer Science, Central Michigan University, Mount Pleasant, MI, USA; Department of Computer Science, University of Missouri, Columbia, MO, USA","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2015","12","1","103","112","Ab initio protein secondary structure (SS) predictions are utilized to generate tertiary structure predictions, which are increasingly demanded due to the rapid discovery of proteins. Although recent developments have slightly exceeded previous methods of SS prediction, accuracy has stagnated around 80 percent and many wonder if prediction cannot be advanced beyond this ceiling. Disciplines that have traditionally employed neural networks are experimenting with novel deep learning techniques in attempts to stimulate progress. Since neural networks have historically played an important role in SS prediction, we wanted to determine whether deep learning could contribute to the advancement of this field as well. We developed an SS predictor that makes use of the position-specific scoring matrix generated by PSI-BLAST and deep learning network architectures, which we call DNSS. Graphical processing units and CUDA software optimize the deep network architecture and efficiently train the deep networks. Optimal parameters for the training process were determined, and a workflow comprising three separately trained deep networks was constructed in order to make refined predictions. This deep learning network approach was used to predict SS for a fully independent test dataset of 198 proteins, achieving a Q<sub>3</sub>accuracy of 80.7 percent and a Sov accuracy of 74.2 percent.","1545-5963","","10.1109/TCBB.2014.2343960","National Institutes of Health; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6872810","Machine learning;neural nets;protein structure prediction;deep learning","Proteins;Accuracy;Training;Neural networks;Testing;Bioinformatics;Computational biology","ab initio calculations;biology computing;graphics processing units;learning (artificial intelligence);molecular biophysics;molecular configurations;neural nets;parallel architectures;proteins","deep learning network approach;ab initio protein secondary structure prediction;tertiary structure;SS prediction;neural networks;position-specific scoring matrix;PSI-BLAST;DNSS;graphical processing units;CUDA software","Databases, Protein;Machine Learning;Neural Networks (Computer);Protein Structure, Secondary;Proteins","66","52","","","","","","IEEE","IEEE Journals & Magazines"
"METEOR: An Enterprise Health Informatics Environment to Support Evidence-Based Medicine","M. Puppala; T. He; S. Chen; R. Ogunti; X. Yu; F. Li; R. Jackson; S. T. C. Wong","Houston Methodist Hospital Research Institute; Houston Methodist Hospital Research Institute; Houston Methodist Hospital Research Institute; Houston Methodist Hospital Research Institute; Houston Methodist Hospital Research Institute; Houston Methodist Hospital Research Institute; Houston Methodist Hospital; Department of Systems Medicine and Bioengineering, Houston Methodist Hospital Research Institute, Houston, TX, USA","IEEE Transactions on Biomedical Engineering","","2015","62","12","2776","2786","Goal: The aim of this paper is to propose the design and implementation of next-generation enterprise analytics platform developed at the Houston Methodist Hospital (HMH) system to meet the market and regulatory needs of the healthcare industry. Methods: For this goal, we developed an integrated clinical informatics environment, i.e., Methodist environment for translational enhancement and outcomes research (METEOR). The framework of METEOR consists of two components: the enterprise data warehouse (EDW) and a software intelligence and analytics (SIA) layer for enabling a wide range of clinical decision support systems that can be used directly by outcomes researchers and clinical investigators to facilitate data access for the purposes of hypothesis testing, cohort identification, data mining, risk prediction, and clinical research training. Results: Data and usability analysis were performed on METEOR components as a preliminary evaluation, which successfully demonstrated that METEOR addresses significant niches in the clinical informatics area, and provides a powerful means for data integration and efficient access in supporting clinical and translational research. Conclusion: METEOR EDW and informatics applications improved outcomes, enabled coordinated care, and support health analytics and clinical research at HMH. Significance: The twin pressures of cost containment in the healthcare market and new federal regulations and policies have led to the prioritization of the meaningful use of electronic health records in the United States. EDW and SIA layers on top of EDW are becoming an essential strategic tool to healthcare institutions and integrated delivery networks in order to support evidence-based medicine at the enterprise level.","0018-9294;1558-2531","","10.1109/TBME.2015.2450181","John S Dunn Research Foundation; TT and WF Chao Center for BRAIN; Houston Methodist Research Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7137654","Clinical Data Warehouse;Smartphone Health App;Cohort Identification;Natural Language Processing;Readmission Risk;Outcomes Research;Clinical data warehouse;cohort identification;natural language processing (NLP);outcomes research;readmission risk;smartphone health app","Hospitals;Data mining;Natural language processing;Databases;Informatics;Data warehouses","data analysis;data integration;data mining;data warehouses;decision support systems;electronic health records;health care;information retrieval","health analytics;cost containment;healthcare market;electronic health records;SIA layers;healthcare institutions;integrated delivery networks;enterprise level;informatics applications;METEOR EDW;data integration;clinical informatics area;preliminary evaluation;usability analysis;data analysis;clinical research training;risk prediction;data mining;cohort identification;hypothesis testing;data access;clinical decision support systems;software intelligence and analytics layer;enterprise data warehouse;methodist environment for translational enhancement and outcomes research;integrated clinical informatics environment;healthcare industry;HMH;Houston Methodist Hospital system;enterprise analytics platform;evidence-based medicine;enterprise health informatics environment","Adult;Aged;Data Mining;Database Management Systems;Decision Support Systems, Clinical;Evidence-Based Medicine;Female;Humans;Male;Middle Aged;Mobile Applications;Natural Language Processing;Risk Assessment;Translational Medical Research","17","27","","","","","","IEEE","IEEE Journals & Magazines"
"A Tabu Search based heuristic for police units positioning","N. F. M. Mendes; A. Gustavo dos Santos","Departamento de Informática, Universidade Federal de Viçosa, Viçosa, MG, Brazil; Departamento de Informática, Universidade Federal de Viçosa, Viçosa, MG, Brazil","2015 Latin American Computing Conference (CLEI)","","2015","","","1","11","Public safety is one of most demanding areas in public administration, having direct consequences on people welfare. Creating crime containment strategies or providing a fast answer to emergency situations when they occurs is a challenge. In this paper, we use Operations Research techniques to solve a police units positioning problem, in order to maximize the profit associated with police coverage in a city. We propose a model to describe the problem, heuristic methods based on Tabu Search and a penalty function for infeasible solutions. The tests are performed using instances with real street network of three different cities. The results show the efficacy of the penalty method, increasing the number of feasible solutions found, the good quality of the solutions generated by the Tabu Search, and a low convergence time, even for large instances.","","978-1-4673-9143-6978-1-4673-9142","10.1109/CLEI.2015.7359471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359471","Police units positioning;Optimization;Covering;Tabu Search","Computational modeling;Law enforcement;Adaptation models;Electronic mail;Cities and towns;Software;Safety","operations research;police;search problems","tabu search;operations research techniques;police units positioning problem;profit;police coverage;penalty function","","","36","","","","","","IEEE","IEEE Conferences"
"Novel adaptive algorithm for optimal relay setting with improved coordination","E. Purwar; M. M. Choudhary","Department of Electrical Engineering, National Institute of Technology, Patna-800005 India; Department of Electrical Engineering, National Institute of Technology, Patna-800005 India","2014 Students Conference on Engineering and Systems","","2014","","","1","6","With the penetration of distributed generation (DGs) at distribution level, protection of a distribution network is quite challenging due to the change in operating mode and system topology. Thus the relay settings have to be updated every time there is a change in network configuration. This paper presents a novel adaptive algorithm with fastest backup protection in which new optimal relay settings (pickup current and time dial setting) calculated by proposed algorithm solves the problem regarding changing of coordination time interval and critical clearing time as mode changed. This proposed algorithm uses the numerical inverse overcurrent relays which have the capability of memory storage and to cope up with communication. The proposed algorithm has been verified on test systems of different operating modes using MATLAB/Simulink and C program. Results showed the importance and necessity of this scheme in maintaining the optimal performance of the relays under changing operating modes.","","978-1-4799-4939-7978-1-4799-4940-3978-1-4799-4941","10.1109/SCES.2014.6880078","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6880078","Distributed generation (DG);Adaptive Protection;Overcurrent Relay (OCR);Co-ordination Time Interval (CTI);Critical clearing time;Time Dial Setting (TDS);optimization","Relays;Fault currents;Circuit faults;Substations;Computers;Adaptive algorithms;Optical character recognition software","distributed power generation;power distribution protection;relay protection","adaptive algorithm;optimal relay;distributed generation;distribution network protection;fastest backup protection;coordination time interval;critical clearing time;numerical inverse overcurrent relays;memory storage capability;MATLAB;Simulink;C program","","1","14","","","","","","IEEE","IEEE Conferences"
"Workload-dependent BTI analysis in a processor core at high level","O. Heron; C. Sandionigi; E. Piriou; S. Mbarek; V. Huard","CEA, LIST, Computing and Design Environment Laboratory, 91191 Gif sur Yvette, France; CEA, LIST, Computing and Design Environment Laboratory, 91191 Gif sur Yvette, France; CEA, LIST, Computing and Design Environment Laboratory, 91191 Gif sur Yvette, France; CEA, LIST, Computing and Design Environment Laboratory, 91191 Gif sur Yvette, France; STMicroelectronics, 850 rue Jean Monnet 38926 Crolles, France","2015 IEEE International Reliability Physics Symposium","","2015","","","CA.6.1","CA.6.6","This work presents a software tool that enables joint architecture simulation and estimation of ageing-induced timing drifts at register-transfer level (RTL). The objective is to enable design exploration of processor microarchitecture under ageing constraint. The tool makes the bridge between user application, micro-architecture design, PVT corner and device-level degradation model. The environment will aid design engineers to apply early software and architecture optimizations at RTL for a better power consumption, performance and failure rate tradeoff.","1938-1891;1541-7026","978-1-4673-7362","10.1109/IRPS.2015.7112784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7112784","NBTI;RTL;RISC processor;Simulation","Timing;Logic gates;Benchmark testing;Degradation;Aging;Computer architecture;Approximation methods","ageing;circuit simulation;failure analysis;integrated circuit design;integrated circuit reliability;low-power electronics;microprocessor chips;negative bias temperature instability","workload-dependent BTI analysis;processor core;joint architecture simulation;ageing-induced timing drifts;register-transfer level;processor microarchitecture;user application;microarchitecture design;PVT corner;device-level degradation model;architecture optimizations;power consumption;failure rate tradeoff","","3","12","","","","","","IEEE","IEEE Conferences"
"Autonomous indoor navigation of low-cost quadcopters","A. Hussein; A. Al-Kaff; A. de la Escalera; J. M. Armingol","Intelligent Systems Lab (LSI) Research Group, Universidad Carlos III de Madrid (UC3M), Legaæs, Madrid, Spain; Intelligent Systems Lab (LSI) Research Group, Universidad Carlos III de Madrid (UC3M), Legaæs, Madrid, Spain; Intelligent Systems Lab (LSI) Research Group, Universidad Carlos III de Madrid (UC3M), Legaæs, Madrid, Spain; Intelligent Systems Lab (LSI) Research Group, Universidad Carlos III de Madrid (UC3M), Legaæs, Madrid, Spain","2015 IEEE International Conference on Service Operations And Logistics, And Informatics (SOLI)","","2015","","","133","138","Many researchers from academia and industry are investigating closely how to control an autonomous mobile robot, especially Unmanned Aerial Vehicles (UAV). This paper shows the ability of a low-cost quadcopter, ""Parrot AR-Drone 2.0"", to navigate a predetermined route. The path is obtained by optimized path planning algorithm. A generic Simulated Annealing (SA) optimization algorithm is implemented to generate the obstacle-free path. This path is divided into several waypoints, which are navigated by the drone in various experiments. The position and orientation of the quadcopter are estimated with the incremental motion estimation approach using Inertial Measurement Unit (IMU), which is mounted on the drone. The quadcopter is controlled via Simulink model with PID, which manipulates the drone internal controller for the pitch, roll, yaw and vertical speed. Four different experiments were tested to evaluate the performance of the proposed algorithm and the obtained results indicate the high performance of the quadcopter and its applicability in various navigation applications.","","978-1-4673-8480-3978-1-4673-6986","10.1109/SOLI.2015.7367607","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7367607","","Path planning;Navigation;Schedules;Cooling;Software packages;Conferences;Logistics","autonomous aerial vehicles;helicopters;indoor navigation;mobile robots;path planning;simulated annealing;three-term control","autonomous indoor navigation;low-cost quadcopters;autonomous mobile robot;unmanned aerial vehicles;UAV;Parrot AR-Drone 2.0;path planning;simulated annealing;SA optimization;obstacle-free path;inertial measurement unit;IMU;Simulink model;PID","","11","23","","","","","","IEEE","IEEE Conferences"
"Power Capping: What Works, What Does Not","P. Petoumenos; L. Mukhanov; Z. Wang; H. Leather; D. S. Nikolopoulos","NA; NA; NA; NA; NA","2015 IEEE 21st International Conference on Parallel and Distributed Systems (ICPADS)","","2015","","","525","534","Peak power consumption is the first order design constraint of data centers. Though peak power consumption is rarely, if ever, observed, the entire data center facility must prepare for it, leading to inefficient usage of its resources. The most prominent way for addressing this issue is to limit the power consumption of the data center IT facility far below its theoretical peak value. Many approaches have been proposed to achieve that, based on the same small set of enforcement mechanisms, but there has been no corresponding work on systematically examining the advantages and disadvantages of each such mechanism. In the absence of such a study, it is unclear what is the optimal mechanism for a given computing environment, which can lead to unnecessarily poor performance if an inappropriate scheme is used. This paper fills this gap by comparing for the first time five widely used power capping mechanisms under the same hardware/software setting. We also explore possible alternative power capping mechanisms beyond what has been previously proposed and evaluate them under the same setup. We systematically analyze the strengths and weaknesses of each mechanism, in terms of energy efficiency, overhead, and predictable behavior. We show how these mechanisms can be combined in order to implement an optimal power capping mechanism which reduces the slowdown compared to the most widely used mechanism by up to 88%. Our results provide interesting insights regarding the different trade-offs of power capping techniques, which will be useful for designing and implementing highly efficient power capping in the future.","1521-9097","978-0-7695-5785-4978-1-4673-8670","10.1109/ICPADS.2015.72","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7384335","Power Optimization;Power Capping;Compiler;Dvfs;Rapl","Power demand;Benchmark testing;Hardware;Servers;Runtime;Instruction sets;Energy consumption","computer centres;energy conservation;power aware computing","power capping mechanisms;energy efficiency;overhead;slowdown reduction;data centers","","6","22","","","","","","IEEE","IEEE Conferences"
"New solutions for modeling and verification of B-based reconfigurable control systems","R. Oueslati; O. Mosbahi; M. Khalgui; S. Ben Ahmed","Faculty of Sciences of Tunis, University of Tunis El Manar, Tunisia; National Institute of Applied Sciences and Technology, University of Carthage, Tunisia; National Institute of Applied Sciences and Technology, University of Carthage, Tunisia; Faculty of Sciences of Tunis, University of Tunis El Manar, Tunisia","2014 11th International Conference on Informatics in Control, Automation and Robotics (ICINCO)","","2014","01","","749","757","The paper deals with the modeling and verification of B method-based reconfigurable control systems. Reconfiguration means the dynamic changes of the system behavior at run-time according to well-defined conditions to adapt it to its environment. A reconfiguration scenario is applied as a response to improve the system's performance, or also to recover and prevent hardware/software errors, or also to adapt its behavior to new requirements according to the environment evolution. A new extension called Reconfigurable B “R-B” is proposed to specify reconfigurable control systems. It consists of two modules: Behavior and Control. The first defines all possible behaviors of the system, and whereas the second is a set of reconfiguration functions applied to change the system from a behavioral configuration to another one at run-time. We verify a reconfigurable control system by using the B method. The goal is to guarantee the consistency and the correctness of the abstract specification level. The second contribution of this paper deals with the verification of the reconfigurable system by avoiding redundant checking of different behaviors sharing similar operations. In order to control the complexity of verification, an optimal algorithm is developed and a prototyped tool called “Check R-B” is implemented. The paper's contribution is applied to a benchmark production system FESTO.","","978-9-8975-8062-8978-9-8975-8061","10.5220/0005091207490757","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7049850","Control System;B method;Reconfiguration;Modeling;Formal Verification;Optimization","Control systems;Drilling machines;Abstracts;Educational institutions;Vehicle dynamics;Benchmark testing","","","","2","13","","","","","","IEEE","IEEE Conferences"
"Lexical Parsing Expression Recognition Schemata","M. Lumpe","NA","2015 24th Australasian Software Engineering Conference","","2015","","","165","174","Parsing expression grammars (PEGs) have emerged as a promising substitute for context-free grammars (CFGs) and regular expressions (REs) in programming language specification. The benefits of PEGs are twofold. First, parsing expression grammars replace unordered choice between alternatives by prioritized choice, which naturally solves the ubiquitous ""dangling else"" problem in grammar definitions. Second, PEGs employ ""character-level syntax"" specifications that eliminate the need to separate the lexical and hierarchical components of a language specification. However, there is ""no free lunch"" in PEGs. PEGs capture only syntactic relationships, but many language constructs cannot be parsed without additional semantic information. Moreover, character-level specifications can become unwieldy, as every aspect of the language, including spacing, has to be accounted for. To overcome these issues, we extend the original PEG formalism to incorporate semantic predicates that yield a programmatic means for state-based token recognition control. Furthermore, rather than requiring a single complete specification, we capture lexical components as PEG closures that provide a self-contained token recognition mechanism to reduce the clutter associated with purely character-level PEGs. To test the effectiveness of our approach, we use it for the construction of a Delphi language front-end and practically confirm that Ford's theoretical linear-time result also holds for PEG closures.","1530-0803","978-1-4673-9390","10.1109/ASWEC.2015.27","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7365805","parsing expression grammars;semantic predicates;language composition;language processing","Grammar;Syntactics;Semantics;Java;Character recognition;Automata","grammars;programming languages;specification languages","lexical parsing expression recognition schemata;parsing expression grammars;programming language specification;semantic predicates;state-based token recognition control;lexical components;PEG closures;self-contained token recognition mechanism;Delphi language front-end","","","28","","","","","","IEEE","IEEE Conferences"
"Bilevel approach for optimal location and contract pricing of distributed generation in radial distribution systems using mixed-integer linear programming","M. J. Rider; J. M. Lopez-Lezama; J. Contreras; A. Padilha-Feltrin","Departamento de Engenharia Eletrica, Faculdade de Engenharia de Ilha Solteira, UNESP - Universidade Estadual Paulista, Ilha Solteira, Sao Paulo, Brazil; Department of Electrical Engineering, Universidad de Antioquia, Efficient Energy Management Research Group (GIMEL), Medellin, Colombia; E.T.S. de Ingenieros Industriales, Universidad de Castilla - La Mancha, 13071 Ciudad Real, Spain; Departamento de Engenharia Eletrica, Faculdade de Engenharia de Ilha Solteira, UNESP - Universidade Estadual Paulista, Ilha Solteira, Sao Paulo, Brazil","IET Generation, Transmission & Distribution","","2013","7","7","724","734","In this study, a novel approach for the optimal location and contract pricing of distributed generation (DG) is presented. Such an approach is designed for a market environment in which the distribution company (DisCo) can buy energy either from the wholesale energy market or from the DG units within its network. The location and contract pricing of DG is determined by the interaction between the DisCo and the owner of the distributed generators. The DisCo intends to minimise the payments incurred in meeting the expected demand, whereas the owner of the DG intends to maximise the profits obtained from the energy sold to the DisCo. This two-agent relationship is modelled in a bilevel scheme. The upper-level optimisation is for determining the allocation and contract prices of the DG units, whereas the lower-level optimisation is for modelling the reaction of the DisCo. The bilevel programming problem is turned into an equivalent single-level mixed-integer linear optimisation problem using duality properties, which is then solved using commercially available software. Results show the robustness and efficiency of the proposed model compared with other existing models. As regards to contract pricing, the proposed approach allowed to find better solutions than those reported in previous works.","1751-8687;1751-8695","","10.1049/iet-gtd.2012.0369","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6552527","","","distributed power generation;integer programming;linear programming;power distribution economics;power markets;pricing","bilevel approach;optimal location;contract pricing;distributed generation;radial distribution systems;mixed-integer linear programming;market environment;distribution company;DisCo;energy market;two-agent relationship;bilevel programming problem;single-level mixed-integer linear optimisation problem;34-node test-distribution system","","4","42","","","","","","IET","IET Journals & Magazines"
"Spatially distributed sequential array stimulation of tibial anterior muscle for foot drop correction","H. Zhou; Y. Wang; W. Chen; N. Zhang; L. Krundel; G. Li","Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Guangdong, China 518005; Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Guangdong, China 518005; Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Guangdong, China 518005; Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Guangdong, China 518005; Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Guangdong, China 518005; Key Laboratory of Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Guangdong, China 518005","2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","","2015","","","3407","3410","Electrode arrays for the ease of electrode placement in the correction of foot drop with surface electrical stimulation have been developed in recent years. However, the configuration and identification of optimal stimulation sites with regard to time efficiency, stimulation comfort, and fatigue resistance is yet to be solved. In this study, the candidate stimulation sites were ranked and selected according to the motor thresholds induced by 1 Hz stimulation trains. Then based on the selected sites, a new stimulation configuration method termed spatially distributed sequential stimulation was tested and compared with traditional single electrode stimulation for foot drop correction in two normal subjects. The preliminary results demonstrated that the motor threshold of spatially distributed sequential stimulation was equal or less than motor thresholds of each stimulus sites. Besides, with the same stimulation parameters, the spatially distributed sequential stimulation induced larger dorsiflexion motion compared with traditional single electrode stimulation. These findings suggest that spatially distributed sequential stimulation on the selected sites might be an effective electrode array configuration method for correcting foot drop with electrical stimulation.","1094-687X;1558-4615","978-1-4244-9271-8978-1-4244-9270","10.1109/EMBC.2015.7319124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7319124","foot drop;functional electrical stimulation;electrode array;dorsiflexion angle","Electrodes;Arrays;Muscles;Foot;Goniometers;Fatigue;Electrical stimulation","bioelectric phenomena;biomechanics;biomedical electrodes;geriatrics;muscle","spatially distributed sequential array stimulation;tibial anterior muscle;foot drop correction;electrode arrays;electrode placement;surface electrical stimulation;optimal stimulation sites;time efficiency;fatigue resistance;motor thresholds;stimulation trains;stimulation configuration method;spatially distributed sequential stimulation;single electrode stimulation;dorsiflexion motion;electrode array configuration method;foot drop;electrical stimulation;frequency 1 Hz","Adult;Electric Stimulation;Electrodes;Equipment Design;Female;Humans;Male;Middle Aged;Monitoring, Ambulatory;Muscle, Skeletal;Skin;Software;Young Adult","","12","","","","","","IEEE","IEEE Conferences"
"The design and implementation of vehicle electrical switch box based on CAN bus","Ci Yanke; Ye Xinzhou","College of Information Engineering, Ningbo Dahongying University, China; Department of Application, Xiamen Linktron Microelectronics Co., Ltd., China","2013 8th International Conference on Computer Science & Education","","2013","","","247","250","In this paper a new design for the low cost but high expansibility with practical multi-function vehicle electrical switch box is proposed, which uses the microchip company's PIC18F45K80 as the core. The system components and the overall program are described, focusing on the design and implementation of the software modules. How to combination the CAN bus technology and SAE_J1939 protocol in the vehicle electrical switch box and how to optimization design for harsh environments are both detailed discussed in the paper. Through the joint modulation of hardware and software, the automobile starting and operation parameters, such as fuse state, system voltage, temperature and starting current are real-time monitored. The actual test and operation show that the system has good reliable guarantee for the road safety. At the same time, the vehicle electrical switch box also can effectively prevent the engine damages resulting from the auto circuit fault.","","978-1-4673-4463-0978-1-4673-4464-7978-1-4673-4462","10.1109/ICCSE.2013.6553918","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553918","vehicle electrical switch box;CAN-bus;SAE_J1939 protocol","Engines;Reliability;Computers;Optical switches;Real-time systems;Monitoring","automotive electronics;controller area networks;road safety","vehicle electrical switch box;CAN bus;microchip;PIC18F45K80;software modules;SAE_J1939 protocol;real-time monitoring;road safety;circuit fault","","","10","","","","","","IEEE","IEEE Conferences"
"Low cost Multi-band planar antenna for mobile phone applications","A. Al-Koukh; J. Zbitou; L. El Abdellaoui; N. Ababssi; A. Tribak; A. Tajmouati; M. Latrach","LITEN Labratory, FPK/FST of Settat Hassan 1st University - Morocco; LITEN Labratory, FPK/FST of Settat Hassan 1st University - Morocco; LITEN Labratory, FPK/FST of Settat Hassan 1st University - Morocco; LITEN Labratory, FPK/FST of Settat Hassan 1st University - Morocco; Microwave group, INPT- Rabat - Morocco; LITEN Labratory, FPK/FST of Settat Hassan 1st University - Morocco; RF &amp; Microwave group ESEO France","2014 International Conference on Multimedia Computing and Systems (ICMCS)","","2014","","","1409","1411","In this paper a novel Multi-band planar antenna structure is presented for GSM 900, DCS 1800, 3G (UMTS 2000) and 4G (LTE Advanced) applications. The antenna is optimized and simulated by using CST Microwave studio, 3-dimentional full wave electromagnetic simulation software. This antenna structure has a partial ground that was optimized into simulation. After the achievement, we have tested and validated this multi-band antenna by using R&amp;S VNA.","","978-1-4799-3824-7978-1-4799-3823","10.1109/ICMCS.2014.6911162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6911162","Microstrip antennas;PIFA;Multi-band antennas;R&S VNA","Microstrip antennas;Mobile handsets;Mobile antennas;Microwave antennas;Wireless communication;Microwave communication","3G mobile communication;4G mobile communication;cellular radio;Long Term Evolution;multifrequency antennas;planar antennas","mobile phone;multi-band planar antenna structure;GSM 900;DCS 1800;3G mobile communication;UMTS 2000;4G mobile communication;LTE Advanced applications;CST Microwave studio;3D full wave electromagnetic simulation software","","","17","","","","","","IEEE","IEEE Conferences"
"Development of UWB planar dipole for near surface applications","Y. S. Maksimovitch; V. A. Badeev; K. P. Gaikovich; Y. V. Petrukhin","Institute of Applied Physics National Academy of Sciences of Belarus, Minsk, Belarus; Institute of Applied Physics National Academy of Sciences of Belarus, Minsk, Belarus; Institute for Physics of Microstructures RAS, Nizhny Novgorod, Russia; Research Institute of Modern Telecommunication Technologies, Smolensk, Russia","2013 IX Internatioal Conference on Antenna Theory and Techniques","","2013","","","329","331","In the paper the results of numerical and experimental studies of ultra-wideband dipole radiator and optimized CPW-to-CPS transformer (balun) are presented. Such dipole is used in GPR scanning system based on the Agilent E5071B VNA, and is located in direct proximity to object surface. Different theoretical models were developed to optimize and analyze the 50 to 100 Ohms balun with passband 1-7 GHz. The special attention was paid to matching antennas open with properties of the media under test. All numerical results were performed using CST Microwave Studio software.","","978-1-4799-2897-2978-1-4799-2896-5978-1-4799-2895","10.1109/ICATT.2013.6650767","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6650767","wideband;coplanar waveguide;coplanar stripline;transformer;balun;antenna;dipole;antenna radiation pattern;gain;side lobe","Dipole antennas;Impedance matching;Coplanar waveguides;Microwave antennas;Wideband;Radar antennas;Aperture antennas","dipole antennas;planar antennas;UHF resonators;ultra wideband antennas","UWB planar dipole development;near surface applications;ultra wideband dipole radiator;CPW-to-CPS transformer;GPR scanning system;object surface;antenna matching;CST microwave studio software","","1","7","","","","","","IEEE","IEEE Conferences"
"PPPS-2013: Pulsed electric field assisted treatment of microorganisms for lysis","S. Qin; I. V. Timoshkin; M. P. Wilson; S. J. MacGregor; M. J. Given; M. Maclean; J. G. Anderson; T. Wang","University of Strathclyde, Department of EEE, 204 George Street, Glasgow, G1 1XW, UK; University of Strathclyde, Department of EEE, 204 George Street, Glasgow, G1 1XW, UK; University of Strathclyde, Department of EEE, 204 George Street, Glasgow, G1 1XW, UK; University of Strathclyde, Department of EEE, 204 George Street, Glasgow, G1 1XW, UK; University of Strathclyde, Department of EEE, 204 George Street, Glasgow, G1 1XW, UK; University of Strathclyde, Department of EEE, 204 George Street, Glasgow, G1 1XW, UK; University of Strathclyde, Department of EEE, 204 George Street, Glasgow, G1 1XW, UK; University of Strathclyde, Department of EEE, 204 George Street, Glasgow, G1 1XW, UK","2013 Abstracts IEEE International Conference on Plasma Science (ICOPS)","","2013","","","1","1","High voltage impulses and plasma impulses, with durations in the range of a few tens of nanoseconds to a few hundred microseconds, can be used to generate electromechanical stresses in biological membranes, in order to facilitate cell lysis. Modelling and analytical evaluations of the PEF treatment process are introduced in this paper. Simulation of the transient electrical processes in a cell, located in liquid between parallel-plane electrodes, is conducted, based on the theory developed in [1]. The effect of the electrical parameters of the external medium are investigated and discussed. The static electric field is analyzed using QuickField electrostatic software for the same model. Two test cells, one with parallelplane electrodes and one with parallel-plane dielectric electrodes, are designed and built. PEF treatment of microalgae is conducted using both test cells, and the results are presented and discussed. The paper also discusses the effects which should be considered when optimising the pulsed electric field treatment parameters, including the pulse waveforms (duration and magnitude), and the topology of the treatment cell.","0730-9244","978-1-4673-5171","10.1109/PLASMA.2013.6633410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6633410","","Electric fields;Electrodes;Biomembranes;Biological system modeling;Analytical models;Transient analysis;Physics","bioelectric phenomena;biomedical electrodes;biomembranes;cellular biophysics;electric field effects;electrostatics;microorganisms;patient treatment","pulsed electric field assisted treatment;microorganisms;electromechanical stresses;biological membranes;cell lysis;high-voltage impulses;plasma impulses;PEF treatment;parallel-plane electrodes;static electric field;QuickField electrostatic software;microalgae","","","1","","","","","","IEEE","IEEE Conferences"
"Microwave solid state power amplifier technology","M. Kasal","Dept. of Radio Electronics, Brno University of Technology, Purkynova 118, 612 00, Czech Republic","2013 Conference on Microwave Techniques (COMITE)","","2013","","","173","176","This paper is focused on our current research in the field of power amplifier technology for microwaves, especially the X-band. For this purpose we considered different active devices. Unfortunately, we did not manage to obtain super modern GaN transistors (chips) for these frequencies such as TGA2023-05 from TriQuint or module such as TGA-2554-GSG from the same producer. Instead we used internally matched GaAs FETs from Eudyna as well as GaAs module XP-1006A from Mimix. Subsequently we have designed, built and tested three power amplifiers with different active devices and output power of 4, 8 and 20 Watts on 10 GHz. In the paper we would like to introduce design and completion of the amplifiers as well as test results. The highest power we achieved by double stage with 90 degrees 3 dB hybrid couplers at the input and output. The microstrip technique on PTFE substrate has been used. Critical parts of layouts were optimized by using ANSYS modeling software. The dc circuitry needs to be designed according to the proper time sequence and high dc currents. Corresponding cooling system was taken into account.","","978-1-4673-5515-5978-1-4673-5512-4978-1-4673-5514","10.1109/COMITE.2013.6545064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6545064","microwave amplifier;solid state power amplifier;linear power amplifier","Power amplifiers;Microwave amplifiers;Microwave communication;Gain;Electromagnetic heating;Microwave circuits","gallium arsenide;III-V semiconductors;microstrip couplers;microwave field effect transistors;microwave power amplifiers;wide band gap semiconductors","microwave solid state power amplifier technology;X-band;active devices;TGA2023-05;transistors;TGA-2554-GSG;internally matched FET;XP-1006A module;hybrid couplers;microstrip technique;PTFE substrate;ANSYS modeling software;high dc currents;time sequence;power 4 W;power 8 W;power 20 W;frequency 10 GHz;GaN;GaAs","","","7","","","","","","IEEE","IEEE Conferences"
"Zynq 7000 series FPGA based Efficient DTMF detection","S. N. Bhavanam; P. Siddaiah; P. R. Reddy","JNTUA, Ananthapuramu, Andhra Pradesh, India; University College of Engineering &amp; Technology, Acharya Nagarjuna University, Guntur, India; JNTUA, Ananthapuramu, Andhra Pradesh, India","2014 IEEE International Conference on Computational Intelligence and Computing Research","","2014","","","1","7","Dual Tone Multi Frequency signal detection is an important to developing for telecommunication equipment. ""This is a standard where keystrokes from the telephone keypad are translated into dual tone signals over the audio link"". FPGA has gains very popularity in recent years due to their reprogram ability and flexibility. This paper presents a new type of ZYBO Board ZYNQ 7000 series FPGA based efficient DTMF detection. FFT based technique is, one of the approaches for DTMF detection but which requires more hardware and also power consuming type. Split Goertzel algorithm itself is an area optimized solution comparison with FFT based technique, but in this project we initiate efforts to further makes it low power by building lot of serialism in the design. Since DTMF based applications don't require high speed tone detection the resource sharing approach can be used. In this approach very minimal set of hardware is scheduled as inputs and outputs at appropriate clock edges, for implementing the algorithm. This paper divided into three phases. In the first phase, FFT based DTMF detection is done using Xilinx FFT core. The area, timing and power results are analyzed. In the second phase the split Goertzel algorithm is implemented and analysis is carried out. In the third phase, the resource sharing approach is studied and suitable state machine based scheduling will be carried with limited resources to implement split Goertzel algorithm. It will be demonstrated that the novel resource sharing based approach consumes less power and can still efficiently detect the DTMF tones. To test the project at various stage DTMF tone generator module also will be implemented with digital carrier generators. Mentor Graphics Modelsim Software is used for functional verification and Xilinx ISE is used for simulation and synthesis respectively. The Xilinx Chipscope is used to test the FPGA inside results while the logic is running on FPGA. The ZYBO board ZYNQ 7000 series FPGA Family board is used in this paper.","","978-1-4799-3975-6978-1-4799-3974","10.1109/ICCIC.2014.7238514","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7238514","DTMT;FFT;Split Goertzel Algorithm;Resource Sharing Approach;Xilinx ISE;Modelsim;ZYBO board;ZYNQ 7000 series FPGA","Resource management;Field programmable gate arrays;Simulation;Algorithm design and analysis;Hardware;Scheduling;Generators","fast Fourier transforms;field programmable gate arrays;finite state machines;scheduling;signal detection","dual tone multifrequency signal detection;DTMF detection;telecommunication equipment;telephone keypad;audio link;ZYBO board;field programmable gate arrays;ZYNQ 7000 series FPGA;FFT based technique;split Goertzel algorithm;resource sharing approach;fast Fourier transforms;Xilinx FFT core;state machine based scheduling;DTMF tone generator module;digital carrier generators;Mentor Graphics Modelsim Software;Xilinx ISE;Xilinx Chipscope","","1","19","","","","","","IEEE","IEEE Conferences"
"Modeling and Solution of the Large-Scale Security-Constrained Unit Commitment","Y. Fu; Z. Li; L. Wu","ECE Department, Mississippi State University, MS, USA; ECE Department, Illinois Institute of Technology, Chicago, IL, USA; ECE Department Department, Clarkson University, Potsdam, NY, USA","IEEE Transactions on Power Systems","","2013","28","4","3524","3533","Security-constrained unit commitment (SCUC), as one of key components in power system operation, is being widely applied in vertically integrated utilities and restructured power systems. The efficient solution framework is to implement iterations between a master problem (unit commitment) and subproblems (network security evaluations). In industrial applications, both Lagrangian relaxation and mixed-integer programming are commonly applied for the unit commitment problem, and both linear sensitivity factor and Benders cut methods are used to generate additional constraints in the phase of network security evaluations. This paper evaluates capabilities and performances of each algorithm through technical discussion and numerical testing. Special topics on the large-scale SCUC engine development are also discussed in this paper, such as input data screening, inactive constrains elimination, contingency management, infeasibility handling, parallel computing, and model simplification. This paper will benefit academic researchers, software developers, and system operators when they design, develop and assess effective models and algorithms for solving large-scale SCUC problems.","0885-8950;1558-0679","","10.1109/TPWRS.2013.2272518","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6570740","Benders decomposition;Lagrangian relaxation;linear sensitivity factors;mixed-integer programming;security-constrained unit commitment (SCUC)","Power system management;Security;Optimization;Power system modeling;Lagrangian functions;Mixed integer linear programming","integer programming;iterative methods;power system economics;power system security;sensitivity analysis","large-scale security-constrained unit commitment;large-scale SCUC problems;power system operation;vertically integrated utilities;restructured power systems;master problem;subproblems;network security evaluations;Lagrangian relaxation;mixed-integer programming;linear sensitivity factor;Benders cut methods;technical discussion;numerical testing;input data screening;inactive constrains elimination;contingency management;infeasibility handling;parallel computing;model simplification;economic scheduling","","41","35","","","","","","IEEE","IEEE Journals & Magazines"
"Design of embedded video surveillance system based on quantum cryptography","Zhang Hongtao; Hu Shunxing; Xu Hui; Tu Lingying","School of electrical and electronic engineering of Hubei University of Technology in Nano Electronics and Micro System Laboratory, Wuhan 430068, China; School of electrical and electronic engineering of Hubei University of Technology in Nano Electronics and Micro System Laboratory, Wuhan 430068, China; School of electrical and electronic engineering of Hubei University of Technology in Nano Electronics and Micro System Laboratory, Wuhan 430068, China; School of electrical and electronic engineering of Hubei University of Technology in Nano Electronics and Micro System Laboratory, Wuhan 430068, China","2014 IEEE Workshop on Advanced Research and Technology in Industry Applications (WARTIA)","","2014","","","914","918","The design of S3C6410-based on the hardware platform and Linux operating systems as the software platform for video surveillance system was illustrated, which captured video data through an external USB camera, H.264 compression algorithm was used to improve the coding efficiency and AVI video format to storage the video. A quantum cryptography communication system based on the technology of single photon is embedded in the video information transmission link from ARM developmental board to computer. The test results showed that the collecting end and transmitting process of surveillance system have achieved the design goal, the system is stable and reliable.","","978-1-4799-6989-0978-1-4799-6988","10.1109/WARTIA.2014.6976422","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976422","Quantum cryptography;S3C6410;Video surveillance system;x.264;RDO","Photonics;Encoding;Linux;Optimization;Communication systems;Monitoring;Quantum mechanics","cameras;data compression;image coding;Linux;microprocessor chips;peripheral interfaces;quantum cryptography;video communication;video surveillance","embedded video surveillance system;S3C6410-based design;Linux operating system;hardware platform;software platform;external USB camera;H.264 compression algorithm;coding efficiency;AVI video format;video storage;quantum cryptography communication system;single photon technology;embedded video information transmission link;ARM developmental board","","","10","","","","","","IEEE","IEEE Conferences"
"Optimal Distributed Generation placement in distribution network","M. Vukobratović; Ž. Hederić; M. Hadžiselimović","Department of Power System, J.J. Strossmayer University of Osijek, Faculty of Electrical Engineering, Kneza Trpimira 2b, 31 000, Croatia; Department of Electromechanical Engineering, J.J. Strossmayer University of Osijek, Faculty of Electrical Engineering, Kneza Trpimira 2b, 31 000, Croatia; Applied Electrical Engineering Laboratory, University of Maribor, Faculty of Energy Technology, Hoćevarjev trg 1, SI-8270 Krško, Slovenia","2014 IEEE International Energy Conference (ENERGYCON)","","2014","","","1176","1183","This paper presents a method for optimal Distributed Generation placement with goal of reducing active power system losses and voltage level regulation. Active power losses in radial distribution network are determined using an Artificial Neural Network (ANN) by simultaneous formulation for the determination process based on voltage level control and injected power. Adequate installed power of distributed generation and the appropriate terminal for distributed generation utilization are selected by means of a genetic algorithm (GA), performed in a distinct manner that fits the type of decision-making assignment. The training data for ANN is obtained by means of load flow simulation performed in DIgSILENT PowerFactory software on a part of the Croatian distribution network. The active power losses and voltage conditions are simulated for various operation scenarios in which the back propagation ANN model has been tested to predict the power losses and voltage levels for each system terminal, and GA is used to determine the optimal terminal for distributed generation placement.","","978-1-4799-2449","10.1109/ENERGYCON.2014.6850572","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850572","Distributed generation;Artificial Neural Networks;Genetic Algorithm;Voltage control;Power losses reduction","Artificial neural networks;Training;Distributed power generation;Optimization;Sociology;Statistics;Power systems","backpropagation;decision making;distributed power generation;genetic algorithms;neural nets;power distribution control;power engineering computing;power generation control;voltage control","optimal distributed generation placement;active power system loss reduction;voltage level regulation;radial distribution network;artificial neural network;ANN training data;determination process;voltage level control;injected power;genetic algorithm;GA;decision-making assignment;DIgSILENT PowerFactory software;load flow simulation;Croatian distribution network;voltage conditions;back propagation ANN model;system terminal","","1","20","","","","","","IEEE","IEEE Conferences"
"A riccati based homogeneous and self-dual interior-point method for linear economic model predictive control","L. E. Sokoler; G. Frison; K. Edlund; A. Skajaa; J. B. Jørgensen","DONG Energy, DK-2820 Gentofte, Denmark; Department of Informatics and Mathematical Modeling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Denmark; DONG Energy, DK-2820 Gentofte, Denmark; Department of Informatics and Mathematical Modeling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Denmark; Department of Informatics and Mathematical Modeling, Technical University of Denmark, DK-2800 Kgs. Lyngby, Denmark","2013 IEEE International Conference on Control Applications (CCA)","","2013","","","592","598","In this paper, we develop an efficient interior-point method (IPM) for the linear programs arising in economic model predictive control of linear systems. The novelty of our algorithm is that it combines a homogeneous and self-dual model, and a specialized Riccati iteration procedure. We test the algorithm in a conceptual study of power systems management. Simulations show that in comparison to state of the art software implementation of IPMs, our method is significantly faster and scales in a favourable way.","1085-1992","978-1-4799-1559","10.1109/CCA.2013.6662814","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662814","","Economics;Generators;Vectors;Optimization;Production;Linear systems","control system analysis computing;iterative methods;linear systems;predictive control","Riccati based homogeneous method;self-dual interior-point method;linear economic model predictive control;IPM;linear programs;economic model predictive control;linear systems;specialized Riccati iteration procedure;software implementation","","4","26","","","","","","IEEE","IEEE Conferences"
"Energy-based state-feedback control of systems with mechanical or virtual springs","I. Mikhailova","Simulation, Systems Optimization and Robotics Group, Technische Universität Darmstadt, Germany","2013 IEEE International Conference on Robotics and Automation","","2013","","","2509","2514","In the last years the classical optimal control of rigid structures gives way to alternatives both in hardware (elastic structures) as in software (non-linear control). In this work we consider both alternatives together. We test the possibility to apply speed-gradient (SG) control [1] to elastic structures. The SG method has many advantages, e.g. exploitation of natural dynamics of the system and mathematically provable criteria of goal achievement. However the cost function that satisfies the requirements of the SG method may be difficult to find. In this work we propose two approaches to this problem: usage of virtual springs and usage of learning methods based on Slow Feature Analysis (SFA). A classical example of a cart-pole system and an example of a system which uses two serial springs for hopping show in simulation the viability of our approach. Proposed here combination of SG control with learning is a novel approach which opens interesting perspectives for further research on passive control.","1050-4729","978-1-4673-5643-5978-1-4673-5641","10.1109/ICRA.2013.6630919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6630919","","Cost function;Springs;Damping;Hardware;Trajectory;Friction","elasticity;learning (artificial intelligence);nonlinear control systems;springs (mechanical);state feedback","energy-based state-feedback control;mechanical springs;speed-gradient control;elastic structures;cost function;virtual springs;learning methods;slow feature analysis;cart-pole system;serial springs;hopping;SG control;SFA;nonlinear control","","2","11","","","","","","IEEE","IEEE Conferences"
"Time optimal control of ground vehicles","G. Max; B. Lantos","Budapest University of Technology and Economics, Department of Control Engineering and Information Technology H-1117 Budapest, Magyar tudósok krt. 2, Hungary; Széchenyi István University, Research Centre of Vehicle Industry, H-9026 Győr, Egyetem tér 1, Hungary","2014 IEEE 12th International Symposium on Intelligent Systems and Informatics (SISY)","","2014","","","245","250","The paper deals with the time optimal control of automatically driven cars modeled with gear shift as discrete control input beside the continuous ones in a test path between corridors. The car is required to avoid a static obstacle or performing double lane change. The problem can be formulated as a mixed-integer optimal control problem (MIOCP). The resulting MIOCP is solved by reformulating it to static mixed-integer nonlinear program (MINLP) using time discretization and direct multiple shooting method. Non-commercial open software packages are applied that substantially use the gradients of the objective function and the Jacobians of the constraints exploiting sparsity. A novel algorithm and implementation is presented for computing the derivatives of the complex state trajectory joining equations. This algorithm was given in the form of matrix differential equations whose structure allowed to compute their solution using RK4 in matrix form. The elaborated method can be applied both for combustion engine and electric driven cars. It can form the basis to generate an offline database of a central general collision avoidance system (CAS) for varying path parameters on a grid which can support real time applications.","1949-047X;1949-0488","978-1-4799-5996","10.1109/SISY.2014.6923594","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923594","Time optimal control;Vehicle model;Direct multiple shooting;Mixed integer nonlinear programming;Sparse gradient and Jacobian","Optimization;Optimal control;Jacobian matrices;Wheels;Gears;Trajectory;Force","automobiles;collision avoidance;differential equations;integer programming;Jacobian matrices;linear programming;road traffic control;time optimal control","time optimal control;ground vehicle control;automatically driven cars;gear shift;discrete control input;static obstacle avoidance;double lane change;mixed-integer optimal control problem;MIOCP;mixed-integer nonlinear program;time discretization;direct multiple shooting method;MINLP;noncommercial open software packages;objective function;constraints Jacobians;complex state trajectory joining equations;matrix differential equations;RK4;combustion engine;electric driven cars","","","18","","","","","","IEEE","IEEE Conferences"
"Design and evaluation of a next generation residential energy management system","T. Clemmer; H. Xu; R. Dougal; H. A. Mantooth","Grid Connected Advanced Power Electronics Systems (GRAPES) University of Arkansas, Fayetteville, AR 72701, U.S.A.; Grid Connected Advanced Power Electronics Systems (GRAPES) University of South Carolina, Columbia, SC 29208, U.S.A; Grid Connected Advanced Power Electronics Systems (GRAPES) University of South Carolina, Columbia, SC 29208, U.S.A; Grid Connected Advanced Power Electronics Systems (GRAPES) University of Arkansas, Fayetteville, AR 72701, U.S.A.","2013 4th IEEE International Symposium on Power Electronics for Distributed Generation Systems (PEDG)","","2013","","","1","8","This paper introduces the concept, development, and evaluation of the next generation in demand side energy management. An integrated hardware/software 2 kW prototype has been designed, built, and currently undergoing testing. The Smart Green Power Node (SGPN) features a modular architecture such that it can be customized for user-specific applications. Additionally, the SGPN incorporates necessary data such as weather for projected PV generation, user input, hardware status and load profile information, as well as a TOU pricing schedule as inputs to a novel system control algorithm. Compared to commercial energy management systems that exist, this system effectively eliminates the need for human interaction by processing the described data and intelligently controlling the charging/discharging of onsite resources in conjunction with the electric grid to yield economic savings for the home owner as well as the utility.","2329-5759;2329-5767","978-1-4799-0692","10.1109/PEDG.2013.6785585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6785585","","Batteries;Schedules;Hardware;Optimization;Heuristic algorithms;Level control;Energy management","demand side management;energy management systems;power grids;power system control","projected PV generation;user input;hardware status;load profile information;TOU pricing schedule;system control algorithm;human interaction;electric grid;economic savings;next generation residential energy management system;demand side energy management;integrated hardware/software;smart green power node;modular architecture;user-specific applications;power 2 kW","","5","24","","","","","","IEEE","IEEE Conferences"
"Simulation for a Shingled Magnetic Recording Disk","S. Tan; W. Xi; Z. Y. Ching; C. Jin; C. T. Lim","Data Center Technologies Division, Agency for Science, Technology and Research (A*STAR),, Data Storage Institute,, Singapore; Data Center Technologies Division Agency for Science, Technology and Research (A*STAR), Data Storage Institute, Singapore; Data Center Technologies Division Agency for Science, Technology and Research (A*STAR), Data Storage Institute, Singapore; Data Center Technologies Division Agency for Science, Technology and Research (A*STAR), Data Storage Institute, Singapore; Data Center Technologies Division Agency for Science, Technology and Research (A*STAR), Data Storage Institute, Singapore","IEEE Transactions on Magnetics","","2013","49","6","2677","2681","Shingled magnetic recording (SMR) is a new technology based on partially overlapping, or shingling, adjacent tracks, which is capable of increasing the density of a disk drive up to 10 Tbit/in<sup>2</sup> . To write data on overlapping tracks may erase data being written previously. Therefore, without any data translation layer to redirect updates to different locations to avoid unintended data erasures, the SMR disk cannot be used as a conventional disk to perform unrestricted reads/writes directly. Various data translation/mapping strategies have been proposed to allow the disk to perform unrestricted reads/writes. However, these strategies have tradeoffs in terms of system performance and cost, such as disk space utilization. Nevertheless, system performance varies for different applications and under different workload conditions. To help optimize the design of a data translation layer for a SMR disk, simulation software is required to analyze tradeoffs and to fine tune parameters for data translation/remapping strategies. We have designed and developed a simulation software for SMR disks. This software can analyze both system performance and disk space utilization. The SMR disk simulation uses the shingled disk profiles generated from the conventional disk profiles which have been experimentally validated. Evaluation studies show that the simulation software can accurately simulate SMR disk performance. This software will be a useful platform to design and test the shingle translation layer (STL) for a SMR disk with different disk layouts and data management strategies and garbage collection algorithms.","0018-9464;1941-0069","","10.1109/TMAG.2013.2245872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522275","Data band layout;data management;shingled disk;shingled recording","Layout;Writing;Software;Computational modeling;Magnetic recording;Analytical models;Data models","disc drives;magnetic recording","shingled magnetic recording disk;partially overlapping tracks;shingling tracks;disk drive density;data translation layer;unintended data erasure;SMR disk;unrestricted read-write;data mapping;disk space utilization;shingle translation layer","","5","15","","","","","","IEEE","IEEE Journals & Magazines"
"Error-energy analysis of hardware logarithmic approximation methods for low power applications","A. Klinefelter; J. Ryan; J. Tschanz; B. H. Calhoun","Dept. of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA USA; Intel Labs, Intel Corporation, Hillsboro, OR USA; Intel Labs, Intel Corporation, Hillsboro, OR USA; Dept. of Electrical and Computer Engineering, University of Virginia, Charlottesville, VA USA","2015 IEEE International Symposium on Circuits and Systems (ISCAS)","","2015","","","2361","2364","This paper presents an overview of methods for combinational, base-two logarithmic approximation using Mitchell's algorithm, piecewise-linear/quadratic error compensation schemes, and direct approximation. Optimization methods are used for computing linear segments for each compensation scheme including Hamming weight minimization and pattern recognition of segment slopes for multiplier-less error compensation. A novel, near-zero-average error quadratic compensation scheme is also presented. A test chip was fabricated in a commercial 130nm technology including fifteen base-two logarithm approximations and each was evaluated for its standalone accuracy, measured energy, delay, and area in order to determine the best classes of approximation for low-energy and high-accuracy operation. A system-level evaluation of these methods was performed using a software model of a keyword detection speech pipeline to predict the impact of inaccurate logarithmic approximation on the detection accuracy of keywords across various noise levels.","0271-4302;2158-1525","978-1-4799-8391","10.1109/ISCAS.2015.7169158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7169158","logarithm;approximation;interpolation;speech;keyword detection","Approximation methods;Accuracy;Hardware;Complexity theory;Speech;Computers;Approximation algorithms","error compensation;low-power electronics;piecewise linear techniques;speech processing","error-energy analysis;hardware logarithmic approximation;low power;base-two logarithmic approximation;Mitchell's algorithm;piecewise-linear/quadratic error compensation schemes;direct approximation;Hamming weight minimization;pattern recognition;multiplier-less error compensation;near-zero-average error quadratic compensation scheme;test chip;software model;size 130 nm","","3","12","","","","","","IEEE","IEEE Conferences"
"Cost effective data center servers","R. Hou; T. Jiang; L. Zhang; P. Qi; J. Dong; H. Wang; X. Gu; S. Zhang","Institute of Computing Technology, Chinese Academy of Sciences, USA; Institute of Computing Technology, Chinese Academy of Sciences, USA; Institute of Computing Technology, Chinese Academy of Sciences, USA; Institute of Computing Technology, Chinese Academy of Sciences, USA; Institute of Computing Technology, Chinese Academy of Sciences, USA; Shannon Laboratory, Huawei Technologies Co., Ltd, China; Shannon Laboratory, Huawei Technologies Co., Ltd, China; Shannon Laboratory, Huawei Technologies Co., Ltd, China","2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)","","2013","","","179","187","The exploding growth of digitalized information has led to the rapid growth of data centers, both in numbers and in size. Cluster has been the dominating system architecture used in most data centers. However, the increasingly diversified data center applications have requirements beyond what the cluster architecture can deliver. For instance, clouding computing requires flexible sharing of all data center resources. Big data applications often need large memory capacity. A few applications can use GPGPU effectively. Existing system might be extended to a certain degree to meet those needs. Those extensions however would often be prohibitively expensive. The paper presents our attempt to design a system using commodity products that can meet the varying needs of many emerging data center applications in a cost-effective way. Our attempt is to create a system by connecting multiple nodes through a PCIe switch and then extend the software stack to support resource sharing among these nodes. In particular, a node can directly use the memory, NIC, and GPGPU of other nodes through the PCIe switch with no or little involvement from other nodes. We build a prototype as our evaluation platform. Our evaluation results indicate that those resources can be shared effectively in many cases. For using remote memory as block device, our prototype system has 5 times bandwidth, 11 times IOPS and 1/12 latency compared with the system connected by 10GigE in average for Orion benchmark; Using remote GPGPU via PCIe switch achieves average 60 times speedup than the case without GPGPU, and the performance loss is also acceptable (its average execution time is 1/3 of local GPGPU) for micro-benchmarks from GPU computing SDK; And using remote NIC via PCIe switch achieves average 95% bandwidth and 1.4 times latency of local NIC in httperf testing. While our prototype system offers multiple benefits, it is not perfect and has a lot room for further optimization and extension. We hope the outcome presented in this paper will encourage more researchers to join us in designing highly efficient and cost-effective servers.","1530-0897;1530-0897","978-1-4673-5587-2978-1-4673-5585-8978-1-4673-5586","10.1109/HPCA.2013.6522317","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6522317","","Switches;Prototypes;Bandwidth;Bridges;Servers;Resource management;Engines","computer centres;graphics processing units;pattern clustering","httperf testing;local NIC;microbenchmarks;Orion benchmark;IOPS;prototype system;block device;remote memory;resource sharing;software stack;PCIe switch;multiple nodes;commodity products;GPGPU;memory capacity;data center resources;clouding computing;cluster architecture;digitalized information;cost effective data center servers","","","25","","","","","","IEEE","IEEE Conferences"
"Large area interposer lithography","W. Flack; R. Hsieh; G. Kenyon; M. Ranjan; J. Slabbekoorn; A. Miller; E. Beyne; M. Toukhy; P. Lu; Y. Cao; C. Chen","Ultratech, Inc 3050 Zanker Road, San Jose. CA. 95124; Ultratech, Inc 3050 Zanker Road, San Jose. CA. 95124; Ultratech, Inc 3050 Zanker Road, San Jose. CA. 95124; Ultratech, Inc 3050 Zanker Road, San Jose. CA. 95124; IMEC, Kapeldreef 75 B-3001 Leuven, Belgium; IMEC, Kapeldreef 75 B-3001 Leuven, Belgium; IMEC, Kapeldreef 75 B-3001 Leuven, Belgium; AZ Electronic Materials USA Corp., 70 Meister Ave, Somerville, NJ 08876 USA; AZ Electronic Materials USA Corp., 70 Meister Ave, Somerville, NJ 08876 USA; AZ Electronic Materials USA Corp., 70 Meister Ave, Somerville, NJ 08876 USA; AZ Electronic Materials USA Corp., 70 Meister Ave, Somerville, NJ 08876 USA","2014 IEEE 64th Electronic Components and Technology Conference (ECTC)","","2014","","","26","32","Large area silicon or glass interposers may exceed the maximum imaging field of step and repeat lithography tools. This paper discusses the lithographic process used to create a large area interposer on a stepper by the combination of multiple subfield exposures. Overlay metrology structures are used to confirm the relative placement of the subfields to construct the interposer. Routing lines from 1.5 to 4.0 μm in width are evaluated to measure critical dimension (CD) control where the lines cross the subfield boundaries. CD metrology at the bottom and top of the photoresist is performed using a top down CD-SEM. Finally large area test interposers are patterned using two subfields on a 1X stepper and processed through a Cu electroplating module for detailed characterization. The CD control of routing lines as they cross the subfield boundary can be optimized by using a shaped or tapered line end design. Lithography simulation using Prolith modeling software by KLA-Tencor is matched to experimental results and then used to evaluate performance of various line end designs. Larger latitude for overlap error was observed for the tapered line end compared to the standard square line end. The experimental and modeled results in this study show the capability of using stepper lithography to produce large area interposers with 1.5 μm I/O routing line dimensions.","0569-5503;2377-5726","978-1-4799-2407","10.1109/ECTC.2014.6897262","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6897262","","Resists;Lithography;Routing;Silicon;Metals;Metrology;Layout","copper alloys;electroplating;photolithography;photoresists;scanning electron microscopy","large area interposer lithography;large area silicon;glass interposers;maximum imaging field;repeat lithography tools;step lithography tools;multiple subfield exposures;overlay metrology structures;critical dimension control measurement;CD control;routing lines;photoresist;top down CD-SEM;large area test interposers;1X stepper;copper electroplating module;subfield boundary;tapered line end design;lithography simulation;Prolith modeling software;KLA-Tencor;overlap error;standard square line end;stepper lithography;I/O routing line dimensions;size 1.5 mum to 4.0 mum;Cu","","1","8","","","","","","IEEE","IEEE Conferences"
"A novel quadratic formulation for customer order scheduling problem","L. Wang; Z. Shi; L. Shi","Department of Industrial Engineering and Management at Peking University, Beijing, China; Department of Industrial Engineering and Management at Peking University, Beijing, China; Department of Industrial Engineering and Management at Peking University, Beijing, China","2013 IEEE International Conference on Automation Science and Engineering (CASE)","","2013","","","576","580","In this paper, we consider a customer order scheduling problem, in which there are n orders from n different customers, and each customer order consists of one or more jobs of different types, and each kind of jobs can be only processed on one machine. The objective is to minimize the total weighted completion time of the orders. We propose a novel quadratic formulation for this problem, then we transform the quadratic model into an equivalent mixed-integer linear programming model through using the standard linearization technique. Also, by exploiting the special structure of this problem, some variables and constrains are eliminated to reduce the problem size. The final linearized model can be solved by commercial software directly, and some computational experiments are designed and conducted to test the effectiveness and efficiency of the model.","2161-8070;2161-8089","978-1-4799-1515","10.1109/CoASE.2013.6654049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6654049","","Job shop scheduling;Computational modeling;Optimized production technology;Quality of service;Mixed integer linear programming;Linear programming;Optimal scheduling","computational complexity;customer satisfaction;integer programming;linear programming;minimisation;quadratic programming;scheduling","quadratic formulation;customer order scheduling problem;minimization;total weighted completion time;mixed-integer linear programming model;standard linearization technique","","1","9","","","","","","IEEE","IEEE Conferences"
"Optimal Protection Coordination for Microgrids With Grid-Connected and Islanded Capability","W. K. A. Najy; H. H. Zeineldin; W. L. Woon","Masdar Institute of Science and Technology, Masdar City, UAE; Masdar Institute of Science and Technology, Masdar City, UAE; Masdar Institute of Science and Technology, Masdar City, UAE","IEEE Transactions on Industrial Electronics","","2013","60","4","1668","1677","Microgrids can be operated either grid-connected to reduce system losses and for peak shaving or islanded to increase reliability and provide backup power during utility outage. Such dual configuration capability imposes challenges on the design of the protection system. Fault current magnitudes will vary depending on the microgrid operating mode. In this paper, a microgrid protection scheme that relies on optimally sizing fault current limiters and optimally setting directional overcurrent relays is proposed. The protection scheme is optimally designed taking into account both modes of operation (grid-connected and islanded). The problem has been formulated as a constrained nonlinear programming problem and is solved using the genetic algorithm with the static penalty constraint-handling technique. The proposed approach is tested on two medium-voltage networks: a typical radial distribution system and on the IEEE 30-bus looped power distribution system equipped with directly connected conventional synchronous generators.","0278-0046;1557-9948","","10.1109/TIE.2012.2192893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6177244","Directional overcurrent relay coordination;distributed generation (DG);fault current limiters (FCLs);short-circuit analysis","Relays;Biological cells;Genetic algorithms;Circuit faults;Optimization;Fault location;Optical character recognition software","distributed power generation;fault currents;nonlinear programming;power distribution protection;synchronous generators","optimal protection coordination;microgrids;grid-connected capability;islanded capability;reduce system losses;utility outage;fault current;protection system;constrained nonlinear programming problem;genetic algorithm;static penalty constraint-handling technique;medium-voltage networks;radial distribution system;IEEE 30-bus looped power distribution system;synchronous generators","","129","30","","","","","","IEEE","IEEE Journals & Magazines"
"Development of driver assistance systems using virtual hardware-in-the-loop","P. Wehner; F. Schwiegelshohn; D. Göhringer; M. Hübner","Ruhr-University Bochum, Germany; Ruhr-University Bochum, Germany; Ruhr-University Bochum, Germany; Ruhr-University Bochum, Germany","2014 International Symposium on Integrated Circuits (ISIC)","","2014","","","380","383","In today's cars, more than 50 electronic control units are used to provide safety and to care about the occupants comfort. The development of advanced driver assistance systems is a key role in the automotive domain. It is essential to validate and verify results and to ensure faultless interoperability of the embedded systems. Not uncommonly, the dimensioning of parameters affects safety aspects and changes need to pass expensive test drives. By increasing the density of integration, future control units will be able to handle more functionality associated with less space requirements. Highly integrated systems will open up new possibilities, but challenges, such as security aspects in the final approval process, need to be faced. In example of advanced driver assistance systems, we introduce a methodology, based on the classical `V'-diagram, for the development in an overall virtual environment. By combining rapid prototyping technologies with commercials off-the-shelf, a safe but realistic instrument is given. The concept is shown with the implementation of cruise-control systems and a drowsiness detector.","2325-0631","978-1-4799-4833","10.1109/ISICIR.2014.7029542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029542","Virtual Hardware-in-the-Loop;Driver Assistant Systems-on-Chip;Virtual Platform;‘V’-diagram;FPGA","Hardware;Vehicles;Sensors;Virtual environments;Face;Software;Optimization","digital simulation;driver information systems;embedded systems;open systems;road safety;virtual reality","advanced driver assistance system;ADAS;virtual hardware-in-the-loop;HIL;embedded system interoperability;safety aspect","","3","8","","","","","","IEEE","IEEE Conferences"
"Overview of TUBITAK UZAY's electric propulsion development project (HALE)","D. Uluşen; B. Ç. Aydın; İ. S. Gülle; E. Bozkurt; H. Özkaya; S. Ontaç; M. Türkmenoğlu","TÜBİTAK Space Technologies Research Institute, Ankara, Turkey; TÜBİTAK Space Technologies Research Institute, Ankara, Turkey; TÜBİTAK Space Technologies Research Institute, Ankara, Turkey; TÜBİTAK Space Technologies Research Institute, Ankara, Turkey; TÜBİTAK Space Technologies Research Institute, Ankara, Turkey; TÜBİTAK Space Technologies Research Institute, Ankara, Turkey; TÜBİTAK Space Technologies Research Institute, Ankara, Turkey","2013 6th International Conference on Recent Advances in Space Technologies (RAST)","","2013","","","643","647","In spacecraft technologies electric propulsion is a well-suited option to meet the in-space propulsion requirements of both near-Earth and deep space missions, promising more affordable and sustainable solutions. Savings in propellant mass and launch costs, and longer lifetime and higher specific impulse make electric propulsion systems more attractive than the chemical counterparts for commercial telecommunication and Earth observation spacecrafts. Considering the advantages of this critical technology, by the support of Turkish Ministry of Development, TUBITAK-UZAY started a project (Hall Effect thruster development project-HALE) aiming at developing electric propulsion systems to meet the in-space propulsion needs of future national missions in the short and long term. The main objective of the project is to build a test facility for experimental studies and lay the necessary technical foundation to initiate the development of electric propulsion systems, focusing on Hall Effect thrusters. This paper will present the state-of-art electric propulsion technology and summarize the current activities and future plans of TUBITAK-UZAY in this field.","","978-1-4673-6396-9978-1-4673-6395-2978-1-4673-6394","10.1109/RAST.2013.6581289","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6581289","Electric propulsion;HALE;Hall effect thruster;TUBITAK UZAY;spacecraft propulsion","Magnetic field measurement;Propulsion;Optimization;Software","aerospace propulsion;space vehicles","TUBITAK UZAY electric propulsion development project;spacecraft technologies;in-space propulsion requirements;near-Earth missions;deep space missions;propellant mass costs;launch costs;commercial telecommunication;chemical counterparts;Earth observation spacecrafts;Hall Effect thrusters","","","13","","","","","","IEEE","IEEE Conferences"
"Topology-aware equipartitioning with coscheduling on multicore systems","J. H. Schönherr; B. Juurlink; J. Richling","Communication and Operating Systems, Technische Universität Berlin, Germany; Embedded Systems Architecture, Technische Universität Berlin, Germany; Communication and Operating Systems. Technische Universität Berlin, Germany","2013 IEEE 6th International Workshop on Multi-/Many-core Computing Systems (MuCoCoS)","","2013","","","1","8","Over the last decade, multicore architectures have become omnipresent. Today, they are used in the whole product range from server systems to handheld computers. The deployed software still undergoes the slow transition from sequential to parallel. This transition, however, is gaining more and more momentum due to the increased availability of more sophisticated parallel programming environments, which replace the some-times crude results of ad-hoc parallelization. Combined with the ever increasing complexity of multicore architectures, this results in a scheduling problem that is different from what it has been, because features such as non-uniform memory access, shared caches, or simultaneous multithreading have to be considered. In this paper, we compare different ways of scheduling multiple parallel applications. Due to emerging parallel programming environments, we only consider malleable applications, i. e., applications where the parallelism degree can be changed on the fly. We propose a topology-aware scheduling scheme that combines equipartitioning and coscheduling. It does not suffer from the drawbacks of the individual concepts and also allows to run applications at different degrees of parallelisms without compromising fairness. We find that topology-awareness increases performance for all evaluated workloads. The combination with coscheduling is more sensitive towards the executed workloads. However, the gained versatility allows new use cases to be explored, which were not possible before.","","978-1-4799-1010","10.1109/MuCoCoS.2013.6633602","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6633602","","Benchmark testing;Multicore processing;Operating systems;Topology;Linux;Parallel processing;Optimization","multiprocessing systems;parallel programming;scheduling","multicore systems;multiple parallel applications;malleable applications;topology-aware scheduling scheme;equipartitioning;coscheduling;multicore architectures","","1","13","","","","","","IEEE","IEEE Conferences"
"Enhancement of the stability and the transient response of inverter based DG unit in isolated micro-grids","A. H. A. Razek; A. M. Abdin; H. S. K. El-Gohary","Electrical Power and Machines Department, Faculty of Engineering, Ain Shams University, Cairo, Egypt; Electrical Power and Machines Department, Faculty of Engineering, Ain Shams University, Cairo, Egypt; Electrical Power and Machines Department, Faculty of Engineering, Ain Shams University, Cairo, Egypt","2015 IEEE Innovative Smart Grid Technologies - Asia (ISGT ASIA)","","2015","","","1","6","This paper introduces a mathematical model and a voltage/frequency control scheme for inverter interfacing Grid Forming DG (VF) unit. The introduced control scheme utilizes the well understood and optimized dq-frame current control scheme that is used with inverter interfacing Grid Following DG (PQ) units. Minimum modifications are made to the control software to allow grid isolation operation of the DG. The introduced control scheme is characterized by fast disturbance and start up transients rejection, and mitigation of dynamic coupling between control loops and load dynamics. Further control enhancements are introduced by adding frequency regulation loop and dynamic stabilizing network. The whole control scheme is tested using a proposed test system on MATLAB/SIMULINK simulation environment.","2378-8542","978-1-5090-1238-1978-1-5090-1237","10.1109/ISGT-Asia.2015.7387028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7387028","Distributed Generation (DG);micro-grid;grid isolation operation;dynamic loads;frequency","Voltage control;Inverters;Frequency control;Current control;Couplings;Mathematical model;Oscillators","distributed power generation;electric current control;frequency control;power system transient stability;voltage control","isolated microgrids;stability response;transient response;mathematical model;voltage-frequency control scheme;inverter interfacing grid forming DG unit;optimized dq-frame current control scheme;grid isolation operation;fast disturbance;start up transients rejection;dynamic coupling mitigation;control loops;load dynamics;frequency regulation loop;dynamic stabilizing network;Matlab-Simulink simulation environment","","","9","","","","","","IEEE","IEEE Conferences"
"Design and Implementation of a Highly Efficient DGEMM for 64-Bit ARMv8 Multi-core Processors","F. Wang; H. Jiang; K. Zuo; X. Su; J. Xue; C. Yang","NA; NA; NA; NA; NA; NA","2015 44th International Conference on Parallel Processing","","2015","","","200","209","This paper presents the design and implementation of a highly efficient Double-precision General Matrix Multiplication (DGEMM) based on Open BLAS for 64-bit ARMv8 eight-core processors. We adopt a theory-guided approach by first developing a performance model for this architecture and then using it to guide our exploration. The key enabler for a highly efficient DGEMM is a highly-optimized inner kernel GEBP developed in assembly language. We have obtained GEBP by (1) maximizing its compute-to-memory access ratios across all levels of the memory hierarchy in the ARMv8 architecture with its performance-critical block sizes being determined analytically, and (2) optimizing its computations through exploiting loop unrolling, instruction scheduling and software-implemented register rotation and taking advantage of A64 instructions to support efficient FMA operations, data transfers and prefetching. We have compared our DGEMM implemented in Open BLAS with another implemented in ATLAS (also in terms of a highly-optimized GEBP in assembly). Our implementation outperforms the one in ALTAS by improving the peak performance (efficiency) of DGEMM from 3.88 Gflops (80.9%) to 4.19 Gflops (87.2%) on one core and from 30.4 Gflops (79.2%) to 32.7 Gflops (85.3%) on eight cores. These results translate into substantial performance (efficiency) improvements by 7.79% on one core and 7.70% on eight cores. In addition, the efficiency of our implementation on one core is very close to the theoretical upper bound 91.5% obtained from micro-benchmarking. Our parallel implementation achieves good performance and scalability under varying thread counts across a range of matrix sizes evaluated.","0190-3918","978-1-4673-7587","10.1109/ICPP.2015.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349575","64-bit ARMv8 processor;register rotation;prefetching;blocking;DGEMM;BLAS;compute-to-memory access ratio","Registers;Kernel;Computational modeling;Program processors;Assembly;Memory management","assembly language;benchmark testing;matrix multiplication;multiprocessing systems;multi-threading;performance evaluation","micro-benchmarking;ALTAS;GEBP;data prefetching;data transfer;FMA operation;A64 instruction;performance-critical block size;ARMv8 architecture;memory hierarchy;compute-to-memory access ratio;assembly language;theory-guided approach;64-bit ARMv8 eight-core processor;OpenBLAS;double-precision general matrix multiplication;64-bit ARMv8 multicore processor;DGEMM","","4","18","","","","","","IEEE","IEEE Conferences"
"Rearrangement-Based Phylogeny Using the Single-Cut-or-Join Operation","P. Biller; P. Feijão; J. Meidanis","University of Campinas, Campinas; University of Campinas, Campinas; University of Campinas, Campinas","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2013","10","1","122","134","Recently, the Single-Cut-or-Join (SCJ) operation was proposed as a basis for a new rearrangement distance between multichromosomal genomes, leading to very fast algorithms, both in theory and in practice. However, it was not clear how well this new distance fares when it comes to using it to solve relevant problems, such as the reconstruction of evolutionary history. In this paper, we advance current knowledge, by testing SCJ's ability regarding evolutionary reconstruction in two aspects: 1) How well does SCJ reconstruct evolutionary topologies? and 2) How well does SCJ reconstruct ancestral genomes? In the process of answering these questions, we implemented SCJ-based methods, and made them available to the community. We ran experiments using as many as 200 genomes, with as many as 3,000 genes. For the first question, we found out that SCJ can recover typically between 60 percent and more than 95 percent of the topology, as measured through the Robinson-Foulds distance (a.k.a. split distance) between trees. In other words, 60 percent to more than 95 percent of the original splits are also present in the reconstructed tree. For the second question, given a topology, SCJ's ability to reconstruct ancestral genomes depends on how far from the leaves the ancestral is. For nodes close to the leaves, about 85 percent of the gene adjacencies can be recovered. This percentage decreases as we move up the tree, but, even at the root, about 50 percent of the adjacencies are recovered, for as many as 64 leaves. Our findings corroborate the fact that SCJ leads to very conservative genome reconstructions, yielding very few false-positive gene adjacencies in the ancestrals, at the expense of a relatively larger amount of false negatives. In addition, experiments with real data from the Campanulaceae and Protostomes groups show that SCJ reconstructs topologies of quality comparable to the accepted trees of the species involved. As far as time is concerned, the methods we implemented can find a topology for 64 genomes with 2,000 genes each in about 10.7 minutes, and reconstruct the ancestral genomes in a 64-leaf tree in about 3 seconds, both on a typical desktop computer. It should be noted that our code is written in Java and we made no significant effort to optimize it.","1545-5963;1557-9964;2374-0043","","10.1109/TCBB.2012.168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6389675","Genome rearrangement;phylogeny","Genomics;Vegetation;Topology;Phylogeny;Extremities;Biological cells;Polynomials","biology computing;cellular biophysics;evolution (biological);genetics;genomics;Java;trees (mathematics)","rearrangement-based phylogeny;single-cut-or-join operation;rearrangement distance;multichromosomal genomes;evolutionary history reconstruction;evolutionary topology reconstruction;ancestral genome reconstruction;Robinson-Foulds distance;split distance;tree reconstruction;leaves;false-positive gene adjacencies;Campanulaceae groups;Protostomes groups;desktop computer;Java","Animals;Campanulaceae;Computer Simulation;Evolution, Molecular;Gene Rearrangement;Genome;Genomics;Models, Genetic;Phylogeny;Software","5","39","","","","","","IEEE","IEEE Journals & Magazines"
"An assessment of the E-learning readiness among EFL university students and its relationship with their English proficiency","Y. Samaneh; I. Marziyeh; R. Mehrak","District Two, Educational Office, Shiraz, Iran; Department of English Language, Shiraz Branch, Islamic azad University, Iran; English Department, Shahid Rajaee Teacher Training University, Iran","4th International Conference on e-Learning and e-Teaching (ICELET 2013)","","2013","","","81","85","This study aimed at investigating Iranian EFL students' e-learning readiness to be involved in an electronic language learning (ELL) environment and its relationship with their English language proficiency. Data were collected from 217 EFL students from Shiraz Azad University using an e-learning readiness self-assessment and the Test of English as a Foreign Language (TOEFL). The results indicated that the sample had relatively high readiness for e-learning. Moreover, a positive correlation between EFL students' English proficiency and their e-learning readiness was found. The findings suggested that EFL students are ready for online and distance language education provided by university and their mastery of language helps them to optimize their language learning in online environment.","2163-6982","978-1-4673-5268-0978-1-4673-5267","10.1109/ICELET.2013.6681650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6681650","E-learning;E-learning readiness;EFL students;English proficiency","Educational institutions;Reliability;Atmospheric measurements;Particle measurements;Internet;Correlation;Software","computer aided instruction;distance learning","EFL university students;Iranian EFL students e-learning readiness;electronic language learning environment;ELL;English language proficiency;Shiraz Azad University;e-Iearning readiness self-assessment;test of English as a foreign language;TOEFL;e-Iearning;distance language education;online language education","","1","35","","","","","","IEEE","IEEE Conferences"
"Return multi sourcing lot-sizing problem in a hybrid manufacturing / remanufacturing system under carbon emission constraint","T. Zouadi; M. Reghioui; K. E. El Kadiri; A. Yalaoui","National School of Applied Sciences, Abdelmalek Essaadi University, Tetouan, Morocco; National School of Applied Sciences, Abdelmalek Essaadi University, Tetouan, Morocco; National School of Applied Sciences, Abdelmalek Essaadi University, Tetouan, Morocco; ICD-LOSI (UMR CNRS STMR-6279), University of Technology of Troyes, France","2015 4th International Conference on Advanced Logistics and Transport (ICALT)","","2015","","","223","228","Owing to Environmental consciousness, the returned products/ treatment becomes a crucial issue. Returns treatment becomes an economical way that helps industries to minimize costs and to reduce energy usage. Along the same lines, the lot sizing problem in systems incorporating products treatment options or remanufacturing returns attracts more interest from industries and the researchers. In the present study, we discuss lot sizing problem in systems incorporating remanufacturing with returns multi sourcing option under a carbon emission constraint. The system studied is a single production line where both regular production and remanufacturing process are carried out, with different set up costs for each operation. We consider a returns collection phase from a certain number of customers / distributors with a deterministic returns quantity in each period of the planning horizon. In this paper, we propose a mathematical model for the problem, an adaptation of the well-known Silver and Meal (SM) and two versions of a hybrid method (HM1 &amp; HM2) providing an approximate solution to the addressed problem. The mathematical model was tested on Cplex (Software optimizer), and the obtained results were compared with the ones provided by the adapted heuristic SM and the hybrid methods. The analysis of the results showed that hybrid methods provide good quality solutions in a moderate computational time.","","978-1-4799-8400","10.1109/ICAdLT.2015.7136628","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7136628","lot-sizing;remanufacturing;collection;hybrid method;Carbon emission","Production;Manufacturing;Erbium","air pollution;cost reduction;lot sizing;recycling","hybrid method 2;HM2;HM1;heuristic SM;software optimizer;Cplex;hybrid method 1;Silver and Meal method;returns collection phase;set up costs;regular production;carbon emission constraint;hybrid manufacturing-remanufacturing system;return multisourcing lot-sizing problem","","1","24","","","","","","IEEE","IEEE Conferences"
"Analysis of end-to-end delay characteristics for various packets in IEC 61850 substation communications system","N. Das; W. Ma; S. Islam","School of Mechanical and Electrical Engineering, Faculty of Health, Engineering, and Sciences, University of Southern Queensland, Toowoomba, QLD 4350, Australia; Department of Electrical and Computer Engineering, Curtin University, Perth, WA 6845, Australia; Department of Electrical and Computer Engineering, Curtin University, Perth, WA 6845, Australia","2015 Australasian Universities Power Engineering Conference (AUPEC)","","2015","","","1","5","Substation plays an important role in power system communications for safe and reliable operation of entire power networks. Substation communication networks are connected with various substation intelligent electronic devices (IEDs), which is substation systems lifeblood and the system availability is decided by its real-time performance. International Electro-technical Commission (IEC) has been developed the standards based on object-oriented technologies for substation automation. IEC 61850 protocol has been applied widely in substation communication applications. It presents new challenges to realtime performance simulation and testing of protective relays. In this paper, an optimized network engineering tool (OPNET) or Riverbed modeler simulation tool/ software has been used for the modeling of IED in substation level network. Based on the simulation results, different types of data stream have been discussed, such as, periodic data stream, random data stream and burst data steam. The typical studies using these models, to construct substation automation system (SAS) network on the OPNET modeler or Riverbed modeler was made to reveal the impact of each affecting parameter or factor to the real-time performance of substation communications system, which is also incorporated in this report.","","978-1-4799-8725-2978-1-4799-8724","10.1109/AUPEC.2015.7324831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7324831","IEC 61850;OPNET or Riverbed Modeler;Power system simulation;Real-time;Substation automation system (SAS);Transmission delay","Substations;IEC Standards;Real-time systems;Delays;Object oriented modeling;Communication networks;Protocols","IEC standards;power system reliability;relay protection;substation automation;substation protection","end-to-end delay characteristics analysis;IEC 61850 substation communication system;power system communication;substation intelligent electronic device;IED;international electrotechnical commission;object-oriented technology;substation automation;protective relay testing;optimized network engineering tool;OPNET;Riverbed modeler simulation tool;SAS network","","4","16","","","","","","IEEE","IEEE Conferences"
"AES encryption engines of many core processor arrays on FPGA by using parallel, pipeline and sequential technique","P. U. Deshpande; S. A. Bhosale","Department of Electronics and Telecommunication Engg., Zeal College of Engineering and Research, Pune, India; Department of Electronics and Telecommunication Engg., Zeal College of Engineering and Research, Pune, India","2015 International Conference on Energy Systems and Applications","","2015","","","75","80","Now a days, the number of Internet and wireless communications users has rapidly grown and that increases demand for security measures to protect user data transmitted over openchannels. Cryptographic algorithms are very essential for security of the systems worldwide. In December 2001, the National Institute of Standards and Technology (NIST) of the United States selected the Rijndael algorithm as the suitable Advanced Encryption Standard (AES) to replace the Data Encryption Standard (DES) algorithm. AES can be considered the most widely used modern symmetric key encryption standard. The AES algorithm is a block cipher that can encrypt and decrypt digital information. This paper explores task level parallelism with three concurrently working AES modules to achieve less area and high throughput. With the area optimization techniques, the system becomes area and time efficient as the throughput of 5.751Gbps is achieved with less area. The design is implemented in Zynq(xc7z020-2clg484) device and tested on Zedboard. As three different implementations of AES are explored, the design has three times higher throughput with less area than the other systems. To encrypt/decrypt a file using the AES algorithm, the file must undergo a set of complex computational steps. Therefore a software implementation of AES algorithm would be slow and consume large amount of time to complete. The immense increase of both stored and transferred data in the recent years had made this problem even more serious when the need to encrypt/decrypt such data arises.","","978-1-4673-6817","10.1109/ICESA.2015.7503316","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503316","Advanced Encryption Standard (AES);Area;Decryption;Encryption;Three AES;Throughput;Parallel;Pipeline and Sequential design","Encryption;Throughput;Standards;Clocks;Field programmable gate arrays;Ciphers","concurrency control;cryptography;data communication;field programmable gate arrays;multiprocessing systems;parallel programming;pipeline processing","AES encryption engine;many core processor array;FPGA;parallel technique;pipeline technique;sequential technique;Internet;wireless communication;security measure;user data protection;cryptographic algorithm;system security;National Institute of Standards and Technology;NIST;United States;Rijndael algorithm;Advanced Encryption Standard;Data Encryption Standard;DES algorithm;symmetric key encryption standard;block cipher;digital information encryption;decryption;task level parallelism;concurrently working AES modules;area optimization technique;Zynq(xc7z020-2clg484) device;Zedboard;software implementation;data storage;data transfer;data transmission","","2","13","","","","","","IEEE","IEEE Conferences"
"Optimal Operation of Distribution Networks Considering Energy Storage Devices","L. H. Macedo; J. F. Franco; M. J. Rider; R. Romero","Departamento de Engenharia ElétricaFaculdade de Engenharia de Ilha Solteira, Universidade Estadual Paulista, Ilha Solteira, São Paulo, Brazil; Departamento de Engenharia ElétricaFaculdade de Engenharia de Ilha Solteira, Universidade Estadual Paulista, Ilha Solteira, São Paulo, Brazil; Departamento de Engenharia ElétricaFaculdade de Engenharia de Ilha Solteira, Universidade Estadual Paulista, Ilha Solteira, São Paulo, Brazil; Departamento de Engenharia ElétricaFaculdade de Engenharia de Ilha Solteira, Universidade Estadual Paulista, Ilha Solteira, São Paulo, Brazil","IEEE Transactions on Smart Grid","","2015","6","6","2825","2836","This paper presents a mixed-integer second-order cone programing (MISOCP) model to solve the optimal operation problem of radial distribution networks (DNs) with energy storage. The control variables are the active and reactive generated power of dispatchable distributed generators (DGs), the number of switchable capacitor bank units in operation, the tap position of the voltage regulators and on-load tap-changers, and the operation state of the energy storage devices. The objective is to minimize the total cost of energy purchased from the distribution substation and the dispatchable DGs. The steady-state operation of the DN is modeled using linear and second-order cone programing. The use of an MISOCP model guarantees convergence to optimality using existing optimization software. A mixed-integer linear programing (MILP) formulation for the original model is also presented in order to show the accuracy of the proposed MISOCP model. An 11-node test system and a 42-node real system were used to demonstrate the effectiveness of the proposed MISOCP and MILP models.","1949-3053;1949-3061","","10.1109/TSG.2015.2419134","Conselho Nacional de Desenvolvimento Científico e Tecnológico; Fundação de Amparo à Pesquisa do Estado de São Paulo; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7095593","Distributed generation;energy storage;mixed-integer linear programing (MILP);mixed-integer second-order cone programing (MISOCP);optimal operation of radial distribution networks;smart grid;Distributed generation;energy storage;mixed-integer linear programing (MILP);mixed-integer second-order cone programing (MISOCP);optimal operation of radial distribution networks;smart grid","Distributed power generation;Reactive power;Energy storage;Mixed integer linear programming","energy storage;integer programming;linear programming;power distribution control;power system simulation;reactive power control","optimal operation problem;energy storage devices;mixed-integer second-order cone programming;MISOCP model;radial distribution networks;reactive generated power control;dispatchable distributed generators;switchable capacitor bank units;voltage regulators;on-load tap-changers;distribution substation;steady-state operation;linear cone programing;optimization software;mixed-integer linear programing;MILP formulation","","42","28","","","","","","IEEE","IEEE Journals & Magazines"
"Trans-ionospheric propagation experiment at HF-band: Channel measurement and modelling","F. Lacoste; V. Fabbro; J. Lemorton; G. Decerprit; R. Fleury; P. Pagani; F. Carvalho; S. Rougerie","CNES, Toulouse, France; ONERA/DEMR, Toulouse, France; ONERA/DEMR, Toulouse, France; NOVELTIS, Toulouse, France; Telecom Bretagne, Lab-STIC, Brest, France; Telecom Bretagne, Lab-STIC, Brest, France; NOVELTIS, Toulouse, France; NOVELTIS, Toulouse, France","2015 9th European Conference on Antennas and Propagation (EuCAP)","","2015","","","1","5","The HFPE technological experiment aims at consolidating a new concept of long range transmissions between ground and the Low Earth Orbit. This concept relies on over-the-horizon transionospheric links in the HF-band. To prepare this experiment, propagation modelling and measurements have been carried out. In particular, a HF link planning software, MSATIS, has been tested with respect to experimental data, and improved. One of the major improvements is a global prediction of the ionospheric foF2 and hmF2 characteristics based on IONEX data. For air interface optimization, multipath effects not addressed by MSATIS had also to be characterized. Ray launching has been used to identify typical short-term propagation channels representative of HFPE propagation conditions.","2164-3342","978-8-8907-0185-6978-8-8907-0182","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7228781","Trans-ionospheric propagation;HF propagation;over-the-horizon;prediction;multipath;diveegence","Ionosphere;Standards;Satellites;Delays;Computational modeling;Orbits;Doppler effect","ionospheric electromagnetic wave propagation;wireless channels","trans-ionospheric propagation experiment;HF-band;channel measurement;channel modelling;HFPE technological experiment;long range transmissions;Low Earth Orbit;over-the-horizon transionospheric links;propagation modelling;HF link planning software;MSATIS;ionospheric characteristics;IONEX data;air interface optimization;multipath effects;ray launching;short-term propagation channels;HFPE propagation conditions","","","8","","","","","","IEEE","IEEE Conferences"
"Optimal economic power dispatch in the presence of intermittent renewable energy sources","S. Elsaiah; M. Benidris; J. Mitra; N. Cai","Department of Electrical and Computer Engineering, Michigan State University, East Lansing, Michigan 48824, USA; Department of Electrical and Computer Engineering, Michigan State University, East Lansing, Michigan 48824, USA; Department of Electrical and Computer Engineering, Michigan State University, East Lansing, Michigan 48824, USA; Department of Electrical and Computer Engineering, Michigan State University, East Lansing, Michigan 48824, USA","2014 IEEE PES General Meeting | Conference & Exposition","","2014","","","1","5","This paper describes a method for solving the economic power dispatch problem in the presence of renewable energy sources. The method proposed in this paper uses linear programming because linear programming based formulations tend to be flexible, reliable, and faster than their nonlinear counterparts. A linearized network model in the form of DC power flow model is utilized in this paper. Thermal limits of transmission lines and real power constraints have both been considered in the proposed model. In addition, piecewise linear models for generating units cost curves are developed during the realization of the presented work. A micro-power optimization model based on Homer software is used to account for the uncertainty in the output power of the intermittent renewable energy sources. The proposed linear programming based method is demonstrated on the standard IEEE 30 bus system. Test results are reported, discussed, and thoroughly analyzed.","1932-5517","978-1-4799-6415","10.1109/PESGM.2014.6939903","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6939903","Economic dispatch;renewable energy;wind turbine generator;linear programming","Wind turbines;Generators;Renewable energy sources;Economics;Wind speed;Linear programming","linear programming;load flow;piecewise linear techniques;power generation dispatch;power generation economics;renewable energy sources","economic power dispatch;renewable energy sources;linear programming based formulations;linearized network model;DC power flow model;transmission lines thermal limits;piecewise linear models;generating units cost curves;micropower optimization model;Homer software;linear programming based method;IEEE 30 bus system","","1","33","","","","","","IEEE","IEEE Conferences"
"Grain-oriented computer architectures for dynamically-reconfigurable avionics systems","C. C. Insaurralde","Institute of Sensors, Signals, and Systems, Heriot-Watt University, Edinburgh, United Kingdom","2013 IEEE/AIAA 32nd Digital Avionics Systems Conference (DASC)","","2013","","","7D3-1","7D3-13","Reconfigurable Computer Architectures (RCAs) still needs to be exploited since no many approaches fully squeeze their capabilities. Efficient allocation of computing resources in dynamic demand-driven deployments for avionics is all a challenge since they face costly and risky certification processes. This paper reviews the current RCAs with a strong emphasis on a full range of granularity and dynamic resource management. It has a good argument for having an effective control at the smallest reconfigurable RCA unit for efficient energy. The review of RCAs has a generic nature but aims to study RCAs for avionics self-adaptation. Unmanned Air Vehicles (UAVs) can be a first suitable test-bed to assess dynamically-reconfigurable avionics solutions. They are simpler than passenger aircraft, and currently required to endure much longer missions. Therefore, efficient power consumption is critical. The software-hardware computer interaction is essential to successfully implement optimization mechanisms to save energy. This paper describes an overview and a grain-based classification of RCAs. It presents a review that reveals gaps of current RCAs for avionics. A dynamically RCA is also proposed based on the flexibility of programming languages, and the reconfigurability of reprogrammable devices. Future research directions are presented as well.","2155-7195;2155-7209","978-1-4799-1538-5978-1-4799-1536","10.1109/DASC.2013.6712645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6712645","","Aerospace electronics;Computer architecture;Computers;Hardware;Field programmable gate arrays;Vehicle dynamics;Resource management","aircraft computers;avionics;certification;reconfigurable architectures;resource allocation","reprogrammable device reconfigurability;programming languages;grain-based classification;optimization mechanisms;software-hardware computer interaction;power consumption;passenger aircraft;UAVs;unmanned air vehicles;avionics self-adaptation;reconfigurable RCA unit;dynamic resource management;computing resource allocation;risky certification processes;dynamically-reconfigurable avionics systems;grain-oriented computer architectures","","","26","","","","","","IEEE","IEEE Conferences"
"Virtualization overhead findings of four hypervisors in the CloudStack with SIGAR","P. V. V. Reddy; L. Rajamani","Department of CSE, OU Hyderabad, India; Department of CSE, OU Hyderabad, India","2014 4th World Congress on Information and Communication Technologies (WICT 2014)","","2014","","","140","145","Virtualization increases the utilization of physical servers and enables portability of virtual servers between physical servers. Virtualization Technology plays an important role in the success of cloud computing as it optimizes hardware and software resources which in turn enables cloud model to deliver the services in a scalable manner. Hypervisor as a virtualization layer provides an infrastructural support to multiple virtual machines above it by virtualizing hardware resources such as CPU, Memory, Disk and NIC. Cloud computing model and virtualization technology both increases the scalability of IT infrastructure but with an overhead of the performance as Hypervisors may not give near native performance. It is encouraging to study how different Hypervisors perform in the Private Cloud as compared to the native system. Hypervisors do come in Para-virtualized [XenServer], Full Virtualized [ESXi] and Hybrid [KVM] flavors. Hyper-V uses microkernelized design. It is novel idea to compare them in the Private Cloud environment. In the experiment, we have created our private cloud using CloudStack. This paper uses SIGAR (System Information Gatherer and Reporter) framework to compare four hypervisors namely XenServer, ESXi, Hyper-V and KVM.","","978-1-4799-8115-1978-1-4799-8114","10.1109/WICT.2014.7077318","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7077318","CloudStack;Hypervisor;Private Cloud;Virtualization;SIGAR","Virtual machine monitors;Virtualization;Cloud computing;Servers;Benchmark testing;Hardware;Operating systems","cloud computing;file servers;resource allocation;virtual machines;virtualisation","virtualization overhead findings;CloudStack;SIGAR;physical server utilization;virtual servers;cloud computing;hardware resources;software resources;virtual machines;IT infrastructure;private cloud;para-virtualized hypervisor;XenServer;full virtualized hypervisor;ESXi;hybrid hypervisor;KVM;microkernelized design;System Information Gatherer and Reporter framework;hyper-V","","","26","","","","","","IEEE","IEEE Conferences"
"Region-Based May-Happen-in-Parallel Analysis for C Programs","P. Di; Y. Sui; D. Ye; J. Xue","NA; NA; NA; NA","2015 44th International Conference on Parallel Processing","","2015","","","889","898","The C programming language continues to play an essential role in the development of system software. May-Happen-in-Parallel (MHP) analysis is the basis of many other analyses and optimisations for concurrent programs. Existing MHP analyses that work well for programming languages such as X10 are often not effective for C (with Pthreads). This paper presents a new MHP algorithm for C that operates at the granularity of code regions rather than individual statements in a program. A flow-sensitive Happens-Before (HB) analysis is performed to account for fork-join semantics of pthreads on an interprocedural thread-sensitive control flow graph representation of a program, enabling the HB relations among its statements to be discovered. All the statements that share the same HB properties are then grouped into one region. As a result, computing the MHP information for all pairs of statements in a program is reduced to one of inferring the HB relations from among its regions. We have implemented our algorithm in LLVM-3.5.0 and evaluated it using 14 programs from the SPLASH2 and PARSEC benchmark suites. Our preliminary results show that our approach is more precise than two existing MHP analyses yet computationally comparable with the fastest MHP analysis.","0190-3918","978-1-4673-7587","10.1109/ICPP.2015.98","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7349644","May-Happen-in-Parallel;concurrent program;multi-threading;static analysis","Instruction sets;Algorithm design and analysis;Runtime;Parallel processing;Programming;Semantics;Benchmark testing","C language;concurrency control;flow graphs;multi-threading;parallel programming;programming languages","region-based may-happen-in-parallel analysis;C programming language;system software;MHP analysis;concurrent programs;programming languages;MHP algorithm;code region granularity;flow-sensitive happens-before analysis;fork-join semantics;Pthreads;interprocedural thread-sensitive control flow graph representation;HB properties;LLVM-3.5.0;SPLASH2 benchmark suites;PARSEC benchmark suites","","3","20","","","","","","IEEE","IEEE Conferences"
"Parallel Architecture Benchmarking: From Embedded Computing to HPC, a FiPS Project Perspective","Y. Lhuillier; J. M. Philippe; A. Guerre; M. Kierzynka; A. Oleksia","NA; NA; NA; NA; NA","2014 12th IEEE International Conference on Embedded and Ubiquitous Computing","","2014","","","154","161","With the growing numbers of both parallel architectures and related programming models, the benchmarking tasks become very tricky since parallel programming requires architecture-dependent compilers and languages as well as high programming expertise. More than just comparing architectures with synthetic benchmarks, benchmarking is also more and more used to design specialized systems composed of heterogeneous computing resources to optimize the performance or performance/watt ratio (e.g. embedded systems designers build System-on-Chip (SoC) out of dedicated and well-chosen components). In the High-Performance-Computing (HPC) domain, systems are designed with symmetric and scalable computing nodes built to deliver the highest performance on a wide variety of applications. However, HPC is now facing cost and power consumption issues which motivate the design of heterogeneous systems. This is one of the rationales of the European FiPS project, which proposes to develop hardware architecture and software methodology easing the design of such systems. Thus, having a fair comparison between architectures while considering an application is of growing importance. Unfortunately, porting it on all available architectures using the related programming models is impossible. To tackle this challenge, we introduced a novel methodology to evaluate and to compare parallel architectures in order to ease the work of the programmer. Based on the usage of micro benchmarks, code profiling and characterization tools, this methodology introduces a semi-automatic prediction of sequential applications performances on a set of parallel architectures. In addition, performance estimation is correlated with the cost of other criteria such as power or portability effort. Introduced for targeting vision-based embedded applications, our methodology is currently being extended to target more complex applications from HPC world. This paper extends our work with new experiments and early results on a real HPC application of DNA sequencing.","","978-0-7695-5249","10.1109/EUC.2014.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6962280","Benchmarking;Heterogeneity;Computing servers;HPC;Bioinformatics","Kernel;Computer architecture;Databases;Benchmark testing;Programming;Hardware;Servers","embedded systems;parallel architectures;performance evaluation;system-on-chip","parallel architecture benchmarking;embedded computing;HPC;FiPS project perspective;related programming models;architecture dependent compilers;synthetic benchmarks;embedded systems designers;System-on-Chip;SoC;high performance computing;scalable computing nodes;symmetric computing nodes;European FiPS project;hardware architecture;software methodology;performance estimation","","6","13","","","","","","IEEE","IEEE Conferences"
"MGSim — A simulation environment for multi-core research and education","R. Poss; M. Lankamp; Q. Yang; J. Fu; I. Uddin; C. R. Jesshope","University of Amsterdam, Computer Systems Architecture group, Science Park 904, 1098XH Amsterdam, The Netherlands; University of Amsterdam, Computer Systems Architecture group, Science Park 904, 1098XH Amsterdam, The Netherlands; University of Amsterdam, Computer Systems Architecture group, Science Park 904, 1098XH Amsterdam, The Netherlands; University of Amsterdam, Computer Systems Architecture group, Science Park 904, 1098XH Amsterdam, The Netherlands; University of Amsterdam, Computer Systems Architecture group, Science Park 904, 1098XH Amsterdam, The Netherlands; University of Amsterdam, Computer Systems Architecture group, Science Park 904, 1098XH Amsterdam, The Netherlands","2013 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS)","","2013","","","80","87","This article presents MGSim<sup>1</sup>, an open source discrete event simulator for on-chip hardware components developed at the University of Amsterdam. MGSim is used as research and teaching vehicle to study the fine-grained hardware/software interactions on many-core chips with and without hardware multithreading. MGSim's component library includes support for core models with different instruction sets, a configurable interconnect, multiple configurable cache and memory models, a dedicated I/O subsystem, and comprehensive monitoring and interaction facilities. The default model configuration shipped with MGSim implements Microgrids, a multi-core architecture with hardware concurrency management. MGSim is furthermore written mostly in C++ and uses object classes to represent chip components. It is optimized for architecture models that can be described as process networks.","","978-1-4799-0103","10.1109/SAMOS.2013.6621109","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621109","","Computer architecture;Microgrids;Hardware;Emulation;Program processors;Benchmark testing","cache storage;computer science education;discrete event simulation;microprocessor chips;multiprocessing systems","MGSim;simulation environment;open source discrete event simulator;on-chip hardware components;University of Amsterdam;many-core chips;fine-grained hardware-software interactions;instruction sets;configurable interconnect;multiple configurable cache;memory models;dedicated I/O subsystem;comprehensive monitoring facility;interaction facilities;Microgrids;multicore architecture;hardware concurrency management;C++;process networks","","6","18","","","","","","IEEE","IEEE Conferences"
"From performance profiling to predictive analytics while evaluating hadoop cost-efficiency in ALOJA","N. Poggi; J. L. Berral; D. Carrera; A. Call; F. Gagliardi; R. Reinauer; N. Vujic; D. Green; J. Blakeley","Barcelona Supercomputing Center (BSC) Universitat Poliècnica de Catlalunya (BarcelonaTech) Barcelona, Spain; Barcelona Supercomputing Center (BSC) Universitat Poliècnica de Catlalunya (BarcelonaTech) Barcelona, Spain; Barcelona Supercomputing Center (BSC) Universitat Poliècnica de Catlalunya (BarcelonaTech) Barcelona, Spain; Barcelona Supercomputing Center (BSC) Universitat Poliècnica de Catlalunya (BarcelonaTech) Barcelona, Spain; Barcelona Supercomputing Center (BSC) Universitat Poliècnica de Catlalunya (BarcelonaTech) Barcelona, Spain; Microsoft Corporation, Microsoft Research (MSR) Redmond, USA; Microsoft Development Center Serbia (MDCS) Belgrade, Serbia; Microsoft Corporation, Microsoft Research (MSR) Redmond, USA; Microsoft Corporation, Microsoft Research (MSR) Redmond, USA","2015 IEEE International Conference on Big Data (Big Data)","","2015","","","1220","1229","During the past years the exponential growth of data, its generation speed, and its expected consumption rate presents one of the most important challenges in IT both for industry and research. For these reasons, the ALOJA research project was created by BSC and Microsoft as an open initiative to increase cost-efficiency and the general understanding of Big Data systems via automation and learning. The development of the project over its first year, has resulted in a open source benchmarking platform used to produce the largest public repository of Big Data results1, featuring over 42,000 job execution details. ALOJA also includes web-based analytic tools to evaluate and gather insights about cost-performance of benchmarked systems. The tools offer means to extract knowledge that can lead to optimize configuration and deployment options in the Cloud i.e., selecting the most cost-effective VMs and cluster sizes. This article describes the evolution of the project focus and research lines, for a period of over a year while continuously benchmarking systems for Big Data. As well discusses the motivation - both technical and market-based - of such changes. It also presents the main results from the evaluation of different OS and Hadoop configurations, covering over 100 hardware deployments. During this time, ALOJA's initial target has shifted from a previous low-level profiling of Hadoop runtime with HPC tools, passing through extensive benchmarking and evaluation of a large body of results via aggregation, to currently leveraging Predictive Analytics (PA) techniques. The ongoing efforts in PA show promising results to automatically model the behavior of systems i.e., predicting job execution times with high accuracy or to reduce the number of benchmark runs needed. As well as for Knowledge Discovery (KD) to find relations among software and hardware components. Techniques that jointly support foresighting cost-effectiveness of new defined systems, reducing benchmarking time and costs.","","978-1-4799-9926-2978-1-4799-9925","10.1109/BigData.2015.7363876","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363876","","Benchmark testing;Big data;Cloud computing;Hardware;Predictive models;Knowledge discovery","Big Data;cloud computing;data mining;parallel processing","hardware components;software components;knowledge discovery;PA techniques;HPC tools;Hadoop runtime;low-level profiling;Hadoop configurations;OS configurations;cloud;knowledge extraction;benchmarked systems;cost-performance;Web-based analytic tools;learning;automation;Big Data systems;ALOJA research project;data consumption rate;data generation speed;data exponential growth;Hadoop cost-efficiency;predictive analytics;performance profiling","","4","29","","","","","","IEEE","IEEE Conferences"
"The design of a dynamic simulation system of a liquid chemical filling process","Z. Zou; X. Gui; Lei Meng; Z. Wang; X. Liu; Meng Yu","China Research Institute of Chemical Defense, P.O. Box 1043, Beijing 102205, China; China Research Institute of Chemical Defense, P.O. Box 1043, Beijing 102205, China; China Research Institute of Chemical Defense, P.O. Box 1043, Beijing 102205, China; China Research Institute of Chemical Defense, P.O. Box 1043, Beijing 102205, China; China Research Institute of Chemical Defense, P.O. Box 1043, Beijing 102205, China; China Research Institute of Chemical Defense, P.O. Box 1043, Beijing 102205, China","2015 Chinese Automation Congress (CAC)","","2015","","","2033","2038","The flow sheet and operation principle of a liquid filling process was briefly discussed, and the dynamic simulation mathematic models of the filling process were developed in detail using the mass balance equations, gas state equations and historical process data. Based on the intelligent process control (IPC) simulation mode, the overall structure and the development scheme of the filling process simulation system was designed. A dynamic simulation system of the filling process was successfully developed, by taking a typical SCADA (supervisory control and data acquisition) configuration software named Century Star as GUI (graphical user interface) development platform, and integrating with a DLL (dynamic link library) function program to perform the calculation of the simulation models programmed using Visual C++. The development of the simulation GUI included the design and Command Language programming of simulated process flow sheet diagrams, simulation command buttons and their displaying diagrams, and the operation and parameters adjustment diagrams of all kinds of manipulators in the process flow sheets. The overall simulation operation of the filling process was fairly good achieved. Then some comprehensive and systematic simulation tests of the filling process were carried out. The simulation results show that the simulation system could satisfy the filling process research and design requirements very well, and the main process operation parameters were optimized through these simulation tests.","","978-1-4673-7189-6978-1-4673-7188","10.1109/CAC.2015.7382838","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382838","Filling process;process modeling;intelligent process control simulation mode;simulation system;simulation test","Solid modeling;Filling;Design automation;Robustness;IP networks","C++ language;chemical industry;control engineering computing;filling;graphical user interfaces;intelligent control;process control;production engineering computing;SCADA systems","dynamic simulation system design;liquid chemical filling process;dynamic simulation mathematic models;mass balance equations;gas state equations;historical process data;intelligent process control;IPC simulation mode;filling process simulation system;SCADA;supervisory control and data acquisition;Century Star;GUI;graphical user interface;DLL;dynamic link library function program;Visual C++;command language programming;process flow sheet diagrams;simulation command buttons","","","24","","","","","","IEEE","IEEE Conferences"
"Topology Characters of the Linux Call Graph","Y. Wang; D. Ding","NA; NA","2015 2nd International Conference on Information Science and Control Engineering","","2015","","","517","518","A call graph is a directed graph in which nodes are functional units and arcs are call events, it is used to represent the call relationship of the program. Generally speaking, call graph plays a very important role in program analysis, and it has been widely used in software engineering (e.g., program understanding, compiling optimization, regression test, etc). In the present paper, we studied the giant strong component of the call graph of the Linux kernel (v2.6.27) with the rising complex network theory. We analyzed some basic topological characteristic of the giant strong component, and we also identified the top 20 functional units.","","978-1-4673-6850-6978-1-4673-6849","10.1109/ICISCE.2015.120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120660","call graph;complex network;Linux kernel;topological characteristic","Linux;Complex networks;Kernel;Biology;Topology;Physics","complex networks;directed graphs;Linux;network theory (graphs);operating system kernels;program diagnostics;topology","topology characters;Linux call graph;directed graph;functional units;call events;program analysis;software engineering;Linux kernel;complex network theory","","","7","","","","","","IEEE","IEEE Conferences"
"Research into fault location methods for DC railway traction system","X. M. Song; J. H. He; T. Yip; B. Kirby; X. Yang","Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Beijing Jiaotong University, Beijing, China; Alstom Grid UK Ltd; Alstom Grid UK Ltd","12th IET International Conference on Developments in Power System Protection (DPSP 2014)","","2014","","","1","6","There are two common methods applied in the traction line fault location, the travelling wave method and the impedance method. Each method has advantages and disadvantages. It is difficult to achieve optimized goal with only one method. Therefore, this paper combines the two algorithms to improve the accuracy of fault location. MATLAB/Simulink software was used to test and validate the proposed fault location approach. Various fault conditions were simulated by varying fault location, using different fault location methods, on a given subway traction power supply system model. After synthetically analysis the enhanced accuracy simulation results are deduced, and the integrated simulation results have been proved to be highly accurate. The combining algorithm in the fault location has promising future in the practical application.","","978-1-84919-834","10.1049/cp.2014.0020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6822828","DC railway system;fault location;impedance calculation;travelling wave","","fault location;railways;traction power supplies","DC railway traction system;traction line fault location;travelling wave method;impedance method;MATLAB-Simulink software;subway traction power supply system model","","","","","","","","","IET","IET Conferences"
"Design of electrically tunable phase shifter for antenna arrays operating in Ku-band","R. Ouali; L. Osman; T. Razban; Y. Mahé","Department of Physics, UR “CSEHF” 13ES37, FST, University of Tunis El Manar, 2092 Tunisia; Department of Physics, UR “CSEHF” 13ES37, FST, University of Tunis El Manar, 2092 Tunisia; IETR UMR CNRS 6164, Polytech Nantes, University of Nantes, 44306, France; IETR UMR CNRS 6164, Polytech Nantes, University of Nantes, 44306, France","2015 IEEE 15th Mediterranean Microwave Symposium (MMS)","","2015","","","1","5","This paper deals with the modeling and simulation of a wideband analog phase shifter for Ku applications. The structure adopted is based on a 90° branch-line coupler loaded by varactor diodes used to provide a wide phase shift range. The main idea of this work is to integrate controlled phase shifters in the antenna array in order to control its main beam between different satellites. Several circuits have been tested and optimized using ADS software from Agilent Technologies to obtain a wide bandwidth and wide phase shift range with low insertion loss over the Ku band.","","978-1-4673-7602-0978-1-4673-7601","10.1109/MMS.2015.7375467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375467","wideband phase shifter;varactor diodes;antenna array;continuous beam steering","Phase shifters;Capacitance;Varactors;Phased arrays;Insertion loss;Satellite broadcasting","antenna arrays;microwave phase shifters;phase shifters","electrically tunable phase shifter;antenna arrays;Ku-band;wideband analog phase shifter;branch-line coupler;varactor diodes;ADS software;Agilent Technologies;insertion loss","","2","10","","","","","","IEEE","IEEE Conferences"
"Design and simulation of power efficient traffic light controller (PTLC)","J. Shridhar; Ruchin; P. Whig","Electronic and Communication Engineering Department, Guru Premsukh Memorial College Of Engineering (GGSIPU), New Delhi, India; Electronic and Communication Engineering Department, Mahatma Gandhi Mission College of Engineering &amp; Technology, Noida, Uttar Pradesh, India; Dept. of Electronics &amp; Communication Engineering, Bhagwan Parshuram Institute of Technology, New Delhi, India","2014 International Conference on Computing for Sustainable Global Development (INDIACom)","","2014","","","348","352","This paper presents design and simulation of a power efficient traffic light controller (PTLC). The main focus is on simulation and optimization of PTLC design and computing its speed of operation. In the conventional system, power consumption is high and expensive. The design of PTLC is better than conventional in terms of LUT's (number of gates), complexity, size and cost. In this research paper a novel PTLC is presented with a minimum number of LEDs which fairly improves its performance and makes the design efficient in terms of power and speed with respect to conventional design. The conventional traffic light controller has been implemented using microcontroller and FPGA's. The research paper by Parasmani in 2013 stated the use of FPGA to design an advanced traffic light controller which uses the sensor to maintain the continuous traffic flow hence the power consumption is too high which can be reduced by the design PTLC. The novel design of PTLC is an economical and possess the characters of high integration, low power and flexibility. The PTLC has been implemented using FPGA. FPGA has many advantages as the speed, number of input/output ports and performance. This system has been successful tested and implemented in hardware using Xilinx v 10.1 software packages using Very High Speed Integrated circuit hardware description language (VHDL), RTL and technology schematic are included to validate simulation results.","","978-93-80544-12-0978-93-80544-10-6978-93-80544-11","10.1109/IndiaCom.2014.6828157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6828157","FPGA;Xilinx;VHDL;PTLC","Light emitting diodes;Field programmable gate arrays;Roads;Junctions;Computational modeling;Hardware;Power demand","control engineering computing;field programmable gate arrays;hardware description languages;light emitting diodes;microcontrollers;road traffic control;sensors;traffic engineering computing","power efficient traffic light controller;PTLC design;power consumption;LUT;lookup tables;LED;light emitting diodes;microcontroller;FPGA;field programmable gate arrays;sensor;Xilinx software packages;very high speed integrated circuit hardware description language;VHDL;technology schematic","","3","15","","","","","","IEEE","IEEE Conferences"
"Investigation of two-phase flow in axial-centrifugal impeller by hydrodynamic modeling methods","V. O. Lomakin","«Hydraulics, hydro machines and hydro-pneumoautomatics», Bauman Moscow State Technical University, Moscow, Russia","2015 International Conference on Fluid Power and Mechatronics (FPM)","","2015","","","1204","1206","The article provides a methodology to study the flow in the wet part of the pump with fundamentally new axial-centrifugal impeller by methods of hydrodynamic modeling in the software package STAR CCM +. The objective of the study was to determine the normal and cavitation characteristics of the pump with a new type of wet part, as well as optimization of the geometrical parameters of the pump. The simulation results may be of interest to specialists in the field of hydrodynamic modeling, and for designers of such pumps. The authors also report production of a full length prototype of the pump in order to conduct further testing for the verification of the data in the article, primarily in terms of cavitation characteristics.","","978-1-4799-8770-2978-1-4799-8769","10.1109/FPM.2015.7337302","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7337302","CFD;cavitation;pump","Mathematical model;Pumps;Impellers;Fasteners;Fluids;Hydrodynamics;Computational modeling","cavitation;hydrodynamics;impellers;mechanical engineering computing;pumps;two-phase flow","two-phase flow;hydrodynamic modeling methods;wet pump part;axial-centrifugal impeller;STAR CCM +;software package;cavitation characteristics;geometrical parameters;full length prototype","","","15","","","","","","IEEE","IEEE Conferences"
"Design of an internal multi-band loop antenna for multiple mobile handset operations","C. Dai; D. Wu; Y. Wu","Guangdong University of Technology, Guangzhou, Guangdong, 510006, P. R. China; Guangdong University of Technology, Guangzhou, Guangdong, 510006, P. R. China; Guangdong University of Technology, Guangzhou, Guangdong, 510006, P. R. China","2013 IEEE International Wireless Symposium (IWS)","","2013","","","1","4","A compact internal multi-band folded loop antenna for GSM850/900/DCS/PCS/UMTS/LTE2300/2500/WLAN2.4GHz/WiMAX2.5GHz multiple mobile operations is proposed. The whole antenna structure proposed in this paper consists of a single folded meander loop track and a T-shape back-coupling element. The size of the total structure is 120 × 60 × 6.5mm<sup>3</sup>, meanwhile the area left for the loop track is only 60 × 15 × 6.5mm<sup>3</sup>. The compact structure makes it very suitable for small mobile phone applications. The design and optimizing of the performance of the proposed antenna are performed by using the simulator software HFSS. An actual model has been prototyped for testing. Measurement results and electromagnetic simulation ones are in good agreement, thus indicating that the proposed antenna can meet the actual demands.","","978-1-4673-2141","10.1109/IEEE-IWS.2013.6616786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616786","Handset antenna;folded loop antenna;mobile phone antenna;multi-band antenna;LTE;WLAN;WiMAX","Mobile antennas;Mobile handsets;Antenna measurements;Antenna feeds;Reflector antennas;Couplings","3G mobile communication;cellular radio;Long Term Evolution;loop antennas;microwave antennas;mobile antennas;mobile handsets;multifrequency antennas;WiMax;wireless LAN","internal multi-band loop antenna;multiple mobile handset operation;folded loop antenna;GSM850/900 mobile phone operation;DCS mobile phone operation;PCS mobile phone operation;UMTS mobile phone operation;LTE2300/2500 mobile phone operation;WLAN mobile phone operation;WiMAX mobile phone operation;single folded meander loop track;T-shape back-coupling element;small mobile phone application;HFSS software;electromagnetic simulation;frequency 2.4 GHz;frequency 2.5 GHz;size 120 mm;size 60 mm;size 6.5 mm;size 15 mm","","2","12","","","","","","IEEE","IEEE Conferences"
"A neural network system for designing new stretch fabrics","H. Alibi; F. Fayala; A. Jemni; X. Zeng","Laboratory of Study of the Thermal and Energy Systems (LESTE) National School Engineers of Monastir, University of Monastir Avenue IBN ELJAZZAR, 5019 Monastir, Tunisia; Laboratory of Study of the Thermal and Energy Systems (LESTE) National School Engineers of Monastir, University of Monastir Avenue IBN ELJAZZAR, 5019 Monastir, Tunisia; Laboratory of Study of the Thermal and Energy Systems (LESTE) National School Engineers of Monastir, University of Monastir Avenue IBN ELJAZZAR, 5019 Monastir, Tunisia; GEMTEX research laboratory National School Of Arts And Textiles Industries (ENSAIT) Roubaix, University North Lille of France, Avenue de l'Hermitage, Roubaix Cedex, France","2013 International Conference on Electrical Engineering and Software Applications","","2013","","","1","6","In this paper, an artificial neural network (ANN) aided system for designing knit stretch materials based on the virtual leave one out approach is presented. This system aims at modeling the relation between functional properties (outputs) and structural parameters (inputs) of knitted fabrics made from pure yarn cotton (cellulose) and viscose (regenerated cellulose) fibers and plated knitted with elasthane (Lycra) fibers. Knitted fabric structure type, yarn count, yarn composition, gauge, elasthane fiber proportion (%), elasthane yarn linear density, fabric thickness and fabric areal density, were used as inputs to ANN model. These models have been validated by a testing data. The developed neural model allows designers to optimize the structure of knit stretch materials according to the functional properties.","","978-1-4673-6301-3978-1-4673-6302-0978-1-4673-6300","10.1109/ICEESA.2013.6578362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578362","Comfort-stretch;Knit fabrics;Elongation;Elastic recovery;Artificial neural network;Virtual leave one out","Fabrics;Yarn;Training;Artificial neural networks;Neurons;Cotton","cotton fabrics;design engineering;fabrics;neural nets;polymer fibres;production engineering computing;textile fibres;woven composites;yarn","design;stretch fabrics;artificial neural network;ANN;knit stretch materials;yarn cotton;regenerated cellulose fibers;viscose fibers;plated knitted fabrics;Lycra;yarn count;yarn composition;gauge;elasthane fiber proportion;elasthane yarn linear density;fabric thickness;fabric areal density","","","26","","","","","","IEEE","IEEE Conferences"
"Simulation techniques for an Efficient Use of Resources: An overview for the steelmaking field","I. Matino; V. Colla; L. Romaniello; F. Rosito; L. Portulano","TeCIP Institute, Scuola Superiore Sant'Anna, SSSA, Pisa, Italy; TeCIP Institute, Scuola Superiore Sant'Anna, SSSA, Pisa, Italy; Innovation and Technological Development, ILVA SpA, Taranto, Italy; Innovation and Technological Development, ILVA SpA, Taranto, Italy; Innovation and Technological Development, ILVA SpA, Taranto, Italy","2015 World Congress on Sustainable Technologies (WCST)","","2015","","","48","54","Resource efficiency has always been a hot topic for the steel industry, due to the large amounts of primary raw materials, by products and water, which are handled in the steelmaking cycle as well as the considerable energy consumptions. The ever more stringent environmental regulations also contribute to pressurize the iron and steel industry to strengthen further its research activities fostering an optimal management and exploitation of resources. Thus important research efforts are spent in order to find the best way to recover wastes and wastewater also involving the implementation of retrofit actions and major plant modifications allowing also the reduction of primary raw material consumption and of waste disposal. In order to achieve such ambitious objective, which implies both environmental benefits and cost savings, process integration is a powerful tool. The assessment of the viability of process integration solution necessarily include a final experimental stage to be performed at plant level, which can be expensive or unsafe. To this aim process simulation can be a useful alternative to accomplish preliminary feasibility studies allowing a deep analysis of all the aspect of a particular technology or retrofit actions, which also allows testing conditions that are very difficult to replicate within practical experiments. Different levels of detail can be achieved and a comprehensive analysis can be carried out. The present paper provides an overview of the simulation techniques exploited during the project entitled ""Efficient Use of Resources in Steel Plants through Process Integration"" to the aim of optimizing the use of resources in an Italian integrated steelmaking plant. In particular, different case studies are proposed and the obtained meaningful and practical results are presented and discussed.","","978-1-9083-2054-4978-1-9083-2053","10.1109/WCST.2015.7415115","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415115","Process simulation;resources efficiency;recycling;steelmaking industry sustainability;process integration","Solids;Analytical models;Wastewater;Software;Data models;Mathematical model;Solid modeling","industrial plants;maintenance engineering;raw materials;steel industry;waste disposal;wastewater treatment","resource efficiency;steel industry;primary raw materials;steelmaking cycle;iron industry;optimal resource management;optimal resource exploitation;wastewater recovery;retrofit action implementation;primary raw material consumption reduction;waste disposal reduction;cost savings;process integration solution;plant level;technology actions;retrofit actions;steel plants;resource optimization;Italian integrated steelmaking plant","","4","38","","","","","","IEEE","IEEE Conferences"
"Design and implementation of high-level compute on Android systems","Hung-Shuen Chen; Jr-Yuan Chiou; C. Yang; Y. Wu; Wei-chung Hwang; Hao-Chien Hung; Shih-wei Liao","National Taiwan University, Taipei, Taiwan; National Taiwan University, Taipei, Taiwan; Industrial Technology Research Institute, Hsinchu, Taiwan; Industrial Technology Research Institute, Hsinchu, Taiwan; Industrial Technology Research Institute, Hsinchu, Taiwan; National Taiwan University, Taipei, Taiwan; National Taiwan University, Taipei, Taiwan","The 11th IEEE Symposium on Embedded Systems for Real-time Multimedia","","2013","","","96","104","As Android devices come with various CPU and GPU cores, the demand for effective parallel computing across-the-board increases. In response, Google has released Renderscript to leverage parallel computing while maintaining portability. However, the adoption has been slow - We hardly see any Renderscript apps on Google Play. In the meantime, the proliferation of heterogeneous cores inside a single device calls for a higher-level, more developer-friendly parallel language. Since most Android developers already use Java, we develop the first Java-based compute system on Android called Android-Aparapi. Android-Aparapi facilitates programmers' adoption of compute by obviating the need of learning a new language like Renderscript, thus the software can start catching up with the hardware trend of doubling the number of cores periodically. Furthermore, Android-Aparapi is a better defined API than the original Aparapi. We support comprehensive set of data types and their array forms in terms of Java objects. In addition, we propose innovative optimizations that effectively reduce the Java-Native Interface overheads. Finally, we develop an Android-Aparapi benchmark suite by extending Rodinia benchmark to Android. The Java's Thread Pool version of the suite runs 3 times slower than the Android-Aparapi version. This demonstrate the effectiveness of our high-level compute system. Furthermore, we compare our Android-Aparapi version with the existing lower-level OpenCL version and show that the performance is comparable (Our performance is at 88% of OpenCL's). In short, we achieve higher-level abstraction without sizable losing performance.","2325-1271;2325-1301","978-1-4799-1284","10.1109/ESTIMedia.2013.6704508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6704508","","Java;Androids;Humanoid robots;Benchmark testing;Graphics processing units;Kernel;Arrays","Android (operating system);application program interfaces;Java","Android systems;Java-based compute system;Android-Aparapi;renderscript;API;innovative optimizations;Java objects;Rodinia benchmark;Java thread pool version;high-level compute system;lower-level OpenCL","","5","20","","","","","","IEEE","IEEE Conferences"
"Fast Dynamic Binary Rewriting for flexible thread migration on shared-ISA heterogeneous MPSoCs","G. Georgakoudis; D. S. Nikolopoulos; H. Vandierendonck; S. Lalis","Queen's University of Belfast, UK; Queen's University of Belfast, UK; Queen's University of Belfast, UK; University of Thessaly, Greece","2014 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS XIV)","","2014","","","156","163","Heterogeneous MPSoCs where different types of cores share a baseline ISA but implement different operational accelerators combine programmability with flexible customization. They hold promise for high performance under power and area limitations. However, transparent binary execution and dynamic scheduling is hard on those platforms. The stateof-the-art approach for transparent accelerated execution is fault-and-migrate (FAM): when a thread executes an accelerating instruction unavailable on the host core, it is forcibly migrated to an accelerating core which implements the instruction natively. Unfortunately, this approach prohibits dynamic scheduling through flexible thread migration, which is essential to any asymmetric platform for efficient utilization of heterogeneous resources. We present two distinct binary-level techniques - Dynamic Binary Rewriting (DBR) and Dynamic Binary Translation (DBT) - which enable selective acceleration, while preserving transparent thread execution and migration, to any core in the system, at any point in time. DBR rewrites binary code to exploit any accelerating instructions available in the host core. DBT implements a-fault-and-rewrite scheme, which sets up trampolines to emulation routines for these accelerating instructions which are not available on the host core. Both methods customize binary code on demand, enabling flexible migration. We evaluate the overhead of DBR and DBT against FAM on a real hardware shared-ISA MPSoC prototype. Experiments with single-thread programs show flexible migration is possible with manageable overhead. We measure the performance of our binary-level techniques by artificially triggering periodic thread migration between a Base and an accelerating (ACC) core. Periodic migration, without aiming for optimized scheduling, results in an average slowdown of about 40% under DBR or about 10% under DBT, compared to FAM driven scheduling. We also show results for a speedup proportional dynamic scheduler, enabled by our techniques, using multi-program workloads. In this case, up to 50% faster execution times can be achieved by leveraging flexible thread migration.","","978-1-4799-3770","10.1109/SAMOS.2014.6893207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6893207","","Distributed Bragg reflectors;Acceleration;Emulation;Benchmark testing;Software;Dynamic scheduling;Computer architecture","multiprocessing systems;processor scheduling;system-on-chip","dynamic binary rewriting;DBR;dynamic binary translation;DBT;flexible thread migration;shared-ISA heterogeneous MPSoCs;dynamic scheduling;fault-and-migrate;FAM","","1","20","","","","","","IEEE","IEEE Conferences"
"Enabling a Quantum Monte Carlo application for the DEEP architecture","A. Emerson; F. Affinito","Supercomputing Applications and Innovation (SCAI), Cineca, 40033 Casalecchio di Reno, Italy; Supercomputing Applications and Innovation (SCAI), Cineca, 40033 Casalecchio di Reno, Italy","2015 International Conference on High Performance Computing & Simulation (HPCS)","","2015","","","453","457","In the DEEP project a prototype Exascale system consisting of a standard Intel Xeon cluster linked to a “Booster” part containing Intel Xeon Phi nodes connected in a high-speed network, is being designed and constructed. In order to evaluate this novel architecture, expected to be available in the second half of 2015, a number of grand challenge applications in computational science and engineering are being modified and optimised. In this study we report on the efforts made by the Cineca project partner and DEEP support staff to enable one of these applications, the TurboRVB Quantum Monte Carlo simulation program, which can be used to study complex phenomena in materials such as superconductivity. The modified code, based on an implementation of the OmpSs offload task model, has been successfully tested on the MareNostrum supercomputer at the Barcelona Supercomputing Center.","","978-1-4673-7813-0978-1-4673-7812-3978-1-4673-7811","10.1109/HPCSim.2015.7237075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7237075","Exascale Systems;Modeling and Simulation using HPC Systems;Parallelization of Simulation","Monte Carlo methods;Computer architecture;Lattices;Software;Programming;Microwave integrated circuits;Prototypes","Monte Carlo methods;parallel machines;quantum computing","DEEP architecture;Exascale system;Intel Xeon cluster;Intel Xeon Phi nodes;high-speed network;Cineca project;TurboRVB Quantum Monte Carlo simulation program;OmpSs offload task model;MareNostrum supercomputer;Barcelona Supercomputing Center","","","6","","","","","","IEEE","IEEE Conferences"
"A novel big data architecture in support of ADS-B data analytic","E. Boci; S. Thistlethwaite","Exelis, Herndon, VA; Exelis, Herndon, VA","2015 Integrated Communication, Navigation and Surveillance Conference (ICNS)","","2015","","","C1-1","C1-8","The first building block of the Federal Aviation Administration's (FAA) Next Generation Air Transportation System (NextGen) initiative to modernize the US national airspace system (NAS) was the implementation of the Automatic Dependent Surveillance-Broadcast (ADS-B) ground infrastructure. A primary aspect of the ADS-B program design is the terrestrial radio station infrastructure. It determined the terrestrial radio stations layout throughout the US and was optimized to meet system performance, safety and security in the NAS. In March 2014, the FAA completed the nationwide infrastructure upgrade, enabling air traffic controllers to track aircraft with greater accuracy and reliability, while giving pilots more information in the cockpit. More than 650 ADS-B radios communicate with equipped aircraft, supporting the new satellite-based surveillance system. Currently, the ADS-B system ingests processes and stores large data sets, while operating at ten percent capacity. As aircraft avionics equipage increases, the volume of data and storage needs will increase beyond our existing system's capacity and processing capability. A new, Hadoop-based architecture was tested to ingest and analyze billions of CAT033 reports in minutes. This paper presents the “Big Data” approach that was adopted to support fast analytics of large ADS-B data volume.","2155-4943;2155-4951","978-1-4799-8952-2978-1-4673-7549","10.1109/ICNSURV.2015.7121218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7121218","","Computer architecture;Big data;Hardware;Lakes;Aircraft;Software;Aerospace electronics","air traffic control;avionics;Big Data;data analysis;driver information systems;parallel processing;surveillance","Big Data architecture;ADS-B data analytic;federal aviation administration;FAA;next generation air transportation system;NextGen;US national airspace system;NAS;automatic dependent surveillance-broadcast ground infrastructure;ADS-B ground infrastructure;ADS-B program design;terrestrial radio station infrastructure;NAS security;air traffic controllers;aircraft track;ADS-B radios;satellite-based surveillance system;ADS-B system;aircraft avionics;data storage;Hadoop-based architecture;CAT033;Big Data approach;ADS-B data volume","","2","12","","","","","","IEEE","IEEE Conferences"
"Modeling the energy consumption of HEVC P- and B-frame decoding","C. Herglotz; D. Springer; A. Kaup","Multimedia Communications and Signal Processing Friedrich-Alexander-Universita¨t Erlangen-Nu¨rnberg (FAU), Cauerstr. 7, 91058 Erlangen, Germany; Multimedia Communications and Signal Processing Friedrich-Alexander-Universita¨t Erlangen-Nu¨rnberg (FAU), Cauerstr. 7, 91058 Erlangen, Germany; Multimedia Communications and Signal Processing Friedrich-Alexander-Universita¨t Erlangen-Nu¨rnberg (FAU), Cauerstr. 7, 91058 Erlangen, Germany","2014 IEEE International Conference on Image Processing (ICIP)","","2014","","","3661","3665","Video decoding on portable devices such as smartphones or tablet PCs requires a considerable amount of battery power, shortening their operating time significantly. Hence, tools aiming at minimizing the decoding energy are of special interest. To this end, this paper presents a novel model capable of estimating the energy consumed when decoding an HEVC-coded video. This information can be used to optimize implementations and improve the encoding procedure. An accurate and a more convenient simple model is proposed that, for the evaluated set of test videos, achieved an average relative estimation error of 2.34% and 3.63%, respectively.","1522-4880;2381-8549","978-1-4799-5751","10.1109/ICIP.2014.7025743","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025743","HEVC;Decoder;Energy consumption","Decoding;Streaming media;Energy measurement;Energy consumption;Software;Estimation;Encoding","decoding;telecommunication power management;video coding","HEVC energy consumption estimation;P-frame decoding;B-frame decoding;video decoding;battery power;decoding energy minimization;encoding procedure improvement;relative estimation error","","8","8","","","","","","IEEE","IEEE Conferences"
"Implementation of a platform for day-ahead market modeling: Case study of Sicilian future scenarios","G. Pecoraro; G. M. Tina; E. M. Carlini; C. Quaciari","DIEES - University of Catania, Catania, Italy; DIEES - University of Catania, Catania, Italy; Terna Rete Italia S.p.A., Rome, Italy; Terna Rete Italia S.p.A., Rome, Italy","2015 12th International Conference on the European Energy Market (EEM)","","2015","","","1","5","Day-ahead price is characterized by high volatility and unpredictability, so it represents a major risk for generating companies, not only those that rely on renewable generation but also those that manage thermoelectric power plants. The typical approach employed by Gencons to address their trading strategy is to simulate energy and service markes under uncertainties to characterize the probability distribution of the future income and, then, optimize the company portfolio to maximize its certainty equivalent. However markets modeling and simulation is a big challenge due to both bidding and clearing rule and dependency on parameters that are difficult to predict, e.g., GDP growth, demand variation, entrance of new market players, as well as the structure of transmission grid. On this regard Italian transmission grid is about to change, specifically a new T-line between Sicily and Continent is going to be operated. So there will be a substantial change both in Sicilian zonal price and in the PUN (single National price for bid). In this article we describe the developed simulation tool of the Italian day-ahead market according with the last rules of the Italian market. This model has been tested extensively against real market data. The results about specific days are reported and the errors evaluated. Then different scenarios have been simulated, such as: flow rate of the global connection between Sicily and the Continent (from 250 MW to 1000 MW with steps of 250 MW) and the effect of the abolition of the single buyer (named Acquirente Unico - AU).","2165-4093;2165-4077","978-1-4673-6692","10.1109/EEM.2015.7216781","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7216781","Day-Ahead Market;modeling;zonal price;congestion","Gold;Continents;Software;Electricity supply industry;Load flow;Predictive models;Economics","power markets;power system simulation","day-ahead market modeling;probability distribution;transmission grid","","1","3","","","","","","IEEE","IEEE Conferences"
"Designing of anfis network for wide area power system monitoring and protection network using matlab simulink tools","A. S. a. Stalin; M. Shakeel; Revanth R.; Durairaj D.","Electrical and Electronics Engineering, Sri Manakula Vinayagar Engineering College, Pondicherry, India; Electrical and Electronics Engineering, Sri Manakula Vinayagar Engineering College, Pondicherry, India; Electrical and Electronics Engineering, Sri Manakula Vinayagar Engineering College, Pondicherry, India; Electrical and Electronics Engineering, Sri Manakula Vinayagar Engineering College, Pondicherry, India","2015 International Conference on Power and Advanced Control Engineering (ICPACE)","","2015","","","280","284","Reliability of the system, efficiency of the system, fault correction and operational performance of the network are the key characteristics of a power system. The issues of deregulation trend in the industry and the requirement of better network monitoring, leads to the development of the solutions for wide area monitoring, protection and control, than the currently used methods which are mostly good for local area monitoring, protection and control In our paper, a wide area power system network is designed and Phasor Measuring Unit (PMU) is used as monitoring system and the protection of the network is done by using Slip back neural control system and the proposed model is then tested under various operational parameters. Our proposed model and control technique implemented offers a smart tool for wide area system which has good optimization in its performance.","","978-1-4799-8371-1978-1-4799-8370","10.1109/ICPACE.2015.7274958","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7274958","Wide area monitoring protection and control;Phasor measurement unit;MATLAB;Wide area monitoring;Wide Area Monitoring;Global positioning system;null deflection zone and Slip back neural control system","Power systems;Monitoring;Phasor measurement units;Software packages;Control systems;Global Positioning System;Mathematical model","phasor measurement;power system control;power system protection","wide area power system monitoring;protection network;ANFIS network;Matlab simulink tools;phasor measuring unit;PMU;slip back neural control system;smart tool","","","17","","","","","","IEEE","IEEE Conferences"
"A Co-processing Method Based on Warm Standby Systems","L. Bo; X. Feng; Y. Yunhong","NA; NA; NA","2014 IEEE Symposium on Computer Applications and Communications","","2014","","","109","113","Dual-computer warm standby redundant architecture is one of the most commonly used architectures in redundant fault tolerance. Using this architecture, the computing results of the backup computer is unused when it has no power although the backup computer is keep running that is a waste of computing resource of the backup computer. This paper proposes a co-processing method which makes rational use of computing resource by taking advantage of the communication unit, and increases the output frequency of the system's optimized module. The test results show that this method improves the performance of the system with the assurance of the reliability.","","978-0-7695-5319","10.1109/SCAC.2014.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6913178","warm standby;dual-computer;reliability model;co-processing;fault tolerance","Computers;Computer architecture;Temperature control;Software reliability;Fault detection;Acceleration","computer architecture;coprocessors;fault tolerant computing;multiprocessing systems","coprocessing method;warm standby systems;dual computer warm standby redundant architecture;redundant fault tolerance;backup computer;communication unit;computing resource;output frequency","","","8","","","","","","IEEE","IEEE Conferences"
"Researches on the vibration characteristics of a new type of non-resonant piezoelectric stack motors","X. Chen; Y. Liang; W. Huang","Key Laboratory of Mechanics and Control of Mechanical Structures. Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Mechanics and Control of Mechanical Structures. Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China; Key Laboratory of Mechanics and Control of Mechanical Structures. Nanjing University of Aeronautics and Astronautics, Nanjing 210016, China","2013 Symposium on Piezoelectricity, Acoustic Waves, and Device Applications","","2013","","","1","4","To research on the mechanism of non-resonant linear piezoelectric motors, longitudinal vibration model and pseudo-rigid-body model were established for the stator of motor through the analysis of the construction and operating process of the motor. Then the longitudinal vibration differential equation and transverse vibration differential equation of the drive tip were produced. Thus longitudinal displacement function and transverse displacement function can be deduced. Utilizing the above researches, formula of average output force in a period was proposed for the motor. Finally, the prototype was designed and fabricated, vibration displacement is tested by laser displacement sensor and vibration displacement curves were fitted through origin software. Thus experiments on the longitudinal amplitude of drive tip were carried on. The output force of motor under different excitation conditions were measured with force measurement unit. Results proved the vibration characteristic theory of stator that the longitudinal amplitude of drive tip increases with excitation voltage and that the output force of motor increases with the longitudinal amplitude of drive tip. The output force prototype can be up to 3.5 N when the longitude amplitude of the stator is 0.95μm. The above researches can provide theoretical basis for structural optimization for the stator of motor.","","978-1-4799-3288-7978-1-4799-3289","10.1109/SPAWDA.2013.6841121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6841121","Vibration characteristics;Non-resonant;Piezoelectric stack;Linear motor","Artificial neural networks","differential equations;force measurement;linear motors;piezoelectric motors;sensors;stators;vibrations","nonresonant linear piezoelectric stack motors;pseudo-rigid-body model;stator;longitudinal vibration differential equation;transverse vibration differential equation;longitudinal displacement function;transverse displacement function;laser displacement sensor;vibration displacement curves;force measurement unit;structural optimization","","","14","","","","","","IEEE","IEEE Conferences"
"Bringing model based ventilation therapy to the bedside","T. Nguyen; T. Lehmann; J. Kretschmer; K. Möller","Institute of Technical Medicine Furtwangen Univerisity Villingen Schwenningen, 78054 Germany; Institute of Technical Medicine Furtwangen Univerisity Villingen Schwenningen, 78054 Germany; Institute of Technical Medicine Furtwangen Univerisity Villingen Schwenningen, 78054 Germany; Institute of Technical Medicine Furtwangen Univerisity Villingen Schwenningen, 78054 Germany","2013 ICME International Conference on Complex Medical Engineering","","2013","","","666","669","Modern approaches to respiratory therapy based on computer modeling ought to keep up with the latest technologies. In the era of tablet PCs and smartphones, lung diagnoses and therapy optimization could be brought to patients right at their bedside. To this end, previously presented simulations of patient physiology [1] were redesigned to run on mobile devices. Moreover, a socket communication over WiFi was established between a desktop computer and a tablet PC so that data read from the ventilator (Evita XL that is connected to the computer) can be transferred to the tablet. In order to run the application on an Android operating system, the previously developed MATLAB code must be rewritten in Java. A Graphical User Interface (GUI) was designed so that one can choose any combination of a ventilation mode (Volume Control, Pressure Control, SIMV or BiP AP), a combination of submodels describing lung mechanics, gas exchange and cardiovascular dynamics. Moreover, simulation can be adapted to individual patients by selecting lung diseases (chronic obstructive pulmonary disease or adult respiratory distress syndrome), as well as enter parameters for those selections. Outputs of interest such as airway pressure or flow can be reviewed after running the simulation thanks to an android library called androidplot. The presented software is constantly improved and extended to further implement additional ventilation modes and patient models. These initial tests show that mobile devices can easily be integrated into today's medical decision support. More sophisticated and less efficient calculations can be performed on servers running in parallel to the handheld system. The mathematical models of human respiratory system [1] and the simulation model for patient ventilator interaction [2] were successfully implemented in mobile devices.","","978-1-4673-2971-2978-1-4673-2970-5978-1-4673-2969","10.1109/ICCME.2013.6548333","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6548333","","Mathematical model;Androids;Humanoid robots;MATLAB;Ventilation;Lungs;Sockets","biomechanics;diseases;graphical user interfaces;lung;mathematics computing;operating systems (computers);patient treatment;pneumodynamics;ventilation","ventilation therapy;respiratory therapy;tablet PC;smartphones;therapy optimization;socket communication;WiFi;Evita XL;Android operating system;MATLAB code;graphical user interface;GUI;volume control;pressure control;SIMV;BiP AP;lung mechanics;gas exchange;cardiovascular dynamics;lung diseases;chronic obstructive pulmonary disease;adult respiratory distress syndrome;androidplot;medical decision support","","","4","","","","","","IEEE","IEEE Conferences"
"Power aware modelling and design","","","2015 Forum on Specification and Design Languages (FDL)","","2015","","","1","1","This special session will explore the different constraints in power aware design as well as different modelling approaches to overcome them. The session will start with a system level power consumption modelling approach that enables the simulation of power consumption transients and their effects on AMS subsystems. The next presentation will introduce a model to estimate and benchmark wireless sensor nodes energy lifetime, including power consumption and battery models. The third presentation will show an approach to measure energy and power consumption in multi-core Systems-on-Chip in order to optimise software. Finally the last presentation will analyse near-threshold logic circuits and its vulnerabilities and will propose models of error-aware logic circuits.","1636-9874","978-1-4673-7735-5978-1-4673-7734","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7306091","","Power demand;Integrated circuit modeling;Benchmark testing;Wireless sensor networks;Logic circuits;Transient analysis;Wireless communication","","","","","","","","","","","IEEE","IEEE Conferences"
"S-BM: A benchmark suite for multi-tenant supplier relationship management service","Z. Di; C. Pu; S. Liu; X. Hao","School of Computer Science and Technology, Shandong University, Jinan 250101, China; Center for Experimental Research in Computer System, Georgia Institute of Technology, Atlanta, USA; School of Computer Science and Technology, Shandong University, Jinan 250101, China; School of Computer Science and Technology, Shandong University, Jinan 250101, China","2013 10th International Conference on Service Systems and Service Management","","2013","","","692","697","Scalability is one of the many challenges in designing and operating SaaS-type applications, which demands dynamic generation, composition, deployment, and monitoring of applications. A fundamental problem that confronts such applications is the efficient approach to evaluate the performance of the applications and resource utilization before they actually deployed. In this paper, S-BM, a benchmark suite for supplier relationship management (SRM) applications is presented in terms of architecture, workloads, and key metrics. The complete implementation description and experiments in typical workloads are also presented. S-BM provides support for two key considerations when deploy SRM applications for new tenants: performance changes according to different business scale and performance changes when deployed in different hosting environments. S-BM generates workloads adapts different tenant's requirements as well as their different business scales. Results from the emulations show that S-BM can help evaluating the performances and quantifying the optimized deployment efficiently.","2161-1890;2161-1904","978-1-4673-4843-0978-1-4673-4434","10.1109/ICSSSM.2013.6602595","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6602595","benchmark;SRM;performance;SaaS","Servers;Business;Benchmark testing;Databases;Time factors;Synchronization","cloud computing;supply chain management","S-BM benchmark suite;multitenant supplier relationship management service;SaaS-type application;software-as-a-service;SRM application;business scale;hosting environment;tenant requirement","","1","18","","","","","","IEEE","IEEE Conferences"
"A distributed algorithm for virtual traffic lights with IEEE 802.11p","A. Bazzi; A. Zanella; B. M. Masini; G. Pasolini","CNR-IEIIT, Bologna, Italy; CNR-IEIIT, Bologna, Italy; CNR-IEIIT, Bologna, Italy; DEI, University of Bologna, Italy","2014 European Conference on Networks and Communications (EuCNC)","","2014","","","1","5","A virtual traffic light (VTL) is a mechanism that allows vehicles to autonomously solve priorities at road junctions in the absence of fixed infrastructures (i.e., conventional traffic lights). To develop an effective VTL system, communication between vehicles is a crucial factor and can be handled either using cellular infrastructure or adopting a vehicle-to-vehicle (V2V) communication paradigm. In this paper we present the design, the implementation, and the field trial of a VTL which exploits V2V communications based on IEEE 802.11p. Specif-ically, we propose a decentralized algorithm, that adopts both broadcast signaling and unicast messages to assign priorities to the vehicles approaching intersections, thus preventing accidents and reducing traffic congestions. The algorithm has been tested both in a controlled laboratory environment and in a field trial with equipped vehicles.","","978-1-4799-5280","10.1109/EuCNC.2014.6882621","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6882621","","Junctions;Vehicles;Roads;Nickel;Unicast;Databases;Software algorithms","cellular radio;road accidents;road safety;road traffic;wireless LAN","distributed algorithm;virtual traffic lights with;VTL system;IEEE 802.11p;road junctions;cellular infrastructure;vehicle-to-vehicle communication paradigm;V2V communication paradigm;broadcast signaling;unicast messages;accident prevention;traffic congestion reduction;road safety","","4","9","","","","","","IEEE","IEEE Conferences"
"SociaLite: An Efficient Graph Query Language Based on Datalog","J. Seo; S. Guo; M. S. Lam","Department of Computer Science, Stanford University, Stanford, 94305 CA; Department of Computer Science, Stanford University, Stanford, 94305 CA; Department of Computer Science, Stanford University, Stanford, 94305 CA","IEEE Transactions on Knowledge and Data Engineering","","2015","27","7","1824","1837","With the rise of social networks, large-scale graph analysis becomes increasingly important. Because SQL lacks the expressiveness and performance needed for graph algorithms, lower-level, general-purpose languages are often used instead. For greater ease of use and efficiency, we propose SociaLite, a high-level graph query language based on Datalog. As a logic programming language, Datalog allows many graph algorithms to be expressed succinctly. However, its performance has not been competitive when compared to low-level languages. With SociaLite, users can provide high-level hints on the data layout and evaluation order; they can also define recursive aggregate functions which, as long as they are meet operations, can be evaluated incrementally and efficiently. Moreover, recursive aggregate functions make it possible to implement more graph algorithms that cannot be implemented in Datalog. We evaluated SociaLite by running nine graph algorithms in total; eight for social network analysis (shortest paths, PageRank, hubs and authorities, mutual neighbors, connected components, triangles, clustering coefficients, and betweenness centrality) and one for biological network analysis (Eulerian cycles). We use two real-life social graphs, LiveJournal and Last.fm, for the evaluation as well as one synthetic graph. The optimizations proposed in this paper speed up almost all the algorithms by 3 to 22 times. SociaLite even outperforms typical Java implementations by an average of 50 percent for the graph algorithms tested. When compared to highly optimized Java implementations, SociaLite programs are an order of magnitude more succinct and easier to write. Its performance is competitive, with only 16 percent overhead for the largest benchmark, and 25 percent overhead for the worst case benchmark. Most importantly, being a query language, SociaLite enables many more users who are not proficient in software engineering to perform network analysis easily and efficiently.","1041-4347;1558-2191;2326-3865","","10.1109/TKDE.2015.2405562","National Science Foundation; Stanford MobiSocial Computing Laboratory; AVG; Google; ING Direct; Nokia; Sony Ericsson; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045548","Datalog;Query Languages;Graph Algorithms;Datalog;query languages;graph algorithms;social network analysis","Aggregates;Java;Semantics;Algorithm design and analysis;Social network services;Optimization;Arrays","DATALOG;graph theory;Java;pattern clustering;social networking (online)","LiveJournal;Last.fm;synthetic graph;Java implementations;real-life social graph;Eulerian cycles;biological network analysis;betweenness centrality;clustering coefficients;triangles;connected components;mutual neighbors;authorities;hubs;PageRank;shortest path;recursive aggregate functions;evaluation order;data layout;high-level hints;logic programming language;general-purpose languages;lower-level languages;large-scale graph analysis;social networks;Datalog;high-level graph query language;SociaLite","","3","42","","","","","","IEEE","IEEE Journals & Magazines"
"iSyn: WebGL-Based Interactive De Novo Drug Design","H. Li; K. Leung; C. H. Chan; H. L. Cheung; M. Wong","NA; NA; NA; NA; NA","2014 18th International Conference on Information Visualisation","","2014","","","302","307","We present iSyn, a WebGL-based tool for interactivede novo drug design. It features an evolutionary algorithm that automatically designs novel ligands with drug-like properties and synthetic feasibility using click chemistry. Isyn interfaces with our popular and fast molecular docking engine idock, remarkably reducing the evaluation and ranking time of drug candidates. Furthermore, inspired by our user friendly and high-performance WebGL visualizer iview, our iSyn also implements a tailor-made interactive visualizer to aid novel drug design. We believe iSyn can supplement the efforts of medicinal chemists in drug discovery research. To illustrate the utility of iSyn in generating novelligands ex nihilo, we designed predicted inhibitors of two important drug targets, which are RNA editing ligase 1(REL1) from T. Brucei, the etiological agent of African sleeping sickness, and cyclin-dependent kinase 2 (CDK2), a positive regulator of eukaryotic cell cycle progression. Results show that iSyn managed to significantly enhance the predicted binding affinity of the best generated ligand by more than 3 orders of magnitude in potency. Isyn is written in C++, Python, HTML5 and JavaScript. It is free and open source, available athttp://istar.cse.cuhk.edu.hk/iSyn.tgz. It has been tested successfully on both Linux and Windows.","1550-6037;2375-0138","978-1-4799-4103","10.1109/IV.2014.10","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6902920","Bioinformatics;Computer-Aided Drug Discovery;Evolutionary Algorithm;WebGL Visualization","Drugs;Proteins;Visualization;Compounds;Libraries;Surface resistance","application program interfaces;C++ language;chemistry computing;drugs;evolutionary computation;hypermedia markup languages;inhibitors;interactive systems;Java;public domain software;RNA","iSyn;interactive de novo drug design;evolutionary algorithm;drug-like properties;click chemistry;molecular docking engine;idock;user friendly WebGL visualizer iview;tailor-made interactive visualizer;drug discovery research;ligands ex nihilo;inhibitors;RNA editing ligase 1;REL1;T brucei;etiological agent;African sleeping sickness;cyclin-dependent kinase 2;CDK2;eukaryotic cell cycle progression;ligand binding affinity;C++;Python;HTML5;JavaScript;open source;Linux;Windows","","1","19","","","","","","IEEE","IEEE Conferences"
"GPU Parallel Implementation of Support Vector Machines for Hyperspectral Image Classification","K. Tan; J. Zhang; Q. Du; X. Wang","Jiangsu Key laboratory of Resources and Environment Information Engineering, School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou City, China; Jiangsu Key laboratory of Resources and Environment Information Engineering, School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou City, China; Department of Electrical and Computer Engineering, Mississippi State University, Starkville, MS, USA; School of Information and Electrical Engineering, China University of Mining and Technology, Xuzhou City, China","IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing","","2015","8","10","4647","4656","Support vector machine (SVM) is considered as one of the most powerful classifiers for hyperspectral remote sensing images. However, it has high computational cost. In this paper, we propose a novel two-level parallel computing framework to accelerate the SVM-based classification by utilizing CUDA and OpenMP. For a binary SVM classifier, the kernel function is optimized on GPU, and then a second-order working set selection (WSS) procedure is employed and optimized especially for GPU to reduce the cost of communication between GPU and host. In addition to the parallel binary SVM classifier on GPU as dataprocessing level parallelization, a multiclass SVM is addressed by a “one-against-one” approach in OpenMP, and several binary SVM classifiers are run simultaneously to conduct task-level parallelization. The experimental results show that the solver in this framework offered a speedup of 18.5× over the popular LIBSVM software in the training process for data with 200 bands, 13 classes, and 95 597 training samples, and 81.9× in the testing process for data with 103 bands, 9 classes, 1892 support vectors (SVs), and 42 776 testing samples.","1939-1404;2151-1535","","10.1109/JSTARS.2015.2453411","Natural Science Foundation of China; Fundamental Research Funds for the Central Universities; Priority Academic Program Development of Jiangsu Higher Education Institutions; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7219385","Classification;hyperspectral data;multicore processing;support vector machines (SVMs);Classification;hyperspectral data;multicore processing;support vector machines (SVMs)","Support vector machines;Graphics processing units;Hyperspectral imaging;Multicore processing;Image classification","geophysical image processing;hyperspectral imaging;image classification;parallel architectures;remote sensing","GPU parallel implementation;support vector machines;hyperspectral image classification;hyperspectral remote sensing images;novel two-level parallel computing framework;SVM-based classification;CUDA;OpenMP;binary SVM classifier;kernel function;second-order working set selection;WSS procedure;parallel binary SVM classifier;data-processing level parallelization;one-against-one approach;LIBSVM software","","15","32","","","","","","IEEE","IEEE Journals & Magazines"
"Complexity analysis of an HEVC decoder based on a digital signal processor","F. Pescador; M. Chavarrias; M. J. Garrido; E. Juarez; C. Sanz","Centre of SW Technologies and Multimedia Systems (CITSEM) of the Universidad Politecnica de Madrid, Spain; Centre of SW Technologies and Multimedia Systems (CITSEM) of the Universidad Politecnica de Madrid, Spain; Centre of SW Technologies and Multimedia Systems (CITSEM) of the Universidad Politecnica de Madrid, Spain; Centre of SW Technologies and Multimedia Systems (CITSEM) of the Universidad Politecnica de Madrid, Spain; Centre of SW Technologies and Multimedia Systems (CITSEM) of the Universidad Politecnica de Madrid, Spain","IEEE Transactions on Consumer Electronics","","2013","59","2","391","399","High Efficiency Video Coding (HEVC) is a new video coding standard created by the JCT-VC group within ISO/IEC and ITU-T. HEVC is targeted to provide the same quality as H.264 at about half of the bit-rate and will replace soon to its predecessor in multimedia consumer applications. Up to now, only a few decoder implementations have been reported, most of them oriented to carry out a complexity analysis. In this paper, a DSP-based implementation of the HEVC HM9.0 decoder is presented. Up to the best of our knowledge, it is the first DSP-based implementation shown in the scientific literature. Several tests have been carried out to measure the decoder performance and the computational load distribution among its functional blocks. These results have been compared with the ones obtained with the decoder implementations reported up to date. Finally, based on the results obtained in previous works regarding software optimization of DSP-based decoders, realtime could be achieved for SD formats with a single DSP after optimizing our HEVC decoder. For HD formats, multi-DSP technology will be needed.","0098-3063;1558-4127","","10.1109/TCE.2013.6531122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6531122","HEVC;DSP;decoder;complexity;H.264.","Decoding;Digital signal processing;Complexity theory;Standards;Artificial intelligence;Software;Computer architecture","code standards;computational complexity;digital signal processing chips;multimedia communication;video coding","complexity analysis;digital signal processor;high efficiency video coding;video coding standard;JCT-VC group;ISO/IEC;ITU-T;H.264;multimedia consumer application;DSP-based implementation;HEVC HM9.0 decoder;decoder performance measure;computational load distribution;functional blocks;DSP-based decoder;SD formats;HD formats;multiDSP technology","","21","23","","","","","","IEEE","IEEE Journals & Magazines"
"Using phoebus data transfer accelerator in cloud environments","M. Zhang; E. Kissel; M. Swany","School of Informatics and Computing, Indiana University, Bloomington, 47405, USA; School of Informatics and Computing, Indiana University, Bloomington, 47405, USA; School of Informatics and Computing, Indiana University, Bloomington, 47405, USA","2015 IEEE International Conference on Communications (ICC)","","2015","","","351","357","The quality of data exchange in cloud computing applications relies on the connection performance between user clients and their cloud storage providers, and is often dependent on the wide area network (WAN) properties among data centers. For certain classes of applications, it can be crucial to provide an end-to-end solution that accelerates large data transfers and improves overall user experience. The development and deployment of WAN optimization technology has been investigated for improving application performance in heterogeneous, multi-domain environments. WAN optimization devices and services implement a number of approaches for performance improvement, and one key insight is that in contrast to traditional end-to-end TCP connections, middleboxes that segment and optimize transport-layer connections can improve the performance of wide area data transfers. In the context of dynamic cloud computing environments, there is an obvious target for implementations of WAN optimization as Network Function Virtualization (NFV), where the flexibility of virtualized cloud environments can be exploited. This paper describes recent developments and experimentation of our Phoebus WAN accelerator framework. We introduce a software suite that includes new Phoebus clients that operate with the Phoebus Gateway network. We test and discuss virtualizing Phoebus Gateways to provide acceleration services in cloud data transfers. Use cases and performance evaluations are conducted on FutureGrid and Internet2 testbeds, and we demonstrate the effectiveness of a virtualized Phoebus deployment.","1550-3607;1938-1883","978-1-4673-6432-4978-1-4673-6431","10.1109/ICC.2015.7248346","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7248346","","Wide area networks;Data transfer;Logic gates;Libraries;Cloud computing;Protocols;Virtualization","cloud computing;computer centres;electronic data interchange;internetworking;wide area networks","data exchange quality;cloud computing applications;user client;cloud storage provider;wide area network properties;WAN optimization technology;data center;overall user experience improvement;phoebus data transfer accelerator;cloud environment;multidomain environmen;heterogeneous environment;dynamic cloud computing environment;network function virtualization;NFV;virtualized cloud environment;phoebus WAN accelerator framework;phoebus gateway network;phoebus client","","1","26","","","","","","IEEE","IEEE Conferences"
"Topics and Terms Mining in Unstructured Data Stores","R. K. Lomotey; R. Deters","NA; NA","2013 IEEE 16th International Conference on Computational Science and Engineering","","2013","","","854","861","One of the major challenges of the ""Big Data"" epoch is unstructured data mining. The problem arises due to the storage of high-dimensional data that has no standard schema. While knowledge discovery in database (KDD) algorithms were designed for data extraction, the algorithms best fit for structured data storages. Moreover, today, at the data storage level, NoSQL databases have been deployed in response to accommodate the unstructured data. However, the over-reliance on multiple APIs by NoSQL storages hampers efficient data extraction from different NoSQL storages. Also, there are limited numbers of tools available that can perform KDD tasks on NoSQL data stores. In this work, we explore the trend in unstructured data mining and detail the future direction and challenges. Then, focusing on topics and terms extraction from NoSQL databases, we propose a tool called TouchR2, which algorithmically relies on bloom filtering and parallelization. Using the CouchDB data storage as the test case, the evaluation of TouchR2 shows high accuracy for terms extraction and organization within a much optimized duration.","","978-0-7695-5096","10.1109/CSE.2013.129","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6755309","Unstructured Data Mining;Big Data;Bloom Filtering;Terms;Topics;NoSQL;Association Rules","Databases;Association rules;Information retrieval;Information management;Data handling;Data storage systems","application program interfaces;data mining;data structures;software tools;storage management","terms mining;topics mining;unstructured data stores;Big Data epoch;unstructured data mining;high-dimensional data storage;knowledge discovery in database algorithms;KDD algorithms;data extraction;structured data storages;NoSQL databases;data storage level;multiple APIs;TouchR2 tool;bloom filtering;CouchDB data storage;terms extraction;topics extraction","","3","33","","","","","","IEEE","IEEE Conferences"
"CLAudit: Planetary-scale cloud latency auditing platform","O. Tomanek; L. Kencl","Czech Technical University in Prague, Czech; Czech Technical University in Prague, Czech","2013 IEEE 2nd International Conference on Cloud Networking (CloudNet)","","2013","","","138","146","Latency is an important, yet often underestimated aspect of the nascent Cloud-Computing scenario. A cloud service based on processing in remote datacenters may exhibit latency and jitter which may be a compound result of many various components of the remote computation and intermediate communication. Our broad vision is to design and develop tools to monitor, model and optimize the global cloud-service latency. To this end, we introduce CLAudit, a prototype planetary-scale cloud-latency auditing platform. It utilizes the experimental PlanetLab network to place globally distributed probes that periodically measure cloud-service latency at various layers of the communication stack. We present CLAudit architecture in detail and show initial test-measurements of the MicrosoftWindows Azure cloud service, demonstrating the platform's practical usefulness by showcasing a few discovered anomalous results in the cloud-service latency measurements.","","978-1-4799-0568","10.1109/CloudNet.2013.6710568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6710568","","Databases;Extraterrestrial measurements;Cloud computing;Web servers;Protocols;Prototypes","cloud computing;computer centres;software performance evaluation","nascent cloud-computing scenario;remote datacenters;jitter;remote computation;intermediate communication;global cloud-service latency;CLAudit;planetary-scale cloud-latency auditing platform;experimental PlanetLab network;CLAudit architecture;Microsoft Windows Azure cloud service;cloud-service latency measurements","","7","27","","","","","","IEEE","IEEE Conferences"
"Investigating the Resilience of Dynamic Loop Scheduling in Heterogeneous Computing Systems","N. Sukhija; I. Banicescu; F. M. Ciorba","NA; NA; NA","2015 14th International Symposium on Parallel and Distributed Computing","","2015","","","194","203","To improve the performance of complex scientific applications, dynamic loop scheduling(DLS) techniques are often employed for load balancing. However, it is a challenge to select the most resilient scheduling technique for guaranteeing optimized performance of scientific applications on large-scale computing systems. Such systems comprise widely distributed and highly heterogeneous resources, and often are prone to failures. Hence, in this work we perform a comprehensive study of resilience of DLS techniques. In our study, we employed Sim Grid-based simulations. The use of a simulation framework assists in overcoming the limits of quantifying the resilience and evaluating the performance of the DLS techniques on real test beds by allowing us to model, control, and reproduce large scale computing systems with irregular behaviour in order to analyze the resilience of DLS technique son computationally intensive scientific applications. The results are used to compare the resilience of scheduling techniques under different case scenarios comprising of variable problem sizes, system sizes, characteristics of the variations in the application task computation times, and those of the processor availabilities and failures.","2379-5352","978-1-4673-7148-3978-1-4673-7147","10.1109/ISPDC.2015.29","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7165146","dynamic loop scheduling;resilience;SimGrid","Resilience;Computational modeling;Processor scheduling;Dynamic scheduling;Analytical models;Load modeling;Adaptation models","grid computing;resource allocation;scheduling;software performance evaluation;system recovery","dynamic loop scheduling technique;heterogeneous computing systems;DLS technique;complex scientific applications;performance improvement;resilient scheduling technique;load balancing;large-scale computing systems;distributed resources;heterogeneous resources;SimGrid-based simulations;simulation framework;variable problem sizes;system sizes;variation characteristics;application task computation times;processor availabilities;processor failures","","4","6","","","","","","IEEE","IEEE Conferences"
"Parallel Prototyping for Multi-language Service Design: A Case Study on Introducing a Multilingual Tool into a Japanese Local Restaurant","H. Cho; D. Kinny; D. Lin","NA; NA; NA","2013 International Conference on Culture and Computing","","2013","","","86","91","Multilingual tools can benefit communities in which language barriers exist. Such tools can be developed easily by combining language services provided via standard interfaces in frameworks such as the Language Grid. However, designing such tools is more challenging than implementing them since they must be customized for the target community. To optimize tool designs, designers must consider stakeholder requirements, available services and design options in an iterative process. This paper shows how a community can design such a multilingual tool by parallel prototyping using Web services, presenting a case study of an izakaya, a Japanese local restaurant. In parallel prototyping, more than one prototype is created and tested simultaneously, and this approach enables the tool users to give clearer feedback. In the case study, two different hi-fidelity prototypes are created by combining available language services, and these are then evaluated in a real life situation, resulting in effective feedback for improving the tool design.","","978-0-7695-5047","10.1109/CultureComputing.2013.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6680336","intercultural collaboration;language services;design;prototyping","Prototypes;Communities;Dictionaries;Web services;Interviews;Tablet computers;Educational institutions","catering industry;iterative methods;language translation;natural language interfaces;natural language processing;parallel processing;software prototyping;Web services","multilanguage service design;Japanese local restaurant;standard interfaces;language grid;stakeholder requirements;iterative process;multilingual tool designs;parallel prototyping;Web services;izakaya;hi-fidelity prototypes;multilingual tool design","","1","16","","","","","","IEEE","IEEE Conferences"
"QoS-Guaranteed Controller Placement in SDN","T. Y. Cheng; M. Wang; X. Jia","NA; NA; NA","2015 IEEE Global Communications Conference (GLOBECOM)","","2015","","","1","6","The controller placement problem tries to place k controllers in a given network to optimize performance metrics such as propagation latency, load distribution, network reliability and failure resilience. However, the quality of service (QoS) is always a primary concern of the network operators in the placement of SDN controllers. Since the SDN controllers are responsible to provide services for switches, the response time of controllers is an important QoS parameter of network operators. In this paper, we introduce the QoS-Guaranteed Controller Placement problem: Given a network topology and a response time bound, how many controllers are needed, where to place them, and which switches are assigned to each of the controller. We propose three heuristic algorithms: incremental greedy algorithm, primal-dual-based algorithm and network-partition-based algorithm. The proposed algorithms are tested and compared on the Internet Topology Zoo, a dataset of public available network topologies. Simulation results show that the proposed incremental greedy method slightly outperforms the other two methods on all input topologies.","","978-1-4799-5952-5978-1-4799-5951","10.1109/GLOCOM.2015.7416960","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7416960","","Control systems;Time factors;Quality of service;Delays;Heuristic algorithms;Partitioning algorithms;Algorithm design and analysis","Internet;quality of service;software defined networking;telecommunication network topology","QoS-guaranteed controller placement problem;quality of service;SDN controllers;network topology;response time bound;incremental greedy algorithm;primal-dual-based algorithm;network-partition-based algorithm;Internet topology zoo","","9","17","","","","","","IEEE","IEEE Conferences"
"How the implementation of new network technologies influenced the changes of e-government federal data centers","W. Urbanczyk","Vienna University of Technology, Austria","PROCEEDINGS OF 2013 International Conference on Sensor Network Security Technology and Privacy Communication System","","2013","","","53","57","For data centers that have their core competence in e-government and are forced to constantly adapt technological changes and geopolitical aspects, the decision what technology to use in the future is more and more difficult. The quite intensive, periodic changes in the CIT sector are irreversible and nonnegotiable. The growth of CIT requires very quick decisions in the field of organizational, technological and strategic management. Technical systems provide security and complexity as dual properties. In the rapidly evolving field of network technology in recent decades the occurrence of a variety of safety problems was observed. These are related and affect the confidentiality, authenticity and integrity of transmitted information. The present study is based on a comparison of specific aspects of MPLS; Carrier Ethernet technologies are argued to be technically significant, but also ultimately economic benefits of migration to Carrier Ethernet and Connect Oriented Ethernet in terms of a reduction of the increased complexity in the data center are considered. It should be confirmed that, reduced by the transition from old technologies, the complexity of the network technology field is increased in every sense, and therefore also the security - one of the most important factors in the data center - is increased. For many security problems which the MPLS technology has, as legacy systems, various solutions are found and implemented. However, as these are not part of a holistic concept, developers, administrators and end users are confronted with an increasing number of possible options for safe design of systems. With the help of best practices in tested and validated architectures, proven network design, the risk of misconfiguration should to be minimized. For IT decision makers which are responsible for network planning in data centers, provide best practices of Cisco Systems and telecom companies optimized solutions in the design and implementation of new network technologies. The aim of this paper is to motivate the IT managers in e-government data centers to take the first step toward Ethernet-based technologies. The technology- and innovation managers must consider not only the technical features of MPLS and CE or COE technologies, but also take into account their profitability and potential market position after the technology change. The motivation for the on-demand solution using ethernet and cloud computing should also lead to minimize operating costs and reduce the use of resources.","","978-1-4673-6453-9978-1-4673-6452-2978-1-4673-6451","10.1109/SNS-PCS.2013.6553834","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553834","e-government data center;cloud computing for egovernment;security in e-government;traffic engineering;QoS in telecom networks","Multiprotocol label switching;IEEE Xplore;Computers;Security;Aging;Blogs","cloud computing;computer centres;computer network security;government data processing;local area networks;software maintenance;telecommunication traffic","e-government federal data centers;CIT sector;organizational field;technological field;strategic management field;network technology;safety problems;transmitted information confidentiality;transmitted information integrity;transmitted information authenticity;Carrier Ethernet technology;connect-oriented Ethernet technology;MPLS technology;legacy systems;network design;data center complexity reduction;geopolitical aspects;technological changes;resource usage reduction;operating cost minimization;cloud computing;market position;profitability;COE technologies;CE technologies;telecom companies;Cisco Systems;network planning;misconfiguration risk minimization","","1","12","","","","","","IEEE","IEEE Conferences"
"Distributed evolutionary approach to data clustering and modeling","M. Hajeer; D. Dasgupta; A. Semenov; J. Veijalainen","Department of computer science, University of Memphis, TN, USA; Department of computer science, University of Memphis, TN, USA; Dept. of Computer Science and Inf. Systems, University of Jyvaskyla, Finland; Dept. of Computer Science and Inf. Systems, University of Jyvaskyla, Finland","2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)","","2014","","","142","148","In this article we describe a framework (DEGA-Gen) for the application of distributed genetic algorithms for detection of communities in networks. The framework proposes efficient ways of encoding the network in the chromosomes, greatly optimizing the memory use and computations, resulting in a scalable framework. Different objective functions may be used for producing division of network into communities. The framework is implemented using open source implementation of MapReduce paradigm, Hadoop. We validate the framework by developing community detection algorithm, which uses modularity as measure of the division. Result of the algorithm is the network, partitioned into non-overlapping communities, in such a way, that network modularity is maximized. We apply the algorithm to well-known data sets, such as Zachary Karate club, bottlenose Dolphins network, College football dataset, and US political books dataset. Framework shows comparable results in achieved modularity; however, much less space is used for network representation in memory. Further, the framework is scalable and can deal with large graphs as it was tested on a larger youtube.com dataset.","","978-1-4799-4518","10.1109/CIDM.2014.7008660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008660","graph clustering;graph analysis;distributed clustering;analysis;social media;evolutionary;multi objective;large graphs;HDFS;MapReduce","Communities;Encoding;Media;Image edge detection;Genetic algorithms;Biological cells;Clustering algorithms","data handling;distributed algorithms;genetic algorithms;parallel processing;pattern clustering;public domain software","distributed evolutionary approach;distributed genetic algorithms;network encoding;chromosomes;open source implementation;MapReduce paradigm;Hadoop;community detection algorithm;nonoverlapping communities;network modularity;data sets;Zachary Karate club;bottlenose Dolphins network;College football dataset;US political books dataset;data clustering;data modeling","","1","35","","","","","","IEEE","IEEE Conferences"
"A channelized hotelling observer for treaty-verification tasks","C. J. MacGahan; M. A. Kupinski; N. R. Hilton; E. M. Brubaker; W. C. Johnson","College of Optical Sciences at the University of Arizona; College of Optical Sciences at the University of Arizona; Sandia National Laboratories, Livermore, CA; Sandia National Laboratories, Livermore, CA; Sandia National Laboratories, Livermore, CA","2015 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC)","","2015","","","1","1","Binary-discrimination tasks useful to arms-control-treaty verification were performed by applying mathematical observer models used by the medical-imaging community. It is a difficult task, as the monitor needs to verify a measured object is a warhead while the host wants to prevent dissemination of sensitive information on their weapons. Inspection objects were classified by their projection data, without reconstructing an image, which often contains sensitive information. Furthermore, the models process data event-by-event, with only a scalar test statistic being updated before the observed data is purged from memory, preventing the aggregation of information which would be sensitive. Template matching models were developed from a set of calibration data on these objects, with the ultimate goal being to develop an observer model that stores only non-sensitive information sufficient for confirmation. These models were also analyzed in the presence of nuisance parameters — unknowns in the objects or imaging system that affect the projection data but aren't of interest to the discrimination task. Examples of these include object location and orientation. The Hotelling observer and channelized Hotelling observer were modeled and their benefits to information security analyzed. The Hotelling observer-the ideal linear observer when the statistics of the data are Gaussian-stores only a set of weights which are the product of the inverse average covariance matrix and difference in mean data between the two objects in the discrimination task. When testing a source, an inner product of the Hotelling weights and binned testing data is taken, resulting in a scalar that is thresholded to make a decision. If nuisance parameters are present, the mean and covariance matrix are found by averaging not only over the always present Poisson noise, but the nuisance parameter distributions as well. Hence, even if detector data is gathered from multiple realizations of an object, the Hotelling weights will be a single smeared out data set the size of the measured data. The Hotelling weights also sifts out all information other than the differences between the two objects. Hence, if the monitor was to gain access to the weights and apply the inverse to their test statistic, they could only back out a scaled version of the template, not the image. The channelized Hotelling observer-a common tool used in medical image quality assessment-was also investigated. This method drastically reduces the size of the data by applying a channeiizing matrix to the binned testing data set. An optimal set of weights for the channelized data can then be found, and the inner product between these weights and the channelized vector results in the scalar test statistic. Optimal performance is retained by optimizing the matrix to maximize the SNR<sup>2</sup>between the test-statistic distributions for the two sources in the task. The channelized Hotelling observer gives the monitor access to multiple non-sensitive test statistics. In practice, the channelizing matrix could be implemented in hardware or software behind an information barrier. The addition of penalty terms to the channelized Hotelling observer offers further potential for this method. Individual channel performance could be penalized, leading to a large number of non-sensitive channels that the monitor can use to verify the channelization routine is working as described without gaining access to sensitive information. Noise could be added to the resulting channels, causing additional reduction of the total stored information. Finally, if the host can define what information in its object geometries is sensitive in advance, they could optimize the differences between the two objects in the task while penalizing out the sensitive information, creating a non-sensitive channeiizing matrix that could be shared with the monitor. To test these models, Monte Carlo simulations were performed with the GEANT4 toolkit. Photons were tracked from plutonium inspection objects developed by Idaho National Laboratory. We simulated the Fast-Neutron Imaging system designed by Oak Ridge National Laboratory and Sandia National Laboratories, which consists of 40×40 1cm<sup>2</sup>liquid scintillator pixels with a plastic coded aperture. Observer models were evaluated using the area under the ROC curve. This work is supported by the Office of Defense Nuclear Nonproliferation Research and Development, Nuclear Weapon and Material Security Team. Sandia National Laboratories is a multi-program laboratory managed and operated by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin Corporation, for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-AC04-94AL85000. (SAND2015-10391A).","","978-1-4673-9862-6978-1-4673-9863","10.1109/NSSMIC.2015.7581997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7581997","","","","","","","","","","","","","IEEE","IEEE Conferences"
"FPGA-based framework for dynamic visual servoing of robot manipulators","A. Alabdo; J. Pérez; J. Pomares; G. J. Garcia; F. Torres","Department of Physics, Systems Engineering and Signal Theory, University of Alicante, Spain; Department of Physics, Systems Engineering and Signal Theory, University of Alicante, Spain; Department of Physics, Systems Engineering and Signal Theory, University of Alicante, Spain; Department of Physics, Systems Engineering and Signal Theory, University of Alicante, Spain; Department of Physics, Systems Engineering and Signal Theory, University of Alicante, Spain","2015 IEEE 20th Conference on Emerging Technologies & Factory Automation (ETFA)","","2015","","","1","8","This paper describes a whole new framework to quickly develop dynamic visual servoing systems embedded in an FPGA. Parallel design of the algorithms increases the precision of this kind of controllers, whereas minimizes the response time. Embedding dynamic visual servoing algorithms converts these systems into real-time systems. Additionally, an optimal control framework to dynamically visual control robot arms is proposed. The architecture re-configurability is tested with the implementation of two dynamic visual servoing controllers. One of the implemented controllers is derived from the proposed control framework and the other is the well-known image transpose Jacobian controller. From these experiments, FPGA resources utilization and timeline of the pipeline show the optimization of the proposed framework.","1946-0740;1946-0759","978-1-4673-7929-8978-1-4673-7928","10.1109/ETFA.2015.7301506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301506","visual servoing;embedded software;robot control;parallel architectures;reconfigurable architectures","Field programmable gate arrays;Visual servoing;Computer architecture;Visualization;Cameras;Manipulators","field programmable gate arrays;manipulators;optimal control;parallel algorithms;robot dynamics;robot vision;visual servoing","FPGA-based framework;robot manipulators;dynamic visual servoing system;algorithmparallel design;response time minimization;embedding dynamic visual servoing algorithm;real-time system;optimal control framework;dynamic visual control;robot arms;architecture reconfigurability;dynamic visual servoing controllers;image transpose Jacobian controller;FPGA resource utilization;pipeline timeline","","1","15","","","","","","IEEE","IEEE Conferences"
"Radar simulation of ship at sea using MOCEM V4 and comparison to acquisitions","C. Cochin; J. Louvigne; R. Fabbri; C. Le Barbu; A. O Knapskog; N. Ødegaard","DGA MI - French MoD - BRUZ - France; DGA MI - French MoD - BRUZ - France; Alyotech - Rennes - France; Alyotech - Rennes - France; Norwegian Defence Research Establishment (FFI), Oslo, Norway; Norwegian Defence Research Establishment (FFI), Oslo, Norway","2014 International Radar Conference","","2014","","","1","6","Known as a useful tool to simulate SAR image, the MOCEM software has been recently extended. Now, its version 4 includes new features to simulate the radar raw data of a maritime scene composed of a ship on a dynamic sea surface. This paper describes the latest developments of the software and illustrates the work done in the context of a cooperation involving FFI (Norway) and DGA MI (France). Thanks to work done by Alyotech, MOCEM now benefits from GPU computation. This offers to take into account the ship motion in the computation of the raw data for SAR and ISAR simulations. It offers to display, in real time, the phenomena of delocalization, defocusing and deformations encountered in images when dealing with complex and realistic ship motion. In the first phase of the cooperation FFI and DGA MI have produced a methodology of how to build a 3D EM model of ships using rough CAD models (bought from internet provider). Some works have been done to adapt them to an EM application and to allocate EM properties. In a second step, simulations have been done to produce raw data to be been processed with SAR/ISAR algorithms. In parallel, real acquisitions have been done by FFI using the PicoSAR radar (bought from SELEX). In the end, simulations and measurements have been compared in order to demonstrate that simulation has reached a high level. Therefore, simulation of a new radar equipment has become a real option to optimize processing, reduce flight tests or for the training of future users, such as radar operators of a maritime patrol aircraft.","1097-5764","978-1-4799-4195","10.1109/RADAR.2014.7060241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7060241","simulation;ship;bright points model;raw data;SAR;ISAR;synthesis;sea;model;modeling","Marine vehicles;Solid modeling;Synthetic aperture radar;Radar imaging;Computational modeling;Sea surface","marine communication;radar imaging;ships;synthetic aperture radar","radar simulation;MOCEM V4;SAR image;MOCEM software;radar raw data;maritime scene;dynamic sea surface;DGA MI;GPU computation;ship motion;rough CAD models;radar equipment;maritime patrol aircraft","","3","14","","","","","","IEEE","IEEE Conferences"
"Optimal State Reference Computation With Constrained MTPA Criterion for PM Motor Drives","M. Preindl; S. Bolognani","Department of Industrial Engineering, University of Padova, Padova, Italy; Department of Industrial Engineering, University of Padova, Padova, Italy","IEEE Transactions on Power Electronics","","2015","30","8","4524","4535","This research proposes a procedure that maps a PMSM torque request onto optimal state (current) references. Combining the procedure with a dynamic (current) controller yields a torque controller. The maximum torque per ampere (MTPA) criterion is used to minimize conduction and switching losses. This research extends the concept to field-weakening operation to obtain high efficiency at any machine speed. The resulting constrained MTPA criterion is formalized as an optimization problem. Since it is difficult to solve directly, the maximum and intersection torque subproblems are identified. An algorithm is obtained that maps a torque onto an optimal state reference, and it is sufficiently efficient for real-time implementation. This method is compatible with a variety of state (current) controllers with/without PWM, SPM and IPM machines with saliency and reverse saliency, and a variable dc-link voltage. The proposed procedure relies on a sufficiently accurate torque model that may not be provided using rated machine parameters. Thus, an approach to compute locally optimized machine parameters is proposed that takes magnetic saturation into account. The concept is developed on a software-in-the-loop platform and evaluated on an experimental test bench.","0885-8993;1941-0107","","10.1109/TPEL.2014.2354299","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6891383","Drive system;field weakening (FW);internal permanent magnet synchronous motor;maximum torque per ampere (MTPA);optimal control","Torque;Trajectory;Mathematical model;Inverters;Vectors;Stator windings;Equations","optimal control;permanent magnet motors;synchronous motor drives;torque control","optimal state reference computation;constrained MTPA criterion;PM motor drives;PMSM torque request;dynamic controller;torque controller;maximum torque per ampere;variable dc-link voltage;magnetic saturation;software-in-the-loop platform","","51","36","","","","","","IEEE","IEEE Journals & Magazines"
"A mapping method of image mosaic algorithm on embedded reconfigurable processor","J. Wang; Z. Shi; K. Pang; T. Gao; Q. Cao","School of Electronic Information Engineering, Tianjin University, Tianjin, China; School of Electronic Information Engineering, Tianjin University, Tianjin, China; School of Electronic Information Engineering, Tianjin University, Tianjin, China; School of Electronic Information Engineering, Tianjin University, Tianjin, China; School of Electronic Information Engineering, Tianjin University, Tianjin, China","2015 8th International Congress on Image and Signal Processing (CISP)","","2015","","","846","850","A mapping method of image mosaic algorithm based on Reconfigurable Processing Element Array (RPEA) is proposed. This method can effectively improve the utilization rate of parallel resources. In the image mosaic algorithm, Harris corner detection is used to extract the feature points. Normalized Cross Correlation (NCC) algorithm is used to match these feature points and Random sample consensus (RANSAC) algorithm is used to eliminate the mismatching. Critical parts of the algorithms described in C language are found out, which are dynamically mapped onto the RPEA to run concurrently. The execution time on RPEA is reported to compare with execution time on general single-core processor. This paper focuses on the mapping methods based on loop unrolling and software pipeline loop and proposes an optimized method. The test results show that the speedups of mapping the mosaic algorithm on the reconfigurable processor with the proposed method versus using Intel Atom 230 reach more than 2.","","978-1-4673-9098-9978-1-4673-9097","10.1109/CISP.2015.7407995","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7407995","reconfigurable processor;image mosaic algorithm;mapping method","Algorithm design and analysis;Feature extraction;Signal processing algorithms;Pipelines;Kernel;Software algorithms","C language;feature extraction;image segmentation","Intel Atom 230;single-core processor;C language;random sample consensus;NCC algorithm;normalized cross correlation algorithm;feature point extraction;Harris corner detection;RPEA;reconfigurable processing element array;embedded reconfigurable processor;image mosaic algorithm;mapping method","","","9","","","","","","IEEE","IEEE Conferences"
"Adaptive - Robust control a computational efficient real time simulation","L. I. Gliga; C. -. C. Mihai; C. Lupu; D. Popescu","“Politehnica” University of Bucharest, Faculty of Automatic Control and Computer Science, Romania; “Politehnica” University of Bucharest, Faculty of Automatic Control and Computer Science, Romania; “Politehnica” University of Bucharest, Faculty of Automatic Control and Computer Science, Romania; “Politehnica” University of Bucharest, Faculty of Automatic Control and Computer Science, Romania","2015 13th International Conference on Engineering of Modern Electric Systems (EMES)","","2015","","","1","4","The adaptive-robust control strategy, proposed in this paper, combines the advantages of robust and adaptive control, while minimizing the computational load. Usually, a lot of mathematical operations are necessary, which limit the real time applicability of such strategies. By developing a simple, yet useful rule, the number of operations needed to determine if a new controller is needed has been greatly reduced. That is the most repetitive part of an adaptive - robust algorithm, because the rule check has to be done at every sampling moment. To test the new rule, a simulation software was developed, which proves the efficiency of the new rule. There are no disadvantages for the new adaptive - robust strategy, when compared to the usual approach.","","978-1-4799-7650-8978-1-4799-7649-2978-1-4799-7648","10.1109/EMES.2015.7158403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158403","Adaptive - robust control;identification;implementation;optimization;real-time;simulation","Robustness;Process control;Adaptation models;Computational modeling;Mathematical model;Robust control;Adaptive control","adaptive control;robust control;sampling methods;simulation","adaptive control;robust control;real time simulation;sampling moment","","2","9","","","","","","IEEE","IEEE Conferences"
"Real-time automatic chroma-key matting using perceptual analysis and prediction","Ling Yin; Jiying Zhao","School of Electrical Engineering and Computer Science, University of Ottawa, 800 King Edward Ave., ON K1N 6N5, Canada; School of Electrical Engineering and Computer Science, University of Ottawa, 800 King Edward Ave., ON K1N 6N5, Canada","2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE)","","2014","","","1","4","This paper presents a novel mechanism for automatically matting monochromatic background images and videos in real-time. The proposed mechanism simulates the process of human perception on isolating foreground elements in a given scene. The perceptual analysis is performed on optimized hue, saturation, lightness and chroma from CIECAM02 color appearance model, which has the best overall performance across the tested data sets. The foreground and background sample-pairs for alpha estimation are adaptively predicted based on the prior analysis rather than direct sampling. To achieve real-time performance, the entire procedures are optimized for parallel processing on the GPUs (Graphics Processing Units). The qualitative evaluation shows that our determined alpha mattes and foreground colors especially in large seemingly translucent areas are more acceptable by human eyes. And the quantitative comparison between our mechanism and other existing approaches also validates the advantage in speed and quality.","0840-7789","978-1-4799-3101-9978-1-4799-3099","10.1109/CCECE.2014.6901000","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6901000","","Image color analysis;Robustness;Estimation;Color;Real-time systems;Videos;Software algorithms","adaptive estimation;graphics processing units;image colour analysis;image sampling;parallel processing","real-time automatic chroma-key matting;automatically matting monochromatic background imaging;automatically matting monochromatic background video;human perception analysis;foreground element isolation;CIECAM02 color appearance model;foreground sample-pair;background sample-pair;alpha adaptive estimation;image sampling;parallel processing;GPU;graphics processing unit","","","14","","","","","","IEEE","IEEE Conferences"
"Statistical learning in chip (SLIC)","R. D. Blanton; X. Li; K. Mai; D. Marculescu; R. Marculescu; J. Paramesh; J. Schneider; D. E. Thomas","Electrical and Computer Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University, Pittsburgh, PA 15213, USA","2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)","","2015","","","664","669","Despite best efforts, integrated systems are “born” (manufactured) with a unique `personality' that stems from our inability to precisely fabricate their underlying circuits, and create software a priori for controlling the resulting uncertainty. It is possible to use sophisticated test methods to identify the best-performing systems but this would result in unacceptable yields and correspondingly high costs. The system personality is further shaped by its environment (e.g., temperature, noise and supply voltage) and usage (i.e., the frequency and type of applications executed), and since both can fluctuate over time, so can the system's personality. Systems also “grow old” and degrade due to various wear-out mechanisms (e.g., negative-bias temperature instability), and unexpectedly due to various early-life failure sources. These “nature and nurture” influences make it extremely difficult to design a system that will operate optimally for all possible personalities. To address this challenge, we propose to develop statistical learning in-chip (SLIC). SLIC is a holistic approach to integrated system design based on continuously learning key personality traits on-line, for self-evolving a system to a state that optimizes performance hierarchically across the circuit, platform, and application levels. SLIC will not only optimize integrated-system performance but also reduce costs through yield enhancement since systems that would have before been deemed to have weak personalities (unreliable, faulty, etc.) can now be recovered through the use of SLIC.","","978-1-4673-8388-2978-1-4673-8389","10.1109/ICCAD.2015.7372633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7372633","Integrated system design;low-power design;statistical and machine learning","Data models;Uncertainty;Statistical learning;Actuators;Training data;Integrated circuit modeling;Time-frequency analysis","electronic engineering computing;integrated circuit design;learning (artificial intelligence)","statistical learning in chip;SLIC;integrated system design;integrated-system performance optimization;cost reduction;yield enhancement","","","32","","","","","","IEEE","IEEE Conferences"
"Suitable placements of multiple FACTS devices to improve the transient stability using trajectory sensitivity analysis","A. Nasri; R. Eriksson; M. Ghandhari","Electric Power System Department, KTH Royal Institute of Technology, Stockholm, Sweden; Electric Power System Department, KTH Royal Institute of Technology, Stockholm, Sweden; Electric Power System Department, KTH Royal Institute of Technology, Stockholm, Sweden","2013 North American Power Symposium (NAPS)","","2013","","","1","6","Trajectory sensitivity analysis (TSA) is used as a tool for suitable placement of multiple series compensators in the power system. The goal is to maximize the benefit of these devices in order to enhance the transient stability of the system. For this purpose, the trajectory sensitivities of the rotor angles of the most critical generators with respect to the reactances of transmission lines are calculated in the presence of the most severe faults. Based on the obtained trajectory sensitivities, a method is proposed to determine how effective the series compensation of each transmission line is for improving the transient stability. This method is applied to the Nordic-32 test system to find the priorities of the transmission lines for installation of several series compensators. Simulation with industrial software shows the validity and efficiency of the proposed method.","","978-1-4799-1255","10.1109/NAPS.2013.6666828","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6666828","","Power system stability;Trajectory;Sensitivity;Power transmission lines;Generators;Stability analysis;Power system transients","flexible AC transmission systems;power system transient stability;rotors;sensitivity analysis","Nordic-32 test system;transmission line reactance;most critical generator;rotor angle;multiple series compensator;trajectory sensitivity analysis;transient stability;multiple FACTS device","","","12","","","","","","IEEE","IEEE Conferences"
"Service level agreements-driven management of distributed applications in cloud computing environments","A. Antonescu; T. Braun","University of Bern, Communication and Distributed Systems (CDS), Neubrückstrasse 10, 3012 Bern; University of Bern, Communication and Distributed Systems (CDS), Neubrückstrasse 10, 3012 Bern","2015 IFIP/IEEE International Symposium on Integrated Network Management (IM)","","2015","","","1122","1128","Cloud Computing enables provisioning and distribution of highly scalable services in a reliable, on-demand and sustainable manner. However, objectives of managing enterprise distributed applications in cloud environments under Service Level Agreement (SLA) constraints lead to challenges for maintaining optimal resource control. Furthermore, conflicting objectives in management of cloud infrastructure and distributed applications might lead to violations of SLAs and inefficient use of hardware and software resources. This dissertation focusses on how SLAs can be used as an input to the cloud management system, increasing the efficiency of allocating resources, as well as that of infrastructure scaling. First, we present an extended SLA semantic model for modelling complex service-dependencies in distributed applications, and for enabling automated cloud-infrastructure management operations. Second, we describe a multi-objective VM allocation algorithm for optimised resource allocation in infrastructure clouds. Third, we describe a method of discovering relations between the performance indicators of services belonging to distributed applications and then using these relations for building scaling rules that a CMS can use for automated management of VMs. Fourth, we introduce two novel VM-scaling algorithms, which optimally scale systems composed of VMs, based on given SLA performance constraints. All presented research works were implemented and tested using enterprise distributed applications.","1573-0077","978-1-4799-8241","10.1109/INM.2015.7140442","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7140442","","Resource management;Monitoring;Benchmark testing;Measurement;Adaptation models;Cloud computing;Semantics","cloud computing;contracts;resource allocation;virtual machines;virtualisation","service level agreement-driven management;SLA semantic model;enterprise distributed application;cloud computing;optimal resource control;cloud infrastructure management;resource allocation;cloud infrastructure scaling;service-dependency modelling;VM allocation algorithm;virtual machine","","2","21","","","","","","IEEE","IEEE Conferences"
"Feasibility Analysis of Engine Control Tasks under EDF Scheduling","A. Biondi; G. Buttazzo; S. Simoncelli","NA; NA; NA","2015 27th Euromicro Conference on Real-Time Systems","","2015","","","139","148","Engine control applications include software tasks that are triggered at predetermined angular values of the crankshaft, thus generating a computational workload that varies with the engine speed. To avoid overloads at high rotation speeds, these tasks are implemented to self adapt and reduce their computational demand by switching mode at given rotation speeds. For this reason, they are referred to as adaptive variable rate (AVR) tasks. Although a few works have been proposed in the literature to model and analyze the schedulability of such a peculiar type of tasks, an exact analysis of engine control applications has been derived only for fixed priority systems, under a set of simplifying assumptions. The major problem of scheduling AVR tasks with fixed priorities, however, is that, due to engine accelerations, the interarrival period of an AVR task is subject to large variations, therefore there will be several speeds at which any fixed priority assignment is far from being optimal, significantly penalizing the schedulability of the system. This paper proposes for the first time an exact feasibility test under the Earliest Deadline First scheduling algorithm for tasks sets including regular periodic tasks and AVR tasks triggered by a common rotation source. In addition, a set of simulation results are reported to evaluate the schedulability gain achieved in this context by EDF over fixed priority scheduling.","2377-5998;1068-3070","978-1-4673-7570","10.1109/ECRTS.2015.20","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7176033","","Engines;Acceleration;Scheduling;Search problems;Real-time systems;Processor scheduling;Analytical models","control engineering computing;engines;scheduling;shafts","engine control task feasibility analysis;EDF scheduling;software tasks;crankshaft;adaptive variable rate tasks;AVR task scheduling;engine accelerations;earliest deadline first scheduling algorithm;fixed priority scheduling","","10","20","","","","","","IEEE","IEEE Conferences"
"A naive approach for cloud service discovery mechanism using ontology","V. S. K. Nagireddi; S. Mishra","Department of Computer and Information Sciences, University of Hyderabad, India; Institute for Development and Research in Banking Technology, Hyderabad, India","2013 National Conference on Parallel Computing Technologies (PARCOMPTECH)","","2013","","","1","7","Cloud computing is an emerging technology which provides computing infrastructure, platform (Operating System, development/testing), software applications and storage as a service on a pay-per-use basis. Although there exists enormous number of services as well as Cloud Service Providers (CSP), identifying the appropriate service as per user requirement has not been addressed yet. The problem has been addressed in the proposed work in two modules. The first module describes about the construction of ontology based on relationship between cloud services and their characteristics. In the second module, a generic based search engine has been developed to search the cloud services for user's requirement. It uses cloud ontology to process the query and fetch the results. The cloud services are ranked on the basis of the cloud service characteristics. The proposed model has been implemented using protégé tool for constructing ontology and packages (owlapi, sparqldl, Jena) for constructing search engine.","","978-1-4799-1591-0978-1-4799-1589","10.1109/ParCompTech.2013.6621403","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6621403","Intercloud;cloud services;ontology;semantics","Ontologies;Cloud computing;Resource description framework;Search engines;Standards;Semantics;Taxonomy","cloud computing;ontologies (artificial intelligence);query processing;search engines","naive approach;cloud service discovery mechanism;cloud ontology;cloud computing;computing infrastructure;software applications;storage as a service;pay-per-use basis;cloud service providers;CSP;generic based search engine;query procesing;cloud service characteristics;Protégé tool","","1","29","","","","","","IEEE","IEEE Conferences"
"Success within App Distribution Platforms: The Contribution of App Diversity and App Cohesivity","V. Dibia; C. Wagner","NA; NA","2015 48th Hawaii International Conference on System Sciences","","2015","","","4304","4313","As the plethora of mobile apps continues to grow, app publishers are increasingly challenged on the appropriate strategies to adopt in order to improve app success. In this report, we adopt theoretical lens from transaction cost theory and economic utility in the development and test of a model that examines the impacts of app diversity and app cohesivity on app success. Using data gathered from the Microsoft Windows Phone App distribution platform, our main conclusions are as follows: app diversity (the number of geographic locales an app is built to support) is of particular significance as a potential driver of app success within platforms and app cohesivity (a measure of integration with the platforms services) is positively associated with app success (its download count/rank within the platform). Interestingly, both relationships appear to be moderated by the pricing scheme adopted by publishers for that app. Theoretical and practical implications are discussed as well as avenues for future research work.","1530-1605","978-1-4799-7367","10.1109/HICSS.2015.515","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7070335","platform ecosystems;transaction costs;app diversity;app Cohesivity;transaction costs;utility;multi-lingual support","Software;Ecosystems;Mobile communication;Pricing;Mobile handsets;Economics;Privacy","economics;mobile computing;pricing;utility theory","app diversity;app cohesivity;mobile apps;app publishers;transaction cost theory;economic utility;app success;Microsoft Windows Phone App distribution platform;geographic locale;download count;pricing scheme","","","31","","","","","","IEEE","IEEE Conferences"
"Land cover mapping capability of multispectral thermal data: The TASI-600 case study","M. F. Carfora; A. Palombo; S. Pascucci; S. Pignatti; F. Santini","National Research Council, Istituto per le Applicazioni del Calcolo “Mauro Picone” (CNR IAC), 80131 Napoli, Italy; National Research Council, Institute of Methodologies for Environmental Analysis (CNR IMAA), 85050 Tito Scalo (PZ), Italy; National Research Council, Institute of Methodologies for Environmental Analysis (CNR IMAA), 85050 Tito Scalo (PZ), Italy; National Research Council, Institute of Methodologies for Environmental Analysis (CNR IMAA), 85050 Tito Scalo (PZ), Italy; National Research Council, Institute of Methodologies for Environmental Analysis (CNR IMAA), 85050 Tito Scalo (PZ), Italy","2013 5th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)","","2013","","","1","4","This study shows the land cover mapping accuracy retrievable by the TASI-600 thermal airborne multispectral sensor and describes some of the classification results tested on the thermal preprocessed data for a rural area. In the paper is provided an overview of the principal TASI-600 characteristics, i.e. 32 spectral bands in the 8.0-11.5 μm spectral range, and land cover classification performances. A full assessment of the TASI-600 spectral bands has been also obtained by ranking them in order to understanding their role in land cover classification. Results accuracies have been validated using available ground truth. The study highlights that the new generation of multi/hyperspectral thermal sensors opens up interesting opportunities for accurate land cover classification.","2158-6276","978-1-5090-1119-3978-1-5090-1118-6978-1-5090-1120","10.1109/WHISPERS.2013.8080714","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8080714","TASI-600;multispectral thermal data;land cover mapping;classification accuracies","Training;Thermal sensors;Hyperspectral imaging;Earth;Software","geophysical image processing;image classification;land cover;remote sensing","TASI-600 thermal airborne multispectral sensor;thermal preprocessed data;rural area;land cover classification performances;TASI-600 spectral bands;multi/hyperspectral thermal sensors;multispectral thermal data;TASI-600 case study;land cover mapping;size 8.0 mum to 11.5 mum","","","13","","","","","","IEEE","IEEE Conferences"
"Efficient recognition of machine printed Arabic text using partial segmentation and Hausdorff distance","R. Saabni","Department of Computer Science Triangle Research &amp; Development Center, Tel-Aviv Yaffo Academic College Kafr Qara, Israel","2014 6th International Conference of Soft Computing and Pattern Recognition (SoCPaR)","","2014","","","284","289","There is an urgent need for reliable and efficient systems for off-line automatic reading of machine printed Arabic texts. A partial list of applications that may use such system includes searching and reading in scanned books and manuscript as a part of digital libraries; recognizing text on digitized maps, vehicle license plates, road signs and others. In this research we aim to contribute to the research of recognizing Arabic machine printed texts using a partial segmentation process and Hausdorff distance. The process analyses the layout of the image and segments it to words and Parts of Words (PAWs). The Stroke Width Transform (SWT) is used to calculate the size and the font in order to define a set of multi size sliding windows to search and identify characters within the given shape of a PAW. The process evaluates the similarity of the two sub images (character and sliding window) using Hausdorff distance. The top k - ranked candidates and their places within the PAW are recorded and used to generate a list of full PAWs images. In the next step elements of this list are matched to the given shape in a holistic manner. We have tested our approach using the APTI, the PATS- A01 data sets and a private collection of text images and encouraging results were obtained.","","978-1-4799-5934","10.1109/SOCPAR.2014.7008020","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7008020","Arabic OCR;Hausdorff Distance;Partial Segmen-tation","Image segmentation;Optical character recognition software;Shape;Text recognition;Feature extraction;Character recognition;Transforms","image segmentation;natural language processing;optical character recognition;text detection;transforms","machine printed Arabic text recognition;Hausdorff distance;off-line automatic reading;partial segmentation process;parts of words;PAW;stroke width transform;SWT;optical character recognition","","1","16","","","","","","IEEE","IEEE Conferences"
"Efficient non-iterative fixed-period SVM training architecture for FPGAs","P. B. A. Phear; R. K. Rajkumar; D. Isa","Department of Electrical and Electronic Engineering, University of Nottingham, Kuala Lumpur, Malaysia; Department of Electrical and Electronic Engineering, University of Nottingham, Kuala Lumpur, Malaysia; Department of Electrical and Electronic Engineering, University of Nottingham, Kuala Lumpur, Malaysia","IECON 2013 - 39th Annual Conference of the IEEE Industrial Electronics Society","","2013","","","2408","2413","A method for efficient non-iterative fixed-period SVM training is presented. A highly pipelined, parallel, and concurrent systolic processing-based hardware architecture overview for FPGA implementation is also provided. The architecture's training performance is simulated in software and tested successfully by solving two classification problems utilising a 2-dimensional linearly-separable dataset and a 2-dimensional XOR-problem dataset. In both cases the trained optimal SVM function-model classified both datasets with 100% accuracy and thus verified the training architecture's feasibility and potential for further investigation and FPGA implementation.","1553-572X","978-1-4799-0224","10.1109/IECON.2013.6699508","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6699508","","Support vector machines;Training;Vectors;Computer architecture;Hardware;Optimization;Linear programming","electronic engineering computing;field programmable gate arrays;support vector machines;systolic arrays","2-dimensional XOR-problem dataset;concurrent systolic processing-based hardware architecture;parallel systolic processing-based hardware architecture;pipelined systolic processing-based hardware architecture;FPGA;noniterative fixed-period SVM training architecture","","","20","","","","","","IEEE","IEEE Conferences"
"An immune intelligent approach for security assurance","A. Enache; M. Ioniţă; V. Sgârciu","Faculty of Automatic Control and Computer Science, University Politehnica, Bucharest, Romania; Faculty of Computer Science, Military Technical Academy, Bucharest, Romania; Faculty of Automatic Control and Computer Science, University Politehnica, Bucharest, Romania","2015 International Conference on Cyber Situational Awareness, Data Analytics and Assessment (CyberSA)","","2015","","","1","5","Information Security Assurance implies ensuring the integrity, confidentiality and availability of critical assets for an organization. The large amount of events to monitor in a fluid system in terms of topology and variety of new hardware or software, overwhelms monitoring controls. Furthermore, the multi-facets of cyber threats today makes it difficult even for security experts to handle and keep up-to-date. Hence, automatic ""intelligent"" tools are needed to address these issues. In this paper, we describe a `work in progress' contribution on intelligent based approach to mitigating security threats. The main contribution of this work is an anomaly based IDS model with active response that combines artificial immune systems and swarm intelligence with the SVM classifier. Test results for the NSL-KDD dataset prove the proposed approach can outperform the standard classifier in terms of attack detection rate and false alarm rate, while reducing the number of features in the dataset.","","978-0-9932-3380","10.1109/CyberSA.2015.7166116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7166116","IDS;Dendritic Cell Algorithm;Binary Bat Algorithm;SVM","Support vector machines;Immune system;Particle swarm optimization;Feature extraction;Silicon;Intrusion detection","artificial immune systems;pattern classification;security of data;support vector machines","immune intelligent approach;information security assurance;asset integrity;asset confidentiality;asset availability;cyber threats;security threats mitigation;anomaly based IDS model;intrusion detection system;artificial immune system;swarm intelligence;SVM classifier;support vector machines;NSL-KDD dataset;attack detection rate;false alarm rate","","1","20","","","","","","IEEE","IEEE Conferences"
"Architecture and functionalities of a smart Distribution Management System","A. Berizzi; C. Bovo; D. Falabretti; V. Ilea; M. Merlo; G. Monfredini; M. Subasic; M. Bigoloni; I. Rochira; R. Bonera","Dipartimento di Energia, Politecnico di Milano, (Italy); Dipartimento di Energia, Politecnico di Milano, (Italy); Dipartimento di Energia, Politecnico di Milano, (Italy); Dipartimento di Energia, Politecnico di Milano, (Italy); Dipartimento di Energia, Politecnico di Milano, (Italy); Dipartimento di Energia, Politecnico di Milano, (Italy); Dipartimento di Energia, Politecnico di Milano, (Italy); IC SG Energy Automation, Siemens S.p.a., Milano (Italy); IC SG Energy Automation, Siemens S.p.a., Milano (Italy); Università degli Studi, Milano (Italy)","2014 16th International Conference on Harmonics and Quality of Power (ICHQP)","","2014","","","439","443","The paper presents the current state of art of a developed smart Distribution Management System (DMS) environment based on the innovative SCADA systems. The tool is designed to intelligently manage, on-line and off-line, the distribution systems with a high penetration of dispersed generation and is particularly adapted to the specifics of the distribution systems in Italy. The work results from the research project InGrid done by Siemens Italy in cooperation with Politecnico di Milano and Università degli Studi di Milano universities. Research activities have been carried out for innovative functions for the management of medium voltage distribution grids in presence of distributed generation. Such functions have been deployed in a software tool and have been tested in real life conditions.","2164-0610;1540-6008","978-1-4673-6487","10.1109/ICHQP.2014.6842857","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6842857","smart grids;distribution systems;dispersed generation;DMS","Substations;Voltage control;Reactive power;Optimization;SCADA systems;Voltage measurement;Mathematical model","distributed power generation;distribution networks;SCADA systems;smart power grids","smart distribution management system;DMS;innovative SCADA systems;project InGrid;Siemens Italy;Politecnico di Milano university;medium voltage distribution grids;distributed generation;Università degli Studi di Milano university","","9","9","","","","","","IEEE","IEEE Conferences"
"Electric vehicle energy management system using National Instruments' CompactRIO and LabVIEW","S. Mohd; S. A. Zulkifli; R. G. A. Rangkuti; M. Ovinis; N. Saad","Mechanical Engineering Department, Universiti Teknologi PETRONAS, Bandar Sri Iskandar, Malaysia; Electrical and Electronics Engineering Department, Universiti Teknologi PETRONAS, Bandar Sri Iskandar, Malaysia; Project Department, PETRONAS Carigali Muriah Ltd., Jakarta, Indonesia; Mechanical Engineering Department, Universiti Teknologi PETRONAS, Bandar Sri Iskandar, Malaysia; Electrical and Electronics Engineering Department, Universiti Teknologi PETRONAS, Bandar Sri Iskandar, Malaysia","2013 IEEE International Conference on Smart Instrumentation, Measurement and Applications (ICSIMA)","","2013","","","1","6","This paper describes development of an electric propulsion system, energy management system (EMS) and battery management system (BMS) to convert a conventional internal-combustion-engine vehicle to a full electric vehicle. An EMS is designed, built and tested with the objective of optimizing electric power consumption of the converted electric vehicle and extend its driving range. The `driver-assist' system monitors vehicle performance via an on-board data acquisition system. It tracks, among others, vehicle speed, motor speed, power consumption, battery and motor temperature and battery state of charge (SOC) and gives feedback in terms of suggested actions for the driver. The EMS is implemented on National Instruments' CompactRIO embedded controller, programmed on LabVIEW Real-Time software. The paper also describes development of a graphical driver interface (GDI), based on the web server function of the CompactRIO, implemented via TCP-IP connection with a tablet PC. The GDI not only offers the driver control of the EMS and in-vehicle data logging, but also remote monitoring and control of the EMS via a wireless 3G internet connection.","","978-1-4799-0843-1978-1-4799-0842","10.1109/ICSIMA.2013.6717928","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6717928","Electric Vehicle Propulsion;Energy Management System;Real-Time Embedded System;Battery Management System;In-Vehicle Data Logging;Graphical Driver Interface","Induction motors;Batteries;Vehicles;Energy management;DC motors;Propulsion","3G mobile communication;battery management systems;data acquisition;electric motors;electric propulsion;electric vehicles;energy management systems;Internet;power consumption;virtual instrumentation","electric vehicle energy management system;National Instrument CompactRIO;electric propulsion system;battery management system;BMS;EMS;electric power consumption;driver-assist system;on-board data acquisition system;motor temperature;motor speed;battery state of charge;SOC;LabVIEW Real-Time software;graphical driver interface;Web server function;TCP-IP connection;tablet PC;GDI;in-vehicle data logging;remote monitoring;wireless 3G Internet connection","","2","6","","","","","","IEEE","IEEE Conferences"
"Optimal placement of PMUs for power system observability with increased redundancy","J. G. Philip; T. Jain","Dept. of Electrical Engineering, IIT Indore, India; Dept. of Electrical Engineering, IIT Indore, India","2015 Conference on Power, Control, Communication and Computational Technologies for Sustainable Growth (PCCCTSG)","","2015","","","1","5","Phasor Measurement Units (PMUs) are nowadays used to monitor and measure the bus voltages and the line currents emanating from the buses. For monitoring all the buses in the power system, optimal placement of PMUs for total power system observability is required. Further, measurement redundancy is also vital to enhance the reliability of state estimation. This paper proposes a new optimization formulation to improve the measurement redundancy with minimum number of PMUs while maintaining full system observability. The problem has been formulated as an integer linear programming problem and solved using CPLEX solver of GAMS software package. The proposed method is tested on IEEE 14 bus and IEEE 30 bus system. The results obtained using the proposed formulation are compared with those obtained using the cost minimization and it has been found that the proposed method is able to increase the measurement redundancy with the same number of PMUs obtained using the cost minimization.","","978-1-4673-6890","10.1109/PCCCTSG.2015.7503889","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7503889","Measurement redundancy;Phasor Measurement Unit (PMU);System observability;System Measurement Redundancy Index (SMRI);Zero Injection Bus (ZIB)","Phasor measurement units;Observability;Redundancy;Voltage measurement;Power systems;Transmission line measurements;Integer linear programming","integer programming;linear programming;phasor measurement;redundancy;system buses","phasor measurement units;PMU;bus voltages;line currents;total power system observability;measurement redundancy;state estimation reliability;full system observability;integer linear programming problem;CPLEX solver;GAMS software package","","2","20","","","","","","IEEE","IEEE Conferences"
"Towards enabling green routing services in real networks","M. Rifai; D. Lopez Pacheco; G. Urvoy-Keller","University Nice Sophia Antipolis, Sophia-Antipolis, France; University Nice Sophia Antipolis, Sophia-Antipolis, France; University Nice Sophia Antipolis, Sophia-Antipolis, France","2014 IEEE Online Conference on Green Communications (OnlineGreenComm)","","2014","","","1","7","Energy-efficient/green routing protocols aim at optimizing the energy consumption of networks. Such solutions usually require to take coordinated actions between routers to setup a green network topology. Green routing protocols have been studied theoretically and using simulations; however, they have not been implemented or tested yet in real routing protocols. Therefore, it is difficult to assess the viability and efficiency of such green solutions in real operational networks. In this article, we aim at bridging the gap between theory and practice in the field of green routing. We report the lessons learned when enabling an energy-efficient routing protocol built on top of the OSPF implementation of Quagga, a popular software router used in operational networks. Firstly, we show that the coordination between green-enabled routers, which requires exchanging special or modified routing information, can be added without introducing new message types and with minimum changes to the routing protocol implementation while maintaining full compatibility with legacy routers. Secondly, we consider the modifications that should be done to prevent inconsistencies since green-enabled routers need to exchange special routing updates that affect the calculation of routing tables. Finally, we demonstrate that our first prototype can draw a more energy-efficient network topology, while preserving the stability of the network, its reconfiguration capabilities and compatibility between green and legacy routers.","","978-1-4799-7384","10.1109/OnlineGreenCom.2014.7114417","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7114417","","Routing;Erbium;Ear;Routing protocols;Green products;Network topology","energy conservation;power consumption;routing protocols;telecommunication network topology;telecommunication power management","real networks;energy efficiency;energy consumption;Green network topology;Green routing protocols;software router;operational networks;reconfiguration capabilities;legacy routers","","","22","","","","","","IEEE","IEEE Conferences"
"Integrated risk management based automated vehicle following system on inner-city streets","Dongwook Kim; Junyoung Lee; B. Kim; Kangwon Lee; K. Yi","Seoul National University, Gwanak 599, Gwanak-Ro, Korea; Seoul National University, Gwanak 599, Gwanak-Ro, Korea; Seoul National University, Gwanak 599, Gwanak-Ro, Korea; Korea Polytechnic, Sangidaehak-ro, Siheung-si, Gyeonggi-do, Korea; Seoul National University, Gwanak 599, Gwanak-Ro, Korea","17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","","2014","","","418","423","This paper represents model predictive steering control scheme for automated vehicle following system. In contrast to highway, there are some unexpected obstacles such as parked or oncoming vehicles on inner-city streets. Therefore, an automated vehicle following system should consider these obstacles as well as a leading vehicle to guarantee the safety of the system. The model predictive steering control scheme of this paper aims to ensure the safety from unexpected obstacles. The MPC computes the sequence of optimal steering input based on a simplified nonlinear model and the information of leading vehicle's and obstacle's states. At each time step, steering control inputs are optimized under constraints related to vehicle stability and safety. The performance of the proposed algorithm has been investigated via computer simulation conducted using the vehicle dynamic software CARSIM and Matlab/Simulink. Test results show the robust performance of vehicle following on an inner-city street scenario.","2153-0009;2153-0017","978-1-4799-6078","10.1109/ITSC.2014.6957726","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6957726","","Vehicles;Mathematical model;Equations;Acceleration;Trajectory;Safety;Sensors","collision avoidance;control engineering computing;nonlinear control systems;predictive control;risk management;road traffic control;steering systems;traffic engineering computing","integrated risk management;automated vehicle following system;inner-city streets;model predictive steering control scheme;parked vehicles;oncoming vehicles;leading vehicle;unexpected obstacles;simplified nonlinear model;steering control inputs;vehicle stability;vehicle safety;CARSIM vehicle dynamic software;Matlab;Simulink","","2","22","","","","","","IEEE","IEEE Conferences"
"Application of PC Cluster Based on MPI2 in Ions Trajectory Simulation","Q. Xie; Y. Lu; Y. Liu; D. Yin","NA; NA; NA; NA","2015 2nd International Conference on Information Science and Control Engineering","","2015","","","147","151","This paper provides that when the beam current increases to a certain level, the number of ions must be increased and the space charge effect must be added in accelerator physics in order to gain high precise status of ions. It's needed high performance computing to complete this work which needs TFLOPS level capability. So, a PC cluster based on Beowulf architecture and MPI2 was constructed, we put forward a parallel algorithm of divides and conquer according the mapping model of parallel problem, and we accomplished the simulation application of ions trajectory. In the end, we tested the application on different ions' simulation scale from 1.0E+4 to 1.0E+6. Experimental results show that optimized parallel algorithm gets 1.12~2.03 times on average speedup under the premise of guarantee accuracy. The best speedup is 2.52 when the number of ion is 1.0E+6 and the loop times is 1000. The simulation computation time is one third of be-fore. The success transplantation of ion parallel simulation software shows the superiority of the parallel computing and the validity of PC cluster. Additionally, it may be a good reference to other applications.","","978-1-4673-6850-6978-1-4673-6849","10.1109/ICISCE.2015.40","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120580","MPI2;PC Cluster;Parallel;Ions trajectory simulation","Ions;Computational modeling;Trajectory;Computers;Analytical models;Parallel processing;Hardware","digital simulation;divide and conquer methods;parallel algorithms;physics computing","PC cluster;MPI2;ions trajectory simulation;beam current;accelerator physics;TFLOPS level capability;Beowulf architecture;parallel algorithm;divide and conquer method;ion parallel simulation software","","","8","","","","","","IEEE","IEEE Conferences"
"Cloud-Based Harvest Management System for Specialty Crops","L. Tan; R. Haley; R. Wortman","NA; NA; NA","2015 IEEE Fourth Symposium on Network Cloud Computing and Applications (NCCA)","","2015","","","91","98","Harvesting labor is a major cost factor in the production of specialty crops. Today accruing harvest labors is still done by hands, which is error-prone and costly. By integrating cloud-based web application with purposely designed labor monitoring devices (LMDs), we developed a harvest management system for monitoring and accruing harvest labors. The system comprises of two major components: an in-orchard data collection network collecting harvest data and transmitting them to a cloud-based labor management software (LMS); and, LMS processing harvest data and delivering results to users via a tablet-friendly web interface. Using a patented technology, the system accurately accrues harvest labor activities for multiple orchards, even under complex many-to-many employment relations. The system provides multi-fold benefits to stakeholders of specialty crop harvesting: a picker can be compensated accurately by the actual weight of the fruits he picked; and an orchard manager may monitor labor activities in real time and improve his orchard operation based on the analytical reports generated by the system. The dynamic resource allocation provided by a cloud computing platform ensures that the system can handle the fluctuating demand for processing real-time harvest data during and off harvest seasons. The design of the system is optimized for cloud computing, improving the access to orchard data while preserving their privacy for growers. A prototype of the system has been validated in field tests in United States' Pacific Northwest Region.","","978-1-4673-7741","10.1109/NCCA.2015.23","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340032","cloud computing;agricultural information systems; harvest labor management; wireless mesh networks.","Least squares approximations;Cloud computing;Agriculture;Monitoring;Data collection;Radiofrequency identification","agriculture;cloud computing;computerised monitoring;crops;user interfaces","cloud-based harvest management system;specialty crop production;harvesting labor;cloud-based Web application;labor monitoring device;LMD;labor management software;LMS;Web interface;cloud computing platform","","","13","","","","","","IEEE","IEEE Conferences"
"Intelligent Control System for Microgrids Using Multiagent System","T. Logenthiran; R. T. Naayagi; W. L. Woo; V. Phan; K. Abidi","School of Electrical and Electronic Engineering, Newcastle University, Singapore; School of Electrical and Electronic Engineering, Newcastle University, Singapore; School of Electrical and Electronic Engineering, Newcastle University, Singapore; School of Electrical and Electronic Engineering, Newcastle University, Singapore; School of Electrical and Electronic Engineering, Newcastle University, Singapore","IEEE Journal of Emerging and Selected Topics in Power Electronics","","2015","3","4","1036","1045","This paper presents an intelligent control of a microgrid in both grid-connected and islanded modes using the multiagent system (MAS) technique. This intelligent control consists of three levels. The first level is based on local droop control, the second level compensates power balance between the supply and the demand optimally, and the third level is at the system level based on electricity market. An intelligent MAS was developed and implemented based on foundation for intelligent physical agents standards by representing each major autonomous component in the microgrid as an intelligent software agent. The agents interact with each other for making their own decisions locally and optimally. The coordination among the agents ensures power quality, voltage, and frequency of the microgrid by determining the set points that optimize the overall operation of the microgrid. The proposed control architecture and strategies for the real-time control of microgrids were analyzed in detail, and tested under various load conditions and different network configurations. The outcomes of the studies demonstrate the feasibility of the proposed control and strategies, as well as the capability of the MAS technique for the operation of microgrids.","2168-6777;2168-6785","","10.1109/JESTPE.2015.2443187","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120083","Intelligent control;Multi-agent system;Microgrid;ICT-enabled architecture;Grid-connected operation;Islanded operation;Grid-connected operation;information and communications technology-enabled architecture;intelligent control;islanded operation microgrid;multiagent system (MAS)","Microgrids;Multi-agent systems;Level control;Power electronics;Real-time systems;Proposals;Generators","distributed power generation;intelligent control;multi-agent systems;power distribution control;power generation control;power markets","microgrids;intelligent control system;multiagent system;grid-connected mode;islanded mode;local droop control;electricity market;intelligent physical agents standards;intelligent software agent;power quality;power voltage;power frequency;real-time control;load conditions;network configurations","","20","36","","","","","","IEEE","IEEE Journals & Magazines"
"A virtual rapid analysis model for DC motors with single tooth windings","C. Wolz; B. Wüchner; M. Greger; J. Kempkes; U. Schäfer","University of Applied Science Wuerzburg-Schweinfurt (FHWS); FHWS; FHWS; FHWS, Germany; Technical University Berlin, Germany","2014 International Conference on Electrical Machines (ICEM)","","2014","","","38","44","In this paper a virtual model for DC motors with single tooth windings and mechanical commutation is presented. The model permits the calculation of the physical values which are otherwise not measurable during actual motor operation while considering the non-linearity of a magnetic circuit. Coil values (current, voltage, flux linkage) and motor values (current, voltage, speed and torque) are both calculated very quickly and with high accuracy. The model utilizes a combination of data tables and simultaneous computations by two separate software programs. A comparison with test data and FEA calculations verifies the model's accuracy. This model can be used to optimize parameters such as brush angle shift, number of turns, or geometric design. The relative speed of this calculation verses the conventional method is an inexpensive alternative for research and development.","","978-1-4799-4389","10.1109/ICELMACH.2014.6960156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6960156","brush motor;calculation of torque;commutation;coupled simulation program;equivalent circuit;magnetic saturation;non-linear magnetic circuit","Coils;Couplings;Magnetic flux;Integrated circuit modeling;Magnetic circuits;Torque;Brushes","coils;commutation;DC motors;finite element analysis;machine theory;machine windings;magnetic circuits","virtual rapid analysis model;DC motor;single tooth winding;mechanical commutation;magnetic circuit;coil value;motor value;data table;software program;FEA calculation;brush angle shift;geometric design","","2","20","","","","","","IEEE","IEEE Conferences"
"Switched flux hybrid magnet memory machine","D. Wu; X. Liu; Z. Q. Zhu; A. Pride; R. Deodhar; T. Sasaki","University of Sheffield, UK; University of Sheffield, UK; University of Sheffield, UK; IMRA Europe S.A.S. UK Research Center, UK; IMRA Europe S.A.S. UK Research Center, UK; IMRA Europe S.A.S. UK Research Center, UK","IET Electric Power Applications","","2015","9","2","160","170","A novel switched flux permanent magnet machine (SFPMM) with both normal NdFeB and low coercive force (LCF) magnets has been developed in this paper. Outside a conventional SFPMM, the LCF magnets are mounted on the back of every U-shaped stator lamination segment with parallel magnetisation direction and alternative polarities. A lamination ring yoke is placed around all the LCF magnets, while extra magnetisation coils are wound on each of them. By injecting different directions of current pulses into the magnetisation coils, the polarities of the LCF magnets can be rewritten in order to enhance or weaken the field generated by normal NdFeB magnets. Therefore the back EMF and torque capability could be adjusted in a wide range. In this paper, the operation principle and machine topology are introduced firstly. Then the stator/rotor pole combinations and design considerations of the machine have been discussed. With the help of two-dimensional finite element software, a 6/5 stator/rotor pole machine has been globally optimised as an example to show the machine performances such as open-circuit field and back EMF, <i>dq</i>-axis inductances, torque capabilities and magnetisation. Finally, a prototype with the same dimensions has been manufactured and tested to validate the analysis results.","1751-8660;1751-8679","","10.1049/iet-epa.2014.0215","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051378","","","coils;electric potential;finite element analysis;laminations;magnetisation;permanent magnet machines;rotors;stators","switched flux hybrid magnet memory machine;low coercive force magnets;SFPMM;U-shaped stator lamination segment;parallel magnetisation direction;lamination ring yoke;LCF magnets;magnetisation coils;back EMF;torque capability;machine topology;stator-rotor pole combinations;design considerations;two-dimensional finite element software;stator-rotor pole machine;machine performances;open-circuit field;dq-axis inductances;torque capabilities;magnetisation;NdFeB","","12","23","","","","","","IET","IET Journals & Magazines"
"OpenBuild : An integrated simulation environment for building control","T. T. Gorecki; F. A. Qureshi; C. N. Jones","Laboratory of Automatic Control in EPFL, Lausanne, Switzerland; Laboratory of Automatic Control in EPFL, Lausanne, Switzerland; Laboratory of Automatic Control in EPFL, Lausanne, Switzerland","2015 IEEE Conference on Control Applications (CCA)","","2015","","","1522","1527","This paper introduces the OpenBuild toolbox for MATLAB. OpenBuild is a toolbox for advanced controller design for buildings heating ventilation and air conditioning systems, with emphasis on Model Predictive Control. It provides researchers in the control community the ability to test algorithms on a wide range of realistic simulation scenarios, by providing most of the data needed to perform simulation and optimization. It combines the convenience of controller design in MATLAB with the simulation capabilities of the building simulation software EnergyPlus. It includes a building modeling tool to construct linear state-space models of building thermodynamics based on building description data, making it useful for design of optimal controllers requiring a good prediction model, as well as providing the input data necessary for simulation such as weather, occupancy and internal gains data. The ability to co-simulate the building between MATLAB and EnergyPlus enables fast prototyping and validation of the models and controllers. This paper presents the working principles and functionality of OpenBuild.","1085-1992","978-1-4799-7787-1978-1-4799-7786","10.1109/CCA.2015.7320826","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7320826","","Buildings;Mathematical model;Data models;Computational modeling;Meteorology;MATLAB;Atmospheric modeling","building management systems;control engineering computing;control system synthesis;digital simulation;HVAC;predictive control","OpenBuild toolbox;building control;Matlab;building heating ventilation and air conditioning system;model predictive control;EnergyPlus building simulation software;building thermodynamics;optimal controller design","","12","19","","","","","","IEEE","IEEE Conferences"
"A simulation study of hybrid wind-ultracapacitor energy conversion system","M. A. Abdullah; C. W. Tan; A. H. M. Yatim","Department of Electrical Power Engineering, Faculty of Electrical Engineering, Universiti Teknologi Malaysia (UTM), 81300 Skudai, Johor, Malaysia; Department of Electrical Power Engineering, Faculty of Electrical Engineering, Universiti Teknologi Malaysia (UTM), 81300 Skudai, Johor, Malaysia; Department of Electrical Power Engineering, Faculty of Electrical Engineering, Universiti Teknologi Malaysia (UTM), 81300 Skudai, Johor, Malaysia","2014 IEEE Conference on Energy Conversion (CENCON)","","2014","","","265","270","Wind energy conversion system (WECS) is controlled to capture the maximum available power upon the available wind speed to make the system economical and operates at optimized efficiency. The nature characteristic of wind generator is nonlinear which leads to variable power output generation. In addition, the demanded load power is subjected to a change due to any disturbances may happen in the load voltage or current. Therefore, it is essential to integrate the WECS with a suitable storage element to either, act as a source of compensating the inadequate in power, or act as a load to absorb the excess power generated by the wind generator. Although the AC rectified voltage of the wind generator is proportional to the wind speed which is variable, however, the DC-bus voltage is usually regulated by the bidirectional converter at a specified value. Hence, the output power of the system can be investigated by means of the output currents from the WECS and the ultracapacitor. This paper presents several different test scenarios of the proposed hybrid WECS-ultracapacitor simulated using MATLAB/Simulink software. It also highlights the important role of the ultracapacitor in ensuring a continuous power flow.","","978-1-4799-4848","10.1109/CENCON.2014.6967513","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6967513","Wind energy conversion system;ultracapacitor;MPPT;boost converter;bidirectional converter","Supercapacitors;Wind turbines;Wind speed;DC-DC power converters;Integrated circuit modeling;Wind energy;Generators","load flow;mathematics computing;power convertors;power generation economics;power generation faults;power system simulation;rectifying circuits;supercapacitors;wind power plants","power flow;MATLAB-Simulink software;bidirectional converter;DC-bus voltage;AC rectified voltage;power compensation;load current;load voltage;power system disturbance;variable power output generation;wind generator characteristics;power system economics;wind speed;WECS;hybrid wind-ultracapacitor energy conversion system","","2","14","","","","","","IEEE","IEEE Conferences"
"Magnetic Focusing Simulator: A 3-D Finite-Element Permanent-Magnet Focusing System Design Tool","W. Chen; Q. Hu; Y. Hu; T. Huang; L. Xu; J. Li; B. Li","Vacuum Electronics National Laboratory, School of Physical Electronics, University of Electronic Science and Technology of China, Chengdu, China; Vacuum Electronics National Laboratory, School of Physical Electronics, University of Electronic Science and Technology of China, Chengdu, China; Vacuum Electronics National Laboratory, School of Physical Electronics, University of Electronic Science and Technology of China, Chengdu, China; Vacuum Electronics National Laboratory, School of Physical Electronics, University of Electronic Science and Technology of China, Chengdu, China; Vacuum Electronics National Laboratory, School of Physical Electronics, University of Electronic Science and Technology of China, Chengdu, China; Vacuum Electronics National Laboratory, School of Physical Electronics, University of Electronic Science and Technology of China, Chengdu, China; Vacuum Electronics National Laboratory, School of Physical Electronics, University of Electronic Science and Technology of China, Chengdu, China","IEEE Transactions on Electron Devices","","2015","62","4","1319","1326","This paper reports the development of a 3-D finite-element design tool for the design of permanent-magnet focusing system called the magnetic focusing simulator (MFS). It is a new module of microwave-tube simulator suite developed by the University of Electronic Science and Technology of China. MFS specifically targets problem classes including periodic permanent magnet, wiggler, permanent-magnet quadrupoles, periodic cusped magnet, and so on. MFS is a finite-element code of magnetic scalar potential and is a solver of magnetostatic field. It can be used for designing, optimizing, testing, and validating the permanent-magnetic focusing systems. The theory of MFS is discussed in detail in this paper. Besides, the accuracy is compared with the commercial software ANSYS Maxwell, and the performance of MFS is also shown.","0018-9383;1557-9646","","10.1109/TED.2015.2400993","National Natural Science Foundations of China; Program for New Century Excellent Talents in University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7047680","Finite-element method (FEM);magnetic focusing simulator (MFS);microwave-tube simulator suite (MTSS);permanent-magnet focusing system.;Finite-element method (FEM);magnetic focusing simulator (MFS);microwave-tube simulator suite (MTSS);permanent-magnet focusing system","Materials;Magnetostatics;Finite element analysis;Perpendicular magnetic recording;Convergence;Focusing;Magnetostatic waves","finite element analysis;magnetic fields;microwave tubes;permanent magnets;wigglers","magnetic focusing simulator;MFS;permanent-magnet focusing system design tool;3D finite-element design tool;microwave-tube simulator;University of Electronic Science and Technology;China;periodic permanent magnet;wiggler;permanent-magnet quadrupoles;periodic cusped magnet;finite-element code;magnetic scalar potential;magnetostatic field;ANSYS Maxwell commercial software","","4","20","","","","","","IEEE","IEEE Journals & Magazines"
"Kinematic analysis of a novel exoskeleton finger rehabilitation robot for stroke patients","S. Guo; F. Zhang; W. Wei; F. Zhao; Y. Wang","Biomedical Robot Laboratory School of Electrical Engineering Tianjin University of Technology Binshui Xidao 391, Tianjin, China; Tianjin Key Laboratory for Control Theory &amp; Applications in Complicated Systems Tianjin University of Technology Binshui Xidao 391, Tianjin, China; Tianjin Key Laboratory for Control Theory &amp; Applications in Complicated Systems Tianjin University of Technology Binshui Xidao 391, Tianjin, China; Tianjin Key Laboratory for Control Theory &amp; Applications in Complicated Systems Tianjin University of Technology Binshui Xidao 391, Tianjin, China; Tianjin Key Laboratory for Control Theory &amp; Applications in Complicated Systems Tianjin University of Technology Binshui Xidao 391, Tianjin, China","2014 IEEE International Conference on Mechatronics and Automation","","2014","","","924","929","The exoskeleton robot technology is more and more used in the assisting stroke patients in implementing rehabilitation training. In this paper, a novel exoskeleton finger robot has been described to aim at helping varieties of hemiparalysis patients recover motor function. The robot system adopts the EEG control and mainly consists of exoskeleton finger robot, EEG system, HMI system, motor controllers unit, some sensors and a workstation. And the hand exoskeleton mechanism is portable, wearable and adjustable for patients doing home rehabilitation training. Base on the Denavit-Hartenberg (DH) parameters method, the kinematic model of finger has built to be used in designing the robot. Through the simulation software ADAMS (Automatic Dynamic Analysis of Mechanical Systems), the parameters of position, velocity and acceleration (PVA) in each joint are simulated. From the result, it can view that the robot has high movement ability to finish the Continuous Passive Motion (CPM). Besides, a comparison test is done to study whether there are some motion blocks in wearing exoskeleton robot. Form the curve figure, in the two situations, the angle range of the MCP (metacarpaophalangeal) joint is equal, which verifies the interference of robot is small. These experiments demonstrate the exoskeleton can provide high efficiency movement ability for stroke doing the rehabilitation. In the future, with the optimization design, the robot will improvement and has a bright application prospect in the rehabilitation field.","2152-7431;2152-744X","978-1-4799-3979-4978-1-4799-3978-7978-1-4799-3977","10.1109/ICMA.2014.6885821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6885821","Exoskeleton finger robot;Rehabilitation;Kinematic simulation analysis","Thumb;Exoskeletons;Joints;Robot sensing systems;Kinematics","digital simulation;electroencephalography;handicapped aids;medical robotics;patient rehabilitation;robot kinematics","kinematic analysis;exoskeleton finger rehabilitation robot;stroke patients;rehabilitation training;hemiparalysis patients;EEG control;EEG system;HMI system;motor controller unit;hand exoskeleton mechanism;Denavit-Hartenberg parameter method;finger kinematic model;ADAMS simulation software;automatic dynamic analysis of mechanical systems;position-velocity-acceleration parameters;continuous passive motion;metacarpaophalangeal joint;MCP joint","","5","15","","","","","","IEEE","IEEE Conferences"
"All programmable and synthetic optical network: Architecture and implementation","B. R. Rofoee; G. Zervas; Y. Yan; N. Amaya; D. Simeonidou","High Performance Networks Group, Merchant Venturers' Building, University of Bristol, Clifton, Bristol BS8 1UB, UK; High Performance Networks Group, Merchant Venturers' Building, University of Bristol, Clifton, Bristol BS8 1UB, UK; High Performance Networks Group, Merchant Venturers' Building, University of Bristol, Clifton, Bristol BS8 1UB, UK; High Performance Networks Group, Merchant Venturers' Building, University of Bristol, Clifton, Bristol BS8 1UB, UK; High Performance Networks Group, Merchant Venturers' Building, University of Bristol, Clifton, Bristol BS8 1UB, UK","IEEE/OSA Journal of Optical Communications and Networking","","2013","5","9","1096","1110","This paper reports on the design, implementation, and evaluation of a multitechnology, multirate, and adaptable network architecture for metropolitan/edge areas. It is empowered by programmability in control and data planes, providing users with an open network platform to redefine and optimize its behavior and performance. It uses a hybrid data plane of fixed-grid [(sub)wave-length] and flex-grid systems to support a broad range of data rates (1 to 555 Gb/s). The programmability in the data plane is achieved by building the nodes with a modular and flexible architecture (architecture on demand nodes) to achieve different functionalities (fixed-/flex-grid switching with or without time multiplexing) on demand. A centralized, modular, and scalable control framework has been constructed for this network. It uses a set of software plug-ins designed for architecture synthesis and adaptation for policing network resources access and as algorithms of routing and resource allocation for network operation. The proposed hybrid network architecture, along with allocation policies and resource allocation algorithms, is evaluated through simulations across a broad range of traffic profiles with bandwidth requests stretching from 1 to 400 Gb/s. Finally, the programmable data-plane/control-plane architecture has been implemented in an experimental testbed and the functionality of the node and network elements individually and together have been tested, demonstrating the feasibility of the system.","1943-0620;1943-0639","","10.1364/JOCN.5.001096","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6608648","Fixed-grid;Flex-grid networking;Programmable networks;Routing and resource allocation","Optical switches;Computer architecture;Resource management;Optical fiber networks;Bandwidth;Routing","metropolitan area networks;optical fibre networks;resource allocation;telecommunication network routing;telecommunication traffic","synthetic optical network;all programmable optical network;metropolitan-edge area;open network platform;hybrid data plane;fixed-grid switching;flex-grid switching;modular architecture;flexible architecture;software plug-ins;network resource access policy;routing algorithm;resource allocation;hybrid network architecture;network traffic profile;programmable data-plane-control-plane architecture","","9","53","","","","","","IEEE","IEEE Journals & Magazines"
"Effective approach for calculating critical speeds of high-speed permanent magnet motor rotor-shaft assemblies","Z. Huang; B. Han","Beihang University, People's Republic of China; Beihang University, People's Republic of China","IET Electric Power Applications","","2015","9","9","628","633","An effective approach is presented for large errors in calculating critical speed of rotor-shaft assembly with the commercial finite element software, is intended to develop the discrete model of the rotor-shaft assembly by using lumped mass method, which is supported by active magnetic bearings. The first two bending critical speeds are analysed by optimising the flexural rigidity coefficient based on transfer matrix method. Compared with experimental modal testing and finite element analysis, the results of the transfer matrix method are in good agreement with modal measurement, the percentage errors of the first two bending natural frequencies are 0.21 and 2.1%, respectively. Owing to the higher accuracy and numerical stability, the method used in this study is an effective way to calculate the critical speed of the rotor-shaft assembly.","1751-8660;1751-8679","","10.1049/iet-epa.2014.0503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7328486","","","assembling;finite element analysis;magnetic bearings;permanent magnet motors;rotors;shafts;transfer function matrices","permanent magnet motor rotor-shaft assembly;commercial finite element software;discrete model;lumped mass method;active magnetic bearing;flexural rigidity coefficient;transfer matrix method;modal measurement;critical speed estimation;numerical stability","","4","19","","","","","","IET","IET Journals & Magazines"
"Research on the dynamic frequency characteristic of large-scale power grid considering the action of power system splitting and load shedding","X. Xialing; X. Jianghui; L. Yong; X. Youping; B. Ruyu; L. Hui; L. Tao; Y. Jing","Central China Branch of Electric Power Dispatching Center, Wuhan Hubei 430077, China; Central China Branch of Electric Power Dispatching Center, Wuhan Hubei 430077, China; NA; NA; NA; NA; NA; NA","2014 International Conference on Power System Technology","","2014","","","103","108","The dynamic frequency characteristic of power system is a significant index for power system operation, and research on the dynamic frequency characteristic of power system is the basis of many important design works such as the design of under frequency load shedding scheme. With the AC and DC ultra-high voltage (UHV) networking and the expanding of the scale of power grid, the economy of power system has been greatly improved, while the ability of resisting disturbance being weakened. Since the frequency deviation has great impact on many aspects of power system, it is of great urgency and necessity to analysis and control the dynamic frequency characteristic of large-scale power grid after disturbance. Based on the analysis of the dynamic frequency characteristic of large-scale power grid, this paper firstly studies the factors which influence the dynamic frequency characteristic of multi-machine power system, and the method of parameter aggregation of dynamic frequency characteristics. Secondly, this paper analyzes the effect of the action of power system splitting on power system dynamic frequency characteristic, and the adaptability of power system under frequency load shedding(UFLS) after the action of power system splitting. Finally, based on a regional power grid as the test case, the correctness and effectiveness of the proposed parameter aggregation method of power system dynamic frequency characteristic is verified by comparing the theoretical calculation value of the dynamic frequency characteristic after specific disturbance with the simulation result of power system simulation software under the same circumstance. On the basis of the previous researches, the foundation for the further study on optimization of under frequency load shedding scheme is proposed.","","978-1-4799-5032","10.1109/POWERCON.2014.6993533","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6993533","dynamic frequency characteristic;power system splitting;power system load shedding;large-scale power grid","Power system dynamics;Generators;Power system stability;Time-frequency analysis;Frequency modulation;Stability criteria","load shedding;power grids;power system economics","dynamic frequency characteristic;large-scale power grid;power system splitting;power system operation;frequency load shedding scheme;ultrahigh voltage networking;UHV networking;power grid;power system economy;multimachine power system;UFLS;power system simulation software","","1","9","","","","","","IEEE","IEEE Conferences"
"Parallelized correlator bank for a 1 GHz bandwidth phase modulated CW radar in the 79GHz band","Y. Gai; A. Dewilde; U. Ahmad; A. Bourdoux; S. Pollin","imec, Leuven, Belgium; KU Leuven, Belgium; KU Leuven, Belgium; KU Leuven, Belgium; imec, Leuven, Belgium","2015 IEEE Symposium on Communications and Vehicular Technology in the Benelux (SCVT)","","2015","","","1","6","The interest for mmWave automotive radar systems comes from the booming market for driver assistance systems. The 77-81 GHz band has been allocated for vehicular radar. Such radars can be used for a wide variety of applications such as collision avoidance, adaptive cruise control, lane-change assistance or monitoring the surroundings of the car. The 79 GHz PMCW radar system belongs to this family. It can not only measure the distance but also the velocity of targets. In this paper, we propose an optimized hardware implementation of the correlator bank of a phase-modulated radar system that modulates a 79 GHz carrier with a 1 GSps Pseudo-noise sequence. It leverages an advanced FPGA platform by using the BPS software from BEEcube. After integration of the digital logic FPGA and the analog transceiver, a real target detection test has been performed in the lab to verify the DSP functionality and the specifications of the radar system. The DSP functionality is shown to support the high sampling rate resulting in a range resolution of 0.15 m.","","978-1-4673-9907-4978-1-4673-9906","10.1109/SCVT.2015.7374226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7374226","79 GHz;PMCW;automotive;radar;millimeter wave;FPGA;BPS;BEEcube;BEE4","Radar;Field programmable gate arrays;Receivers;Transmitters;Hardware;Generators;Correlators","correlators;CW radar;field programmable gate arrays;millimetre wave radar;object detection;radar resolution;random sequences;road vehicle radar;signal sampling;transceivers","parallelized correlator bank;phase modulated CW radar;mm-Wave automotive radar system;driver assistance system;vehicular radar;PMCW radar system;pseudonoise sequence;BPS software;BEEcube;digital logic FPGA;analog transceiver;target detection;DSP functionality;sampling rate;radar resolution;frequency 77 GHz to 81 GHz","","","9","","","","","","IEEE","IEEE Conferences"
"Boosting the FM-Index on the GPU: Effective Techniques to Mitigate Random Memory Access","A. Chacón; S. Marco-Sola; A. Espinosa; P. Ribeca; J. C. Moure","Computer Architecture and Operating Systems, Universitat Autònoma de Barcelona, Bellaterra, Spain; Centro Nacional de Análisis Genómico, Barcelona, Spain; Computer Architecture and Operating Systems, Universitat Autònoma de Barcelona, Bellaterra, Spain; Pirbright Institute, Woking GU24 0NF, United Kingdom; Computer Architecture and Operating Systems, Universitat Autònoma de Barcelona, Bellaterra, Spain","IEEE/ACM Transactions on Computational Biology and Bioinformatics","","2015","12","5","1048","1059","The recent advent of high-throughput sequencing machines producing big amounts of short reads has boosted the interest in efficient string searching techniques. As of today, many mainstream sequence alignment software tools rely on a special data structure, called the FM-index, which allows for fast exact searches in large genomic references. However, such searches translate into a pseudo-random memory access pattern, thus making memory access the limiting factor of all computation-efficient implementations, both on CPUs and GPUs. Here, we show that several strategies can be put in place to remove the memory bottleneck on the GPU: more compact indexes can be implemented by having more threads work cooperatively on larger memory blocks, and a k-step FM-index can be used to further reduce the number of memory accesses. The combination of those and other optimisations yields an implementation that is able to process about two Gbases of queries per second on our test platform, being about 8× faster than a comparable multi-core CPU version, and about 3× to 5× faster than the FM-index implementation on the GPU provided by the recently announced Nvidia NVBIO bioinformatics library.","1545-5963","","10.1109/TCBB.2014.2377716","MICINN; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6975110","GPGPU;Bioinformatics;Short Read Mapping;FM-index;Fine-Grain Parallelism;Memory-Level Parallelism;GPGPU;bioinformatics;short read mapping;FM-index;fine-grain parallelism;memory-level parallelism","Graphics processing units;Instruction sets;Radiation detectors;Bioinformatics;Indexes;Frequency modulation;Genomics","bioinformatics;data structures;genomics;graphics processing units;random-access storage","Nvidia NVBIO bioinformatics library;multicore CPU version;k-step FM-index;pseudorandom memory access pattern;genomic references;data structure;mainstream sequence alignment software tools;string searching techniques;high-throughput sequencing machines;GPU","Algorithms;Computer Graphics;Computer Storage Devices;Equipment Design;Equipment Failure Analysis;High-Throughput Nucleotide Sequencing;Information Storage and Retrieval;Signal Processing, Computer-Assisted","2","24","","","","","","IEEE","IEEE Journals & Magazines"
"Finding competence characteristics among first semester students in computer science","D. Zehetmeier; A. Böttcher; A. Brüggemann-Klein; V. Thurner; K. Schlierkamp","Technische Universität München, Germany, Garching Munich University of Applied Sciences, Germany, Munich; Munich University of Applied Sciences, Germany, Munich; Technische Universitát München, Germany, Garching; Munich University of Applied Sciences, Germany, Munich; Munich University of Applied Sciences, Germany, Munich","2015 IEEE Frontiers in Education Conference (FIE)","","2015","","","1","9","First-year students are often not well equipped with the base competencies that are a necessary precondition for effectively acquiring complex new knowledge. Among others, these base competencies comprise self-organization, analytical thinking, or communication skills. Shortcomings in these competencies often lead to problems in the study process. To solve these problems, we need to investigate the students' initial competencies, in order to become aware of possible deficits, find ways to deal with them, and thus to enable students to reach their goals. Therefore, we developed a self-assessment on base competencies, an accompanying knowledge test and a questionnaire on personal information. The data collected by these tools was analyzed, searching for competence characteristics which influence whether our students persevere and participate in the final exam, or drop out early. The results of our data analysis support the assumption that both non-technical and technical competencies are crucial to study successfully. Correlation analysis has identified several characteristics which distinguish the students that participate in the exam from those who do not. Based on these results, we suggest ways to optimize university courses, teaching methods, and the support of first-year students to meet their needs.","","978-1-4799-8454","10.1109/FIE.2015.7344201","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7344201","","Computer science;Education;Electronic mail;Software;Computers;Mathematics;Data mining","computer science education;data analysis;educational courses;educational institutions;teaching","competence characteristics;computer science;self-organization;analytical thinking;communication skills;data analysis;correlation analysis;university courses;teaching methods","","","23","","","","","","IEEE","IEEE Conferences"
"An algorithm to calculate coordinated trajectories of intelligent vehicles in roads considering individual priority","R. Reghelin; L. V. R. de Arruda","Electrical Engineering Department, CPGEI, Universidade Tecnológica Federal do Paraná (UTFPR), Av. Sete de Setembro, 3165, Curitiba, Brazil; Electrical Engineering Department, CPGEI, UTFPR, Brazil","16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)","","2013","","","1725","1730","This paper presents a simulation algorithm to calculate optimized coordinated trajectories of intelligent vehicles in a segment of road considering individual priority. The objective is to reduce travel time for each vehicle. The configuration of the road can be modeled regarding the number of lanes, direction preference and exclusivity. The calculation considers the main elements of the traffic system, such as topography of the lane, traffic rules and individual capacity of acceleration. A methodology to deal with priority is also proposed. Emergency vehicles can cross the traffic in a shorter time. Simulation tests to evaluate the algorithm are presented.","2153-0009;2153-0017","978-1-4799-2914","10.1109/ITSC.2013.6728478","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6728478","","Vehicles;Roads;Trajectory;Acceleration;Computer aided software engineering;Intellectual property;Vehicle dynamics","intelligent transportation systems;road traffic control;traffic engineering computing","coordinated trajectory;intelligent vehicle;simulation algorithm;road configuration;direction preference;exclusivity;traffic system;lane topography;traffic rules;acceleration;emergency vehicle","","3","9","","","","","","IEEE","IEEE Conferences"
"CAHR: A Contextually Adaptive Home-Based Rehabilitation Framework","A. Karime; M. Eid; H. Dong; M. Halimi; W. Gueaieb; A. E. Saddik","Multimedia Communication Research Laboratory, School of Electrical Engineering and Computer Society, University of Ottawa, Ottawa, ON, Canada; Department of Electrical EngineeringEngineering Division, New York University Abu Dhabi, Abu Dhabi, United Arab Emirates; Multimedia Communication Research Laboratory, School of Electrical Engineering and Computer Society, University of Ottawa, Ottawa, ON, Canada; Institute of Physiotherapy, St. Joseph University, Beirut, Lebanon; Multimedia Communication Research Laboratory, School of Electrical Engineering and Computer Society, University of Ottawa, Ottawa, ON, Canada; Multimedia Communication Research Laboratory, School of Electrical Engineering and Computer Society, University of Ottawa, Ottawa, ON, Canada","IEEE Transactions on Instrumentation and Measurement","","2015","64","2","427","438","Home-based rehabilitation has evolved in recent years as a cost-effective and convenient alternative to traditional clinical rehabilitation. This has consequently created the need to design reliable assessment and adaptation mechanisms that are able to measure and analyze the patient's performance and to accordingly make proper adjustments that conform to the abilities of the patient during the training. This paper proposes a novel context-based adaptive home-based rehabilitation framework to optimize recovery based on patient's performance and ambient conditions. Adaptation is mathematically modeled to configure the rehabilitation task based on user's psychophysiological and environmental conditions. A prototype of the framework is tested for three weeks by two patients with wrist deficiencies. The results obtained are promising and indicate that the proposed framework may be very beneficial for home-based rehabilitation.","0018-9456;1557-9662","","10.1109/TIM.2014.2342432","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6874554","Cloud rehabilitation;intelligent adaptation;medical measurement;rehabilitation framework;tangible user interface;Cloud rehabilitation;intelligent adaptation;medical measurement;rehabilitation framework;tangible user interface","Games;Training;Adaptation models;Engines;Context;Software;Correlation","medical computing;patient care;patient rehabilitation;virtual reality","CAHR;virtual reality;environmental conditions;user psychophysiological conditions;mathematical modeling;ambient conditions;context-based adaptive home-based rehabilitation;training;patient performance measurement;patient performance analysis;adaptation mechanism;reliable assessment mechanism;clinical rehabilitation","","5","27","","","","","","IEEE","IEEE Journals & Magazines"
"Končar Smart Metering - Innovative solutions for well known problems","D. Cmuk; D. Cihlar; M. Šimunović","Končar-Inem/PJ ICT sustavi, Zagreb, Hrvatska; Končar-Inem/PJ ICT sustavi, Zagreb, Hrvatska; Končar-Inem/PJ ICT sustavi, Zagreb, Hrvatska","2013 36th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)","","2013","","","119","124","EU climate and energy package and Energy Efficiency Plan 2011 has set very ambitious goals for all the Member States regarding Smart Metering (SM). In Croatia, different infrastructural problems and lack of independent regulating body allowed only partial solutions by each utility. On the technical level rollouts were slugged by lack of fully multifunctional solutions with minimal installation and maintenance costs, able to adapt to different meter producers with multiple access to the same data. Končar made strategic efforts to overcome these obstacles trough an integrated R&D approach. Realized solution Končar AMI described in this paper relies on the optimized network architecture with innovative long range and short range wireless communication solutions and modular communication equipment. Such a novel approach allows great flexibility, excellent network coverage, good signal propagation and easy installation process with reliable data transfer with minimum operating expenditures. Combined with a cloud based MDM solution they make the milestones for a powerful and highly adaptable, integrated Smart metering solution that could bring numerous advantages to final users and short ROI. Končar AMI was tested trough a number of pilot projects and proved a concept particularly appropriate for bigger SM rollouts.","","978-953-233-073-1978-953-233-076","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6596236","","Protocols;Synchronization;Wireless sensor networks;Energy consumption;Radio frequency;Software;Batteries","energy conservation;innovation management;research and development;smart meters","Koncar smart metering;innovative solutions;EU climate;energy package;energy efficiency;integrated R&D approach","","","10","","","","","","IEEE","IEEE Conferences"
"Challenges and Opportunities in Game Artificial Intelligence Education Using Angry Birds","D. Yoon; K. Kim","Department of Computer Engineering, Sejong University, Seoul, Korea; Department of Computer Engineering, Sejong University, Seoul, Korea","IEEE Access","","2015","3","","793","804","Games have been an important tool for motivating undergraduate students majoring in computer science and engineering. However, it is difficult to build an entire game for education from scratch, because the task requires high-level programming skills and expertise to understand the graphics and physics. Recently, there have been many different game artificial intelligence (AI) competitions, ranging from board games to the state-of-the-art video games (car racing, mobile games, first-person shooting games, real-time strategy games, and so on). The competitions have been designed such that participants develop their own AI module on top of public/commercial games. Because the materials are open to the public, it is quite useful to adopt them for an undergraduate course project. In this paper, we report our experiences using the Angry Birds AI Competition for such a project-based course. In the course, teams of students consider computer vision, strategic decision-making, resource management, and bug-free coding for their outcome. To promote understanding of game contents generation and extensive testing on the generalization abilities of the student's AI program, we developed software to help them create user-created levels. Students actively participated in the project and the final outcome was comparable with that of successful entries in the 2013 International Angry Birds AI Competition. Furthermore, it leads to the development of a new parallelized Angry Birds AI Competition platform with undergraduate students aiming to use advanced optimization algorithms for their controllers.","2169-3536","","10.1109/ACCESS.2015.2442680","National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (2013 R1A2A2A01016589); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7130561","Computing education;Angry Birds;Game;edutainment;Program design;Artificial intelligence (AI);Game competition;Game-based learning;Computing education;Angry Birds;game;edutainment;program design;artificial intelligence (AI);game competition;game-based learning","Game theory;Computer science education;Entertainment;Learning systems;Artificial intelligence;Education","artificial intelligence;computer aided instruction;computer games;computer science education;computer vision;decision making;educational courses;further education","advanced optimization algorithms;undergraduate students;bug-free coding;resource management;strategic decision-making;computer vision;project-based course;Angry Birds AI competition;game AI competitions;game artificial intelligence education","","","35","","","","","","IEEE","IEEE Journals & Magazines"
"Analysis and modeling of HPP Tala/Bhutan for network restoration studies","A. Holst; Karchung; D. Sharma; R. Chhetri","University of Rostock, Germany; CST Phuentsholing, Bhutan; DGPC Thimphu, Bhutan; CST Phuentsholing, Bhutan","2015 5th International Youth Conference on Energy (IYCE)","","2015","","","1","8","This paper describes the modeling and simulation method and corresponding results of the Hydropower Plant (HPP) Tala, which is the largest existing plant in Bhutan with 6 units of 170 MW high pressure Pelton turbines and a complex control structure. The goal of the associated overall project is to create a complete dynamic model of the Bhutan power system for investigations concerning stability, island operation, network restoration and system optimization. To test the dynamic behavior, measurements of transient processes in different operation modes of the HPP Tala for one exemplary unit have been conducted. Based on the analysis of the measurements and the plant documentation, a detailed practice-oriented high quality nonlinear model was developed and validated step by step, using the Matlab/Simulink software. Relying on the model, some initial investigations of the dynamic behavior of the plant with the focus on islanded operation were conducted.","","978-1-4673-7172-8978-1-4673-7171","10.1109/IYCE.2015.7180768","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180768","Hydroelectric power;Islanding;Power control;Power system dynamics;Power system measurements;Power system restoration;Power system simulation;Power system stability;Renewable Energy;Voltage control","Mathematical model;Generators;Voltage control;Turbines;Power system stability;Hydroelectric power generation;Voltage measurement","distributed power generation;hydraulic turbines;hydroelectric power stations;power distribution faults;power system restoration","hydropower plant;network restoration study;high pressure Pelton turbine;Bhutan power system;system optimization;practice-oriented high quality nonlinear model;island operation","","","4","","","","","","IEEE","IEEE Conferences"
"Power and Energy Footprint of OpenMP Programs Using OpenMP Runtime API","A. Nandamuri; A. M. Malik; A. Qawasmeh; B. M. Chapman","NA; NA; NA; NA","2014 Energy Efficient Supercomputing Workshop","","2014","","","79","88","Power and energy have become dominant aspects of hardware and software design in the High Performance Computing (HPC). Recently, the Department of Defense (DOD) has put a constraint that applications and architectures need to attain 75 GFLOPS/Watt in order to support the future missions. This requires a significant research effort towards power and energy optimization. OpenMP programming model is an integral part of HPC. Comprehensive analysis of OpenMP programs for power and execution performance is an active research area. Work has been done to characterize OpenMP programs with respect to power performance at kernel level. However, no work has been done at the OpenMP event level. OpenMP Runtime API (ORA), proposed by the OpenMP standard committee, allow a performance tool to collect information at the OpenMP event level. In this paper, we present a comprehensive analysis of the OpenMP programs using ORA for power and execution performance. Using hardware counters in the Intel SandyBridge x86-64 and Running Average Power Limit (RAPL) energy sensors, we measure power and energy characteristics of OpenMP benchmarks. Our results show that the best execution performance does not always give the best energy usage. We also find out that the waiting time at the barriers and in queue are the main factors for high power consumption for a given OpenMP program. Our results also show that there are unique patterns at the fine level that can be used by the dynamic power management system to enhance the power performance. Our results show substantial variation in energy usage depending upon the runtime environment.","","978-1-4799-7036","10.1109/E2SC.2014.11","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7016390","Power; Energy; OpenMP; Runtime API; Performance Analysis","Runtime;Hardware;Benchmark testing;Radiation detectors;Kernel;Programming","application program interfaces;parallel processing","OpenMP runtime API;high performance computing;Department of Defense;OpenMP programming model;HPC;OpenMP event level;ORA;Running Average Power Limit;Intel SandyBridge x86-64;DOD","","3","23","","","","","","IEEE","IEEE Conferences"
"High performance user space sockets on low power System on a Chip platforms","C. H. Crawford; P. Padkowski; T. Baranski; A. Czubak; Ł. Raszka","IBM Thomas J. Watson Research Center, Yorktown Heights, New York, USA; IBM Research Krakow, Poland; IBM Research Krakow, Poland; IBM Research Krakow, Poland; IBM Research Krakow, Poland","2015 IEEE High Performance Extreme Computing Conference (HPEC)","","2015","","","1","6","With the introduction of low power System on a Chip (SoC) processor architectures in enterprise server configurations, there is a growing need to develop the software that will support scale-out, data intensive cloud applications that are deployed in data centers today. In this paper, we describe the design and implementation of a low latency user space fully compliant TCP/IP socket stack on a low power System on a Chip (SoC) architecture and demonstrate that this library can become the basis for “Big Data” applications that require both high throughput and low latency capabilities all on a power optimized system platform. For our work, we are specifically targeting cloud applications that are developed on runtimes which are seeing great growth in programmer communities and enterprise deployment as well as for which the I/O bottlenecks outweigh the compute requirements, e.g. memcached. On low-power embedded-class SoC servers, these I/O bottlenecks can be prohibitively expensive for performance and scaling requirements of such applications, even when the CPU efficiency and memory bandwidth are adequate. Our approach removes this bottleneck by leveraging available SoC integrated Network Interface Cards (NICs) as well as user space communication - thereby improving pathlength to data as well as preserving CPU cycles from context switching. Our experiments show that we can achieve sub 5 μsec ping-pong latency for 8B packets, and also provide substantive improvement to the memslap benchmark not just when compared to memcached running on the T4240 with the kernel stack (3.5 times better for 16B SETs) but also when compared to a standard x86_64 server with ConnectX 10GbE adapters when power based metrics are used (close to a factor of 2 improvement with power normalized metrics).","","978-1-4673-9286-0978-1-4673-9285","10.1109/HPEC.2015.7322441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7322441","fast networking;low latency;QorIQ;Linux;user space drivers;DPAA;networking offload;wire speed;user space sockets;low power networking","Kernel;Servers;Benchmark testing;Hardware;IP networks;Computer architecture","Big Data;cloud computing;computer centres;input-output programs;network interfaces;system-on-chip;transport protocols","high performance user space socket;system on a chip platform;SoC processor architecture;enterprise server configuration;TCP/IP socket stack;Big Data;cloud application;data center;I/O bottleneck;network interface card;NIC","","","13","","","","","","IEEE","IEEE Conferences"
"Design of intelligent infrared vehicle detect system based on ZigBee","Li Lirong; Zhang Yunmei; Lv Shanshan; Chen Ye; Liu Tianqi; Xu Xinyuan","Electric and Electronic Engineering School, Hubei University of Technology, Wuhan, China; Electric and Electronic Engineering School, Hubei University of Technology, Wuhan, China; Electric and Electronic Engineering School, Hubei University of Technology, Wuhan, China; Electric and Electronic Engineering School, Hubei University of Technology, Wuhan, China; Electric and Electronic Engineering School, Hubei University of Technology, Wuhan, China; Electric and Electronic Engineering School, Hubei University of Technology, Wuhan, China","2015 Chinese Automation Congress (CAC)","","2015","","","1845","1849","In recent years, with the development of China's economy and the urban vehicles increased greatly, the intelligent parking system is developing rapidly all over the country. As the important part of an intelligent parking system, vehicles and the traffic volume detection technology is also in continuous developing. The practical and feasible infrared detection technology has been widely used in traffic volume detection. The article presents a ZigBee-based infrared vehicle detect method, which can scan vehicles synchronously by installing infrared transmitting tube and receiving tube separately on each side of the road. When a car pass through the detection area, the detector can accurately and quickly judge the existence of the vehicles and record the information for traffic volume detection. According to the counts of sensors being intercepted, the location of the car and the car's other information such as the shape, height etc. can be used to determine the car's model, and then may give out corresponding control. In addition, the detect system first use ZigBee wireless network, which has advantages of low power consumption, low cost, low complexity and high efficiency. On the other side we can make the detector as a screen, and then make some modification, the system's function can be more diverse, such as safety light curtain, anti-theft devices, etc. Combining with software programming, the system can detect more information, and can be reasonable optimized, especially for data processing and analysis. This detection system has wide range of uses, not only can be used in the detection of vehicle parking lot to improve the efficiency of the yard management, and also be used to detect road traffic and others, even in other areas, such as security, scientific research, mechanical control and the emerging field of intelligent building systems. With its simple structure, low cost and high stability advantages, the system has a good application prospect and developing potential.","","978-1-4673-7189-6978-1-4673-7188","10.1109/CAC.2015.7382804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7382804","vehicle detect system;infrared light curtain;zigbee wireless network","Pollution measurement;Testing;Extraterrestrial measurements","infrared detectors;intelligent transportation systems;road vehicles;Zigbee","intelligent infrared vehicle detection system;ZigBee wireless network;intelligent parking system;yard management;road traffic detection","","1","19","","","","","","IEEE","IEEE Conferences"
"Efficient and Customizable Data Partitioning Framework for Distributed Big RDF Data Processing in the Cloud","K. Lee; L. Liu; Y. Tang; Q. Zhang; Y. Zhou","NA; NA; NA; NA; NA","2013 IEEE Sixth International Conference on Cloud Computing","","2013","","","327","334","Big data business can leverage and benefit from the Clouds, the most optimized, shared, automated, and virtualized computing infrastructures. One of the important challenges in processing big data in the Clouds is how to effectively partition the big data to ensure efficient distributed processing of the data. In this paper we present a Scalable and yet customizable data PArtitioning framework, called SPA, for distributed processing of big RDF graph data. We choose big RDF datasets as our focus of the investigation for two reasons. First, the Linking Open Data cloud has put forwards a good number of big RDF datasets with tens of billions of triples and hundreds of millions of links. Second, such huge RDF graphs can easily overwhelm any single server due to the limited memory and CPU capacity and exceed the processing capacity of many conventional data processing software systems. Our data partitioning framework has two unique features. First, we introduce a suite of vertexcentric data partitioning building blocks to allow efficient and yet customizable partitioning of large heterogeneous RDF graph data. By efficient, we mean that the SPA data partitions can support fast processing of big data of different sizes and complexity. By customizable, we mean that the SPA partitions are adaptive to different query types. Second, we propose a selection of scalable techniques to distribute the building block partitions across a cluster of compute nodes in a manner that minimizes inter-node communication cost by localizing most of the queries on distributed partitions. We evaluate our data partitioning framework and algorithms through extensive experiments using both benchmark and real datasets. Our experimental results show that the SPA data partitioning framework is not only efficient for partitioning and distributing big RDF datasets of diverse sizes and structures but also effective for processing big data queries of different types and complexity.","2159-6190;2159-6182","978-0-7695-5028-2978-0-7695-5028","10.1109/CLOUD.2013.63","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676711","","Resource description framework;Query processing;Information management;Data handling;Data storage systems;Benchmark testing;Distributed databases","cloud computing;data handling;distributed processing;graph theory","vertex-centric data partitioning;distributed big RDF data processing;cloud computing;big RDF graph data;SPA data partitioning;building block partition;internode communication cost","","1","21","","","","","","IEEE","IEEE Conferences"
"A simple statically reconfigurable processor architecture","A. Moosa; M. Aneesh","Dept. of Electronics Engg, School of Engg. and Technology, Pondicherry University, Puducherry, India; Dept. of Electronics Engg, School of Engg. and Technology, Pondicherry University, Puducherry, India","2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT)","","2015","","","1","5","A reconfigurable Processor architecture is presented that has been designed prioritizing modularity, scalability and simplicity. Modular design enables swapping of functional units within the main processing core while maintaining the same programming model. This ensures that the associated software tools chain such as Assembler and Compiler need not be redesigned. Scalable design enables reconfiguring the datapath width to suite application requirements without redesigning the processor architecture or making changes to the software program already written. Applications for such design range from academia where real world performance of many proposed Adder/Multiplier structures may be tested; to data centers where the nature of operation to be performed on massive chunks of data changes regularly requiring ASIC like performance.","","978-1-4799-6085-9978-1-4799-6084-2978-1-4799-6083","10.1109/ICECCT.2015.7226112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7226112","Verilog;Architecture;VLSI;Reconfigurable;CPU","Pipeline processing;Registers;Field programmable gate arrays;Tuning;Table lookup;Embedded systems","adders;application specific integrated circuits;field programmable gate arrays;hardware description languages;reconfigurable architectures;VLSI","FPGA;Verilog;VLSI;ASIC like performance;multiplier structures;adder structures;scalable design;modular design;statically reconfigurable processor architecture","","","5","","","","","","IEEE","IEEE Conferences"
"Using social learning in network engineering curricula","M. E. Sousa-Vieira; J. C. López-Ardao; C. López-García; M. Rodríguez-Pérez; J. C. López-Ardao","Department of Telematics Engineering, Vigo, Spain; Department of Telematics Engineering, Vigo, Spain; Department of Telematics Engineering, Vigo, Spain; Department of Telematics Engineering, Vigo, Spain; Department of Telematics Engineering, Vigo, Spain","2015 IEEE Global Engineering Education Conference (EDUCON)","","2015","","","471","478","It is commonly accepted that contemporary cohorts of students witness and experience the benefits of information technologies in their learning processes. The so-called “digital natives” acquire, as a consequence of their early exposure to these technologies, different patterns of work, distinct attention conducts, new learning preferences and, generally, better skills for learning and working within rich online social contexts. So, it seems reasonable that the traditional education systems evolve, and shape their practice to leverage those new patterns. Though online social networks (OSNs) stand as a powerful tool to extend existing online learning management systems (LMSs) by using them as a gateway to OSNs, LMSs do not exploit fully the advantages of an active social environment for reinforcing the learning experience. We report in this paper the design, development and use of a software platform which enlarges and adapts the basic features of an OSN in order to be useful for very general learning environments. The software allows the creation, assessment and reporting of a range of collaborative activities based on social interactions among the students, and offers a reward mechanism by means of ranking and reputation. We argue that this approach is helpful in increasing the students' motivation, besides improving the learning experience and performance. The software has been tested in an undergraduate course about computer networks. Different tests confirm that the impact on learning success is statistically significant and positive.","2165-9567;2165-9559","978-1-4799-1908","10.1109/EDUCON.2015.7096012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7096012","informal learning;social learning;online social networks","Games;Collaboration;Logic gates;Computer networks;Color;Engineering education;Conferences","educational courses;engineering education;further education;human factors;learning management systems;social networking (online);telecommunication computing;telecommunication engineering education","LMS;learning management systems;computer networks;undergraduate course;learning experience improvement;students motivation;reward mechanism;collaborative activities;OSN;online social networks;network engineering curricula;social learning","","","18","","","","","","IEEE","IEEE Conferences"
"Consistent Quality Evaluation Method for Shape Sensors based on FBG - Optical Fibers Used in Minimally Invasive Surgery","H. Pauer; C. Ledermann; W. Tuschmann; H. Woern","NA; NA; NA; NA","Sensors and Measuring Systems 2014; 17. ITG/GMA Symposium","","2014","","","1","6","Shape sensing is a field of study with growing interest. In minimally invasive surgery, flexible medical instruments need to be tracked and the position and orientation of the manipulator tool at the instrument tip have to be detected. This can be realized via integrated shape sensors. Sensor use in the field of medicine calls for sensors with high levels of accuracy. To accurately assess sensor quality, and to compare different types of sensors, uniform evaluations must be done. Based on that evaluation, shape sensores can than be optimized and a sufficiently precice sensor device can be reached. A specific set of guidelines determining how to assess collected data is needed to allow an accurate assessment of shape sensor quality. The evaluation must be adjusted to correlate with the application, the sensor is supposed to be used for. Quality data needs to be gathered to compare different aspects of sensor efficiency and to evaluate the sensor on various performance criteria. Gathering the required data needs to be done without great effort using consistent procedures and tests, to allow for direct comparison of the achived results. The goal is to be able to describe the quality of a sensor using a scalar quantity, so that the sensor quality can be ordered on a quantitative interval scale. A concept of a uniform evaluation of this kind is not yet known. A new method of evaluation will be introduced and discussed in this paper. This method consists of measuring data where the data is gathered with a specific shape giving device. This data collecting is supported by a software tool to simply implement the procedure. Another part of the evaluation method is a quantitative and qualitative assessment of the quality by analyzing the collected data. This way, shape sensors can be compared and optimized with regard to different hardware and software components. Using the presented method shape sensors can be continuously optimized and applied to detect deformations of high complexity.","","978-3-8007-3622","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6856694","","","","","","","","","","","","","VDE","VDE Conferences"
"TQoSM: Total quality of service model","Z. M. Aljazzaf","College of Computing Sciences and Engineering, Department of Information Science, Kuwait","2015 International Conference on Industrial Engineering and Operations Management (IEOM)","","2015","","","1","8","A distributed application may be composed of global services provided by different organizations and have different properties. Providing provisioned services is of primary importance due to the multi-tenant and potentially multi-provider nature of services. When selecting a service from many similar services, it is important to distinguish between them. Quality of service (QoS) has been used as a distinguishing factor between similar services and as a criterion for service selection. To date, the majority of research on QoS is not comprehensive in identifying QoS suitable for online open environments with diverse services. Even fewer studies have been conducted to identify the QoS evaluation techniques. However, there is a need to identify the QoS, build a comprehensive QoS model, and identify the QoS evaluation approaches. Thus, this paper proposes Total QoS Model (TQoSM), a generic QoS model that identifies the QoS for online services, and describes their evaluation approaches. Accordingly, this study helps the researchers in the online service discovery, selection, composition, compliance, ranking, trust, and general management areas by providing a generic QoS model and QoS evaluation approaches to follow in their studies. Experiments are conducted to test various QoS metrics from the proposed QoS model.","","978-1-4799-6065-1978-1-4799-6064","10.1109/IEOM.2015.7228116","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7228116","","Quality of service;Security;Web services;Time factors;Reliability;Measurement;Standards","distributed processing;quality of service","total quality of service model;TQoSM;service selection;online open environments;QoS evaluation techniques;online services;online service discovery;distributed software system","","","41","","","","","","IEEE","IEEE Conferences"
"A Novel Online Estimation Scheme for Static Voltage Stability Margin Based on Relationships Exploration in a Large Data Set","Y. Fan; S. Liu; L. Qin; H. Li; H. Qiu","School of Electrical Engineering, Wuhan University, Wuhan, China; School of Electrical Engineering, Wuhan University, Wuhan, China; School of Electrical Engineering, Wuhan University, Wuhan, China; School of Electrical Engineering, Wuhan University, Wuhan, China; School of Electrical Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Power Systems","","2015","30","3","1380","1393","A novel integrated scheme based on relationships exploration (RE) is presented for the online estimation of the relative voltage stability margin (VSM). The proposed scheme can select the optimal variables as input features for estimation by detecting the relationship between each operation variable and the relative VSM in a large data set. Each relationship is given scores by the maximal information coefficient (MIC) and the Pearson correlation coefficient (PCC). The variables selected for estimation are corresponding to the relationships highly ranked by MIC and PCC, including linear relationships and nonlinear functional ones. Some of the highly ranked relationships are shown, curve fitted and described from the perspective of power system operation. If the data of the selected variables are obtained online from the wide area measurement system (WAMS), the relative VSM can be online estimated based on the explored relationships. The integrated scheme using RE-based process is examined on a 21-bus test system and a practical 1648-bus system provided by the software PSS/E, and it presents acceptable estimation accuracy. The impacts of training set size, selected relationships' types, total number and ranks on estimation accuracy are analyzed. The robustness of the scheme to measurement errors and topology variation is studied.","0885-8950;1558-0679","","10.1109/TPWRS.2014.2349531","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6883220","Large data sets;maximal information coefficient;Pearson correlation coefficient;relationships exploration;voltage stability margin","Estimation;Microwave integrated circuits;Network topology;Power system stability;Stability analysis;Topology;Generators","power system measurement;power system reliability;power system stability;voltage control","static voltage stability margin;online estimation scheme;relationships exploration;large data set;topology variation;measurement errors;WAMS;wide area measurement system;power system operation;nonlinear functional ones;linear relationships;PCC;Pearson correlation coefficient;MIC;maximal information coefficient;relative VSM;operation variable","","9","25","","","","","","IEEE","IEEE Journals & Magazines"
"AEGIS autonomous targeting for the Curiosity rover's ChemCam instrument","R. Francis; T. Estlin; D. Gaines; B. Bornstein; S. Schaffer; V. Verma; R. Anderson; M. Burl; S. Chu; R. Castaño; D. Thompson; D. Blaney; L. de Flores; G. Doran; T. Nelson; R. Wiens","Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Jet Propulsion Laboratory, California Institute of Technology, Pasadena, United States; Los Alamos National Laboratory, New Mexico, United States; Los Alamos National Laboratory, New Mexico, United States","2015 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","","2015","","","1","5","AEGIS (Autonomous Exploration for Gathering Increased Science) is a software suite that will imminently be operational aboard NASA's Curiosity Mars rover, allowing the rover to autonomously detect and prioritize targets in its surroundings, and acquire geochemical spectra using its ChemCam instrument. ChemCam, a Laser-Induced Breakdown Spectrometer (LIBS), is normally used to study targets selected by scientists using images taken by the rover on a previous sol and relayed by Mars orbiters to Earth. During certain mission phases, ground-based target selection entails significant delays and the use of limited communication bandwidth to send the images. AEGIS will allow the science team to define the properties of preferred targets, and obtain geochemical data more quickly, at lower data penalty, without the extra ground-inthe-loop step. The system uses advanced image analysis techniques to find targets in images taken by the rover's stereo navigation cameras (NavCam), and can rank, filter, and select targets based on properties selected by the science team. AEGIS can also be used to analyze images from ChemCam's Remote Micro Imager (RMI) context camera, allowing it to autonomously target very fine-scale features - such as veins in a rock outcrop - which are too small to detect with the range and resolution of NavCam. AEGIS allows science activities to be conducted in a greater range of mission conditions, and saves precious time and command cycles during the rover's surface mission. The system is currently undergoing initial tests and checkouts aboard the rover, and is expected to be operational by late 2015. Other current activities are focused on science team training and the development of target profiles for the environments in which AEGIS is expected to be used on Mars.","2332-5615","978-1-4673-9558","10.1109/AIPR.2015.7444544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444544","autonomous science;mars;planetary exploration;image interpretation;natural scene interpretation","Mars;Earth;Instruments;Rocks;Cameras;Planning","aerospace computing;aerospace robotics;cameras;computerised instrumentation;control engineering computing;object detection;remotely operated vehicles;stereo image processing","AEGIS autonomous targeting;Autonomous Exploration for Gathering Increased Science;ChemCam instrument;NASA Curiosity Mars rover;National Aeronautics and Space Administration;LIBS;laser-induced breakdown spectrometer;geochemical spectra;ground-based target selection;remote micro imager;RMI context camera;image analysis;stereo navigation cameras","","","5","","","","","","IEEE","IEEE Conferences"
"Validation of SAR/ISAR imagery radar processing using simulated raw data showing realistic ship motions","C. L. Barbu; J. Genin; R. Fabbri; C. Cochin; L. Boudet","NA; NA; NA; NA; NA","EUSAR 2014; 10th European Conference on Synthetic Aperture Radar","","2014","","","1","4","The resulting image of a SAR or ISAR processing of a moving ship is highly impacted by its dynamics. TSA (Thales Systemes aeroportes) has studied for DGA (French Ministry of Defence) a new algorithm for airborne radar. Performances have firstly been evaluated using real raw data acquisitions of several scenes. But, in order to extend the available measurement campaigns and to increase the number of test cases, simulated radar signals have been used. The simulation uses for each ship a 3D EM model produced by MOCEM V3 software. A dedicated raw data simulator has been developed by ALYOTECH and is currently optimized in the version V4 of the MOCEM software.","","978-3-8007-3607","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6856797","","","","","","","","","","","","","VDE","VDE Conferences"
"Research on the detection system for impact energy of pneumatic drill based on stress wave technique","Zheng Sulu; Wang Dongdong; Zheng Ruihang; Wang Xiangping; Mao Yunyan","Quzhou quality and technology supervision and inspection center, 324000, Zhejiang, China; Quzhou quality and technology supervision and inspection center, 324000, Zhejiang, China; Quzhou quality and technology supervision and inspection center, 324000, Zhejiang, China; Quzhou quality and technology supervision and inspection center, 324000, Zhejiang, China; Quzhou quality and technology supervision and inspection center, 324000, Zhejiang, China","2015 12th IEEE International Conference on Electronic Measurement & Instruments (ICEMI)","","2015","03","","1415","1419","The impact energy is very important as a performance parameter for pneumatic drill to optimize structure and improve performance. The method to detect impact energy of pneumatic drill with stress wave technology is used widely. According to the one dimension wave method, the working process of pneumatic drill is a process of energy transmission and conversion which in the form of stress wave, and the impact energy could be obtained accurately by detecting the stress wave through any point in the drill rod when piston collides drill rod. In which, the collecting and processing of waveform data is the key technology. In this paper, the theoretical foundation of stress wave technology is analyzed by using one dimension wave theory. According to this theory, the detection system for impact energy of pneumatic drill is designed and developed by using advanced micro computer measuring technology which based on electronic and computer technology. This system consists of calibration device, energy-absorbing device, signal collecting and processing system and PC control software. The calibration device and the energy absorbing device are used in getting calibration factor and reducing the influence of reflected wave respectively and they could meet the relevant requirements of GB5621-2008. The signal collecting and processing system is the most important part of the whole design and the stm32 single ship microcomputer with UCOSII micro operating system is used as control center of it to control the data collecting and processing. The PC control software written by VB could realize the interaction of human and this system. Under the standard experiment condition: atmosphere pressure is 101kPa, the temperature is 16 °C, the pressure of compression air is 0.4MPa., the impact energy of a Kaishan YO18 pneumatic drill is tested by using this system which developed by ourselves, and the stress waveform, the impact energy and impact frequency could be obtained and which indicate this system could collect the stress wave on the drill rod efficiently and calculate the values of impact energy and impact frequency. All of the experimental results could meets the factory index of this machine and the reliability of this system could be proven.","","978-1-4799-7071-1978-1-4799-7618","10.1109/ICEMI.2015.7494516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7494516","stress wave;impact energy;detection system","Energy measurement;Performance evaluation;Stress measurement;Calibration;Reliability;Printers;Pistons","","","","1","12","","","","","","IEEE","IEEE Conferences"
"Simulation tools for model-based robotics: Comparison of Bullet, Havok, MuJoCo, ODE and PhysX","T. Erez; Y. Tassa; E. Todorov","University of Washington, Departments of Applied Mathematics and Computer Science &amp; Engineering, USA; University of Washington, Departments of Applied Mathematics and Computer Science &amp; Engineering, USA; University of Washington, Departments of Applied Mathematics and Computer Science &amp; Engineering, USA","2015 IEEE International Conference on Robotics and Automation (ICRA)","","2015","","","4397","4404","There is growing need for software tools that can accurately simulate the complex dynamics of modern robots. While a number of candidates exist, the field is fragmented. It is difficult to select the best tool for a given project, or to predict how much effort will be needed and what the ultimate simulation performance will be. Here we introduce new quantitative measures of simulation performance, focusing on the numerical challenges that are typical for robotics as opposed to multi-body dynamics and gaming. We then present extensive simulation results, obtained within a new software framework for instantiating the same model in multiple engines and running side-by-side comparisons. Overall we find that each engine performs best on the type of system it was designed and optimized for: MuJoCo wins the robotics-related tests, while the gaming engines win the gaming-related tests without a clear leader among them. The simulations are illustrated in the accompanying movie.","1050-4729","978-1-4799-6923-4978-1-4799-6921","10.1109/ICRA.2015.7139807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7139807","","Engines;Joints;Accuracy;Computational modeling;Robot kinematics;Mathematical model","control engineering computing;digital simulation;middleware;robots","simulation tools;model-based robotics;Bullet;Havok;MuJoCo;ODE;PhysX;gaming engines","","23","30","","","","","","IEEE","IEEE Conferences"
"Tutorial T4: MEMS: Design, Fabrication, and their Applications as Chemical and Biosensors","N. S. Kale","NA","2015 28th International Conference on VLSI Design","","2015","","","8","9","The microfabrication technology has had a chequered history of over 50 years in the field of microelectronics. Aggressive miniaturization of microelectronic devices has resulted in faster logic circuits and it has also reduced their power requirements. MOSFET device dimensions have already entered the sub-100 nanometer regime. The same principles of microfabrication were applied to make miniaturized 3-dimensional mechanical structures. This helped in the advent of micro electro-mechanical systems or MEMS. Initially, i.e. in early nineties, the MEMS field was dominated by mechanical applications. However, now MEMS refers to all miniaturized systems including silicon based mechanical drivers, chemical and biological sensors and actuators, and miniature devices made from plastics or ceramics. The half-day tutorial would begin with a synoptic overview of the area, highlight some of the challenges and outline the scope of the tutorial. It would be followed with an introduction to the design of microsensors, such as the pressure sensor and the accelerometer that began the MEMS revolution. Micromachined Electro-Mechanical Systems (MEMS), also called Microfabricated Systems (MS), have evoked great interest in the scientific and engineering communities. This is primarily due to several substantive advantages that MEMS offer: orders of magnitude smaller size, better performance than other solutions, possibilities for batch fabrication and cost-effective integration with electronics, virtually zero dc power consumption and potentially large reduction in power consumption, etc. The application domains cover microsensors and actuators for physical quantities (MEMS), of which MEMS for automobile &amp; consumer electronics forms a large segment; microfabricated subsystems for communications and computer systems (RF-MEMS &amp; MOMS); and microfabricated systems for chemical assay (microTAS) and for biochemical and biomedical assay (bioMEMS and DNA chips). This tutorial would give an introduction to these exciting developments and the technology and design approaches for the realization of these integrated systems. We will also introduce the importance of material selection by understanding the impact of material properties, even at the micron scale. We will discuss polymeric materials such as SU-8 and also compare them with traditional materials such as Silicon. We will also discuss about the possibility of integrating MEMS with VLSI electronics. Simulators provide an excellent way to design, optimize and understand micromechanical systems. Particularly so because such systems are not of isolated, stand alone type; instead, they are based on the interplay of several domains. For example, in a microcantilever based biosensing system the different domains are: materials, mechanical, biological, electrical and chemical. Recently developed software packages such as Coventorware, Intellisuite etc. have the ability to simulate a system in different domains. One can, for example, use a thermoelectromechanical solver (i.e. study a system in the domains of temperature, mechanics and electricity). We will discuss basic philosophy of using MEMS simulation tools for simple devices. Unit processes for bulk and surface micromachining of silicon and integration of processes for fabricating silicon microsensors will be presented. Smart cell phones and wireless enabled devices are poised to become commercial engines for the next generation of MEMS, since MEMS provide not only better functionality with smaller chip area, but also alternative transceiver fabrication, characterization etc) include printerheads for inkjet printers, Digital Micromirror Devices and pressure sensors. We will discuss applications of microcantilevers &amp; microheaters in detecting volatile organic compounds, and show that these devices can detect in particles in the order of a few parts per billion. We shall have a lecture on bioMEMS to highlight the immense possibilities that exist for MEMS in the life sciences &amp; medicine. The idea of integrating microfluidics and biological or biomimetic material with electronic systems is alien to electronic systems designers and there are problems with integrating wet systems with electronics. We give a synopsis of the types of structures required and approaches for the design and test of such systems. Finally, we shall discuss the issues involved with embedding MEMS in complete systems, including issues related to design tools, simulation, test and parameter extraction &amp; de-embedding. Ofcourse, we will present the future of MEMS (and NEMS); and they role they would play in smartwatches, mobiles, diagnostics; and thereby impact our lives.","1063-9667;2380-6923","978-1-4799-6658","10.1109/VLSID.2015.112","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7031693","","Micromechanical devices;Tutorials;Chemicals;Nanobioscience;Silicon;Very large scale integration","accelerometers;bioMEMS;biomimetics;biosensors;cantilevers;chemical sensors;DNA;elemental semiconductors;logic circuits;microactuators;microfluidics;micromirrors;MOSFET;nanosensors;pressure sensors;radio transceivers;silicon","smart cell phones;wireless enabled devices;transceiver fabrication;digital micromirror devices;microheaters;microfluidics;biological material;biomimetic material;NEMS;Intellisuite;Coventorware;microcantilever;SU-8 polymeric materials;DNA chips;bioMEMS;biomedical assay;biochemical assay;microTAS;consumer electronics;automobile electronics;dc power consumption;accelerometer;pressure sensor;microsensors;actuators;silicon based mechanical drivers;3-dimensional mechanical structures;MOSFET;logic circuits;microelectronic devices;biosensors;chemical sensors;micromachined electromechanical systems;Si","","","","","","","","","IEEE","IEEE Conferences"
"Characterization of detection limits using mock waste matrices in a<sup>3</sup>He passive drum counter for plutonium waste verification","M. G. Paff; B. Pedersen; J. Crochemore","Department of Nuclear Engineering and Radiological Sciences at the University of Michigan, Ann Arbor, 48109 USA; Nuclear Security Unit at the European Commission Joint Research Centre, Ispra, VA 21027 Italy; Nuclear Security Unit at the European Commission Joint Research Centre, Ispra, VA 21027 Italy","2013 IEEE Nuclear Science Symposium and Medical Imaging Conference (2013 NSS/MIC)","","2013","","","1","4","Waste streams of fuel cycle, research facilities or decommissioning activities may contain measurable quantities of plutonium that must be accounted for under international safeguards agreements. Inspectors must perform measurements to verify operator declared plutonium masses within waste matrices. In some cases, inspectors might be asked to verify the presence of only a few grams of plutonium distributed in a waste drum. Neutron coincidence measurements are a standard tool to measure<sup>240</sup>Pu content due to its high spontaneous fission rate. Given prior knowledge of isotopics, total plutonium mass can be estimated. For such measurements, the European Commission's Joint Research Centre (JRC) maintains, tests and upgrades a<sup>3</sup>He drum monitor for deployment to European nuclear facilities at the request of EURATOM. This drum monitor consists of 148 standard<sup>3</sup>He tubes in a 4π geometry for passive neutron coincidence measurements using a shift register and standard INCC software. The neutron drum monitor is designed to handle standard 208 L (55 gallon) waste drums. The drum counter's design is optimized to handle conditioned waste drums of low plutonium content. This nondestructive measurement tool recently underwent extensive electronic and structural refurbishments resulting in CE certification (compliance with relevant European Union legislation). Before redeployment, the passive neutron detection system must undergo an extensive measurement campaign to ascertain its limits of detection of plutonium in a variety of waste matrices. For this purpose, mock waste matrices were produced. These consist of concrete filled standard waste drums. A number of cavities at different distances from the drum center allow for a variety of source locations to be tested. Plutonium metal and oxide samples ranging from milligram to gram quantities were measured during this campaign.","1082-3654","978-1-4799-0534-8978-1-4799-0533-1978-1-4799-3423","10.1109/NSSMIC.2013.6829639","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6829639","","Neutrons;Monitoring;Europe;Standards;Concrete;Joints","helium-3 counters;radioactive waste","detection limit characterization;mock waste matrices;helium-3 passive drum counter;plutonium waste verification;fuel cycle waste streams;research facilities;decommissioning activities;plutonium masses;waste matrices;neutron coincidence measurements;high spontaneous fission rate;European Commission JRC;Joint Research Centre;European nuclear facilities;EURATOM;neutron drum monitor;plutonium metal sample;oxide sample","","","5","","","","","","IEEE","IEEE Conferences"
"ALEA: Fine-Grain Energy Profiling with Basic Block Sampling","L. Mukhanov; D. S. Nikolopoulos; B. R. d. Supinski","NA; NA; NA","2015 International Conference on Parallel Architecture and Compilation (PACT)","","2015","","","87","98","Energy efficiency is an essential requirement for all contemporary computing systems. We thus need tools to measure the energy consumption of computing systems and to understand how workloads affect it. Significant recent research effort has targeted direct power measurements on production computing systems using on-board sensors or external instruments. These direct methods have in turn guided studies of software techniques to reduce energy consumption via workload allocation and scaling. Unfortunately, direct energymeasurementsarehamperedbythelowpowersampling frequency of power sensors. The coarse granularity of power sensing limits our understanding of how power is allocated in systems and our ability to optimize energy efficiency via workload allocation. We present ALEA, a tool to measure power and energy consumption at the granularity of basic blocks, using a probabilistic approach. ALEA provides fine-grained energy profiling via statistical sampling, which overcomes the limitations of power sensing instruments. Compared to state-of-the-art energy measurement tools, ALEA provides finer granularity without sacrificing accuracy. ALEA achieves low overhead energy measurements with mean error rates between 1.4% and 3.5% in 14 sequential and parallel benchmarks tested on both Intel and ARM platforms. The sampling method caps execution time overhead at approximately 1%. ALEA is thus suitable for online energy monitoring and optimization. Finally, ALEA is a user-space tool with a portable, machine-independent sampling method. We demonstrate three use cases of ALEA, where we reduce the energy consumption of a k-means computational kernel by 37%, an ocean modeling code by 33%, and a ray tracing code by 6% compared to high-performance execution baselines, by varying the power optimization strategy between basic blocks.","1089-795X","978-1-4673-9524","10.1109/PACT.2015.16","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429297","energy profiling;sampling;energy efficiency;power measurement;ALEA","Energy measurement;Power measurement;Energy consumption;Instruments;Hardware;Radiation detectors","power aware computing;probability;sampling methods","ALEA;fine-grain energy profiling;basic block sampling;energy efficiency;energy consumption;onboard sensor;direct energy measurement;power sampling frequency;power sensor;coarse granularity;power consumption;probabilistic approach;statistical sampling;machine-independent sampling method;k-means computational kernel;ocean modeling code;ray tracing code","","3","35","","","","","","IEEE","IEEE Conferences"
"Numerical Simulation of Inertial Navigation Technology and Arbitrary Path Generator","L. Zhigang; W. Dandan; H. Tao","NA; NA; NA","2014 Fourth International Conference on Instrumentation and Measurement, Computer, Communication and Control","","2014","","","569","572","To test the hardware or software of a strap-down inertial navigation system, or conduct a related simulation study, the flight trajectory data and the output parameters of gyroscope and accelerometer should be known in advance. And in practice a set of discrete data is often used to represent flight trajectory information. Therefore this paper proposes a new method that can produce discrete trajectory of any path, so that we can quickly generate required simulation trajectories according to the actual task requirements. This paper also optimizes the inertial device simulation model, presents a co-simulation idea of the trajectory generator and inertial navigation system. And this paper gives the simulation results of the whole system. By analyzing the simulation results, we can verify the correctness and feasibility of the method described above and the overall design of the simulation system.","","978-1-4799-6575-5978-1-4799-6574","10.1109/IMCCC.2014.122","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6995092","numerical simulation;SINS;trajetory generator;IMU","Trajectory;Generators;Mathematical model;Data models;Inertial navigation;Acceleration","accelerometers;gyroscopes;inertial navigation;numerical analysis","numerical simulation;inertial navigation technology;arbitrary path generator;strap-down inertial navigation system;flight trajectory data;gyroscope;accelerometer;inertial device simulation model;cosimulation idea;trajectory generator;inertial navigation system","","","8","","","","","","IEEE","IEEE Conferences"
"An effective and systematic design FMEA approach","B. Denson; S. Y. Tang; K. Gerber; V. Blaignan","Reliability and Mechanical Science Department, SP-TD-01, Corning Incorporated, Corning, NY 14931; Reliability and Mechanical Science Department, SP-TD-01, Corning Incorporated, Corning, NY 14931; Reliability and Mechanical Science Department, SP-TD-01, Corning Incorporated, Corning, NY 14931; Niceasoft Inc., 10740 Ridgeview Acres Rd., Corning, NY 14830","2014 Reliability and Maintainability Symposium","","2014","","","1","6","An innovative approach has been developed to objectively and systematically identify failure causes of a product in real time, using a reliability physics methodology. It is intended to overcome problems associated with traditional design FMEAs, which can be inefficient in aiding the design process in a timely fashion. The approach is based on the premise that, at the most fundamental level, failure causes are a result of a manageable number of predictable processes. These processes result when combinations of material type, stress and conditions occur. A knowledge base forms the basis for this methodology and is in the form of a multidimensional matrix that relates element type, stresses, conditions, processes, resulting stresses, resulting conditions, and failure modes. A software tool has been developed that enables the analysis based on rudimentary data elements of a design that are entered by the user. It then generates output tables that can be used by the system designer or reliability engineer for a variety of purposes that require the hypothesizing of possible failure causes, including optimizing the design, developing reliability test plans, and root cause analysis.","0149-144X","978-1-4799-2848-4978-1-4799-2847","10.1109/RAMS.2014.6798488","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6798488","FMEA;Physical Reliability Models;Reliability Modeling;Risk Analysis and Management;GURU","Stress;Materials;Reliability engineering;Logic gates;Knowledge based systems","design;matrix algebra;production engineering computing;reliability","systematic design FMEA approach;failure mode and effect analysis;reliability physics methodology;product failure;design process;material type;material stress;material conditions;multidimensional matrix;rudimentary data elements;root cause analysis","","2","5","","","","","","IEEE","IEEE Conferences"
"Large Area Experimental Telemetry Network for Infrastructure Monitoring Applications","J. Robert; H. Lieske; A. Heuberger; J. Bernhard; G. Kilian","NA; NA; NA; NA; NA","Smart SysTech 2015; European Conference on Smart Objects, Systems and Technologies","","2015","","","1","6","The topic of machine to machine (M2M) communications has gained increasing interest over the recent years. The Lehrstuhl fuer Informationstechnik mit dem Schwerpunkt Kommunikationselektronik (LIKE) and the Fraunhofer IIS have jointly developed an M2M ultra-low power telemetry concept that allows for transmission ranges of up to several km with very tiny transmitter sizes. For testing the performance of our telemetry concept we built up an experimental telemetry network that can be used e.g. for infrastructure monitoring applications. The telemetry network offers a high degree of flexibility based on a software-defined radio architecture for the evaluation and optimization of our telemetry concept. The receiver stations digitize complete frequency bands and transmit the sampled baseband data to a data collection server. Decoders can connect to the data collection server and decode different wireless protocol standards. Finally, the decoded data are written into a database, which makes them accessible for a wide variety of applications. The current network consists of four outdoor receiver stations, which cover a large area in the region of Erlangen/Nuremberg. A continuous temperature measurement demonstrator proves the high reliability of the system.","","978-3-8007-3996","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7323350","","","","","","","","","","","","","VDE","VDE Conferences"
"A high-speed data readout method based on ethernet for particle physics experiment","X. Huang; P. Cao; L. Gao; X. Lin; X. Ji","State Key Laboratory of Particle Detection and Electronics, University of Science and Technology of China, Hefei, 230026 China; State Key Laboratory of Particle Detection and Electronics, University of Science and Technology of China, Hefei, 230026 China; State Key Laboratory of Particle Detection and Electronics, University of Science and Technology of China, Hefei, 230026 China; State Key Laboratory of Particle Detection and Electronics, University of Science and Technology of China, Hefei, 230026 China; Institute of High Energy Physics, Chinese Academy of Sciences, Beijing, 100049 China","2014 19th IEEE-NPSS Real Time Conference","","2014","","","1","1","Generally, the readout system for a typical particle physics experiment is implemented with multi electronic modules resided in a standard crate, e.g. VME. Each module receives data from the front-end electronics (FEE) and transmits them to the crate controller for transferring to the data acquisition (DAQ) system in real time. With the increasing physical event rate and number of electronic channels, this readout scheme meets the challenge of improving readout speed caused by the limited bandwidth of crate backplane. One of valid methods is to adopt crates with high performance backplane, e.g. ATCA. In this paper, instead of improving backplane performance, we make each readout module having capability of transmitting data to DAQ. A high-speed data readout method based on Ethernet is designed for each module. Features of explicitly parallel data transmitting and distributed network architecture make the readout system has advantage of adapting varying requirements of particle physics experiments. To guarantee the readout performance and flexibility, a standalone embedded CPU system is utilized for network protocol stack processing. To receive customized data format and protocol from FEE, a field programmable gate array (FPGA) is used for logic reconfiguration. To optimize the interface and improve the data swap speed between CPU and FPGA, a sophisticated method based on SRAM is presented in this paper. In this method, in the aspect of hardware, FPGA fakes itself into a SRAM for CPU. Data from FEE is cached firstly in FPGA and then put on the SRAM bus for CPU fetching. In the aspect of software, Linux operating system pre-allocates physical address for this fake SRAM, and the embedded device driver controls and manages it. To synchronize the data swap, a hand-shaking scheme is put forward in this method. For the purpose of evaluating this high-speed readout method, a simplified readout module is designed and implemented. Test results show that this module can support up to 70Mbps data throughput from the readout module to DAQ smoothly.","","978-1-4799-3659-5978-1-4799-3658","10.1109/RTC.2014.7097519","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7097519","","Physics;Data acquisition;Field programmable gate arrays;Random access memory;Backplanes;Electronic mail;Protocols","computerised instrumentation;data acquisition;data communication;device drivers;embedded systems;field programmable gate arrays;Linux;local area networks;nuclear electronics;readout electronics;SRAM chips","high-speed data readout method;Ethernet;particle physics experiment;multi electronic modules;frontend electronics;data transmission;data acquisition system;electronic channel;DAQ;distributed network architecture;standalone embedded CPU system;network protocol stack processing;customized data format;FEE;field programmable gate array;logic reconfiguration;data swap;FPGA;SRAM;Linux operating system;embedded device driver;hand-shaking scheme;simplified readout module","","","","","","","","","IEEE","IEEE Conferences"
"Losses minimization in network reconfiguration for fault restoration via a uniform crossover of genetic algorithm","N. H. Shamsudin; M. S. S. M. Basir; A. R. Abdullah; M. F. Sulaima; E. F. Shair","Faculty of Electrical Engineering, Universiti Teknikal Malaysia Melaka, Hang Tuah Jaya, 76100 Durian Tunggal, Malaysia; Faculty of Electrical Engineering, Universiti Teknikal Malaysia Melaka, Hang Tuah Jaya, 76100 Durian Tunggal, Malaysia; Faculty of Electrical Engineering, Universiti Teknikal Malaysia Melaka, Hang Tuah Jaya, 76100 Durian Tunggal, Malaysia; Faculty of Electrical Engineering, Universiti Teknikal Malaysia Melaka, Hang Tuah Jaya, 76100 Durian Tunggal, Malaysia; Faculty of Electrical Engineering, Universiti Teknikal Malaysia Melaka, Hang Tuah Jaya, 76100 Durian Tunggal, Malaysia","2015 International Symposium on Technology Management and Emerging Technologies (ISTMET)","","2015","","","330","334","Fault is a type of disturbance that affecting the continuity of the power supply to loads. Therefore, it is essential for a distribution power system to have a flexible, stable and more reliable load restoration system. The aim of the load restoration in this paper is to restore as many loads as possible through the network reconfiguration while minimizing the power losses after the occurrences of fault. Distribution network reconfiguration (DNR) is applied to determine the best combination of open switches that acts as the best route to optimize the reduction of power losses during load restoration process. An improved genetic algorithm (IGA) is proposed in this paper. The algorithm proposed is tested and validated on 69 IEEE bus using MATLAB software. A detail analysis is performed to demonstrate the effectiveness of IGA. The proposed method is applied and the effects of method on the power losses are examined. Results show that IGA method for load restoration via DNR is more effective compared with genetic algorithm (GA) solution.","","978-1-4799-1723-5978-1-4799-1722","10.1109/ISTMET.2015.7359053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359053","Load Restoration;Fault Occurrences;Distribution Network Reconfiguration (DNR;Reconfiguration;Genetic algorithm (GA);Improved Genetic Algorithm (IGA)","Genetic algorithms;Circuit faults;Genetics;Biological cells;Electric breakdown;Sociology;Statistics","genetic algorithms;IEEE standards;power distribution faults;power system restoration","network reconfiguration;fault restoration;power supply continuity;distribution power system;load restoration;power losses minimization;distribution network reconfiguration;DNR;power loss reduction;IGA;IEEE bus;improved genetic algorithm","","2","10","","","","","","IEEE","IEEE Conferences"
"The McCad Code for the Automatic Generation of MCNP 3-D Models: Applications in Fusion Neutronics","F. Moro; U. Fischer; L. Lu; P. Pereslavtsev; S. Podda; R. Villari","Association EURATOM-ENEA, Rome, Italy; Association KIT-Euratom, Eggenstein-Leopoldshafen, Germany; Association KIT-Euratom, Eggenstein-Leopoldshafen, Germany; Association KIT-Euratom, Eggenstein-Leopoldshafen, Germany; Association EURATOM-ENEA, Rome, Italy; Association EURATOM-ENEA, Rome, Italy","IEEE Transactions on Plasma Science","","2014","42","4","1036","1041","The Monte Carlo (MC) code MCNP is the reference tool in fusion neutronics, allowing the description and analysis of full and detailed 3-D geometry of a tokamak machine. The geometrical models of the components used are typically available through computer aided design (CAD) files: the main benefits of this system are related to its portability and compatibility with several tools commonly used in engineering analyses. However, at the present stage, the information contained in CAD files cannot be directly provided to MC as inputs, because of the different representation scheme used. This issue leads to the necessity to develop interfaces that can translate them into the correct MC geometrical description. McCad is a software developed by the Karlsruhe Institute of Technology, dedicated to the fully automated generation of the MCNP geometrical models from CAD files (step, iges, and brep formats): it is provided with a graphical user interface allowing the visualization of the geometries and tools for data exchange and modeling. This paper summarizes the results of some benchmark tests performed on JET components and a DEMO reactor aimed at the assessment of the suitability of McCad for fusion neutronic applications. The reliability of the conversion algorithm has been evaluated comparing the results of stochastic MCNP volume calculations carried out using the generated models, and the corresponding volumes provided by the CAD kernel of the interface program. Moreover, the consistency of a converted DEMO MCNP model has been verified through particle transport calculations for the estimation of the neutron wall loading poloidal distribution. Several aspects related to the use of the code have been evaluated such as its portability, performances, and impact of the geometric approximation introduced on the neutronic analyses. Furthermore, a useful feedback for the optimization and enhancement of the McCad interface has been provided.","0093-3813;1939-9375","","10.1109/TPS.2014.2308957","European Community through the EURATOM and the European Fusion Development Agreement; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6778760","Analysis;DEMO;design;JET;neutronics.;Analysis;DEMO;design;JET;neutronics","Solid modeling;Design automation;Geometry;Solids;Computational modeling;Monte Carlo methods;Analytical models","CAD;fusion reactor design;Monte Carlo methods;neutron physics;nuclear engineering computing;Tokamak devices","McCad Code;MCNP 3D model;fusion neutronics;3D geometry;tokamak machine;computer aided design;engineering analyses;CAD files;JET components;DEMO reactor","","1","11","","","","","","IEEE","IEEE Journals & Magazines"
"Direct modeling of scintillator thickness for optimal light output and spatial resolution","S. E. Mitchell; A. Luttman; M. Fowler; K. T. Joyce","National Security Technologies, LLC, North Las Vegas, NV, 89030; National Security Technologies, LLC, North Las Vegas, NV, 89030; National Security Technologies, LLC, North Las Vegas, NV, 89030; National Security Technologies, LLC, North Las Vegas, NV, 89030","2013 19th IEEE Pulsed Power Conference (PPC)","","2013","","","1","5","It is common in x-ray radiography to use scintillators (e.g., BGO or LSO) to convert x-rays to visible light, which is then recorded by an imaging system. The response of the scintillator depends fundamentally on its thickness, with respect to both its visible light emittance and its spatial resolution. This is important for optimizing light output, signal to noise ratio, or optical response time. Given that it is often cost-prohibitive to procure a variety of scintillator samples and empirically test the performance, it is essential to be able to model and accurately simulate the performance of a scintillator with respect to thickness and other properties, and a direct way of doing this is using Monte Carlo-based radiation transport codes. Such simulations can be expensive in terms of computational time, and the codes are not easily obtained. In this work we first show such simulations, and demonstrate that there is a natural trade-off between light output of a scintillator and its spatial resolution. We then derive a first-principles model that accurately approximates the light output, using straightforward calculations that can be performed quickly with any basic computing software. We compare the results to those obtained from Monte Carlo simulations and show that our simplified model can be used to analyze the tradeoff between emittance and resolution nearly as well as using a full-scale radiation transport code.","2158-4915;2158-4923","978-1-4673-5168-3978-1-4673-5167","10.1109/PPC.2013.6627699","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6627699","","Photonics;Computational modeling;Spatial resolution;Monte Carlo methods;Crystals;Approximation methods;Analytical models","Monte Carlo methods;scintillation counters;X-ray imaging","first principle model;Monte Carlo based radiation transport code;imaging system;X-ray radiography;spatial resolution;optimal light output;scintillator thickness","","","7","","","","","","IEEE","IEEE Conferences"
"Low power electrocardiography and impedance cardiography detection system based on LabVIEW and Bluetooth Low Energy","X. Chen; J. Xie; Z. Fang; S. Xia","Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China; Institute of Electronics, Chinese Academy of Sciences, Beijing, China","2015 IET International Conference on Biomedical Image and Signal Processing (ICBISP 2015)","","2015","","","1","4","Due to the sporadic occurrence of cardiovascular disease, long term continuous monitoring is needed. An embedded wireless health monitoring system is designed and implemented, which is made up of wearable monitoring nodes with Bluetooth Low Energy (BLE) wireless transmission, a sensor data gateway and smart terminal etc. The wearable monitoring nodes are integrated electrocardiography (ECG) and impedance cardiography (ICG) sensors, which can not only detect ECG (indication of electric working status of heart) and ICG (indication of mechanical pumping working performance of heart), but also detect respiration, temperature, heart rate variability and moving status of the patient in real time. The low power of the sensor node is achieved through the optimized design of the circuit and firmware. The software architecture of the sensor data gateway with a WebSocket data server implemented is constructed with multi-thread mechanism and object-oriented method of LabVIEW to achieve stable waveform data transmission. Noise and baseline wandering of ECG and ICG are removed by Undecimated Wavelet Transform (UWT), which is also used in real-time feature extraction of ECG and ICG waveform. Test results show that the current-consuming of the sensor node in working state with BLE transmission is 11mA and 69μA for standby low power mode. The sensor data gateway supports connection with any mobile terminal device with Wi-Fi capability.","","978-1-78561-045-5978-1-78561-044","10.1049/cp.2015.0798","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450374","Healthcare;Electrocardiography;Impedance Cardiography;Bluetooth Low Energy;LabVIEW","","bioelectric potentials;biomedical telemetry;diseases;electric impedance;electrocardiography;feature extraction;medical signal processing;patient monitoring;wavelet transforms","low power electrocardiography;impedance cardiography detection system;labview;cardiovascular disease;wireless health monitoring system;wearable monitoring nodes;bluetooth low energy wireless transmission;sensor data gateway;smart terminal;impedance cardiography sensor;heart mechanical pumping working performance;heart rate variability;WebSocket data server;waveform data transmission;undecimated wavelet transform;ECG waveform feature extraction;ICG waveform feature extraction;mobile terminal device;Wi-Fi capability","","2","","","","","","","IET","IET Conferences"
"Precision irrigation performance measurement using wireless sensor network","I. Mat; M. R. M. Kassim; A. N. Harun","MIMOS, Ministry of Science, Technology and Innovation, Kuala Lumpur, Malaysia; MIMOS, Ministry of Science, Technology and Innovation, Kuala Lumpur, Malaysia; MIMOS, Ministry of Science, Technology and Innovation, Kuala Lumpur, Malaysia","2014 Sixth International Conference on Ubiquitous and Future Networks (ICUFN)","","2014","","","154","157","It is observed that farmers have to bear huge financial loss due to wrong prediction of weather and incorrect irrigation method to crops. Evolution of wireless sensor network (WSN) technology enabled automatic irrigation in a greenhouse for Precision Agriculture (PA) application. In this paper, we describe the design requirements, system architecture, implementation, system test results of WSN for PA in a greenhouse. Sensors are used to monitor temperature, humidity and moisture. Software monitors data from the sensors in a feedback loop which activates the control devices based on threshold value. Implementation of WSN in PA will optimize the usage of water fertilizer and also maximised the yield of the crops.","2165-8528;2165-8536","978-1-4799-3494","10.1109/ICUFN.2014.6876771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6876771","Wireless Sensor Network (WSN);Sensors;Precision Agriculture (PA);Greenhouse Monitoring System (GHMS);Irrigation Management;Moisture Sensor","Irrigation;Wireless sensor networks;Sensors;Moisture;Monitoring;Fertilizers","agriculture;computerised monitoring;crops;fertilisers;greenhouses;humidity;irrigation;moisture;precision engineering;wireless sensor networks","financial loss;wireless sensor network technology;automatic irrigation;greenhouse;precision agriculture application;design requirements;system architecture;WSN;PA application;temperature monitoring;humidity monitoring;moisture monitoring;feedback loop;control devices;threshold value;water fertilizer;crops;precision irrigation performance measurement","","3","16","","","","","","IEEE","IEEE Conferences"
"Robotic additive manufacturing along curved surface — A step towards free-form fabrication","G. Q. Zhang; W. Mondesir; C. Martinez; X. Li; T. A. Fuhlbrigge; H. Bheda","ABB US Corporate Research Center, Mechatronic and Sensor Department, 5 Waterside Crossing, Windsor. CT 06095, USA; Arevo Labs Inc., 2960 Scott Blvd., Santa Clara, California 95054, USA; ABB US Corporate Research Center, Mechatronic and Sensor Department, 5 Waterside Crossing, Windsor. CT 06095, USA; ABB US Corporate Research Center, Mechatronic and Sensor Department, 5 Waterside Crossing, Windsor. CT 06095, USA; ABB US Corporate Research Center, Mechatronic and Sensor Department, 5 Waterside Crossing, Windsor. CT 06095, USA; Arevo Labs Inc., 2960 Scott Blvd., Santa Clara, California 95054, USA","2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)","","2015","","","721","726","This paper presents a method and process to fabricate parts or structures by printing along curved surfaces with use of industrial robots. Software is developed to generate the fabrication path program that can take advantage of the 6-DOF articulated industrial robotics. Unlike the traditional vertically layer-by-layer 3D printing scheme, with the angled printing capability, a robotic fabrication path can be designed along the direction of curved surfaces or the direction of physical property requirements such as higher mechanical strength, specific thermal, electrical and even biomedical characteristics. The fabricating path program such as G-code will be then converted to a robot program such as RAPID. The part building process will be simulated and optimized for given robot specification, mounting configuration and building plate location. Robot dispensing software and hardware interface are employed to coordinate the motion speed and material extrusion rate. Robotic Additive Manufacturing (RAM) testing cells have been built to carry out and verify the printing scheme along curved surface, which is one step further towards free-form fabrication.","","978-1-4673-9675-2978-1-4673-9674","10.1109/ROBIO.2015.7418854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7418854","","Robot kinematics;Service robots;Printing;Buildings;Random access memory;Additives","industrial robots;rapid prototyping (industrial);three-dimensional printing","robotic additive manufacturing;curved surface;free-form fabrication;fabrication path program;6-DOF articulated industrial robotics;3D printing scheme;physical property requirements","","3","13","","","","","","IEEE","IEEE Conferences"
"SLIC: Statistical learning in chip","R. D. Blanton; X. Li; K. Mai; D. Marculescu; R. Marculescu; J. Paramesh; J. Schneider; D. E. Thomas","Electrical and Computer Engineering Department, Carnegie Mellon University Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University Pittsburgh, PA 15213, USA; Electrical and Computer Engineering Department, Carnegie Mellon University Pittsburgh, PA 15213, USA","2014 International Symposium on Integrated Circuits (ISIC)","","2014","","","119","123","Despite best efforts, integrated systems are “born” (manufactured) with a unique `personality' that stems from our inability to precisely fabricate their underlying circuits, and create software a priori for controlling the resulting uncertainty. It is possible to use sophisticated test methods to identify the best-performing systems but this would result in unacceptable yields and correspondingly high costs. The system personality is further shaped by its environment (e.g., temperature, noise and supply voltage) and usage (i.e., the frequency and type of applications executed), and since both can fluctuate over time, so can the system's personality. Systems also “grow old” and degrade due to various wear-out mechanisms (e.g., negative-bias temperature instability), and unexpectedly due to various early-life failure sources. These “nature and nurture” influences make it extremely difficult to design a system that will operate optimally for all possible personalities. To address this challenge, we propose to develop statistical learning in-chip (SLIC). SLIC is a holistic approach to integrated system design based on continuously learning key personality traits on-line, for self-evolving a system to a state that optimizes performance hierarchically across the circuit, platform, and application levels. SLIC will not only optimize integrated-system performance but also reduce costs through yield enhancement since systems that would have before been deemed to have weak personalities (unreliable, faulty, etc.) can now be recovered through the use of SLIC.","2325-0631","978-1-4799-4833","10.1109/ISICIR.2014.7029574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7029574","Integrated system design;low-power design;statistical and machine learning","Data models;Integrated circuit modeling;Algorithm design and analysis;System-on-chip;Aging;Statistical learning;Uncertainty","integrated circuit design;learning (artificial intelligence);low-power electronics","statistical learning in-chip;integrated system design;SLIC;low-power design","","2","20","","","","","","IEEE","IEEE Conferences"
"An integrated gate driver in 4H-SiC for power converter applications","M. N. Ericson; S. S. Frank; C. L. Britton; L. D. Marlino; D. D. Janke; D. B. Ezell; S. Ryu; R. Lamichhane; A. M. Francis; P. D. Shepherd; M. D. Glover; H. A. Mantooth; B. Whitaker; Z. Cole; B. Passmore; T. McNutt","Oak Ridge National Laboratory, Oak Ridge, TN; Oak Ridge National Laboratory, Oak Ridge, TN; Oak Ridge National Laboratory, Oak Ridge, TN; Oak Ridge National Laboratory, Oak Ridge, TN; Oak Ridge National Laboratory, Oak Ridge, TN; Oak Ridge National Laboratory, Oak Ridge, TN; CREE Semiconductor, Durham, NC; University of Arkansas, Fayetteville, AR; University of Arkansas, Fayetteville, AR; University of Arkansas, Fayetteville, AR; University of Arkansas, Fayetteville, AR; University of Arkansas, Fayetteville, AR; Arkansas Power Electronics Intl. Inc. Fayetteville, AR; Arkansas Power Electronics Intl. Inc. Fayetteville, AR; Arkansas Power Electronics Intl. Inc. Fayetteville, AR; Arkansas Power Electronics Intl. Inc. Fayetteville, AR","2014 IEEE Workshop on Wide Bandgap Power Devices and Applications","","2014","","","66","69","A gate driver fabricated in a 2-μm 4H silicon carbide (SiC) process is presented. This process was optimized for vertical power MOSFET fabrication but accommodated integration of a few low-voltage device types including N-channel MOSFETs, resistors, and capacitors. The gate driver topology employed incorporates an input level translator, variable power connections, and separate power supply connectivity allowing selection of the output signal drive amplitude. The output stage utilizes a source follower pull-up device that is both overdriven and body source connected to improve rise time behavior. Full characterization of this design driving a SiC power MOSFET is presented including rise and fall times, propagation delays, and power consumption. All parameters were measured to elevated temperatures exceeding 300°C. Details of the custom test system hardware and software utilized for gate driver testing are also provided.","","978-1-4799-5493","10.1109/WiPDA.2014.6964626","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6964626","Gate Driver;SiC;Silicon Carbide;Wide Bandgap;Power Electronics","Decision support systems","capacitors;energy consumption;MOSFET;network topology;power convertors;resistors;silicon compounds;wide band gap semiconductors","integrated gate driver;power converter applications;silicon carbide process;vertical power MOSFET fabrication;low-voltage device;N-channel MOSFET;resistors;capacitors;gate driver topology;variable power connections;power supply connectivity;rise time behavior;propagation delays;fall time behavior;SiC","","3","5","","","","","","IEEE","IEEE Conferences"
"Semiautomated Model Validation of Power Plant Equipment Using Online Measurements","P. Pourbeik; R. Rhinier; S. Hsu; B. Agrawal; R. Bisbee","Electrical Power Research Institute, Cary, USA; Duke Energy , Charlotte, USA; Southern Company , Atlanta, USA; Arizona Public Service, Phoenix, USA; Tri-State, Denver, USA","IEEE Transactions on Energy Conversion","","2013","28","2","308","316","Model validation of power plant equipment is expected to be mandated imminently by the North American Electric Reliability Corporation. These standards will result in a great need for performing model validation for a large fleet of existing and future power plants. The historic approach to model validation of synchronous generators has typically been a battery of offline tests. In addition, the process of validation depends on a trial and error series of simulations, comparing the results of such simulations with measured test data, to gradually tend toward a validated model and its parameters. Such a method presents significant challenges for application on a large fleet of units. This paper documents a practical approach to model validation. The method is based on collecting online data from the generating unit, typically near or at base-load. Data are collected either from digital-fault recorders capturing system disturbances, or from digital excitation system for an imposed small voltage-reference step. The data are then used in a software tool to perform model validation using an automated optimization algorithm-the entire process may be said to be semiautomated. The benefits are explained in the paper through actual examples of the application.","0885-8969;1558-0059","","10.1109/TEC.2013.2242074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6459572","Model validation;power generation modeling;synchronous generator modeling","Generators;Voltage measurement;Power generation;Data models;Mathematical model;Current measurement;Standards","power system simulation;synchronous generators","semiautomated model validation;power plant equipment;online measurements;North American Electric Reliability Corporation;synchronous generators;online data","","8","19","","","","","","IEEE","IEEE Journals & Magazines"
"Coded aperture compressive imaging array applied for surveillance systems","J. Chen; Y. Wang; H. Wu","School of Optoelectronics, Beijing Institute of Technology, Beijing 100081, China; School of Optoelectronics, Beijing Institute of Technology, Beijing 100081, China; School of Optoelectronics, Beijing Institute of Technology, Beijing 100081, China","Journal of Systems Engineering and Electronics","","2013","24","6","1019","1028","This paper proposes an application of compressive imaging systems to the problem of wide-area video surveillance systems. A parallel coded aperture compressive imaging system and a corresponding motion target detection algorithm in video using compressive image data are developed. Coded masks with random Gaussian, Toeplitz and random binary are utilized to simulate the compressive image respectively. For compressive images, a mixture of the Gaussian distribution is applied to the compressed image field to model the background. A simple threshold test in compressive sampling image is used to declare motion objects. Foreground image retrieval from underdetermined measurement using the total variance optimization algorithm is explored. The signal-to-noise ratio (SNR) is employed to evaluate the image quality recovered from the compressive sampling signals, and receiver operation characteristic (ROC) curves are used to quantify the performance of the motion detection algorithm. Experimental results demonstrate that the low dimensional compressed imaging representation is sufficient to determine spatial motion targets. Compared with the random Gaussian and Toeplitz mask, motion detection algorithms using the random binary phase mask can yield better detection results. However using the random Gaussian and Toeplitz phase mask can achieve high resolution reconstructed images.","1004-4132","","10.1109/JSEE.2013.00119","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6756024","compressive imaging;coded aperture;compressive sensing;motion detection","Image coding;Surveillance;Apertures;Motion detection;Optical sensors;Cameras","","","","","","","","","","","BIAI","BIAI Journals & Magazines"
"Ray tracing within a data parallel framework","M. Larsen; J. S. Meredith; P. A. Navrátil; H. Childs","University of Oregon, USA; Oak Ridge National Laboratory, USA; Texas Advanced Computing Center, The University of Texas at Austin, USA; University of Oregon, USA","2015 IEEE Pacific Visualization Symposium (PacificVis)","","2015","","","279","286","Current architectural trends on supercomputers have dramatic increases in the number of cores and available computational power per die, but this power is increasingly difficult for programmers to harness effectively. High-level language constructs can simplify programming many-core devices, but this ease comes with a potential loss of processing power, particularly for cross-platform constructs. Recently, scientific visualization packages have embraced language constructs centering around data parallelism, with familiar operators such as map, reduce, gather, and scatter. Complete adoption of data parallelism will require that central visualization algorithms be revisited, and expressed in this new paradigm while preserving both functionality and performance. This investment has a large potential payoff: portable performance in software bases that can span over the many architectures that scientific visualization applications run on. With this work, we present a method for ray tracing consisting of entirely of data parallel primitives. Given the extreme computational power on nodes now prevalent on supercomputers, we believe that ray tracing can supplant rasterization as the work-horse graphics solution for scientific visualization. Our ray tracing method is relatively efficient, and we describe its performance with a series of tests, and also compare to leading-edge ray tracers that are optimized for specific platforms. We find that our data parallel approach leads to results that are acceptable for many scientific visualization use cases, with the key benefit of providing a single code base that can run on many architectures.","2165-8765;2165-8773","978-1-4673-6879","10.1109/PACIFICVIS.2015.7156388","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7156388","","Ray tracing;Computer architecture;Parallel processing;Graphics processing units;Data visualization;DSL","data visualisation;parallel machines;ray tracing","data parallel framework;supercomputer;computational power per die;high-level language;many-core device;cross-platform construct;scientific visualization package;language construct;data parallelism;central visualization algorithm;investment;scientific visualization application;data parallel primitive;rasterization;work-horse graphics solution;ray tracing method","","7","40","","","","","","IEEE","IEEE Conferences"
"A resolution-adaptive interpolation filter for video codec","H. Lv; R. Wang; Y. Li; C. Zhu; H. Jia; X. Xie; W. Gao","School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, China; School of Electronic and Computer Engineering, Peking University Shenzhen Graduate School, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China; National Engineering Laboratory of Video Technology, Peking University, Beijing, China","2014 IEEE International Symposium on Circuits and Systems (ISCAS)","","2014","","","542","545","The fraction-pel interpolation filter varies in the video coding standards such as H.264/AVC, AVS and HEVC. Since fractional-pel motion compensation plays an important role in the video encoder, the interpolation of fractional-pel pixels can be refined and designed better to enhance the coding efficiency. In this paper, we firstly propose the generation algorithm of interpolation filter coefficients, and four different tap filters, namely 4tap, 6 tap, 8 tap and 10tap, are tested. A resolution-adaptive interpolation filter for different resolution videos is then introduced based on this algorithm to achieve the maximum bitrate saving. In the proposed scheme, 4 tap filter is applied for the UHD (2560×1600 and above) videos, 6 tap filter and 10 tap filter are performed in the videos whose resolution ranging from 720P (1280×720) to 1080P (1920×1080) and the videos with the resolution below 720P, respectively. When 4 tap filter and 6 tap filter are used in high-definition video, the coding efficiency can increase and the computational complexity will reduce greatly, which is actually beneficial to make hardware optimization more effectively especially SIMD (Single Instruction Multiple Data) and VLSI design. Experiments show that the average BD-rate gains on luma Y, chroma U and V are 1.4%, 0.7% and 0.7% for LP-Main configuration, when conducted in HEVC reference software HM11.0. The coding efficiency gains are significant for some video sequences and can reach up to 6.1%.","0271-4302;2158-1525","978-1-4799-3432-4978-1-4799-3431","10.1109/ISCAS.2014.6865192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6865192","Resolution-adaptive;HEVC;interpolation","Interpolation;Filtering theory;Finite impulse response filters;Complexity theory;Encoding;Video coding;Filtering algorithms","computational complexity;data compression;filtering theory;image resolution;integrated circuit design;interpolation;motion compensation;video codecs;video coding;VLSI","resolution-adaptive interpolation filter;video codec;fraction-pel interpolation filter;video coding standards;H.264-AVC standard;AVS standard;HEVC standard;fractional-pel motion compensation;video encoder;fractional-pel pixel interpolation;tap filters;4tap filter;6 tap filter;8 tap filter;10tap filter;maximum bitrate saving;coding efficiency gains;computational complexity reduction;VLSI design;SIMD;single instruction multiple data;video sequences;video compression","","1","12","","","","","","IEEE","IEEE Conferences"
"A unified, clinician-focused ""packets-to-patients"" measurement approach for meaningful and usable electronic health records","R. Baldwin; M. Danis","Riverside Research, 2640 Hibiscus Way, Beavercreek, Ohio 45431, USA; Morgan Borszcz Consulting, LLC, 2290 Lakeview Drive, Beavercreek, Ohio 45431, USA","2015 17th International Conference on E-health Networking, Application & Services (HealthCom)","","2015","","","44","49","In most medical facilities, the systems, software, and data that support clinical and business operations have been developed independently and in isolation. This makes it difficult for the systems to efficiently and effectively access the data and metrics needed to improve clinical processes, achieve desired patient outcomes, or make informed business decisions. Furthermore, determining the benefit of current or proposed clinical processes or business practices from these systems is often inefficient and ad hoc, even though the data required is available within these systems. In response to this, we have developed a unified “Packets-to-Patients” approach to measuring key elements of the clinical, business, and technical operations of the medical enterprise. From these elements we derive benefits that are validated as meaningful within those domains. Our approach automatically and efficiently collects data that establishes whether the stages of Electronic Health Record (EHR) Meaningful Use criteria are being met. It also measures EHR adoption rates, clinical benefits like quality of care and patient safety, and business benefits like decreased length of stay and improved workflow efficiency. Our analytical approach has been field-tested and proven through nearly three years of development. This methodology has measured baseline system performance at several medical treatment facilities and also provided those organizations with an in-depth understanding of how health Information Technology (health IT) performance impacts clinical and business operations. More importantly, our approach has enabled rigorous analysis and fact-based decision-making on how best to integrate health IT into clinical operations. Most importantly, this approach allows health care delivery organizations to design a business case based on hard data and increases the likelihood of success in adopting and optimizing health IT.","","978-1-4673-8325","10.1109/HealthCom.2015.7454471","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7454471","EHR;Value-Based Medicine;Meaningful Use;process improvement","Business;Medical services;Usability;Current measurement;Safety","decision making;electronic health records;health care","health care;fact-based decision-making;health Information Technology;EHR adoption rates;electronic health records;clinician-focused packets-to-patients measurement approach","","","15","","","","","","IEEE","IEEE Conferences"
"Multiport antenna performance analysis from ray-traced channels for small cells","M. Dehghani Estarki; J. X. Yun; Y. L. C. de Jong; R. G. Vaughan","Sierra Wireless Mobile Communications Lab., Simon Fraser University Burnaby, BC, Canada, V5A 1S6; Sierra Wireless Mobile Communications Lab., Simon Fraser University Burnaby, BC, Canada, V5A 1S6; Communications Research Centre Canada Ottawa, ON, Canada, K2H 8S2; Sierra Wireless Mobile Communications Lab., Simon Fraser University Burnaby, BC, Canada, V5A 1S6","The 8th European Conference on Antennas and Propagation (EuCAP 2014)","","2014","","","3305","3309","The evaluation of multi-element antenna (MEA) performance in urban environments is particularly important for the planning and design of reliable, spectrally efficient systems. To date, MIMO communication using large dimensions has not been implemented for commercial systems. But the potential capacity of MIMO is much more significant when the MIMO dimensions become large. The hold-up is not just the shortfall in understanding how to deploy large-dimension MIMO, but also how to design the antenna systems. There is no widely agreed method for evaluating MEAs used by MIMO communications. Even the performance evaluation of a well-established MIMO configuration, such as the small scale systems already used, remains limited. The evaluation often defers to OTA tests in artificial environmental conditions. The real-world performance remains unclear, so care must be taken in design optimization. In this paper, we discuss the incorporation of some physical modeling of the propagation environment to evaluating the MEA performance for a typical urban wireless channel. A ray-optic method is applicable to any 3D environment, and any number of antennas with arbitrary patterns. In this sense it offers a practical approach for large-dimension MIMO evaluation by being able to change the antennas and the environment, all in software. The evaluation method is illustrated by using an MEA slot cube (12 antennas) operating in a specific environment, namely downtown Ottawa.","2164-3342","978-8-8907-0184","10.1109/EuCAP.2014.6902535","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6902535","physical modeling;ray-tracing;multiport antenna;MIMO;diversity gain;diversity combining","Diversity reception;MIMO;Antenna measurements;Signal to noise ratio;Receiving antennas","antenna arrays;MIMO communication;wireless channels","multiport antenna performance analysis;ray-traced channels;MIMO communication;antenna system design;artificial environmental conditions;ray-optic method;3D environment;urban wireless channel;MEA slot cube","","1","18","","","","","","IEEE","IEEE Conferences"
"RadioBOT: A spatial cognitive radio testbed","B. M. Beck; J. H. Kim; R. J. Baxley; B. T. Walkenhorst","Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30318, USA; Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30318, USA; Electrical and Computer Engineering, Georgia Tech Research Institute, Atlanta, 30318, USA; Electrical and Computer Engineering, Georgia Tech Research Institute, Atlanta, 30318, USA","2013 IEEE Aerospace Conference","","2013","","","1","9","This paper introduces RadioBOT, a flexible system of mobile robots for acquisition of radio frequency data. The motivation for such a test system is described, namely the difficulty in acquiring real world data for the purpose of spatial cognitive radio (CR) research. Some current areas of CR research are presented for which RadioBOT can gather data. We then describe the hardware and software components of our system. As a demonstration of the system's capability, we present here the results of a spectrum mapping experiment. In this experiment, we uniformly sample average signal power in a laboratory hallway where an emitter is present. From this data, we form an interpolated spectrum map of the signal power as a function of space. Knowledge of the area's spectrum map is then used to optimize the relay channel communication rate between a transmitter and receiver, by optimally positioning the relay node.","1095-323X;1095-323X","978-1-4673-1813-6978-1-4673-1812-9978-1-4673-1811","10.1109/AERO.2013.6497362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497362","","Robots;Hardware;Relays;Servers;Sensors;Radio frequency;Receivers","","","","3","14","","","","","","IEEE","IEEE Conferences"
"Modeling and simulation of an HVAC system for energy analysis and management of commercial buildings","A. Sreedevi; A. Kaul; K. Radhika","Department of Electrical and Electronics, R. V. College of Engineering Bengaluru, Karnataka, India; Department of Electrical and Electronics, R. V. College of Engineering Bengaluru, Karnataka, India; Department of Electrical and Electronics, R. V. College of Engineering Bengaluru, Karnataka, India","International Conference on Circuits, Communication, Control and Computing","","2014","","","186","191","Commercial buildings consume a considerable amount of the world's energy and this can be substantially reduced through strategic monitoring and optimization of their energy consumption. Hence, monitoring and managing the available electrical power is a very important part of conserving energy resources, reducing greenhouse gas emissions and cutting down energy expenses. An attempt has been made to model and simulate the HVAC (Heating, Ventilation and Air-Conditioning) system for a typical commercial building, which consumes the most electrical power in such buildings. The various components of an HVAC system were mathematically modeled and were integrated to simulate real-time energy and process parameters. A software application was developed using LabVIEW, which enables the user to configure the system parameters through a Human Machine Interface (HMI). The HMI would enable building design engineers to design and set-up an effective Energy Management System, and test such designs with negligible cost and minimal complexity compared to that involved in using physical loads.","","978-1-4799-6546-5978-1-4799-6545","10.1109/CIMCA.2014.7057787","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7057787","Air conditioning;Air Handling Unit;Commercial buildings;Energy Analysis;Energy Management;Human Machine Interface (HMI);Modelling and Simulation;State Machine model in LabVIEW","Buildings;Atmospheric modeling;Boilers;Monitoring;Load modeling;Mathematical model;Energy consumption","building management systems;energy management systems;HVAC;man-machine systems","HVAC system;energy analysis;energy management;commercial buildings;strategic monitoring;energy consumption;heating ventilation and air-conditioning system;human machine interface","","","9","","","","","","IEEE","IEEE Conferences"
"3D simulation of industrial large-scale ceramics furnace in SIMIO platform environment","G. Tsaousoglou; S. Manesis","Department of Electrical and Computer Engineering, University of Patras, Eratosthenous Str., Rio, Greece; Department of Electrical and Computer Engineering, University of Patras, Eratosthenous Str., Rio, Greece","2014 11th International Conference on Informatics in Control, Automation and Robotics (ICINCO)","","2014","02","","716","723","This article presents a 3D simulation of an industrial large-scale furnace operating in the NGC (Northern Greece Ceramics) ceramics industrial plant. The 3D modelling and simulation of the long industrial furnace is based on the SIMIO software platform the capabilities of which are explored and tested in such a complex production plant. After the understanding of the real system operational characteristics, its macroscopic behaviour has been extracted and modelled; then a 3D model was created. Using the model, matters concerning the production process optimization are explored, while also alternative production scenarios are simulated, so that conclusions for the system's behaviour at value variation of the functional parameters of the real system can be extracted, without experimenting on the real process. In the four main sections of this article, the following aspects are presented: the modeling approach of the furnace operation, the SIMIO model of the furnace, its operation, examples of possible model applications and utility of the developed model and further modeling of the whole production procedure preceding the furnace.","","978-9-8975-8062-8978-9-8975-8061","10.5220/0005046207160723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7049678","Industrial Production;3D Modeling;Simulation;Production Management","Furnaces;Temperature;Production;Object oriented modeling;Ovens;Solid modeling;Mathematical model","","","","","10","","","","","","IEEE","IEEE Conferences"
"Strategies of Numerical Modeling in Component Development for Fusion Devices","O. Neubauer; A. Panin; G. Czymek; B. Giesen","Institute of Energy and Climate, Research—Plasma Physics, Forschungszentrum Jülich GmbH, Jülich, Germany; Institute of Energy and Climate, Research—Plasma Physics, Forschungszentrum Jülich GmbH, Jülich, Germany; Institute of Energy and Climate, Research—Plasma Physics, Forschungszentrum Jülich GmbH, Jülich, Germany; IBG, Heinsberg, Germany","IEEE Transactions on Plasma Science","","2014","42","3","682","689","The development of components for fusion devices includes numerous simulations to validate their design compatibility with specified loads according to applicable codes and standards. The strategy of multifield finite element analyses is determined by the iterative nature of the design process. The level of structural complexity dictates the analytical approaches and modeling techniques. Even though nowadays powerful software exploiting the performance of modern computers allows structures to be simulated in a very detailed way, direct meshing of CAD models with all their complexity is often not beneficial for nonlinear and multifield analyses. Reasonable modeling simplifications, submodeling, and integration with analytical modeling and testing can optimize the analysis process. This paper gives examples of simplified modeling for engineering purposes with a clear understanding of the analysis goal and the role of simulations in the design process, such as for the W7-X stellarator busbar system. Engineering methods to perform multifield analyses including electromagnetic transients for ITER diagnostic port plug components are also presented. An engineering material model is discussed for calculating the superconducting coil steel structures operating in the inelastic range.","0093-3813;1939-9375","","10.1109/TPS.2013.2282179","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6615971","Electromagnetic (EM) analysis;ITER diagnostics;multifield analysis;nuclear fusion;W7-X stellarator","Solid modeling;Atmospheric modeling;Load modeling;Numerical models;Analytical models;Iron;Coils","fusion reactor design;plasma toroidal confinement;stellarators;Tokamak devices","numerical modeling strategies;fusion devices;design compatibility;multifield finite element analysis;CAD models;design process;W7-X stellarator busbar system;ITER diagnostic port plug components;superconducting coil steel structures","","3","23","","","","","","IEEE","IEEE Journals & Magazines"
"Personalized kinematics for human-robot collaborative manipulation","A. M. Bestick; S. A. Burden; G. Willits; N. Naikal; S. S. Sastry; R. Bajcsy","Department of Electrical Engineering and Computer Sciences, Berkeley, USA; Department of Electrical Engineering and Computer Sciences, Berkeley, USA; Department of Bioengineering, University of California, Berkeley, USA; Department of Electrical Engineering and Computer Sciences, Berkeley, USA; Department of Electrical Engineering and Computer Sciences, Berkeley, USA; Department of Electrical Engineering and Computer Sciences, Berkeley, USA","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","","2015","","","1037","1044","We present a framework for parameter and state estimation of personalized human kinematic models from motion capture data. These models can be used to optimize a variety of human-robot collaboration scenarios for the comfort or ergonomics of an individual human collaborator. Our approach offers two main advantages over prior approaches from the literature and commercial software: the kinematic models are estimated for a specific individual without a priori assumptions on limb dimensions or range of motion, and our kinematic formalism explicitly encodes the natural kinematic constraints of the human body. The personalized models are tested in a human-robot collaborative manipulation experiment. We find that human subjects with a restricted range of motion rotate their torso significantly less during bimanual object handoffs if the robot uses a personalized kinematic model to plan the handoff configuration, as compared to previous approaches using generic human kinematic models.","","978-1-4799-9994-1978-1-4799-9993","10.1109/IROS.2015.7353498","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353498","","Kinematics;Robots;Collaboration;Skeleton;Injuries;Training;Ergonomics","human-robot interaction;manipulator kinematics;parameter estimation;state estimation","generic human kinematic models;motion capture;personalized human kinematic model;state estimation;parameter estimation;human-robot collaborative manipulation;personalized kinematics","","10","34","","","","","","IEEE","IEEE Conferences"
"Improving MEG Performance With Additional Tangential Sensors","J. Nurminen; S. Taulu; J. Nenonen; L. Helle; J. Simola; A. Ahonen","BioMag Laboratory, HUS Medical Imaging Center, Hospital District of Helsinki and Uusimaa, Helsinki, Finland; Elekta Oy, Helsinki, Finland; Elekta Oy, Helsinki, Finland; Elekta Oy, Helsinki, Finland; Elekta Oy, Helsinki, Finland; Elekta Oy, Helsinki, Finland","IEEE Transactions on Biomedical Engineering","","2013","60","9","2559","2566","Recently, the signal space separation (SSS) method, based on the multipole expansion of the magnetic field, has become increasingly important in magnetoencephalography (MEG). Theoretical arguments and simulations suggest that increasing the asymmetry of the MEG sensor array from the traditional, rather symmetric geometry can significantly improve the performance of the method. To test this concept, we first simulated addition of tangentially oriented standard sensor elements to the existing 306-channel Elekta Neuromag sensor array, and evaluated and optimized the performance of the new sensor configuration. Based on the simulation results, we then constructed a prototype device with 18 additional tangential triple-sensor elements and a total of 360 channels. The experimental results from the prototype are largely in agreement with the simulations. In application of the spatial SSS method, the 360-channel device shows an approximately 100% increase in software shielding capability, while residual reconstruction noise of evoked responses is decreased by 20%. Further, the new device eliminates the need for regularization while applying the SSS method. In conclusion, we have demonstrated in practice the benefit of reducing the symmetry of the MEG array, without the need for a complete redesign.","0018-9294;1558-2531","","10.1109/TBME.2013.2260541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6509958","Biomagnetics;biomedical signal processing;magnetoencephalography","Magnetic sensors;Sensor arrays;Noise;Calibration;Magnetic noise","magnetoencephalography;medical signal processing;neurophysiology;noise;signal reconstruction","signal space separation method;SSS method;magnetoencephalography;MEG sensor array;Elekta Neuromag sensor array;tangential triple-sensor elements;residual reconstruction noise","Adult;Algorithms;Computer Simulation;Head;Humans;Magnetoencephalography;Magnetoencephalography;Models, Theoretical;Reproducibility of Results;Signal Processing, Computer-Assisted","5","22","","","","","","IEEE","IEEE Journals & Magazines"
"Self Contained Adaptable Optical Wireless Communications System for Stroke Rate During Swimming","R. M. Hagem; S. G. O'Keefe; T. Fickenscher; D. V. Thiel","Centre for Wireless Monitoring and Applications, School of Engineering, Griffith University, Nathan, Australia; Centre for Wireless Monitoring and Applications, School of Engineering, Griffith University, Nathan, Australia; Department of Electrical Engineering, Helmut Schmidt University, University of the Federal Armed Force Hamburg, Hamburg, Germany; Centre for Wireless Monitoring and Applications, School of Engineering, Griffith University, Nathan, Australia","IEEE Sensors Journal","","2013","13","8","3144","3151","The velocity of a swimmer can be determined from the stroke rate and the stroke length or by integrating the forward acceleration. In competitive swimming, these parameters are very important for race planning. This paper presents a wrist mounted accelerometer and optical wireless communications to display goggles to give real time feedback to a swimmer during swimming. The system data rate is 2.4 kbps ON-OFF keying modulation for the optical wireless signal. The system uses visible light communication in the green-blue wavelength. Design challenges include interference from bubbles and strong background light. The final device is low cost with low power consumption and small size. Intra-stroke transmit times are scheduled using the acceleration sensor data. Experiments are conducted in air and under water for this system to optimize the link availability. Algorithms for finding the absolute maximum of the y-axis acceleration for each stroke cycle and the goggles display decision are implemented at the transmitter and the receiver, respectively. Hardware, software, and implementation modifications to improve the system are successfully tested.","1530-437X;1558-1748;2379-9153","","10.1109/JSEN.2013.2262933","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6515615","Underwater optical wireless communication;real time swimmers feedback;visible light communication;stroke rate;accelerometer","","accelerometers;amplitude shift keying;biomechanics;eye protection;interference (signal);optical modulation;optical receivers;optical transmitters","self contained adaptable optical wireless communication system;stroke rate;swimming;race planning;wrist mounted accelerometer;display goggles;ON-OFF keying modulation;optical wireless signal;visible light communication;green-blue wavelength;interference;bubbles;power consumption;intrastroke transmit time;acceleration sensor data;transmitter;receiver;background light;bit rate 2.4 kbit/s","","19","28","","","","","","IEEE","IEEE Journals & Magazines"
"A energy efficient WSN system for limited power source environments","R. S. Semente; A. Silva; A. O. Salazar; F. D. M. Oliveira; A. S. Lock","DCA/DEE, UFRN, Natal, Brasil; DCA/DEE, UFRN, Natal, Brasil; DCA/DEE, UFRN, Natal, Brasil; DCC-Natal, UERN, Natal, Brasil; CEAR/DEER, UFPB Paraíba, Brasil","2013 Seventh International Conference on Sensing Technology (ICST)","","2013","","","193","197","In this work, a Wireless Sensor Networks (WSN) is analyzed and implemented to control a instrumented process in environments with limited power source. Present propose is based on IEEE 802.15.4 standard, using Zigbee and Modbus protocols. A simplified and robust system architecture is presented, emphasizing the control subsystem of charge and discharge, using solar panels, as well as software optimized for task of network controlling and sensing, both characteristics which that a reduced consumption of energy. These characteristics were proved by energetic efficiency tests and a system study that will be automated to define a rate optimal operation.","2156-8073;2156-8065","978-1-4673-5222-2978-1-4673-5220","10.1109/ICSensT.2013.6727641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6727641","Wireless Sensor Network;IEEE 802.15.4;Energy Efficiency;Power Management","Batteries;Sensors;Wireless sensor networks;Actuators;IEEE 802.15 Standards;Energy efficiency;Microcontrollers","protocols;solar power;wireless sensor networks;Zigbee","energy efficient WSN system;power source environments;wireless sensor networks;instrumented process;IEEE 802.15.4 standard;Modbus protocols;Zigbee protocols;robust system architecture;solar panels;network controlling;optimal operation","","1","21","","","","","","IEEE","IEEE Conferences"
"MARX-TV: An affordable mobile solution to adjust and monitor DVB-T reception antennas","F. P. Silva; P. Malai; R. S. Freitas; J. A. Salvado; J. R. Oliveira","NA; NA; NA; NA; NA","IEEE Latin America Transactions","","2015","13","9","2887","2894","Competitiveness and customer care are key factors that influence success or failure in any business. Therefore, to survive and succeed, the entrepreneurs tend to concentrate efforts on optimize working methods and procedures towards efficiency. However, the possible changes must not neglect personal security by any circumstance. This paper presents and proposes a low-cost prototype solution capable of increasing the technical work efficiency on installing TV reception antennas, which also reduces the risks associated to works at heights (on roofs). The solution provides monitoring and adjustment of TV antennas without the need to carry any heavy equipment, thus leading to a higher mobility for the technicians. The system proposed consists at a microcontroller based hardware and an application for mobile devices. The hardware solution (HMARX-TV) uses a Raspberry Pi model B minicomputer (ARM, 32 bit architecture), an Arduino Uno board (ATmega328, 8 bit) and a USB DVB-T receiver; the software application (AppMARX-TV) is supported in Android 4.x.x OS and runs on common mobile devices (e.g. smartphones or tablet PCs). The AppMARX-TV connects to the HMARX-TV hardware via a wireless link (IEEE 802.11b/g/n) and takes control of its operation fully. It also allows to adjust a programmable band-pass filter thus improving tuning and reception quality. Upon conformity, a detailed report is issued containing the most important parameters associated to the installation. This allows both, the technician and the customer, to become aware on the correctness and conformity of the work, thus strengthening customer relations and technical prestige. To evaluate the overall prototype performance, several relevant reception parameters were measured and compared with the results obtained with laboratory and certified commercial equipment. The functional tests conducted and performed on site, for several scenarios, demonstrate the conformity in operation for the system prototype.","1548-0992","","10.1109/TLA.2015.7350035","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350035","Android App;DVB-T reception;Raspberry Pi hardware;RF tuning;Wi-Fi link","Hardware;Androids;Humanoid robots;TV;Irrigation;OFDM","band-pass filters;digital video broadcasting;microcontrollers;radio links;smart phones;television antennas","competitiveness;customer care;TV reception antennas;microcontroller based hardware;mobile devices application;HMARX-TV;Raspberry Pi model B minicomputer;Arduino Uno board;USB DVB-T receiver;AppMARX-TV;common mobile devices;wireless link;programmable band-pass filter;reception quality;customer relations;word length 8 bit;word length 32 bit","","","","","","","","","IEEE","IEEE Journals & Magazines"
"Embedded realization of a real time Heart Rate Variability logger for at-home sleep studies","B. B. Rekha; A. Kandaswamy; V. M. Mitha","Department of Biomedical Engineering, PSG College of Technology, Coimbatore, India; Department of Biomedical Engineering, PSG College of Technology, Coimbatore, India; Department of Biomedical Engineering, Manipal Institute of Technology, Manipal, India","2015 IEEE Workshop on Computational Intelligence: Theories, Applications and Future Directions (WCI)","","2015","","","1","6","Sleep has no substitute and quality of sleep is a major concern for healthy living of human beings. Sleep breathing disorders are events characterized by pauses of breathing during sleep. Sleep breathing disorders like Obstructive Sleep Apnea (OSA) may result in cardiac disorders and fatalities. Though Polysomnography is considered as the gold standard for conducting sleep studies, current research directs that the trend of Heart Rate Variability (HRV) during sleep is indicative of sleep breathing disorder. Hence, reliable HRV recorders with ease of use may contribute to early screening of these disorders. This paper reports the prototype development of an embedded system for logging HRV during sleep for screening during sleep. The system is built with open source Arduino platform consisting of an ATMEGA328 microcontroller along with a provision for storage on a Secure-Digital card. ‘R’ peak detection is carried out using a combination of dynamic threshold and amplitude threshold. The logger is able to work on two modes: (1) plain, long duration logger and (2) HRV Logger. The estimated duration of logging is 72 hours with a +9 V battery supply. The system performance is compared with a commercially available Electrocardiogram (ECG) recorder system and a MATLAB based R peak detection system with real time recordings of 30 healthy adults. The system code is optimized to achieve a logging time of 6.25 milliseconds per sample and 0.98 seconds for each ‘R’ peak detection and storage. The proposed system was also tested with Sleep ECG samples from Physionet database and it achieved a maximum sensitivity of 97.7% and specificity of 95.56%. The maximum recorded percentage error of detection was 2%. The results indicate that the proposed system and software design can be developed as a compact, economical and portable device for early detection of sleep breathing disorders.","","978-1-4673-8215","10.1109/WCI.2015.7495532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7495532","Heart Rate Variability;Obstructive Sleep Apnea;Arduino platform;Secure Digital card;HRV Logger","Sleep apnea;Heart rate variability;Electrocardiography;Real-time systems;Clocks","","","","","20","","","","","","IEEE","IEEE Conferences"
"Quintic spline smooth semi-supervised support vector classification machine","X. Zhang; J. Ma; A. Li; A. Li","School of Mathematics and Physics, University of Science and Technology Beijing, Beijing 100083, China; School of Mathematics and Physics, University of Science and Technology Beijing, Beijing 100083, China; Department of Mathematical Sciences, Montclair State University, New Jersey 07043, USA; School of Mathematics and Physics, University of Science and Technology Beijing, Beijing 100083, China","Journal of Systems Engineering and Electronics","","2015","26","3","626","632","A semi-supervised vector machine is a relatively new learning method using both labeled and unlabeled data in classification. Since the objective function of the model for an unstrained semi-supervised vector machine is not smooth, many fast optimization algorithms cannot be applied to solve the model. In order to overcome the difficulty of dealing with non-smooth objective functions, new methods that can solve the semi-supervised vector machine with desired classification accuracy are in great demand. A quintic spline function with three-times differentiability at the origin is constructed by a general three-moment method, which can be used to approximate the symmetric hinge loss function. The approximate accuracy of the quintic spline function is estimated. Moreover, a quintic spline smooth semi-support vector machine is obtained and the convergence accuracy of the smooth model to the non-smooth one is analyzed. Three experiments are performed to test the efficiency of the model. The experimental results show that the new model outperforms other smooth models, in terms of classification performance. Furthermore, the new model is not sensitive to the increasing number of the labeled samples, which means that the new model is more efficient.","1004-4132","","10.1109/JSEE.2015.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7170021","semi-supervised;support vector classification machine;smooth;quintic spline function;convergence","Splines (mathematics);Support vector machines;Fasteners;Modeling;Accuracy;Mathematical model;Convergence","","","","1","","","","","","","BIAI","BIAI Journals & Magazines"
"Verification and performance analysis of embedded and cyber-physical systems using UPPAAL","K. G. Larsen","Aalborg University, Denmark","2014 2nd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)","","2014","","","IS-11","IS-11","Timed automata, priced timed automata and energy automata have emerged as useful formalisms for modeling a real-time and energy-aware systems as found in several embedded and cyber-physical systems. Whereas the real-time model checker UPPAAL allows for efficient verification of hard timing constraints of timed automata, model checking of priced timed automata and energy automata are in general undecidable — notable exception being cost-optimal reachability for priced timed automata as supported by the branch UPPAAL Cora. These obstacles are overcome by UPPAAL-SMC, the new highly scalable engine of UPPAAL, which supports (distributed) statistical model checking of stochastic hybrid systems with respect to weighted metric temporal logic. The talk will review UPPAAL-SMC and its applications to the domains of energy-harvesting wireless sensor networks, schedulability and execution time analysis for mixed criticality systems, battery-aware scheduling with respect to correctness, fault- and performance analysis. In the talk I will also indicate how UPPAAL SMC may play be of benefit for counter example generation, refinement checking, testing, controller synthesis and optimization.","","978-9-8975-8066-6978-9-8975-8065","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7018436","","","","","","","","","","","","","IEEE","IEEE Conferences"
"[Front-cover]","","","2013 IEEE Sixth International Conference on Software Testing, Verification and Validation","","2013","","","C4","C4","The following topics are dealt with: software testing; software verification; software validation; industrial context; mutation testing; software safety; debugging; fault localization; concurrency; model-based testing; coverage-based testing; combinatorial testing; defect prediction; program repair; test-case selection; test-case prioritization; software maintenance; test case generation; and security.","2159-4848","978-0-7695-4968-2978-1-4673-5961","10.1109/ICST.2013.85","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6569699","","","concurrency control;fault diagnosis;program debugging;program testing;program verification;security of data;software maintenance","software testing;software verification;software validation;industrial context;mutation testing;software safety;debugging;fault localization;concurrency;model-based testing;coverage-based testing;combinatorial testing;defect prediction;program repair;test-case selection;test-case prioritization;software maintenance;test case generation;security","","","","","","","","","IEEE","IEEE Conferences"
"Release and testing stop time of a software: A new insight","P. K. Kapur; A. K. Shrivastava","Center for Interdisciplinary Research, Amity University, Noida, U.P., India; Department of Operational Research, University of Delhi, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","7","Testing is a vital phase in the software development life cycle. But, the way it is performed, varies from one organization to another. One of the prime concern in software industry is to determine the optimal duration of testing. Both researchers as well as software developers have been working towards solving this issue since long. The duration of testing is directly proportional to its reliability level but prolonged testing costs a lot in terms of higher testing and market opportunity cost. Therefore determination of optimal testing time has become an important optimization problem in the field of software development. As a common industrial practice, software release also marks the end of testing phase of a software. But, this often accompanies issues like delayed release in case the testing is continued to ensure a high reliability level or a low reliability level in case the software is released early. To counter these problems, now a days testing is divided into two phases i.e. pre-release and post release testing phase. During post release testing phase organization aims at treating remaining software faults and subsequently enhance product experience for customers. In this paper we present a generalized approach of optimal scheduling policy to determine the optimal release and testing stop time of a software while minimizing overall testing cost. In our proposed work, software testing &amp; operational phases are governed by different distribution functions in distinct phases, i.e. in prerelease, post release phase (before and after testing stop time) in our proposed cost model. Numerical example is given to support our findings with the help of a real life software failure data set of Tandem Computers.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359202","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359202","Software Reliability;Release;Testing Stop Time","","program debugging;program testing;scheduling;software houses;software reliability","Tandem Computers;software testing & operational phases;optimal scheduling policy;customer product experience;software fault treatment;reliability level;software release;optimal testing time determination;software industry;software development life cycle;software stop time testing","","3","30","","","","","","IEEE","IEEE Conferences"
"An Insight of software quality models applied in predicting software quality attributes: A comparative analysis","K. Sheoran; O. P. Sangwan","Maharaja Surajmal Institute of Technology, Kavita, India; Guru Jambheshwar University of Science and Technology, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","5","Software is crucial in giving a competitive edge to most of the organizations. Software has become main part of business products, systems and services. Software products quality was seen as a significant element in success of business. The purpose of the research is to the existing models related to software quality which is used for predicting the quality attributes in the software. SQM models selected for this study to compare are ISO 9126, Bertoa, ISO 25010, CBQM and Alvaro model. Main characteristics selected are portability, maintainability, flexibility, usability, reliability and efficiency and sub characteristics are accuracy, testability, extendibility, compatibility, understandability and performance. This study adopts secondary source of data. It was noticed that for the selected set of quality attributes characteristics, Alvaro model excel well in main characteristics as well as other sub characteristics namely understandability, accuracy and testability.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359355","Software quality models (SQM);software quality;ISO 9126 model;Alvaro model;Bertoa model;ISO 25010 model;CBQM model","","program testing;software maintenance;software performance evaluation;software portability;software quality;software reliability","software quality models;software quality attribute prediction;business products;business systems;business services;software product quality;SQM models;ISO 9126 model;Bertoa model;ISO 25010 model;CBQM model;Alvaro model;software portability;software maintainability;software flexibility;software usability;software reliability;software efficiency;software accuracy;software testability;software extendibility;software compatibility;software understandability;software performance","","1","33","","","","","","IEEE","IEEE Conferences"
"Predicting change using software metrics: A review","R. Malhotra; A. Bansal","Department of Software Engineering, Delhi Technological University (formerly known as Delhi College of Engineering), India; Department of Software Engineering, Delhi Technological University (formerly known as Delhi College of Engineering), India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","Software change prediction deals with identifying the classes that are prone to changes during the early phases of software development life cycle. Prediction of change prone classes leads to higher quality, maintainable software with low cost. This study reports a systematic review of change prediction studies published in journals and conference proceedings. This review will help researchers and practitioners to examine the previous studies from different viewpoints: metrics, data analysis techniques, datasets, and experimental results perspectives. Besides this, the research questions formulated in the review allow us to identify gaps in the current technology. The key findings of the review are: (i) less use of method level metrics, machine learning methods and commercial datasets; (ii) inappropriate use of performance measures and statistical tests; (iii) lack of use of feature reduction techniques; (iv) lack of risk indicators used for identifying change prone classes and (v) inappropriate use of validation methods.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359253","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359253","empirical validation;change prediction;machine learning;software maintenance;software metrics","","program testing;program verification;software maintenance;software metrics;software quality;statistical testing","software metrics;software change prediction;software development life cycle;change prone class prediction;software quality;software maintainability;data analysis techniques;method level metrics;machine learning methods;commercial datasets;performance measures;statistical tests;feature reduction techniques;risk indicators;validation methods","","1","32","","","","","","IEEE","IEEE Conferences"
"Modelling multi up gradations and related release time problems","P. K. Kapur","Centre for Interdisciplinary Research, Amity University, Sector-125, Noida, Uttar Pradesh, India","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","1","Summary form only given. Testing forms an important aspect of software development life cycle and plays an imperative role in defining the quality of a software. But, due to limited resources and time constraint, firms have to decide upon their upper limits while investing. Moreover, due to fierce competition in the market, software nowadays are short-lived. Hence, instead of delivering the complete product in a single development cycle, firms plan periodic releases of software and come up with multiple successive up gradations to survive. But at the same time newly added features increase the software complexity which further magnifies faults in the software. To follow the effect of faults generated in the software due to functional enrichment, a multi up gradation software reliability model has been discussed in this paper. This model identifies left over faults from the operational phase of previous version of the software, which are debugged during the testing phase of the next version. Multi up gradation elucidate the real world approach for software development. But, it accompanies other concerns as well. One such matter of contention is the optimal release time problem. Many models have been designed in the past which talks about when to stop testing and release the software to the users. In this paper we have discussed a unified approach to address this crucial argument of when to stop testing multi up-gradation of software under the assumption that the software is supported till the warranty period is over. While discussing the cost model for each up-gradation it was considered that some of the remaining faults of previous release are reported and removed partly during the testing period and partly during the warranty period of new up-gradation. Further, we discussed about estimating optimal software release time using Multi Attribute Utility Theory. More precisely, three considerable attributes, viz. Reliability, Cost and Detection rate indicator are employed in order to determine the optimal release time. Finally, we have examined the multi attribute utility theory approach to find optimal release time in case of multi-release of a software with constraints on reliability &amp; cost. The parameters of the model are estimated using Statistical Package for Social Sciences (SPSS) based on real life data set &amp; optimum stopping time for each version of software is obtained using Maple software.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014655","Software Reliability Growth Models (SRGMs);Multi Up gradation;Non Homogeneous Poisson Process (NHPP);Release Time Problem;Multi Attribute Utility Theory (MAUT)","","program testing;software development management;software quality;software reliability","multiup gradation;release time problem;software testing;software development life cycle;software quality;software reliability;warranty period;multiattribute utility theory;cost rate indicator;detection rate indicator;statistical package for social sciences;SPSS;Maple software","","","","","","","","","IEEE","IEEE Conferences"
"Table of contents","","","2014 IEEE 23rd Asian Test Symposium","","2014","","","v","xi","The following topics are dealt with: 3D testing; circuit reliability; resilient circuit design and test; testing of emerging technologies; SoC testing; design, verification, and application of IEEE 1687; post-silicon validation; testability and test generation; yield optimization of memory; on-line parameter testing; power/temperature-aware testing; trojan/fault detection with high resolution; analog/memory testing; big data for test; in-field techniques for performance adaption, test, and power-noise diagnosis; timing variation detection; delay testing; high quality system level test and diagnosis; test compression; and hardware security.","1081-7735;2377-5386","978-1-4799-6030","10.1109/ATS.2014.4","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6979059","","","data compression;design for testability;fault simulation;integrated circuit design;integrated circuit testing;integrated memory circuits;invasive software;logic testing;system-on-chip;three-dimensional integrated circuits","hardware security;test compression;high quality system level test;delay testing;timing variation detection;power-noise diagnosis;in-field techniques;memory testing;analog testing;fault detection;trojan detection;temperature-aware testing;power-aware testing;online parameter testing;yield optimization;test generation;post-silicon validation;IEEE 1687;SoC testing;emerging technologies;resilient circuit design;circuit reliability;3D testing","","","","","","","","","IEEE","IEEE Conferences"
"Role of parameter estimation &amp; prediction during development of Software using SRGM","A. Varshney; R. Majumdar; C. Choudhary; A. Srivastava","Amity University, Uttar Pradesh, Noida, India; Amity University, Uttar Pradesh, Noida, India; Amity University, Uttar Pradesh, Noida, India; Amity University, Uttar Pradesh, Noida, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","Predicting Reliability of Software using SRGMs are a big challenge for Software Engineers. Software Reliability is a way to find the probability of software to operate in a given time limit and in a specified environment without causing any failure; hence it is reviewed as Quantifiable Metric. System faults can be the result of software or hardware errors. Software fault is comparatively more difficult to predict than hardware faults. Even the Reliability of a web application is also hard to determine due to its highly distributed nature. Predicting the reliability of application helps the Engineers to compute the software's release date/time and to manage various resources of software such as people, money, time etc. In this paper SRGM Models are considered namely Goel Okumoto Model (GO Model), Yamada S-shaped Model and Kapur &amp; Garg Model (KG Model) to estimate and predict Software Reliability by detecting the cumulative number of faults in software application within specified time. Additional number of days are also calculated to remove them and release the software on time. By implementing the test cases of actual defects per day, Rate of Change or Test Case Efficiency can be measured.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359222","Software Reliability Growth Model;NHPP;GO Model;Y Model;KG Model;Software Reliability","","parameter estimation;software fault tolerance;software metrics;software reliability","parameter estimation;parameter prediction;software development;software reliability prediction;software engineers;quantifiable metric;software fault;Web application reliability;SRGM models;Goel Okumoto model;GO model;Yamada S-shaped model;Kapur & Garg model;KG model;test case efficiency","","1","14","","","","","","IEEE","IEEE Conferences"
"[Title page]","","","2015 International Conference on Futuristic Trends on Computational Analysis and Knowledge Management (ABLAZE)","","2015","","","1","1","The following topics are dealt with: vibration signal based monitoring; mechanical microdrilling; rule based inflectional urdu stemmer usal; rule based derivational urdu stemmer usal; fuzzy logic controller; heat exchanger temperature process; text dependent speaker recognition; MFCC; SBC; multikeyword based sorted querying; encrypted cloud data; communication understandability enhancement; GSD; parsing; input power quality; switched reluctance motor drive; externally powered upper limb prostheses; program test data generation; launch vehicle optimal trajectory generation; misalignment fault detection; induction motors; current signature analysis; vibration signature analysis; wind power plants; vortex induced vibration; mechanical structure modal analysis; machining parameter optimization; diesel engines; high speed nonvolatile NEMS memory devices; image fusion; RGB color space; LUV color space; offline English character recognition; human skin detection; tumor boundary extraction; MR images; OdiaBraille; text transcription; shadow detection; YIQ color models; color aerial images; moving object segmentation; image data deduplication; iris recognition; two-stage series connected thermoelectric generator; education information system; cyclone separator CFD simulation; imperfect debugging; vulnerability discovery model; stochastic differential equation; cloud data access; attribute based encryption; agile SCRUM framework; PID controller optimisation; hybrid watermarking technique; privacy preservation; vertical partitioned medical database; power amplifier; software reliability growth modeling; cochlear implantation; cellular towers; feedforward neural networks; MBSOM; agent based semantic ontology matching; phonetic word identification; test case selection; MANET security issues; online movie data classification; modified LEACH protocol; mobile ad hoc networks; virtual machine introspection; task scheduling; cluster computing; image compression; green cloud computing; critical health data transmission system; irreversible regenerative Brayton cycle; task set based adaptive round robin scheduling; database security; heterogeneous online social networks; aspect oriented systems; IP network; MPLS network; DBSCAN algorithm; VANET; self-organizing feature map; image segmentation; enzyme classification; wireless sensor networks; energy smart routing protocol; adaptive gateway discovery mechanism; heuristic job scheduling; AODV based congestion control protocol; expert system; home appliances; relay node based heer protocol; data storage; TORA security; data aggregation; low energy adaptive stable energy efficient protocol; fuzzy logic based clustering algorithm; hybrid evolutionary MPLS tunneling algorithm; English mobile teaching; eigenvector centrality; genetic algorithms; data mining; heart disease prediction; lossless data compression; reconfigurable ring resonator; triple band stacked patch antenna; energy based spectrum sensing; cognitive radio networks; FPGA; knowledge representation; multiband microstrip antenna; Web indexing; HTML priority system; Web cache recommender system; e-learning; IT skill learning for visual impaired; user review data analysis; software up-gradation model; software testing; Web crawlers; secret key watermarking; WAV audio file; SRM drive; ZETA converter; fractional PID tuning; medical image reconstruction; speech recognition system; video authentication; digital forensics; content based image retrieval; image classification; hybrid wavelet transform; facial feature extraction; RBSD adder; smart home environment; generalized discrete time model; We Chat marketing; foreign language learning; carbon dioxide emission mitigation; power generation; smartphone storage enhancement; and virtualization.","","978-1-4799-8433-6978-1-4799-8432","10.1109/ABLAZE.2015.7155050","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155050","","","adders;aspect-oriented programming;audio watermarking;biomedical MRI;Brayton cycle;cardiology;character recognition;cloud computing;cochlear implants;cognitive radio;computational fluid dynamics;computer science education;content-based retrieval;cryptography;cyclone separators;data analysis;data compression;data mining;data privacy;diesel engines;differential equations;digital forensics;domestic appliances;drilling;educational administrative data processing;eigenvalues and eigenfunctions;enzymes;expert systems;face recognition;fault diagnosis;feature extraction;feedforward neural nets;field programmable gate arrays;fuzzy control;genetic algorithms;grammars;green computing;handicapped aids;heat exchangers;home computing;image classification;image coding;image colour analysis;image fusion;image reconstruction;image retrieval;image segmentation;image watermarking;indexing;induction motors;internetworking;IP networks;iris recognition;knowledge representation;linguistics;medical image processing;microstrip antennas;mobile learning;modal analysis;nanoelectromechanical devices;object detection;ontologies (artificial intelligence);pattern clustering;power amplifiers;power supply quality;program debugging;program testing;radio spectrum management;recommender systems;reluctance motor drives;resonators;routing protocols;scheduling;self-organising feature maps;social networking (online);software reliability;speaker recognition;speech processing;storage management;telecommunication congestion control;thermoelectric conversion;three-term control;trajectory optimisation (aerospace);tumours;vehicular ad hoc networks;vibrations;video signal processing;virtual machines;virtualisation;wavelet transforms;wind power plants;wireless sensor networks","vibration signal based monitoring;mechanical microdrilling;rule based inflectional urdu stemmer usal;rule based derivational urdu stemmer usal;fuzzy logic controller;heat exchanger temperature process;text dependent speaker recognition;MFCC;SBC;multikeyword based sorted querying;encrypted cloud data;communication understandability enhancement;GSD;parsing;input power quality;switched reluctance motor drive;externally powered upper limb prostheses;program test data generation;launch vehicle optimal trajectory generation;misalignment fault detection;induction motors;current signature analysis;vibration signature analysis;wind power plants;vortex induced vibration;mechanical structure modal analysis;machining parameter optimization;diesel engines;high speed nonvolatile NEMS memory devices;image fusion;RGB color space;LUV color space;offline English character recognition;human skin detection;tumor boundary extraction;MR images;OdiaBraille;text transcription;shadow detection;YIQ color models;color aerial images;moving object segmentation;image data deduplication;iris recognition;two-stage series connected thermoelectric generator;education information system;cyclone separator CFD simulation;imperfect debugging;vulnerability discovery model;stochastic differential equation;cloud data access;attribute based encryption;agile SCRUM framework;PID controller optimisation;hybrid watermarking technique;privacy preservation;vertical partitioned medical database;power amplifier;software reliability growth modeling;cochlear implantation;cellular towers;feedforward neural networks;MBSOM;agent based semantic ontology matching;phonetic word identification;test case selection;MANET security issues;online movie data classification;modified LEACH protocol;mobile ad hoc networks;virtual machine introspection;task scheduling;cluster computing;image compression;green cloud computing;critical health data transmission system;irreversible regenerative Brayton cycle;task set based adaptive round robin scheduling;database security;heterogeneous online social networks;aspect oriented systems;IP network;MPLS network;DBSCAN algorithm;VANET;self-organizing feature map;image segmentation;enzyme classification;wireless sensor networks;energy smart routing protocol;adaptive gateway discovery mechanism;heuristic job scheduling;AODV based congestion control protocol;expert system;home appliances;relay node based heer protocol;data storage;TORA security;data aggregation;low energy adaptive stable energy efficient protocol;fuzzy logic based clustering algorithm;hybrid evolutionary MPLS tunneling algorithm;English mobile teaching;eigenvector centrality;genetic algorithms;data mining;heart disease prediction;lossless data compression;reconfigurable ring resonator;triple band stacked patch antenna;energy based spectrum sensing;cognitive radio networks;FPGA;knowledge representation;multiband microstrip antenna;Web indexing;HTML priority system;Web cache recommender system;e-learning;IT skill learning for visual impaired;user review data analysis;software up-gradation model;software testing;Web crawlers;secret key watermarking;WAV audio file;SRM drive;ZETA converter;fractional PID tuning;medical image reconstruction;speech recognition system;video authentication;digital forensics;content based image retrieval;image classification;hybrid wavelet transform;facial feature extraction;RBSD adder;smart home environment;generalized discrete time model;We Chat marketing;foreign language learning;carbon dioxide emission mitigation;power generation;smartphone storage enhancement;virtualization","","","","","","","","","IEEE","IEEE Conferences"
"Enhancing software quality through systematic object mapping","R. Muthukumar; D. Damodaran","Centre for Reliability, STQC Directorate, Dept. of Electronics and I.T., Ministry of Communications and I.T., Dr.V.S.I.Estate, Thiruvanmiyur, Chennai - 600041, Tamilnadu, India; Centre for Reliability, STQC Directorate, Dept. of Electronics and I.T., Ministry of Communications and I.T., Dr.V.S.I.Estate, Thiruvanmiyur, Chennai - 600041, Tamilnadu, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","4","Indian Software industries are uniquely placed world over as the software solution providers due to the advantage of high calibre, good English speaking technical manpower employers. The companies enjoy a trusted status among the western companies. However India is still lacking in becoming a world leader in providing quality software solution. The software industries demonstrate compliance to specific western standards implying a mere follower rather than a leader. Countries in the South East Asia, Eastern Europe and also South America pose serious threat to Indian Companies from becoming world leaders. Opinions are floating that most serious development cannot be carried out in India owing to less than mature Quality levels of both the outsourced as well as original software. The basic strategy to demonstrate software quality is to reduce the frequency of the defects and issues and evolving mitigating methods to weaken the severity experienced by the users. In this paper an attempt is made to improve the software quality by mapping the relational objects of the software. Functional defect data was gathered for an application Software and categorised into High, Medium and Low type of defects with underlying need for urgent attention of High category ones. The number of defects generated through test cases for each object (module) is normalised and treated to form relational clusters. Dendrogram is drawn and cluster members are arrived. Ward method of maxima and minima tabulated for each of the category of defects and the mean is found. Defects by test cases is found and graded as needing primary attention to that of less significant ones. Modules contributing to the highest defect density is identified and listed. These defects are systematically analysed and root causes identified. By placing the best mitigating practices the Software Quality can be enhanced.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359213","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359213","","","DP industry;object-oriented methods;pattern clustering;software fault tolerance;software process improvement;software quality","software quality enhancement;systematic object mapping;Indian software industries;software solution providers;mitigating methods;software quality improvement;relational object mapping;functional defect data;relational clusters;dendrogram;cluster members;root cause analysis","","","10","","","","","","IEEE","IEEE Conferences"
"State of art in the field of Search-based Mutation Testing","N. Jatana; S. Rani; B. Suri","Maharaja Surajmal Institute of Technology, JanakPuri, New Delhi, India; USICT, GGS Indraprastha University, Dwarka, New Delhi, India; USICT, GGS Indraprastha University, Dwarka, New Delhi, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","Search-based Mutation Testing makes use of Meta-heuristic optimization techniques like Genetic Algorithm, Hill Climbing and other such Evolutionary Approaches for the process of mutation testing. This paper presents the opportunities and challenges faced in applying Search-based techniques to mutation testing. Enormous amount of research has been done in the recent years in this area. The major challenge is the storage and computational cost for a large set of mutants. This paper also lists the latest research, available tools and updates to SBMT.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359256","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359256","Meta-heuristics;Mutation testing;Search-based;Testing Tools","","optimisation;program testing","software testing;SBMT;search-based technique;metaheuristic optimization technique;search-based mutation testing","","1","65","","","","","","IEEE","IEEE Conferences"
"Dynamic metrics are superior than static metrics in maintainability prediction: An empirical case study","H. Sharma; A. Chug","IT, Lal Bahadur Shastri Institute of Management, New Delhi, India; USICT, GGS IP University, New Delhi, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","Software metrics help us to make meaningful estimates for software products and guide us in taking managerial and technical decisions like budget planning, cost estimation, quality assurance testing, software debugging, software performance optimization, and optimal personnel task assignments. Many design metrics have proposed in literature to measure various constructs of Object Oriented (OO) paradigm such as class, coupling, cohesion, inheritance, information hiding and polymorphism and use them further in determining the various aspects of software quality. However, the use of conventional static metrics have found to be inadequate for modern OO software due to the presence of run time polymorphism, templates class, template methods, dynamic binding and some code left unexecuted due to specific input conditions. This gap gave a cue to focus on the use of dynamic metrics instead of traditional static metrics to capture the software characteristics and further deploy them for maintainability predictions. As the dynamic metrics are more precise in capturing the execution behavior of the software system, in the current empirical investigation with the use of open source code, we validate and verify the superiority of dynamic metrics over static metrics. Four machine learning models are used for making the prediction model while training is performed simultaneously using static as well as dynamic metric suite. The results are analyzed using prevalent prediction accuracy measures which indicate that predictive capability of dynamic metrics is more concise than static metrics irrespective of any machine learning prediction model. Results of this would be helpful to practitioners as they can use the dynamic metrics in maintainability prediction in order to achieve precise planning of resource allocation.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359354","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359354","Static metrics;Dynamic metrics;Software maintainability prediction;Software quality;Machine learning","","learning (artificial intelligence);object-oriented methods;public domain software;resource allocation;software maintenance;software metrics;software quality","dynamic metrics;static metrics;maintainability prediction;software metrics;software product estimation;design metrics;object oriented paradigm;OO software;software quality;run time polymorphism;template class;template methods;dynamic binding;software characteristics;open source code;prevalent prediction accuracy;machine learning prediction model;resource allocation","","2","24","","","","","","IEEE","IEEE Conferences"
"Experimental comparison of automated mutation testing tools for java","S. Rani; B. Suri; S. K. Khatri","USICT, GGS Indraprastha University, Delhi, India; USICT, GGS Indraprastha University, Delhi, India; Amity Institute of Information Technology, Amity University, Noida, India","2015 4th International Conference on Reliability, Infocom Technologies and Optimization (ICRITO) (Trends and Future Directions)","","2015","","","1","6","Mutation testing has been used to evaluate the quality of the test set and provides the confidence in testing activity. Generation as well as running of mutants needs practice. A huge amount of mutants are generated and therefore, it is a very costly activity in terms of time and effort. Automatic mutant generation and execution is essential to support testing. Automation achieves more attention, saves time and effort as developers and testers use automated tools. In the recent years, researchers have found that practitioners have diminutive knowledge about tools and their effectiveness. This paper compares five well-known publicly accessible mutation testing tools: `MuClipse', `Judy', `Jumble', `Jester' and PIT. This comparison uses a collection of Java classes taken from various easily accessible sources. Tests were designed and generated with the help of test generation techniques. The performance of each mutation tool was analyzed and was compared based on their mutation operators, mutation score and execution time.","","978-1-4673-7231-2978-1-4673-7230","10.1109/ICRITO.2015.7359265","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7359265","Mutation Testing;Mutation Score;Automated Tools;Test Suite","","Java;program testing;software tools","automated mutation testing tools;test set quality;testing activity;automatic mutant generation;automatic mutant execution;automated tools;MuClipse;Judy tool;Jumble tool;Jester tool;PIT tool;Java classes;test generation techniques;mutation score;execution time","","4","34","","","","","","IEEE","IEEE Conferences"
"Table of contents","","","Proceedings of the 2014 IEEE 15th International Conference on Information Reuse and Integration (IEEE IRI 2014)","","2014","","","i","xiii","The following topics are dealt with: information security &amp; privacy; software design, testing and reuse; social network security; social media, crowdsourcing, and public health; Big Data/networks &amp; cloud computing; information reuse &amp; extraction; multiagent system; data analysis, management &amp; integration; health informatics; data mining and knowledge discovery; AI &amp; decision support systems; formal methods integration; cyber physical systems; and fuzzy systems &amp; heuristic optimization.","","978-1-4799-5880","10.1109/IRI.2014.7051721","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7051721","","","artificial intelligence;Big Data;cloud computing;data analysis;data integration;data privacy;decision support systems;fuzzy systems;information retrieval;medical information systems;multi-agent systems;optimisation;program testing;security of data;social networking (online);software reusability","information security;information privacy;software design;software testing;software reuse;social network security;social media;public health;crowdsourcing;Big Data;cloud computing;information reuse;information extraction;multiagent system;data analysis;data management;data integration;health informatics;data mining;knowledge discovery;AI;decision support systems;formal methods integration;fuzzy systems;heuristic optimization","","","","","","","","","IEEE","IEEE Conferences"
"[Title page i]","","","2015 IEEE Computer Society Annual Symposium on VLSI","","2015","","","i","i","The following topics are dealt with: computer aided design; digital designs; physical design and testing; FPGA and NoC based designs; IP protection; biosignal processing embedded systems; mixed-signal and optimization; energy consumption; unconventional computing; nonvolatile memories; post-CMOS computing systems; secure and trusted systems; software engineering; VLSI; carbon-based materials; THz nanoelectronics; ultra low power digital; fault-tolerant design; reliable design techniques; manycore embedded systems; reliable circuits; power and noise aware systems; analog/RF circuits; signal converter circuits; and analog design and test.","2159-3469;2159-3477","978-1-4799-8719-1978-1-4799-8718","10.1109/ISVLSI.2015.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7308661","","","analogue integrated circuits;CAD;carbon;CMOS integrated circuits;digital integrated circuits;embedded systems;fault tolerance;field programmable gate arrays;integrated circuit design;integrated circuit reliability;integrated circuit testing;low-power electronics;medical signal processing;mixed analogue-digital integrated circuits;nanoelectronics;network-on-chip;optimisation;power aware computing;power consumption;radiofrequency integrated circuits;random-access storage;software engineering;VLSI","reliable circuits;power aware systems;noise aware systems;analog/RF circuits;signal converter circuits;analog design;analog test;manycore embedded systems;reliable design techniques;fault-tolerant design;ultra low power digital;THz nanoelectronics;carbon-based materials;VLSI;software engineering;trusted systems;secure systems;post-CMOS computing systems;nonvolatile memories;unconventional computing;energy consumption;optimization;mixed-signal;biosignal processing;IP protection;NoC;FPGA;physical testing;physical design;digital designs;computer aided design","","","","","","","","","IEEE","IEEE Conferences"
"Contents","","","Proceedings of 3rd International Conference on Reliability, Infocom Technologies and Optimization","","2014","","","1","7","The following topics are dealt with: software engineering; data security; data privacy; pattern classification; learning; neural networks; cloud computing; program testing; wireless sensor networks; computer games; robotics; signal processing; image processing; ontologies; and support vector machines.","","978-1-4799-6896-1978-1-4799-6895-4978-1-4799-6894","10.1109/ICRITO.2014.7014652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7014652","","","cloud computing;computer games;data privacy;learning (artificial intelligence);neural nets;ontologies (artificial intelligence);pattern classification;program testing;security of data;signal processing;software engineering;support vector machines;wireless sensor networks","software engineering;data security;data privacy;pattern classification;learning;neural networks;cloud computing;program testing;wireless sensor networks;computer games;signal processing;image processing;ontologies;support vector machines","","","","","","","","","IEEE","IEEE Conferences"
"Table of content","","","2014 Dynamics of Systems, Mechanisms and Machines (Dynamics)","","2014","","","1","9","The following topics are dealt with: fuzzy network attack classifier; invasive weed optimization; Hartley transform; Fourier transform; digital data processing; oil refining emission automated monitoring system; induction heating; railway car axis unit; supply voltage nonsinusoidality; submersible motors; borehole fluid production; ion-plasma method; ammonia electrothermal microengines; spacecraft; Brillouin backscattering spectrum; optical fibers; synchronous-infase electric drive; diamond wheel flattening; high-precision hard-alloy products; vibration isolated support; hydraulic inertial motion convertor; rubber-cord shell; displacement sensors; helical instability; thermodynamic analysis; mobile compressor power plants; heat loses recuperation; electromechanical handling process parameters; position control electric drive; nickel alloy hardening phases; elliptical polarization electric field intensity; season-acting cooling device operation; climactic soil zone; integrated differential crystal VCO; radiocommunication systems; short diffusers; compact heat exchangers; geometric product specification; security systems; complex network; continuous design automation; petrochemical processes equipment; DC motor; pulse-duration transducer; liquid heating; electrochemical processing; induction systems ;data transmission speed; HF communication system; accidental oil product spills consequences; penalty function calculation; enterprise object layout diagrams; RLC models; electrotechnical devices; CDSE-CDTE system components; oxidation reaction; acid-base property changes; crystal-chemical properties; spectroscopic properties; electrical properties; computer aided design; hybrid discrete optimization; Stewart platform kinematic characteristics; electrical submersible motors; sealing device; parametric space investigation method; finite element modelling; software system; error control code model; media planning optimization treatment; wear surface layer dynamics; base station modernization; phase noise decrease; information transmission radio systems; reference oscillator; thermal stress; quartz resonator; work piece axis radial displacement; cylindrical grinding; information processing systems; hidden operator identification; heart rate variability; reference generator design; pressure control system; pumping units; frequency-controlled induction motor; cognitive modelling; small business management; electric submersible pumps; adjustable speed drive; electro-spark processing; gas-tube boilers; high-precision nonrigid broaches; CNC machines; ultrasonic vibration; mechanical properties; tribological properties; HF-UHF pulse shaping; high-speed circuit testing; integrated circuit testing; viscous fluid splitting; pneumatic shock absorber; frequency hopping spread spectrum; asynchronous radio system; state estimation program; electrical power systems steady-state mode; 3D model; power amplifier-transmitter; Gunn diode; measurement while drilling system; surface magnetic sputtering modification; thermal-oxidative treatment; electrical conductivity properties; magnetic system; magnetoelectric machines; cylindrical grinding machines; active control device; aerodynamic maneuver; carrier rocket; one-component gas generator; fractal queues; road construction machines; asphalt concrete synergetic properties; and modern engineering.","","978-1-4799-6407-9978-1-4799-6406-2978-1-4799-6405","10.1109/Dynamics.2014.7005631","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7005631","","","computer applications;control systems;engineering;heating;machinery;manufacturing processes;materials;mathematics;mechanical properties;networks (circuits);optimisation;pumps;telecommunication;vehicles","fuzzy network attack classifier;invasive weed optimization;Hartley transform;Fourier transform;digital data processing;oil refining emission automated monitoring system;induction heating;railway car axis unit;supply voltage nonsinusoidality;submersible motors;borehole fluid production;ion-plasma method;ammonia electrothermal microengines;spacecraft;Brillouin backscattering spectrum;optical fibers;synchronous-infase electric drive;diamond wheel flattening;high-precision hard-alloy products;vibration isolated support;hydraulic inertial motion convertor;rubber-cord shell;displacement sensors;helical instability;thermodynamic analysis;mobile compressor power plants;heat loses recuperation;electromechanical handling process parameters;position control electric drive;nickel alloy hardening phases;elliptical polarization electric field intensity;season-acting cooling device operation;climactic soil zone;integrated differential crystal VCO;radiocommunication systems;short diffusers;compact heat exchangers;geometric product specification;security systems;complex network;continuous design automation;petrochemical processes equipment;DC motor;pulse-duration transducer;liquid heating;electrochemical processing;induction systems;data transmission speed;HF communication system;accidental oil product spills consequences;penalty function calculation;enterprise object layout diagrams;RLC models;electrotechnical devices;CDSE-CDTE system components;oxidation reaction;acid-base property changes;crystal-chemical properties;spectroscopic properties;electrical properties;computer aided design;hybrid discrete optimization;Stewart platform kinematic characteristics;electrical submersible motors;sealing device;parametric space investigation method;finite element modelling;software system;error control code model;media planning optimization treatment;wear surface layer dynamics;base station modernization;phase noise decrease;information transmission radio systems;reference oscillator;thermal stress;quartz resonator;work piece axis radial displacement;cylindrical grinding;information processing systems;hidden operator identification;heart rate variability;reference generator design;pressure control system;pumping units;frequency-controlled induction motor;cognitive modelling;small business management;electric submersible pumps;adjustable speed drive;electro-spark processing;gas-tube boilers;high-precision nonrigid broaches;CNC machines;ultrasonic vibration;mechanical properties;tribological properties;HF-UHF pulse shaping;high-speed circuit testing;integrated circuit testing;viscous fluid splitting;pneumatic shock absorber;frequency hopping spread spectrum;asynchronous radio system;state estimation program;electrical power systems steady-state mode;3D model;power amplifier-transmitter;Gunn diode;measurement while drilling system;surface magnetic sputtering modification;thermal-oxidative treatment;electrical conductivity properties;magnetic system;magnetoelectric machines;cylindrical grinding machines;active control device;aerodynamic maneuver;carrier rocket;one-component gas generator;fractal queues;road construction machines;asphalt concrete synergetic properties;modern engineering","","","","","","","","","IEEE","IEEE Conferences"
"Table of contents","","","2014 International Conference on Applied Electronics","","2014","","","vii","xii","The following topics are dealt with: high-speed analog-to-digital converters; nanometer CMOS technology; pseudo random code regeneration; oscillation frequency value; oscillation-based tests; reconfigurable filter structure; universal embedded controller; matrix converter; continuous-time sigma-delta modulator; integrated LC filter; human-robot cooperation; seismic sensor system; MEMS accelerometer; memristor pinched hysteresis loops; PV panels; MPPT controller; PV power system modelling; PLP feature extraction optimization; LVCSR recognition; MP3 data; dead-time compensation strategy; adaptive harmonic compensator; Cartesian genetic programming; desktop based real time oxygen auto-ventilation; gas monitoring system; homecare respiratory application; subband optimization; EEG-based classification; HART compatible HART line powered communication module; interactive hybrid control-flow checking method; polyphase FIR filter structure; VHDL language; FPGA; B3C converter; heat transfer; electronic system; moving object searching; virtual instrumentation; ISO26262 motorbikes; nonlinear stabilization; state space energy error feedback; quadrature oscillator; operational transresistance amplifier; quantum computation system emulator; low noise amplifier; SVPWM algorithm; H-bridge power inverter; LCL filter; 3D machine vision system; contact strips inspection; railway vehicle current collectors; wrist cuff method; mean arterial pressure; dual-cuff blood pressure system; contactless electrical energy transfer; varying air gap; resonant converter; nonlinear inductance; programmable PWM modulator; OPWM test platform; 3D FEM simulation; EMC testing; input current harmonics; voltage-source active rectifier; motion detection system; NiTi pressure sensors; phase coupled oscillator; dissipation normal form; space vector pulse width modulation; RF circuits response; software defined radio system; adaptive mechanical model; cardiovascular system; RF network combiner; wireless multicommunication system; smart households; graph method; SI circuit solution; M-FSK intrapulse modulation analysis; subspectral decomposition method; small loop antennas parameter measurement; GTEM cell; radio frequency energy harvesting system; boost power factor correction topology; average current control; critical conduction mode; EKF based position estimators; speed estimators; sensorless DTC; permanent magnet synchronous machines; E-D-Mode InAlN-GaN HFET inverter; low frequency amplifier; real-time multicore systems; genetic algorithm; runnable sequencing; bandgap voltage reference; high-order temperature compensation; two stepped system; train dynamic motion; wheel slip; CAN nodes health monitoring; ADS-B channel; electrical resistance measurement; contact resistance; sensor fusion; IR sensors; Kalman filter;CMOS class-AB amplifier; rendering moving sound source; headphone-based virtual acoustic reality; high spectral quality sinusoidal oscillator; energy based amplitude control; wireless electronic systems; physiological parameters measuring; hidden pacemaker pulses detection; wavelet transform; Hilbert transform; feedback control; electrospinning process; iterated maps; nonlinear dynamics generation; genetic reasoning; finger sign identification; forearm electromyogram; statistical multirate high-resolution signal reconstruction; empirical mode decomposition based denoising approach; RF signal denoising; OFDM system; AWGN channels; feedforward approach; DC cancellation; fully-differential instrumentation amplifiers; fuzzy expert systems; and human gait phase.","1803-7232","978-8-0261-0277-9978-8-0261-0276","10.1109/AE.2014.7011653","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7011653","","","AWGN channels;carrier transmission on power lines;CMOS integrated circuits;compensation;computer vision;contact resistance;electric current control;electric resistance measurement;electroencephalography;electromagnetic compatibility;electromyography;electrospinning;energy harvesting;expert systems;feature extraction;field programmable gate arrays;fingerprint identification;finite element analysis;FIR filters;frequency shift keying;gait analysis;genetic algorithms;graph theory;high electron mobility transistors;Hilbert transforms;human-robot interaction;infrared detectors;instrumentation amplifiers;Kalman filters;loop antennas;low noise amplifiers;matrix convertors;maximum power point trackers;memristors;microsensors;motorcycles;multiprocessing systems;OFDM modulation;operational amplifiers;oscillators;permanent magnet machines;photovoltaic power systems;power factor correction;pressure sensors;PWM invertors;random number generation;rectifiers;resonant power convertors;sensor fusion;sensorless machine control;sigma-delta modulation;signal denoising;signal reconstruction;software radio;synchronous machines;TEM cells;traction current collection;ventilation;virtual instrumentation;virtual reality;wavelet transforms","high-speed analog-to-digital converters;nanometer CMOS technology;pseudo random code regeneration;oscillation frequency value;oscillation-based tests;reconfigurable filter structure;universal embedded controller;matrix converter;continuous-time sigma-delta modulator;integrated LC filter;human-robot cooperation;seismic sensor system;MEMS accelerometer;memristor pinched hysteresis loops;PV panels;MPPT controller;PV power system modelling;PLP feature extraction optimization;LVCSR recognition;MP3 data;dead-time compensation strategy;adaptive harmonic compensator;Cartesian genetic programming;desktop based real time oxygen auto-ventilation;gas monitoring system;homecare respiratory application;subband optimization;EEG-based classification;HART compatible HART line powered communication module;interactive hybrid control-flow checking method;polyphase FIR filter structure;VHDL language;FPGA;B3C converter;heat transfer;moving object searching;virtual instrumentation;ISO26262 motorbikes;nonlinear stabilization;state space energy error feedback;quadrature oscillator;operational transresistance amplifier;quantum computation system emulator;low noise amplifier;SVPWM algorithm;H-bridge power inverter;LCL filter;3D machine vision system;contact strips inspection;railway vehicle current collectors;wrist cuff method;mean arterial pressure;dual-cuff blood pressure system;contactless electrical energy transfer;varying air gap;resonant converter;nonlinear inductance;programmable PWM modulator;OPWM test platform;3D FEM simulation;EMC testing;input current harmonics;voltage-source active rectifier;motion detection system;NiTi pressure sensors;phase coupled oscillator;dissipation normal form;space vector pulse width modulation;RF circuits response;software defined radio system;adaptive mechanical model;cardiovascular system;RF network combiner;wireless multicommunication system;smart households;graph method;SI circuit solution;M-FSK intrapulse modulation analysis;subspectral decomposition method;small loop antennas parameter measurement;GTEM cell;radio frequency energy harvesting system;boost power factor correction topology;average current control;critical conduction mode;EKF based position estimators;speed estimators;sensorless DTC;permanent magnet synchronous machines;E-D-Mode InAlN-GaN HFET inverter;low frequency amplifier;real-time multicore systems;genetic algorithm;runnable sequencing;bandgap voltage reference;high-order temperature compensation;two stepped system;train dynamic motion;wheel slip;CAN nodes health monitoring;ADS-B channel;electrical resistance measurement;contact resistance;sensor fusion;IR sensors;Kalman filter;CMOS class-AB amplifier;rendering moving sound source;headphone-based virtual acoustic reality;high spectral quality sinusoidal oscillator;energy based amplitude control;wireless electronic systems;physiological parameters measuring;hidden pacemaker pulses detection;wavelet transform;Hilbert transform;feedback control;electrospinning process;iterated maps;nonlinear dynamics generation;genetic reasoning;finger sign identification;forearm electromyogram;statistical multirate high-resolution signal reconstruction;empirical mode decomposition based denoising approach;RF signal denoising;OFDM system;AWGN channels;feedforward approach;DC cancellation;fully-differential instrumentation amplifiers;fuzzy expert systems;human gait phase","","","","","","","","","IEEE","IEEE Conferences"
"[Copyright notice]","","","2013 Federated Conference on Computer Science and Information Systems","","2013","","","iii","iii","The following topics are dealt with: artificial intelligence; medical applications; semantic information retrieval; computational optimization; network systems; computer aspects of numerical algorithms; multimedia applications and processing; research methods; information technology advances; information systems education; curricula workshop; innovative network systems; network applications; Web services; wireless sensor networks; information technology for management, business and society; advances in business ICT; agent-based systems; advanced information technology for management; logistics; knowledge acquisition and management; mobile commerce; software systems development and applications; automating test case design, selection and evaluation; cyber-physical systems; business database applications; and advances in programming languages.","","978-83-60810-52","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6643965","","","artificial intelligence;educational courses;information retrieval;information science education;information systems;information technology;medical computing;multimedia computing;optical waveguides;optimisation;research and development;Web services;wireless sensor networks","software systems development;software systems applications;test case design automation;cyber-physical systems;business database applications;programming languages;mobile commerce;knowledge management;knowledge acquisition;logistics;advanced information technology;agent-based systems;business ICT;society;wireless sensor networks;Web services;network applications;innovative network systems;curricula workshop;information systems education;information technology advances;research methods;multimedia processing;multimedia applications;numerical algorithms;computer aspects;computational optimization;semantic information retrieval;medical applications;artificial intelligence","","","","","","","","","IEEE","IEEE Conferences"
"DoE-based performance optimization of energy management in sensor nodes powered by tunable energy-harvesters","T. J. Kazmierski; L. Wang; B. Al-Hashimi; G. Merrett","Faculty of Physics and Applied Science, University of Southampton, UK; Faculty of Physics and Applied Science, University of Southampton, UK; Faculty of Physics and Applied Science, University of Southampton, UK; Faculty of Physics and Applied Science, University of Southampton, UK","2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)","","2013","","","484","484","An energy-harvester-powered wireless sensor node is a complicated system with many design parameters. To investigate the various trade-offs among these parameters, it is desirable to explore the multi-dimensional design space quickly. However, due to the large number of parameters and costly simulation CPU times, it is often difficult or even impossible to explore the design space via simulation. A design of experiment (DoE) approach using the response surface model (RSM) technique can enable fast design space exploration of a complete wireless sensor node powered by a tunable energy harvester. As a proof of concept, a software toolkit has been developed which implements the DoE-based design flow and incorporates the energy harvester, tuning controller and wireless sensor node. Several test scenarios are considered, which illustrate how the proposed approach permits the designer to adjust a wide range of system parameters and evaluate the effect almost instantly but still with high accuracy‥","1530-1591;1530-1591;1530-1591","978-3-9815370-0-0978-1-4673-5071","10.7873/DATE.2013.110","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6513556","energy harvesters;design of experiment;wireless sensor nodes","","","","","","4","","","","","","IEEE","IEEE Conferences"
"[Front cover]","","","2015 First International Conference on New Technologies of Information and Communication (NTIC)","","2015","","","c1","c1","The following topics are dealt with: MANET; target tracking; VANET; V2V communication; packet load enhancement; ad hoc routing protocol simulation; VolP traffic support; mobile ad hoc networks; path planning; fixed wing UAV formation; ARTI taxonomy; cyber-attack framework; enhancement optimized MAC protocol; block design multichannel MAC protocol; WSN; PAPR reduction; STBC MIMO-OFDM systems; 4G wireless communications; PTS scheme; SCADA open protocol PUR 2.4; smart walk mechanism; unstructured mobile P2P networks; BLE-based data collection system; Finger-Knuckle-print identification; SVM classifier; optimal object-oriented image classification; MLLH approach; image denoising algorithm; multireprsentation palmprint image; automatic personnel identification; optimized audio watermarking scheme; swarm intelligence; speech synthesis; Arabic language; model transformation; time-extended multirobot task allocation problem; Web applications; model-based approach; Chi-Square test; heuristic search; metamorphic malware detection; materialized view definition; semantic Web service; content-based dermoscopic image retrieval; and fuzzy labeled transitions system.","","978-1-4673-6685-4978-1-4673-6684-7978-1-4673-6683","10.1109/NTIC.2015.7368734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7368734","","","4G mobile communication;access protocols;audio watermarking;autonomous aerial vehicles;content-based retrieval;distributed databases;fuzzy set theory;image classification;image denoising;image representation;image retrieval;Internet of Things;Internet telephony;invasive software;mobile ad hoc networks;multi-robot systems;OFDM modulation;palmprint recognition;path planning;peer-to-peer computing;routing protocols;semantic Web;speech synthesis;support vector machines;swarm intelligence;target tracking;Web services","MANET;target tracking;VANET;V2V communication;packet load enhancement;ad hoc routing protocol simulation;VolP traffic support;mobile ad hoc networks;path planning;fixed wing UAV formation;ARTI taxonomy;cyber-attack framework;enhancement optimized MAC protocol;block design multichannel MAC protocol;WSN;PAPR reduction;STBC MIMO-OFDM systems;4G wireless communications;PTS scheme;SCADA open protocol PUR 2.4;smart walk mechanism;unstructured mobile P2P networks;BLE-based data collection system;Finger-Knuckle-print identification;SVM classifier;optimal object-oriented image classification;MLLH approach;image denoising algorithm;multireprsentation palmprint image;automatic personnel identification;optimized audio watermarking scheme;swarm intelligence;speech synthesis;Arabic language;model transformation;time-extended multirobot task allocation problem;Web applications;model-based approach;Chi-Square test;heuristic search;metamorphic malware detection;materialized view definition;semantic Web service;content-based dermoscopic image retrieval;fuzzy labeled transitions system","","","","","","","","","IEEE","IEEE Conferences"
"[Title page i]","","","2015 Fifth International Conference on Instrumentation and Measurement, Computer, Communication and Control (IMCCC)","","2015","","","i","i","The following topics are dealt with: actuators; instrumentation; satellite data management system; energy detection; fuzzy clustering; data acquisition; 3D reconstruction; anomaly detection; EEG signal; corona current measurement system; measurement signal processing; voltage measurement; power measurement; frequency measurement; impedance precision measurement; virtual test; virtual measurement; network computing; parallel computing; artificial intelligence; pattern recognition; software module; encoding; modulation; multiplexing; MIMO system; adaptive control; robust control; distributed control; optimized control; embedded systems; navigation system; control equipment; sensor information processing; satellite attitude control system; decoding; and demodulation.","","978-1-4673-7723-2978-1-4673-7722","10.1109/IMCCC.2015.1","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7405780","","","computers;control engineering;instrumentation;measurement;telecommunication","multiplexing;MIMO system;adaptive control;robust control;distributed control;optimized control;embedded systems;navigation system;control equipment;sensor information processing;satellite attitude control system;decoding;demodulation;modulation;encoding;software module;pattern recognition;artificial intelligence;parallel computing;network computing;virtual measurement;virtual test;impedance precision measurement;frequency measurement;power measurement;voltage measurement;measurement signal processing;corona current measurement system;EEG signal;anomaly detection;3D reconstruction;data acquisition;fuzzy clustering;energy detection;satellite data management system;instrumentation;actuators","","","","","","","","","IEEE","IEEE Conferences"
"Table of contents","","","2015 IEEE International Conference on Communication Workshop (ICCW)","","2015","","","1","46","The following topics are dealt with: usability of higher frequency bands for back-haul/front-haul; smart back-haul/front-haul solutions for emerging 5G technologies; higher capacity back-haul/front-haul links; flexible duplexing techniques and user association; interference and mobility management, device-to-device (D2D) communication; optimized transmission design; D2D, energy efficiency and back-haul issues; ambient assisted living platforms and systems; user-centric services and devices; fiber supported wireless networks; fiber wireless techniques; software defined networks; context-aware wireless networks; context-aware cognitive networks; multiantenna secure transmissions; coding and cooperative secrecy; secure communication design and performance analysis; spectrum sharing technologies; MIMO and cognitive radio; D2D resource allocation, performance analysis and mode selection; D2D enabling technologies; relay, multihop, and networking aspects of D2D; robust localization; SLAM and map awareness; cooperative localisation; fundamental limits; cognitive radio networks; cooperative and relaying networks; cooperative cognitive mobile networks; 5G enabling technologies, architecture, and design; OFDM over visible light communication; visible light communication MIMO; LTE and machine-to-machine communication; smart systems and network coding; network and communications; user behavior and characteristics' impact on dynamic social networks; spectrum sensing; resource allocation; channel propagation and issues; subjective testing; quality of experience frameworks and management; data centers and cloud networks; cloud services and application; smart grids and energy harvesting; uncoordinated medium access; advance physical layer for superdense networks; physical layer network coding and cooperative communication; advanced MAC for superdense networks; advanced MIMO and multicell technology; advanced MAC design; coexistence in unlicensed band; dependable vehicular communications; radar and sonar networks; 5G heterogeneous networks; wireless converged networks; cloud and middleware security; secure communications; intrusion detection and risk management; green physical layer; green MAC layer; green networking and Internet of Things; and green cellular networks.","2164-7038","978-1-4673-6305-1978-1-4673-6304","10.1109/ICCW.2015.7247065","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7247065","","","5G mobile communication;access protocols;assisted living;cellular radio;cloud computing;cognitive radio;computer centres;cooperative communication;energy conservation;energy harvesting;green computing;Long Term Evolution;microwave photonics;middleware;MIMO communication;mobility management (mobile radio);multiplexing;network coding;optical fibre subscriber loops;power engineering computing;quality of experience;radio spectrum management;radiofrequency interference;radionavigation;relay networks (telecommunication);resource allocation;smart power grids;social networking (online);telecommunication power management;telecommunication security;telecommunication services;ubiquitous computing","higher frequency band usability;smart back-haul solutions;smart front-haul solutions;emerging 5G technology;higher capacity back-haul link;higher capacity front-haul link;flexible duplexing techniques;user association;interference management;mobility management;device-to-device communication;optimized transmission design;energy efficiency;back-haul issues;ambient assisted living;user-centric services;user-centric devices;fiber supported wireless networks;fiber wireless techniques;software defined networks;context aware wireless networks;context aware cognitive networks;multiantenna secure transmissions;network coding;cooperative secrecy;secure communication design;spectrum sharing technologies;MIMO communication;resource allocation;relay networks;multihop network;robust localization;SLAM;map awareness;cooperative localisation;fundamental limits;cognitive radio networks;cooperative relaying;cooperative cognitive mobile networks;5G enabling technologies;OFDM;visible light communication;LTE;machine-to-machine communication;smart systems;user behavior;dynamic social networks;spectrum sensing;channel propagation;subjective testing;quality of experience;data centers;cloud networks;cloud services;smart power grids;energy harvesting;uncoordinated medium access;advance physical layer;superdense networks;cooperative communication;multicell technology;advanced MAC design;unlicensed band coexistence;dependable vehicular communications;radar networks;sonar networks;5G heterogeneous networks;wireless converged networks;cloud security;middleware security;secure communications;intrusion detection;risk management;green physical layer;green MAC layer;green networking;Internet of Things;green cellular networks","","","","","","","","","IEEE","IEEE Conferences"
